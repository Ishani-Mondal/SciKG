{"title": [{"text": "How we BLESSed distributional semantic evaluation", "labels": [], "entities": [{"text": "BLESSed", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.9911321997642517}, {"text": "distributional semantic evaluation", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.8054298559824625}]}], "abstractContent": [{"text": "We introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9970118999481201}]}, {"text": "BLESS contains a set of tu-ples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9437206983566284}]}, {"text": "It is thus possible to assess the ability of a model to detect truly related word pairs, as well as to perform in-depth analyses of the types of semantic relations that a model favors.", "labels": [], "entities": []}, {"text": "We discuss the motivations for BLESS, describe its construction and structure , and present examples of its usage in the evaluation of distributional semantic models.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9359169006347656}]}], "introductionContent": [{"text": "In NLP, it is customary to distinguish between intrinsic evaluations, testing a system in itself, and extrinsic evaluations, measuring its performance in some task or application.", "labels": [], "entities": []}, {"text": "For instance, the intrinsic evaluation of a dependency parser will measure its accuracy in identifying specific syntactic relations, while its extrinsic evaluation will focus on the impact of the parser on tasks such as question answering or machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9986401200294495}, {"text": "question answering", "start_pos": 220, "end_pos": 238, "type": "TASK", "confidence": 0.8312720358371735}, {"text": "machine translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.7266616225242615}]}, {"text": "Current approaches to the evaluation of Distributional Semantic Models (DSMs, also known as semantic spaces, vector-space models, etc.; see fora survey) are taskoriented.", "labels": [], "entities": []}, {"text": "Model performance is evaluated in \"semantic tasks\", such as detecting synonyms, recognizing analogies, modeling verb selectional preferences, ranking paraphrases, etc.", "labels": [], "entities": [{"text": "detecting synonyms", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.8765234351158142}]}, {"text": "Measuring the performance of DSMs on such tasks represents an indirect test of their ability to capture lexical meaning.", "labels": [], "entities": []}, {"text": "The task-oriented benchmarks adopted in distributional semantics have not specifically been designed to evaluate DSMs.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 113, "end_pos": 117, "type": "TASK", "confidence": 0.952260434627533}]}, {"text": "For instance, the widely used TOEFL synonym detection task was designed to test the learners' proficiency in English as a second language, and not to investigate the structure of their semantic representations (cf. Section 2).", "labels": [], "entities": [{"text": "TOEFL synonym detection task", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8705025613307953}]}, {"text": "To gain areal insight into the abilities of DSMs to address lexical semantics, existing benchmarks must be complemented with a more intrinsically oriented approach, to perform direct tests on the specific aspects of lexical knowledge captured by the models.", "labels": [], "entities": []}, {"text": "In order to achieve this goal, three conditions must be met: (i) to single out the particular aspects of meaning that we want to focus on in the evaluation of DSMs; (ii) to design a data set that is able to explicitly and reliably encode the target semantic information; (iii) to specify the evaluation criteria of the system performance on the data set, in order to get an estimate of the intrinsic ability of DSMs to cope with the selected semantic aspects.", "labels": [], "entities": []}, {"text": "In this paper, we address these three conditions by presenting BLESS (Baroni and Lenci Evaluation of Semantic Spaces), anew data set specifically geared towards the intrinsic evaluation of DSMs, downloadable from: http://clic.cimec.unitn.it/distsem.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9988881945610046}]}], "datasetContent": [{"text": "DSMs measure the distributional similarity between words, under the assumption that proximity in distributional space models semantic relatedness, including, as a special case, semantic similarity).", "labels": [], "entities": []}, {"text": "However, semantically related words in turn differ for the type of relation holding between them: e.g., dog is strongly related to both animal and tail, but with different types of relations.", "labels": [], "entities": []}, {"text": "Therefore, evaluating the intrinsic ability of DSMs to represent the semantic space of a word entails both (i) determining to what extent words close in semantic space are actually semantically related, and (ii) analyzing, among related words, which type of semantic relation they tend to instantiate.", "labels": [], "entities": []}, {"text": "Two models can be equally very good in identifying semantically related words, while greatly differing for the type of related pairs they favor.", "labels": [], "entities": [{"text": "identifying semantically related words", "start_pos": 39, "end_pos": 77, "type": "TASK", "confidence": 0.7711574733257294}]}, {"text": "The BLESS data set complies with both these constraints.", "labels": [], "entities": [{"text": "BLESS data set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9280070463816324}]}, {"text": "The set is populated with tuples expressing a relation between a target concept (henceforth referred to as concept) and a relatum concept (henceforth referred to as relatum).", "labels": [], "entities": []}, {"text": "For instance, in the BLESS tuple coyote-hyper-animal, the concept coyote is linked to the relatum animal via the hypernymy relation (the relatum is a hypernym of the concept).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.7610857486724854}]}, {"text": "BLESS focuses on a coherent set of basiclevel nominal concrete concepts and a small but explicit set of semantic relations, each instantiated by multiple relata.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9260631799697876}]}, {"text": "Depending on the type of relation, relata can be nouns, verbs or adjectives.", "labels": [], "entities": []}, {"text": "Moreover, BLESS also contains, for each concept, a number of random \"relatum\" words that are not semantically related to the concept.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9743624925613403}]}, {"text": "Thus, it also allows to evaluate a model in terms of its ability to harvest related words given a concept (by comparing true and random relata), and to identify specific types of relata, both in terms of semantic relation and part of speech.", "labels": [], "entities": []}, {"text": "A data set intending to represent a gold standard for evaluation should include tests items that are as little controversial as possible.", "labels": [], "entities": []}, {"text": "The choice of restricting BLESS to concrete concepts is motivated by the fact that they are by far the most studied ones, and there is better agreement about the relations that characterize them).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8934614658355713}]}, {"text": "As for the types of relation to include, we are faced with a dilemma.", "labels": [], "entities": []}, {"text": "On the one hand, there is wide evidence that taxonomic relations, the best understood type, only represent a tiny portion of the rich spectrum covered by semantic relatedness.", "labels": [], "entities": []}, {"text": "On the other hand, most of these wider semantic rela-tions are also highly controversial, and may easily lead to questionable classifications.", "labels": [], "entities": []}, {"text": "For instance, concepts are related to events, but often it is not clear how to distinguish the events expressing atypical function of nominal concepts (e.g., car and transport), from those events that are also strongly related to them but without representing their typical function sensu stricto (e.g., car and fix).", "labels": [], "entities": []}, {"text": "As will be shown in Section 4, the BLESS data set tries to overcome this dilemma by attempting a difficult compromise: Semantic relations are not limited to taxonomic types and also include attributes and events strongly related to a concept, but in these cases we have resorted to underspecification, rather than committing ourselves to questionable granular relations.", "labels": [], "entities": [{"text": "BLESS data set", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.895683209101359}]}, {"text": "BLESS strives to capture those differences and similarities among DSMs that do not depend on coverage, processing choices or lexical preferences.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9413906931877136}]}, {"text": "BLESS has been constructed using a publicly available collection of corpora for reference (see Section 4.4 below), which means that anybody can train a DSM on the same data and be sure to have perfect coverage (but this is not strictly necessary).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5440474152565002}]}, {"text": "For each concept and relation, we pick a variety of relata (see next section) in order to abstract away from incidental gaps of models or different lexical/topical preferences.", "labels": [], "entities": []}, {"text": "For example, the concept robin has 7 hypernyms including the very general and non-technical animal and bird and the more specific and technical passerine.", "labels": [], "entities": []}, {"text": "A model more geared toward technical terminology might assign a high similarity score to the latter, whereas a commonsense-knowledgeoriented DSM might pick bird.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 69, "end_pos": 85, "type": "METRIC", "confidence": 0.963484525680542}, {"text": "DSM", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.7950919270515442}]}, {"text": "Both models have captured similarity with a hypernym, and we have no reason, in general semantic terms, to penalize one or the other.", "labels": [], "entities": []}, {"text": "To maximize coverage, we also make sure that, for each concept and relation, a reasonable number of relata are frequently attested in our reference corpora (see statistics below), we only include single-word relata and, where appropriate, we include multiple forms for the same relatum (both sock and socks as coordinates of scarf -as discussed in Section 4.1, we avoided similar ambiguous items as target concepts).", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.940068244934082}]}, {"text": "Currently, distributional models for attributional similarity and relational similarity) are tested on different data sets, e.g., TOEFL and SAT respectively (briefly, attributional similarity pertains to similarity between a pair of concepts in terms of shared properties, whereas relational similarity measures the similarity of the relations instantiated by couples of concept pairs).", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.5319443941116333}, {"text": "SAT", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.8952744603157043}]}, {"text": "Conversely, BLESS is not biased towards any particular type of semantic similarity and thus allows both families of models to be evaluated on the same data set.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9944508075714111}]}, {"text": "Given a concept, we can analyze the types of relata that are selected by a model as more attributionally similar to the target.", "labels": [], "entities": []}, {"text": "Alternatively, given a concept-relatum pair instantiating a specific semantic relation (e.g., hypernymy) we can evaluate a model ability to identify analogically similar pairs, i.e., others conceptrelatum pairs instantiating the same relation (we do not illustrate this possibility here).", "labels": [], "entities": []}, {"text": "Finally, by collecting distributions of 200 similarity values for each relation, BLESS allows reliable statistical testing of the significance of differences in similarity within a DSM (for example, using the procedure we present in Section 5 below), as well as across DSMs (for example, via a linear/ANOVA model with relations and DSMs as factors -not illustrated here).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9940390586853027}]}, {"text": "All 200 BLESS concepts are single-word nouns in the singular form (we avoided concepts such as socks whose surface form might change depending on lemmatization choices).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9952806830406189}]}, {"text": "The major source we used to select the concepts were the McRae Norms (), a collection of living and nonliving basic-level concepts described by 725 sub-jects with semantic features, each tagged with its property type.", "labels": [], "entities": [{"text": "McRae Norms", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9406193792819977}]}, {"text": "As further constraints guiding our selection, we wanted concepts with a reasonably high frequency (cf. Section 4.4), we avoided ambiguous or highly polysemous concepts and we balanced inter-and intra-class composition.", "labels": [], "entities": []}, {"text": "Classes include both prototypical and atypical instances (e.g., robin and penguin for BIRD), and have a wide spectrum of internal variation (e.g., the class VEHICLE contains wheeled, air and sea vehicles).", "labels": [], "entities": [{"text": "BIRD", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.5372043251991272}, {"text": "VEHICLE", "start_pos": 157, "end_pos": 164, "type": "DATASET", "confidence": 0.8695181012153625}]}, {"text": "175 BLESS concepts are attested in the McRae Norms, while the remnants were selected by the authors according to the above constraints.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9975726008415222}, {"text": "McRae Norms", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9711536467075348}]}, {"text": "The average number of concepts per class is 11.76 (median 11; min.", "labels": [], "entities": []}, {"text": "5 AMPHIB-IAN REPTILE; max.", "labels": [], "entities": [{"text": "AMPHIB-IAN", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9893677830696106}, {"text": "REPTILE", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.7738912105560303}]}, {"text": "This section illustrates one possible way to use BLESS to explore and evaluate DSMs.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9466121196746826}, {"text": "DSMs", "start_pos": 79, "end_pos": 83, "type": "TASK", "confidence": 0.8568183183670044}]}, {"text": "Given the similarity scores provided by a model fora concept with all its relata across all relations, we pick the relatum with the highest score (nearest neighbour) for each relation (see discussion in Section 3 above on why we allow models to pick their favorite from a set of relata instantiating the same relation).", "labels": [], "entities": []}, {"text": "In this way, for each of the 200 BLESS concepts, we obtain 8 similarity scores, one per relation.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9892036318778992}, {"text": "similarity", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.960342288017273}]}, {"text": "In order to factor out concept-specific effects that might add to the overall score variance (for example, a frequent concept might have a denser neighborhood than a rarer one, and consequently the nearest relatum scores of the former are trivially higher than those of the latter), we transform the 8 similarity scores of each concept onto standardized z scores (mean: 0; s.d: 1) by subtracting from each their mean, and dividing by their standard deviation.", "labels": [], "entities": []}, {"text": "After this transformation, we produce a boxplot summarizing the distribution of scores per relation across the 200 concepts (i.e., each box of the plot summarizes the distribution of the 200 standardized scores picked for each relation).", "labels": [], "entities": []}, {"text": "Our boxplots (see examples in display the median of a distribution as a thick horizontal line within a box extending from the first to the third quartile, with whiskers covering 1.5 of the interquartile range in each direction from the box, and values outside this extended range -extreme outliers -plotted as circles (these are the default boxplotting option of the R statistical package).", "labels": [], "entities": []}, {"text": "While the boxplots are extremely informative about the relation types that are best captured by models, we expect some degree of overlap among the distributions of different relations, and in such cases we might want to ask whether a certain model assigns significantly higher scores to one relation rather than another (for example, to coordinates rather than random nouns).", "labels": [], "entities": []}, {"text": "It is difficult to decide a priori which pairwise statistical comparisons will be interesting.", "labels": [], "entities": []}, {"text": "We thus take a conservative approach in which we perform all pairwise comparisons using the Tukey Honestly Significant Difference test, that is similar to the standard t test, but accounts for the greater likelihood of Type I errors when multiple comparisons are performed (.", "labels": [], "entities": [{"text": "Tukey", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.6551392674446106}, {"text": "Honestly Significant Difference test", "start_pos": 98, "end_pos": 134, "type": "METRIC", "confidence": 0.6250872761011124}]}, {"text": "We only report the Tukey test results for those comparisons that are of interest in the analysis of the boxplots, using the standard \u03b1 = 0.05 significance threshold.", "labels": [], "entities": [{"text": "\u03b1 = 0.05 significance threshold", "start_pos": 133, "end_pos": 164, "type": "METRIC", "confidence": 0.675154572725296}]}], "tableCaptions": [{"text": " Table 1: Distribution (minimum, mean and maximum) of  the relata of all BLESS concepts: the frequency columns  report summary statistics for corpus counts across relata  instantiating a relation; the cardinality columns report  summary statistics for number of relata instantiating a  relation across the 200 concepts, only considering relata  with corpus frequency \u2265 100.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.8651644587516785}]}]}