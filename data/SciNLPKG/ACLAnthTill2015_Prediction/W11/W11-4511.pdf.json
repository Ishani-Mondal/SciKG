{"title": [{"text": "Improving Lexical Alignment Using Hybrid Discriminative and Post-Processing Techniques", "labels": [], "entities": [{"text": "Improving Lexical Alignment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8968777855237325}]}], "abstractContent": [{"text": "Automatic lexical alignment is a vital step for empirical machine translation, and although good results can be obtained with existent models (e.g. Giza++), more precise alignment is still needed for successfully handling complex constructions such as multiword expressions.", "labels": [], "entities": [{"text": "Automatic lexical alignment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7407930890719095}, {"text": "empirical machine translation", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.6140997310479482}]}, {"text": "In this paper we propose an approach for lexical alignment combining statistical and linguistic information.", "labels": [], "entities": [{"text": "lexical alignment", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7428025007247925}]}, {"text": "We describe the development of a baseline discriminative aligner and a set of language dependent post-processing functions that allow the inclusion of shallow linguistic knowledge.", "labels": [], "entities": []}, {"text": "The post-processing functions were designed to significantly improve word alignment mainly on verb-particle constructs both over our baseline and over Giza++.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7665015757083893}]}], "introductionContent": [{"text": "Automatic lexical (word) alignment is a previous step necessary for the creation of an empirical machine translation system.", "labels": [], "entities": [{"text": "Automatic lexical (word) alignment", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.680675228436788}, {"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7260726392269135}]}, {"text": "Given a sentencealigned parallel corpus, the lexical alignment process consists of producing a set of relations (alignments) between the lexical units of both languages.", "labels": [], "entities": []}, {"text": "The lexical units are usually simple words, but may also be multiword expressions (MWEs), which can be defined as \"idiosyncratic interpretations that crossword boundaries (or spaces)\".", "labels": [], "entities": []}, {"text": "The task of lexical alignment has traditionally been done in a completely statistical, unsupervised manner, whereby a generative aligner tries to infer the parameters of a model of the statistical process by which a source sentence generates a target one.", "labels": [], "entities": [{"text": "lexical alignment", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7085614800453186}]}, {"text": "[] describe a set of increasingly complex generative processes (the IBM models) and algorithms to estimate its parameters.", "labels": [], "entities": [{"text": "IBM models", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9096924662590027}]}, {"text": "The difficulty in adding complex linguistic knowledge to generative models has given rise to several so-called discriminative lexical aligners such as in and in.", "labels": [], "entities": []}, {"text": "Ina discriminative aligner it is not necessary to model a complex statistical problem, instead a set of feature functions are created each one capturing a specific facet of a word alignment.", "labels": [], "entities": []}, {"text": "Thus, the features are combined (usually linearly) and it is then necessary to search the space of all possible alignments for the one which has the highest score.", "labels": [], "entities": []}, {"text": "For example,] linearly combines a small number of features, while propose a very similar technique, but using a log-linear combination of the individual features.", "labels": [], "entities": []}, {"text": "With respect to parameter training, techniques range from voted perceptron] to conditional random field, while go even further and propose a semi-supervised parameter optimization for their discriminative aligner.", "labels": [], "entities": []}, {"text": "The search for the best alignment is made with a modified hillclimbing method in, while] use a greedy algorithm.", "labels": [], "entities": []}, {"text": "One subject which has drawn comparatively little attention is the correct alignment of MWEs.", "labels": [], "entities": [{"text": "alignment of MWEs", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7318546175956726}]}, {"text": "MWEs area significant part of the lexicon of a speaker, perhaps as numerous as the single words, and various techniques to identify and process them have been proposed using different kinds of information, from syntax] to statistics.", "labels": [], "entities": []}, {"text": "In this work we focus on a specific type of MWEs, namely verb-particle constructs (VPCs), which are combinations of verb and particle such as turn up and made up.", "labels": [], "entities": []}, {"text": "In terms of syntax, VPCs can have complex subcategorization frames, such as transitive VPCs, which take a NP argument between the verb and the particle, e.g.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}