{"title": [{"text": "Unsupervised Bilingual POS Tagging with Markov Random Fields", "labels": [], "entities": [{"text": "Bilingual POS Tagging", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.675347218910853}]}], "abstractContent": [{"text": "In this paper, we give a treatment to the problem of bilingual part-of-speech induction with parallel data.", "labels": [], "entities": [{"text": "bilingual part-of-speech induction", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.6102291146914164}]}, {"text": "We demonstrate that na\u00a8\u0131vena\u00a8\u0131ve optimization of log-likelihood with joint MRFs suffers from a severe problem of local maxima , and suggest an alternative-using con-trastive estimation for estimation of the parameters.", "labels": [], "entities": []}, {"text": "Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset.", "labels": [], "entities": [{"text": "1984 dataset", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.7274982929229736}]}], "introductionContent": [{"text": "This paper considers unsupervised learning of linguistic structure-specifically, parts of speech-in parallel text data.", "labels": [], "entities": []}, {"text": "This setting, and more generally the multilingual learning scenario, has been found advantageous fora variety of unsupervised NLP tasks (.", "labels": [], "entities": []}, {"text": "We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions.", "labels": [], "entities": [{"text": "globally normalized Markov random fields (MRFs)", "start_pos": 12, "end_pos": 59, "type": "TASK", "confidence": 0.6210048794746399}]}, {"text": "This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure.", "labels": [], "entities": []}, {"text": "Such models, sometimes called \"undirected,\" are widespread in supervised NLP; the most notable instances are conditional random fields (), which have enabled rich feature engineering to incorporate knowledge and improve performance.", "labels": [], "entities": []}, {"text": "We conjecture that the \"features view\" of NLP problems is also more appropriate in unsupervised settings than the contrived, acyclic causal stories required by directed models.", "labels": [], "entities": []}, {"text": "Indeed, as we will discuss below, previous work on multilingual POS induction has had to resort to objectionable independence assumptions to avoid introducing cyclic dependencies in the causal network.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.8465543985366821}]}, {"text": "While undirected models are formally attractive, they are computationally demanding, particularly when they are used generatively, i.e., as joint distributions over input and output spaces.", "labels": [], "entities": []}, {"text": "Inference and learning algorithms for these models are usually intractable on realistic datasets, so we must resort to approximations.", "labels": [], "entities": []}, {"text": "Our emphasis here is primarily on the machinery required to support overlapping features, not on weakening independence assumptions, although we weaken them slightly.", "labels": [], "entities": []}, {"text": "Specifically, our parameterization permits us to model the relationship between aligned words in any configuration, rather than just those that conform to an acyclic generative process, as previous work in this area has done ( \u00a72).", "labels": [], "entities": []}, {"text": "We incorporate word prefix and suffix features (up to four characters) in an undirected version of a model designed by.", "labels": [], "entities": [{"text": "word prefix and suffix", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7665265798568726}]}, {"text": "Our experiments suggest that feature-based MRFs offer advantages over the previous approach.", "labels": [], "entities": [{"text": "MRFs", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.8645080924034119}]}], "datasetContent": [{"text": "We next describe experiments using our undirected model to unsupervisedly learn POS tags.", "labels": [], "entities": []}, {"text": "With unsupervised part-of-speech tagging, it is common practice to use a full or partial dictionary that maps words to possible part-of-speech tags.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.6964071094989777}]}, {"text": "The goal of the learner is then to discern which tag a word should take among the tags available for that word.", "labels": [], "entities": []}, {"text": "Indeed, in all of our experiments we make use of a tag dictionary.", "labels": [], "entities": []}, {"text": "We consider both a complete tag dictionary, where all of the POS tags for all words in the data are known, 2 and a smaller tag dictionary that only provides possible tags for the 100 most frequent words in each language, leaving the other words completely ambiguous.", "labels": [], "entities": []}, {"text": "The former dictionary makes the problem easier by reducing ambiguity; it also speeds up inference.", "labels": [], "entities": []}, {"text": "Our experiments focus on the Orwell novel 1984 dataset for our experiments, the same data used by.", "labels": [], "entities": [{"text": "Orwell novel 1984 dataset", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.9501097053289413}]}, {"text": "It consists of parallel text of the 1984 novel in English, Bulgarian, Slovene and Serbian), totalling 5,969 sentences in each language.", "labels": [], "entities": []}, {"text": "The 1984 datset uses fourteen partof-speech tags, two of which denote punctuation.", "labels": [], "entities": []}, {"text": "The tag sets for English and other languages have minor differences in determiners and particles.", "labels": [], "entities": []}, {"text": "We use the last 25% of sentences in the dataset as a test set, following previous work.", "labels": [], "entities": []}, {"text": "The dataset is manually annotated with part-of-speech tags.", "labels": [], "entities": []}, {"text": "We use automatically induced word alignments using Giza++.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7279437184333801}]}, {"text": "The data show very regular patterns of tags that are aligned together: words with the same tag in two languages tend to be aligned with each other.", "labels": [], "entities": []}, {"text": "When a complete tag dictionary derived from the Slavic language data is available, the level of ambiguity is very low.", "labels": [], "entities": [{"text": "ambiguity", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9693334698677063}]}, {"text": "The baseline of choosing random tags for each word gives an accuracy in the low 80s.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996907711029053}]}, {"text": "For English, we use an extended tag dictionary built from the Wall Street Journal and the 1984 data.", "labels": [], "entities": [{"text": "Wall Street Journal and the 1984 data", "start_pos": 62, "end_pos": 99, "type": "DATASET", "confidence": 0.7927365984235492}]}, {"text": "The English tag dictionary is much more ambiguous because it is obtained from a much larger dataset.", "labels": [], "entities": []}, {"text": "The random baseline gives an accuracy of around 56%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9997424483299255}]}, {"text": "(See In our first set of experiments ( \u00a75.1), we perform a \"sanity check\" with a monolingual version of the MRF that we described in earlier sections.", "labels": [], "entities": []}, {"text": "We compare it against plain HMM to assure that the MRFs behave well in the unsupervised setting.", "labels": [], "entities": []}, {"text": "In our second set of experiments ( \u00a75.2), we compare the bilingual HMM model from to the joint MRF model.", "labels": [], "entities": []}, {"text": "We show that using an MRF has an advantage over an HMM model in the partial tag dictionary setting.", "labels": [], "entities": []}, {"text": "We turn now to two monolingual experiments that verify our model's suitability for the tagging problem.", "labels": [], "entities": [{"text": "tagging problem", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.9448665976524353}]}, {"text": "Unsupervised Learning We trained our model under the monolingual setting as a sanity check for our approximate training algorithm.", "labels": [], "entities": []}, {"text": "Our model under monolingual mode is exactly the same as the models introduced in \u00a72.", "labels": [], "entities": []}, {"text": "We ran our model on the 1984 data with the complete tag dictionary.", "labels": [], "entities": [{"text": "1984 data", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.8040655553340912}]}, {"text": "A comparison between our result and monolingual directed model is shown in.", "labels": [], "entities": []}, {"text": "\"Random\" is obtained by choosing a random tag for each word according to the tag dictionary.", "labels": [], "entities": [{"text": "Random", "start_pos": 1, "end_pos": 7, "type": "METRIC", "confidence": 0.945094108581543}]}, {"text": "\"HMM\" is a Bayesian HMM implemented by).", "labels": [], "entities": []}, {"text": "We also implemented a basic (non-Bayesian) HMM.", "labels": [], "entities": []}, {"text": "We trained the HMM with EM and obtained rsults similar to the Bayesian HMM (not shown).", "labels": [], "entities": [{"text": "EM", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9551051259040833}]}, {"text": "The reason that crossing links do not change the results much could be related to fact that most of the sentence pairs in the 1984 dataset do not contain many crossing links (only 5% of links cross another link).", "labels": [], "entities": [{"text": "1984 dataset", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.7822506427764893}]}, {"text": "To see whether crossing links do have an effect when they come in larger number, we tested our model on French-English data.", "labels": [], "entities": [{"text": "French-English data", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.7786752581596375}]}, {"text": "We aligned 10,000 sentences from the Europarl corpus (, resulting in 87K crossing links out of a total of 673K links.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9936368763446808}]}, {"text": "Using the Penn treebank ( and the French treebank () to evaluate the model, results are given in.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9915827512741089}, {"text": "French treebank", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9562346339225769}]}, {"text": "It is evident that crossing links have a larger effect here, but it is mixed: crossing links improve performance for French while harming it for English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Unsupervised monolingual tagging accura- cies with complete tag dictionary on 1984 data.", "labels": [], "entities": [{"text": "1984 data", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.7510263919830322}]}, {"text": " Table 2: Unsupervised bilingual tagging accuracies with tag dictionary only for the top 100 frequent words.  \"HMM\" is the result reported by (Snyder et al., 2008). \"MRF\" is our contrastive model averaged over ten  runs. \"MRF w/o cross.\" is our model trained without crossing links, like Snyder et al.'s HMM. \"MRF  w/o spell.\" is our model without prefix and suffix features. Numbers appearing next to results are standard  deviations over the ten runs.", "labels": [], "entities": []}, {"text": " Table 3: Effect of removing crossing links when  learning French and English in a bilingual setting.", "labels": [], "entities": []}]}