{"title": [{"text": "Why is \"SXSW\" trending? Exploring Multiple Text Sources for Twitter Topic Summarization", "labels": [], "entities": [{"text": "SXSW\" trending", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.6944311261177063}, {"text": "Topic Summarization", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7753899097442627}]}], "abstractContent": [{"text": "User-contributed content is creating a surgeon the Internet.", "labels": [], "entities": []}, {"text": "A list of \"buzzing topics\" can effectively monitor the surge and lead people to their topics of interest.", "labels": [], "entities": []}, {"text": "Yet a topic phrase alone, such as \"SXSW\", can rarely present the information clearly.", "labels": [], "entities": []}, {"text": "In this paper, we propose to explore a variety of text sources for summarizing the Twitter topics, including the tweets, normalized tweets via a dedicated tweet normalization system, web contents linked from the tweets, as well as integration of different text sources.", "labels": [], "entities": []}, {"text": "We employ the concept-based optimization framework for topic summarization, and conduct both automatic and human evaluation regarding the summary quality.", "labels": [], "entities": [{"text": "topic summarization", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7407759130001068}]}, {"text": "Performance differences are observed for different input sources and types of topics.", "labels": [], "entities": []}, {"text": "We also provide a comprehensive analysis regarding the task challenges.", "labels": [], "entities": []}], "introductionContent": [{"text": "User contributed content has become a major source of information in the Web 2.0 era.", "labels": [], "entities": []}, {"text": "People follow their topics of interest, share their experience or opinions on a variety of interactive platforms, including forums, blogs, microblogs, social networking sites, etc.", "labels": [], "entities": []}, {"text": "To keep track of the trends online and suggest topics of interest to the general public, many leading websites provide a \"buzzing\" service by publishing the current most popular topics on their entrance page and update them regularly, such as the \"popular now\" column on Bing.com, \"trending topics\" on Twitter.com, \"trending now\" on Yahoo.com, Google Trends, and so forth.", "labels": [], "entities": []}, {"text": "Often popular topics are in the form of a list of keywords or phrases . Take Twitter.com as an example.", "labels": [], "entities": []}, {"text": "Clicking on a trending topic phrase will return a set of relevant Twitter posts (tweets) or web pages.", "labels": [], "entities": []}, {"text": "Nonetheless, whether this is a convenient way for users to navigate through the popular topic information is still arguable.", "labels": [], "entities": []}, {"text": "For example, when \"SXSW\" was listed as a trending topic, it seems difficult to understand at the first glance.", "labels": [], "entities": []}, {"text": "A condensed topic summary would be extremely helpful for the users before diving into the massive search results to figure out what this topic phrase is about and why it is trending.", "labels": [], "entities": []}, {"text": "In this paper, our goal is to generate a short text summary for any given topic phrase.", "labels": [], "entities": []}, {"text": "Note that the proposed approach is not limited to trending topics, but can be applied to arbitrary Twitter topics.", "labels": [], "entities": []}, {"text": "There area lot of differences between tweets and traditional written text that has been widely used for automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.6888891458511353}]}, {"text": "In, we show example tweets for the topic \"SXSW\".", "labels": [], "entities": []}, {"text": "The tweets were extracted by searching the Twitter site using the topic phrase as a query.", "labels": [], "entities": []}, {"text": "We also provide an excerpt of the linked web content to help understand the topic.", "labels": [], "entities": []}, {"text": "The tweets present some unique characteristics: \u2022 All tweets are limited to 140 characters.", "labels": [], "entities": []}, {"text": "Some tweets are news headlines from the official media, others are generated by users with various degrees of familiarity with the social media.", "labels": [], "entities": []}, {"text": "The resulting tweets can be very different regarding the text quality and word usage.: Example tweets and an excerpt of the linked web content for Twitter topic \"SXSW\".", "labels": [], "entities": []}, {"text": "\u2022 Tweets lack structure information, contain various ill-formed sentences and grammatical errors.", "labels": [], "entities": []}, {"text": "There are lots of noisy nonstandard tokens, such as abbreviations (\"feelin\" for \"feeling\"), substitutions (\"Pr1mr0se\" for \"Primrose\"), emoticons, etc.", "labels": [], "entities": []}, {"text": "\u2022 Twitter invented its own markup language.", "labels": [], "entities": []}, {"text": "\"@user\" is used to reply to a specific user or call for attentions.", "labels": [], "entities": []}, {"text": "The hashtag \"#topic\" aims to assign a topic label to the tweet, and is frequently employed by the twitter users.", "labels": [], "entities": []}, {"text": "\u2022 Tweets frequently contain embedded URLs that direct users to other online content, such as news web pages, blogs, organization homepages ().", "labels": [], "entities": []}, {"text": "According to Twitter's news release in September 2010, 25% of tweets contain an URL.", "labels": [], "entities": []}, {"text": "These linked web pages provide a much richer source of information than is possible in the 140-character tweet.", "labels": [], "entities": []}, {"text": "These Twitter-specific characteristics may pose challenges to the automatic summarization systems for identifying the essential information.", "labels": [], "entities": [{"text": "summarization", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9013403654098511}]}, {"text": "In this paper, we focus on two such characteristics that are not studied in previous literature, the web content link and the non-standard tokens in tweets.", "labels": [], "entities": []}, {"text": "Specifically, we ask two questions: (1) Is the web content linked from the tweets useful for summarization?", "labels": [], "entities": [{"text": "summarization", "start_pos": 93, "end_pos": 106, "type": "TASK", "confidence": 0.9929510354995728}]}, {"text": "Can we integrate different text sources, including the tweets and linked web pages, to generate more informative Twitter topic summaries?", "labels": [], "entities": []}, {"text": "what is the effect of nonstandard tokens on summarization performance?", "labels": [], "entities": [{"text": "summarization", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.988457977771759}]}, {"text": "Will the summaries be improved if the noisy tweets were pre-normalized into standard English sentences?", "labels": [], "entities": [{"text": "summaries", "start_pos": 9, "end_pos": 18, "type": "TASK", "confidence": 0.950372040271759}]}, {"text": "We investigate these two questions under a concept-based summarization framework using integer linear programming (ILP).", "labels": [], "entities": []}, {"text": "We utilize text input that has various quality and is originated from multiple sources, and thoroughly analyze the resulting summaries using both automatic and human evaluation metrics.", "labels": [], "entities": []}], "datasetContent": [{"text": "Among the collected topics, we select 500 general topics (such as \"Chilean miners\") and 50 hashtag topics (such as \"#octoberwish\", \"#wheniwasakid\") for experimentation.", "labels": [], "entities": []}, {"text": "On average, a general topic contains 1673 tweets and 3.43 extracted linked web pages; while a hashtag topic contains 3316 tweets but does not have meaningful linked web pages.", "labels": [], "entities": []}, {"text": "The concept-based optimization system was configured to extract a collection of sentences/tweets for each topic, using either the sentence-or wordconstraint (denoted as \"#Sent\" and \"#Word\").", "labels": [], "entities": []}, {"text": "We opt to set individual length constraint for each topic rather than using a uniform length limit for all the topics, since the topics can be very different in length and duration.", "labels": [], "entities": []}, {"text": "We use the number of sentences/words in the reference summary as the sentence/word constraint for each topic.", "labels": [], "entities": []}, {"text": "Note that in practice this reference summary length information may not be available.", "labels": [], "entities": [{"text": "length", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.8070924878120422}]}, {"text": "We use the length constraints obtained from the reference summary in this exploratory study, since our focus is to first evaluate if twitter trending summarization is feasible, and what are the effects of different information sources and non-standard tokens.", "labels": [], "entities": []}, {"text": "For a comparison to our approach, we implement the Hybrid TF-IDF approach in ( as a baseline using \"OrigTweets\" as input.", "labels": [], "entities": []}, {"text": "For the baseline, the summary length is altered according to the sentenceor word-constraint.", "labels": [], "entities": []}, {"text": "The last summary tweet is cut in the middle if it exceeds the word limit.", "labels": [], "entities": []}, {"text": "The ROUGE-1 F-scores) are used to measure the n-gram (n=1) overlap between the system summaries and reference summaries.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9903661012649536}, {"text": "F-scores", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.5075534582138062}]}, {"text": "Since the ROUGE scores may not correlate well with the human judgments (Liu and Liu, 2010b), we also performed human evaluation by asking annotators to score both the system and reference summaries regarding the linguistic quality and content responsiveness, in the hope this will benefit future research in this direction.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9798546433448792}]}, {"text": "We present the results (ROUGE-1 F-measure) for the general topics in  ROUGE-4 scores show similar trends and thus are not presented.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8200809359550476}, {"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.6008191704750061}]}, {"text": "Five different text sources were exploited as the system inputs, as described in Section 4.2.", "labels": [], "entities": []}, {"text": "To measure the quality of the input for summarization, we also include reference summary coverage score in the table, defined as the percentage of words in the reference summary that are covered by the input text source.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9881536364555359}, {"text": "reference summary coverage score", "start_pos": 71, "end_pos": 103, "type": "METRIC", "confidence": 0.5481524392962456}]}, {"text": "When using tweets as input, we also investigate whether we should apply tweet normalization before or after the summarization process, that is \"pre-normalization\" (using \"NormTweets\" as input), or \"post-normalization\" (using \"OrigTweets\" as input, and rendering the normalized summary tweets).", "labels": [], "entities": []}, {"text": "Compared to the Hybrid TF-IDF approach (Sharifi et al., 2010b; Inouye, 2010), our system performs significantly better (p < 0.05) according to the paired t-test; however, we also notice the ROUGE scores are lower compared to summarization in other text domains.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 190, "end_pos": 195, "type": "METRIC", "confidence": 0.9982742071151733}]}, {"text": "This indicates that Twitter topic summarization is very challenging.", "labels": [], "entities": [{"text": "Twitter topic summarization", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.5056682725747427}]}, {"text": "Comparing the two constraints used in the concept-based optimization framework, we found that the word constraint performs constantly better for the general topics.", "labels": [], "entities": []}, {"text": "This is natural since the word constraint tightly bounds the length of the system output, while the sentence constraint is relatively loose.", "labels": [], "entities": []}, {"text": "For the different sources, we notice using linked web pages alone yields worse summarization performance, as well as lower reference summary coverage; however, when combined with the tweets, there is a slight increase in the coverage scores, and sometimes improved summarization results.", "labels": [], "entities": [{"text": "summarization", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.9744057059288025}, {"text": "reference summary coverage", "start_pos": 123, "end_pos": 149, "type": "METRIC", "confidence": 0.46844761570294696}, {"text": "coverage", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9544817805290222}, {"text": "summarization", "start_pos": 265, "end_pos": 278, "type": "TASK", "confidence": 0.9680529832839966}]}, {"text": "This suggests that the linked web pages can contain extra useful information for generating summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 92, "end_pos": 101, "type": "TASK", "confidence": 0.6990834474563599}]}, {"text": "Regarding normalization, results show that the \"prenormalization\" (using normalized tweets as input) can generally improve the summary tweet selection.", "labels": [], "entities": []}, {"text": "For general topics, the best performance was achieved by combining the normalized tweets and linked web pages as input source and using the word-level constraint.", "labels": [], "entities": []}, {"text": "Results for hashtag topics were shown in using tweets as input (there are no linked webpages for these topics).", "labels": [], "entities": []}, {"text": "We notice the reference coverage scores are satisfying, yet the system output barely matches the reference summaries (very low ROUGE-1 scores).", "labels": [], "entities": [{"text": "coverage", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.7619380950927734}, {"text": "ROUGE-1", "start_pos": 127, "end_pos": 134, "type": "METRIC", "confidence": 0.9957518577575684}]}, {"text": "Looking at the reference and system generated summaries for the hashtag topics, we found the system output is more specific (e.g., \"#octoberwish everything goes well.\"), while the reference summaries are often very general (e.g., \"people tweeting about their wishes for October.\").", "labels": [], "entities": []}, {"text": "The human annotators also noted that most hashtag topics (such as \"#octoberwish\", \"#wheniwasakid\") are self-explainable and may require special attention to redefine an appropriate summary.", "labels": [], "entities": []}, {"text": "Using sentence constraints yields better performance than word-based one, with larger performance difference than that for the general topics.", "labels": [], "entities": []}, {"text": "We found the word-constraint summaries tend to include tweets that are very short and noisy.", "labels": [], "entities": []}, {"text": "Our system with sentence-based length constraint also significantly outperforms the Hybrid TF-IDF approach (.", "labels": [], "entities": []}, {"text": "For hashtag topics, the best performance was achieved using the \"prenormalization\" with sentence constraint.", "labels": [], "entities": []}, {"text": "For general topics, the \"Web\" summaries outperform the \"Tweet\" summaries on both grammaticality and non-redundancy, confirming the advantage of using the high-quality linked web pages.", "labels": [], "entities": []}, {"text": "The referential clarity and focus scores of the \"Web\" summaries are not very high, since the summary sentences were extracted simultaneously from several web pages, and the system subjects to similar challenges as in multi-document summarization.", "labels": [], "entities": [{"text": "clarity", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9392487406730652}]}, {"text": "The content coverage scores of both system summaries seem to correlate well with the ROUGE-1 F-measure, with a higher score for \"Tweet\" summaries.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9887252449989319}, {"text": "F-measure", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.6104409098625183}]}, {"text": "The assessors also rated that 48% of the \"Web\" summaries contain \"Somewhat Useful\" extra topic information, and 21% are \"Very Useful\".", "labels": [], "entities": []}, {"text": "Note that this could be just because of the inherent difference of the two summaries, regardless of the input source, but in general we believe the linked web pages (such as the news documents) can provide more detailed and coherent stories as compared to the 140-character tweets.", "labels": [], "entities": []}, {"text": "For hashtag topics, the \"Tweet\" summaries yield worse grammaticality and focus scores, but have very high non-redundancy score.", "labels": [], "entities": []}, {"text": "On the contrary, the reference summaries often contain redundant information.", "labels": [], "entities": []}, {"text": "The content match score between the system and reference summaries (2.6) does not seem to reflect the ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.992071807384491}]}, {"text": "We hypothesize that even though the specificity of the two summaries is different, the assessors may still think the system summaries match the reference ones to some extent.", "labels": [], "entities": []}, {"text": "A larger scale human evaluation is needed to study the correlation between human and automatic evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: ROUGE-1 F-measure and reference summary  coverage scores for general topics.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9882185459136963}, {"text": "F-measure", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.7928500175476074}, {"text": "reference summary  coverage scores", "start_pos": 32, "end_pos": 66, "type": "METRIC", "confidence": 0.6976350769400597}]}, {"text": " Table 4: ROUGE-1 F-measure and reference summary  coverage scores for hashtag topics.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.989398717880249}, {"text": "F-measure", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.7466737031936646}, {"text": "reference summary  coverage scores", "start_pos": 32, "end_pos": 66, "type": "METRIC", "confidence": 0.8235100507736206}]}, {"text": " Table 5: Linguistic quality, content coverage, and useful- ness scores judged by human assessors.", "labels": [], "entities": []}]}