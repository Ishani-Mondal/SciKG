{"title": [{"text": "Generating Semantic Orientation Lexicon using Large Data and Thesaurus", "labels": [], "entities": [{"text": "Generating Semantic Orientation Lexicon", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7468607351183891}]}], "abstractContent": [{"text": "We propose a novel method to construct semantic orientation lexicons using large data and a thesaurus.", "labels": [], "entities": [{"text": "semantic orientation lexicons", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.7733409901460012}]}, {"text": "To deal with large data, we use Count-Min sketch to store the approximate counts of all word pairs in a bounded space of 8GB.", "labels": [], "entities": []}, {"text": "We use a thesaurus (like Roget) to constrain near-synonymous words to have the same polarity.", "labels": [], "entities": []}, {"text": "This framework can easily scale to any language with a thesaurus and a unzipped corpus size \u2265 50 GB (12 billion tokens).", "labels": [], "entities": []}, {"text": "We evaluate these lexicons intrinsically and extrinsically, and they perform comparable when compared to other existing lexicons.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, the field of natural language processing (NLP) has seen tremendous growth and interest in the computational analysis of emotions, sentiments, and opinions.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.8323760628700256}, {"text": "computational analysis of emotions, sentiments, and opinions", "start_pos": 111, "end_pos": 171, "type": "TASK", "confidence": 0.8385982447200351}]}, {"text": "This work has focused on many application areas, such as sentiment analysis of consumer reviews e.g.,, product reputation analysis e.g.,, tracking sentiments toward events e.g.,, and automatically producing plot unit representations e.g.,).", "labels": [], "entities": [{"text": "sentiment analysis of consumer reviews", "start_pos": 57, "end_pos": 95, "type": "TASK", "confidence": 0.8878917336463928}, {"text": "product reputation analysis", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.7132701476414999}]}, {"text": "An important resource in accomplishing the above tasks is a list of words with semantic orientation (SO): positive or negative.", "labels": [], "entities": []}, {"text": "The goal of this work is to automatically create such a list of words using large data and a thesaurus structure.", "labels": [], "entities": []}, {"text": "For this purpose, we store exact counts of all the words in a hash table and use Count-Min (CM) sketch to store the approximate counts of all word pairs fora large corpus in a bounded space of 8GB.", "labels": [], "entities": [{"text": "Count-Min (CM) sketch", "start_pos": 81, "end_pos": 102, "type": "METRIC", "confidence": 0.8693450450897217}]}, {"text": "(Storing the counts of all word pairs is computationally expensive and memory intensive on large data ().", "labels": [], "entities": []}, {"text": "Storage space saving in CM sketch is achieved by approximating the frequency of word pairs in the corpus without explicitly storing the word pairs themselves.", "labels": [], "entities": []}, {"text": "Both updating (adding anew word pair or increasing the frequency of existing word pair) and querying (finding the frequency of a given word pair) are constant time operations making it an efficient online storage data structure for large data.", "labels": [], "entities": []}, {"text": "Once we have these counts, we find semantic orientation (SO)) of a word using its association strength with positive (e.g. good, and nice) and negative (e.g., bad and nasty) seeds.", "labels": [], "entities": [{"text": "semantic orientation (SO))", "start_pos": 35, "end_pos": 61, "type": "METRIC", "confidence": 0.5656185269355773}]}, {"text": "Next, we make use of a thesaurus (like Roget) structure in which near-synonymous words appear in a single group.", "labels": [], "entities": []}, {"text": "We compute the SO of the whole group by computing SO of each individual word in the group and assign that SO to all the words in the group.", "labels": [], "entities": [{"text": "SO", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.8635446429252625}, {"text": "SO", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9938821792602539}]}, {"text": "The hypothesis is that near synonym words should have similar polarity.", "labels": [], "entities": []}, {"text": "However, similar words in a group can still have different connotations.", "labels": [], "entities": []}, {"text": "For example, one group has \"slender\", \"slim\", \"wiry\" and \"lanky\".", "labels": [], "entities": []}, {"text": "One can argue that, first two words have positive connotation and last two have negative.", "labels": [], "entities": []}, {"text": "To remove these ambiguous words errors from the lexicon, we discard those words which have conflicting SO compared to their group SO.", "labels": [], "entities": []}, {"text": "The idea behind using thesaurus structure is motivated from the idea of using number of positive and negative seed words () in thesaurus group to determine the polarity of words in the group.", "labels": [], "entities": []}, {"text": "In our experiments, we show the effectiveness of the lexicons created using large data and freely avail-able thesaurus both intrinsically and extrinsically.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the lexicons proposed in Section 3 both intrinsically (by comparing their lexicon entries against General Inquirer (GI) lexicon) and extrinsically (by using them in a phrase polarity annotation task).", "labels": [], "entities": []}, {"text": "We remove stop words and phrases for comparison from existing lexicons as our framework does not assign polarity to them.", "labels": [], "entities": []}, {"text": "We compare the lexicon entries of \"SO\", \"SO-TP\" , and \"SO-WTP\" against entries of GI Lexicon.", "labels": [], "entities": [{"text": "GI Lexicon", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.9325922131538391}]}, {"text": "This evaluation is similarly used by other authors) to evaluate sentiment lexicons.", "labels": [], "entities": []}, {"text": "shows the percentage of GI positive (Pos), negative (Neg) and all (All) lexicon entries that Pos All SO  match the proposed lexicons.", "labels": [], "entities": []}, {"text": "The recall of our precision orientated lexicon SO-WTP is only 5 and 4 % less compared to SO and SO-TP respectively which are more recall oriented.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.999402642250061}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.97636479139328}]}, {"text": "We evaluate these lexicons against Roget-ASL (discussed in Section 5.2.1).", "labels": [], "entities": []}, {"text": "Even, Our SO-WTP precision oriented lexicon has more recall than Roget-ASL.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9992367029190063}]}, {"text": "In this section, we compare the effectiveness of our lexicons on a task of phrase polarity identification.", "labels": [], "entities": [{"text": "phrase polarity identification", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.8516501983006796}]}, {"text": "We use the MPQA corpus which contains news articles from a wide variety of news sources manually annotated for opinions and other private states (like beliefs, emotions, sentiments, speculations, etc.).", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9658616781234741}]}, {"text": "Moreover, it has polarity annotations (positive/negative) at the phrase level.", "labels": [], "entities": []}, {"text": "We use MPQA 5 version 2.0 collection of 2789 positive and 6079 negative phrases.", "labels": [], "entities": [{"text": "MPQA 5 version 2.0 collection", "start_pos": 7, "end_pos": 36, "type": "DATASET", "confidence": 0.9374944686889648}]}, {"text": "We perform an extrinsic evaluation of our automatic generated lexicons (using large data and thesaurus) against existing automated and manually generated lexicons by using them to automatically determine the phrase polarity.", "labels": [], "entities": []}, {"text": "This experimental setup is similar to.", "labels": [], "entities": []}, {"text": "However, in their work, they used MPQA version 1.0.", "labels": [], "entities": []}, {"text": "We use a similar algorithm as used by to determine the polarity of the phrase.", "labels": [], "entities": []}, {"text": "If any of the words in the target phrase is labeled in the lexicon as having negative SO, then the phrase is marked as negative.", "labels": [], "entities": []}, {"text": "If there are no negative words in the target phrase and it contains one or more positive words, then the phrase is marked as positive.", "labels": [], "entities": []}, {"text": "In all other cases, do not assign any tag.", "labels": [], "entities": []}, {"text": "The only difference with respect to is that we use a list of 58 negation words used in OpinionFinder 6 () (Version 1.4) to flip the polarity of a phrase if it contains odd number of negation words.", "labels": [], "entities": []}, {"text": "We can get better accuracies on phrase polarity identification using supervised classifiers.", "labels": [], "entities": [{"text": "phrase polarity identification", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.8472970525423685}]}, {"text": "However, the goal of this work is only to show the effectiveness of large data and thesaurus learned lexicons.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The percentage of GI entries (positive, negative, and", "labels": [], "entities": []}, {"text": " Table 5: Results on marking polarity of phrases using various", "labels": [], "entities": [{"text": "marking polarity of phrases", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.9068962186574936}]}]}