{"title": [], "abstractContent": [{"text": "Most work on evaluation of named-entity recognition has been done in the context of competitions, as apart of Information Extraction.", "labels": [], "entities": [{"text": "evaluation of named-entity recognition", "start_pos": 13, "end_pos": 51, "type": "TASK", "confidence": 0.6739443019032478}, {"text": "Information Extraction", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.717594176530838}]}, {"text": "There has been little work on any form of extrinsic evaluation, and how one tagger compares with another on the major classes: PERSON , ORGANIZATION, and LOCATION.", "labels": [], "entities": [{"text": "extrinsic evaluation", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7758332788944244}, {"text": "PERSON", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9571614861488342}, {"text": "ORGANIZATION", "start_pos": 136, "end_pos": 148, "type": "METRIC", "confidence": 0.9794532656669617}, {"text": "LOCATION", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.880849301815033}]}, {"text": "We report on a comparison of three state-of-the-art named entity taggers: Stanford, LBJ, and IdentiFinder.", "labels": [], "entities": []}, {"text": "The taggers were compared with respect to: 1) Agreement rate on the classification of entities by class, and 2) Percentage of ambiguous entities (belonging to more than one class) co-occurring in a document.", "labels": [], "entities": [{"text": "Agreement rate", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.9789575040340424}, {"text": "Percentage", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9803608059883118}]}, {"text": "We found that the agreement between the tag-gers ranged from 34% to 58%, depending on the class and that more than 40% of the globally ambiguous entities co-occur within the same document.", "labels": [], "entities": []}, {"text": "We also propose a unit test based on the problems we encountered.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named-Entity Recognition (NER) has been an important task in Computational Linguistics for more than 15 years.", "labels": [], "entities": [{"text": "Named-Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8265909373760223}]}, {"text": "The aim is to recognize and classify different types of entities in text.", "labels": [], "entities": []}, {"text": "These might be people's names, or organizations, or locations, as well as dates, times, and currencies.", "labels": [], "entities": []}, {"text": "Performance assessment is usually made in the context of Information Extraction, of which NER is generally a component.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7514703571796417}, {"text": "NER", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.6623663902282715}]}, {"text": "Competitions have been held from the earliest days of MUC (Message Understanding Conference), to the more recent shared tasks in CoNLL.", "labels": [], "entities": [{"text": "MUC (Message Understanding Conference)", "start_pos": 54, "end_pos": 92, "type": "TASK", "confidence": 0.61867027481397}, {"text": "CoNLL", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.8997896909713745}]}, {"text": "Recent research has focused on non-English languages such as Spanish,, and on improving the performance of unsupervised learning methods (.", "labels": [], "entities": []}, {"text": "There are no well-established standards for evaluation of NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9639195203781128}]}, {"text": "Since criteria for membership in the classes can change from one competition to another, it is often not possible to compare performance directly.", "labels": [], "entities": []}, {"text": "Moreover, since some of the systems in the competition may use proprietary software, the results in a competition might not be replicable by others in the community; however, this applies to the state of the art for most NLP applications rather than just NER.", "labels": [], "entities": []}, {"text": "Our work is motivated by a vocabulary assessment project in which we needed to identify multi-word expressions and determine their association with other words and phrases.", "labels": [], "entities": []}, {"text": "However, we found that state-of-the-art software for namedentity recognition was not reliable; false positives and tagging inconsistencies significantly hindered our work.", "labels": [], "entities": [{"text": "namedentity recognition", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7681158781051636}]}, {"text": "These results led us to examine the stateof-the-art in more detail.", "labels": [], "entities": []}, {"text": "The field of Information Extraction (IE) has been heavily influenced by the Information Retrieval (IR) community when it comes to evaluation of system performance.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.8720399975776673}, {"text": "Information Retrieval (IR)", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.7517612099647522}]}, {"text": "The use of Recall and Precision metrics for evaluating IE comes from the IR community.", "labels": [], "entities": [{"text": "Precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9305686354637146}, {"text": "IE", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9743040800094604}, {"text": "IR", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9346171021461487}]}, {"text": "However, while the IR community regularly conducts a set of competitions and shared tasks using standardized test collections, the IE community does not.", "labels": [], "entities": []}, {"text": "Furthermore, NER is just one component of an IE pipeline and any proposed improvements to this component must be evaluated by determining whether the performance of the overall IE pipeline has improved.", "labels": [], "entities": [{"text": "NER", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6192106008529663}]}, {"text": "However, most, if not all, NER evaluations and shared tasks only focus on intrinsic NER performance and ignore any form of extrinsic evaluation.", "labels": [], "entities": [{"text": "NER evaluations", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.9224396347999573}]}, {"text": "One of the contributions of this paper is a freely available unit test based on the systematic problems we found with existing taggers.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared three state-of-the-art NER taggers: one from Stanford University (henceforth, Stanford tagger), one from the University of Illinois (henceforth, the LBJ tagger) and BBN IdentiFinder (henceforth, IdentiFinder).", "labels": [], "entities": [{"text": "BBN IdentiFinder", "start_pos": 177, "end_pos": 193, "type": "DATASET", "confidence": 0.8509211838245392}]}, {"text": "The Stanford Tagger is based on Conditional Random Fields ().", "labels": [], "entities": [{"text": "Stanford Tagger", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9456913769245148}]}, {"text": "It was trained on 100 million words from the English Gigawords corpus.", "labels": [], "entities": [{"text": "English Gigawords corpus", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.8451179265975952}]}, {"text": "The LBJ Tagger is based on a regularized average perceptron.", "labels": [], "entities": [{"text": "LBJ Tagger", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8927834033966064}]}, {"text": "It was trained on a subset of the Reuters 1996 news corpus, a subset of the North American News Corpus, and a set of 20 web pages.", "labels": [], "entities": [{"text": "Reuters 1996 news corpus", "start_pos": 34, "end_pos": 58, "type": "DATASET", "confidence": 0.9332400858402252}, {"text": "North American News Corpus", "start_pos": 76, "end_pos": 102, "type": "DATASET", "confidence": 0.8682861626148224}]}, {"text": "The features for both these taggers are based on local context fora target word, orthographic features, label sequences, and distributional similarity.", "labels": [], "entities": []}, {"text": "Both taggers include nonlocal features to ensure consistency in the tagging of identical tokens that are in close proximity.", "labels": [], "entities": [{"text": "consistency", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9852713346481323}]}, {"text": "IdentiFinder is a state-of-the-art commercial NER tagger that uses Hidden Markov Models (HMMs) ().", "labels": [], "entities": [{"text": "IdentiFinder", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8968273401260376}, {"text": "NER tagger", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.92843097448349}]}, {"text": "Since we did not have gold standard annotations for any of the real-world data we evaluated on, we instead compared the three taggers along two dimensions: \u2022 Agreement on classification.", "labels": [], "entities": []}, {"text": "How well do the taggers work on the three most difficult classes: PERSON, ORGANIZATION, and LOCATION and, more importantly, to what extent does one tagger agree with another?", "labels": [], "entities": [{"text": "PERSON", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9884627461433411}, {"text": "ORGANIZATION", "start_pos": 74, "end_pos": 86, "type": "METRIC", "confidence": 0.9839445948600769}, {"text": "LOCATION", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9820115566253662}]}, {"text": "What types of mistakes do they make systematically?", "labels": [], "entities": []}, {"text": "1 \u2022 Ambiguity in discourse.", "labels": [], "entities": []}, {"text": "Although entities can potentially have more than one entity classification, such as Clinton (PERSON or LO-CATION), it would be surprising if they cooccurred in a single discourse unit such as a document.", "labels": [], "entities": []}, {"text": "How frequently does each tagger produce multiple classifications for the same entity in a single document?", "labels": [], "entities": []}, {"text": "We first compared the two freely available, academic taggers (Stanford and LBJ) on a corpus of 425 million words that is used internally at the Educational Testing Service.", "labels": [], "entities": [{"text": "Educational Testing Service", "start_pos": 144, "end_pos": 171, "type": "DATASET", "confidence": 0.846031387646993}]}, {"text": "Note that we could not compare these two taggers to IdentiFinder on this corpus since IdentiFinder is not available for public use without a license.", "labels": [], "entities": []}, {"text": "Next, we compared all three taggers on the American National Corpus.", "labels": [], "entities": [{"text": "American National Corpus", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.8646705150604248}]}, {"text": "The American National Corpus (ANC) has recently released a copy which is tagged by IdentiFinder.", "labels": [], "entities": [{"text": "American National Corpus (ANC)", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.9637514253457388}, {"text": "IdentiFinder", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8169468641281128}]}, {"text": "Since the ANC is a publicly available corpus, we tagged it using both the Stanford and LBJ taggers and could then compare all three taggers along the two intended dimensions.", "labels": [], "entities": [{"text": "Stanford and LBJ taggers", "start_pos": 74, "end_pos": 98, "type": "DATASET", "confidence": 0.750290647149086}]}, {"text": "We found that the public corpus had many of the same problems as the ones we found with our internally used corpus.", "labels": [], "entities": []}, {"text": "Some of these problems have been discussed before () but not in sufficient detail.", "labels": [], "entities": []}, {"text": "The following section describes our evaluation of the Stanford and LBJ taggers on the internal ETS corpus.", "labels": [], "entities": [{"text": "Stanford and LBJ taggers", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.6637273579835892}, {"text": "ETS corpus", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.7855600118637085}]}, {"text": "Section 4 describes a comparison of all three taggers on the American National Corpus.", "labels": [], "entities": [{"text": "American National Corpus", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.9539279739061991}]}, {"text": "Section 5 describes the unit test we propose.", "labels": [], "entities": []}, {"text": "In Section 6, we propose and discuss the viability of the \"one namedentity tag per discourse\" hypothesis.", "labels": [], "entities": []}, {"text": "In Section 7, we highlight the problems we find during our comparisons and propose a methodology for improved intrinsic evaluation for NER.  articles.", "labels": [], "entities": [{"text": "NER.  articles", "start_pos": 135, "end_pos": 149, "type": "DATASET", "confidence": 0.7861336668332418}]}, {"text": "The articles were extracted from a set of 60 different journals, newspapers and magazines focused on both literary and scientific topics.", "labels": [], "entities": []}, {"text": "Although Named Entity Recognition is reported in the literature to have an accuracy rate of 85-95% (), it was clear by inspection that both the Stanford and the LBJ tagger made a number of mistakes.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.6379863421122233}, {"text": "accuracy rate", "start_pos": 75, "end_pos": 88, "type": "METRIC", "confidence": 0.9937704205513}, {"text": "LBJ tagger", "start_pos": 161, "end_pos": 171, "type": "DATASET", "confidence": 0.9207512140274048}]}, {"text": "The ETS corpus begins with an article about Tim BernersLee, the man who created the World Wide Web.", "labels": [], "entities": [{"text": "ETS corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7538436949253082}]}, {"text": "At the beginning of the article, \"Tim\" as well as \"Berners-Lee\" are correctly tagged by the Stanford tagger as belonging to the PERSON class.", "labels": [], "entities": [{"text": "Berners-Lee", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8413777947425842}]}, {"text": "But later in the same article, \"Berners-Lee\" is incorrectly tagged as ORGANIZATION.", "labels": [], "entities": [{"text": "Berners-Lee\"", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8714333176612854}, {"text": "ORGANIZATION", "start_pos": 70, "end_pos": 82, "type": "METRIC", "confidence": 0.9842020273208618}]}, {"text": "The LBJ tagger makes many mistakes as well, but they are not necessarily the same mistakes as the mistakes made by the Stanford tagger.", "labels": [], "entities": [{"text": "LBJ tagger", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.7025777399539948}]}, {"text": "For example, the LBJ tagger sometimes classifies \"The Web\" as a PERSON, and the Stanford tagger classifies \"Italian\" as a LOCATION.", "labels": [], "entities": [{"text": "LBJ tagger", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.8926627039909363}, {"text": "Stanford tagger classifies \"Italian\"", "start_pos": 80, "end_pos": 116, "type": "DATASET", "confidence": 0.8459700445334116}]}, {"text": "3 provides an anecdotal list of the \"entities\" that were misclassified by the two taggers.", "labels": [], "entities": []}, {"text": "Both taggers produced about the same number of entities overall: 1.95 million for Stanford, and \"Italian\" is classified primarily as MISC by the LBJ tagger.", "labels": [], "entities": [{"text": "LBJ tagger", "start_pos": 145, "end_pos": 155, "type": "DATASET", "confidence": 0.9622498452663422}]}, {"text": "These terms are sometimes called Gentilics or Demonyms.", "labels": [], "entities": []}, {"text": "Both taggers can use a fourth class MISC in addition to the standard entity classes PERSON, ORGANIZATION, and LOCATION.", "labels": [], "entities": [{"text": "MISC", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6773844957351685}, {"text": "PERSON", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.986305832862854}, {"text": "ORGANIZATION", "start_pos": 92, "end_pos": 104, "type": "METRIC", "confidence": 0.9843961000442505}, {"text": "LOCATION", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9035912752151489}]}, {"text": "We ran Stanford without the MISC class and LBJ with MISC.", "labels": [], "entities": [{"text": "Stanford", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.959658145904541}, {"text": "MISC class", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.7730139195919037}, {"text": "LBJ", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.5029153227806091}, {"text": "MISC", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8748335838317871}]}, {"text": "However, the problems highlighted in this paper remain equally prevalent even without this discrepancy.", "labels": [], "entities": []}, {"text": "The agreement rate between the taggers is shown in.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.964861124753952}]}, {"text": "We find that the highest rate of agreement is for PERSONS, with an agreement rate of 58%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.802621066570282}, {"text": "PERSONS", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.7295712232589722}, {"text": "agreement rate", "start_pos": 67, "end_pos": 81, "type": "METRIC", "confidence": 0.9666589498519897}]}, {"text": "The agreement rate on LOCATIONS is 37%, and the agreement rate on ORGANIZATIONS is 34%.", "labels": [], "entities": [{"text": "LOCATIONS", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.6811582446098328}, {"text": "agreement rate", "start_pos": 48, "end_pos": 62, "type": "METRIC", "confidence": 0.9564260840415955}, {"text": "ORGANIZATIONS", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.9094416499137878}]}, {"text": "Even on cases where the taggers agree, the classification can be incorrect.", "labels": [], "entities": []}, {"text": "Both taggers classify \"African Americans\" as LO-CATIONS.", "labels": [], "entities": [{"text": "LO-CATIONS", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.8184133768081665}]}, {"text": "Both treat \"Jr.\" as being part of a person's name, as well as being a LOCATION (in fact, the tagging of \"Jr.\" as a LOCATION is more frequent in both).", "labels": [], "entities": []}, {"text": "For our second evaluation criterion, i.e., withindiscourse ambiguity, we determined the percentage of globally ambiguous entities (entities that had more than one classification across the entire corpus) that occurred with multiple taggings within a single document.", "labels": [], "entities": []}, {"text": "This analysis showed that the problems described above are not anecdotal.", "labels": [], "entities": []}, {"text": "shows that at least 40% of the entities that have more than one classification co-occur within a document.", "labels": [], "entities": []}, {"text": "This is true for both taggers and all of the named entity classes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A sampling of false positives for each class as tagged by the Stanford and LBJ taggers", "labels": [], "entities": [{"text": "Stanford and LBJ taggers", "start_pos": 72, "end_pos": 96, "type": "DATASET", "confidence": 0.8155737221240997}]}, {"text": " Table 2: Agreement rate by class between the Stanford and LBJ taggers", "labels": [], "entities": [{"text": "Agreement rate", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9738225340843201}, {"text": "Stanford and LBJ taggers", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.6947611570358276}]}, {"text": " Table 3: Co-occurrence rates between entities with more than one tag for Stanford and LBJ taggers", "labels": [], "entities": [{"text": "LBJ taggers", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.6276705265045166}]}, {"text": " Table 4: Agreement rate by class between the Stanford (and LBJ) and BBN IdentiFinder taggers on the ANC Corpus", "labels": [], "entities": [{"text": "Agreement rate", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9421731233596802}, {"text": "Stanford (and LBJ) and BBN IdentiFinder taggers", "start_pos": 46, "end_pos": 93, "type": "DATASET", "confidence": 0.7602784368726943}, {"text": "ANC", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.704083263874054}]}, {"text": " Table 5: A sampling of terms that were tagged as belonging to more than one class in the American National Corpus", "labels": [], "entities": [{"text": "American National Corpus", "start_pos": 90, "end_pos": 114, "type": "DATASET", "confidence": 0.9715197285016378}]}, {"text": " Table 6: Co-occurrence rates between entities with more than one tag for the American National Corpus", "labels": [], "entities": [{"text": "American National Corpus", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.8969297210375468}]}]}