{"title": [{"text": "Text Mining Techniques for Leveraging Positively Labeled Data", "labels": [], "entities": [{"text": "Text Mining", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6512382626533508}]}], "abstractContent": [{"text": "Suppose we have a large collection of documents most of which are unlabeled.", "labels": [], "entities": []}, {"text": "Suppose further that we have a small subset of these documents which represent a particular class of documents we are interested in, i.e. these are labeled as positive examples.", "labels": [], "entities": []}, {"text": "We may have reason to believe that there are more of these positive class documents in our large unlabeled collection.", "labels": [], "entities": []}, {"text": "What data mining techniques could help us find these unlabeled positive examples?", "labels": [], "entities": [{"text": "data mining", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.7169692814350128}]}, {"text": "Here we examine machine learning strategies designed to solve this problem.", "labels": [], "entities": []}, {"text": "We find that a proper choice of machine learning method as well as training strategies can give substantial improvement in retrieving, from the large collection, data enriched with positive examples.", "labels": [], "entities": []}, {"text": "We illustrate the principles with areal example consisting of multiword UMLS phrases among a much larger collection of phrases from Medline.", "labels": [], "entities": [{"text": "Medline", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.9582821130752563}]}], "introductionContent": [{"text": "Given a large collection of documents, a few of which are labeled as interesting, our task is to identify unlabeled documents that are also interesting.", "labels": [], "entities": []}, {"text": "Since the labeled data represents the data we are interested in, we will refer to it as the positive class and to the remainder of the data as the negative class.", "labels": [], "entities": []}, {"text": "We use the term negative class, however, documents in the negative class are not necessarily negative, they are simply unlabeled and the negative class may contain documents relevant to the topic of interest.", "labels": [], "entities": []}, {"text": "Our goal is to retrieve these unknown relevant documents.", "labels": [], "entities": []}, {"text": "A na\u00efve approach to this problem would simply take the positive examples as the positive class and the rest of the collection as the negative class and apply machine learning to learn the difference and rank the negative class based on the resulting scores.", "labels": [], "entities": []}, {"text": "It is reasonable to expect that the top of this ranking would be enriched for the positive class.", "labels": [], "entities": []}, {"text": "But an appropriate choice of methods can improve over the na\u00efve approach.", "labels": [], "entities": []}, {"text": "One issue of importance would be choosing the most appropriate machine learning method.", "labels": [], "entities": []}, {"text": "Our problem can be viewed from two different perspectives: the problem of learning from imbalanced data as well as the problem of recommender systems.", "labels": [], "entities": []}, {"text": "In terms of learning from imbalanced data, our positive class is significantly smaller than the negative, which is the remainder of the collection.", "labels": [], "entities": []}, {"text": "Therefore we are learning from imbalanced data.", "labels": [], "entities": []}, {"text": "Our problem is also a recommender problem in that based on a few examples found of interest to a customer we seek similar positive examples amongst a large collection of unknown status.", "labels": [], "entities": []}, {"text": "Our bias is to use some form of wide margin classifier for our problem as such classifiers have given good performance for both the imbalanced data problem and the recommender problem (; Lewis,).", "labels": [], "entities": []}, {"text": "Imbalanced data sets arise very frequently in text classification problems.", "labels": [], "entities": [{"text": "text classification", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8119130730628967}]}, {"text": "The issue with imbalanced learning is that the large prevalence of negative documents dominates the decision process and harms classification performance.", "labels": [], "entities": []}, {"text": "Several approaches have been proposed to deal with the problem including sampling methods and cost-sensitive learning methods and are described in.", "labels": [], "entities": []}, {"text": "These studies have shown that there is no clear advantage of one approach versus another.", "labels": [], "entities": []}, {"text": "points out that cost-sensitive methods and sampling methods are related in the sense that altering the class distribution of training data is equivalent to altering misclassification cost.", "labels": [], "entities": []}, {"text": "Based on these studies we examine cost-sensitive learning in which the cost on the positive set is increased, as a useful approach to consider when using an SVM.", "labels": [], "entities": []}, {"text": "In order to show how cost-sensitive learning for an SVM is formulated, we write the standard equations for an SVM following The cost-sensitive version modifies (1) to become and now we can chooser \uf02b and r \uf02d to magnify the losses appropriately.", "labels": [], "entities": []}, {"text": "Generally we taker \uf02d to be 1, and r \uf02b to be some factor larger than 1.", "labels": [], "entities": []}, {"text": "We refer to this formulation as CS-SVM.", "labels": [], "entities": []}, {"text": "Generally, the same algorithms used to minimize (1) can be used to minimize (3).", "labels": [], "entities": []}, {"text": "Recommender systems use historical data on user preferences, purchases and other available data to predict items of interest to a user.", "labels": [], "entities": []}, {"text": "propose a wide margin classifier with a quadratic loss function as very effective for this purpose (see appendix).", "labels": [], "entities": []}, {"text": "It is used in (1) and requires no adjustment in cost between positive and negative examples.", "labels": [], "entities": []}, {"text": "It is proposed as a better method than varying costs because it does not require searching for the optimal cost relationship between positive and negative examples.", "labels": [], "entities": []}, {"text": "We will use for our wide margin classifier the modified Huber loss function).", "labels": [], "entities": [{"text": "Huber loss function", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.6740677654743195}]}, {"text": "The modified Huber loss function is quadratic where this is important and has the form We also use it in (1).", "labels": [], "entities": [{"text": "Huber loss function", "start_pos": 13, "end_pos": 32, "type": "METRIC", "confidence": 0.6180898050467173}]}, {"text": "We refer to this approach as the Huber method) as opposed to SVM.", "labels": [], "entities": []}, {"text": "We compare it with SVM and CS-SVM.", "labels": [], "entities": []}, {"text": "We used our own implementations for SVM, CS-SVM, and Huber that use gradient descent to optimize the objective function.", "labels": [], "entities": [{"text": "Huber", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9370179176330566}]}, {"text": "The methods we develop are related to semisupervised learning approaches) and active learning.", "labels": [], "entities": []}, {"text": "Our method differs from active learning in that active learning seeks those unlabeled examples for which labels prove most informative in improving the classifier.", "labels": [], "entities": []}, {"text": "Typically these examples are the most uncertain.", "labels": [], "entities": []}, {"text": "Some semisupervised learning approaches start with labeled examples and iteratively seek unlabeled examples closest to already labeled data and impute the known label to the nearby unlabeled examples.", "labels": [], "entities": []}, {"text": "Our goal is simply to retrieve plausible members for the positive class with as high a precision as possible.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9973800778388977}]}, {"text": "Our method has value even in cases where human review of retrieved examples is necessary.", "labels": [], "entities": []}, {"text": "The imbalanced nature of the data and the presence of positives in the negative class make this a challenging problem.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss additional strategies proposed in this work, describe the data used and design of experiments, and provide the evaluation measure used.", "labels": [], "entities": []}, {"text": "In Section 3 we present our results, in Sections 4 and 5 we discuss our approach and draw conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "A standard way to measure the success of a classifier is to evaluate its performance on a collection of documents that have been previously classified as positive or negative.", "labels": [], "entities": []}, {"text": "This is usually accomplished by randomly dividing up the data into training and test portions which are separate.", "labels": [], "entities": []}, {"text": "The classifier is then trained on the training portion, and is tested on test portion.", "labels": [], "entities": []}, {"text": "This can be done in a cross-validation scheme or by randomly re-sampling train and test portions repeatedly.", "labels": [], "entities": []}, {"text": "We are interested in studying the case where only some of the positive documents are labeled.", "labels": [], "entities": []}, {"text": "We simulate that situation by taking a portion of the positive data and including it in the negative training set.", "labels": [], "entities": []}, {"text": "We refer to that subset of positive documents as tracer data (Tr).", "labels": [], "entities": []}, {"text": "The tracer data is then effectively mislabeled as negative.", "labels": [], "entities": []}, {"text": "By introducing such an artificial supplement to the negative training set we are not only certain that the negative set contains mislabeled positive examples, but we know exactly which ones they are.", "labels": [], "entities": []}, {"text": "Our goal is to automatically identify these mislabeled documents in the negative set and knowing their true labels will allow us to measure how successful we are.", "labels": [], "entities": []}, {"text": "Our measurements will be carried out on the negative class and for this purpose it is convenient to write the negative class as composed of true negatives and tracer data (false negatives)  We evaluate performance using Mean Average Precision (MAP) (Baeza-Yates and Ribeiro-Neto 1999).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 220, "end_pos": 248, "type": "METRIC", "confidence": 0.9672150015830994}]}, {"text": "The mean average precision is the mean value of the average precisions computed for all topics in each of the datasets in our study.", "labels": [], "entities": [{"text": "mean average precision", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.854302187760671}]}, {"text": "Average precision is the average of the precisions at each rank that contains a true positive document.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.8466641902923584}, {"text": "precisions", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.989234447479248}]}], "tableCaptions": [{"text": " Table 1: MAP scores trained with three levels of tracer data introduced to the negative training set.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5318192839622498}]}, {"text": " Table 2: MAP scores for Huber and SVM trained with two levels of tracer data introduced to the  negative training set using cross training technique.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8517208695411682}, {"text": "Huber", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.8864918351173401}]}, {"text": " Table 3: MAP scores for Huber and SVM  trained with 20% and 50% tracer data introduced to  the negative training set for Reuters dataset.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8993301391601562}, {"text": "Huber", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.9434792399406433}, {"text": "Reuters dataset", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.9706678688526154}]}, {"text": " Table 4: MAP scores for Huber and SVM  trained with 20% and 50% tracer data introduced to  the negative training set for 20NewsGroups dataset.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8728604316711426}, {"text": "Huber", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.9407208561897278}, {"text": "20NewsGroups dataset", "start_pos": 122, "end_pos": 142, "type": "DATASET", "confidence": 0.9070483148097992}]}]}