{"title": [{"text": "Toward Construction of Spoken Dialogue System that Evokes Users' Spontaneous Backchannels", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper addresses a first step toward a spoken dialogue system that evokes user's spontaneous backchannels.", "labels": [], "entities": []}, {"text": "We construct an HMM-based dialogue-style text-to-speech (TTS) system that generates human-like cues that evoke users' backchannels.", "labels": [], "entities": [{"text": "HMM-based dialogue-style text-to-speech (TTS)", "start_pos": 16, "end_pos": 61, "type": "TASK", "confidence": 0.5673030912876129}]}, {"text": "A spoken dialogue system for information navigation was implemented and the TTS was evaluated in terms of evoked user backchannels.", "labels": [], "entities": [{"text": "information navigation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.8707407712936401}, {"text": "TTS", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.4371185302734375}]}, {"text": "We conducted user experiments and demonstrated that the user backchannels evoked by our TTS are more informative for the system in detecting users' feelings than those by conventional reading-style TTS.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most enduring problems in spoken dialogue systems research is realizing a natural dialogue in a human-human form.", "labels": [], "entities": []}, {"text": "One direction researchers have been utilizing spontaneous nonverbal and paralinguistic information.", "labels": [], "entities": []}, {"text": "For example, This paper focuses on backchannels, one of the most common forms of para-linguistic information in human-human dialogue.", "labels": [], "entities": []}, {"text": "In particular, we focus on users' verbal feedback, such as \"uh-huh\" (called Aizuchi in Japanese), and non-verbal feedback in the form of nods.", "labels": [], "entities": []}, {"text": "Such backchannels are very common phenomena, and considered to be used to facilitate smooth human-human communications.", "labels": [], "entities": []}, {"text": "In this regard, Maynard indicated that such backchannels are listener's signals to let the speaker continue speaking (continuer), to indicate that the listener understands and consents.", "labels": [], "entities": []}, {"text": "It was also hypothesized that humans detect feelings expressed via backchannels, and the correlation between backchannel patterns and user interests was examined ().", "labels": [], "entities": []}, {"text": "These studies indicate that detection of spontaneous user backchan- * currently with Japan Advanced Institute of nels can benefit spoken dialogue systems by providing informative cues that reflect the user's situation.", "labels": [], "entities": [{"text": "Japan Advanced Institute of nels", "start_pos": 85, "end_pos": 117, "type": "DATASET", "confidence": 0.9312312841415405}]}, {"text": "For instance, if a spoken dialogue system can detect user's backchannels, it can facilitate smooth turntaking.", "labels": [], "entities": []}, {"text": "The system can also detect user's feelings and judge if it should continue the current topic or change it.", "labels": [], "entities": []}, {"text": "Despite these previous studies and decades of analysis on backchannels, few practical dialogue systems have made use of them.", "labels": [], "entities": []}, {"text": "This is probably due to the fact that users do not react as spontaneously to dialogue systems as they do to other humans.", "labels": [], "entities": []}, {"text": "We presume one of the reasons for this is the unnatural intonation of synthesized speech.", "labels": [], "entities": []}, {"text": "That is, conventional speech synthesizers do not provide users with signs to elicit backchannels; an appropriate set of lexical, acoustic and prosodic cues (or backchannel-inviting cues (A.), which tends to precede the listener's backchannels in human-human communication.", "labels": [], "entities": []}, {"text": "Though recorded human speech can provide such cues, it is costly to re-record system's speech every time system scripts are updated.", "labels": [], "entities": []}, {"text": "In this work, we therefore tackle the challenge of constructing dialogue-style text-to-speech (TTS) system that inspires users to make spontaneous backchannels under the hypothesis of: People will give more spontaneous backchannels to a spoken dialogue system that makes more spontaneous backchannel-inviting cues than a spoken dialogue system that makes less spontaneous ones.", "labels": [], "entities": []}, {"text": "which is derived from the Media Equation ().", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our TTS system based on users' reactions, a sightseeing guidance spoken dialogue sys- tem that assist users in making decision was implemented.", "labels": [], "entities": [{"text": "TTS", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7814689874649048}]}, {"text": "The system can explain six sightseeing spots in Kyoto.", "labels": [], "entities": []}, {"text": "The system provides responses to user requests for explanation about a certain spot.", "labels": [], "entities": []}, {"text": "Each descriptive text on a sightseeing spot consists of 500 (\u00b11%) characters, 30 phrases.", "labels": [], "entities": []}, {"text": "The text is synthesized using section 3 TTS 2 . We set the speech rate of our TTS as nine phoneme per second.", "labels": [], "entities": []}, {"text": "A display is used to present photos of the target sightseeing spot and an animated 3D desktop avatar named Hanna.", "labels": [], "entities": []}, {"text": "shows the GUI the user sees.", "labels": [], "entities": []}, {"text": "The avatar can express its status through several motions.", "labels": [], "entities": []}, {"text": "For example, when the user begins speaking, it can express the state of listening using the listener's motion, as shown in the figure.", "labels": [], "entities": []}, {"text": "A sample dialogue with the system is shown in.", "labels": [], "entities": []}, {"text": "A video (with English subtitles) of an sample dialogue with a user can be seen at http://mastarpj.nict.go.jp/\u02dcxtmisu/video/TTS.wmv.", "labels": [], "entities": []}, {"text": "To compare the effectiveness of our TTS in evoking users' spontaneous backchannels, we constructed a comparison system that adopts a conventional reading-style TTS system.", "labels": [], "entities": []}, {"text": "An HMM model was trained using 10-hour reading-style speech by another professional female narrator.", "labels": [], "entities": []}, {"text": "Other settings, such as the descriptive text and avatar agent, were the same as those of the base system.", "labels": [], "entities": []}, {"text": "We evaluated the TTS systems using 30 subjects who had not previously used spoken dialogue systems.", "labels": [], "entities": [{"text": "TTS", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8416297435760498}]}, {"text": "Subjects were asked to use the dialogue system in two settings; dialogue-style TTS system and reading-style TTS system.", "labels": [], "entities": []}, {"text": "The experiment was conducted in a small (about 2 m 2 ) soundproof room with no one else present.", "labels": [], "entities": []}, {"text": "We instructed the subjects to speak with the avatar agent Hanna (not with the system).", "labels": [], "entities": []}, {"text": "We also told them that the avatar agent was listening to their speech at all times using the microphone, and was observing their reactions using the camera above the display . Subjects were given the task of acquiring information about three candidate sightseeing spots in Kyoto shown on the display and then selecting one that they liked.", "labels": [], "entities": []}, {"text": "An example dialogue with the system is shown in.", "labels": [], "entities": []}, {"text": "A video (with English subtitles) showing areal user dialogue can be seen at http://mastarpj.nict.go.jp/\u02dcxtmisu/video/exp.avi.", "labels": [], "entities": []}, {"text": "The system did not actually sense the subjects' reactions.", "labels": [], "entities": []}, {"text": "1. Overall, which speech was better?", "labels": [], "entities": []}, {"text": "2. Which speech had easier-to-understand explanations?", "labels": [], "entities": []}, {"text": "3. For which speech did you feel compelled to give backchannels?", "labels": [], "entities": []}, {"text": "4. Which speech was more appropriate for this system?", "labels": [], "entities": []}, {"text": "5. Which speech had more human-like explanation?", "labels": [], "entities": []}, {"text": "After the subject selected from candidate spots, we changed the TTS system settings and instructed the user to have another dialogue session selecting one of another three spots.", "labels": [], "entities": [{"text": "TTS", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.4178963303565979}]}, {"text": "Considering the effects of the order, the subjects were divided into four groups; the first group (Group 1) used the system in the order of \"Spot list A with dialogue-style speech \ud97b\udf59 Spot list B with reading-style speech,\" the second group (Group 2) worked in reverse order.", "labels": [], "entities": []}, {"text": "Groups 3 and 4 used a system alternating the order of the spot sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Confusion matrix of classification", "labels": [], "entities": []}]}