{"title": [{"text": "The Impact of Task-Oriented Feature Sets on HMMs for Dialogue Modeling", "labels": [], "entities": [{"text": "Dialogue Modeling", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8120392858982086}]}], "abstractContent": [{"text": "Human dialogue serves as a valuable model for learning the behavior of dialogue systems.", "labels": [], "entities": []}, {"text": "Hidden Markov models' sequential structure is well suited to modeling human dialogue, and their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process with a layer of implicit, highly influential structure.", "labels": [], "entities": []}, {"text": "HMMs have been shown to be effective fora variety of descriptive and pre-dictive dialogue tasks.", "labels": [], "entities": []}, {"text": "For task-oriented dialogue , understanding the learning behavior of HMMs is an important step toward building unsupervised models of human dialogue.", "labels": [], "entities": []}, {"text": "This paper examines the behavior of HMMs under six experimental conditions including different task-oriented feature sets and preprocessing approaches.", "labels": [], "entities": []}, {"text": "The findings highlight the importance of providing HMM learning algorithms with rich task-based information.", "labels": [], "entities": [{"text": "HMM learning algorithms", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.8929209113121033}]}, {"text": "Additionally, the results suggest how specific metrics should be used depending on whether the models will be employed primarily in a descriptive or predictive manner.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human dialogue serves as a valuable model for learning the behavior of dialogue systems.", "labels": [], "entities": []}, {"text": "For this reason, corpus-based approaches to dialogue management tasks have been an increasingly active area of research.", "labels": [], "entities": [{"text": "dialogue management tasks", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.8416163523991903}]}, {"text": "Modeling the dialogue policies that humans employ permits us to directly extract conversational and task-based expertise.", "labels": [], "entities": []}, {"text": "These techniques hold great promise for scaling gracefully to large corpora, and for transferring well across domains.", "labels": [], "entities": []}, {"text": "The richness and flexibility of human dialogue introduce nondeterministic and complex patterns that present challenges for machine learning approaches.", "labels": [], "entities": []}, {"text": "One approach that has been successfully employed in dialogue modeling is the hidden Markov model (HMM).", "labels": [], "entities": [{"text": "dialogue modeling", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.9256148040294647}]}, {"text": "These models are well suited to the sequential nature of dialogue ().", "labels": [], "entities": []}, {"text": "Moreover, their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process whose observations are influenced by a layer of implicit, yet highly relevant, structure.", "labels": [], "entities": []}, {"text": "HMMs have been shown to perform well on important dialogue management tasks such as automatic dialogue act classification ().", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7845158576965332}, {"text": "dialogue act classification", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.6753005683422089}]}, {"text": "Our work has employed HMMs fora different goal: learning dialogue policies, or strategies, from corpora).", "labels": [], "entities": []}, {"text": "This work can be viewed from two perspectives.", "labels": [], "entities": []}, {"text": "First, a descriptive goal of the work is to learn models that describe the nature of human dialogues in succinct probabilistic terms, in away that facilitates important qualitative investigations.", "labels": [], "entities": []}, {"text": "The second and complementary goal is predictive: learning models that accurately predict the dialogue moves of humans, in order to capture a dialogue policy that can be used within a system.", "labels": [], "entities": []}, {"text": "Both of these goals are of paramount importance in tutorial dialogue, in which tutors and students engage in dialogue in support of a learning task.", "labels": [], "entities": []}, {"text": "Descriptive modeling represents a critical step toward more fully understanding the phenomena that contribute to the high effectiveness of human tutoring, which has to date been unmatched by tutorial dialogue systems.", "labels": [], "entities": [{"text": "Descriptive modeling", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8034273386001587}]}, {"text": "Predictive models, on the other hand, maybe used directly as dialogue policies within systems.", "labels": [], "entities": []}, {"text": "The HMMs considered here were learned from an annotated corpus of textual human-human tutorial dialogue.", "labels": [], "entities": []}, {"text": "In this domain, HMMs have been shown to correspond qualitatively to widely held conceptions of tutorial dialogue strategies, and adjacency pair analysis before model learning has been shown to enhance this qualitative correspondence ().", "labels": [], "entities": []}, {"text": "Moreover, HMMs can identify in an unsupervised fashion structural components that correlate with student knowledge gain.", "labels": [], "entities": []}, {"text": "However, to date, several important questions have not been explored.", "labels": [], "entities": []}, {"text": "The answers to these questions have implications for learning HMMs for task-oriented dialogues.", "labels": [], "entities": []}, {"text": "The questions include the following: 1) How reliably does the HMM learning framework converge to the hyperparameter N, the best-fit number of hidden states?", "labels": [], "entities": []}, {"text": "2) What are the effects of preprocessing approaches, specifically, adjacency pair analysis, on the resulting HMMs?", "labels": [], "entities": [{"text": "adjacency pair analysis", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.6722437043984731}]}, {"text": "3) How do different feature sets for task-oriented dialogue impact the descriptive fit and predictive power of learned HMMs?", "labels": [], "entities": []}, {"text": "This paper addresses these questions.", "labels": [], "entities": []}, {"text": "The findings suggest that model stability and predictive power benefit from the richest possible input sequences, which include not only dialogue acts but also information about the task state and the absence of particular tutor dialogue moves.", "labels": [], "entities": []}, {"text": "Additionally, we find that traditional measures of HMM goodness-of-fit may not identify the most highly predictive models under some conditions.", "labels": [], "entities": []}], "datasetContent": [{"text": "HMMs were learned using three separate feature sets, each providing a progressively more complete picture of the task-oriented dialogues: dialogue acts only (DAONLY), dialogue acts and task events (DATASK), and dialogue acts with both task correctness events and tags for delayed tutor feedback (DATASKDELAY).", "labels": [], "entities": []}, {"text": "In addition to the three different feature sets, each condition included one of two types of preprocessing.", "labels": [], "entities": []}, {"text": "Each type of model was trained on unaltered sequences of the annotated tags, which we refer to as the UNIGRAM condition.", "labels": [], "entities": []}, {"text": "Additionally, each type of model was trained on sequences with statistically dependent adjacency pairs joined in a preprocessing step as described in Section 3.4.", "labels": [], "entities": []}, {"text": "The UNIGRAM and ADJPAIR conditions were explored for each of the three feature sets, resulting in six experimental conditions.", "labels": [], "entities": [{"text": "UNIGRAM", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8851302862167358}, {"text": "ADJPAIR", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.48775815963745117}]}, {"text": "These conditions were chosen in order to explore the convergence behavior of HMMs under the different feature sets and preprocessing, and to compare measures of descriptive fit with measures of predictive power.", "labels": [], "entities": []}, {"text": "show a subset of the DAONLY UNIGRAM model and the DATASKDELAY ADJPAIR model.", "labels": [], "entities": [{"text": "DAONLY UNIGRAM model", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.5971187253793081}, {"text": "DATASKDELAY", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.7815163135528564}, {"text": "ADJPAIR", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.47345998883247375}]}, {"text": "These figures depict the structure of our HMMs: each hidden state is associated with an emission probability distribution over the possible observation symbols.", "labels": [], "entities": []}, {"text": "For the DAONLY condition, both the UNIGRAM and ADJPAIR models generally improve until N=12 or 13, after which the fit generally worsens.", "labels": [], "entities": [{"text": "DAONLY", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9069176316261292}, {"text": "UNIGRAM", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.5376067161560059}, {"text": "ADJPAIR", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.6412368416786194}, {"text": "fit", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9592347741127014}]}, {"text": "A different pattern emerges for the DATASK condition, in which the UNIGRAM sequences are optimally fit to a model with 16 states, while the ADJPAIR sequences are fit to a model with 8 states.", "labels": [], "entities": [{"text": "DATASK", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.7326224446296692}]}, {"text": "Finally, for the DATASKDELAY condition, the UNIGRAM sequences are best fit by a model with 10 hidden states, while the ADJPAIR sequences are fit best by 9.", "labels": [], "entities": [{"text": "DATASKDELAY", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.8641306757926941}]}, {"text": "Typically, we see that ADJPAIR sequences are fit to slightly simpler models in terms of the hyperparameter N, number of hidden states.", "labels": [], "entities": []}, {"text": "Stability in the hyperparameter N is an important consideration because an underlying assumption of our work is that the hidden states correspond to unobserved stochastic structures of the real world process-that is, we hypothesize that a \"true\" value for N exists.", "labels": [], "entities": []}, {"text": "We would like models to exhibit decreasing variation in goodness of fit measures around this true N.", "labels": [], "entities": []}, {"text": "To examine this stability we consider the three best AIC values for each condition and their corresponding Ns: the set {N k-best | k=1,2,3}.", "labels": [], "entities": []}, {"text": "The range of this set indicates how \"far apart\" the best three Ns are: for example, in the DAONLY UNIGRAM condition, the top three models have Ns of {13,10,15}, yielding a range of 5.", "labels": [], "entities": [{"text": "DAONLY", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9325131773948669}, {"text": "UNIGRAM", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.45649653673171997}]}, {"text": "Intuitively, a small value for this metric indicates that the model has converged tightly on N. shows the stability results for the six different experimental conditions.", "labels": [], "entities": []}, {"text": "As shown in the for the DATASK and DATASKDELAY conditions, the ADJPAIR models achieve the smallest range among the top three values of N; these models converge most tightly to the \"best\" value.", "labels": [], "entities": []}], "tableCaptions": []}