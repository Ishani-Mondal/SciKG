{"title": [{"text": "Extractive Multi-Document Summaries Should Explicitly Not Contain Document-Specific Content", "labels": [], "entities": [{"text": "Extractive Multi-Document Summaries", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7107951641082764}]}], "abstractContent": [{"text": "Unsupervised approaches to multi-document summarization consist of two steps: finding a content model of the documents to be summarized, and then generating a summary that best represents the most salient information of the documents.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.650063693523407}]}, {"text": "In this paper, we present a sentence selection objective for ex-tractive summarization in which sentences are penalized for containing content that is specific to the documents they were extracted from.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7076708376407623}, {"text": "ex-tractive summarization", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6093797981739044}]}, {"text": "We modify an existing system, HIER-SUM (Haghighi & Vanderwende, 2009), to use our objective, which significantly outperforms the original HIERSUM in pairwise user evaluation.", "labels": [], "entities": []}, {"text": "Additionally, our ROUGE scores advance the current state-of-the-art for both supervised and unsupervised systems with statistical significance.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9390507340431213}]}], "introductionContent": [{"text": "Multi-document summarization is the task of generating a single summary from a set of documents that are related to a single topic.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7521378993988037}]}, {"text": "Summaries should contain information that is relevant to the main ideas of the entire document set, and should not contain information that is too specific to anyone document.", "labels": [], "entities": []}, {"text": "For example, a summary of multiple news articles about the Star Wars movies could contain the words \"Lucas \"and \"Jedi\", but should not contain the name of a fan who was interviewed in one article.", "labels": [], "entities": []}, {"text": "Most approaches to this problem generate summaries extractively, selecting whole or partial sentences from the original text, then attempting to piece them together in a coherent manner.", "labels": [], "entities": []}, {"text": "Extracted text is selected based on its relevance to the main ideas of the document set.", "labels": [], "entities": []}, {"text": "Summaries can be evaluated manually, or with automatic metrics such as ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9884397983551025}]}, {"text": "The use of structured probabilistic topic models has made it possible to represent document set content with increasing complexity.", "labels": [], "entities": []}, {"text": "demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models.", "labels": [], "entities": []}, {"text": "Their most complex hierarchial model improves summary content by teasing out the words that are not general enough to represent the document set as a whole.", "labels": [], "entities": []}, {"text": "Once those words are no longer included in the content word distribution, they are implicitly less likely to appear in the extracted summary as well.", "labels": [], "entities": []}, {"text": "But this objective does not sufficiently keep document-specific content from appearing in multi-document summaries.", "labels": [], "entities": []}, {"text": "In this paper, we present a selection objective that explicitly excludes document-specific content.", "labels": [], "entities": []}, {"text": "We re-implement the HIERSUM system from, and show that using our objective dramatically improves the content of extracted summaries.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.4834759831428528}]}], "datasetContent": [{"text": "Systems are automatically evalatued using ROUGE), which has good correlation with human judgments of summary content.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9741045832633972}]}, {"text": "ROUGE compares n-gram recall between system-generated summaries, and human-authored reference summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5557408332824707}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9773609638214111}]}, {"text": "The first two metrics we compare are unigram and bigram recall, R-1 and R-2, respectively.", "labels": [], "entities": [{"text": "bigram recall", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.7360926270484924}]}, {"text": "The last metric, R-SU4, measures recall of skip-4 bigrams, which may skip one or two words in between the two words to be measured.", "labels": [], "entities": [{"text": "R-SU4", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.6812261343002319}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9985654950141907}]}, {"text": "We set ROUGE to stem both the system and reference summaries, scale our results by 10 2 and present scores with and without stopwords removed.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9971914887428284}]}, {"text": "The ROUGE scores of the original HIERSUM system are given in the first row of table 1, followed by the scores of HIERSUM using our KL(c-d) selection.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9964918494224548}, {"text": "HIERSUM", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.9266404509544373}]}, {"text": "The KL(c-d) selection outperforms the KL selection in each of the ROUGE metrics shown.", "labels": [], "entities": [{"text": "KL(c-d) selection", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.7804155349731445}, {"text": "ROUGE", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.8600292205810547}]}, {"text": "In fact, these results are statistically significant over the baseline KL selection for all but the unigram metrics (R-1 with and without stopwords).", "labels": [], "entities": []}, {"text": "These results show that our KL(c-d) selection yields significant improvements in terms of ROUGE performance, since having fewer irrelevant words in the summaries leaves room for words that are more relevant to the content topic, and therefore more likely to appear in the reference summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9877066016197205}]}, {"text": "The last two rows of table 1 show the scores of two recent state-of-the-art multi-document sum-: ROUGE scores on the DUC 2007 document sets.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.988297164440155}, {"text": "DUC 2007 document sets", "start_pos": 117, "end_pos": 139, "type": "DATASET", "confidence": 0.9751366227865219}]}, {"text": "The first two rows compare the results of the unigram HIERSUM system with its original and our improved selection metrics.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.5564780831336975}]}, {"text": "Bolded scores represent where our system has a significant improvement over the orignal HIERSUM.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.6960244178771973}]}, {"text": "For further comparison, the last two rows show the ROUGE scores of two other state-of-the-art multi-document summarization systems (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9863502383232117}]}, {"text": "See section 6.2 for more details.", "labels": [], "entities": []}, {"text": "Both of these systems select sentences discriminatively on many features in order to maximize ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.9682470858097076}]}, {"text": "The first, PYTHY (, trains on dozens of sentence-level features, such as n-gram and skipgram frequency, named entities, sentence length and position, and also utilizes sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 168, "end_pos": 188, "type": "TASK", "confidence": 0.7143488079309464}]}, {"text": "The second, HYBHSUM, uses a nested Chinese restaurant process () to model a hierarchical content distribution with more complexity than HIERSUM, and uses a regression model to predict scores for new sentences.", "labels": [], "entities": [{"text": "HYBHSUM", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9352993369102478}]}, {"text": "For both of these systems, our summaries are significantly better for R-2 and R-SU4 without stopwords, and comparable in all other metrics.", "labels": [], "entities": []}, {"text": "These results show that our selection objective can make a simple unsupervised model competitive with more complicated supervised models.", "labels": [], "entities": []}, {"text": "For manual evaluation, we performed a pairwise comparison of summaries generated by HIERSUM with both the original and our modified sentence selection objective.", "labels": [], "entities": []}, {"text": "Users were given the two summaries to compare, plus a human-generated reference summary.", "labels": [], "entities": []}, {"text": "The order that the summaries appeared in was random.", "labels": [], "entities": []}, {"text": "We asked users to select which summary was better for the following ques-  Q1 Which was better in terms of overall content?", "labels": [], "entities": []}, {"text": "Q2 Which summary had less repetition?", "labels": [], "entities": [{"text": "repetition", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9976493716239929}]}, {"text": "Q3 Which summary was more coherent?", "labels": [], "entities": []}, {"text": "Q4 Which summary had better focus?", "labels": [], "entities": [{"text": "focus", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9819152355194092}]}, {"text": "We took 87 pairwise preferences from participants over Mechanical Turk.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.869674563407898}]}, {"text": "The results of our evaluation are shown in table 2.", "labels": [], "entities": []}, {"text": "For all attributes, our criterion performs better than the original HIERSUM selection criterion, and our results for Q1 and Q3 are significantly better as determined by Fisher sign test (two-tailed P value < 0.01).", "labels": [], "entities": []}, {"text": "These results confirm that our objective noticably improves the content of extractive summaries by selecting sentences that contain less document-specific information.", "labels": [], "entities": []}, {"text": "This leaves more room in the summary for content that is relevant to the main idea of the document set (Q1) and keeps out content that is not relevant (Q4).", "labels": [], "entities": []}, {"text": "Additionally, although neither criterion explicitly addresses coherence, we found that a significant proportion of users found our summaries to be more coherent (Q3).", "labels": [], "entities": []}, {"text": "We believe this maybe the case because the presence of document-specific information can distract from the main ideas of the summary, and make it less likely that the extracted sentences will flow together.", "labels": [], "entities": []}, {"text": "There is no immediate explanation for why users found our our summaries less repetitive (Q2), since if anything the narrowing of topics due to the negative KL(\u03c6 d ||P S ) term should make for more repetition.", "labels": [], "entities": []}, {"text": "We currently hypothesize that the improved score is simply a spillover from the general improvement in document quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE scores on the DUC 2007 document sets. The first two rows compare the results of the unigram  HIERSUM system with its original and our improved selection metrics. Bolded scores represent where our system has  a significant improvement over the orignal HIERSUM. For further comparison, the last two rows show the ROUGE  scores of two other state-of-the-art multi-document summarization systems (", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9292462468147278}, {"text": "DUC 2007 document sets", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.9812656491994858}, {"text": "ROUGE", "start_pos": 327, "end_pos": 332, "type": "METRIC", "confidence": 0.9681894183158875}]}]}