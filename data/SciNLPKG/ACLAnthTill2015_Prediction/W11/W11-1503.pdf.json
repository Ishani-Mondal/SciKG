{"title": [{"text": "Evaluating an 'off-the-shelf' POS-tagger on Early Modern German text", "labels": [], "entities": [{"text": "Early Modern German text", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.7481766492128372}]}], "abstractContent": [{"text": "The goal of this study is to evaluate an 'off-the-shelf' POS-tagger for modern German on historical data from the Early Modern period (1650-1800).", "labels": [], "entities": []}, {"text": "With no specialised tagger available for this particular stage of the language, our findings will be of particular interest to smaller, humanities-based projects wishing to add POS annotations to their historical data but which lack the means or resources to train a POS tagger themselves.", "labels": [], "entities": []}, {"text": "Our study assesses the effects of spelling variation on the performance of the tagger, and investigates to what extent tagger performance can be improved by using 'normalised' input, where spelling variants in the corpus are standardised to a modern form.", "labels": [], "entities": []}, {"text": "Our findings show that adding such a normalisation layer improves tagger performance considerably.", "labels": [], "entities": [{"text": "tagger", "start_pos": 66, "end_pos": 72, "type": "TASK", "confidence": 0.9792486429214478}]}], "introductionContent": [{"text": "The work described in this paper is part of a larger investigation whose goal is to create a representative corpus of Early Modern German from 1650-1800.", "labels": [], "entities": []}, {"text": "The GerManC corpus, which is due to be completed this summer, was developed to allow for comparative studies of the development and standardisation of English and German in the 17th and 18th centuries.", "labels": [], "entities": [{"text": "GerManC corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8305271565914154}]}, {"text": "In order to facilitate corpus-linguistic investigations, one of the major goals of the project is to annotate the corpus with POS tags.", "labels": [], "entities": []}, {"text": "However, no specialised tools are yet available for processing data from this period.", "labels": [], "entities": []}, {"text": "The goal of this study is therefore to evaluate the performance of an 'off-theshelf' POS-tagger for modern German on data from the Early Modern period, in order to assess if modern tools are suitable fora semi-automatic approach, and how much manual post-processing work would be necessary to obtain gold standard POS annotations.", "labels": [], "entities": []}, {"text": "We report on our results of running the TreeTagger on a subcorpus of GerManC containing over 50,000 tokens of text annotated with gold standard POS tags.", "labels": [], "entities": [{"text": "GerManC", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9146367311477661}]}, {"text": "This subcorpus is the first resource of its kind for this variant of German, and due to its complex structure it represents an ideal test bed for evaluating and adapting existing NLP tools on data from the Early Modern period.", "labels": [], "entities": []}, {"text": "The study described in this paper represents a first step towards this goal.", "labels": [], "entities": []}, {"text": "Furthermore, as spelling variants in our corpus have been manually normalised to a modern standard, this paper also aims to explore the extent to which tagger performance is affected by spelling variation, and to what degree performance can be improved by using 'normalised' input.", "labels": [], "entities": []}, {"text": "Our findings promise to be of considerable interest to other current corpus-based projects of earlier periods of German.", "labels": [], "entities": []}, {"text": "Before presenting the results in Section 4, we describe the corpus design (Section 2), and the preprocessing steps necessary to create the gold standard annotations, including adaptations to the POS tagset (Section 3).", "labels": [], "entities": [{"text": "POS tagset", "start_pos": 195, "end_pos": 205, "type": "DATASET", "confidence": 0.8582563996315002}]}], "datasetContent": [{"text": "The evaluation described in this section aims to complement the findings of for Early Modern English, and a recent study by, in which the TreeTagger is applied to a corpus of texts from Middle High German (MHG) -i.e. a period earlier than ours, from 1050-1350.", "labels": [], "entities": []}, {"text": "Both studies report considerable improvement of POS-tagging accuracy on normalised data.", "labels": [], "entities": [{"text": "POS-tagging", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.5156946182250977}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9015212059020996}]}, {"text": "However, unlike, whose experiments involve retraining the TreeTagger on a modified version of STTS, our experiments assess the \"off-theshelf\" performance of the modern tagger on historical data.", "labels": [], "entities": [{"text": "STTS", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.8241657614707947}]}, {"text": "We further explore the question of what effect spelling variation has on the performance of a tagger, and what improvement can be achieved when running the tool on normalised data.", "labels": [], "entities": [{"text": "spelling variation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7837225198745728}]}, {"text": "shows the results of running the TreeTagger on the original data vs. normalised data in our corpus using the parameter file for modern German supplied with the tagger 2 . The results show that while overall accuracy for running the tagger on the original input is relatively low at 69.6%, using the normalised tokens as input results in an overall improvement of 10% (79.7%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9992477893829346}]}, {"text": "ON Accuracy 69.6% 79.7%: TreeTagger accuracy on original (O) vs. normalised (N) input However, improvement through normalisation is not distributed evenly across the corpus.", "labels": [], "entities": [{"text": "ON", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9422432780265808}, {"text": "Accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.5136770606040955}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9862613081932068}]}, {"text": "shows the performance curves of using TreeTagger on original (O) and normalised (N) input plotted against publication date.", "labels": [], "entities": []}, {"text": "While both curves gradually rise overtime, the improvement curve (measured as difference inaccuracy between N and O) diminishes, a direct result of spelling variation being more prominent in earlier texts (cf.).", "labels": [], "entities": [{"text": "improvement curve", "start_pos": 47, "end_pos": 64, "type": "METRIC", "confidence": 0.9905520975589752}]}, {"text": "Compared with the performance of the TreeTagger on modern data (ca. 97%;), the current results seem relatively low.", "labels": [], "entities": []}, {"text": "However, two issues should betaken into account when interpreting these findings: First, the modern accuracy figures result from an evaluation of the tagger on the text type it was developed on (newspaper text), while GerManC-GS includes a variety of genres, which is bound to result in lower performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9990264177322388}]}, {"text": "Secondly, inter-annotator agreement was also found to be considerably lower in the present task (91.6%) than in one reported for modern German (98.6%;.", "labels": [], "entities": [{"text": "agreement", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.8328065276145935}]}, {"text": "This is likely to be due to the large number of unfamiliar word forms and variants in the corpus, which represent a problem for human annotators.", "labels": [], "entities": []}, {"text": "Finally, provides a more detailed overview of the effects of spelling variation on POS tagger performance.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 83, "end_pos": 93, "type": "TASK", "confidence": 0.806205153465271}]}, {"text": "Of 12,744 normalised tokens in the corpus, almost half (5981; 47%) are only tagged correctly when using the normalised variants as input.", "labels": [], "entities": []}, {"text": "Using the original word form as input results in a false POS tag in these cases.", "labels": [], "entities": []}, {"text": "Overall, this accounts for an improvement of around 10.3% (5981 out of 57,845 tokens in the corpus).", "labels": [], "entities": []}, {"text": "However, 32% (4119) of normalised tokens are tagged correctly using both N and O input, while 18% (2339) of tokens are tagged incorrectly using both types of input.", "labels": [], "entities": []}, {"text": "This means that for 50% of all annotated spelling variants, normalisation has no effect on POS tagger performance.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 91, "end_pos": 101, "type": "TASK", "confidence": 0.6965692937374115}]}, {"text": "Ina minority of cases (305; 3%) normalisation has a negative effect on tagger accuracy.", "labels": [], "entities": [{"text": "tagger", "start_pos": 71, "end_pos": 77, "type": "TASK", "confidence": 0.9654281139373779}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9174301624298096}]}], "tableCaptions": []}