{"title": [{"text": "Acoustic transformations to improve the intelligibility of dysarthric speech", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes modifications to acoustic speech signals produced by speakers with dysarthria in order to make those utterances more intelligible to typical listeners.", "labels": [], "entities": []}, {"text": "These modifications include the correction of tempo, the adjustment of formant frequencies in sonorants, the removal of aberrant voicing , the deletion of phoneme insertion errors, and the replacement of erroneously dropped phonemes.", "labels": [], "entities": [{"text": "tempo", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.8702445030212402}]}, {"text": "Through simple evaluations of in-telligibility with na\u00a8\u0131vena\u00a8\u0131ve listeners, we show that the correction of phoneme errors results in the greatest increase in intelligibility and is therefore a desirable mechanism for the eventual creation of augmentative application software for individuals with dysarthria.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dysarthria is a set of neuromotor disorders that impair the physical production of speech.", "labels": [], "entities": []}, {"text": "These impairments reduce the normal control of the primary vocal articulators but do not affect the regular comprehension or production of meaningful, syntactically correct language.", "labels": [], "entities": []}, {"text": "For example, damage to the recurrent laryngeal nerve reduces control of vocal fold vibration (i.e., phonation), which can result in aberrant voicing.", "labels": [], "entities": []}, {"text": "Inadequate control of soft palate movement caused by disruption of the vagus cranial nerve may lead to a disproportionate amount of air being released through the nose during speech (i.e., hypernasality).", "labels": [], "entities": []}, {"text": "The lack of articulatory control also leads to various involuntary non-speech sounds including velopharyngeal or glottal noise).", "labels": [], "entities": []}, {"text": "More commonly, alack of tongue and lip dexterity often produces heavily slurred speech and a more diffuse and less differentiable vowel target space ().", "labels": [], "entities": []}, {"text": "The neurological damage that causes dysarthria usually affects other physical activity as well which can have a drastically adverse affect on mobility and computer interaction.", "labels": [], "entities": []}, {"text": "For instance, severely dysarthric speakers are 150 to 300 times slower than typical users in keyboard interaction ().", "labels": [], "entities": []}, {"text": "However, since dysarthric speech is often only 10 to 17 times slower than that of typical speakers, speech is a viable input modality for computer-assisted interaction.", "labels": [], "entities": []}, {"text": "Consider a dysarthric individual who must travel into a city by public transportation.", "labels": [], "entities": []}, {"text": "This might involve purchasing tickets, asking for directions, or indicating intentions to fellow passengers, all within a noisy and crowded environment.", "labels": [], "entities": []}, {"text": "A personal portable communication device in this scenario (either hand-held or attached to a wheelchair) would transform relatively unintelligible speech spoken into a microphone to make it more intelligible before being played over a set of speakers.", "labels": [], "entities": []}, {"text": "Such a system could facilitate interaction and overcome difficult or failed attempts at communication in daily life.", "labels": [], "entities": []}, {"text": "We propose a system that avoids drawbacks of other voice-output communication aids that output only synthetic speech.", "labels": [], "entities": []}, {"text": "Before software for such a device is designed, our goal is to establish and evaluate a set of modifications to dysarthric speech to produce a more intelligible equivalent.", "labels": [], "entities": []}, {"text": "Understanding the utility of each of these techniques will be crucial to effectively designing the proposed system.", "labels": [], "entities": []}, {"text": "11 2 Background and related work described an experiment in which 8 dysarthric individuals (with either cerebral palsy or multiple sclerosis) controlled noncritical devices in their home (e.g., TV) with automatic speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 213, "end_pos": 231, "type": "TASK", "confidence": 0.7328125834465027}]}, {"text": "Command vocabularies consisted of very simple phrases (e.g., \"TV channel up\", \"Radio volume down\") and feedback was provided to the user either by visual displays or by auditory cues.", "labels": [], "entities": []}, {"text": "This speech-based environmental control was compared with a 'scanning' interface in which a button is physically pressed to iteratively cycle through a list of alternative commands, words, or phrases.", "labels": [], "entities": []}, {"text": "While the speech interface made more errors (between 90.8% and 100% accuracy after training) than the scanning interface (100% accuracy), the former was significantly faster (7.7s vs 16.9s, on average).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9948234558105469}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9924319982528687}]}, {"text": "Participants commented that speech was significantly less tiring than the scanning interface, and just as subjectively appealing (.", "labels": [], "entities": []}, {"text": "Similar results were obtained in other comparisons of speech and scanning interfaces, and command-and-control systems (.", "labels": [], "entities": []}, {"text": "Speech is a desirable method of expression for individuals with dysarthria.", "labels": [], "entities": []}, {"text": "There are many augmentative communication devices that employ synthetic text-to-speech in which messages can be written on a specialized keyboard or played back from a repository of pre-recorded phrases).", "labels": [], "entities": []}, {"text": "This basic system architecture can be modified to allow for the replacement of textual input with spoken input.", "labels": [], "entities": []}, {"text": "However, such a scenario would involve some degree of automatic speech recognition, which is still susceptible to fault despite recent advances.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.6701746731996536}]}, {"text": "Moreover, the type of synthetic speech output produced by such systems often lacks a sufficient degree of individual affectation or natural expression that one might expect in typical human speech (.", "labels": [], "entities": []}, {"text": "The use of prosody to convey personal information such as one's emotional state is generally not supported by such systems but is nevertheless a key part of a general communicative ability.", "labels": [], "entities": []}, {"text": "Transforming one's speech in away that preserves the natural prosody will similarly also preserve extra-linguistic information such as emotions, and is therefore a pertinent response to the limitations of current technology.", "labels": [], "entities": []}, {"text": "proposed the voice transformation system shown in figure 1 which produced output speech by concatenating together original unvoiced segments with synthesized voiced segments that consisted of a superposition of the original high-bandwidth signal with synthesized low-bandwidth formants.", "labels": [], "entities": [{"text": "voice transformation", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.745559424161911}]}, {"text": "These synthesized formants were produced by modifications to input energy, pitch generation, and formant modifications.", "labels": [], "entities": []}, {"text": "Modifications to energy and formants were performed by Gaussian mixture mapping, as described below, in which learned relationships between dysarthric and target acoustics were used to produce output closer to the target space.", "labels": [], "entities": [{"text": "Gaussian mixture mapping", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6480782131354014}]}, {"text": "This process was intended to be automated, but performed extensive hand-tuning and manually identified formants in the input.", "labels": [], "entities": []}, {"text": "This will obviously be impossible in a real-time system, but these processes canto some extent be automated.", "labels": [], "entities": []}, {"text": "For example, voicing boundaries can be identified by the weighted combination of various acoustic features (e.g., energy, zero-crossing rate), and formants can be identified by the Burg algorithm ( or through simple linear predictive analysis with continuity constraints on the identified resonances between adjacent frames.", "labels": [], "entities": []}, {"text": "Spectral modifications traditionally involve filtering or amplification methods such as spectral subtraction or harmonic filtering), but these are not useful for dealing with more serious mispronounciations (e.g., /t/ for /n/).", "labels": [], "entities": [{"text": "Spectral modifications", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9046911299228668}]}, {"text": "showed that Gaussian mixture mapping can be used to transform audio from one set of spectral acoustic features to another.", "labels": [], "entities": []}, {"text": "During analysis, context-independent frames of speech are analyzed for bark-scaled energy and their 24 th order cepstral coefficients.", "labels": [], "entities": []}, {"text": "For synthesis, a cepstral analysis approximates the original spectrum, and a high-order linear predictive filter is applied to each frame, and excited by impulses or white noise (for voiced and unvoiced segments).", "labels": [], "entities": []}, {"text": "showed that given 99% human accuracy in recognizing normal speech data, this method of reconstruction gave 93% accuracy on the same data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9889740943908691}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9989420771598816}]}, {"text": "A probabilistic least-squares regression mapped the source features x onto the target (regular speaker) features y, producing the model W d (x) + b d for each class, and a simple spectral distortion is performed to produce regularized versions of dysarthic speech\u02c6yspeech\u02c6 speech\u02c6y: for posterior probabilities h d (x).", "labels": [], "entities": []}, {"text": "This model is interesting in that it explicitly maps the acoustic differences for different features between disordered and regular speech . Reconstructing the dysarthric spectrum in this way to sound more 'typical' while leaving pitch (F 0 ), timing, and energy characteristics intact resulted in a 59.4% relative error rate reduction (68% to 87% accuracy) among a group of 18 naive human listeners each of whom annotated a total of 206 dysarthric test words ().", "labels": [], "entities": [{"text": "pitch (F 0 )", "start_pos": 230, "end_pos": 242, "type": "METRIC", "confidence": 0.8531561613082885}, {"text": "timing", "start_pos": 244, "end_pos": 250, "type": "METRIC", "confidence": 0.8148632645606995}, {"text": "relative error rate reduction", "start_pos": 306, "end_pos": 335, "type": "METRIC", "confidence": 0.857039138674736}, {"text": "accuracy", "start_pos": 348, "end_pos": 356, "type": "METRIC", "confidence": 0.9976468682289124}]}], "datasetContent": [{"text": "The intelligibility of both purely synthetic and modified speech signals can be measured objectively by simply having a set of participants transcribe what they hear from a selection of word, phrase, or sentence prompts (), although no single standard has emerged as pre-eminent.", "labels": [], "entities": []}, {"text": "suggested that orthographic transcriptions provide a more accurate predictor of intelligibility among dysarthric speakers than the more subjective estimates used in clinical settings, e.g.,.", "labels": [], "entities": []}, {"text": "That study had 80 listeners who transcribed audio (which is an atypically large group for this task) and showed that intelligibility increased from 61.9% given only acoustic stimuli to 66.75% given audiovisual stimuli on the transcription task in normal speech.", "labels": [], "entities": []}, {"text": "In the current work, we modify only the acoustics of dysarthric speech; however future work might consider how to prompt listeners in a more multimodal context.", "labels": [], "entities": []}, {"text": "In order to gauge the intelligibility of our modifications, we designed a simple experiment in which human listeners attempt to identify words in sentence-level utterances under a number of acoustic scenarios.", "labels": [], "entities": []}, {"text": "Sentences are either uttered by a speaker with dysarthria, modified from their original source acoustics, or manufactured by a text-to-speech synthesizer.", "labels": [], "entities": []}, {"text": "Each participant is seated at a personal computer with a simple graphical user interface with a button which plays or replays the audio (up to 5 times), a text box in which to write responses, and a second button to submit those responses.", "labels": [], "entities": []}, {"text": "Audio is played over a pair of headphones.", "labels": [], "entities": []}, {"text": "The participants are told to only transcribe the words with which they are reasonably confident and to ignore those that they cannot discern.", "labels": [], "entities": []}, {"text": "They are also informed that the sentences are grammatically correct but not necessarily semantically coherent, and that there is no profanity.", "labels": [], "entities": []}, {"text": "Each participant listens to 20 sentences selected at random with the constraints that at least two utterances are taken from each category of audio, described below, and that at least five utterances are also provided to another listener, in order to evaluate inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Participants are self-selected to have no extensive prior experience in speaking with individuals with dysarthria, in order to reflect the general population.", "labels": [], "entities": []}, {"text": "Although dysarthric utterances are likely to be contextualized within meaningful conversations in real-world situations, such pragmatic aspects of discourse are not considered herein order to concentrate on acoustic effects alone.", "labels": [], "entities": []}, {"text": "No cues as to the topic or semantic context of the sentences are given, as there is no evidence that such aids to comprehension affect intelligibility (.", "labels": [], "entities": []}, {"text": "In this study we use sentence-level utterances uttered by male speakers from the TORGO database.", "labels": [], "entities": [{"text": "TORGO database", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.943355530500412}]}, {"text": "Baseline performance is measured on the original dysarthric speech.", "labels": [], "entities": []}, {"text": "Two other systems are used for reference: Synthetic Word sequences are produced by the Cepstral commercial text-to-speech system using the U.S. English voice 'David'.", "labels": [], "entities": []}, {"text": "This system is based on Festival in almost every respect, including its use of linguistic pre-processing (e.g., part-of-speech tagging) and rule-based generation.", "labels": [], "entities": [{"text": "part-of-speech tagging)", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.7753288547197977}, {"text": "rule-based generation", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.7087652087211609}]}, {"text": "This approach has the advantage that every aspect of the synthesized speech (e.g., the word sequence) can be controlled although here, as in practice, synthesized speech will not mimic the user's own acoustic patterns, and will often sound more 'mechanical' due to artificial prosody).", "labels": [], "entities": []}, {"text": "GMM This system uses the Gaussian mixture mapping type of modification suggested by Toda, Black, and and.", "labels": [], "entities": [{"text": "GMM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9687539339065552}]}, {"text": "Here, we use the FestVox implementation of this algorithm, which includes pitch extraction, some phonological knowledge, and a method for resynthesis.", "labels": [], "entities": [{"text": "pitch extraction", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.6951109915971756}]}, {"text": "Parameters for this model are trained by the FestVox system using a standard expectationmaximization approach with 24 th -order cepstral coefficients and 4 Gaussian components.", "labels": [], "entities": [{"text": "FestVox", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9456185698509216}]}, {"text": "The training set consists of all vowels uttered by a male speaker in the TORGO database and their synthetic realizations produced by the method above.", "labels": [], "entities": [{"text": "TORGO database", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9129761457443237}]}, {"text": "Performance is evaluated on the three other acoustic transformations, namely those described in sections 3.2, 3.3, and 3.4 above.", "labels": [], "entities": []}, {"text": "respectively show the percentage of words and phonemes correctly identified by each listener relative to the expected word sequence under each acoustic condition.", "labels": [], "entities": []}, {"text": "In each case, annotator transcriptions were aligned with the 'true' or expected sequences using the Levenshtein algorithm described in section 3.", "labels": [], "entities": []}, {"text": "Plural forms of singular words, for example, are considered incorrect in word alignment although one obvious spelling mistake (i.e., 'skilfully') is corrected before evaluation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.7533572018146515}]}, {"text": "Words are split into component phonemes according to the CMU dictionary, with words having multiple pronunciations given the first decomposition therein.", "labels": [], "entities": [{"text": "CMU dictionary", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9341832101345062}]}, {"text": "In these experiments there is not enough data from which to make definitive claims of statistical significance, but it is clear that the purely synthetic speech has afar greater intelligibility than other approaches, more than doubling the average accuracy of the  TORGOMorph modifications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9988106489181519}]}, {"text": "The GMM transformation method proposed by gave poor performance, although our experiments are distinguished from theirs in that our formant traces are detected automatically, rather than by hand.", "labels": [], "entities": [{"text": "GMM transformation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6676341742277145}]}, {"text": "The relative success of the synthetic approach is not an argument against the type of modifications proposed here and by, since our aim is to avoid the use of impersonal and invariant utterances.", "labels": [], "entities": []}, {"text": "Indeed, future study in this area should incorporate subjective measures of 'naturalness'.", "labels": [], "entities": []}, {"text": "Further uses of acoustic modifications not attainable by text-tospeech synthesis are discussed in section 5.", "labels": [], "entities": []}, {"text": "In all cases, the splicing technique of removing accidentally inserted phonemes and inserting missing ones gives the highest intelligibility relative to all acoustic transformation methods.", "labels": [], "entities": []}, {"text": "Although more study is required, this result emphasizes the importance of lexically correct phoneme sequences.", "labels": [], "entities": []}, {"text": "In the word-recognition experiment, there are an average of 5.2 substitution errors per sentence in the unmodified dysarthric speech against 2.75 in the synthetic speech.", "labels": [], "entities": []}, {"text": "There are also 2.6 substitution errors on average per sentence for the speech modified in frequency, but 3.1 deletion errors, on average, against 0.24 in synthetic speech.", "labels": [], "entities": []}, {"text": "No correlation is found be-18 tween the 'loudness' of the speech (determined by the overall energy in the sonorants) and intelligibility results, although this might change with the acquisition of more data., for instance, found that loud or amplified speech from individuals with Parkinson's disease was more intelligible to human listeners than quieter speech.", "labels": [], "entities": []}, {"text": "Our results are comparable in many respects to the experiments of, although they only looked at simple consonant-vowel-consonant stimuli.", "labels": [], "entities": []}, {"text": "Their results showed an average of 92% correct synthetic vowel recognition (compared with 94.2% phoneme recognition in table 2) and 48% correct dysarthric vowel recognition (compared with 52.9% in table 2).", "labels": [], "entities": [{"text": "correct synthetic vowel recognition", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.7705326974391937}, {"text": "correct dysarthric vowel recognition", "start_pos": 136, "end_pos": 172, "type": "TASK", "confidence": 0.8438139706850052}]}, {"text": "Our results, however, show that modified timing and modified frequencies do not actually benefit intelligibility in either the word or phoneme cases.", "labels": [], "entities": [{"text": "timing", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9717584848403931}]}, {"text": "This disparity may in part be due to the fact that our stimuli are much more complex (quicker sentences do not necessarily improve intelligibility).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percentage of words correctly identified by each  listener (L0*) relative to the expected sequence. Sections  3.2, 3.3, and 3.4 discuss the 'Splice', 'Time', and 'Freq.'  techniques, respectively.", "labels": [], "entities": [{"text": "Freq.", "start_pos": 173, "end_pos": 178, "type": "METRIC", "confidence": 0.9296027719974518}]}, {"text": " Table 2: Percentage of phonemes correctly identified by  each listener relative to the expected sequence. Sections  3.2, 3.3, and 3.4 discuss the 'Splice', 'Time', and 'Freq.'  techniques, respectively.", "labels": [], "entities": [{"text": "Freq.", "start_pos": 170, "end_pos": 175, "type": "METRIC", "confidence": 0.9333324730396271}]}]}