{"title": [{"text": "Learning with Lookahead: Can History-Based Models Rival Globally Optimized Models?", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper shows that the performance of history-based models can be significantly improved by performing lookahead in the state space when making each classification decision.", "labels": [], "entities": []}, {"text": "Instead of simply using the best action output by the classifier, we determine the best action by looking into possible sequences of future actions and evaluating the final states realized by those action sequences.", "labels": [], "entities": []}, {"text": "We present a perceptron-based parameter optimization method for this learning framework and show its convergence properties.", "labels": [], "entities": []}, {"text": "The proposed framework is evaluated on part-of-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6859495043754578}, {"text": "named entity recognition", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.6264431476593018}, {"text": "dependency parsing", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7200212776660919}]}, {"text": "Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons.", "labels": [], "entities": []}], "introductionContent": [{"text": "History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing).", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.6615863680839539}, {"text": "named entity recognition", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.6128616233666738}, {"text": "syntactic parsing", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7124283015727997}]}, {"text": "The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features.", "labels": [], "entities": []}, {"text": "Although history-based models have many practical merits, their accuracy is often surpassed by globally optimized models such as CRFs () and structured perceptrons, mainly due to the label bias problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9992325305938721}]}, {"text": "Today, vanilla history-based models such as maximum entropy Markov models (MEMMs) are probably not the first choice for those who are looking fora machine learning model that can deliver the state-ofthe-art accuracy for their NLP task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9983476400375366}]}, {"text": "Globally optimized models, by contrast, are gaining popularity in the community despite their relatively high computational cost.", "labels": [], "entities": []}, {"text": "In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking process.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9978017210960388}]}, {"text": "It should be emphasized that we use the word \"lookahead\" differently from some literature on syntactic parsing in which lookahead simply means looking at the succeeding words to choose the right parsing actions.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7190630584955215}]}, {"text": "In this paper, we use the word to refer to the process of choosing the best action by considering different sequences of future actions and evaluating the structures realized by those sequences.", "labels": [], "entities": []}, {"text": "In other words, we introduce a lookahead mechanism that performs a search in the space of future actions.", "labels": [], "entities": []}, {"text": "We present a perceptron-based training algorithm that can work with the lookahead process, together with a proof of convergence.", "labels": [], "entities": []}, {"text": "The algorithm enables us to tune the weight of the perceptron in such away that we can correctly choose the right action for the State Operation reduce L saw(I) a dog with eyebrows . .", "labels": [], "entities": []}, {"text": "To answer the question of whether the historybased models enhanced with lookahead can actually compete with globally optimized models, we evaluate the proposed framework with a range of standard NLP tasks, namely, POS tagging, text chunking (a.k.a. shallow parsing), named entity recognition, and dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 214, "end_pos": 225, "type": "TASK", "confidence": 0.8464640080928802}, {"text": "text chunking (a.k.a. shallow parsing)", "start_pos": 227, "end_pos": 265, "type": "TASK", "confidence": 0.6857610046863556}, {"text": "named entity recognition", "start_pos": 267, "end_pos": 291, "type": "TASK", "confidence": 0.648186981678009}, {"text": "dependency parsing", "start_pos": 297, "end_pos": 315, "type": "TASK", "confidence": 0.8420444130897522}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the idea of lookahead with a motivating example from dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8473357856273651}]}, {"text": "Section 3 describes our search algorithm for lookahead and a perceptron-based training algorithm.", "labels": [], "entities": []}, {"text": "Experimental results on POS tagging, chunking, named entity recognition, and dependency parsing are presented in Section 4.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.8765941262245178}, {"text": "chunking", "start_pos": 37, "end_pos": 45, "type": "TASK", "confidence": 0.9679457545280457}, {"text": "named entity recognition", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.6094342470169067}, {"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7854987680912018}]}, {"text": "We discuss relationships between our approach and some related work in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 offers concluding remarks with some potential research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents four sets of experimental results to show how the lookahead process improves the accuracy of history-based models in common NLP tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9979827404022217}]}], "tableCaptions": [{"text": " Table 1: Performance of English POS tagging (training times and accuracy scores on test data)", "labels": [], "entities": [{"text": "English POS tagging", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.5674971044063568}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9983110427856445}]}, {"text": " Table 2: Performance of text chunking (training times and accuracy scores on test data).", "labels": [], "entities": [{"text": "text chunking", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.7756621241569519}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9977872371673584}]}, {"text": " Table 3: Performance of biomedical named entity recognition (training times and accuracy scores on test data).", "labels": [], "entities": [{"text": "biomedical named entity recognition", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.6137955114245415}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9982149600982666}]}]}