{"title": [{"text": "Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9744443893432617}, {"text": "WMT11 System Combination", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.4776548345883687}]}], "abstractContent": [{"text": "BBN submitted system combination outputs for Czech-English, German-English, Spanish-English, and French-English language pairs.", "labels": [], "entities": [{"text": "BBN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.965431809425354}]}, {"text": "All combinations were based on confusion network decoding.", "labels": [], "entities": []}, {"text": "The confusion networks were built using incremental hypothesis alignment algorithm with flexible matching.", "labels": [], "entities": []}, {"text": "A novel bi-gram count feature, which can penalize bi-grams not present in the input hypotheses corresponding to a source sentence, was introduced in addition to the usual decoder features.", "labels": [], "entities": []}, {"text": "The system combination weights were tuned using a graph based expected BLEU as the objective function while incre-mentally expanding the networks to bi-gram and 5-gram contexts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9984245300292969}]}, {"text": "The expected BLEU tuning described in this paper naturally generalizes to hypergraphs and can be used to optimize thousands of weights.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9986823201179504}]}, {"text": "The combination gained about 0.5-4.0 BLEU points over the best individual systems on the official WMT11 language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9996123909950256}, {"text": "WMT11 language pairs", "start_pos": 98, "end_pos": 118, "type": "DATASET", "confidence": 0.9230354229609171}]}, {"text": "A 39 system multi-source combination achieved an 11.1 BLEU point gain.", "labels": [], "entities": [{"text": "BLEU point gain", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.971011221408844}]}], "introductionContent": [{"text": "The confusion networks for the BBN submissions to the WMT11 system combination task were built using incremental hypothesis alignment algorithm * This work was supported by DARPA/I2O Contract No. HR0011-06-C-0022 under the GALE program (Approved for Public Release, Distribution Unlimited).", "labels": [], "entities": [{"text": "WMT11 system combination task", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.6946813613176346}, {"text": "DARPA/I2O Contract No. HR0011-06-C-0022", "start_pos": 173, "end_pos": 212, "type": "DATASET", "confidence": 0.6383265852928162}]}, {"text": "The views, opinions, and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of with flexible matching (.", "labels": [], "entities": []}, {"text": "A novel bi-gram count feature was used in addition to the standard decoder features.", "labels": [], "entities": []}, {"text": "The N-best list based expected BLEU tuning (, similar to the one proposed by, was extended to operate on word lattices.", "labels": [], "entities": [{"text": "BLEU tuning", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9758083522319794}]}, {"text": "This method is closely related to the consensus BLEU (CoBLEU) proposed by.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9744028449058533}]}, {"text": "The minimum operation used to compute the clipped counts (matches) in the BLEU score () was replaced by a differentiable function, so there was no need to use sub-gradient ascent as in CoBLEU.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9633101522922516}, {"text": "CoBLEU", "start_pos": 185, "end_pos": 191, "type": "DATASET", "confidence": 0.9134108424186707}]}, {"text": "The expected BLEU (xBLEU) naturally generalizes to hypergraphs by simply replacing the forwardbackward algorithm with inside-outside algorithm when computing the expected n-gram counts and sufficient statistics for the gradient.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9980664849281311}]}, {"text": "The gradient ascent optimization of the xBLEU appears to be more stable than the gradient-free direct 1-best BLEU tuning or N -best list based minimum error rate training, especially when tuning a large number of weights.", "labels": [], "entities": [{"text": "gradient ascent optimization", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8355401357014974}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9309043288230896}, {"text": "N -best list based minimum error rate", "start_pos": 124, "end_pos": 161, "type": "METRIC", "confidence": 0.7261065617203712}]}, {"text": "On the official WMT11 language pairs with up to 30 weights, there was no significant benefit from maximizing xBLEU.", "labels": [], "entities": [{"text": "WMT11 language pairs", "start_pos": 16, "end_pos": 36, "type": "DATASET", "confidence": 0.9045595526695251}]}, {"text": "However, on a 39 system multi-source combination (43 weights total), it yielded a significant gain over gradient-free BLEU tuning and Nbest list based expected BLEU tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.971356213092804}, {"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9778866767883301}]}], "datasetContent": [{"text": "System outputs for all language pairs with English as the target were combined (cz-en, de-en, es-en, and fr-en).", "labels": [], "entities": []}, {"text": "Unpruned English bi-gram and 5-gram language model components were trained using the WMT11 corpora: EuroParl, GigaFrEn, UNDoc Es, UNDoc Fr, NewsCommentary, News2007, News2008, News2009, News2010, and News2011.", "labels": [], "entities": [{"text": "WMT11 corpora", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9517288506031036}, {"text": "EuroParl", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.8946340084075928}, {"text": "UNDoc Fr", "start_pos": 130, "end_pos": 138, "type": "DATASET", "confidence": 0.8123550713062286}, {"text": "NewsCommentary", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.76645427942276}, {"text": "News2007", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.7903032898902893}, {"text": "News2008", "start_pos": 166, "end_pos": 174, "type": "DATASET", "confidence": 0.8546412587165833}, {"text": "News2009", "start_pos": 176, "end_pos": 184, "type": "DATASET", "confidence": 0.9121320247650146}, {"text": "News2010", "start_pos": 186, "end_pos": 194, "type": "DATASET", "confidence": 0.9385575652122498}, {"text": "News2011", "start_pos": 200, "end_pos": 208, "type": "DATASET", "confidence": 0.9838497638702393}]}, {"text": "Additional six Gigaword v4 components included: AFP, APW, XIN+CNA, LTW, NYT, and Headlines+Datelines.", "labels": [], "entities": [{"text": "AFP", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7004199028015137}, {"text": "NYT", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.5555379390716553}]}, {"text": "The total number of words used to train the LMs was about 6.4 billion.", "labels": [], "entities": []}, {"text": "Interpolation weights for the sixteen components were tuned to minimize perplexity on the newstest2010-ref.en development set.", "labels": [], "entities": [{"text": "newstest2010-ref.en development set", "start_pos": 90, "end_pos": 125, "type": "DATASET", "confidence": 0.9247663815816244}]}, {"text": "The modified Kneser-Ney smoothing) was used in training.", "labels": [], "entities": [{"text": "Kneser-Ney smoothing", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.4726283699274063}]}, {"text": "Experiments using a LM trained on the system outputs and interpolated with the general LM were also conducted.", "labels": [], "entities": []}, {"text": "The interpolation weights between 0.1 and 0.9 were tried, and the weight yielding the highest BLEU score on the tuning set was selected.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9812185764312744}]}, {"text": "A tri-gram true casing model was trained on all the LM training data.", "labels": [], "entities": [{"text": "LM training data", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.6534810960292816}]}, {"text": "This model was used to restore the case of the lower-case system combination output.", "labels": [], "entities": []}, {"text": "All twelve 1-best system outputs on cz-en, 26 outputs on de-en, 16 outputs on es-en, and 24 outputs on fr-en were combined.", "labels": [], "entities": []}, {"text": "Three different weight optimization methods were tried.", "labels": [], "entities": [{"text": "weight optimization", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7256942838430405}]}, {"text": "First, lattice based 1-best BLEU optimization of the bi-gram decoding weights followed by N-best list based BLEU optimization of 5-gram re-scoring weights using 300-best lists, both using downhill simplex.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9801043272018433}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9496163725852966}]}, {"text": "Second, N-best list based expected BLEU optimization of the bi-gram and 5-gram weights using 300-best lists with merging between bi-gram decoding iterations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9989878535270691}]}, {"text": "Third, lattice based expected BLEU optimization of bi-gram and 5-gram decoding weights.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9972461462020874}]}, {"text": "The L-BFGS ( algorithm was used in gradient ascent.", "labels": [], "entities": [{"text": "gradient ascent", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.9003952443599701}]}, {"text": "Results for all four single source experiments are shown in, including case insensitive TER () and BLEU scores for the worst and best systems, and the system combination outputs for the three tuning methods.", "labels": [], "entities": [{"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.8016656637191772}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9659921526908875}]}, {"text": "The gains on tuning and test sets were consistent, though relatively smaller on cz-en due to a single system (online-B) dominating the other systems by about 5-6 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9987020492553711}]}, {"text": "The tuning method had very little influence on the test set scores apart from de-en where the lattice BLEU optimization yields slightly lower BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9944565296173096}, {"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9985950589179993}]}, {"text": "This seems to suggest that the gradient free optimization is not as stable with a larger number of weights.", "labels": [], "entities": []}, {"text": "The novel bi-gram feature did not have significant influence on the TER or BLEU scores, but the number of novel bi-grams was reduced by up to 100%.", "labels": [], "entities": [{"text": "TER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9979640245437622}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9808961153030396}]}, {"text": "Finally, experiments combining 39 system outputs by taking the top half of the outputs from each language pair were performed.", "labels": [], "entities": []}, {"text": "The selection was based on case insensitive BLEU scores on the tuning set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9878379106521606}]}, {"text": "nations using the three tuning methods with or without the system output biased LM, and finally without the novel bi-gram count feature.", "labels": [], "entities": []}, {"text": "There is a clear advantage from the expected BLEU tuning on the tuning set, and lattice tuning yields better scores than N-best list based tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9986876845359802}]}, {"text": "The difference between latBLEU and nbExpBLEU without biasLM is not quite as large on the test set but latExpBLEU yields significant gains over both.", "labels": [], "entities": [{"text": "biasLM", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9743412733078003}]}, {"text": "The biasLM also yields significant gains on all but latBLEU tuning.", "labels": [], "entities": [{"text": "biasLM", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9906512498855591}]}, {"text": "Finally, removing the novel bi-gram count feature results in a significant loss, probably due to the large number of input hypotheses.", "labels": [], "entities": []}, {"text": "The number of novel bi-grams in the test set output was reduced to zero when using this feature.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Case insensitive TER and BLEU scores on newssyscombtune (tune) and newssyscombtest (test)  for combinations of outputs from four source languages. Three tuning methods were used: lattice BLEU (latBLEU),  N-best list based expected BLEU (nbExpBLEU), and lattice expected BLEU (latExpBLEU).", "labels": [], "entities": [{"text": "TER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.7913188338279724}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9840990304946899}, {"text": "BLEU", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9074960947036743}, {"text": "N-best list based expected BLEU", "start_pos": 214, "end_pos": 245, "type": "METRIC", "confidence": 0.5314420938491822}, {"text": "BLEU", "start_pos": 280, "end_pos": 284, "type": "METRIC", "confidence": 0.506542444229126}]}]}