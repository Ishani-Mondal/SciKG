{"title": [{"text": "Detecting Forum Authority Claims in Online Discussions", "labels": [], "entities": [{"text": "Detecting Forum Authority Claims in Online Discussions", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8458200011934552}]}], "abstractContent": [{"text": "This paper explores the problem of detecting sentence-level forum authority claims in on-line discussions.", "labels": [], "entities": [{"text": "detecting sentence-level forum authority claims in on-line discussions", "start_pos": 35, "end_pos": 105, "type": "TASK", "confidence": 0.7510262355208397}]}, {"text": "Using a maximum entropy model, we explore a variety of strategies for extracting lexical features in a sparse training scenario, comparing knowledge-and data-driven methods (and combinations).", "labels": [], "entities": []}, {"text": "The augmentation of lexical features with parse context is also investigated.", "labels": [], "entities": []}, {"text": "We find that certain markup features perform remarkably well alone, but are outperformed by data-driven selection of lexical features augmented with parse context.", "labels": [], "entities": []}], "introductionContent": [{"text": "In multi-party discussions, language is used to establish identity, status, authority and connections with others in addition to communicating information and opinions.", "labels": [], "entities": []}, {"text": "Automatically extracting this type of social information in language from discussions is useful for understanding group interactions and relationships.", "labels": [], "entities": []}, {"text": "The aspect of social communication most explored so far is the detection of participant role, particularly in spoken genres such as broadcast news, broadcast conversations, and meetings.", "labels": [], "entities": []}, {"text": "Several studies have explored different types of features (lexical, prosodic, and turn-taking) in a variety of statistical modeling frameworks (;;.", "labels": [], "entities": []}, {"text": "Typically, these studies assume that a speaker inhabits a role for the duration of the discussion, so multiple turns contribute to the decision.", "labels": [], "entities": []}, {"text": "Participant status is similar although the language of others is often more relevant than that of the participant in question.", "labels": [], "entities": []}, {"text": "Communication of other types of social information can be more localized.", "labels": [], "entities": []}, {"text": "For example, an attempt to establish authority frequently occurs within a single sentence or turn when entering a discussion, though authority bids may involve multiple turns when the participant is challenged.", "labels": [], "entities": []}, {"text": "Similarly, discussion participants may align with or distance themselves from other participants with a single statement, or someone could agree with one person at a particular point in the conversation and disagree with them at a different point.", "labels": [], "entities": []}, {"text": "Such localized phenomena are also important for understanding the broader context of that participant's influence or role in the conversation.", "labels": [], "entities": []}, {"text": "In this paper, we focus on a particular type of authority claim, namely forum claims, as defined in a companion paper).", "labels": [], "entities": []}, {"text": "Forum claims are based on policy, norms, or contextual rules of behavior in the interaction.", "labels": [], "entities": []}, {"text": "In our experiments, we explore the phenomenon using Wikipedia discussion (\"talk\") pages, which are discussions associated with a Wikipedia article in which changes to the article are debated by the editors in a series of discussion threads.", "labels": [], "entities": []}, {"text": "Examples of such forum claims are: \u2022 I do think my understanding of Wikipedia and policy is better than yours.", "labels": [], "entities": []}, {"text": "\u2022 So it has all those things going for it, and \u2022 Folks, please be specific and accurate when you [[WP:CITE-cite your sources]].", "labels": [], "entities": [{"text": "accurate", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9851654767990112}]}, {"text": "We treat each discussion thread as a unique \"conversation\".", "labels": [], "entities": []}, {"text": "Each contiguous change to a conversation is treated as a unique \"post\" or turn.", "labels": [], "entities": []}, {"text": "The dataset and annotation scheme are described in more detail in the companion paper.", "labels": [], "entities": []}, {"text": "Related previous work on a similar task focused on detecting attempts to establish topic expertise in Wikipedia discussions).", "labels": [], "entities": [{"text": "detecting attempts to establish topic expertise in Wikipedia discussions", "start_pos": 51, "end_pos": 123, "type": "TASK", "confidence": 0.638635492987103}]}, {"text": "Their work used a different annotation process than that which we build on here.", "labels": [], "entities": []}, {"text": "In particular, the annotation was performed at the discussion participant level, with evidence marked at the turn level without distinguishing the different types of claims as in.", "labels": [], "entities": []}, {"text": "Treating the problem of detecting forum claims as a sentence-level classification problem is similar to other natural language processing tasks, such as sentiment classification.", "labels": [], "entities": [{"text": "sentence-level classification", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.7041875123977661}, {"text": "sentiment classification", "start_pos": 153, "end_pos": 177, "type": "TASK", "confidence": 0.9464074671268463}]}, {"text": "Early work in sentiment analysis used unigram features ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.967327207326889}]}, {"text": "However, error analyses suggested that highly accurate sentiment classification requires deeper understanding of the text, or at least higher order n-gram features.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.9416232407093048}]}, {"text": "used unigrams, bigrams, and trigrams for extracting the polarity of online reviews.", "labels": [], "entities": [{"text": "extracting the polarity of online reviews", "start_pos": 41, "end_pos": 82, "type": "TASK", "confidence": 0.8599009414513906}]}, {"text": "employed weighted n-grams together with additional features to classify blog comments based on agreement polarity.", "labels": [], "entities": []}, {"text": "We conjecture that authority claim detection will also benefit from moving beyond unigram features.", "labels": [], "entities": [{"text": "authority claim detection", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.7386637528737386}]}, {"text": "The focus of the paper is on two questions in feature extraction: \u2022 Can we exploit domain knowledge to address overtraining issues in sparse data conditions?", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7459406852722168}]}, {"text": "\u2022 Is parse context more effective than n-gram context?", "labels": [], "entities": []}, {"text": "Our experiments compare the performance obtained using multiple methods for incorporating linguisticor data-driven knowledge and context into the feature space, relative to the baseline n-gram features.", "labels": [], "entities": []}, {"text": "Section 2 describes the general classification architecture.", "labels": [], "entities": []}, {"text": "Section 3 describes the various features implemented.", "labels": [], "entities": []}, {"text": "Experimental results are presented in section 4.", "labels": [], "entities": []}, {"text": "We conclude with some analysis in section 5 and remarks on future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use data from the Authority and Alignment in Wikipedia Discussions (AAWD) corpus described in our companion paper).", "labels": [], "entities": [{"text": "Authority and Alignment in Wikipedia Discussions (AAWD)", "start_pos": 21, "end_pos": 76, "type": "TASK", "confidence": 0.784478035238054}]}, {"text": "The dataset contains English Wikipedia discussions annotated with authority claims by four annotators.", "labels": [], "entities": []}, {"text": "Not all the discussions are annotated by multiple annotators.", "labels": [], "entities": []}, {"text": "Thereby in the train/dev/eval split, we select most of the discussions that are multiply annotated for the dev and eval sets.", "labels": [], "entities": []}, {"text": "We tune the number of features selected by the mutual information between a feature and the class labels, which is a common approach applied in text categorization).", "labels": [], "entities": []}, {"text": "Feature selection and parameter tuning of the decision threshold \u03b8 are performed independently for each condition.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7492723166942596}]}, {"text": "We include the number of features selected in each case alongside the results.", "labels": [], "entities": []}, {"text": "The performance of the various systems described in this paper is evaluated using F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9933004379272461}]}, {"text": "The numbers corresponding to the overall best performance obtained on the dev and eval sets are highlighted in boldface in the appropriate table.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: N-gram feature results", "labels": [], "entities": []}, {"text": " Table 5: Smart word feature results", "labels": [], "entities": []}, {"text": " Table 6: Parse-augmented feature results", "labels": [], "entities": []}, {"text": " Table 7: Parse feature examples", "labels": [], "entities": []}, {"text": " Table 9: Combined feature results", "labels": [], "entities": []}, {"text": " Table 10: Precision and recall of the rule-based system", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.990592896938324}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9986122846603394}]}]}