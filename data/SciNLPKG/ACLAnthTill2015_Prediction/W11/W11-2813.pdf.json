{"title": [{"text": "Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with human subjects", "labels": [], "entities": [{"text": "Adaptive Information Presentation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.789172093073527}]}], "abstractContent": [{"text": "We present evaluation results with human subjects fora novel data-driven approach to Natural Language Generation in spoken dialogue systems.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.6697568396727244}]}, {"text": "We evaluate a trained Information Presentation (IP) strategy in a deployed tourist-information spoken dialogue system.", "labels": [], "entities": []}, {"text": "The IP problem is formulated as statistical decision making under uncertainty using Reinforcement Learning, where both content planning and attribute selection are jointly opti-mised based on data collected in a Wizard-of-Oz study.", "labels": [], "entities": [{"text": "content planning", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.6979911029338837}]}, {"text": "After earlier work testing and training this model in simulation, we now present results from an extensive online user study, involving 131 users and more than 800 test dialogues, which explores its contribution to overall 'global' task success.", "labels": [], "entities": []}, {"text": "We find that the trained Information Presentation strategy significantly improves dialogue task completion , with up to a 9.7% increase (30% relative) compared to the deployed dialogue system which uses conventional, hand-coded presentation prompts.", "labels": [], "entities": [{"text": "dialogue task completion", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.8655457099278768}]}, {"text": "We also present subjective evaluation results and discuss the implications of these results for future work in dialogue management and NLG.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.8413977026939392}]}], "introductionContent": [{"text": "Natural Language Generation (NLG) for Spoken Dialogue Systems serves two goals.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7518345564603806}]}, {"text": "On the one hand the \"local\" NLG task is to present \"enough\" information to the user (for example helping them to feel confident that they have a good overview of the search results) while keeping the utterances short and understandable.", "labels": [], "entities": []}, {"text": "On the other hand, better Information Presentation should also contribute to the \"global/ overall\" dialogue task, so as to maximise task completion.", "labels": [], "entities": [{"text": "Information Presentation", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.700055718421936}]}, {"text": "We have developed a novel framework for adaptive Natural Language Generation (NLG) where the problem is formulated as incremental decision making under uncertainty, which can be approached using Reinforcement Learning).This model is also being explored by other researchers and.", "labels": [], "entities": [{"text": "adaptive Natural Language Generation (NLG)", "start_pos": 40, "end_pos": 82, "type": "TASK", "confidence": 0.7914184757641384}]}, {"text": "We have applied the theory to a variety of NLG problems, such as referring expression generation, and here we focus on adaptive Information Presentation (IP) in spoken dialogue.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.7993095517158508}, {"text": "adaptive Information Presentation (IP) in spoken dialogue", "start_pos": 119, "end_pos": 176, "type": "TASK", "confidence": 0.7282554143004947}]}, {"text": "The IP model is adaptive to noisy feedback from the current generation context (e.g. a user, a surface realiser, and a TTS engine), and it incrementally adapts the IP policy at the turn level.", "labels": [], "entities": []}, {"text": "Reinforcement Learning is used to automatically optimise the IP policy with respect to a data-driven objective function.", "labels": [], "entities": []}, {"text": "In previous simulation-based work, we demonstrated that this IP model \"locally\" outperforms other IP strategies as used by conventional dialogue systems ( , as well as a more elaborate IP baseline strategy mimicking human \"wizard\" IP behaviour ().", "labels": [], "entities": []}, {"text": "We have now integrated this policy into a full online dialogue system using Voice Over IP (VoIP), and evaluated its performance with real users.", "labels": [], "entities": []}, {"text": "In particular, we test its ability to contribute to overall dialogue task success.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly review the NLG framework as planning under uncertainty and how it was tested and trained in simulation.", "labels": [], "entities": [{"text": "NLG framework", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.7906214594841003}]}, {"text": "Section 3 explains how this trained policy was integrated into a fully working spoken dialogue system.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental setup.", "labels": [], "entities": []}, {"text": "In Section 5 we present the results, and in Section 6 we conclude with a discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation of the two systems, two approaches to managing subjects were taken.", "labels": [], "entities": []}, {"text": "In the first approach, subjects were recruited using mailshots and web-based advertising amongst people from Cambridge and Edinburgh, mostly students.", "labels": [], "entities": []}, {"text": "From the resulting pool of subjects, people were gradually invited to start the tasks, in their own time, and within a given trial period of around two weeks.", "labels": [], "entities": []}, {"text": "After the trial period, they were paid (using PayPal) per completed task, with a required minimum of 15 tasks, and a maximum of 40 tasks.", "labels": [], "entities": []}, {"text": "For the two systems, this resulted in a corpus of 304 dialogues.", "labels": [], "entities": []}, {"text": "In the second approach, an alternative method of managing subjects was used, using Amazon Mechanical Turk).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.928929845492045}]}, {"text": "In this setup, tasks are published as so-called HITs (Human Intelligence Tasks) on a web-server and registered workers can complete them.", "labels": [], "entities": []}, {"text": "This setup resulted in 532 collected dialogues for the two systems compared 1 . In the remainder of this paper, we will refer to the corpus obtained with 'locally' managed subjects as Feb11-LOC and to the corpus obtained using Amazon Mechanical Turk as Feb11-AMT.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 227, "end_pos": 249, "type": "DATASET", "confidence": 0.9342253605524699}]}, {"text": "In both of the above-mentioned approaches, the subjects were directed to a webpage with detailed instructions and for each task, a phone number to call and the scenario to follow.", "labels": [], "entities": []}, {"text": "The subjects were randomly assigned to interact with one of the systems (BASE or TIP).", "labels": [], "entities": [{"text": "BASE", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9913177490234375}]}, {"text": "A scenario describes a place to eat in town, with some constraints, for example: \"You want to find a moderately priced restaurant and it should be in the Riverside area.", "labels": [], "entities": []}, {"text": "You want to know the address, phone number, and type of food.\".", "labels": [], "entities": []}, {"text": "After the dialogue, the subjects were asked to fill in a short questionnaire, assessing the impact of IP strategies on the users' perception of various system components: summarises the two corpora of collected data.", "labels": [], "entities": []}, {"text": "For the Feb11-AMT corpus, considerably more subjects were used, although many of them did only a small number of tasks.", "labels": [], "entities": [{"text": "Feb11-AMT corpus", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9777465760707855}]}, {"text": "For the Feb11-LOC corpus, it was more difficult to recruit many subjects, but in this setup, the subjects could be asked to complete a minimum number of tasks, hence the higher average number of dialogues per user.", "labels": [], "entities": [{"text": "Feb11-LOC corpus", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9513970017433167}]}, {"text": "Also note, that the Word Error Rate (WER) is relatively high in both corpora.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.8908235629399618}]}, {"text": "This is partly due to the fact that the ASR module had not been trained properly for this particular domain due to lack of training data.", "labels": [], "entities": [{"text": "ASR", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.969931423664093}]}, {"text": "Furthermore, some of the subjects were non-native speakers and some subjects used Skype to call the systems, which causes distortion of the audio signal.", "labels": [], "entities": []}, {"text": "These conditions are the same for both BASE and TIP systems.", "labels": [], "entities": [{"text": "BASE", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.35186564922332764}]}, {"text": "Despite the high ASR error rates, overall task completion rates were high, due to the robustness of the POMDP dialogue manager.", "labels": [], "entities": [{"text": "ASR", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9243815541267395}, {"text": "POMDP dialogue manager", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.6032809019088745}]}, {"text": "The overall most frequently employed IP strategy is SUMMARY(2)+RECOMMEND(2), see.", "labels": [], "entities": [{"text": "RECOMMEND", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.7773414254188538}]}, {"text": "Also, note that the trained policy never employed more than 3 attributes, and always chose to use the same number of attributes for its combined IP strategies.", "labels": [], "entities": []}, {"text": "Strategy(Attributes) 1  For the objective evaluation of the two dialogue systems we focused on measuring goal completion rates, which can be done in different ways.", "labels": [], "entities": []}, {"text": "First, we can take the goal specification assigned to the user for each dialogue and then analyse the system dialogue acts.", "labels": [], "entities": []}, {"text": "Partial completion (ObjSucc-PC) is achieved when the system has offered avenue that matches the constraints as specified in the assigned goal, for example it has provided the name of a cheap chinese restaurant in the riverside area.", "labels": [], "entities": [{"text": "ObjSucc-PC)", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.8925355970859528}]}, {"text": "Full completion (ObSucc-FC) is achieved when the system has also provided the required additional information about that venue, for example the phone number and address.", "labels": [], "entities": [{"text": "completion", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.9734430909156799}, {"text": "ObSucc-FC)", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9124188721179962}]}, {"text": "In, all success rates obtained from the February 2011 evaluation are given, for the corpus  Overview of all success rates (%) obtained for the two corpora, including subjective success obtained from Q1 of the user questionnaire(SubjSucc), objective success based on assigned goals (ObjSucc-PC for partial completion and ObjSucc-FC for full completion).", "labels": [], "entities": []}, {"text": "95% confidence intervals for all success rates are indicated in brackets; statistically significant improvements (p < 0.05 using a z-test) are indicated with an asterisk (*).", "labels": [], "entities": []}, {"text": "Also given are the number of dialogues (nDials) and dialogue length in terms of the average number of user turns per dialogue(nTurns). with data from locally recruited subjects (Feb11-LOC), and the corpus with data from Amazon Mechanical Turk workers, as well as both corpora pooled together (Feb11-TOT).", "labels": [], "entities": []}, {"text": "The results show that the system with our NLG component (TIP) outperforms the baseline system (BASE) on all objective success rates in both corpora.", "labels": [], "entities": [{"text": "BASE", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9841267466545105}]}, {"text": "Relative improvements of up to 30% for full completion on the Feb11-AMT corpus were obtained.", "labels": [], "entities": [{"text": "Feb11-AMT corpus", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9721727073192596}]}, {"text": "After pooling the two corpora together, we have a sufficient number of dialogues to show that the improvement from our NLG strategy is statistically significant on both partial and full completion (using a 2-tailed z-test for two proportions).", "labels": [], "entities": []}, {"text": "It is also interesting to note that the average number of user turns per dialogue is not significantly different between systems in both corpora, suggesting that the contribution of the trained IP policy to system performance manifests itself primarily in terms of effectiveness rather than efficiency.", "labels": [], "entities": []}, {"text": "By providing more useful information to the user, the system might help them to find an appropriate venue in fewer turns, but due to the lengthy system prompts, more turns might be needed to recover from speech recognition errors (see WER in).", "labels": [], "entities": [{"text": "WER", "start_pos": 235, "end_pos": 238, "type": "METRIC", "confidence": 0.8131694197654724}]}, {"text": "summarises the subjective user scores from the questionnaire (see Section 4).", "labels": [], "entities": []}, {"text": "In terms of subjective success rates (Q1), the baseline system (BASE) obtains slightly higher scores on both corpora, although no statistically significant differences were found.", "labels": [], "entities": [{"text": "BASE", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9829853177070618}]}, {"text": "We will further discuss these results in section 6.", "labels": [], "entities": []}, {"text": "When comparing the other subjective scores (Q2-Q4) on a scale of, using a Mann-Whitney, where an asterisk (*) denotes a significant difference at p < 0.05 (using a z-test for Q1 and a Mann-Whitney test for Q2-Q4).", "labels": [], "entities": []}, {"text": "test, the only case where a statistically significant difference is found between the two systems is the score for Q4:VoiceQuality in the Feb11-LOC corpus, where the baseline system is significantly better.", "labels": [], "entities": [{"text": "Feb11-LOC corpus", "start_pos": 138, "end_pos": 154, "type": "DATASET", "confidence": 0.9695781469345093}]}, {"text": "Since the the TTS voice is exactly the same for both systems, the difference in perceived voice quality might be influenced by the longer system prompts for the TIP system.", "labels": [], "entities": [{"text": "TTS voice", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.8265380561351776}]}, {"text": "However, we don't see this pattern in the Feb11-AMT corpus.", "labels": [], "entities": [{"text": "Feb11-AMT corpus", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9751947522163391}]}, {"text": "We also compared the Mechanical Turk setup to the setup where subjects where recruited locally (Feb11-AMT vs. Feb11-LOC for both systems).", "labels": [], "entities": []}, {"text": "For the TIP system, Q2:Understanding and Q3:Phrasing are significantly higher in the Feb11-AMT corpus compared to the FEB11-LOC corpus.", "labels": [], "entities": [{"text": "Phrasing", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.7957852482795715}, {"text": "Feb11-AMT corpus", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.9493427276611328}, {"text": "FEB11-LOC corpus", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.9765948355197906}]}, {"text": "Similarly, the BASE system performs significantly better for Q3:Phrasing under the Mechanical Turk setting.", "labels": [], "entities": [{"text": "BASE", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9475055932998657}, {"text": "Q3:Phrasing", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.49289653698603314}]}, {"text": "However, when combining the results for all the subjective scores (similar to the objective scores), none of the differences are significant.", "labels": [], "entities": []}, {"text": "In sum, there is no difference in user ratings between the original BASE system and the TIP system with the integrated trained NLG strategy, except for Q4:VoiceQuality, which is better rated for the BASE system in the Feb11-LOC corpus, even though the systems had identical TTS.", "labels": [], "entities": [{"text": "Feb11-LOC corpus", "start_pos": 218, "end_pos": 234, "type": "DATASET", "confidence": 0.9447357058525085}]}, {"text": "The difference in ratings between the Feb11-LOC and Feb11-AMT corpora suggests that the way in which subjects are recruited, instructed and payed, as well as the user population targeted, has an impact on subjective ratings obtained.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Overview of collected data, with for each corpus  the number of dialogues (nDials), the average number of  user turns per dialogue (AvgTurns), the number of unique  users (nUsers), the average number of dialogues per user  (nDsUsr), and the word error rate (WER).", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 251, "end_pos": 272, "type": "METRIC", "confidence": 0.8528794050216675}]}, {"text": " Table 5: Subjective evaluation results, based on the ques- tionnaire [Q1-Q4", "labels": [], "entities": []}]}