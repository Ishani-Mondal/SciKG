{"title": [{"text": "Using Query Patterns to Learn the Duration of Events", "labels": [], "entities": [{"text": "Learn the Duration of Events", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7439468026161193}]}], "abstractContent": [{"text": "We present the first approach to learning the durations of events without annotated training data, employing web query patterns to infer duration distributions.", "labels": [], "entities": []}, {"text": "For example, we learn that \"war\" lasts years or decades, while \"look\" lasts seconds or minutes.", "labels": [], "entities": []}, {"text": "Learning aspectual information is an important goal for computational semantics and duration information may help enable rich document understanding.", "labels": [], "entities": []}, {"text": "We first describe and improve a supervised baseline that relies on event duration annotations.", "labels": [], "entities": []}, {"text": "We then show how web queries for linguistic patterns can help learn the duration of events without labeled data, producing fine-grained duration judgments that surpass the supervised system.", "labels": [], "entities": []}, {"text": "We evaluate on the TimeBank duration corpus, and also investigate how an event's participants (arguments) effect its duration using a corpus collected through Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "TimeBank duration corpus", "start_pos": 19, "end_pos": 43, "type": "DATASET", "confidence": 0.7023839155832926}, {"text": "duration", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9515698552131653}]}, {"text": "We make available anew database of events and their duration distributions for use in research involving the temporal and aspectual properties of events.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bridging the gap between lexical knowledge and world knowledge is crucial for achieving natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.6683749357859293}]}, {"text": "For example, knowing whether a nominal is a person or organization and whether a person is male or female substantially improves coreference resolution, even when such knowledge is gathered through noisy unsupervised approaches.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.9591179192066193}]}, {"text": "However, existing algorithms and resources for such semantic knowledge have focused primarily on static properties of nominals (e.g. gender or entity type), not dynamic properties of verbs and events.", "labels": [], "entities": []}, {"text": "This paper shows how to learn one such property: the typical duration of events.", "labels": [], "entities": []}, {"text": "Since an event's duration is highly dependent on context, our algorithm models this aspectual property as a distribution over durations rather than a single mean duration.", "labels": [], "entities": []}, {"text": "For example, a \"war\" typically lasts years, sometimes months, but almost never seconds, while \"look\" typically lasts seconds or minutes, but rarely years or decades.", "labels": [], "entities": []}, {"text": "Our approach uses web queries to model an event's typical distribution in the real world.", "labels": [], "entities": []}, {"text": "Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g.,) in much the same way that gender has benefited nominal coreference systems.", "labels": [], "entities": []}, {"text": "Event durations are also key to building event timelines and other deeper temporal understandings of a text.", "labels": [], "entities": []}, {"text": "The contributions of this work are: \u2022 Demonstrating how to acquire event duration distributions by querying the web with patterns.", "labels": [], "entities": []}, {"text": "\u2022 Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data.", "labels": [], "entities": []}, {"text": "\u2022 Making available an event duration lexicon with duration distributions for common English events.", "labels": [], "entities": []}, {"text": "We first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations.", "labels": [], "entities": [{"text": "predicting event durations", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.8812537988026937}]}, {"text": "Next, we present our approach to learning event distributions based on web counts.", "labels": [], "entities": []}, {"text": "We then evaluate both of these models on an existing annotated corpus of event durations and make comparisons to durations we collected using Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 142, "end_pos": 166, "type": "DATASET", "confidence": 0.9090838134288788}]}, {"text": "Finally, we present a generated database of event durations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We also collected event durations from Amazon's Mechanical Turk (MTurk), an online marketplace from Amazon where requesters can find workers to solve Human Intelligence Tasks (HITs) for small amounts of money.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (MTurk)", "start_pos": 39, "end_pos": 71, "type": "DATASET", "confidence": 0.8428824203354972}]}, {"text": "Prior work has shown that human judgments from MTurk can often be as reliable as trained annotators () or subjects in controlled lab studies, particularly when judgments are aggregated over many MTurk workers (\"Turkers\").", "labels": [], "entities": []}, {"text": "Our motivation for using Turkers is to better analyze system errors.", "labels": [], "entities": []}, {"text": "For example, if we give humans an event in isolation (no sentence context), how well can they guess the durations assigned by the Pan et. al. annotators?", "labels": [], "entities": []}, {"text": "This measures how big the gap is between a system that looks only at the event, and a system that integrates all available context.", "labels": [], "entities": []}, {"text": "To collect event durations from MTurk, we presented Turkers with an event from the TimeBank (a superset of the events annotated by) and asked them to decide whether the event was most likely to take seconds, minutes, hours, days, weeks, months, years or decades.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8961173892021179}, {"text": "TimeBank", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.6174899935722351}]}, {"text": "We had events annotated in two different contexts: in isolation, where only the event itself was given (e.g., \"allocated\"), and in subject-object context, where a minimal phrase including the event and its subject and object was given (e.g., \"the mayor allocated funds\").", "labels": [], "entities": []}, {"text": "In both types of tasks, we asked 10 Turkers to label each event, and they were paid $0.0025 for each annotation ($0.05 fora block of 20 events).", "labels": [], "entities": []}, {"text": "To filter out obvious spammers, we added a test item randomly to each block, e.g., adding the event \"minutes\" and rejecting work from Turkers who labeled this anything other than the duration minutes.", "labels": [], "entities": []}, {"text": "The resulting annotations give duration distributions for each of our events.", "labels": [], "entities": [{"text": "duration", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9895488023757935}]}, {"text": "For example, when presented the event \"remodeling\", 1 Turker responded with days, 6 with weeks, 2 with months and 1 with years.", "labels": [], "entities": []}, {"text": "These annotations suggest that we generally expect \"remodeling\" to take weeks, but it may sometimes take more or less.", "labels": [], "entities": []}, {"text": "To produce a single fine-grained label from these distributions, we take the duration bin with the largest number of Turker annotations, e.g. for \"remodeling\", we would produce the label weeks.", "labels": [], "entities": [{"text": "duration", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9714150428771973}]}, {"text": "To produce a single coarse-grained label, we use the label less-than-a-day if the fine-grained label was seconds, minutes or hours and more-than-a-day otherwise.", "labels": [], "entities": []}, {"text": "As discussed in Section 3, we convert the minimum and maximum duration annotations into labels by converting each to seconds using ISO standards and calculating the arithmetic mean.", "labels": [], "entities": [{"text": "arithmetic mean", "start_pos": 165, "end_pos": 180, "type": "METRIC", "confidence": 0.940759539604187}]}, {"text": "If the mean is \u2264 86400 seconds, it is considered less-than-a-day for the coarse-grained task.", "labels": [], "entities": []}, {"text": "The fine-grained buckets are similarly calculated, e.g., X is labeled days if 86400 < X \u2264 604800.", "labels": [], "entities": []}, {"text": "The evaluation does not include a decades bucket, but our system still uses \"decades\" in its queries.", "labels": [], "entities": []}, {"text": "We optimized all parameters of both the supervised and unsupervised systems on the training set, only running on test after selecting our best performing model.", "labels": [], "entities": []}, {"text": "We compare to the majority class as a baseline,: System accuracy compared against supervised and majority class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9310619831085205}]}, {"text": "* indicates statistical significance (McNemar's Test, two-tailed) against majority class at the p < 0.01 level, \u2020 at p < 0.05 tagging all events as more-than-a-day in the coarse-grained task and months in the fine-grained task.", "labels": [], "entities": [{"text": "McNemar's Test", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.5398511290550232}]}, {"text": "To evaluate our models, we use simple accuracy on the coarse-grained task, and approximate agreement matching as in on the fine-grained task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9993658661842346}]}, {"text": "In this approximate agreement, a guess is considered correct if it chooses either the gold label or its immediate neighbor (e.g., hours is correct if minutes, hours or days is the gold class).", "labels": [], "entities": []}, {"text": "Pan et al. use this approach since human labeling agreement is low (44.4%) on the exact agreement fine-grained task.", "labels": [], "entities": []}, {"text": "compares the performance of our two supervised models; the reimplementation of, and our improved model with new features (Supervised, all).", "labels": [], "entities": []}, {"text": "The new model performs similarily to the Pan model on the in-domain Test set, but better on the out-of-domain financial news articles in the TestWSJ test.", "labels": [], "entities": [{"text": "Pan", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.7783345580101013}, {"text": "TestWSJ test", "start_pos": 141, "end_pos": 153, "type": "DATASET", "confidence": 0.9566501975059509}]}, {"text": "On the latter, the new model improves over Pan et al. by 1.3% absolute on the coarse-grained task, and by 4.1% absolute on the fine-grained task.", "labels": [], "entities": []}, {"text": "We report results from the maximum entropy model as it slightly outperformed the naive bayes and support vector machine models .", "labels": [], "entities": []}], "tableCaptions": []}