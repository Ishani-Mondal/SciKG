{"title": [{"text": "Guided Self Training for Sentiment Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.9898766279220581}]}], "abstractContent": [{"text": "The application of machine learning techniques to classify text documents into sentiment categories has become an increasingly popular area of research.", "labels": [], "entities": [{"text": "classify text documents into sentiment categories", "start_pos": 50, "end_pos": 99, "type": "TASK", "confidence": 0.7859844962755839}]}, {"text": "These techniques rely upon the availability of labelled data, but in certain circumstances the availability of pre-classified documents maybe limited.", "labels": [], "entities": []}, {"text": "Limited labelled data can impact the performance of the model induced from it.", "labels": [], "entities": []}, {"text": "There area number of strategies which can compensate for the lack of labelled data, however these techniques maybe suboptimal if the initial labelled data selection does not contain a sufficient cross section of the total document collection.", "labels": [], "entities": []}, {"text": "This paper proposes a variant of self-training as a strategy to this problem.", "labels": [], "entities": []}, {"text": "The proposed technique uses a high precision classifier (linguistic rules) to influence the selection of training candidates which are labelled by the base learner in an iterative self-training process.", "labels": [], "entities": []}, {"text": "The linguistic knowledge encoded in the high precision classifier corrects high-confidence errors made by the base clas-sifier in a preprocessing step.", "labels": [], "entities": []}, {"text": "This step is followed by a standard self training cycle.", "labels": [], "entities": []}, {"text": "The technique was evaluated in three domains: user generated reviews for (1) airline meals, (2) university professors and (3) music against: (1) constrained learning strategies (voting and veto), (2) induction and (3) standard self-training.", "labels": [], "entities": [{"text": "induction", "start_pos": 200, "end_pos": 209, "type": "TASK", "confidence": 0.8878489136695862}]}, {"text": "The evaluation measure was by estimated F-Measure.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9984164237976074}]}, {"text": "The results demonstrate clear advantage for the proposed method for classifying text documents into sentiment categories in domains where there is limited amounts of training data.", "labels": [], "entities": [{"text": "classifying text documents into sentiment categories", "start_pos": 68, "end_pos": 120, "type": "TASK", "confidence": 0.8457545538743337}]}], "introductionContent": [{"text": "The application of machine learning techniques to classify text into sentiment categories has become an increasingly popular area of research.", "labels": [], "entities": [{"text": "classify text into sentiment categories", "start_pos": 50, "end_pos": 89, "type": "TASK", "confidence": 0.8181208729743957}]}, {"text": "Models induced from data can be very accurate), but a learner may require a significant amount of data to induce a model which can accurately classify a text collection.", "labels": [], "entities": []}, {"text": "Large volumes of labelled documents may not be readily available or maybe expensive to obtain.", "labels": [], "entities": []}, {"text": "Models induced from small volumes of labelled data maybe suboptimal because the pre-classified data may not contain a sufficient cross-section of the document collection.", "labels": [], "entities": []}, {"text": "The field of Semi-Supervised Learning (SSL) offers a number of possible strategies to compensate for the lack of labelled data.", "labels": [], "entities": [{"text": "Semi-Supervised Learning (SSL)", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.755435311794281}]}, {"text": "These techniques may not be effective if the model induced from the initial set of labelled data is biased or ineffective because these strategies can exacerbate the weaknesses in the initial model.", "labels": [], "entities": []}, {"text": "This paper describes a SSL strategy that is a variant of Self-Training (ST).", "labels": [], "entities": [{"text": "SSL", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9903550148010254}]}, {"text": "ST is an iterative process that obtains models with increasingly larger samples of labelled data.", "labels": [], "entities": [{"text": "ST", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9811610579490662}]}, {"text": "At each iteration the current model is used to classify the unlabelled data.", "labels": [], "entities": []}, {"text": "The observations which the model has a high confidence in the classification are added to the next training sample with the classification of the model as the label.", "labels": [], "entities": []}, {"text": "The evaluation of the effectiveness of our proposal involved the experimental comparison with the following types of methods: (1) constrained, (2) inductive and (3) standard self-training.", "labels": [], "entities": []}, {"text": "Training data was randomly selected from the total document collection and ranged from 1% of the total collection to 5%.", "labels": [], "entities": []}, {"text": "The F-Measure was estimated by testing the model against the total document collection with the training data removed.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9973737001419067}]}, {"text": "The experiments were run 20 times for each training intervals with two separate learners: Naive Bayes and Language Models.", "labels": [], "entities": []}, {"text": "The domains which were evaluated were:(1) airline meals, (2) university teachers and (3) music reviews.", "labels": [], "entities": []}, {"text": "The results demonstrate clear advantage for the proposed method for classifying text documents in domains where the models induced from the training data were weak.", "labels": [], "entities": [{"text": "classifying text documents", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.8458653092384338}]}], "datasetContent": [{"text": "The rating was taken as an indication of the polarity of the review.", "labels": [], "entities": []}, {"text": "The criteria for class assignment is described in.", "labels": [], "entities": [{"text": "class assignment", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.704073041677475}]}, {"text": "Documents not satisfying the criteria for class assignment were removed from our experiments.", "labels": [], "entities": [{"text": "class assignment", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7529599368572235}]}, {"text": "These resulting labelled data sets were used to compare: \u2022  The evaluation was by means of an estimated FMeasure.", "labels": [], "entities": [{"text": "FMeasure", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.4709177315235138}]}, {"text": "The experiments used increasing larger random selection of documents as training data.", "labels": [], "entities": []}, {"text": "The smallest selection of data was 1% of the total and the largest 5%.", "labels": [], "entities": []}, {"text": "The increments were in steps of 1%, for example the second iteration of the experiment was 2%, the third 3% etc.", "labels": [], "entities": [{"text": "increments", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9689077138900757}]}, {"text": "At each iteration the experiment was repeated 20 times, for example the 1st iteration there would be 20 random samples of 1% and 20 estimations of F-Measure.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.966566264629364}]}, {"text": "An overview of the process is the following: 1.", "labels": [], "entities": []}, {"text": "randomly select training data (the LD set in Algorithm 1) and 2.", "labels": [], "entities": []}, {"text": "\"artificially unlabel\" the remaining documents to create the UD.", "labels": [], "entities": []}, {"text": "The experiments were repeated using Language Models and Naive Bayes Classifier as the baseline classifiers within the GST algorithm.", "labels": [], "entities": []}, {"text": "We have compared our proposed method against three alternative strategies: (1) inductive, (2) self-training and (3) constrained learning.", "labels": [], "entities": []}, {"text": "\u2022 Inductive: An inductive strategy induces a classification model using only the labelled data).", "labels": [], "entities": []}, {"text": "\u2022 Self-Training: An iterative process whereat each step a model is induced from the current labelled data and it is used to classify the unlabelled data set.", "labels": [], "entities": []}, {"text": "The model assigns a \"confidence measure\" to each classification.", "labels": [], "entities": []}, {"text": "If the classification confidence measure is greater than a predefined threshold then the respective unlabelled cases are added to the new iteration training data with the classifier assigned label.", "labels": [], "entities": []}, {"text": "At the end of the cycle the learner is trained on the \"new labelled data set\".", "labels": [], "entities": []}, {"text": "This cycle continues until a stopping condition is met.", "labels": [], "entities": []}, {"text": "To ensure an equitable comparison the stopping condition for both self-training and GST was 5 iterations.", "labels": [], "entities": [{"text": "GST", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.6304722428321838}]}, {"text": "\u2022 Constrained Learning: The alternate constrained learning strategies were Voting and Veto.", "labels": [], "entities": []}, {"text": "-Voting strategy: Selects documents if both the classifiers agree on the classification of the document -Veto strategy: The base learner selects the data, but high precision classifier  The Airline Food Domain results are presented in.", "labels": [], "entities": [{"text": "Airline Food Domain results", "start_pos": 190, "end_pos": 217, "type": "DATASET", "confidence": 0.9233561754226685}]}, {"text": "The results demonstrates that the proposed strategy does not show any distinct advantage over the competing strategies.", "labels": [], "entities": []}, {"text": "The models induced from the labelled data seem robust and the various SSL strategies fail to improve this strategy.", "labels": [], "entities": [{"text": "SSL", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9331740140914917}]}], "tableCaptions": [{"text": " Table 3: Airline Meals Experimental Results", "labels": [], "entities": []}, {"text": " Table 4: Teacher Review Experimental Results", "labels": [], "entities": []}, {"text": " Table 5: Music Reviews Experimental Results", "labels": [], "entities": []}, {"text": " Table 6: GST Strategy with lower precision classifier", "labels": [], "entities": [{"text": "GST", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9410416483879089}]}]}