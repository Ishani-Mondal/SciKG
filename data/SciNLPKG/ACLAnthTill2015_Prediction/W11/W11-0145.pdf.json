{"title": [], "abstractContent": [{"text": "Recent work on evaluativity or sentiment in the language sciences has focused on the contributions that lexical items provide.", "labels": [], "entities": []}, {"text": "In this paper, we discuss contextual evaluativity, stance that is inferred from lexical meaning and pragmatic environments.", "labels": [], "entities": []}, {"text": "Focusing on assessor-grounding claims like We liked him because he so clearly disliked Margaret Thatcher, we build a corpus and construct a system employing compositional principles of evaluativity calculation to derive that we dislikes Mar-garet Thatcher.", "labels": [], "entities": []}, {"text": "The resulting system has an F-score of 0.90 on our dataset, outperforming reasonable baselines, and indicating the viability of inferencing in the evaluative domain.", "labels": [], "entities": [{"text": "F-score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9992669224739075}]}, {"text": "1 Contextual Evaluativity A central aim of contemporary research on sentiment or evaluative language is the extraction of eval-uative triples: evaluator, target, evaluation.", "labels": [], "entities": []}, {"text": "To date, both formal (e.g., Martin and White 2005, Potts 2005) and computational approaches (e.g., Pang and Lee 2008) have focused on how such triples are lexically encoded (e.g., the negative affect of scoundrel or dislike).", "labels": [], "entities": []}, {"text": "While lexical properties area key source of evaluative information, word-based considerations alone can miss pragmatic inferences resulting from context.", "labels": [], "entities": []}, {"text": "(1), for example, communicates that the referent of we bears not only positive stance towards the referent of him, but also negative stance towards Margaret Thatcher: (1) We liked him because he so clearly disliked Margaret Thatcher.", "labels": [], "entities": []}, {"text": "LEXICAL EVALUATIVITY: we, him, +; he, M.T.,-CONTEXTUAL EVALUATIVITY: we, M.T.,-This paper argues fora compositional approach to contextual evaluativity similar to the compositional methods adopted for lexical evaluativity in Moilanen and Pulman (2007) and Nasukawa and Yi (2003).", "labels": [], "entities": [{"text": "LEXICAL", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9372603893280029}]}, {"text": "At the the heart of the approach is the treatment of verbal predicates (dislike in (1)) as evaluativity functors which relate argument/entity-level evaluativity to event-level evaluativity.", "labels": [], "entities": []}, {"text": "As discussed in \u00a72, the utitlity of such a model surfaces in cases where the event-level evaluativity is known from context, and thus new information about the contextual evaluativity of the event participants (e.g. Margaret Thatcher) can be inferred.", "labels": [], "entities": []}, {"text": "Consequently, the empirical focus of this paper is on structures like (1), where the second clause provides grounds for the sentiment encoded in the first, and hence has a predictable event-level evaluation from the first clause's evaluator.", "labels": [], "entities": []}, {"text": "In \u00a73 we describe the collection and annotation of a corpus of such assessment-grounding configurations from large-scale web data.", "labels": [], "entities": []}, {"text": "This annotated corpus serves as a test bed for experimental evaluation of various implementations of the proposed compositional approach.", "labels": [], "entities": []}, {"text": "The results of these experiments (\u00a74) strongly support a com-positional approach to contextual evaluativity inference.", "labels": [], "entities": [{"text": "contextual evaluativity inference", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.6115182240804037}]}, {"text": "A simple compositional algorithm based on a small, manually created evaluativity functor lexicon demonstrated significantly better precision than non-compositional baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9975207448005676}]}, {"text": "Moreover, a method for automatically expanding coverage to novel predicates based on similarity with the manually created lexicon is shown to increase recall dramatically with modest reduction in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.9992577433586121}, {"text": "precision", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9976485371589661}]}], "introductionContent": [], "datasetContent": [{"text": "Restricting ourselves to the assessment-grounding configuration discussed above, we treat contextual polarity inference as a binary classification problem with two inputs: the INPUT EVENT event-level polarity (derived from the assessment clause) and the main verb of the grounding clause (henceforth FUNCTOR VERB).", "labels": [], "entities": [{"text": "contextual polarity inference", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.6225124299526215}, {"text": "INPUT EVENT", "start_pos": 176, "end_pos": 187, "type": "METRIC", "confidence": 0.864147812128067}, {"text": "FUNCTOR VERB)", "start_pos": 300, "end_pos": 313, "type": "METRIC", "confidence": 0.8439323902130127}]}, {"text": "The goal of the classifier is to correctly predict the polarity of the target NP (direct object to the functor verb) given these inputs.", "labels": [], "entities": []}, {"text": "As mentioned in \u00a72, we may categorize the functor verbs in our lexicon into preservers and reversers.", "labels": [], "entities": []}, {"text": "Two sources populate our lexicon.", "labels": [], "entities": []}, {"text": "First, positively subjective verbs from the MPQA subjectivity lexicon were marked as preservers and negatively subjective verbs were marked as reversers (1249 verbs total).", "labels": [], "entities": [{"text": "MPQA subjectivity lexicon", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.8841326236724854}]}, {"text": "For example, E dislike is a reverser.", "labels": [], "entities": [{"text": "E dislike", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.6688913702964783}]}, {"text": "Second, 487 verbs were culled from FrameNet () based on their membership in six entailment classes: verbs of injury, destruction, lacking, benefit, creation, and having.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9041603207588196}]}, {"text": "Class membership was determined by identifying 124 FrameNet frames aligning with one or more classes, then manually selecting from these frames verbs whose class membership was unambiguous.", "labels": [], "entities": []}, {"text": "Verbs of benefit, creation, and having were marked as preservers.", "labels": [], "entities": []}, {"text": "Verbs of injury, destruction, and lacking were marked as reversers.", "labels": [], "entities": [{"text": "lacking", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9833306670188904}]}, {"text": "Our system (SYS) classifies objects in context as follows: If the functor verb is a preserver, the target NP is assigned the same polarity as the input event polarity.", "labels": [], "entities": []}, {"text": "If the functor verb is a reverser, the target NP is assigned the opposite of the input event polarity.", "labels": [], "entities": []}, {"text": "This procedure is modulated by the presence of negation, as detected by a neg relation in the dependency parse.", "labels": [], "entities": []}, {"text": "Under negation, a preserver acts like a reverser, and vice versa.", "labels": [], "entities": []}, {"text": "We tested the performance of this system (SYS) on our annotated corpus against three baselines.", "labels": [], "entities": []}, {"text": "The first baseline (B-Functor) attempts to determine the importance of the input event to the calculation.", "labels": [], "entities": [{"text": "B-Functor", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9576489329338074}]}, {"text": "It thus ignores the preceding context, and attempts to classify the target object from the functor verb directly, based on the verb's polarity in the MPQA subjectivity lexicon.", "labels": [], "entities": [{"text": "MPQA subjectivity lexicon", "start_pos": 150, "end_pos": 175, "type": "DATASET", "confidence": 0.8775985240936279}]}, {"text": "It has poor precision and recall, 4 reflecting both the importance of the assessment context for object polarity and the fact that the functor verbs are often not lexically sentiment bearing (e.g., predicates of possession).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993829727172852}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.99928879737854}]}, {"text": "The second baseline (B-Input), conversely, ignores the functor verb and uses the input event polarity as listed in the MPQA lexicon (modulo negation) for object classification.", "labels": [], "entities": [{"text": "MPQA lexicon", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.9536233246326447}, {"text": "object classification", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7322741001844406}]}, {"text": "The purpose of this baseline is to approximate a classifier that predicts target polarity solely from the global/contextual polarity of the preceding clause.", "labels": [], "entities": []}, {"text": "This has sharply increased precision, indicating contextual information's importance.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993252754211426}]}, {"text": "The third baseline (B-MLE) picked the majority object class (+), and had the highest precision, indicating the general bias in our corpus for positive objects.", "labels": [], "entities": [{"text": "B-MLE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9820037484169006}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9988341927528381}]}, {"text": "shows the performance (precision vs. recall) of our system compared to the three baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9990220069885254}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9875363707542419}]}, {"text": "Its precision is significantly higher, but its F-score is limited by the lower coverage of our manually constructed lexicon.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994813799858093}, {"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9987555742263794}]}, {"text": "SYS-MPQA and SYS-Frame show the performance of the system when the functor lexicon is limited to the MPQA and Framenet predicates, respectively.", "labels": [], "entities": []}, {"text": "Both are high precision sources of functor prediction, and pick out somewhat distinct predicates (given the recall gain of combining them).", "labels": [], "entities": [{"text": "functor prediction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7054903656244278}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9967987537384033}]}, {"text": "SYS+Maj and SYS+Sim are attempts to handle the low recall of SYS caused by functor verbs in the test data which aren't in the system's lexicon.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9986575841903687}]}, {"text": "SYS+Maj simply assigns these out-of-vocabulary verbs to the majority class: preservers.", "labels": [], "entities": [{"text": "SYS+Maj", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7551142374674479}]}, {"text": "SYS+Sim classifies out-of-vocabulary verbs as preservers or reversers based on their relative similarity to the known preservers and reversers selected from FrameNet -an unknown verb is categorized as a preserver if its average similarity to preservers is greater than its average similarity to reversers.", "labels": [], "entities": []}, {"text": "Similarity was determined according to the Jiang-Conrath distance measure, which based on links in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9822872281074524}]}, {"text": "(Note: this process cannot occur for words not found in WordNet -e.g. misspellings -hence the less than perfect recall).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.936750054359436}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.7883009314537048}]}, {"text": "These two systems outperform all baselines, but have indistinguishable F-scores (if misspellings are excluded, SYS+Sim has a Recall of 0.99 and F-score of 0.91).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.991500973701477}, {"text": "Recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.998343825340271}, {"text": "F-score", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.9982060194015503}]}, {"text": "Most of the precision errors incurred by our systems were syntactic: incorrect parsing, incorrect extraction of the object, or faulty negation handling (e.g., negative quantifiers or verbs).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9967694282531738}]}, {"text": "26% of errors are due to word-sense disambiguation.", "labels": [], "entities": []}, {"text": "The verbs spoil and own each have positive and negative uses (own can mean defeat), but only one sense was registered in the lexicon, leading to errors.", "labels": [], "entities": []}, {"text": "The lion's share of these errors (22%) were due to the use of hate and similar expressions to convey jealousy (e.g. I was mad at him because he had both Boardwalk and Park Place).", "labels": [], "entities": [{"text": "errors", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9759467840194702}, {"text": "Boardwalk and Park Place", "start_pos": 153, "end_pos": 177, "type": "DATASET", "confidence": 0.7305664867162704}]}, {"text": "In these scenarios, although the assessment is negative, the event-level polarity of the grounding clause event type is positive (because it is desired), a fact which our current system cannot handle.", "labels": [], "entities": []}, {"text": "One way forward would be to apply WSD techniques to distinguish jealous from non-jealous uses of predicates of dislike.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9856348633766174}]}], "tableCaptions": [{"text": " Table 3: Performance of systems and baselines  for contextual evaluativity classification", "labels": [], "entities": []}]}