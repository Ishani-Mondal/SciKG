{"title": [], "abstractContent": [{"text": "The new trend in sentiment classification is to use semantic features for representation of documents.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.96285080909729}]}, {"text": "We propose a semantic space based on WordNet senses fora supervised document-level sentiment classifier.", "labels": [], "entities": [{"text": "document-level sentiment classifier", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.6308288276195526}]}, {"text": "Not only does this show a better performance for sentiment classification, it also opens opportunities for building a robust sentiment classifier.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.9718134999275208}]}, {"text": "We examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9743555784225464}]}, {"text": "Using three popular similarity met-rics, we replace unknown synsets in the test set with a similar synset from the training set.", "labels": [], "entities": []}, {"text": "An improvement of 6.2% is seen with respect to baseline using this approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment classification is a task under Sentiment Analysis (SA) that deals with automatically tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9293515682220459}, {"text": "Sentiment Analysis (SA)", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.80756516456604}]}, {"text": "Thus, a sentiment classifier tags the sentence 'The movie is entertaining and totally worth your money!'", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.7780020236968994}]}, {"text": "in a movie review as positive with respect to the movie.", "labels": [], "entities": []}, {"text": "On the other hand, a sentence 'The movie is so boring that I was dozing away through the second half.' is labeled as negative.", "labels": [], "entities": []}, {"text": "Finally, 'The movie is directed by Nolan' is labeled as neutral.", "labels": [], "entities": []}, {"text": "For the purpose of this work, we follow the definition of and consider a binary classification task for output labels as positive and negative.", "labels": [], "entities": []}, {"text": "Lexeme-based (bag-of-words) features are commonly used for supervised sentiment classification (.", "labels": [], "entities": [{"text": "supervised sentiment classification", "start_pos": 59, "end_pos": 94, "type": "TASK", "confidence": 0.7151357134183248}]}, {"text": "In addition to this, there also has been work that identifies the roles of different parts-of-speech (POS) like adjectives in sentiment classification ().", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.9244516789913177}]}, {"text": "Complex features based on parse trees have been explored for modeling high-accuracy polarity classifiers ().", "labels": [], "entities": []}, {"text": "Text parsers have also been found to be helpful in modeling valence shifters as features for classification ().", "labels": [], "entities": []}, {"text": "In general, the work in the context of supervised SA has focused on (but not limited to) different combinations of bagof-words-based and syntax-based models.", "labels": [], "entities": []}, {"text": "The focus of this work is to represent a document as a set of sense-based features.", "labels": [], "entities": []}, {"text": "We ask the following questions in this context: 1.", "labels": [], "entities": []}, {"text": "Are WordNet senses better features as compared to words?", "labels": [], "entities": [{"text": "WordNet", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9007959365844727}]}, {"text": "2. Can a sentiment classifier be made robust with respect to features unseen in the training corpus using similarity metrics defined for concepts in WordNet?", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8385849893093109}, {"text": "WordNet", "start_pos": 149, "end_pos": 156, "type": "DATASET", "confidence": 0.9287207722663879}]}, {"text": "We modify the corpus by for the purpose of our experiments related to sense-based sentiment classification.", "labels": [], "entities": [{"text": "sense-based sentiment classification", "start_pos": 70, "end_pos": 106, "type": "TASK", "confidence": 0.7282917002836863}]}, {"text": "To address the first question, we show that the approach that uses senses (either manually annotated or obtained through automatic WSD techniques) as features performs better than the one that uses words as features.", "labels": [], "entities": []}, {"text": "Using senses as features allows us to achieve robustness for sentiment classification by exploiting the definition of concepts (sense) and hierarchical structure of WordNet.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.9395686388015747}, {"text": "WordNet", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.9677923321723938}]}, {"text": "Hence to address the second question, we replace a synset not present in the test set with a similar synset from the training set using similarity metrics defined on WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9772802591323853}]}, {"text": "Our results show that replacement of this nature provides a boost to the classification performance.", "labels": [], "entities": []}, {"text": "The road map for the rest of the paper is as follows: Section 2 describes the sense-based features that we use for this work.", "labels": [], "entities": []}, {"text": "We explain the similaritybased replacement technique using WordNet synsets 132 in section 3.", "labels": [], "entities": [{"text": "similaritybased replacement", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.633982315659523}, {"text": "WordNet synsets 132", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.9497339129447937}]}, {"text": "Details about our experiments are described in Section 4.", "labels": [], "entities": []}, {"text": "In section 5, we present our results and discussions.", "labels": [], "entities": []}, {"text": "We contextualize our work with respect to other related works in section 6.", "labels": [], "entities": []}, {"text": "Finally, section 7 concludes the paper and points to future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe the variants of the corpus generated and the experiments in this section.", "labels": [], "entities": []}, {"text": "The experiments are performed using C-SVM (linear kernel with default parameters 1 ) available as apart of LibSVM 2 package.", "labels": [], "entities": []}, {"text": "We choose to use SVM since it performs the best for sentiment classification).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.9627085030078888}]}, {"text": "All results reported are average of five-fold cross-validation accuracies.", "labels": [], "entities": []}, {"text": "To conduct experiments on words as features, we first perform stop-word removal.", "labels": [], "entities": [{"text": "stop-word removal", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7870955765247345}]}, {"text": "The words are not stemmed as per observations by).", "labels": [], "entities": []}, {"text": "To conduct the experiments based on the synset representation, words in the corpus are annotated with synset identifiers along with POS category identifiers.", "labels": [], "entities": []}, {"text": "For automatic sense disambiguation, we used the trained IWSD engine (trained on tourism domain) from.", "labels": [], "entities": [{"text": "automatic sense disambiguation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.575200210014979}]}, {"text": "These synset identifiers along with POS category identifiers are then used as features.", "labels": [], "entities": []}, {"text": "For replacement using semantic similarity measures, we used WordNet::Similarity 2.05 package by.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9195253252983093}]}, {"text": "To evaluate the result, we use accuracy, F-score, recall and precision as the metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9996397495269775}, {"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9982702732086182}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9995228052139282}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9993882179260254}]}, {"text": "Classification accuracy defines the ratio of the number of true instances to the total number of instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8863444924354553}]}, {"text": "Recall is calculated as a ratio of the true instances found to the total number of false positives and true positives.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9876149892807007}]}, {"text": "Precision is defined as the number of true instances divided by number of true positives and false negatives.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9727221727371216}]}, {"text": "Positive Precision (PP) and Positive Recall (PR) are precision and recall for positive documents while Negative Precision (NP) and Negative Recall (NR) are precision and recall for negative documents.", "labels": [], "entities": [{"text": "Positive Recall (PR)", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.6795896470546723}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9989632368087769}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9969984292984009}, {"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9990378618240356}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9970952272415161}]}, {"text": "F-score is the weighted precision-recall: Classification Results; M-Manual, I-IWSD, W-Words, PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%) score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9604452848434448}, {"text": "precision-recall", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.9927748441696167}, {"text": "PR-Positive Recall", "start_pos": 196, "end_pos": 214, "type": "METRIC", "confidence": 0.8129135370254517}, {"text": "NR-Negative Recall (%) score", "start_pos": 220, "end_pos": 248, "type": "METRIC", "confidence": 0.6787954643368721}]}, {"text": "shows results of classification for different feature representations.", "labels": [], "entities": []}, {"text": "The baseline for our results is the unigram bag-of-words model (Words).", "labels": [], "entities": []}, {"text": "An improvement of 4.2% is observed in the accuracy of sentiment prediction when manually annotated sense-based features (M) are used in place of word-based features (Words).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9992524981498718}, {"text": "sentiment prediction", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.9433431327342987}]}, {"text": "The precision of both the classes using features based on semantic space is also better than one based on lexeme space.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9992714524269104}]}, {"text": "Reported results suggest that it is more difficult to detect negative sentiment than positive sentiment (.", "labels": [], "entities": []}, {"text": "However, using sensebased representation, it is important to note that negative recall increases by around 8%.", "labels": [], "entities": [{"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9838805794715881}]}], "tableCaptions": [{"text": " Table 1: Annotation Statistics for IWSD; P-Precision,R- Recall", "labels": [], "entities": [{"text": "IWSD", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.36895957589149475}, {"text": "P-Precision,R- Recall", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.8701169888178507}]}, {"text": " Table 2: Classification Results; M-Manual, I-IWSD, W-Words, PF-Positive F-score(%), NF-Negative F-score (%),  PP-Positive Precision (%), NP-Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)", "labels": [], "entities": [{"text": "PR-Positive Recall", "start_pos": 165, "end_pos": 183, "type": "METRIC", "confidence": 0.8960713446140289}]}, {"text": " Table 3: Similarity Metric Analysis using different  similarity metrics with synsets and a combinations of  synset and words; SM-Similarity Metric, A-Accuracy,  PF-Positive F-score(%), NF-Negative F-score (%)", "labels": [], "entities": [{"text": "Similarity Metric Analysis", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8582190871238708}]}]}