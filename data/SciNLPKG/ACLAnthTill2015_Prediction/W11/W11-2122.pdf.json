{"title": [{"text": "Multiple-stream Language Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.8325366576512655}]}], "abstractContent": [{"text": "We consider using online language models for translating multiple streams which naturally arise on the Web.", "labels": [], "entities": []}, {"text": "After establishing that using just one stream can degrade translations on different domains, we present a series of simple approaches which tackle the problem of maintaining translation performance on all streams in small space.", "labels": [], "entities": []}, {"text": "By exploiting the differing throughputs of each stream and how the decoder translates prior test points from each stream, we show how translation performance can equal specialised, per-stream language models, but do this in a single language model using far less space.", "labels": [], "entities": []}, {"text": "Our results hold even when adding three billion tokens of additional text as a background language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is more natural language data available today than there has ever been and the scale of its production is increasing quickly.", "labels": [], "entities": []}, {"text": "While this phenomenon provides the Statistic Machine Translation (SMT) community with a potentially extremely useful resource to learn from, it also brings with it nontrivial computational challenges of scalability.", "labels": [], "entities": [{"text": "Statistic Machine Translation (SMT)", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.8411757946014404}]}, {"text": "Text streams arise naturally on the Web where millions of new documents are published each day in many different languages.", "labels": [], "entities": []}, {"text": "Examples in the streaming domain include the thousands of multilingual websites that continuously publish newswire stories, the official proceedings of governments and other bureaucratic organisations, as well as the millions of \"bloggers\" and host of users on social network services such as Facebook and Twitter.", "labels": [], "entities": []}, {"text": "Recent work has shown good results using an incoming text stream as training data for either a static or online language model (LM) in an SMT setting (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9816085696220398}]}, {"text": "A drawback of prior work is the oversimplified scenario that all training and test data is drawn from the same distribution using a single, in-domain stream.", "labels": [], "entities": []}, {"text": "Ina real world scenario multiple incoming streams are readily available and test sets from dissimilar domains will be translated continuously.", "labels": [], "entities": []}, {"text": "As we show, using stream data from one domain to translate another results in poor average performance for both streams.", "labels": [], "entities": []}, {"text": "However, combining streams naively together hurts performance further still.", "labels": [], "entities": []}, {"text": "In this paper we consider this problem of multiple stream translation.", "labels": [], "entities": [{"text": "multiple stream translation", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6457349757353464}]}, {"text": "Since monolingual data is very abundant, we focus on the subtask of updating an online LM using multiple incoming streams.", "labels": [], "entities": []}, {"text": "The challenges in multiple stream translation include dealing with domain differences, variable throughput rates (the size of each stream per epoch), and the need to maintain constant space.", "labels": [], "entities": [{"text": "multiple stream translation", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.642037053902944}]}, {"text": "Importantly, we impose the key requirement that our model match translation performance reached using the single stream approach on all test domains.", "labels": [], "entities": []}, {"text": "We accomplish this using the n-gram history of prior translations plus subsampling to maintain a constant bound on memory required for language modelling throughout all stream adaptation.", "labels": [], "entities": []}, {"text": "In particular, when considering two test streams, we are able to improve performance on both streams from an average (per stream) BLEU score of 39.71 and 37.09 using a single stream approach) to an average BLEU score of 41.28 and 42.73 using multiple streams within a single LM using equal memory).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9991626739501953}, {"text": "BLEU", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.9989345669746399}]}, {"text": "We also show additive im-provements using this approach when using a large background LM consisting of over one billion ngrams.", "labels": [], "entities": []}, {"text": "To our knowledge our approach is the first in the literature to deal with adapting an online LM to multiple streams in small space.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we report on our SMT experiments with multiple streams for translation using the approaches outlined in the previous section.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9947224855422974}, {"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9782358407974243}]}, {"text": "The SMT setup we employ is standard and all resources used are publicly available.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9903314113616943}]}, {"text": "We translate from Spanish into English using phrase-based decoding with Moses ( as our decoder.", "labels": [], "entities": []}, {"text": "Our parallel data came from Europarl.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9916163086891174}]}, {"text": "We use three streams (all are timestamped): RCV1 (), Europarl (EP), and Gigaword (GW) (.", "labels": [], "entities": [{"text": "RCV1", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.9301005005836487}, {"text": "Europarl", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9624046683311462}]}, {"text": "GW is taken from six distinct newswire sources but in our initial experiments we limit the incoming stream from Gigaword to one of the sources (xie).", "labels": [], "entities": [{"text": "GW", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8162758350372314}, {"text": "Gigaword", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.9215047359466553}]}, {"text": "GW and RCV1 are both newswire domain streams with high rates of incoming data whereas EP is a more nuanced, smaller throughput domain of spoken transcripts taken from sessions of the European Parliament.", "labels": [], "entities": [{"text": "GW", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9423114061355591}]}, {"text": "The RCV1 corpus only spans one calender year from October, 1996 through September, 1997 so we selected only data in this time frame from the other two streams so our timeline consists of the same full calendar year for all streams.", "labels": [], "entities": [{"text": "RCV1 corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9796344041824341}]}, {"text": "For this work we use the ORLM.", "labels": [], "entities": [{"text": "ORLM", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9269670844078064}]}, {"text": "The crux of the ORLM is an online perfect hash function that provides the ability to insert and delete from the data structure.", "labels": [], "entities": []}, {"text": "Consequently the ORLM has the ability to adapt to an unbounded input stream whilst maintaining both constant memory usage and error rate.", "labels": [], "entities": [{"text": "ORLM", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.7625624537467957}, {"text": "error rate", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9621561765670776}]}, {"text": "All the ORLMs were 5-gram models built with training data from the streams discussed above and used Stupid Backoff smoothing for n-gram scoring (.", "labels": [], "entities": []}, {"text": "All results are reported using the BLEU metric (  from both the RCV1 and EP stream's timeline fora total of six test points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.997853696346283}, {"text": "RCV1 and EP stream's timeline", "start_pos": 64, "end_pos": 93, "type": "DATASET", "confidence": 0.8335220019022623}]}, {"text": "This divided the streams into three epochs, and we updated the online LM using the data encountered in the epoch prior to each translation point.", "labels": [], "entities": []}, {"text": "The n-grams and their counts from the streams are combined in the LM using one of the approaches from the previous section.", "labels": [], "entities": []}, {"text": "Using the notation from Section 4 we have the RCV1, EP, and GW streams described above and K = 3 as the number of incoming streams from two distinct domains (newswire and spoken dialogue).", "labels": [], "entities": []}, {"text": "Our timeline T is one year's worth of data split into three epochs, t \u2208 {1, 2, 3}, with test points at the end of each epoch t.", "labels": [], "entities": []}, {"text": "Since we have no test points from the GW stream it acts as a background stream for these experiments.", "labels": [], "entities": [{"text": "GW stream", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.9269436895847321}]}, {"text": "We have shown that using data from multiple streams benefits SMT performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9966855645179749}]}, {"text": "Our best approach, using history based combination along with subsampling, combines all incoming streams into a single, succinct LM and obtains translation performance equal to single stream, domain specific LMs on all test domains.", "labels": [], "entities": []}, {"text": "Crucially we do this in bounded space, require less memory than storing each stream separately, and do not incur translation degradations on any single domain.", "labels": [], "entities": []}, {"text": "A note on memory usage.", "labels": [], "entities": []}, {"text": "The multiple LM approach uses the most memory since this requires all overlapping n-grams in the streams to be stored separately.", "labels": [], "entities": []}, {"text": "The naive and history combination approaches useless memory since they store all ngrams from all the streams in a unified LM.", "labels": [], "entities": []}, {"text": "For the sampling the exact amount of memory is of course dependent on the sampling rate used.", "labels": [], "entities": []}, {"text": "For the results in we used significantly less memory (300MB) but still achieved comparable performance to approaches that used more memory by storing the full streams (600MB).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sample statistics of unique n-gram counts from  the streams from epoch 2 of our timeline. The throughput  rate varies a lot between streams.", "labels": [], "entities": []}, {"text": " Table 2: Results for the RCV1 test points. RCV1 and GW  streams are in-domain and EP is out-of-domain. Transla- tion results are improved using more stream data since  most n-grams are in-domain to the test points.", "labels": [], "entities": []}, {"text": " Table 3: EP results using in and out-of-domain streams.  The last two rows show that naive combination gets poor  results compared to single stream approaches.", "labels": [], "entities": []}, {"text": " Table 4: Weighted LM interpolation results for the RCV1  test points where E = Europarl, R = RCV1, and G =  Gigaword (xie).", "labels": [], "entities": [{"text": "RCV1  test points", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9480203787485758}, {"text": "Europarl", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.946240246295929}, {"text": "Gigaword", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9240667819976807}]}, {"text": " Table 5: EP results in BLEU for the interpolated LMs.", "labels": [], "entities": [{"text": "EP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9689449667930603}, {"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9994528889656067}]}, {"text": " Table 6: RCV1 test results using history and subsampling  approaches.", "labels": [], "entities": []}, {"text": " Table 7: Europarl test results with history and subsam- pling approaches.", "labels": [], "entities": [{"text": "Europarl test", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9453367590904236}]}, {"text": " Table 8: Singleton-pruned n-gram counts (in millions)  for the GW3 background LM.", "labels": [], "entities": [{"text": "GW3 background LM", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.9315269986788431}]}, {"text": " Table 9: Test results for the RCV1 stream using the large  background LM. Using stream data benefits translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.9762378931045532}]}, {"text": " Table 10: EP test results using the background GW LM.", "labels": [], "entities": [{"text": "GW LM", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.693411260843277}]}]}