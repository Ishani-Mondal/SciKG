{"title": [{"text": "Two-step translation with grammatical post-processing *", "labels": [], "entities": [{"text": "Two-step translation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6175591051578522}]}], "abstractContent": [{"text": "This paper describes an experiment in which we try to automatically correct mistakes in grammatical agreement in English to Czech MT outputs.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 130, "end_pos": 140, "type": "TASK", "confidence": 0.6145631372928619}]}, {"text": "We perform several rule-based corrections on sentences parsed to dependency trees.", "labels": [], "entities": []}, {"text": "We prove that it is possible to improve the MT quality of majority of the systems participating in WMT shared task.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9811835289001465}, {"text": "WMT shared task", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.870716373125712}]}, {"text": "We made both automatic (BLEU) and manual evaluations.", "labels": [], "entities": [{"text": "automatic (BLEU)", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.8450826406478882}]}], "introductionContent": [{"text": "This paper is a joint report on two English-to-Czech submissions to the WMT11 shared translation task.", "labels": [], "entities": [{"text": "WMT11 shared translation task", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.7139599546790123}]}, {"text": "The main contribution is however the proposal and evaluation of a rule-based post-processing system DEPFIX aimed at correcting errors in Czech grammar applicable to any MT system.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.7493565082550049}, {"text": "MT", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.955504834651947}]}, {"text": "This is somewhat the converse of other approaches (e.g.) where a statistical system was applied for the post-processing of a rule-based one.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our CU-TWOSTEP system with DEPFIX post-processing on both WMT10 and WMT11 testing data.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9126560688018799}, {"text": "WMT10", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.9790928363800049}, {"text": "WMT11 testing data", "start_pos": 78, "end_pos": 96, "type": "DATASET", "confidence": 0.9055952231089274}]}, {"text": "This combined system was submitted to shared translation task as CU-MARECEK.", "labels": [], "entities": [{"text": "translation task", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.793701559305191}, {"text": "CU-MARECEK", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.9166989922523499}]}, {"text": "We also ran the DEPFIX post-processing on all other participating systems.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8102545142173767}]}, {"text": "The achieved BLEU scores are shown in  Two independent annotators evaluated DEPFIX manually on the outputs of CU-TWOSTEP and ONLINE-B.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9988468885421753}, {"text": "DEPFIX", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.8162364959716797}, {"text": "CU-TWOSTEP", "start_pos": 110, "end_pos": 120, "type": "DATASET", "confidence": 0.9321094155311584}, {"text": "ONLINE-B", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.947394073009491}]}, {"text": "We randomly selected 1000 sentences from the newssyscombtest2011 data set and the appropriate translations made by these two systems.", "labels": [], "entities": [{"text": "newssyscombtest2011 data set", "start_pos": 45, "end_pos": 73, "type": "DATASET", "confidence": 0.9467105468114217}]}, {"text": "The annotators got the outputs before and after DEPFIX post-processing and their task was to decide which translation 6 from these two is better and label it by the letter 'a'.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.8475807905197144}]}, {"text": "If it was not possible to determine They were also provided with the source English sentence and the reference translation.", "labels": [], "entities": []}, {"text": "The options were shuffled and indentical candidate sentences were collapsed.", "labels": [], "entities": []}, {"text": "A 114 64 516  which is better, they labeled both by 'n'.", "labels": [], "entities": []}, {"text": "below shows that about 60% of sentences fixed by DEPFIX were improved and only about 20% were worsened.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.8004904389381409}]}, {"text": "DEPFIX worked a little better on the ONLINE-B, making fewer changes but also fewer wrong changes.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6942715644836426}, {"text": "ONLINE-B", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8827317953109741}]}, {"text": "It is probably connected with the fact that overall better translations by ONLINE-B are easier to parse.", "labels": [], "entities": [{"text": "ONLINE-B", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.5826087594032288}]}, {"text": "The matrix of inter-annotator agreement is in Table 5.", "labels": [], "entities": []}, {"text": "Our two annotators agreed in 374 sentences (out of 516), that is 72.5%.", "labels": [], "entities": []}, {"text": "On the other hand, if we consider only cases where both annotators chose different translation as better (no indefinite marks), we get only 8.8% disagreement (32 out of 364).", "labels": [], "entities": []}, {"text": "Using the manual evaluation, we can also measure performance of the individual rules.", "labels": [], "entities": []}, {"text": "shows the number of all, improved or worsened sentences where a particular rule was applied.", "labels": [], "entities": []}, {"text": "Definitely, the most useful rule (used often and quite reliable) was the one correcting noun-adjective agreement, followed by the subject-pastparticiple agreement rule.", "labels": [], "entities": [{"text": "correcting noun-adjective agreement", "start_pos": 77, "end_pos": 112, "type": "TASK", "confidence": 0.7689894040425619}]}, {"text": "In each changed sentence, two rules (not necessarily related ones) were applied on average.", "labels": [], "entities": []}, {"text": "The fact that the improvements in BLEU scores on WMT10 test set are much higher has led us to one more experiment: we compare manual annotations of 330 sentences from each of the WMT10 and", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9993197917938232}, {"text": "WMT10 test set", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.9880826075871786}, {"text": "WMT10", "start_pos": 179, "end_pos": 184, "type": "DATASET", "confidence": 0.9591985940933228}]}], "tableCaptions": [{"text": " Table 1: Depfix improvements on the WMT10 systems  in BLEU score. Confidence intervals, which were com- puted on 1000 bootstrap samples, are in brackets.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.7751473784446716}, {"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9726220965385437}, {"text": "Confidence intervals", "start_pos": 67, "end_pos": 87, "type": "METRIC", "confidence": 0.9815761148929596}]}, {"text": " Table 2: Depfix improvements on the WMT11 systems  in BLEU score. Confidence intervals are in brackets.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.7209014892578125}, {"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9585299789905548}, {"text": "Confidence intervals", "start_pos": 67, "end_pos": 87, "type": "METRIC", "confidence": 0.987572580575943}]}, {"text": " Table 5: Matrix of the inter-annotator agreement", "labels": [], "entities": []}, {"text": " Table 6: Rules and their utility.", "labels": [], "entities": []}, {"text": " Table 3: Manual evaluation of the DEPFIX post-processing on 1000 randomly chosen sentences from WMT11 test set.", "labels": [], "entities": [{"text": "WMT11 test set", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9603991905848185}]}, {"text": " Table 4: Manual and automatic evaluation of the DEPFIX post-processing on CU-TWOSTEP system across different  datasets. 330 sentences were randomly selected from each of the WMT10 and WMT11 test sets. Both manual scores  and BLEU are computed only on the sentences that were changed by the DEPFIX post-processing.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.8177664279937744}, {"text": "WMT10 and WMT11 test sets", "start_pos": 175, "end_pos": 200, "type": "DATASET", "confidence": 0.8190351605415345}, {"text": "BLEU", "start_pos": 226, "end_pos": 230, "type": "METRIC", "confidence": 0.9993855953216553}, {"text": "DEPFIX post-processing", "start_pos": 291, "end_pos": 313, "type": "DATASET", "confidence": 0.9153694212436676}]}]}