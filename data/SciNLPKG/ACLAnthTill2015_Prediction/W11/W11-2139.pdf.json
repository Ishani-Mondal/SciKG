{"title": [], "abstractContent": [{"text": "This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11).", "labels": [], "entities": [{"text": "German-English translation", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.5571708083152771}, {"text": "Sixth Workshop on Machine Translation (WMT11)", "start_pos": 133, "end_pos": 178, "type": "TASK", "confidence": 0.6721686199307442}]}, {"text": "We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudo-references for training.", "labels": [], "entities": [{"text": "handing of OOVs", "start_pos": 230, "end_pos": 245, "type": "TASK", "confidence": 0.7173629999160767}]}], "introductionContent": [{"text": "We describe the German-English translation system submitted to the shared translation task in the Sixth Workshop on Machine Translation (WMT11) by the ARK research group at Carnegie Mellon University.", "labels": [], "entities": [{"text": "shared translation task in the Sixth Workshop on Machine Translation (WMT11)", "start_pos": 67, "end_pos": 143, "type": "TASK", "confidence": 0.7156517413946298}]}, {"text": "The core translation system is a hierarchical phrase-based machine translation system that has been extended in several ways described in this paper.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.6672037045160929}]}, {"text": "Some of our innovations focus on modeling.", "labels": [], "entities": []}, {"text": "Since German and English word orders can diverge considerably, particularly in non-matrix clauses, we focused on feature engineering to improve the modeling of long-distance relationships, which are poorly captured in standard hierarchical phrasebased translation models.", "labels": [], "entities": []}, {"text": "To do so, we developed features that assess the goodness of the source 1 http://www.ark.cs.cmu.edu language parse tree under the translation grammar (rather than of a \"linguistic\" grammar).", "labels": [], "entities": []}, {"text": "To train the feature weights, we made use of a novel two-phase training algorithm that incorporates a probabilistic training objective and standard minimum error training.", "labels": [], "entities": []}, {"text": "These segmentation features were supplemented with a 7-gram class-based language model, which more directly models long-distance relationships.", "labels": [], "entities": []}, {"text": "Together, these features provide a modest improvement over the baseline and suggest interesting directions for future work.", "labels": [], "entities": []}, {"text": "While our work on parse modeling was involved and required substantial changes to the training pipeline, some other modeling enhancements were quite simple: for example, improving how out-of-vocabulary words are handled.", "labels": [], "entities": [{"text": "parse modeling", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.9754540026187897}]}, {"text": "We propose a very simple change, and show that it provides a small, consistent gain.", "labels": [], "entities": []}, {"text": "On the training side, we had two improvements over our baseline system.", "labels": [], "entities": []}, {"text": "First, we were inspired by the work of, who showed that when training to optimize), overfitting is reduced by supplementing a single human-generated reference translation with additional computer-generated references.", "labels": [], "entities": []}, {"text": "We generated supplementary pseudo-references for our development set (which is translated into many languages, but once) by using MT output from a secondary Spanish-English translation system.", "labels": [], "entities": []}, {"text": "Second, following, we used a secondary development set to select from among many optimization runs, which further improved generalization.", "labels": [], "entities": []}, {"text": "We largely sought techniques that did not require language-specific resources (e.g., treebanks, POS annotations, morphological analyzers).", "labels": [], "entities": []}, {"text": "An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German.", "labels": [], "entities": []}, {"text": "Aside from this, no further manually annotated data was used, and we suspect many of the improvements described here can be had in other language pairs.", "labels": [], "entities": []}, {"text": "Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Additional features designed to improve model  of long-range reordering.", "labels": [], "entities": []}, {"text": " Table 4: Effect of different sets of reference translations  used during tuning.", "labels": [], "entities": []}]}