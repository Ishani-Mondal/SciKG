{"title": [{"text": "Simple Semi-Supervised Learning for Prepositional Phrase Attachment", "labels": [], "entities": [{"text": "Prepositional Phrase Attachment", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7710281213124593}]}], "abstractContent": [{"text": "Prepositional phrase attachment is an important subproblem of parsing, performance on which suffers from limited availability of labelled data.", "labels": [], "entities": [{"text": "Prepositional phrase attachment", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7743901411692301}, {"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.967589259147644}]}, {"text": "We present a semi-supervised approach.", "labels": [], "entities": []}, {"text": "We show that a discriminative lexical model trained from labelled data, and a generative lexical model learned via Expectation Maximization from unlabelled data can be combined in a product model to yield a PP-attachment model which is better than either is alone, and which outper-forms the modern parser of Petrov and Klein (2007) by a significant margin.", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.6294265240430832}]}, {"text": "We show that, when learning from unlabelled data, it can be beneficial to model the generation of modifiers of ahead collectively, rather than individually.", "labels": [], "entities": []}, {"text": "Finally, we suggest that our pair of models will be interesting to combine using new techniques for discrimina-tively constraining EM.", "labels": [], "entities": []}], "introductionContent": [{"text": "Labelled data for NLP tasks will always be in short supply.", "labels": [], "entities": []}, {"text": "Thus, a statistical parser trained with labelled data alone will always be troubled by unseen events-primarily when parsing out-ofdomain data, or when faced with rare events from in-domain data.", "labels": [], "entities": []}, {"text": "Thus, a major focus of current work is the use of cheap, abundant unlabelled data to improve state-of-the-art parser performance.", "labels": [], "entities": []}, {"text": "We focus on an important sub-problem of parsing-prepositional phrase attachment-and demonstrate a successful semi-supervised learning strategy.", "labels": [], "entities": [{"text": "parsing-prepositional phrase attachment-and", "start_pos": 40, "end_pos": 83, "type": "TASK", "confidence": 0.9263315399487814}]}, {"text": "We show that, using a mix of labelled and unlabelled data we can improve both the in-domain and out-of-domain performance of a prepositional phrase attachment classifier.", "labels": [], "entities": [{"text": "prepositional phrase attachment classifier", "start_pos": 127, "end_pos": 169, "type": "TASK", "confidence": 0.7593560069799423}]}, {"text": "Prepositional phrase attachment, for us, is the decision as to which heads a series of prepositional phrases of the form  Prepositional phrase attachment is an important sub-problem of parsing in and of itself.", "labels": [], "entities": [{"text": "Prepositional phrase attachment", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7856536507606506}, {"text": "Prepositional phrase attachment", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.7819203933080038}]}, {"text": "Structural heuristics perform poorly (cf.,, and so lexical knowledge is crucial.", "labels": [], "entities": []}, {"text": "Moreover, the highly lexicalized nature of prepositional phrase attachment makes it a kind of microcosm of the general problem of learning dependency structure, and so acts as a computationally less-demanding testing ground on which to tryout learning techniques.", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.657593717177709}]}, {"text": "We have endeavoured to approach the problem with a strategy that might be likely to generalize: a mix of generative and discriminative lexical models, trained using techniques that have worked for parsers.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: \u2022 We compare the performance on the prepositional phrase attachment task of natural lexicalized dependency parsing strategies, to the popular semi-lexicalized model of, and show that a lexical is more effective for this problem.", "labels": [], "entities": [{"text": "prepositional phrase attachment task of natural lexicalized dependency parsing", "start_pos": 78, "end_pos": 156, "type": "TASK", "confidence": 0.7550277908643087}]}, {"text": "\u2022 We show that a discriminative lexical model trained from labelled data and a generative lexical model learned through Expectation Maximization on unlabelled data can perform better in a product model than either does alone, yielding a significant improvement over our baseline reference, the parser of.", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.6453042924404144}]}, {"text": "\u2022 We show that, in this case, when learning from unlabelled data, a strategy of generating all modifiers of ahead collectively works better than generating them individually.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of the two discriminative classi- fiers.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the two generative models. Variables are: i) the model learned, independent vs. collective  modifier generation ( \u00a75.2, 5.3), ii) the intial guess at a conditional distribution for hidden variables ( \u00a75.4), and iii)  the number of iterations of EM. Score is the binary decision score (cf.  \u00a73.2).", "labels": [], "entities": [{"text": "Score", "start_pos": 274, "end_pos": 279, "type": "METRIC", "confidence": 0.9609586596488953}]}, {"text": " Table 4: Performance of the combined model. Score is  the binary decision score (cf.  \u00a73.2).", "labels": [], "entities": [{"text": "Score", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.983504056930542}]}, {"text": " Table 5: Performance of the combined model on exam- ples that cannot be settled by our two structural con- straints. I.e., examples where i) the preposition is not  of, and ii) the direct object is not a pronoun. Score is  the binary decision score (cf.  \u00a73.2)", "labels": [], "entities": [{"text": "Score", "start_pos": 214, "end_pos": 219, "type": "METRIC", "confidence": 0.9757956862449646}]}, {"text": " Table 6: Performance of the combined model. Score  is the percentage of prepositional phrases attached cor- rectly in all cases in which more than one attachment  was possible.", "labels": [], "entities": []}]}