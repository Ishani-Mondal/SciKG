{"title": [{"text": "Morphological Features for Parsing Morphologically-rich Languages: A Case of Arabic", "labels": [], "entities": [{"text": "Parsing Morphologically-rich Languages", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.8484752376874288}]}], "abstractContent": [{"text": "We investigate how morphological features in the form of part-of-speech tags impact parsing performance, using Arabic as our test case.", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9665855169296265}]}, {"text": "The large, fine-grained tagset of the Penn Arabic Treebank (498 tags) is difficult to handle by parsers, ultimately due to data sparsity.", "labels": [], "entities": [{"text": "Penn Arabic Treebank", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.9862486720085144}]}, {"text": "However, ad-hoc conflations of treebank tags runs the risk of discarding potentially useful parsing information.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is to describe several automated, language-independent methods that search for the optimal feature combination to help parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 155, "end_pos": 162, "type": "TASK", "confidence": 0.9727684259414673}]}, {"text": "We first identify 15 individual features from the Penn Arabic Treebank tagset.", "labels": [], "entities": [{"text": "Penn Arabic Treebank tagset", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.9812161475419998}]}, {"text": "Either including or excluding these features results in 32,768 combinations, so we then apply heuristic techniques to identify the combination achieving the highest parsing performance.", "labels": [], "entities": []}, {"text": "Our results show a statistically significant improvement of 2.86% for vocalized text and 1.88% for unvocalized text, compared with the baseline provided by the Bikel-Bies Arabic POS mapping (and an improvement of 2.14% using product models for vocalized text, 1.65% for unvocalized text), giving state-of-the-art results for Ara-bic constituency parsing.", "labels": [], "entities": [{"text": "Bikel-Bies Arabic POS mapping", "start_pos": 160, "end_pos": 189, "type": "DATASET", "confidence": 0.7269525825977325}, {"text": "Ara-bic constituency parsing", "start_pos": 325, "end_pos": 353, "type": "TASK", "confidence": 0.6418416400750478}]}], "introductionContent": [{"text": "Parsing Arabic is challenging due to its morphological richness and syntactic complexity.", "labels": [], "entities": [{"text": "Parsing Arabic", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9164558351039886}]}, {"text": "In particular, the number of distinct word-forms, the relative freedom with respect to word order, and the information expressed at the level of words make parsing Arabic a difficult task.", "labels": [], "entities": [{"text": "parsing Arabic", "start_pos": 156, "end_pos": 170, "type": "TASK", "confidence": 0.8958912491798401}]}, {"text": "Previous research established that adapting constituency parsing models developed from English to Arabic (and other languages) is a non-trivial task.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7344227731227875}]}, {"text": "Significant effort has been deployed to parse Chinese using the unlexicalized parser of with modest performance gains over previous approaches.", "labels": [], "entities": [{"text": "parse Chinese", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8931894898414612}]}, {"text": "Due to the specification of head rules, lexicalized parsing models also turned out to be difficult to generalize to other languages: describe Arabic parsing results far below English or even Chinese using the Collins parsing model as implemented in the Bikel parser.", "labels": [], "entities": []}, {"text": "In order to side-step the surface representations involved in constituency parsing, several studies have focused on Arabic dependency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.863190621137619}, {"text": "Arabic dependency parsing", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.5855525135993958}]}, {"text": "The general assumption is that dependency structures are better suited for representing syntactic information for morphologically rich and free-word order languages.", "labels": [], "entities": []}, {"text": "However, the results of CoNLL shared tasks on 18 different languages, including Arabic () using either the MaltParser () or the MSTParser suggests that Arabic is nonetheless quite a difficult language to parse , leaving open the question as to the effectiveness of dependency parsing for Arabic.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 128, "end_pos": 137, "type": "DATASET", "confidence": 0.9550196528434753}, {"text": "dependency parsing", "start_pos": 265, "end_pos": 283, "type": "TASK", "confidence": 0.7348280847072601}]}, {"text": "One reason for this ineffectiveness is that many parsers do not make much, if any, use of morphological information.", "labels": [], "entities": []}, {"text": "In fact, many established parsing models do not capture visible morphological information provided by wordforms and thus fail to make important distributional distinctions.", "labels": [], "entities": []}, {"text": "In this paper, using a re-implementation of the Berkeley latent-variable PCFG parser we study how morphological features, as encoded in POS tags, can be learned automatically by modifying the distributional restriction of initial grammar symbols, and how they impact Arabic constituency parsing.", "labels": [], "entities": [{"text": "Arabic constituency parsing", "start_pos": 267, "end_pos": 294, "type": "TASK", "confidence": 0.5429424444834391}]}, {"text": "We have selected PCFG-LA parsing models because they have been shown to be relatively language-independent with state-ofthe-art performance for several languages.", "labels": [], "entities": [{"text": "PCFG-LA parsing", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.740604043006897}]}, {"text": "reported that extending POS tags with definiteness information helps Arabic PCFG parsing 2 , enriched the POS tagset with gender, number and definiteness to improve Arabic base phrase chunking, and reported that definiteness, person, number and gender were most helpful for Arabic dependency parsing on predicted tag input.", "labels": [], "entities": [{"text": "Arabic PCFG parsing", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.5434817671775818}, {"text": "Arabic base phrase chunking", "start_pos": 165, "end_pos": 192, "type": "TASK", "confidence": 0.6023344397544861}, {"text": "Arabic dependency parsing", "start_pos": 274, "end_pos": 299, "type": "TASK", "confidence": 0.5858708520730337}]}, {"text": "Our method is comparable to this work in terms of the investigation of the morphological features.", "labels": [], "entities": []}, {"text": "However, the results are not comparable, as we use a different parsing paradigm, a different form of the treebank, and most importantly, we extend the investigation to use several automated feature selection methods.", "labels": [], "entities": []}, {"text": "Increasing the tagset size can lead to data sparsity and generally exacerbates the problem of unknown words (that is, word:POS pairs not attested during training).", "labels": [], "entities": []}, {"text": "To overcome this problem, we have experimented with the technique presented in to handle unknown words (out of vocabulary words -OOV) within a generative parsing model.", "labels": [], "entities": [{"text": "generative parsing", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.9287851750850677}]}, {"text": "This method employs a list of heuristics to extract morphological clues from word forms and builds a set of word-classes.", "labels": [], "entities": []}, {"text": "Our results show that enriching basic POS tags with morphological information, accompanied by a method for handling unknown words, jointly and statistically significantly improve upon the parsing baseline, which uses the Bikel-Bies collapsed POS tagset ( and does not employ morphological information to handle unknown words.", "labels": [], "entities": [{"text": "Bikel-Bies collapsed POS tagset", "start_pos": 221, "end_pos": 252, "type": "DATASET", "confidence": 0.6561447605490685}]}, {"text": "This paper is organized as follows: In Section 2, we review the PCFG-LA parsing model and the dataset for our experiments.", "labels": [], "entities": [{"text": "PCFG-LA parsing", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.8466005623340607}]}, {"text": "Section 3 addresses the different techniques we have applied to explore the morphological features space over the possible combinations, including a comparison with the 2 By comparison, the case feature improved parsing for Czech () and the combination of the number feature for adjectives and mode feature for verbs improved results for Spanish best morphological features of previous works.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the results of applying the parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimented with the strategies presented in section 3.", "labels": [], "entities": []}, {"text": "All our feature selection experiments are based on the results of vocalized no-tag parsing on the development set where the parser assigns the tags learned during the training phase.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7302522659301758}, {"text": "vocalized no-tag parsing", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7312914530436198}]}, {"text": "However we also provide the results of gold tag and unvocalized no-tag parsing in our final experiments.", "labels": [], "entities": [{"text": "no-tag parsing", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.640559196472168}]}, {"text": "We measure quality and coverage of the output trees using the standard EVALB).", "labels": [], "entities": [{"text": "EVALB", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.5037546753883362}]}, {"text": "The initial results on parsing are presented in  The results presented in correspond to the non-iterative method.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9706736207008362}]}, {"text": "Line 6 shows that the improvement for this configuration is mainly due to adding feature 1 (determiner) and lines 7-8 show that feature 11 (proper noun) is helpful only when it is associated with feature 6 (mood).", "labels": [], "entities": []}, {"text": "presents the results of the greedyiterative method using a single heuristic function (left) and a merged heuristic function (right).", "labels": [], "entities": []}, {"text": "The method using single heuristic function was also employed by.", "labels": [], "entities": []}, {"text": "Grey rows indicate that the feature decreased the F -score, and thus was discarded.", "labels": [], "entities": [{"text": "F -score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9948008060455322}]}, {"text": "The number of iterations with this algorithm is fixed at f -1 after the initial heuristic function is obtained, where f is the number of morphological features.", "labels": [], "entities": []}, {"text": "We can observe that the features 11,13 (proper noun & negation) work together to produce the highest jump in the score.", "labels": [], "entities": []}, {"text": "In contrast with the results of Marton et al.", "labels": [], "entities": []}, {"text": "provides afield where morphological features are specified individually.", "labels": [], "entities": []}, {"text": "This encourages dependency parsers to inspect individual components of a tag, and make use of specific features when helpful.", "labels": [], "entities": []}, {"text": "The CoNLL-X format is used by most dependency parsers, including the MaltParser used in.", "labels": [], "entities": []}, {"text": "The landscape of the search space contains many local maxima, which can prevent (nonbacktracking) greedy algorithms from exploring paths that eventually lead to high scores.", "labels": [], "entities": []}, {"text": "Consider the case below: 82.56 fruitful 1,10,11,13,14 , never arriving at the highest feature combination 1,4,8,10,11,14 . Features 4 (aspect) and 8 (state) may work together to provide a useful distinction for parsing that individually would otherwise only increase sparsity.", "labels": [], "entities": [{"text": "parsing", "start_pos": 211, "end_pos": 218, "type": "TASK", "confidence": 0.9637864232063293}]}, {"text": "gives the results of using the bestfirst with backtracking algorithm.", "labels": [], "entities": []}, {"text": "We join the highest-ranked individual feature (12: genitive clitics) with all other individual features (i.e. {12, 1} {12, 2} {12, 3} . .", "labels": [], "entities": []}, {"text": "), and select the combination achieving the highest F -score ({12, 11}).", "labels": [], "entities": [{"text": "F -score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9951940178871155}]}, {"text": "The initial heuristic function h \u22a5+1 (x) is used only in the first iteration (to select feature 12 in our case).", "labels": [], "entities": []}, {"text": "Afterward, we join the resulting set with all other remaining individual features (i.e. {12, 11, 1} {12, 11, 2} {12, 11, 3} . .", "labels": [], "entities": []}, {"text": "), and select the combination achieving the highest score.", "labels": [], "entities": []}, {"text": "With each iteration, the number of remaining features is decremented by one.", "labels": [], "entities": []}, {"text": "While this algorithm is exhaustive, we have not explored all possible combinations.", "labels": [], "entities": []}, {"text": "We also investigated a probabilistic search algorithm to see how well it could overcome the challenges posed by many local maxima.", "labels": [], "entities": []}, {"text": "Using simulated annealing), we performed 50 experiments, with 50 cycles each.", "labels": [], "entities": []}, {"text": "The mean of the final dev-set F -scores was 82.84, 17 82.87 . .", "labels": [], "entities": [{"text": "mean", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9775266647338867}, {"text": "F -scores", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9828096230824789}]}], "tableCaptions": [{"text": " Table 2: Parsing results (F -score) for including features", "labels": [], "entities": [{"text": "F -score)", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9845920503139496}]}, {"text": " Table 3: Parsing results for the non-iterative method. The", "labels": [], "entities": []}, {"text": " Table 7: Final F -scores on the vocalized development  set for the best feature combination, and handling un- known words. The best feature combination included  the following features: genitive clitics, proper noun,  negation, and determiner. Statistically significant with", "labels": [], "entities": [{"text": "F", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.5028122663497925}]}, {"text": " Table 8: Final F -scores on the vocalized test set for  the best feature combination, and handling unknown  words. Statistically significant with *=p < 0.05, **=p <", "labels": [], "entities": [{"text": "Final F -scores", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8289965242147446}]}, {"text": " Table 9: Final F -scores on the unvocalized develop- ment set for the best feature combination, and han- dling unknown words. The best feature combination  included the following features: genitive clitics, proper  noun, negation, and determiner. Statistically significant", "labels": [], "entities": [{"text": "Final F -scores", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8364243060350418}]}, {"text": " Table 10: Final F -scores on the unvocalized test set for  the best feature combination, and handling unknown  words. Statistically significant with *=p < 0.05, **=p <", "labels": [], "entities": [{"text": "Final F -scores", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.8518197536468506}]}]}