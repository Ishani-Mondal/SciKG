{"title": [{"text": "Twitter Polarity Classification with Label Propagation over Lexical Links and the Follower Graph", "labels": [], "entities": [{"text": "Twitter Polarity Classification", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7119637727737427}]}], "abstractContent": [{"text": "There is high demand for automated tools that assign polarity to microblog content such as tweets (Twitter posts), but this is challenging due to the terseness and informality of tweets in addition to the wide variety and rapid evolution of language in Twitter.", "labels": [], "entities": []}, {"text": "It is thus impractical to use standard supervised machine learning techniques dependent on annotated training examples.", "labels": [], "entities": []}, {"text": "We do without such annotations by using label propagation to incorporate labels from a maximum entropy classifier trained on noisy labels and knowledge about word types encoded in a lexicon, in combination with the Twitter follower graph.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7356723248958588}]}, {"text": "Results on polarity classification for several datasets show that our label propagation approach rivals a model supervised with in-domain annotated tweets, and it outperforms the noisily supervised classifier it exploits as well as a lexicon-based polarity ratio classifier.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7377296835184097}, {"text": "label propagation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7341119199991226}]}], "introductionContent": [{"text": "Twitter is a microblogging service where users post messages (\"tweets\") of no more than 140 characters.", "labels": [], "entities": []}, {"text": "With around 200 million users generating 140 million tweets per day, Twitter represents one of the largest and most dynamic datasets of user generated content.", "labels": [], "entities": []}, {"text": "Along with other social networking websites such as Facebook, the content on Twitter is real time: tweets about everything from a friend's birthday to a devastating earthquake can be found posted during and immediately after an event in question.", "labels": [], "entities": []}, {"text": "This vast stream of real time data has major implications for any entity interested in public opinion and even acting on what is learned and engaging with the public directly.", "labels": [], "entities": []}, {"text": "Companies have the opportunity to examine what customers and potential customers are saying about their products and services without costly and time-consuming surveys or explicit requests for feedback.", "labels": [], "entities": []}, {"text": "Political organizations and candidates might be able to determine what issues the public is most interested in, as well as where they stand on those issues.", "labels": [], "entities": []}, {"text": "Manual inspection of tweets can be useful for many such analyses, but many applications and questions require realtime analysis of massive amounts of social media content.", "labels": [], "entities": []}, {"text": "Computational tools that automatically extract and analyze relevant information about opinion expressed on Twitter and other social media sources are thus in high demand.", "labels": [], "entities": []}, {"text": "Full sentiment analysis fora given question or topic requires many stages, including but not limited to: (1) extraction of tweets based on an initial query, (2) filtering out spam and irrelevant items from those tweets, (3) identifying subjective tweets, and (4) identifying the polarity of those tweets.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.739521861076355}]}, {"text": "Like most work in sentiment analysis, we focus on the last stage, polarity classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.9600143134593964}, {"text": "polarity classification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7518253028392792}]}, {"text": "The simplest approaches are based on the presence of words or emoticons that are indicators of positive or negative polarity (e.g. Twitter's own API, O'), or calculating a ratio of positive to negative terms (.", "labels": [], "entities": []}, {"text": "Though these area useful first pass, the nuance of language often defeats them.", "labels": [], "entities": []}, {"text": "Tweets provide additional challenges compared to edited text; e.g. they are short and include informal/colloquial/abbreviated language.", "labels": [], "entities": []}, {"text": "53 Standard supervised classification methods improve the situation somewhat (), but these require texts labeled with polarity as input and they do not adapt to changes in language use.", "labels": [], "entities": []}, {"text": "One way around this is to use noisy labels (also referred to as \"distant supervision\"), e.g. by taking emoticons like ':)' as positive and ':(' as negative, and train a standard classifier).", "labels": [], "entities": []}, {"text": "1 Semi-supervised methods can also reduce dependence on labeled texts: for example, use a polarity lexicon combined with label propagation.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7197318226099014}]}, {"text": "Several have used label propagation starting with a small number of hand-labeled words to induce a lexicon for use in polarity classification.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7288068234920502}, {"text": "polarity classification", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.7725091874599457}]}, {"text": "In this paper, we bring together several of the above approaches via label propagation using modified adsorption.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7574590444564819}]}, {"text": "This also allows us to explore the possibility of exploiting the Twitter follower graph to improve polarity classification, under the assumption that people influence one another or have shared affinities about topics.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.7097854167222977}]}, {"text": "We construct a graph that has users, tweets, word unigrams, word bigrams, hashtags, and emoticons as its nodes; users are connected based on the Twitter follower graph, users are connected to the tweets they created, and tweets are connected to the unigrams, bigrams, hashtags and emoticons they contain.", "labels": [], "entities": []}, {"text": "We seed the graph using the polarity values in the OpinionFinder lexicon (), the known polarity of emoticons, and a maximum entropy classifier trained on 1.8 million tweets with automatically assigned labels based on the presence of positive and negative emoticons, like and.", "labels": [], "entities": []}, {"text": "We compare the label propagation approach to the noisily supervised classifier itself and to a standard lexicon-based method using positive/negative ratios.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7743095457553864}]}, {"text": "Evaluation is performed on several datasets of tweets that have been annotated for polarity: the Stanford Twitter Sentiment set (), tweets from the 2008 debate between Obama and McCain (, and anew dataset of tweets about healthcare reform that we have created.", "labels": [], "entities": [{"text": "Stanford Twitter Sentiment set", "start_pos": 97, "end_pos": 127, "type": "DATASET", "confidence": 0.7941034287214279}]}, {"text": "In addition to performing standard per-tweet accuracy, we also measure per-target accuracy (for healthcare reform) and an aggregate error metric overall users in our test set that captures how similar predicted positivity of each user is to their actual positivity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9120259881019592}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.5125219225883484}]}, {"text": "Across all datasets and measures, we find that label propagation is consistently better than the noisily supervised classifier, which in turn outperforms the lexicon-based method.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7134948819875717}]}, {"text": "Additionally, for the healthcare reform dataset, the label propagation approach-which uses no gold labeled tweets, just a hand-created lexicon-outperforms a maximum entropy classifier trained on gold labels.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7223510295152664}]}, {"text": "However, we do not find the follower graph to improve performance with our current implementation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use several different Twitter datasets as training or evaluation resources.", "labels": [], "entities": []}, {"text": "From the annotated datasets, only tweets with positive or negative polarity are used, so neutral tweets are ignored.", "labels": [], "entities": []}, {"text": "While important, subjectivity detection is largely a different problem from polarity classification.", "labels": [], "entities": [{"text": "subjectivity detection", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8141946196556091}, {"text": "polarity classification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8691928386688232}]}, {"text": "For example, use minimum cuts in graphs for the former and machine-learned text classification for the latter.", "labels": [], "entities": [{"text": "machine-learned text classification", "start_pos": 59, "end_pos": 94, "type": "TASK", "confidence": 0.6393233637015024}]}, {"text": "We also do not give any special treatment to retweets, though doing so is a possible future improvement.", "labels": [], "entities": []}, {"text": "Three annotated datasets, summarized in and described below, are used for training, development, or evaluation of polarity classifiers.", "labels": [], "entities": []}, {"text": "Stanford Twitter Sentiment (STS).", "labels": [], "entities": [{"text": "Stanford Twitter Sentiment (STS)", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.8266356786092123}]}, {"text": "We filter this dataset with two constraints in order to ensure high inter-annotator agreement.", "labels": [], "entities": []}, {"text": "First, at least three votes must have been provided fora tweet to be included.", "labels": [], "entities": []}, {"text": "Second, more than half of the votes must have been positive or negative; the majority label is taken as the gold standard for that tweet.", "labels": [], "entities": []}, {"text": "This results in a set of 1,898 tweets.", "labels": [], "entities": []}, {"text": "Of these, 705 had positive gold labels and 1192 had negative gold labels, and the average inter-annotator agreement of the Turk votes for these tweets was 83.7%.", "labels": [], "entities": [{"text": "inter-annotator agreement", "start_pos": 90, "end_pos": 115, "type": "METRIC", "confidence": 0.6951413154602051}]}, {"text": "To our knowledge, we are the first to perform automatic polarity classification on this dataset.", "labels": [], "entities": [{"text": "automatic polarity classification", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.6254052321116129}]}, {"text": "Health Care Reform (HCR).", "labels": [], "entities": [{"text": "Health Care Reform (HCR)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8164953788121542}]}, {"text": "We create anew annotated dataset based on tweets about healthcare reform in the USA.", "labels": [], "entities": []}, {"text": "This was a strongly debated topic that created a large number of polarized tweets, especially in the run up to the signing of the healthcare bill on March 23, 2010.", "labels": [], "entities": []}, {"text": "We extract tweets containing the healthcare reform hashtag \"#hcr\" from early 2010; a subset of these are annotated by us and colleagues for polarity (positive, negative, neutral, irrelevant) and polarity targets (health care reform, Obama, Democrats, Republicans, Tea Party, conservatives, liberals, and Stupak).", "labels": [], "entities": []}, {"text": "These are separated into training, dev and test sets.", "labels": [], "entities": []}, {"text": "As with the other datasets, we restrict attention in this paper only to positive and negative tweets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Positive and negative emoticons.", "labels": [], "entities": []}, {"text": " Table 2: Top 20 most predictive common unigram fea- tures for the positive and negative classes, in order from  more predictive to less predictive.", "labels": [], "entities": []}, {"text": " Table 5: Mean squared error (MSE) per-user on HCR- TEST, for users with at least 3 tweets", "labels": [], "entities": [{"text": "Mean squared error (MSE)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.980458676815033}, {"text": "HCR- TEST", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.8502808411916097}]}, {"text": " Table 4: Per-tweet accuracy percentages. The models and parameters were developed while tracking performance on  STS, OMD, and HCR-DEV, and HCR-TEST results were obtained from a single, blind run.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.8826761245727539}, {"text": "STS", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.8282837271690369}, {"text": "OMD", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.770087718963623}, {"text": "HCR-DEV", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9373352527618408}]}, {"text": " Table 6: Per-target accuracy percentages for HCR-TEST. The number of tweets for each target is given in parentheses.", "labels": [], "entities": [{"text": "accuracy percentages", "start_pos": 21, "end_pos": 41, "type": "METRIC", "confidence": 0.8976735174655914}, {"text": "HCR-TEST", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.7259330153465271}]}]}