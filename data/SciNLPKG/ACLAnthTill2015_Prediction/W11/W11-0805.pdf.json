{"title": [{"text": "Detecting Multi-Word Expressions improves Word Sense Disambiguation", "labels": [], "entities": [{"text": "Detecting Multi-Word Expressions", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8663657108942667}, {"text": "Word Sense Disambiguation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.6687439779440562}]}], "abstractContent": [{"text": "Multi-Word Expressions (MWEs) are prevalent in text and are also, on average, less poly-semous than mono-words.", "labels": [], "entities": [{"text": "Multi-Word Expressions (MWEs)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7451451539993286}]}, {"text": "This suggests that accurate MWE detection should lead to a non-trivial improvement in Word Sense Disam-biguation (WSD).", "labels": [], "entities": [{"text": "MWE detection", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9867063760757446}, {"text": "Word Sense Disam-biguation (WSD)", "start_pos": 86, "end_pos": 118, "type": "METRIC", "confidence": 0.6035265624523163}]}, {"text": "We show that a straightforward MWE detection strategy, due to Ar-ranz et al.", "labels": [], "entities": [{"text": "MWE detection", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.987235814332962}]}, {"text": "(2005), can increase a WSD al-gorithm's baseline f-measure by 5 percentage points.", "labels": [], "entities": [{"text": "WSD al-gorithm's baseline f-measure", "start_pos": 23, "end_pos": 58, "type": "METRIC", "confidence": 0.6679631531238556}]}, {"text": "Our measurements are consistent with Arranz's, and our study goes further by using a portion of the Semcor corpus containing 12,449 MWEs-over 30 times more than the approximately 400 used by Arranz.", "labels": [], "entities": [{"text": "Semcor corpus", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9465080201625824}]}, {"text": "We also show that perfect MWE detection over Sem-cor only nets a total 6 percentage point increase in WSD f-measure; therefore there is little room for improvement over the results presented here.", "labels": [], "entities": [{"text": "MWE detection", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9560674130916595}, {"text": "WSD f-measure", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.6277837455272675}]}, {"text": "We provide our MWE detection algorithms, along with a general detection framework, in a free, open-source Java library called jMWE.", "labels": [], "entities": [{"text": "MWE detection", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.971957802772522}]}, {"text": "Multi-word expressions (MWEs) are prevalent in text.", "labels": [], "entities": [{"text": "Multi-word expressions (MWEs)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.624270212650299}]}, {"text": "This is important for the classic task of Word Sense Disambiguation (WSD) (Agirre and Ed-monds, 2007), in which an algorithm attempts to assign to each word in a text the appropriate entry from a sense inventory.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7654178341229757}]}, {"text": "A WSD algorithm that cannot correctly detect the MWEs that are listed in its sense inventory will not only miss those sense assignments, it will also spuriously assign senses to MWE constituents that themselves have sense entries, dealing a double-blow to WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 2, "end_pos": 5, "type": "TASK", "confidence": 0.9338296055793762}, {"text": "WSD", "start_pos": 256, "end_pos": 259, "type": "TASK", "confidence": 0.9637328386306763}]}, {"text": "Beyond this penalty, MWEs listed in a sense inventory also present an opportunity to WSD algorithms they are, on average, less polysemous than mono-words.", "labels": [], "entities": []}, {"text": "In Wordnet 1.6, multi-words have an average polysemy of 1.07, versus 1.53 for mono-words.", "labels": [], "entities": [{"text": "Wordnet 1.6", "start_pos": 3, "end_pos": 14, "type": "DATASET", "confidence": 0.9542073011398315}]}, {"text": "As a concrete example, consider sentence She broke the world record.", "labels": [], "entities": []}, {"text": "In Wordnet 1.6 the lemma world has nine different senses and record has fourteen, while the MWE world record has only one.", "labels": [], "entities": [{"text": "Wordnet 1.6", "start_pos": 3, "end_pos": 14, "type": "DATASET", "confidence": 0.9231754541397095}, {"text": "MWE world record", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.7846291661262512}]}, {"text": "If a WSD algorithm correctly detects MWEs, it can dramatically reduce the number of possible senses for such sentences.", "labels": [], "entities": []}, {"text": "Measure Us Arranz Number of MWEs 12,449 382 Fraction of MWEs 7.4% 9.4% WSD impr.", "labels": [], "entities": [{"text": "Arranz Number", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.9203069806098938}, {"text": "Fraction", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9601234197616577}, {"text": "WSD", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.49770423769950867}]}, {"text": "(Best v. Baseline) 0.016 F 1 0.012 F 1 WSD impr.", "labels": [], "entities": [{"text": "Baseline) 0.016 F 1 0.012 F 1 WSD", "start_pos": 9, "end_pos": 42, "type": "METRIC", "confidence": 0.6850921511650085}]}, {"text": "(Baseline v. None) 0.033 F 1-WSD impr.", "labels": [], "entities": [{"text": "F 1-WSD", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9196118116378784}]}, {"text": "(Best v. None) 0.050 F 1-WSD impr.", "labels": [], "entities": [{"text": "F 1-WSD", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9719411134719849}]}, {"text": "(Perfect v. None) 0.061 F 1-Table 1: Improvement of WSD f-measures over an MWE-unaware WSD strategy for various MWE detection strategies.", "labels": [], "entities": [{"text": "F 1-Table", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9128879904747009}, {"text": "MWE detection", "start_pos": 112, "end_pos": 125, "type": "TASK", "confidence": 0.9618412256240845}]}, {"text": "Baseline, Best, and Perfect refer to the MWE detection strategy used in the WSD preprocess.", "labels": [], "entities": [{"text": "Best", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9572845697402954}, {"text": "Perfect", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9819522500038147}, {"text": "MWE detection", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9757575392723083}, {"text": "WSD preprocess", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.6462122201919556}]}, {"text": "With this in mind, we expected that accurate MWE detection will lead to a small yet non-trivial improvement in WSD performance, and this is indeed the case.", "labels": [], "entities": [{"text": "MWE detection", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9848026037216187}, {"text": "WSD", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9736430048942566}]}, {"text": "Table 1 summarizes our results.", "labels": [], "entities": []}, {"text": "In particular, a relatively straightforward MWE detection strategy, here called the 'best' strategy and due to Arranz et al.", "labels": [], "entities": [{"text": "MWE detection", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9895511567592621}]}, {"text": "(2005), yielded a 5 percentage point improvement 1 in WSD f-measure.", "labels": [], "entities": [{"text": "WSD f-measure", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.6287200897932053}]}, {"text": "We also measured an improvement similar to that of Arranz when 1 For example, if the WSD algorithm has an f-measure of 20", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Improvement of WSD f-measures over an  MWE-unaware WSD strategy for various MWE detec- tion strategies. Baseline, Best, and Perfect refer to the  MWE detection strategy used in the WSD preprocess.", "labels": [], "entities": [{"text": "Best", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9685113430023193}, {"text": "Perfect", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.9885819554328918}, {"text": "MWE detection", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.9300549626350403}, {"text": "WSD preprocess", "start_pos": 191, "end_pos": 205, "type": "TASK", "confidence": 0.7355550229549408}]}, {"text": " Table 2: All the relevant numbers for the study. For purposes of comparison we recalculated the token counts for the  gold-annotated portion of the XWN corpus, and found discrepancies with Arranz's reported values. They reported  1300 fully-gold-annotated glosses containing 397 MWEs; we found 1307 glosses containing 382 MWEs. The table  contains our token counts, but Arranz's actual MWE detection and WSD f-measures, precisions, and recalls.", "labels": [], "entities": [{"text": "XWN corpus", "start_pos": 149, "end_pos": 159, "type": "DATASET", "confidence": 0.9644052088260651}, {"text": "MWE detection", "start_pos": 387, "end_pos": 400, "type": "TASK", "confidence": 0.8487255573272705}, {"text": "WSD", "start_pos": 405, "end_pos": 408, "type": "TASK", "confidence": 0.6483582854270935}, {"text": "precisions", "start_pos": 421, "end_pos": 431, "type": "METRIC", "confidence": 0.9895215034484863}, {"text": "recalls", "start_pos": 437, "end_pos": 444, "type": "METRIC", "confidence": 0.9908902049064636}]}]}