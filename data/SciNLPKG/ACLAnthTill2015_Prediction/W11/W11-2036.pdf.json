{"title": [{"text": "Examining the Impacts of Dialogue Content and System Automation on Affect Models in a Spoken Tutorial Dialogue System", "labels": [], "entities": []}], "abstractContent": [{"text": "Many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users' affect.", "labels": [], "entities": []}, {"text": "Previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7295766323804855}]}, {"text": "Thus, we wish to investigate how more minor differences, like small dialogue content changes and switching from a wizarded system to a fully automated system, influence the performance of our affect detection models.", "labels": [], "entities": [{"text": "affect detection", "start_pos": 192, "end_pos": 208, "type": "TASK", "confidence": 0.7353322505950928}]}, {"text": "We perform a post-hoc experiment where we use various data sets to train multiple models , and compare against a test set from the most recent version of our dialogue system.", "labels": [], "entities": []}, {"text": "Analyzing these results strongly suggests that these differences do impact these models' performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Many dialogue system developers use data gathered from previous versions of a system to train models for analyzing users' interactions with later versions of the system in new ways, e.g. detecting users' affect enables the system to respond more appropriately.", "labels": [], "entities": []}, {"text": "However, this training data does not always accurately reflect the current version of the system.", "labels": [], "entities": []}, {"text": "In particular, differences in the levels of automation and the presentation of dialogue content commonly vary between versions.", "labels": [], "entities": []}, {"text": "For example,) changed dialogue strategies for their Let's Go bus information system after real-world testing.", "labels": [], "entities": [{"text": "Let's Go bus information system", "start_pos": 52, "end_pos": 83, "type": "DATASET", "confidence": 0.7710838715235392}]}, {"text": "Previous work in dialogue systems with regards to analyzing the impact of using differing training data has primarily been in the domain adaptation field, and has focused on two areas.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7447374165058136}]}, {"text": "First, previous work empirically analyzed the need for domain adaptation, i.e. methods for porting existing classifiers to unrelated domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.739915132522583}]}, {"text": "For example, Webb and Liu (2008) developed a cue-phrase-based dialogue act classifier using the Switchboard corpus, and tested on call center data.", "labels": [], "entities": [{"text": "cue-phrase-based dialogue act classifier", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.5962565541267395}, {"text": "Switchboard corpus", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.9035001695156097}]}, {"text": "While this performed reasonably, training on the call center corpus and testing on Switchboard performed poorly.", "labels": [], "entities": [{"text": "call center corpus", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.5990113715330759}]}, {"text": "The second research direction involves proposing methods for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8441316187381744}]}, {"text": "observed similar poor performance when porting their dialogue act classifier between three corpora: Switchboard, the Meeting Recorder Dialog Act corpus, and a machine-translated version of the Spanish Callhome corpus.", "labels": [], "entities": [{"text": "Meeting Recorder Dialog Act corpus", "start_pos": 117, "end_pos": 151, "type": "DATASET", "confidence": 0.9383840322494507}, {"text": "Spanish Callhome corpus", "start_pos": 193, "end_pos": 216, "type": "DATASET", "confidence": 0.6932953198750814}]}, {"text": "They report promising results through varying their feature set.", "labels": [], "entities": []}, {"text": "also observed poor performance and the need for adaptation when porting product review sentiment classifiers.", "labels": [], "entities": [{"text": "porting product review sentiment classifiers", "start_pos": 64, "end_pos": 108, "type": "TASK", "confidence": 0.8845578670501709}]}, {"text": "They used four review corpora from Amazon (books, DVDs, electronics, and small appliances), which yielded 12 cross-domain training/testing pairs.", "labels": [], "entities": []}, {"text": "Their algorithmic adaptation methods showed promising results.", "labels": [], "entities": [{"text": "algorithmic adaptation", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.7306574583053589}]}, {"text": "Our work is in the first direction, as we also empirically analyze the impact of differences in training and testing corpora to demonstrate the need for adaptation methods.", "labels": [], "entities": []}, {"text": "However, our work differs from domain adaptation, as the corpora in this experiment all come from one intelligent spoken physics tutor.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7552388906478882}]}, {"text": "Instead, we analyze differences resulting from vary-ing levels of automation and small changes in dialogue content between versions of our system.", "labels": [], "entities": []}, {"text": "With respect to analyzing automation, we empirically compare the impact of differences in training on data from wizarded (WOZ) versus fully automated systems.", "labels": [], "entities": []}, {"text": "Though many systems use data from a WOZ version of the system to train models which are then used in fully automated versions of the system, the effectiveness of this method of dialogue system development has not been tested.", "labels": [], "entities": [{"text": "dialogue system development", "start_pos": 177, "end_pos": 204, "type": "TASK", "confidence": 0.6880029439926147}]}, {"text": "We hypothesize that models built with automated data will outperform models built with wizarded data.", "labels": [], "entities": []}, {"text": "Additionally, minor dialogue content changes typically exist between versions of systems.", "labels": [], "entities": []}, {"text": "While large changes, like changing domains, have been shown to affect model performance, no work has investigated the impact of these more minute changes.", "labels": [], "entities": []}, {"text": "We hypothesize that these differences in dialogue content presentation will also affect the models.", "labels": [], "entities": [{"text": "dialogue content presentation", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6392088631788889}]}, {"text": "Finally, the amount of training data is a well known factor which affects performance of models built using supervised machine learning.", "labels": [], "entities": []}, {"text": "We hypothesize that combining some, but not all, types of training corpora will improve the performance of the trained models, e.g. adding automated data to WOZ data will improve performance, as this provides fully automated examples.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 157, "end_pos": 165, "type": "DATASET", "confidence": 0.8249596059322357}]}, {"text": "We hypothesize only providing more WOZ data will not be as useful.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.6936061680316925}]}], "datasetContent": [{"text": "In this post-hoc analysis, we will analyze the impact of content differences by comparing the performance of models built with WOZ-07 and WOZ-08, and automation differences by comparing models built with WOZ-08 and ASR-08 data.", "labels": [], "entities": [{"text": "WOZ-07", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.9607616662979126}, {"text": "WOZ-08", "start_pos": 138, "end_pos": 144, "type": "DATASET", "confidence": 0.5650972723960876}, {"text": "WOZ-08 and ASR-08 data", "start_pos": 204, "end_pos": 226, "type": "DATASET", "confidence": 0.7422778606414795}]}, {"text": "Instead of the original study design, where WOZ-08 and ASR-08 subjects were run in parallel, we could have gathered the WOZ data first, and used the WOZ data and the first few ASR users for system evaluation and development purposes.", "labels": [], "entities": [{"text": "WOZ-08", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.8746921420097351}, {"text": "WOZ data", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.8563382923603058}, {"text": "WOZ data", "start_pos": 149, "end_pos": 157, "type": "DATASET", "confidence": 0.8916482627391815}]}, {"text": "Thus, for the post-hoc analysis, we mimic this by using WOZ-08 as a training set, and splitting ASR-08 into two data sets-ASR-08-Train (the first few users), and ASR-08-Test.", "labels": [], "entities": [{"text": "WOZ-08", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.8805537819862366}, {"text": "ASR-08-Test", "start_pos": 162, "end_pos": 173, "type": "DATASET", "confidence": 0.7677366137504578}]}, {"text": "(Please seethe last two rows of.)", "labels": [], "entities": []}, {"text": "We held out the first 19 users for ASR-08-Train, since this approximates the amount of data used to train the model built with WOZ-08.", "labels": [], "entities": [{"text": "ASR-08-Train", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.5657644867897034}, {"text": "WOZ-08", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.9487853646278381}]}, {"text": "For our post-hoc study, the remaining 53 ASR users were used as a test set for all training sets, to mimic an authentic development lifestyle fora dialogue system.", "labels": [], "entities": []}, {"text": "Additionally, this guaranteed that no users appear in both the training and testing set given any training set.", "labels": [], "entities": []}, {"text": "As all uncertainty remediation happens at the turn-level, we classified uncertainty at the turn-level, and compared these automated results with the goldstandard annotations.", "labels": [], "entities": [{"text": "uncertainty remediation", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.6867136508226395}]}, {"text": "We used all the features that were designed for the original model.", "labels": [], "entities": []}, {"text": "Since previous experiments with our data showed little variance between different machine learning algorithms, we chose a J48 decision tree, implemented by WEKA, 1 for all experiments due to its easy readability.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 156, "end_pos": 160, "type": "DATASET", "confidence": 0.6595196723937988}]}, {"text": "Since our class distribution is skewed (see), we also used a cost matrix which heavily penalizes classifying an uncertain instance ascertain.", "labels": [], "entities": []}, {"text": "We use simple lexical, prosodic and systemspecific features described in) to build our models.", "labels": [], "entities": []}, {"text": "These features were kept constant through all experiments, so the results could be directly comparable.", "labels": [], "entities": []}, {"text": "For all lexical features for all data sets, ASR text was used.", "labels": [], "entities": []}, {"text": "For all WOZ conditions, we gathered ASR text post-hoc.", "labels": [], "entities": [{"text": "WOZ", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.7110469937324524}, {"text": "ASR", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8390292525291443}]}, {"text": "We trained models on individual training sets, to inspect the impact of content and automation differences.", "labels": [], "entities": []}, {"text": "We then trained new models on combinations of these original training sets, to investigate possible interactions.", "labels": [], "entities": []}, {"text": "To allow for direct comparison, we used ASR-08-Test to evaluate all models.", "labels": [], "entities": [{"text": "ASR-08-Test", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.6441220641136169}]}, {"text": "Since detecting uncertainty is related to detecting affective user states, we use the evaluation measures Unweighted Average (UA) Recall and UA Precision, presented in ().We also use UA F-measure.", "labels": [], "entities": [{"text": "Unweighted Average (UA) Recall", "start_pos": 106, "end_pos": 136, "type": "METRIC", "confidence": 0.9467221399148306}, {"text": "UA Precision", "start_pos": 141, "end_pos": 153, "type": "METRIC", "confidence": 0.7543362677097321}, {"text": "UA", "start_pos": 183, "end_pos": 185, "type": "DATASET", "confidence": 0.6579263806343079}, {"text": "F-measure", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.8391739726066589}]}, {"text": "Note that because only one hold-out evaluation set was used, rather than using multiple sets for cross-fold validation, we do not test for statistical significance between models' results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Description of data sets", "labels": [], "entities": []}, {"text": " Table 2: Results; Testing on ASR-08-Test (n = 5305). Bold denotes best performance per metric.", "labels": [], "entities": [{"text": "ASR-08-Test", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.4850819706916809}]}, {"text": " Table 3: Dialogue-level description of corpora", "labels": [], "entities": []}]}