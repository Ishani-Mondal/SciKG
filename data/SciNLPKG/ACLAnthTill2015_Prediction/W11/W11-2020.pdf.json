{"title": [{"text": "Modeling and Predicting Quality in Spoken Human-Computer Interaction", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work we describe the modeling and prediction of Interaction Quality (IQ) in Spoken Dialogue Systems (SDS) using Support Vector Machines.", "labels": [], "entities": [{"text": "prediction of Interaction Quality (IQ)", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.5817367817674365}]}, {"text": "The model can be employed to estimate the quality of the ongoing interaction at arbitrary points in a spoken human-computer interaction.", "labels": [], "entities": []}, {"text": "We show that the use of 52 completely automatic features characterizing the system-user exchange significantly outperforms state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "The model is evaluated on publically available data from the CMU Let's Go Bus Information system.", "labels": [], "entities": [{"text": "CMU Let's Go Bus Information system", "start_pos": 61, "end_pos": 96, "type": "DATASET", "confidence": 0.9336149522236415}]}, {"text": "It reaches a performance of 61.6% un-weighted average recall when discriminating between 5 classes (good to very poor).", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.7825750112533569}]}, {"text": "It can be further shown that incorporating knowledge about the user's emotional state does hardly improve the performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "For years, the research community has been trying to model quality of Spoken Dialogue Systems (SDS) with statistical approaches.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems (SDS)", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.6660703917344412}]}, {"text": "Most vividly discussed has been the PARADISE approach which tries to map objective performance metrics of an SDS to subjective user ratings ().", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.909365177154541}]}, {"text": "The paradigm assumes that task success and dialogue costs contribute to user satisfaction which is the target variable in the model.", "labels": [], "entities": []}, {"text": "By that, an automatic evaluation of an SDS should be enabled.", "labels": [], "entities": []}, {"text": "While the intention of PARADISE is to evaluate and compare SDS or different system versions among each other, it is not suited to evaluate a spoken dialogue at arbitrary points during an interaction.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.7551774978637695}]}, {"text": "Such a model can be helpful fora number of reasons: Firstly, it allows fora prediction of critical dialogue situations.", "labels": [], "entities": [{"text": "prediction of critical dialogue situations", "start_pos": 76, "end_pos": 118, "type": "TASK", "confidence": 0.7482373714447021}]}, {"text": "These predictions could be employed to adapt the dialogue strategy or -in telephone applications with human assistance -escalate to human operators.", "labels": [], "entities": []}, {"text": "Secondly, it could help to uncover potentially weak dialogue design and point out problematic turns that need a re-design.", "labels": [], "entities": []}, {"text": "Thirdly, user satisfaction models help understand the satisfaction of the users.", "labels": [], "entities": []}, {"text": "In this study we present such a statistical model that is trained with a large set of domainindependent features taken from system logs and use additional manually created features, such as emotional state and dialogue acts, to create an upper baseline.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In Section 2 we present related work and discuss afterwards in Section 3 further issues that need to be addressed in this field.", "labels": [], "entities": []}, {"text": "There, we also disambiguate the term user satisfaction from Interaction Quality.", "labels": [], "entities": []}, {"text": "After that, we describe the annotation scheme as well as the rating process for modeling IQ and present, how we derive a generic label from the different raters' opinions in Section 4.", "labels": [], "entities": []}, {"text": "The input feature groups along with their features are presented in Section 5.", "labels": [], "entities": []}, {"text": "We anticipate that the problem is best modeled with Support Vector Machines (SVM), which is addressed in Section 6.", "labels": [], "entities": []}, {"text": "Ensuing, the performance of the model is evaluated.", "labels": [], "entities": []}, {"text": "In the first place, we analyze the impact of different feature groups on the SVM classifier in Section 7 and secondly, we optimize the model and determine the most relevant features for predicting the IQ score in Section 8.", "labels": [], "entities": []}, {"text": "A linear modeling approach of IQ by use of multivariate linear regression will be presented and discussed in Section 9 to obtain comparability with PARADISE.", "labels": [], "entities": [{"text": "IQ", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9679273366928101}, {"text": "PARADISE", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.7330262660980225}]}, {"text": "This study closes with a conclusion and a discussion in Section 10.", "labels": [], "entities": [{"text": "Section 10", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.5945455133914948}]}], "datasetContent": [{"text": "The skew distribution of the five classes requires the employment of an evaluation metric that weights the prediction of all classes equally.", "labels": [], "entities": []}, {"text": "Hence, a performance metric, such as accuracy, would not be a reliable measurement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996365308761597}]}, {"text": "We select the unweighted average recall (UAR) to assess the model performance.", "labels": [], "entities": [{"text": "unweighted average recall (UAR)", "start_pos": 14, "end_pos": 45, "type": "METRIC", "confidence": 0.807836135228475}]}, {"text": "Although it does not consider the severity of the error, i.e. predicting \"1\" for an IQ of \"5\" is considered as fatal as predicting \"4\", it has been proven to be superior to other evaluation metrics, see, where the UAR is called Match Rate per Rating (MR/R).", "labels": [], "entities": [{"text": "UAR", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.9322674870491028}, {"text": "Match Rate per Rating (MR/R)", "start_pos": 228, "end_pos": 256, "type": "METRIC", "confidence": 0.9669228593508402}]}, {"text": "It is defined as follows: where K is the number of classes, here \"5\", and 'match' is either '1' or '0' depending on whether the classifier's hypothesis H i for the class r matches the reference label R i . In the course of this work we will stick to the expression MR/R by reason of clearness.", "labels": [], "entities": []}, {"text": "We further list Cohen's \u03ba and Spearman's \u03c1 to make our work comparable to other studies but will use MR/R as central evaluation criterion and for feature selection.", "labels": [], "entities": [{"text": "MR/R", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9305217862129211}, {"text": "feature selection", "start_pos": 146, "end_pos": 163, "type": "TASK", "confidence": 0.7832841873168945}]}, {"text": "We have split all available data into two disjoint subsets consisting of 60% of the dialogues for training and testing via 10-fold cross-validation and the remaining 40% of the dialogues for optimization.", "labels": [], "entities": []}, {"text": "The dialogues have been selected randomly.", "labels": [], "entities": []}, {"text": "In order to assess the performance contribution of the single feature groups, we trained the SVM respectively with all features from the DAct, ASR, SLU and DM groups.", "labels": [], "entities": []}, {"text": "Further, we subsumed the groups ASR, SLU and DM as AUTO features since they can automatically be derived from logs without manual intervention.", "labels": [], "entities": [{"text": "AUTO", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.5952553749084473}]}, {"text": "In addition, the AUTOEMO group contains all AUTO features plus the emotion label.", "labels": [], "entities": [{"text": "AUTOEMO group", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.7428702414035797}]}, {"text": "Finally, the ALL group contains the AUTOEMO features plus the DAct features.", "labels": [], "entities": [{"text": "AUTOEMO", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.5218449234962463}]}, {"text": "For all groups, the support vector classifier has been trained and evaluated in 10-fold cross validation with the 3110 exchanges from the 118 training/testing dialogues.", "labels": [], "entities": []}, {"text": "The first turn of each dialogue has been excluded from the evaluation since each dialogue starts with a score of \"5\".", "labels": [], "entities": []}, {"text": "Results are depicted in the first half of  As can be seen, the model reaches a similar performance as () with MR/R=0.26, when trained with dialogue act features alone.", "labels": [], "entities": [{"text": "MR/R", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9441831906636556}]}, {"text": "The slightly higher performance of our model can potentially be explained by the lower number of classes (5 vs. 7), a different definition of the dialogue act set, the employment of Support Vector Machines instead of Hidden Markov Models or the difference in the target variable (IQ vs. closeness/smoothness/willingness).", "labels": [], "entities": [{"text": "IQ", "start_pos": 280, "end_pos": 282, "type": "METRIC", "confidence": 0.9366455674171448}]}, {"text": "It can be noted that the utilization of other features considerably outperforms dialogue act features.", "labels": [], "entities": []}, {"text": "Particularly the group of the ASR features alone reaches a performance of 60.5%.", "labels": [], "entities": [{"text": "ASR", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8887872695922852}]}, {"text": "The employment of all AUTO features delivers 58.4% which is 2.1% below the ASR features.", "labels": [], "entities": [{"text": "AUTO", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.561331033706665}]}, {"text": "Consequently, other variables seem to be less meaningful for predicting the Interaction Quality and seem to harm the performance of the SVM.", "labels": [], "entities": []}, {"text": "The knowledge of the emotional state of the user contributes with merely another 0.1% in comparison to the ASR features.", "labels": [], "entities": []}, {"text": "It can be assumed that the emotion feature increases the recognition rate of the lower IQ scores \"1\" and \"2\".", "labels": [], "entities": [{"text": "recognition rate", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.8415545225143433}, {"text": "IQ", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9098523259162903}]}, {"text": "However, this could not be confirmed: even when considering class-wise performance values a significant contribution of the emotion feature cannot be observed.", "labels": [], "entities": []}, {"text": "We also have to bear in mind that we employed hand-annotated emotions.", "labels": [], "entities": []}, {"text": "Emotion recognition itself is error-prone and a distinction of the emotional state of the caller with the employed annotation scheme can be expected with approximately 70%-80% UAR, see e.g..", "labels": [], "entities": [{"text": "Emotion recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9202285706996918}, {"text": "UAR", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.998099148273468}]}, {"text": "The influence of emotion recognition on the IQ distinction can be considered as limited and is insofar not surprising as the occurrence of strong anger in the data is not dominant (5.0%).", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7465754151344299}, {"text": "IQ distinction", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9736410677433014}]}, {"text": "The contribution of the single features to the classification result (across the groups they are assigned to) is analyzed in the following.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Agreement of single rater opinions to the merged  label when determined by mean and median, measured in  \u03ba, \u03c1 and accuracy. (*)=significantly higher (\u03b1 < 0.05)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.999782145023346}]}, {"text": " Table 2: Model performance after 10-fold cross valida- tion on training/test set. The first half comprises results  when all features of a group are employed. The second  half contains results after feature selection on the opti- mization set ((x/y)=where x is the number of features  used from all y available features.)", "labels": [], "entities": []}, {"text": " Table 3: Top 10 features on optimization set according to  IGR.", "labels": [], "entities": [{"text": "IGR", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.7716330885887146}]}, {"text": " Table 4: Confusion matrix including class-wise preci- sion and recall values after 10-fold cross validation (train- ing/test set) using the AUTO set. A (weighted average)  accuracy of 67.5% can be derived.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9954714775085449}, {"text": "AUTO set", "start_pos": 141, "end_pos": 149, "type": "DATASET", "confidence": 0.769958108663559}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.86958247423172}]}, {"text": " Table 5: Example dialogue (ID: 2061122025) from the CMU Let's Go System (2006 corpus) with low Interaction  Quality. The user utterances are printed in italic.", "labels": [], "entities": [{"text": "CMU Let's Go System (2006 corpus)", "start_pos": 53, "end_pos": 86, "type": "DATASET", "confidence": 0.951741185453203}]}, {"text": " Table 6: Rater guidelines for annotating Interaction Quality.", "labels": [], "entities": []}]}