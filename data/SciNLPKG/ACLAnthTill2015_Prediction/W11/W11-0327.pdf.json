{"title": [{"text": "Adapting Text instead of the Model: An Open Domain Approach", "labels": [], "entities": [{"text": "Adapting Text", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8863419592380524}]}], "abstractContent": [{"text": "Natural language systems trained on labeled data from one domain do not perform well on other domains.", "labels": [], "entities": []}, {"text": "Most adaptation algorithms proposed in the literature train anew model for the new domain using unlabeled data.", "labels": [], "entities": []}, {"text": "However , it is time consuming to retrain big models or pipeline systems.", "labels": [], "entities": []}, {"text": "Moreover, the domain of anew target sentence may not be known, and one may not have significant amount of unlabeled data for every new domain.", "labels": [], "entities": []}, {"text": "To pursue the goal of an Open Domain NLP (train once, test anywhere), we propose ADUT (ADaptation Using label-preserving Transformation), an approach that avoids the need for retraining and does not require knowledge of the new domain, or any data from it.", "labels": [], "entities": []}, {"text": "Our approach applies simple label-preserving transformations to the target text so that the transformed text is more similar to the training domain ; it then applies the existing model on the transformed sentences and combines the predictions to produce the desired prediction on the target text.", "labels": [], "entities": []}, {"text": "We instantiate ADUT for the case of Semantic Role Labeling (SRL) and show that it compares favorably with approaches that retrain their model on the target domain.", "labels": [], "entities": [{"text": "ADUT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.763535737991333}, {"text": "Semantic Role Labeling (SRL)", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.7805049270391464}]}, {"text": "Specifically, this \"on the fly\" adaptation approach yields 13% error reduction fora single parse system when adapting from the news wire text to fiction.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 63, "end_pos": 78, "type": "METRIC", "confidence": 0.9623048305511475}]}], "introductionContent": [{"text": "In several NLP tasks, systems trained on annotated data from one domain perform well when tested on the same domain but adapt poorly to other domains.", "labels": [], "entities": []}, {"text": "For example, all systems of CoNLL 2005 shared task) on Semantic Role Labeling showed a performance degradation of almost 10% or more when tested on a different domain.", "labels": [], "entities": [{"text": "CoNLL 2005 shared task)", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.8582907080650329}, {"text": "Semantic Role Labeling", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.704710861047109}]}, {"text": "Most works in domain adaptation have focused on learning a common representation across training and test domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7269356101751328}]}, {"text": "Using this representation, they retrain the model for every new domain.", "labels": [], "entities": []}, {"text": "But these are not Open Domain Systems since the model needs to be retrained for every new domain.", "labels": [], "entities": []}, {"text": "This is very difficult for pipeline systems like SRL where syntactic parser, shallow parser, POS tagger and then SRL need to be retrained.", "labels": [], "entities": [{"text": "SRL", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.965548574924469}]}, {"text": "Moreover, these methods need to have a lot of unlabeled data that is taken from the same domain, in order to learn meaningful feature correspondences across training and test domain.", "labels": [], "entities": []}, {"text": "These approaches cannot work when they do not have a lot of unlabeled data from the test domain or when the test domain in itself is very diverse, e.g., the web.", "labels": [], "entities": []}, {"text": "The contribution of this paper is anew framework for adaptation.", "labels": [], "entities": []}, {"text": "We propose ADUT (ADaptation Using label-preserving Transformation) as a framework in which a previously learned model can be used on an out-of-domain example without retraining and without looking at any labeled or unlabeled data for the domain of the new example.", "labels": [], "entities": []}, {"text": "The framework transforms the test sentence to generate sentences that have, in principle, identical labeling but that are more like instances from the training domain.", "labels": [], "entities": []}, {"text": "Consequently, it is expected that the exist-ing model will make better predictions on them.", "labels": [], "entities": []}, {"text": "All these predictions are then combined to choose the most probable and consistent prediction for the test sentence.", "labels": [], "entities": []}, {"text": "ADUT is a general technique which can be applied to any natural language task.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate its usefulness on the task of semantic role labeling).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7216480771700541}]}, {"text": "Starting with a system that was trained on the news text and does not perform well on fiction, we show that ADUT provides significant improvement on fiction, and is competitive with the performance of algorithms that were re-trained on the test domain.", "labels": [], "entities": [{"text": "ADUT", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.679716169834137}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses two motivating examples.", "labels": [], "entities": []}, {"text": "Section 3 gives a formal definition of our adaptation framework.", "labels": [], "entities": []}, {"text": "Section 4 describes the transformation operators that we applied for this task.", "labels": [], "entities": []}, {"text": "Section 5 presents our joint inference approach.", "labels": [], "entities": []}, {"text": "Section 6 describes our semantic role labeling system and our experimental results are in Section 7.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6089424788951874}]}, {"text": "Section 8 describes the related works for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7522716522216797}]}, {"text": "Finally in Section 9 we conclude the paper with a discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss our experimental setup for the semantic role labeling system.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.752657155195872}]}, {"text": "Similar to the CoNLL 2005 shared tasks, we train our system using sections 02-21 of the Wall Street Journal portion of Penn TreeBank labeled with PropBank.", "labels": [], "entities": [{"text": "CoNLL 2005 shared tasks", "start_pos": 15, "end_pos": 38, "type": "DATASET", "confidence": 0.8230322748422623}, {"text": "Wall Street Journal portion of Penn TreeBank", "start_pos": 88, "end_pos": 132, "type": "DATASET", "confidence": 0.9400865861347744}, {"text": "PropBank", "start_pos": 146, "end_pos": 154, "type": "DATASET", "confidence": 0.6084481477737427}]}, {"text": "We test our system on an annotated Brown corpus consisting of three sections (ck01 -ck03).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9455907046794891}]}, {"text": "Since we need to annotate new sentences with syntactic parse, POS tags and shallow parses, we do not use annotations in the CoNLL distribution; instead, we re-annotate the data using publicly available part of speech tagger and shallow parser 1 , Charniak 2005 parser) and Stanford parser (.", "labels": [], "entities": [{"text": "CoNLL distribution", "start_pos": 124, "end_pos": 142, "type": "DATASET", "confidence": 0.8613006770610809}]}, {"text": "Our baseline SRL model is an implementation of () which was the top performing system in CoNLL 2005 shared task.", "labels": [], "entities": [{"text": "SRL", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9403002858161926}, {"text": "CoNLL 2005 shared task", "start_pos": 89, "end_pos": 111, "type": "DATASET", "confidence": 0.8141230791807175}]}, {"text": "Due to space constraints, we omit the details of the system and refer readers to).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparing single parse system on Brown.", "labels": [], "entities": [{"text": "Brown", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.955545961856842}]}, {"text": " Table 3: Comparison of the multi parse system on Brown.", "labels": [], "entities": []}, {"text": " Table 4: Ablation Study for ADUT-Charniak", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9945504069328308}, {"text": "ADUT-Charniak", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.5462195873260498}]}, {"text": " Table 5: Performance on Infrequent Verbs for the Trans- formation of Replacement of Predicate", "labels": [], "entities": [{"text": "Trans- formation of Replacement of Predicate", "start_pos": 50, "end_pos": 94, "type": "TASK", "confidence": 0.8383032296385083}]}]}