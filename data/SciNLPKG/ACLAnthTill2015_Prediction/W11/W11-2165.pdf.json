{"title": [{"text": "Generative Models of Monolingual and Bilingual Gappy Patterns", "labels": [], "entities": []}], "abstractContent": [{"text": "A growing body of machine translation research aims to exploit lexical patterns (e.g., n-grams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7221328914165497}]}, {"text": "Typically, these \"gappy patterns\" are discovered using heuristics based on word alignments or local statistics such as mutual information.", "labels": [], "entities": []}, {"text": "In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps.", "labels": [], "entities": []}, {"text": "We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus.", "labels": [], "entities": []}, {"text": "We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.983848512172699}]}], "introductionContent": [{"text": "Beginning with the success of phrase-based translation models (, a trend arose of modeling larger and increasingly complex structural units in translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6178680658340454}]}, {"text": "One thread of work has focused on the use of lexical patterns with gaps.", "labels": [], "entities": []}, {"text": "proposed using phrase pairs with gaps in a phrase-based translation model, providing a heuristic method to extract gappy phrase pairs from wordaligned parallel corpora.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6172383278608322}]}, {"text": "The widely-used hierarchical phrase-based translation framework was introduced by and also relies on a simple heuristic for phrase pair extraction.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.6625602692365646}, {"text": "phrase pair extraction", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.7336356441179911}]}, {"text": "On the monolingual side, researchers have taken inspiration from trigger-based language modeling for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7689360082149506}]}, {"text": "Recently used monolingual trigger pairs to improve handling of long-distance dependencies in machine translation output.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.7938451170921326}]}, {"text": "All of this previous work used heuristics or local statistical tests to extract patterns from corpora.", "labels": [], "entities": []}, {"text": "In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps.", "labels": [], "entities": []}, {"text": "We exploit nonparametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text.", "labels": [], "entities": []}, {"text": "We first inspect these patterns manually and discuss the categories of phenomena that they capture.", "labels": [], "entities": []}, {"text": "We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights and incorporate them during decoding).", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.8827165365219116}]}, {"text": "We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.6353841722011566}]}], "datasetContent": [{"text": "We conducted evaluation to determine what types of phenomena are captured by the most probable patterns discovered by our models, and whether including the patterns as features can improve translation quality.", "labels": [], "entities": []}, {"text": "We consider the Spanish-to-English (ES\u2192EN) translation task from the ACL-2010 Workshop on Statistical Machine Translation).", "labels": [], "entities": [{"text": "Spanish-to-English (ES\u2192EN) translation", "start_pos": 16, "end_pos": 54, "type": "TASK", "confidence": 0.5589198384966169}, {"text": "Statistical Machine Translation", "start_pos": 90, "end_pos": 121, "type": "TASK", "confidence": 0.6075172821680704}]}, {"text": "We trained a Moses system ( ) following the baseline training instructions for the shared task.", "labels": [], "entities": []}, {"text": "In particular, we performed word alignment in each direction using GIZA++, used the \"grow-diag-final-and\" heuristic for symmetrization, and extracted phrase pairs up to a maximum length of seven.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7652156352996826}]}, {"text": "After filtering sentence pairs with one sentence longer than 50 words, we ended up with 1.45M sentence pairs of Europarl data and 91K sentence pairs of news commentary data.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.9917840361595154}, {"text": "news commentary data", "start_pos": 152, "end_pos": 172, "type": "DATASET", "confidence": 0.7823774615923563}]}, {"text": "Language models (N = 5) were estimated using the SRI language modeling toolkit) with modified Kneser-Ney smoothing).", "labels": [], "entities": [{"text": "SRI language modeling toolkit", "start_pos": 49, "end_pos": 78, "type": "DATASET", "confidence": 0.7437286376953125}]}, {"text": "Language models were trained on the target side of the parallel corpus as well as the first 5 million additional sentences from the extra English monolingual newswire data provided for the shared tasks.", "labels": [], "entities": []}, {"text": "We used news-test2008 for tuning and news-test2009 for testing.", "labels": [], "entities": []}, {"text": "We also consider Chinese-English (ZH\u2192EN) and followed a similar training procedure as above.", "labels": [], "entities": []}, {"text": "We used 303K sentence pairs from the FBIS corpus (LDC2003E14) and segmented the Chinese data using the Stanford Chinese segmenter in \"CTB\" mode (), giving us 7.9M Chinese words and 9.4M English words.", "labels": [], "entities": [{"text": "FBIS corpus (LDC2003E14", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.8976894468069077}]}, {"text": "A trigram language model was estimated using modified KneserNey smoothing from the English side of the parallel www.statmt.org/wmt10/baseline.html.", "labels": [], "entities": []}, {"text": "corpus concatenated with 200M words of randomlyselected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times).", "labels": [], "entities": [{"text": "Gigaword v4 corpus (excluding the NY Times and LA Times", "start_pos": 75, "end_pos": 130, "type": "DATASET", "confidence": 0.7859540581703186}]}, {"text": "We used NIST MT03 for tuning and NIST MT05 for testing.", "labels": [], "entities": [{"text": "NIST MT03", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.8398491442203522}, {"text": "NIST MT05", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8224594593048096}]}, {"text": "For evaluation, we used case-insensitive IBM BLEU ().", "labels": [], "entities": [{"text": "IBM", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.5707207322120667}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.8372243046760559}]}], "tableCaptions": [{"text": " Table 5: Comparing MERT to our training procedure. All  numbers are %BLEU.", "labels": [], "entities": [{"text": "MERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.5276995301246643}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9995834231376648}]}, {"text": " Table 6: Adding gappy pattern features. All numbers are  %BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9993115663528442}]}, {"text": " Table 7: Comparing ways of ranking patterns from pos- terior samples. Scores are on MT05 for ZH\u2192EN transla- tion.", "labels": [], "entities": [{"text": "MT05", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.7228825092315674}]}]}