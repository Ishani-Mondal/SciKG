{"title": [{"text": "Deterministic Statistical Mapping of Sentences to Underspecified Semantics", "labels": [], "entities": [{"text": "Deterministic Statistical Mapping of Sentences", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.754855751991272}]}], "abstractContent": [{"text": "We present a method for training a statistical model for mapping natural language sentences to semantic expressions.", "labels": [], "entities": []}, {"text": "The semantics are expressions of an underspecified logical form that has properties making it particularly suitable for statistical mapping from text.", "labels": [], "entities": [{"text": "statistical mapping from text", "start_pos": 120, "end_pos": 149, "type": "TASK", "confidence": 0.8461495041847229}]}, {"text": "An encoding of the semantic expressions into dependency trees with automatically generated labels allows application of existing methods for statistical dependency parsing to the mapping task (without the need for separate traditional dependency labels or parts of speech).", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 141, "end_pos": 171, "type": "TASK", "confidence": 0.6488637427488962}]}, {"text": "The encoding also results in a natural per-word semantic-mapping accuracy measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9439753890037537}]}, {"text": "We report on the results of training and testing statistical models for mapping sentences of the Penn Treebank into the semantic expressions, for which per-word semantic mapping accuracy ranges between 79% and 86% depending on the experimental conditions.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9922656118869781}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.95252525806427}]}, {"text": "The particular choice of algorithms used also means that our trained mapping is deterministic (in the sense of deterministic parsing), paving the way for large-scale text-to-semantic mapping.", "labels": [], "entities": []}], "introductionContent": [{"text": "Producing semantic representations of text is motivated not only by theoretical considerations but also by the hypothesis that semantics can be used to improve automatic systems for tasks that are intrinsically semantic in nature such as question answering, textual entailment, machine translation, and more generally any natural language task that might benefit from inference in order to more closely approximate human performance.", "labels": [], "entities": [{"text": "question answering", "start_pos": 238, "end_pos": 256, "type": "TASK", "confidence": 0.8038495779037476}, {"text": "textual entailment", "start_pos": 258, "end_pos": 276, "type": "TASK", "confidence": 0.6737635135650635}, {"text": "machine translation", "start_pos": 278, "end_pos": 297, "type": "TASK", "confidence": 0.7480875551700592}]}, {"text": "Since formal logics have formal denotational semantics, and are good candidates for supporting inference, they have often been taken to be the targets for mapping text to semantic representations, with frameworks emphasizing (more) tractable inference choosing first order predicate logic while those emphasizing representational power favoring one of the many available higher order logics.", "labels": [], "entities": []}, {"text": "It was later recognized that in order to support some tasks, fully specifying certain aspects of a logic representation, such as quantifier scope, or reference resolution, is often not necessary.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.7160398364067078}]}, {"text": "For example, for semantic translation, most ambiguities of quantifier scope can be carried over from the source language to the target language without being resolved.", "labels": [], "entities": [{"text": "semantic translation", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8083097338676453}]}, {"text": "This led to the development of underspecified semantic representations (e.g. QLF, and MRS, Copestake et al) which are easier to produce from text without contextual inference but which can be further specified as necessary for the task being performed.", "labels": [], "entities": []}, {"text": "While traditionally mapping text to formal representations was predominantly rule-based, for both the syntactic and semantic components,,), good progress in statistical syntactic parsing (e.g.,) led to systems that applied rules for semantic interpretation to the output of a statistical syntactic parser (e.g.).", "labels": [], "entities": [{"text": "statistical syntactic parsing", "start_pos": 157, "end_pos": 186, "type": "TASK", "confidence": 0.6305651466051737}]}, {"text": "More recently researchers have looked at statistical methods to provide robust and trainable methods for mapping text to formal representations of meaning.", "labels": [], "entities": []}, {"text": "In this paper we further develop the two strands of work mentioned above, i.e. mapping text to underspecified semantic representations and using statistical parsing methods to perform the analysis.", "labels": [], "entities": []}, {"text": "Here we take a more direct route, starting from scratch by designing an underspecified semantic representation (Natural Logical Form, or NLF) that is purpose-built for statistical text-to-semantics mapping.", "labels": [], "entities": [{"text": "statistical text-to-semantics mapping", "start_pos": 168, "end_pos": 205, "type": "TASK", "confidence": 0.7102705836296082}]}, {"text": "An underspecified logic whose constructs are motivated by natural language and that is amenable to trainable direct semantic mapping from text without an intervening layer of syntactic representation.", "labels": [], "entities": []}, {"text": "In contrast, the approach taken by, for example, maps into traditional logic via lambda expressions, and the approach taken by) depends on an initial step of syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.6991258412599564}]}, {"text": "In this paper, we describe a supervised training method for mapping text to NLF, that is, producing a statistical model for this mapping starting from training pairs consisting of sentences and their corresponding NLF expressions.", "labels": [], "entities": []}, {"text": "This method makes use of an encoding of NLF expressions into dependency trees in which the set of labels is automatically generated from the encoding process (rather than being pre-supplied by a linguistically motivated dependency grammar).", "labels": [], "entities": []}, {"text": "This encoding allows us to perform the text-to-NLF mapping using any existing statistical methods for labeled dependency parsing (e.g.,, McDonald, Crammer,).", "labels": [], "entities": [{"text": "labeled dependency parsing", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.6106257438659668}]}, {"text": "A side benefit of the encoding is that it leads to a natural per-word measure for semantic mapping accuracy which we use for evaluation purposes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.8204153180122375}]}, {"text": "By combing our method with deterministic statistical dependency models together with deterministic (hard) clusters instead of parts of speech, we obtain a deterministic statistical text-tosemantics mapper, opening the way to feasible mapping of text-to-semantics at a large scale, for example the entire web.", "labels": [], "entities": []}, {"text": "This paper concentrates on the text-to-semantics mapping which depends, in part, on some properties of NLF.", "labels": [], "entities": []}, {"text": "We will not attempt to defend the semantic representation choices for specific constructions illustrated here.", "labels": [], "entities": []}, {"text": "NLF is akin to a variable-free variant of QLF or an MRS in which some handle constraints are determined during parsing.", "labels": [], "entities": []}, {"text": "For the purposes of this paper it is sufficient to note that NLF has roughly the same granularity of semantic representation as these earlier underspecified representations.", "labels": [], "entities": []}, {"text": "We outline the steps of our text-to-semantics mapping method in Section 2, introduce NLF in Section 3, explain the encoding of NLF expressions as formal dependency trees in Section 4, and report on experiments for training and testing statistical models for mapping text to NLF expressions in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Formal labels for an example sentence.", "labels": [], "entities": []}, {"text": " Table 4: Datasets used in experiments.", "labels": [], "entities": []}, {"text": " Table 7: Per-word semantic accuracy after pruning label sets in Train-Null+AAlign (and testing with  Test-Null-AAlign).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9675207138061523}, {"text": "AAlign", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.956558108329773}]}]}