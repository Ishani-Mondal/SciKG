{"title": [{"text": "Language-Independent Context Aware Query Translation using Wikipedia", "labels": [], "entities": [{"text": "Language-Independent Context Aware Query Translation", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.6093849658966064}]}], "abstractContent": [{"text": "Cross lingual information access (CLIA) systems are required to access the large amounts of multilingual content generated on the worldwide web in the form of blogs, news articles and documents.", "labels": [], "entities": [{"text": "Cross lingual information access (CLIA)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7277033627033234}]}, {"text": "In this paper, we discuss our approach to query formation for CLIA systems where language resources are replaced by Wikipedia.", "labels": [], "entities": [{"text": "query formation", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7931474447250366}]}, {"text": "We claim that Wikipedia, with its rich multilingual content and structure , forms an ideal platform to build a CLIA system.", "labels": [], "entities": []}, {"text": "Our approach is particularly useful for under-resourced languages, as all the languages don't have the resources(tools) with sufficient accuracies.", "labels": [], "entities": []}, {"text": "We propose a context aware language-independent query formation method which, with the help of bilingual dictionaries , forms queries in the target language.", "labels": [], "entities": [{"text": "context aware language-independent query formation", "start_pos": 13, "end_pos": 63, "type": "TASK", "confidence": 0.5946112036705017}]}, {"text": "Results are encouraging with a precision of 69.75% and thus endorse our claim on using Wikipedia for building CLIA systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9988781809806824}]}], "introductionContent": [{"text": "Cross lingual information access (CLIA) systems enable users to access the rich multilingual content that is created on the web daily.", "labels": [], "entities": [{"text": "Cross lingual information access (CLIA)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7290992311068943}]}, {"text": "Such systems are vital to bridge the gap between information available and languages known to the user.", "labels": [], "entities": []}, {"text": "Considerable amount of research has been done on building such systems but most of them rely heavily on the language resources and tools developed.", "labels": [], "entities": []}, {"text": "With a constant increase in the number of languages around the world with their content on the web, CLIA systems are in need.", "labels": [], "entities": []}, {"text": "Language independent approach is particularly useful for languages that fall into the category of under-resourced (African, few Asian languages), that doesn't have sufficient resources.", "labels": [], "entities": []}, {"text": "In our approach towards language-independent CLIA system, we have developed context aware query translation using Wikipedia.", "labels": [], "entities": [{"text": "context aware query translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6131206750869751}]}, {"text": "Due to voluntary contribution of millions of users, Wikipedia gathers very significant amount of updated knowledge and provides a structured way to access it.", "labels": [], "entities": []}, {"text": "The statistics in the show that it has rich multilingual content and is growing independent of the presence of English counterpart.", "labels": [], "entities": []}, {"text": "With its structurally rich content, it provides an ideal platform to perform cross lingual research.", "labels": [], "entities": []}, {"text": "We harness Wikipedia and its structure to replace the language specific resources required for CLIA.", "labels": [], "entities": []}, {"text": "Our work is different from existing approaches in", "labels": [], "entities": []}], "datasetContent": [{"text": "A classification based approach and a dictionary based approach are employed to calculate the accuracy of the queries translated.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9989296793937683}]}, {"text": "400 sentences with their corresponding translations (English-Hindi) have been used as test set to evaluate the performance of the query formation.", "labels": [], "entities": [{"text": "query formation", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.6932809352874756}]}, {"text": "The sentence pairs are provided by FIRE 2 . These sentences contain all types of words (Named entities, Verbs etc) and will be referred to as samples.", "labels": [], "entities": [{"text": "FIRE", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.7066865563392639}]}, {"text": "The English language sentences are used as queries and are translated to Hindi using the approach described.", "labels": [], "entities": []}, {"text": "Before forming the query, stop words are removed from the English sentence.", "labels": [], "entities": []}, {"text": "The query lengths after removing stop words vary from 2 words to 8 words.", "labels": [], "entities": []}, {"text": "The dictionary used for evaluation is an existing one, Shabdanjali 3 . In the following sections, we describe our two evaluation strategies and the performance of our system using them.", "labels": [], "entities": []}, {"text": "Shabdanjali dictionary has been used to evaluate the translated queries.", "labels": [], "entities": []}, {"text": "The evaluation metric is word overlap, though it is relaxed further.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.6014335006475449}]}, {"text": "The formula used for calculating the precision is A sample is said to be correct if its overLapScore is greater than threshold instead of complete overlap.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9993638396263123}]}, {"text": "The overLapScore of each sample is measured using Formula 3.", "labels": [], "entities": []}, {"text": "Threshold is the average overLapScore of the positive training set used for training the classifier (Training dataset is discussed in Section 4.1.2).", "labels": [], "entities": [{"text": "Threshold", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9941633343696594}, {"text": "Training dataset", "start_pos": 101, "end_pos": 117, "type": "DATASET", "confidence": 0.6617507934570312}]}, {"text": "The number of word overlaps are measured both manually and automatically to avoid inconsistent results due to varios syntactic representation of the same word in Wikipedia.", "labels": [], "entities": []}, {"text": "The precision for the test dataset using this approach is 42.8%.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996979236602783}]}, {"text": "As described in Section 2, we have used a classification based technique for identifying whether the translated queries contain the same information or not.", "labels": [], "entities": []}, {"text": "We have collected 1600 pairs of sentences where 800 sentences are parallel to each other (positive samples, exact translations) while the other half have word overlaps, but not parallel, (not exact translations but have similar content) form the negative samples.", "labels": [], "entities": []}, {"text": "Various statistics are extracted from Wikipedia for each sentence pair to construct feature vector as described in.", "labels": [], "entities": []}, {"text": "Each English and Hindi sentences are queried as bag-of-words query to corresponding Wikipedia articles and statistics are extracted based on the articles retrieved.", "labels": [], "entities": []}, {"text": "The classifier used is SVM and is trained on the feature vectors generated for 1600 samples.", "labels": [], "entities": []}, {"text": "The precision in this approach is the accuracy of the classifier.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991876482963562}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9995588660240173}]}, {"text": "The formula used for calculating the accuracy is The correctness of the sample is the prediction of the classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996670484542847}, {"text": "correctness", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9571850299835205}]}, {"text": "The precision for the test set is 69.75%.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9997977614402771}]}], "tableCaptions": []}