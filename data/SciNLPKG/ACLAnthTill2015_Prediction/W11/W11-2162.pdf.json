{"title": [], "abstractContent": [{"text": "This paper describes the system presented for the English-Spanish translation task by the collaboration between CEU-UCH and UPV for 2011 WMT.", "labels": [], "entities": [{"text": "English-Spanish translation task", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.7086625695228577}, {"text": "CEU-UCH", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9355962872505188}, {"text": "UPV for 2011 WMT", "start_pos": 124, "end_pos": 140, "type": "DATASET", "confidence": 0.7626273036003113}]}, {"text": "A comparison of independent phrase-based translation models interpolation for each available training corpora were tested, giving an improvement of 0.4 BLEU points over the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9993811845779419}]}, {"text": "Output N-best lists were rescored via a target Neural Network Language Model.", "labels": [], "entities": []}, {"text": "An improvement of one BLEU point over the baseline was obtained adding the two features, giving 31.5 BLEU and 57.9 TER for the primary system, computed over lowercased and detokenized outputs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9986907839775085}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9991660118103027}, {"text": "TER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9952698349952698}]}, {"text": "The system was positioned second in the final ranking.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of Statistical Machine Translation (SMT) is to translate a sentence between two languages.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.8893905480702718}]}, {"text": "Giving the source language sentence f , it would be translated to an equivalent target language sentence e.", "labels": [], "entities": []}, {"text": "The most extended formalization is done via loglinear models () as follows: where h k (f , e) is a score function representing an important feature for the translation off into e, K is the number of models (or features) and \u03bb k are the weights of the log-linear combination.", "labels": [], "entities": []}, {"text": "Typically, the weights \u03bb k are optimized during the tuning stage with the use of a development set.", "labels": [], "entities": []}, {"text": "SMT systems rely on a bilingual sentence aligned training corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9897873401641846}]}, {"text": "These sentences are aligned at the word level (, and after that, different h k feature functions are trained.", "labels": [], "entities": []}, {"text": "In some practical cases, the out-of-domain training data is larger than the in-domain training data.", "labels": [], "entities": []}, {"text": "In these cases the target Language Model (LM) is composed of a linear interpolation of independent LMs, one for each available training domain or corpus.", "labels": [], "entities": []}, {"text": "Nevertheless, the training of phrase-based translation models is an open problem in these cases.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.713716596364975}]}, {"text": "Some recent works) related to corpus weighting, make use of data selection, data weighting, and translation model adaptation to overcome this problem.", "labels": [], "entities": [{"text": "corpus weighting", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7821317613124847}, {"text": "data weighting", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7897556126117706}, {"text": "translation model adaptation", "start_pos": 96, "end_pos": 124, "type": "TASK", "confidence": 0.9280823071797689}]}, {"text": "In this work, we explore a simple corpus weighting technique to interpolate any number of different phrase tables.", "labels": [], "entities": [{"text": "corpus weighting", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.688882976770401}]}, {"text": "Two different approaches are tested, obtaining similar performance.", "labels": [], "entities": []}, {"text": "On the one hand, a count-based smoothing technique that applies a weight to the counting of phrases and lexical links depending on the relevance of each corpus.", "labels": [], "entities": []}, {"text": "On the other hand, a linear interpolation of independent trained phrase tables.", "labels": [], "entities": []}, {"text": "Another important feature of this work is the use of Neural Network Language Models (NN LMs).", "labels": [], "entities": []}, {"text": "This kind of LMs has been successfully applied in some connectionist approaches to language modeling (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7173231244087219}]}, {"text": "The advantage of these NN LMs is the projection of words on a continuous space were the probabilities of n-grams are learned.", "labels": [], "entities": []}, {"text": "A Neural Network (NN) is proposed to learn both the word projections and the n-gram probabilities.", "labels": [], "entities": []}, {"text": "The presented system combines a standard, stateof-the-art SMT system with a NN LM via log-linear combination and N -best output re-scoring.", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9837088584899902}]}, {"text": "We chose to participate in the English-Spanish direction.", "labels": [], "entities": []}], "datasetContent": [{"text": "The baseline SMT system is built with the opensource SMT toolkit Moses ( , in its standard setup.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9906564950942993}, {"text": "opensource SMT toolkit Moses", "start_pos": 42, "end_pos": 70, "type": "DATASET", "confidence": 0.6244595497846603}]}, {"text": "The decoder includes a log-linear model comprising a phrase-based translation model, a language model, a lexicalized distortion model and word and phrase penalties.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7071482390165329}]}, {"text": "The weights of the log-linear interpolation were optimized by means of MERT, using the News-Commentary test set of the 2008 shared task as a development set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.6912924647331238}, {"text": "News-Commentary test set of the 2008 shared task", "start_pos": 87, "end_pos": 135, "type": "DATASET", "confidence": 0.9335295930504799}]}, {"text": "The phrase-based translation model uses the con-  The baseline LM was a regular n-gram LM with Kneser-Ney smoothing and interpolation by means of the SRILM toolkit).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6126482486724854}]}, {"text": "Specifically, we trained a 6-gram LM on United Nations, a 5-gram on Europarl and NewsShuffled, and a 4-gram on News-Commentary.", "labels": [], "entities": [{"text": "United Nations", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.8816647827625275}, {"text": "Europarl", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.993780791759491}, {"text": "NewsShuffled", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.7679954171180725}, {"text": "News-Commentary", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.9556174874305725}]}, {"text": "Once these LMs had been built, they were interpolated so as to maximize the perplexity of the NewsCommentary test set of the 2009 shared task.", "labels": [], "entities": [{"text": "NewsCommentary test set of the 2009 shared task", "start_pos": 94, "end_pos": 141, "type": "DATASET", "confidence": 0.931915856897831}]}, {"text": "The final model was pruned out using a threshold of 10 \u22128 . This was done so according to preliminary research.", "labels": [], "entities": []}, {"text": "Three different weights for the count smoothing technique described in section 3.1 were tested.", "labels": [], "entities": [{"text": "count smoothing", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.8458602726459503}]}, {"text": "For the interpolation model of section 3.2, we select the weights minimizing the perplexity of the corresponding three LMs (Europarl, NC, and UN) over the News2008 set.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.9891796112060547}, {"text": "News2008 set", "start_pos": 155, "end_pos": 167, "type": "DATASET", "confidence": 0.9811953902244568}]}, {"text": "NN LM was trained with all the corpora described in, using a weighted replacement algorithm to modify the impact of each corpus in the training algorithm.", "labels": [], "entities": [{"text": "NN LM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9189349710941315}]}, {"text": "The weights were the same that for the standard LM.", "labels": [], "entities": []}, {"text": "In order to reduce the complexity of the model, the input vocabulary of the NN LM was restricted using only words that appears more than 10 times in the corpora.", "labels": [], "entities": [{"text": "NN LM", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.878375768661499}]}, {"text": "The vocabulary is formed by the 107 607 more frequent words, with two additional inputs: one to represent the words out of this vocabulary, and another for the begin-of-sentence cue.", "labels": [], "entities": []}, {"text": "The output of the NN LM was restricted much more, using only a shortlist of the 10K more frequent words, plus the end-of-sentence cue.", "labels": [], "entities": [{"text": "NN LM", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.9210129976272583}]}, {"text": "The rest of words are collected by an additional output in the neural network.", "labels": [], "entities": []}, {"text": "When the probability of an out-of-shortlist word is required, its probability is computed multiplying this additional output activation by the unigram probability distribution of every out-of-shortlist word.", "labels": [], "entities": []}, {"text": "This implies that 10.7% of the running words of the News2009 set, and 11.1% of the running words of the News2011 official test set, will be considered as out-of-shortlist words for the NN LM.", "labels": [], "entities": [{"text": "News2009 set", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9855653047561646}, {"text": "News2011 official test set", "start_pos": 104, "end_pos": 130, "type": "DATASET", "confidence": 0.9689491093158722}, {"text": "NN LM", "start_pos": 185, "end_pos": 190, "type": "DATASET", "confidence": 0.9699919521808624}]}, {"text": "A 6-gram NN LM was trained for this task, based in previous works).", "labels": [], "entities": []}, {"text": "Four NN LMs with different values for the projection of each word were linearly combined for the final NN LM.", "labels": [], "entities": []}, {"text": "Each NN LM had 320 units in the hidden layer.", "labels": [], "entities": [{"text": "NN LM", "start_pos": 5, "end_pos": 10, "type": "DATASET", "confidence": 0.910201907157898}]}, {"text": "The combination weights were computed maximizing the perplexity over the News2009 set.", "labels": [], "entities": [{"text": "News2009 set", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9925408065319061}]}, {"text": "The training procedure was conducted by means of the stochastic back-propagation algorithm with weight decay, with a replacement of 300K training samples and 200K validation samples in each training epoch, selecting the random sample using a different distribution weight for each corpus.", "labels": [], "entities": []}, {"text": "The validation set was the News2009 set.", "labels": [], "entities": [{"text": "News2009 set", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9894892275333405}]}, {"text": "The networks were stopped after 99, 70, 53, and 42 epochs respectively (unfortunately, without achieving convergence, due to the competition timings).", "labels": [], "entities": []}, {"text": "This resulted in very few training samples compared with the size of the training set: 29M in the best case, versus more than 500M of the full set.", "labels": [], "entities": []}, {"text": "The training of the NN LMs was accomplished with the April toolkit).", "labels": [], "entities": [{"text": "NN LMs", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.9078506529331207}, {"text": "April toolkit", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9859167337417603}]}, {"text": "The perplexity achieved by the 6-gram NN LM in the Spanish News2009 set was 281, versus 145 obtained with the standard 6-gram language model with interpolation and Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "Spanish News2009 set", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.8831685980161031}]}, {"text": "The number of sentences in the N -best list was set to 2 000 unique output sentences.", "labels": [], "entities": []}, {"text": "Results can be seen in.", "labels": [], "entities": []}, {"text": "In order to assess the reliability of such results, we computed pairwise improvement intervals as described in, by means of bootstrapping with 1 000 bootstrap iterations and at a 95% confidence level.", "labels": [], "entities": []}, {"text": "Such confidence test reported the improvements to be statistically significant.", "labels": [], "entities": []}, {"text": "A difference of more than 0.3 points of BLEU is considered significant in the pairwise comparison.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9981484413146973}]}, {"text": "The final results leads to 31.5 points of BLEU, positioning this system as second in the final classification.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9987760186195374}]}], "tableCaptions": [{"text": " Table 1: Spanish corpora statistics. NC stands for  News-Commentary and UN for United Nations, while  |\u2126| stands for vocabulary size, and M /K for mil- lions/thousands of elements. All numbers are computed  with tokenized and lowercased data.", "labels": [], "entities": []}, {"text": " Table 2: Weights of different combination of phrase- based translation models.", "labels": [], "entities": [{"text": "phrase- based translation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.604792408645153}]}, {"text": " Table 3: Main results of the experimentation", "labels": [], "entities": []}, {"text": " Table 3. In order to assess the reliability  of such results, we computed pairwise improvement  intervals as described in", "labels": [], "entities": []}]}