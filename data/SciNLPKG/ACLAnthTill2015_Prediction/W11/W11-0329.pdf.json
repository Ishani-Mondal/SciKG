{"title": [{"text": "Learning Discriminative Projections for Text Similarity Measures", "labels": [], "entities": [{"text": "Text Similarity Measures", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7838984231154124}]}], "abstractContent": [{"text": "Traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms.", "labels": [], "entities": []}, {"text": "We propose a novel discriminative training method that projects the raw term vectors into a common , low-dimensional vector space.", "labels": [], "entities": []}, {"text": "Our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function (e.g., cosine) of the projected vectors, and is able to efficiently handle a large number of training examples in the high-dimensional space.", "labels": [], "entities": []}, {"text": "Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches , but also achieves high accuracy at low dimensions and is thus more efficient.", "labels": [], "entities": [{"text": "cross-lingual document retrieval", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.6038169264793396}, {"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9954128861427307}]}], "introductionContent": [{"text": "Measures of text similarity have many applications and have been studied extensively in both the NLP and IR communities.", "labels": [], "entities": [{"text": "Measures of text similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6266579926013947}]}, {"text": "For example, a combination of corpus and knowledge based methods have been invented for judging word similarity.", "labels": [], "entities": [{"text": "judging word similarity", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7472941478093466}]}, {"text": "Similarity derived from a largescale Web corpus has been used for automatically extending lists of typed entities.", "labels": [], "entities": []}, {"text": "Judging the degree of similarity between documents is also fundamental to classical IR problems such as document retrieval.", "labels": [], "entities": [{"text": "IR", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9772927761077881}, {"text": "document retrieval", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.6980232447385788}]}, {"text": "In all these applications, the vector-based similarity method is the most widely used.", "labels": [], "entities": []}, {"text": "Term vectors are first constructed to represent the original text objects, where each term is associated with a weight indicating its importance.", "labels": [], "entities": []}, {"text": "A pre-selected function operating on these vectors, such as cosine, is used to output the final similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 96, "end_pos": 112, "type": "METRIC", "confidence": 0.8551588654518127}]}, {"text": "This approach has not only proved to be effective, but is also efficient.", "labels": [], "entities": []}, {"text": "For instance, only the term vectors rather than the raw data need to be stored.", "labels": [], "entities": []}, {"text": "A pruned inverse index can be built to support fast similarity search.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.6939425617456436}]}, {"text": "However, the main weakness of this term-vector representation is that different but semantically related terms are not matched and cannot influence the final similarity score.", "labels": [], "entities": []}, {"text": "As an illustrative example, suppose the two compared term-vectors are: {purchase:0.4, used:0.3, automobile:0.2} and {buy:0.3, pre-owned: 0.5, car: 0.4}.", "labels": [], "entities": []}, {"text": "Even though the two vectors represent very similar concepts, their similarity score will be 0, for functions like cosine, overlap or Jaccard.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 67, "end_pos": 83, "type": "METRIC", "confidence": 0.9557934701442719}]}, {"text": "Such an issue is more severe in cross-lingual settings.", "labels": [], "entities": []}, {"text": "Because language vocabularies typically have little overlap, term-vector representations are completely inapplicable to measuring similarity between documents in different languages.", "labels": [], "entities": []}, {"text": "The general strategy to handle this problem is to map the raw representation to a common concept space, where extensive approaches have been proposed.", "labels": [], "entities": []}, {"text": "Existing methods roughly fall into three categories.", "labels": [], "entities": []}, {"text": "Generative topic models like Latent Dirichlet Allocation (LDA) ( assume that the terms are sampled by probability distributions governed by hidden topics.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.5604241689046224}]}, {"text": "Linear projection methods like Latent Semantic Analysis (LSA)) learn a projection matrix and map the original term-vectors to the dense low-dimensional space.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA))", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.7897248367468516}]}, {"text": "Finally, metric learning approaches for high-dimensional spaces have also been proposed (.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew projection learning framework, Similarity Learning via Siamese Neural Network (S2Net), to discriminatively learn the concept vector representations of input text objects.", "labels": [], "entities": []}, {"text": "Following the general Siamese neural network architecture (, our approach trains two identical networks concurrently.", "labels": [], "entities": []}, {"text": "The input layer corresponds to the original term vector and the output layer is the projected concept vector.", "labels": [], "entities": []}, {"text": "Model parameters (i.e., the weights on the edges) are equivalently the projection matrix.", "labels": [], "entities": []}, {"text": "Given pairs of raw term vectors and their labels (e.g., similar or not), the model is trained by minimizing the loss of the similarity scores of the output vectors.", "labels": [], "entities": []}, {"text": "S2Net is closely related to the linear projection and metric learning approaches, but enjoys additional advantages over existing methods.", "labels": [], "entities": []}, {"text": "While its model form is identical to that of LSA, CCA and OPCA, its objective function can be easily designed to match the true evaluation metric of interest for the target task, which leads to better performance.", "labels": [], "entities": []}, {"text": "Compared to existing high-dimensional metric learning methods, S2Net can learn from a much larger number of labeled examples.", "labels": [], "entities": []}, {"text": "These two properties are crucial in helping S2Net outperform existing methods.", "labels": [], "entities": []}, {"text": "For retrieving comparable cross-lingual documents, S2Net achieves higher accuracy than the best approach (OPCA) at a much lower dimension of the concept space (500 vs. 2,000).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9987273812294006}]}, {"text": "Ina monolingual setting, where the task is to judge the relevance of an ad landing page to a query, S2Net also has the best performance when compared to a number of approaches, including the raw TFIDF cosine baseline.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we first survey some existing work in Sec.", "labels": [], "entities": []}, {"text": "2, with an emphasis on approaches included in our experimental comparison.", "labels": [], "entities": []}, {"text": "We present our method in Sec.", "labels": [], "entities": []}, {"text": "3 and report on an extensive experimental study in Sec.", "labels": [], "entities": [{"text": "Sec.", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.6188588440418243}]}, {"text": "4. Other related work is discussed in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare S2Net experimentally with existing approaches on two very different tasks: cross-lingual document retrieval and ad relevance measures.", "labels": [], "entities": [{"text": "cross-lingual document retrieval", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.6470017035802206}]}], "tableCaptions": [{"text": " Table 1: Test results for comparable document retrieval  in Wikipedia. Results of OPCA, CCA and CL-LSI are  from (Platt et al., 2010).", "labels": [], "entities": [{"text": "OPCA", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.6349964141845703}]}, {"text": " Table 2: The AUC and NDCG scores of the cosine sim- ilarity scores on different vector representations. The di- mension for all models except TFIDF is 1000.", "labels": [], "entities": [{"text": "AUC", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9583703279495239}, {"text": "NDCG", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.8239564299583435}]}, {"text": " Table 3: The AUC and NDCG scores of S2Net at different dimensions. PCA, HDLR & CPLSA (at dimension 1000)  along with the raw TFIDF representation are used for reference.", "labels": [], "entities": [{"text": "AUC", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.6805400252342224}]}]}