{"title": [{"text": "Investigations on Translation Model Adaptation Using Monolingual Data", "labels": [], "entities": [{"text": "Translation Model Adaptation", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.9507437745730082}]}], "abstractContent": [{"text": "Most of the freely available parallel data to train the translation model of a statistical machine translation system comes from very specific sources (European parliament, United Nations, etc).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.6098032295703888}]}, {"text": "Therefore, there is increasing interest in methods to perform an adaptation of the translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.9628576040267944}]}, {"text": "A popular approach is based on unsupervised training, also called self-enhancing.", "labels": [], "entities": []}, {"text": "Both only use monolingual data to adapt the translation model.", "labels": [], "entities": []}, {"text": "In this paper we extend the previous work and provide new insight in the existing methods.", "labels": [], "entities": []}, {"text": "We report results on the translation between French and English.", "labels": [], "entities": [{"text": "translation between French and English", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.8688125133514404}]}, {"text": "Improvements of up to 0.5 BLEU were observed with respect to a very competitive baseline trained on more than 280M words of human translated parallel data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9990978240966797}]}], "introductionContent": [{"text": "Adaptation of a statistical machine translation system (SMT) is a topic of increasing interest during the last years.", "labels": [], "entities": [{"text": "Adaptation of a statistical machine translation system (SMT)", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7314823806285858}]}, {"text": "Statistical (n-gram) language models are used in many domains and several approaches to adapt such models were proposed in the literature, for instance in the framework of automatic speech recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 172, "end_pos": 200, "type": "TASK", "confidence": 0.5995874603589376}]}, {"text": "Many of these approaches were successfully used to adapt the language model of an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9922034740447998}]}, {"text": "On the other hand, it seems more challenging to adapt the other components of an SMT system, namely the translation and reordering models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9934163093566895}]}, {"text": "In this work we consider the adaptation of the translation model of a phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.8478943705558777}]}, {"text": "While rule-based machine translation rely on rules and linguistic resources built for that purpose, SMT systems can be developed without the need of any language-specific expertise and are only based on bilingual sentence-aligned data (\"bitexts\") and large monolingual texts.", "labels": [], "entities": [{"text": "rule-based machine translation", "start_pos": 6, "end_pos": 36, "type": "TASK", "confidence": 0.6588233808676401}, {"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.99100261926651}]}, {"text": "However, while monolingual data are usually available in large amounts and fora variety of tasks, bilingual texts area sparse resource for most language pairs.", "labels": [], "entities": []}, {"text": "Current parallel corpora mostly come from one domain (proceedings of the Canadian or European Parliament, or of the United Nations).", "labels": [], "entities": []}, {"text": "This is problematic when SMT systems trained on such corpora are used for general translations, as the language jargon heavily used in these corpora is not appropriate for everyday life translations or translations in some other domain.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9905732274055481}, {"text": "general translations", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.5888278186321259}]}, {"text": "This problem could be attacked by either searching for more in-domain training data, e.g. by exploring comparable corpora or the WEB, or by adapting the translation model to the task.", "labels": [], "entities": [{"text": "WEB", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.8558794260025024}]}, {"text": "In this work we consider translation model adaptation without using additional bilingual data.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.9423609773317972}]}, {"text": "One can distinguish two types of translation model adaptation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.914596954981486}]}, {"text": "These two directions are complementary and could be simultaneously applied.", "labels": [], "entities": []}, {"text": "In this work we focus on the second type of adaptation.", "labels": [], "entities": []}, {"text": "In this work, we focus on statistical phrasebased machine translations systems (PBSMT), but the methods could be also applied to hierarchical systems.", "labels": [], "entities": [{"text": "statistical phrasebased machine translations", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.5691659599542618}]}, {"text": "In PBSMT, the translation model is represented by a large list of all known source phrases and their translations.", "labels": [], "entities": []}, {"text": "Each entry is weighted using several probabilities, e.g. the popular Moses system uses phrase translation probabilities in the forward and backward direction, as well as lexical probabilities in both directions.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7022388130426407}]}, {"text": "The entries of the phrase-table are automatically extracted from sentence aligned parallel data and they are usually quite noisy.", "labels": [], "entities": []}, {"text": "It is not uncommon to encounter several hundreds, or even thousands of possible translations of frequent source phrases.", "labels": [], "entities": []}, {"text": "Many of these automatically extracted translations are probably wrong and are never used since their probabilities are (fortunately) small in comparison to better translations.", "labels": [], "entities": []}, {"text": "Therefore, several approaches were proposed to filter these phrase-tables, reducing considerably their size without any loss of the quality, or even achieving improved performance.", "labels": [], "entities": []}, {"text": "Given these observations, adaptation of the translation model of PBSMT systems could be performed by modifying the probability distribution of the existing phrases without necessarily modifying the entries.", "labels": [], "entities": []}, {"text": "The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones.", "labels": [], "entities": []}, {"text": "Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data.", "labels": [], "entities": []}, {"text": "A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain.", "labels": [], "entities": []}, {"text": "This was investigated in the framework of SMT by several authors, for instance for word alignment, for language modeling ( and to a lesser extent for the translation model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.990572988986969}, {"text": "word alignment", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.8392444550991058}, {"text": "language modeling", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7432018220424652}]}, {"text": "This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients.", "labels": [], "entities": []}, {"text": "On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases.", "labels": [], "entities": []}, {"text": "Another direction of research is self-enhancing of the translation model.", "labels": [], "entities": []}, {"text": "This was first proposed by.", "labels": [], "entities": []}, {"text": "The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic phrase table.", "labels": [], "entities": []}, {"text": "This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set.", "labels": [], "entities": []}, {"text": "In practice, such an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. atypical evaluation setup with a test set of about 50k words.", "labels": [], "entities": []}, {"text": "This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.980721116065979}]}, {"text": "In followup work, this approach was refined (.", "labels": [], "entities": []}, {"text": "Domain adaptation was also performed simultaneously for the translation, language and reordering model.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7744529545307159}, {"text": "translation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9730098843574524}]}, {"text": "A somehow related approach was named lightlysupervised training.", "labels": [], "entities": []}, {"text": "In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9838865399360657}]}, {"text": "This approach was reported to obtain interesting improvements in the translations quality.", "labels": [], "entities": [{"text": "translations", "start_pos": 69, "end_pos": 81, "type": "TASK", "confidence": 0.9677610397338867}]}, {"text": "In comparison to self enhancing as proposed by, lightly-supervised training does not adapt itself to the test data, but large amounts of monolingual training data are translated and a completely new model is built.", "labels": [], "entities": []}, {"text": "This model can be applied to any test data, including a WEB service.", "labels": [], "entities": [{"text": "WEB service", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.7578470706939697}]}, {"text": "In this paper we propose to extend this approach in several ways.", "labels": [], "entities": []}, {"text": "First, we argue that the automatic translations should not be performed from the source to the target language, but in the opposite direction.", "labels": [], "entities": []}, {"text": "Second, we propose to use the segmentation obtained during translation instead of performing word alignments with GIZA++ of the automatic translations.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.6952122896909714}, {"text": "GIZA", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.848213791847229}]}, {"text": "Finally, we propose to enrich the vocabulary of the adapted system by detecting untranslated words and automatically inferring possible translations from the stemmed form and the existing translations in the phrase table.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section we first describe our approach in detail.", "labels": [], "entities": []}, {"text": "Section 3 describes the considered task, the available resources and the baseline PBSMT system.", "labels": [], "entities": []}, {"text": "Results are summarized in section 4 and the paper concludes with a discussion and perspectives of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The system trained on Europarl, News Commentary and the sub-sampled version of the 10 9 bitexts (\"eparl+nc+crawled2\", in the third line of, was used to translate parts of the crawled news in French and English.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.9903671741485596}, {"text": "News Commentary", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9183871448040009}]}, {"text": "Statistics on the translated data are given in.", "labels": [], "entities": []}, {"text": "We focused on the most recent data since the time period of our development and test data was end of 2008 and 2009 respectively.", "labels": [], "entities": []}, {"text": "In the future we will translate all the available monolingual data and make it available to the community in order to ease the widespread use of this kind of translation model adaptation methods.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.876459538936615}]}, {"text": "These automatic translations were filtered using the sentence normalized log-score of the decoder, as proposed by.", "labels": [], "entities": []}, {"text": "However, we did not perform systematic experiments to find the optimal threshold on this score, but simply used a value which seems to be a good compromise of quality and quantity of the translations.", "labels": [], "entities": []}, {"text": "This gave us about 45M English words of", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Case sensitive BLEU scores as a function of the amount of parallel training data. (Eparl=Europarl, nc=News  Commentary, crawled1/2=sub-sampled crawled bitexts, un=sub-sampled United Nations bitexts).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9539580345153809}, {"text": "Europarl", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.8334576487541199}, {"text": "United Nations bitexts", "start_pos": 185, "end_pos": 207, "type": "DATASET", "confidence": 0.8591989278793335}]}, {"text": " Table 3: Available training data for the translation be- tween French and English for the translation evaluation  at WMT'11 (number of words after tokenisation).", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.8801229596138}, {"text": "WMT'11", "start_pos": 118, "end_pos": 124, "type": "DATASET", "confidence": 0.8474728465080261}]}, {"text": " Table 5: Results for systems trained via different word alignment configurations. The values are the average over  3 MERT runs performed with different seeds. The numbers in parentheses are the standard deviation of these three  values. Translation was performed from English to French, adding 45M words of automatic translations (translated  from French to English) to the baseline system \"eparl+nc+crawled2\".", "labels": [], "entities": [{"text": "word alignment", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7315040975809097}]}, {"text": " Table 8: Translation results of the French-English systems augmented with a bitext obtained by translating news data  from English to French (ef) and French to English (fe). 45M/65M/100M refers to the number of English running  words.", "labels": [], "entities": []}, {"text": " Table 9: Phrase table statistics for French-English systems augmented with bitexts built via automatic translations.  Only the entries useful to translate the development set were present in the considered phrase table.", "labels": [], "entities": []}]}