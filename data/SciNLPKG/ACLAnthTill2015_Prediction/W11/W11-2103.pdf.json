{"title": [{"text": "Findings of the 2011 Workshop on Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.76090407371521}]}], "abstractContent": [{"text": "This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics.", "labels": [], "entities": [{"text": "WMT11 shared tasks", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.515399714310964}, {"text": "translation task", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.8969654738903046}, {"text": "machine translation evaluation metrics", "start_pos": 136, "end_pos": 174, "type": "TASK", "confidence": 0.8546410202980042}]}, {"text": "We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7479597926139832}]}, {"text": "We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics.", "labels": [], "entities": []}, {"text": "This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.", "labels": [], "entities": [{"text": "translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake", "start_pos": 52, "end_pos": 157, "type": "TASK", "confidence": 0.7445323430001736}]}, {"text": "We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011.", "labels": [], "entities": [{"text": "statistical Machine Translation (WMT)", "start_pos": 71, "end_pos": 108, "type": "TASK", "confidence": 0.7906804531812668}, {"text": "EMNLP 2011", "start_pos": 128, "end_pos": 138, "type": "DATASET", "confidence": 0.910291850566864}]}, {"text": "This workshop builds on five previous WMT workshops ().", "labels": [], "entities": [{"text": "WMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.7080968022346497}]}, {"text": "The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics.", "labels": [], "entities": [{"text": "translation task between English and other languages", "start_pos": 44, "end_pos": 96, "type": "TASK", "confidence": 0.8351669822420392}]}, {"text": "The performance for each of these shared tasks is determined through a comprehensive human evaluation.", "labels": [], "entities": []}, {"text": "There were a two additions to this year's workshop that were not part of previous workshops: \u2022 Haitian Creole featured task -In addition to translation between European language pairs, we featured anew translation task: translating Haitian Creole SMS messages that were sent to an emergency response hotline in the immediate aftermath of the 2010 Haitian earthquake.", "labels": [], "entities": [{"text": "translation between European language pairs", "start_pos": 140, "end_pos": 183, "type": "TASK", "confidence": 0.8369825720787049}, {"text": "translating Haitian Creole SMS messages that were sent to an emergency response hotline in the immediate aftermath of the 2010 Haitian earthquake", "start_pos": 220, "end_pos": 365, "type": "TASK", "confidence": 0.7555358518253673}]}, {"text": "The goal of this task is to encourage researchers to focus on challenges that may arise in future humanitarian crises.", "labels": [], "entities": []}, {"text": "We invited Will Lewis, Rob Munro and Stephan Vogel to publish a paper about their experience developing translation technology in response to the crisis ().", "labels": [], "entities": []}, {"text": "They provided the data used in the Haitian Creole featured translation task.", "labels": [], "entities": [{"text": "Haitian Creole featured translation task", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.6706551849842072}]}, {"text": "We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages.", "labels": [], "entities": []}, {"text": "\u2022 Tunable metric shared task -We conducted a pilot of anew shared task to use evaluation metrics to tune the parameters of a machine translation system.", "labels": [], "entities": []}, {"text": "Although previous workshops have shown evaluation metrics other than BLEU are more strongly correlated with human judgments when ranking outputs from multiple systems, BLEU remains widely used by system developers to optimize their system parameters.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9949594736099243}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9758871793746948}]}, {"text": "We challenged metric developers to tune the parameters of a fixed system, to see if their metrics would lead to perceptibly better translation quality for the system's resulting output.", "labels": [], "entities": []}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8729994297027588}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8300673961639404}, {"text": "machine translation", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.8182332217693329}]}, {"text": "As with previous workshops, all of the data, translations, and collected human judgments are publicly available.", "labels": [], "entities": []}, {"text": "We hope these datasets form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6837265193462372}, {"text": "system combination", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7204270362854004}]}], "datasetContent": [{"text": "As with past workshops, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores.", "labels": [], "entities": []}, {"text": "It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality.", "labels": [], "entities": []}, {"text": "Therefore, we define the manual evaluation to be primary, and use the human judgments to validate automatic metrics.", "labels": [], "entities": []}, {"text": "Manual evaluation is time consuming, and it requires a large effort to conduct on the scale of our workshop.", "labels": [], "entities": [{"text": "Manual evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6070550680160522}]}, {"text": "We distributed the workload across a number of people, including shared-task participants, interested volunteers, and a small number of paid annotators (recruited by the participating sites).", "labels": [], "entities": []}, {"text": "More than 130 people participated in the manual evaluation, with 91 people putting in more than an hour's worth of effort, and 29 putting in more than four hours.", "labels": [], "entities": []}, {"text": "There was a collective total of 361 hours of labor.", "labels": [], "entities": []}, {"text": "We asked annotators to evaluate system outputs by ranking translated sentences relative to each other.", "labels": [], "entities": []}, {"text": "This was our official determinant of translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8743048310279846}]}, {"text": "The total number of judgments collected for the different ranking tasks is given in Table 6.", "labels": [], "entities": []}, {"text": "We performed the manual evaluation of the individual systems separately from the manual evaluation of the system combination entries, rather than comparing them directly against each other.", "labels": [], "entities": []}, {"text": "Last year's results made it clear that there is a large (expected) gap in performance between the two groups.", "labels": [], "entities": []}, {"text": "This year, we opted to reduce the number of pairwise comparisons with the hope that we would be more likely to find statistically significant differences between the systems in the same groups.", "labels": [], "entities": []}, {"text": "To that same end, we also eliminated the editing/acceptability task that was featured in last year's evaluation, instead we had annotators focus solely on the system ranking task.", "labels": [], "entities": []}, {"text": "In addition to allowing us to analyze the translation quality of different systems, the data gathered during the manual evaluation is useful for validating automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "Our evaluation shared task is similar to the MetricsMATR workshop (Metrics for MAchine TRanslation) that NIST runs).", "labels": [], "entities": [{"text": "NIST", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.9168112277984619}]}, {"text": "Table 11 lists the participants in this task, along with their metrics.", "labels": [], "entities": []}, {"text": "A total of 21 metrics and their variants were submitted to the evaluation task by 9 research groups.", "labels": [], "entities": []}, {"text": "We asked metrics developers to score the outputs of the machine translation systems and system combinations at the system-level and at the segmentlevel.", "labels": [], "entities": []}, {"text": "The system-level metrics scores are given in the Appendix in.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9793871641159058}]}, {"text": "The main goal of the evaluation shared task is not to score the systems, but instead to validate the use of automatic metrics by measuring how strongly they correlate with human judgments.", "labels": [], "entities": []}, {"text": "We used the human judgments collected during the manual evaluation for the translation task and the system combination task to calculate how well metrics correlate at system-level and at the segment-level.", "labels": [], "entities": [{"text": "translation task", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.9196377098560333}]}, {"text": "This year the strongest metric was anew metric developed by Columbia and ETS called MTeRaterPlus.", "labels": [], "entities": [{"text": "ETS", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.6286040544509888}, {"text": "MTeRaterPlus", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9054889678955078}]}, {"text": "MTeRater-Plus is a machine-learning-based metric that use features from ETS's e-rater, an automated essay scoring engine designed to assess writing proficiency).", "labels": [], "entities": [{"text": "MTeRater-Plus", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9093831181526184}, {"text": "ETS's e-rater", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.8724345564842224}]}, {"text": "The features include sentence-level and document-level information.", "labels": [], "entities": []}, {"text": "Some examples of the e-rater features include: \u2022 Preposition features that calculate the probability of prepositions appearing in the given context of a sentence \u2022 Collocation features that indicate whether the collocations in the document are typical of native use ().", "labels": [], "entities": []}, {"text": "\u2022 A sentence fragment feature that counts the number of ill-formed sentences in a document.", "labels": [], "entities": []}, {"text": "\u2022 A feature that counts the number of words with inflection errors \u2022 A feature that counts the the number of article errors in the sentence citeHan2006.", "labels": [], "entities": []}, {"text": "MTeRater uses only the e-rater features, and measures fluency without any need for reference translations.", "labels": [], "entities": [{"text": "MTeRater", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9506005644798279}]}, {"text": "MTeRater-Plus is a meta-metric that incorporates adequacy by combining MTeRater with other MT evaluation metrics and heuristics that take the reference translations into account.", "labels": [], "entities": [{"text": "MTeRater-Plus", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9507174491882324}]}, {"text": "Please refer to the proceedings for papers providing detailed descriptions of all of the metrics.: System-level Spearman's rho correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value.", "labels": [], "entities": []}, {"text": "We did not calculate correlations with the human judgments for the system combinations for the out of English direction, because none of them had more than 4 items.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: A summary of the WMT11 ranking task, showing the number of systems and number of labels collected in  each of the individual and system combination tracks. The system count does not include the reference translation,  which was included in the evaluation, and so a value under \"Labels per System\" can be obtained only after adding 1  to the system count, before dividing the label count (e.g. in German-English, 4, 620/21 = 220.0).", "labels": [], "entities": [{"text": "WMT11 ranking task", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6973444223403931}]}, {"text": " Table 7: Inter-and intra-annotator agreement rates, for the various manual evaluation tracks of WMT11. See", "labels": [], "entities": [{"text": "WMT11", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9221858978271484}]}, {"text": " Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics  as baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9988270401954651}, {"text": "TER", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9902993440628052}]}, {"text": " Table 12: System-level Spearman's rho correlation of the  automatic evaluation metrics with the human judgments  for translation out of English, ordered by average abso- lute value. We did not calculate correlations with the hu- man judgments for the system combinations for the out of  English direction, because none of them had more than 4  items.", "labels": [], "entities": [{"text": "abso- lute value", "start_pos": 165, "end_pos": 181, "type": "METRIC", "confidence": 0.9184203296899796}]}, {"text": " Table 13: System-level Spearman's rho correlation of the automatic evaluation metrics with the human judgments  for translation into English, ordered by average absolute value for the European languages. We did not calculate  correlations with the human judgments for the system combinations for Czech to English and for Haitian Creole to  English, because they had too few items (\u2264 4) for reliable statistics.", "labels": [], "entities": []}, {"text": " Table 14: Segment-level Kendall's tau correlation of the  automatic evaluation metrics with the human judgments  for translation into English, ordered by average correla- tion.", "labels": [], "entities": []}, {"text": " Table 15: Segment-level Kendall's tau correlation of the  automatic evaluation metrics with the human judgments  for translation out of English, ordered by average corre- lation.", "labels": [], "entities": [{"text": "translation out of English", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.8891282081604004}, {"text": "corre- lation", "start_pos": 165, "end_pos": 178, "type": "METRIC", "confidence": 0.96735546986262}]}, {"text": " Table 18: Head to head comparisons for the tunable met- rics task. The numbers indicate how often the system in  the column was judged to be better than the system in  the row. The difference between 100 and the sum of the  corresponding cells is the percent of time that the two  systems were judged to be equal.", "labels": [], "entities": []}, {"text": " Table 19: Ranking scores for entries in the Czech-English task (individual system track).", "labels": [], "entities": []}, {"text": " Table 20: Ranking scores for entries in the English-Czech task (individual system track).", "labels": [], "entities": []}, {"text": " Table 21: Ranking scores for entries in the German-English task (individual system track).", "labels": [], "entities": []}, {"text": " Table 22: Ranking scores for entries in the English-German task (individual system track).", "labels": [], "entities": []}, {"text": " Table 23: Ranking scores for entries in the Spanish-English task (individual system track).", "labels": [], "entities": []}, {"text": " Table 24: Ranking scores for entries in the English-Spanish task (individual system track).", "labels": [], "entities": []}, {"text": " Table 25: Ranking scores for entries in the French-English task (individual system track).", "labels": [], "entities": []}, {"text": " Table 26: Ranking scores for entries in the English-French task (individual system track).", "labels": [], "entities": []}, {"text": " Table 27: Ranking scores for entries in the Haitian Creole (Clean)-English task (individual system track).", "labels": [], "entities": []}, {"text": " Table 28: Ranking scores for entries in the Haitian Creole (Raw)-English task (individual system track).", "labels": [], "entities": []}, {"text": " Table 29: Ranking scores for entries in the Czech-English task (system combination track).", "labels": [], "entities": []}, {"text": " Table 30: Ranking scores for entries in the English-Czech task (system combination track).", "labels": [], "entities": []}, {"text": " Table 31: Ranking scores for entries in the German-English task (system combination track).", "labels": [], "entities": []}, {"text": " Table 32: Ranking scores for entries in the English-German task (system combination track).", "labels": [], "entities": []}, {"text": " Table 33: Ranking scores for entries in the Spanish-English task (system combination track).", "labels": [], "entities": []}, {"text": " Table 34: Ranking scores for entries in the English-Spanish task (system combination track).", "labels": [], "entities": []}, {"text": " Table 35: Ranking scores for entries in the French-English task (system combination track).", "labels": [], "entities": []}, {"text": " Table 36: Ranking scores for entries in the English-French task (system combination track).", "labels": [], "entities": []}, {"text": " Table 37: Ranking scores for entries in the Haitian Creole (Clean)-English task (system combination track).", "labels": [], "entities": []}, {"text": " Table 38: Ranking scores for entries in the Haitian Creole (Raw)-English task (system combination track).", "labels": [], "entities": []}, {"text": " Table 39: Automatic evaluation metric scores for systems in the WMT11 Czech-English News Task  (newssyscombtest2011)", "labels": [], "entities": [{"text": "WMT11 Czech-English News Task  (newssyscombtest2011)", "start_pos": 65, "end_pos": 117, "type": "DATASET", "confidence": 0.8920771053859166}]}, {"text": " Table 43: Automatic evaluation metric scores for systems in the WMT11 English-Czech News Task  (newssyscombtest2011)", "labels": [], "entities": [{"text": "WMT11 English-Czech News Task  (newssyscombtest2011)", "start_pos": 65, "end_pos": 117, "type": "DATASET", "confidence": 0.8902496354920524}]}, {"text": " Table 44: Automatic evaluation metric scores for systems in the WMT11 English-German News Task  (newssyscombtest2011)", "labels": [], "entities": [{"text": "WMT11 English-German News Task  (newssyscombtest2011)", "start_pos": 65, "end_pos": 118, "type": "DATASET", "confidence": 0.888482425894056}]}, {"text": " Table 45: Automatic evaluation metric scores for systems in the WMT11 English-French News Task  (newssyscombtest2011)", "labels": [], "entities": [{"text": "WMT11 English-French News Task  (newssyscombtest2011)", "start_pos": 65, "end_pos": 118, "type": "DATASET", "confidence": 0.8829152839524406}]}, {"text": " Table 46: Automatic evaluation metric scores for systems in the WMT11 English-Spanish News Task  (newssyscombtest2011)", "labels": [], "entities": [{"text": "WMT11 English-Spanish News Task  (newssyscombtest2011)", "start_pos": 65, "end_pos": 119, "type": "DATASET", "confidence": 0.8814394899777004}]}, {"text": " Table 47: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (clean)-English Haitian  Creole SMS Emergency Response Featured Translation Task (newssyscombtest2011)", "labels": [], "entities": [{"text": "WMT11 Haitian Creole (clean)-English Haitian  Creole SMS Emergency Response Featured Translation Task", "start_pos": 65, "end_pos": 166, "type": "TASK", "confidence": 0.8144992470741272}]}]}