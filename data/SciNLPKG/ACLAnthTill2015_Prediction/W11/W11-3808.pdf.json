{"title": [], "abstractContent": [{"text": "Problems for parsing morphologically rich languages are, amongst others, caused by the higher variability in structure due to less rigid word order constraints and by the higher number of different lexical forms.", "labels": [], "entities": [{"text": "parsing morphologically rich languages", "start_pos": 13, "end_pos": 51, "type": "TASK", "confidence": 0.9155252575874329}]}, {"text": "Both properties can result in sparse data problems for statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8616437613964081}]}, {"text": "We present a simple approach for addressing these issues.", "labels": [], "entities": []}, {"text": "Our approach makes use of self-training on instances selected with regard to their similarity to the annotated data.", "labels": [], "entities": []}, {"text": "Our similarity measure is based on the perplexity of part-of-speech trigrams of new instances measured against the annotated training data.", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9728367924690247}]}, {"text": "Preliminary results show that our method outperforms a self-training setting where instances are simply selected by order of occurrence in the corpus and argue that self-training is a cheap and effective method for improving parsing accuracy for morphologically rich languages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 225, "end_pos": 232, "type": "TASK", "confidence": 0.9605217576026917}, {"text": "accuracy", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.8214746117591858}]}], "introductionContent": [{"text": "Up to now, most work on statistical parsing has been focussed on English, a language with a configurational word order and little morphology.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.9034528434276581}]}, {"text": "The inherent properties of morphologically rich languages include a higher variability in structure due to less rigid word order constraints, thus leading to greater attachment ambiguities, and a higher number of different word forms, leading to coverage problems caused by sparse data.", "labels": [], "entities": []}, {"text": "These issues pose a great challenge to statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8543252348899841}]}, {"text": "One sensible way to treat these issues is the development of more sophisticated parsing models adapted to the language-specific properties of morphologically rich languages.", "labels": [], "entities": []}, {"text": "Another, simpler approach, tries to overcome the problems outlined above by expanding the training data.", "labels": [], "entities": []}, {"text": "Possible approaches for expansion include self-training and active learning.", "labels": [], "entities": []}, {"text": "For self-training a parser is trained on a seed dataset of gold trees and applied to new text, either coming from the same domain or, in the context of domain adaptation, from a domain different from the seed data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.7573627233505249}]}, {"text": "The parser output trees are then added to the seed data and the parser is re-trained on its own output.", "labels": [], "entities": []}, {"text": "For the in-domain setting it is quite unintuitive why this approach should work, as we only add more of what the parser already knows, and we also include a considerable amount of errors in the training set.", "labels": [], "entities": []}, {"text": "Active learning, on the other hand, tries to expand the training set by selecting those instances which provide the parser with a high amount of new information.", "labels": [], "entities": []}, {"text": "The underlying idea is that those instances have yet to be learned by the parser and thus will support the learning process.", "labels": [], "entities": []}, {"text": "These instances have to be labelled by a human coder (often called the oracle) and then added to the seed data.", "labels": [], "entities": []}, {"text": "The parser is re-trained and new instances can be selected, based on the new model.", "labels": [], "entities": []}, {"text": "The intuition why this approach should work is more straightforward than for the self-training setting: we do provide the model with new, unseen information and, assuming that our oracle is right, the amount of noise is kept to a minimum.", "labels": [], "entities": []}, {"text": "The great advantage of self-training, however, is that it is unsupervised, thus obviating the need for human annotation.", "labels": [], "entities": []}, {"text": "In this study we test the potential of self-training for parsing morphologically rich languages.", "labels": [], "entities": [{"text": "parsing morphologically rich languages", "start_pos": 57, "end_pos": 95, "type": "TASK", "confidence": 0.9099715650081635}]}, {"text": "We present experiments for German, a language with rich morphology (relative to English) and semi-free word order, and show that self-training can improve parsing accuracy when only a small amount of labelled training data is available.", "labels": [], "entities": [{"text": "parsing", "start_pos": 155, "end_pos": 162, "type": "TASK", "confidence": 0.9715538620948792}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9013534188270569}]}, {"text": "Furthermore, we show that selecting sentences for self-training on the basis of similarity to the training data is a good strategy which can further improve results while avoiding the downside of expensive human annotation.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reports on related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the setup of our experiments and reports preliminary results.", "labels": [], "entities": []}, {"text": "In Section 4 we conclude and outline future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we use the unlexicalised Berkeley parser () and the lexicalised form of the Stanford parser (.", "labels": [], "entities": []}, {"text": "The Berkeley parser is an unlexicalised latent variable PCFG parser which uses a split-andmerge technique to automatically refine the training data.", "labels": [], "entities": []}, {"text": "The splits result in more and more finegrained subcategories, which are merged again if not proven useful.", "labels": [], "entities": []}, {"text": "We train a PCFG from each of the 8 training subsets by carrying out six cycles of the split-and-merge process.", "labels": [], "entities": []}, {"text": "The Stanford parser provides a factored probabilistic model combining a PCFG with a dependency model.", "labels": [], "entities": []}, {"text": "We use the Stanford parser in its lexicalised, markovised form.", "labels": [], "entities": []}, {"text": "Both parsers were trained on the syntactic nodes of the trees only, stripping off the grammatical function (GF) labels from the trees.", "labels": [], "entities": []}, {"text": "We add the GF to the parser output in a postprocessing step, using the method of, and include GF in the evaluation.", "labels": [], "entities": []}, {"text": "Training the parser on syntactic node labels without GF has the advantage of considerably reducing the number of atomic labels in the grammar.", "labels": [], "entities": []}, {"text": "As a result, we obtain smaller grammars which are more efficient for parsing, and we also avoid sparse data problems.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9805473685264587}]}, {"text": "We also lose information, but the treebank refinement techniques used by the Berkeley parser easily recover this information and thus yield comparable results for both settings.", "labels": [], "entities": []}, {"text": "As an additional benefit we avoid the problem of multiple governable GF assigned to children of the same parent node, an error occasionally made by the Berkeley parser.", "labels": [], "entities": []}, {"text": "The method by), on the other hand, uses linguistically informed hard constraints to prevent these errors.", "labels": [], "entities": []}, {"text": "While we computed perplexity on the basis of the gold POS tags in TiGer treebank and automatically assigned POS tags to the T\u00fcBa-D/Z sentences, for parsing we used raw text as input and let the parsers assign their own POS tags.", "labels": [], "entities": [{"text": "TiGer treebank", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9666276574134827}]}], "tableCaptions": [{"text": " Table 1: Parsing results (PARSEVAL) for the different self-training settings, including GF in the evaluation (asterisks  indicate significant differences between self-training and the baseline: p=0.001***, p=0.005**, p=0.01*, p=0.05 .)", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9502638578414917}, {"text": "PARSEVAL", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9897714257240295}, {"text": "GF", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9889333248138428}]}]}