{"title": [{"text": "Exploring Effective Dialogue Act Sequences in One-on-one Computer Science Tutoring Dialogues", "labels": [], "entities": [{"text": "Exploring Effective Dialogue Act Sequences in One-on-one Computer Science Tutoring Dialogues", "start_pos": 0, "end_pos": 92, "type": "TASK", "confidence": 0.7235949256203391}]}], "abstractContent": [{"text": "We present an empirical study of one-on-one human tutoring dialogues in the domain of Computer Science data structures.", "labels": [], "entities": []}, {"text": "We are interested in discovering effective tutoring strategies, that we frame as discovering which Dialogue Act (DA) sequences correlate with learning.", "labels": [], "entities": []}, {"text": "We employ multiple linear regression, to discover the strongest models that explain why students learn during one-on-one tutoring.", "labels": [], "entities": []}, {"text": "Importantly, we define \"flexible\" DA sequence, in which extraneous DAs can easily be discounted.", "labels": [], "entities": []}, {"text": "Our experiments reveal several cognitively plausible DA sequences which significantly correlate with learning outcomes.", "labels": [], "entities": []}], "introductionContent": [{"text": "One-on-one tutoring has been shown to be a very effective form of instruction compared to other educational settings.", "labels": [], "entities": []}, {"text": "Much research on discovering why this is the case has focused on the analysis of the interaction between tutor and students).", "labels": [], "entities": []}, {"text": "In the last fifteen years, many such analyses have been approached from a Natural Language Processing (NLP) perspective, with the goal of building interfaces that allow students to naturally interact with Intelligent Tutoring Systems (ITSs) ().", "labels": [], "entities": []}, {"text": "There have been two main types of approaches to the analysis of tutoring dialogues.", "labels": [], "entities": []}, {"text": "The first kind of approach compares groups of subjects interacting with different tutors (, in some instances contrasting the number of occurrences of relevant features between the groups (.", "labels": [], "entities": []}, {"text": "However, as we already argued in (, this code-and-count methodology only focuses on what a certain type of tutor (assumed to be better according to certain criteria) does differently from another tutor, rather than on strategies that maybe effective independently from their frequencies of usage by different types of tutor.", "labels": [], "entities": []}, {"text": "Indeed we had followed this same methodology in previous work), but a key turning point for our work was to discover that our expert and novice tutors were equally effective (please see below).", "labels": [], "entities": []}, {"text": "The other kind of approach uses linear regression analysis to find correlations between dialogue features and learning gains.", "labels": [], "entities": []}, {"text": "Whereas linear regression is broadly used to analyze experimental data, only few analyses of tutorial data or tutoring experiments use it.", "labels": [], "entities": []}, {"text": "In this paper, we follow Litman and Forbes- in correlating sequences of Dialogue Acts (DAs) with learning gains.", "labels": [], "entities": []}, {"text": "We extend that work in that our bigram and trigram DAs are not limited to tutor-student DA bigramsLitman and Forbes- only considers bigrams where one DA comes from the tutor's turn and one from the student's turn, in either order.", "labels": [], "entities": []}, {"text": "Importantly, we further relax constraints on how these sequences are built, in particular, we are able to model DA sequences that include gaps.", "labels": [], "entities": []}, {"text": "This allows us to discount the noise resulting from intervening DAs that do not contribute to the effectiveness of the specific sequence.", "labels": [], "entities": []}, {"text": "For example, if we want to explore sequences in which the tutor first provides some knowledge to solve the problem (DPI) and then knowledge about the problem (DDI) (DPI and DDI will be explained later), an exchange such as the one in should betaken into account (JAC and later LOW are the tutors, students are indicated with a numeric code, such as 113 in).", "labels": [], "entities": []}, {"text": "However, if we just use adjacent utterances, the ok from the student (113) interrupts the sequence, and we could not take this example into account.", "labels": [], "entities": []}, {"text": "By allowing gaps in our sequences, we test a large number of linear regression models, some of which result in significant models that can be used as guidelines to design an ITS.", "labels": [], "entities": []}, {"text": "Specifically, these guidelines will be used for further improvement of iList, an ITS that provides feedback on linked list problems and that we have developed over the last few years.", "labels": [], "entities": []}, {"text": "Five different versions of iList have been evaluated with 220 users (.", "labels": [], "entities": []}, {"text": "iList is available at http://www.digitaltutor.net, and has been used by more than 550 additional users at 15 different institutions.", "labels": [], "entities": [{"text": "iList", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8829185366630554}]}, {"text": "JAC: so we would set k equal toe and then delete.", "labels": [], "entities": [{"text": "JAC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8676459193229675}, {"text": "toe", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.931401789188385}]}, {"text": "JAC: so we've inserted this whole list inhere.", "labels": [], "entities": [{"text": "JAC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8986445665359497}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the CS-Tutoring corpus, including data collection, transcription, and annotation.", "labels": [], "entities": [{"text": "CS-Tutoring corpus", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.7965678572654724}]}, {"text": "In Section 3, we introduce our methodology that combines multiple linear regression with ngrams of DAs that allow for gaps.", "labels": [], "entities": []}, {"text": "We discuss our experiments and results in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we build on our previous results reported in . There we had shown that, for lists and stacks, models that include positive and negative feedback are significant and explain more of the variance with respect to models that only include pre-test score, or include pre-test score and session length.", "labels": [], "entities": []}, {"text": "still follows the same approach, but adds to the regression models the additional DAs, DPI, DDI, Prompt and SI that had not been included in that earlier work.", "labels": [], "entities": [{"text": "DAs", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9747143387794495}, {"text": "Prompt", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9938653111457825}]}, {"text": "The column M refers to three types of models, Model 1 only includes Pre-test, Model 2 adds session length to Pretest, and Model 3 adds to Pre-test all the DAs.", "labels": [], "entities": []}, {"text": "As evidenced by the table, only DPI provides a marginally significant contribution, and only for lists.", "labels": [], "entities": [{"text": "DPI", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.5999670624732971}]}, {"text": "Note that length is not included in Model 3's.", "labels": [], "entities": [{"text": "length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9982998967170715}]}, {"text": "We did run all the equivalent models to Model 3's including length.", "labels": [], "entities": [{"text": "length", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9854595065116882}]}, {"text": "The R 2 's stay the same (literally, to the second decimal digit), or minimally decrease.", "labels": [], "entities": []}, {"text": "However, in all these Model 3+'s that include length no DA is significant, hence we consider them as less explanatory than the Model 3's in: finding that a longer dialogue positively affects learning does not tell us what happens during that dialogue which is conducive to learning.", "labels": [], "entities": [{"text": "DA", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9859646558761597}]}, {"text": "Note that the \u03b2 weights on the pre-test are always negative in every model, namely, students with higher pre-test scores learn less than students with lower pre-test scores.", "labels": [], "entities": []}, {"text": "This is an example of the wellknown ceiling effect: students with more previous knowledge have less learning opportunity.", "labels": [], "entities": []}, {"text": "Also noticeable is that the R 2 for the Trees models are much higher than for Lists and Stacks, and that for Trees no DA is significant (although there will be significant trigram models that involve DAs for Trees).", "labels": [], "entities": [{"text": "R 2", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9829586446285248}, {"text": "DA", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.992472767829895}]}, {"text": "We have observed that Lists are in general more difficult than Stacks and Trees (well, at least than binary search trees) for students.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CS Tutoring Corpus -Descriptives", "labels": [], "entities": [{"text": "CS Tutoring Corpus -Descriptives", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.5365184128284455}]}, {"text": " Table 2: Inter-Coder Agreement in Corpus", "labels": [], "entities": []}, {"text": " Table 3: Learning gains and t-test statistics", "labels": [], "entities": []}, {"text": " Table 4: Linear Regression -Human Tutoring", "labels": [], "entities": []}, {"text": " Table 5: DPI, Feedback Model", "labels": [], "entities": []}, {"text": " Table 6: {FB, DDI} Model", "labels": [], "entities": [{"text": "FB", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.6317655444145203}]}, {"text": " Table 4. This suggests that an ef- fective tutoring sequence is to provide instruction on  how to solve the problem (DPI), then Feedback on  what the student does, and finally some declarative  instruction (DDI).", "labels": [], "entities": []}, {"text": " Table 8: Highest R 2 Models", "labels": [], "entities": [{"text": "Highest R 2", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8534250458081564}]}]}