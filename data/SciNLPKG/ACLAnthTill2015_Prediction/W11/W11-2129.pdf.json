{"title": [{"text": "Productive Generation of Compound Words in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.7329076727231344}]}], "abstractContent": [{"text": "In many languages the use of compound words is very productive.", "labels": [], "entities": []}, {"text": "A common practice to reduce sparsity consists in splitting compounds in the training data.", "labels": [], "entities": []}, {"text": "When this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order.", "labels": [], "entities": []}, {"text": "Furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output.", "labels": [], "entities": [{"text": "compound merging", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7675256431102753}]}, {"text": "We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order.", "labels": [], "entities": []}, {"text": "We also propose new heuristic methods for merging components that outper-form all known methods, and a learning-based method that has similar accuracy as the heuris-tic method, is better at producing novel compounds , and can operate with no background linguistic resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9968709349632263}]}], "introductionContent": [{"text": "In many languages including most of the Germanic (German, Swedish etc.) and Uralic (Finnish, Hungarian etc.) language families so-called closed compounds are used productively.", "labels": [], "entities": []}, {"text": "Closed compounds are written as single words without spaces or other word boundaries, as the Swedish: gatstenshuggare gata + sten + huggare paving stone cutter street stone cutter To cope with the productivity of the phenomenon, any effective strategy should be able to correctly process compounds that have never been seen in the training data as such, although possibly their components have, either in isolation or within a different compound.", "labels": [], "entities": []}, {"text": "The extended use of compounds make them problematic for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7982886135578156}]}, {"text": "For translation into a compounding language, often fewer compounds than in normal texts are produced.", "labels": [], "entities": [{"text": "translation into a compounding language", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.6697265923023223}]}, {"text": "This can be due to the fact that the desired compounds are missing in the training data, or that they have not been aligned correctly.", "labels": [], "entities": []}, {"text": "When a compound is the idiomatic word choice in the translation, a MT system can often produce separate words, genitive or other alternative constructions, or translate only one part of the compound.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9837498664855957}]}, {"text": "Most research on compound translation in combination with SMT has been focused on translation from a compounding language, into a noncompounding one, typically into English.", "labels": [], "entities": [{"text": "compound translation", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8371423184871674}, {"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9885966181755066}]}, {"text": "A common strategy then consists in splitting compounds into their components prior to training and translation.", "labels": [], "entities": []}, {"text": "Only few have investigated translation into a compounding language.", "labels": [], "entities": [{"text": "translation", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9786170125007629}]}, {"text": "For translation into a compounding language, the process becomes: \u2022 Splitting compounds on the target (compounding language) side of the training corpus; \u2022 Learn a translation model from this split training corpus from source (e.g. English) into decomposed-target (e.g. decomposed-German) \u2022 At translation time, translate using the learned model from source into decomposed-target.", "labels": [], "entities": []}, {"text": "\u2022 Apply a post-processing \"merge\" step to reconstruct compounds.", "labels": [], "entities": [{"text": "Apply", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9632866978645325}]}, {"text": "The merging step must solve two problems: identify which words should be merged into compounds, and choose the correct form of the compound parts.", "labels": [], "entities": []}, {"text": "The former problem can become hopelessly difficult if the translation did not put components nicely side by side and in the correct order.", "labels": [], "entities": []}, {"text": "Preliminary to merging, then, the problem of promoting translations where compound elements are correctly positioned needs to be addressed.", "labels": [], "entities": []}, {"text": "We call this promoting compound coalescence.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on translation from English into Swedish and Danish on two different corpora, an automotive corpus collected from a proprietary translation memory, and on Europarl () for the merging experiments.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 180, "end_pos": 188, "type": "DATASET", "confidence": 0.9719240069389343}]}, {"text": "We used factored translation   Compounds were split before training using a corpus-based method (.", "labels": [], "entities": []}, {"text": "For each word we explored all possible segmentations into parts that had at least 3 characters, and choose the segmentation which had the highest arithmetic mean of frequencies for each part in the training corpus.", "labels": [], "entities": [{"text": "arithmetic mean of frequencies", "start_pos": 146, "end_pos": 176, "type": "METRIC", "confidence": 0.863696962594986}]}, {"text": "We constrained the splitting based on part-of-speech by only allowing splitting options where the compound head had the same tag as the full word.", "labels": [], "entities": [{"text": "splitting", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.9651104807853699}]}, {"text": "The split compound parts kept their form, which can be special to compounds, and no symbols or other markup were added.", "labels": [], "entities": []}, {"text": "The experiment setup is summarized in.", "labels": [], "entities": []}, {"text": "The extra training sentences for CRF are sentences that were not also used to train the SMT system.", "labels": [], "entities": [{"text": "CRF", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.834868848323822}, {"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.990874171257019}]}, {"text": "For tuning, test and validation data we used 1,000 sentence sets, except for Swedish auto, where we used 2,000 sentences for tuning.", "labels": [], "entities": []}, {"text": "In the Swedish experiments we split nouns, adjectives and verbs, and used the full POS-set, except in the coalescence experiments where we compared the full and restricted POS-sets.", "labels": [], "entities": []}, {"text": "For Danish we only split nouns, and used the restricted POS-set.", "labels": [], "entities": []}, {"text": "For frequency calculations of compounds and compound parts that were needed for compound splitting and some of the com-pound merging strategies, we used the respective training data in all cases.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8227162957191467}]}, {"text": "Significance testing was performed using approximate randomization), with 10,000 iterations, and \u03b1 < 0.05.", "labels": [], "entities": []}, {"text": "We performed experiments with factored translation models with the restricted part-of-speech set on the Danish and Swedish automotive corpus.", "labels": [], "entities": [{"text": "Danish and Swedish automotive corpus", "start_pos": 104, "end_pos": 140, "type": "DATASET", "confidence": 0.8153666734695435}]}, {"text": "In these experiments we compared the restricted part-of-speech set we suggest in this work to several baseline systems without any compound processing and with factored models using the extended part-of-speech set suggested by.", "labels": [], "entities": []}, {"text": "Compound parts were merged using the POS-based heuristic.", "labels": [], "entities": []}, {"text": "Results are reported on two standard metrics, NIST) and Bleu (), on lower-cased data.", "labels": [], "entities": [{"text": "NIST", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9228455424308777}, {"text": "Bleu", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9919895529747009}]}, {"text": "For all sequence models we use 3-grams.", "labels": [], "entities": []}, {"text": "Results on the two Automotive corpora are summarized in.", "labels": [], "entities": [{"text": "Automotive corpora", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.7534712851047516}]}, {"text": "The scores are very high, which is due to the fact that it is an easy domain with many repetitive sentence types.", "labels": [], "entities": []}, {"text": "On the Danish dataset, we observe significant improvements in BLEU and NIST over the baseline for all methods where compounds were split before translation and merged afterwards.", "labels": [], "entities": [{"text": "Danish dataset", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.9830777645111084}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9984105825424194}]}, {"text": "Some of the gain is already obtained using a language model on the extended part-of-speech set.", "labels": [], "entities": []}, {"text": "Additional gains can however be obtained using instead a language model on a reduced set of POS-tags (RPOS), and with a count feature explicitly boosting desirable RPOS sequences.", "labels": [], "entities": []}, {"text": "The count feature on undesirable sequences did not bring any improvements over any of the systems with compound splitting.", "labels": [], "entities": [{"text": "count", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9618415832519531}, {"text": "compound splitting", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7417015731334686}]}, {"text": "Results on the Swedish automotive corpus are less clear-cut than for Danish, with mostly insignificant differences between systems.", "labels": [], "entities": [{"text": "Swedish automotive corpus", "start_pos": 15, "end_pos": 40, "type": "DATASET", "confidence": 0.7687150041262308}]}, {"text": "The system with decomposition and a restricted part-of-speech model is significantly better on Bleu than all other systems, except the system with decomposition and a standard part-of-speech model.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9553192853927612}]}, {"text": "Not splitting actually gives the highest NIST score, even though the difference to the other systems is not significant, except for the system with a combination of a trained RPOS model and a boost model, which also has significantly lower Bleu score than the other systems with compound splitting.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.5792669653892517}, {"text": "Bleu score", "start_pos": 240, "end_pos": 250, "type": "METRIC", "confidence": 0.9857036769390106}]}, {"text": "We compared alternative combinations of heuristics on our three validation datasets, see.", "labels": [], "entities": []}, {"text": "In order to estimate the amount of false negatives for all three heuristics, we inspected the first 100 sentences of each validation set, looking for words that should be merged, but were not marked by any of the heuristics.", "labels": [], "entities": []}, {"text": "In no case we could find any such words, so we thus assume that between them, the heuristics can find the overwhelming majority of all compounds to be merged.", "labels": [], "entities": []}, {"text": "We conducted around of preliminary experiments to identify the best combination of the heuristics available at training time (modified list-based, POSbased, and reference-based) to use to create automatically the training data for the CRF.", "labels": [], "entities": []}, {"text": "Best results on the validation data are obtained by different combination of heuristics for the three datasets, as could be expected by the different distribution of errors in.", "labels": [], "entities": []}, {"text": "In the experiments below we trained the CRF using for each dataset the combination of heuristics corresponding to leaving out the grey portions of the Venn diagrams.", "labels": [], "entities": []}, {"text": "This sort of preliminary optimization requires hand-labelling a certain amount of data.", "labels": [], "entities": []}, {"text": "Based on our experiments, skipping this optimization and just using ref\u2228(list\u2227POS) (the optimal configuration for the Swedish-English Europarl corpus) seems to be a reasonable alternative.", "labels": [], "entities": [{"text": "Swedish-English Europarl corpus", "start_pos": 118, "end_pos": 149, "type": "DATASET", "confidence": 0.6968141992886862}]}, {"text": "The validation data was also used to set a frequency cut-off for feature occurrences (set at 3 in the following experiments) and to tune the regularization parameter in the CRF objective function.: Results of experiments with methods for promoting coalescence.", "labels": [], "entities": []}, {"text": "Compounds are merged based on the POS heuristic.", "labels": [], "entities": []}, {"text": "Scores that are significantly better than Base+POSLM, are marked '*', and scores that are also better than POSLM with '**'.", "labels": [], "entities": []}, {"text": "Results are largely insensitive to variations in these hyper-parameters, especially to the CRF regularization parameter.", "labels": [], "entities": []}, {"text": "For the Danish auto corpus we had access to training data that were not also used to train the SMT system, that we used to compare the performance with that on the possibly biased training data that was also used to train the SMT system.", "labels": [], "entities": [{"text": "Danish auto corpus", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.9585990905761719}, {"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9843176603317261}, {"text": "SMT", "start_pos": 226, "end_pos": 229, "type": "TASK", "confidence": 0.9803264141082764}]}, {"text": "There were no significant differences between the two types of training data on validation data, which confirmed that reusing the SMT training data for CRF training was a reasonable strategy.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 130, "end_pos": 142, "type": "TASK", "confidence": 0.8838382363319397}, {"text": "CRF training", "start_pos": 152, "end_pos": 164, "type": "TASK", "confidence": 0.847850114107132}]}, {"text": "The overall merging results of the heuristics, the best sequence labeler, and the sequence labeler without POS are shown in.", "labels": [], "entities": [{"text": "POS", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9554674029350281}]}, {"text": "Notice how the (modified) list and POS heuristics have complementary sets of false negatives: when merging on the OR of the two heuristics, the number of false negatives decreases drastically, in general compensating for the inevitable increase in false positives.", "labels": [], "entities": [{"text": "OR", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9543891549110413}]}, {"text": "Among the heuristics, the combination of the improved list heuristic and the POS-based heuristic has a significantly higher recall and F-score than the POS-based heuristic alone in all cases except on the validation data for Swedish Auto, and than the listbased strategy in several cases.", "labels": [], "entities": [{"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9995001554489136}, {"text": "F-score", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.9763426184654236}, {"text": "Swedish Auto", "start_pos": 225, "end_pos": 237, "type": "DATASET", "confidence": 0.8898322284221649}]}, {"text": "The list heuristic alone performs reasonably well on the two Swedish data sets, but has a very low recall on the Danish dataset.", "labels": [], "entities": [{"text": "Swedish data sets", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.8611403107643127}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9992074370384216}, {"text": "Danish dataset", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.9652183353900909}]}, {"text": "In all three cases the SMT training data has been used for the list used by the heuristic, so this is unexpected, especially considering the fact that the Danish dataset is in the same domain as one of the Swedish datasets.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.8630526065826416}, {"text": "Danish dataset", "start_pos": 155, "end_pos": 169, "type": "DATASET", "confidence": 0.9428848922252655}, {"text": "Swedish datasets", "start_pos": 206, "end_pos": 222, "type": "DATASET", "confidence": 0.752937525510788}]}, {"text": "The Danish training data is smaller than the Swedish data though, which might bean influencing factor.", "labels": [], "entities": [{"text": "Danish training data", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7950982252756754}, {"text": "Swedish data", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.7499238848686218}]}, {"text": "It is possible that this heuristic could perform better also for Danish given more data for frequency calculations.", "labels": [], "entities": []}, {"text": "The sequence labeler is competitive with the heuristics; on F-score it is only significantly worse than any of the heuristics once, for Danish auto test data, and in several cases it has a significantly higher F-score than some of the heuristics.", "labels": [], "entities": [{"text": "F-score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9980868101119995}, {"text": "Danish auto test data", "start_pos": 136, "end_pos": 157, "type": "DATASET", "confidence": 0.7559495866298676}, {"text": "F-score", "start_pos": 210, "end_pos": 217, "type": "METRIC", "confidence": 0.9977890253067017}]}, {"text": "The sequence labeler has a higher precision, significantly so in three cases, than the best heuristic, the combination heuristic, which is positive, since erroneously merged compounds are usually more disturbing fora reader or post-editor than non-merged compounds.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9975460171699524}]}, {"text": "The sequence-labelling approach can be used also in the absence of a POS tagger, which can be important if no such tool of suitable quality is available for the target language and the domain of interest.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 69, "end_pos": 79, "type": "TASK", "confidence": 0.5981330126523972}]}, {"text": "We thus also trained a CRF-based compound merger without using POS features, and without using the POS-based heuristic when constructing the training data.", "labels": [], "entities": [{"text": "CRF-based compound merger", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6269021431605021}]}, {"text": "Compared to the CRF with access to POS-tags, on validation data F-score is significantly worse on the Europarl Swedish condition and the Automotive Danish condition, and are unchanged on Automotive Swedish.", "labels": [], "entities": [{"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9977995753288269}, {"text": "Europarl Swedish condition", "start_pos": 102, "end_pos": 128, "type": "DATASET", "confidence": 0.9441707531611124}, {"text": "Automotive Danish condition", "start_pos": 137, "end_pos": 164, "type": "DATASET", "confidence": 0.9087984164555868}, {"text": "Automotive Swedish", "start_pos": 187, "end_pos": 205, "type": "DATASET", "confidence": 0.9359554350376129}]}, {"text": "On test data there are no significant differences of the two sequence labelers on the two Automotive corpora.", "labels": [], "entities": [{"text": "Automotive corpora", "start_pos": 90, "end_pos": 108, "type": "DATASET", "confidence": 0.8517279326915741}]}, {"text": "On Swedish Europarl, the CRF without POS has a higher recall at the cost of a lower precision.", "labels": [], "entities": [{"text": "Swedish Europarl", "start_pos": 3, "end_pos": 19, "type": "DATASET", "confidence": 0.8104956448078156}, {"text": "POS", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9086777567863464}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9991921782493591}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9983189702033997}]}, {"text": "Compared to the list heuristic, which is the only other alternative strategy that works in the absence of a POS tagger, the CRF without POS performs significantly better on recall and F-score for Danish automotive, and mostly comparative on the two Swedish corpora.: Precision, Recall, and F-score for compound merging methods based on heuristics or sequence labelling on validation data and on held-out test data.", "labels": [], "entities": [{"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9995728135108948}, {"text": "F-score", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.9967729449272156}, {"text": "Precision", "start_pos": 267, "end_pos": 276, "type": "METRIC", "confidence": 0.9956842660903931}, {"text": "Recall", "start_pos": 278, "end_pos": 284, "type": "METRIC", "confidence": 0.947235107421875}, {"text": "F-score", "start_pos": 290, "end_pos": 297, "type": "METRIC", "confidence": 0.9974462985992432}]}, {"text": "The superscripts marks the systems that are significantly worse than the system in question (l-list, p-POS, lp-list\u2228POS, c-best CRF configuration, cp-CRF without POS).", "labels": [], "entities": []}, {"text": "The sequence labeler has the advantage over the heuristics that it is able to merge completely novel compounds, whereas the list strategy can only merge compounds that it has seen, and the POS-based strategy can create novel compounds, but only with known modifiers.", "labels": [], "entities": []}, {"text": "An inspection of the test data showed that there were a few novel compounds merged by the sequence labeler that were not identified with either of the heuristics.", "labels": [], "entities": []}, {"text": "In the test data we found knap+start (button start) and vand+nedsaenkning (water submersion) in Danish Auto, and kvarts sekel (quarter century) and bostad(s)+ers\u00e4ttning (housing grant) in Swedish Europarl.", "labels": [], "entities": [{"text": "Danish Auto", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.9099505841732025}, {"text": "Swedish Europarl", "start_pos": 188, "end_pos": 204, "type": "DATASET", "confidence": 0.8636133670806885}]}, {"text": "This confirms that the sequence labeler, from automatically labeled data based on heuristics, can learn to merge new compounds that the heuristics themselves cannot find.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of experiments with methods for promoting coalescence. Compounds are merged based on the POS  heuristic. Scores that are significantly better than Base+POSLM, are marked '*', and scores that are also better than  POSLM with '**'.", "labels": [], "entities": []}, {"text": " Table 4: Precision, Recall, and F-score for compound merging methods based on heuristics or sequence labelling on  validation data and on held-out test data. The superscripts marks the systems that are significantly worse than the  system in question (l-list, p-POS, lp-list\u2228POS, c-best CRF configuration, cp-CRF without POS).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999404788017273}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9932317733764648}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9990860223770142}]}]}