{"title": [{"text": "Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems", "labels": [], "entities": [{"text": "Meteor 1.3", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8712721467018127}, {"text": "Machine Translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.705381229519844}]}], "abstractContent": [{"text": "This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks.", "labels": [], "entities": [{"text": "EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks", "start_pos": 60, "end_pos": 143, "type": "TASK", "confidence": 0.7974331200122833}]}, {"text": "New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7567632794380188}, {"text": "paraphrase matching", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7354801297187805}]}, {"text": "We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to out-perform BLEU in minimum error rate training fora phrase-based Urdu-English system.", "labels": [], "entities": [{"text": "Ranking", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9825511574745178}, {"text": "Adequacy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9180445075035095}, {"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.998502254486084}]}], "introductionContent": [{"text": "The Meteor 1 metric () has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR.", "labels": [], "entities": [{"text": "Meteor 1 metric", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.558687557776769}, {"text": "ACL Workshop on Statistical Machine Translation", "start_pos": 116, "end_pos": 163, "type": "TASK", "confidence": 0.5688464641571045}, {"text": "NIST Metrics MATR", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.6789977351824442}]}, {"text": "However, previous versions of the metric are still limited by lack of punctuation handling, noise in paraphrase matching, and lack of discrimination between word types.", "labels": [], "entities": [{"text": "paraphrase matching", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7034809589385986}]}, {"text": "We introduce new resources for all WMT languages including text normalizers, filtered paraphrase tables, and function word lists.", "labels": [], "entities": [{"text": "WMT languages", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.8375024497509003}, {"text": "text normalizers", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7029489129781723}]}, {"text": "We show that the addition of these resources to Meteor allows tuning versions of the metric that show higher correlation with human translation rankings and adequacy scores on unseen The metric name has previously been stylized as \"ME-TEOR\" or \"METEOR\".", "labels": [], "entities": [{"text": "Meteor", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.9587014317512512}]}, {"text": "As of version 1.3, the official stylization is simply \"Meteor\".", "labels": [], "entities": []}, {"text": "The evaluation resources are modular, usable with any other evaluation metric or MT software.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9114311337471008}]}, {"text": "We also conduct a MT system tuning experiment on Urdu-English data to compare the effectiveness of using multiple versions of Meteor in minimum error rate training.", "labels": [], "entities": [{"text": "MT system tuning", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.8286113142967224}]}, {"text": "While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9982755184173584}, {"text": "Meteor", "start_pos": 144, "end_pos": 150, "type": "DATASET", "confidence": 0.8815763592720032}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9974539875984192}]}, {"text": "The versions of Meteor corresponding to the translation evaluation task submissions, (Ranking and Adequacy), are described in Sections 3 through 5 while the submission to the tunable metrics task, (Tuning), is described in Section 6.", "labels": [], "entities": [{"text": "translation evaluation task submissions", "start_pos": 44, "end_pos": 83, "type": "TASK", "confidence": 0.9031803756952286}]}], "datasetContent": [{"text": "To compare Meteor 1.3 against previous versions of the metric on the task of evaluating MT system outputs, we tune aversion for each language on 2009 WMT data and evaluate on 2010 data.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9656689763069153}, {"text": "aversion", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9924378991127014}, {"text": "WMT data", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.920531839132309}]}, {"text": "This replicates the 2010 WMT shared evaluation task, allowing comparison to Meteor 1.2.", "labels": [], "entities": [{"text": "WMT shared evaluation task", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.5576295554637909}]}, {"text": "lists correlation of each metric version with ranking judgments on tune and test data.", "labels": [], "entities": []}, {"text": "Meteor 1.3 shows significantly higher correlation on both tune and test data for English, French, and Spanish while Czech and German demonstrate overfitting with higher correlation on tune data but lower on test data.", "labels": [], "entities": [{"text": "Meteor 1.3", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9505521357059479}]}, {"text": "This overfitting effect is likely due to the limited number of systems providing translations into these languages and the difficulty of these target languages leading to significantly noisier translations skewing the space of metric scores.", "labels": [], "entities": []}, {"text": "We believe that tuning to combined 2009 and 2010 data will counter these issues for the official Ranking version. and GALE P2 and P3 H-TER data).", "labels": [], "entities": [{"text": "GALE P2 and P3 H-TER data", "start_pos": 118, "end_pos": 143, "type": "DATASET", "confidence": 0.5894993245601654}]}, {"text": "For each type of judgment, metric versions are tuned and tested on each year and scores are compared.", "labels": [], "entities": []}, {"text": "We compare Meteor 1.3 results with those from version 1.2 with results shown in.", "labels": [], "entities": []}, {"text": "For both adequacy data sets, Meteor 1.3 significantly outperforms version 1.2 on both tune and test data.", "labels": [], "entities": []}, {"text": "The version tuned on MT09 data is selected as the official Adequacy version of Meteor 1.3.", "labels": [], "entities": [{"text": "MT09 data", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.9674034714698792}, {"text": "Adequacy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9349619746208191}, {"text": "Meteor 1.3", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9343060255050659}]}, {"text": "H-TER versions either show no improvement or degradation due to overfitting.", "labels": [], "entities": []}, {"text": "Examination of the optimal H-TER parameter sets reveals a mismatch between evaluation metric and human judgment type.", "labels": [], "entities": []}, {"text": "As H-TER evaluation is ultimately limited by the TER aligner, there is no distinction between content and function words, and words sharing stems are considered nonmatches.", "labels": [], "entities": [{"text": "TER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.86139976978302}]}, {"text": "As such, these features do not help Meteor improve correlation, but rather act as a source of additional possibility for overfitting.", "labels": [], "entities": []}, {"text": "The 2011 WMT Tunable Metrics task consists of using Z-MERT to tune a pre-built Urdu-English Joshua () system to anew evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set.", "labels": [], "entities": [{"text": "WMT Tunable Metrics task", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.7405608147382736}]}, {"text": "As this task does not provide a  devtest set, we select aversion of Meteor by exploring the effectiveness of using multiple versions of the metric to tune phrase-based translation systems for the same language pair.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.9026216268539429}]}, {"text": "We use the 2009 NIST Open Machine Translation Evaluation Urdu-English parallel data) plus 900M words of monolingual data from the English Gigaword corpus) to build a standard Moses system as follows.", "labels": [], "entities": [{"text": "NIST Open Machine Translation Evaluation Urdu-English parallel data", "start_pos": 16, "end_pos": 83, "type": "DATASET", "confidence": 0.7302073538303375}]}, {"text": "Parallel data is word aligned using the MGIZA++ toolkit ( and alignments are symmetrized using the \"growdiag-final-and\" heuristic.", "labels": [], "entities": [{"text": "MGIZA++ toolkit", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8917511105537415}]}, {"text": "Phrases are extracted using standard phrase-based heuristics ( and used to build a translation table and lexicalized reordering model.", "labels": [], "entities": []}, {"text": "A standard SRI 5-gram language model) is estimated from monolingual data.", "labels": [], "entities": []}, {"text": "Using Z-MERT, we tune this system to baseline metrics as well as the versions of Meteor discussed in previous sections.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.9595869183540344}]}, {"text": "We also tune to a balanced Tuning version of Meteor designed to minimize bias.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.9738733172416687}]}, {"text": "This data set provides a single set of reference translations for MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 66, "end_pos": 70, "type": "TASK", "confidence": 0.68475341796875}]}, {"text": "To account for the variance of MERT, we run end-to-end tuning 3 times for each metric and report the average results on two unseen test sets: newswire and weblog.", "labels": [], "entities": [{"text": "MERT", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.6459774971008301}]}, {"text": "Test set translations are evaluated using BLEU, TER, and Meteor 1.2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9970898032188416}, {"text": "TER", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9934467673301697}, {"text": "Meteor 1.2", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.8950303792953491}]}, {"text": "The parameters for each Meteor version are listed in while the results are listed in.", "labels": [], "entities": []}, {"text": "The results are fairly consistent across both test sets: the Tuning version of Meteor outperforms BLEU across all metrics while versions of Meteor that perform well on other tasks perform poorly in tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9985073208808899}]}, {"text": "This illustrates the differences between evaluation and tuning tasks.", "labels": [], "entities": []}, {"text": "In evaluation tasks, metrics are engineered to score 1-best translations from systems most often tuned to BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9815271496772766}]}], "tableCaptions": [{"text": " Table 3: Human ranking judgment data from 2009 and  2010 WMT evaluations", "labels": [], "entities": [{"text": "WMT evaluations", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.8574299514293671}]}, {"text": " Table 5: Meteor 1.2 and 1.3 correlation with ranking  judgments on tune and test data", "labels": [], "entities": []}, {"text": " Table 6: Meteor 1.2 and 1.3 correlation with adequacy  and H-TER scores on tune and test data", "labels": [], "entities": [{"text": "H-TER", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.869667649269104}]}, {"text": " Table 4: Optimal Meteor parameters for WMT target languages on 2009 and 2010 data (Meteor 1.3 Ranking)", "labels": [], "entities": [{"text": "WMT target languages", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7219197948773702}]}, {"text": " Table 8: Average metric scores for Urdu-English systems  tuned to baseline metrics and versions of Meteor", "labels": [], "entities": [{"text": "Meteor", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.9152224659919739}]}, {"text": " Table 7: Parameters for Meteor 1.3 tasks", "labels": [], "entities": [{"text": "Parameters", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9383614659309387}]}]}