{"title": [{"text": "Incorporating Source-Language Paraphrases into Phrase-Based SMT with Confusion Networks", "labels": [], "entities": [{"text": "Phrase-Based SMT", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.5648861825466156}]}], "abstractContent": [{"text": "To increase the model coverage, source-language paraphrases have been utilized to boost SMT system performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9941970109939575}]}, {"text": "Previous work showed that word lattices constructed from paraphrases are able to reduce out-of-vocabulary words and to express inputs in different ways for better translation quality.", "labels": [], "entities": []}, {"text": "However, such a word-lattice-based method suffers from two problems: 1) path duplications in word lattices decrease the capacities for potential paraphrases; 2) lattice decoding in SMT dramatically increases the search space and results in poor time efficiency.", "labels": [], "entities": [{"text": "SMT", "start_pos": 181, "end_pos": 184, "type": "TASK", "confidence": 0.9550734758377075}]}, {"text": "Therefore, in this paper, we adopt word confusion networks as the input structure to carry source-language paraphrase information.", "labels": [], "entities": []}, {"text": "Similar to previous work, we use word lattices to build word confusion networks for merging of duplicated paths and faster decoding.", "labels": [], "entities": []}, {"text": "Experiments are carried out on small-, medium-and large-scale English-Chinese translation tasks, and we show that compared with the word-lattice-based method, the decoding time on three tasks is reduced significantly (up to 79%) while comparable translation quality is obtained on the large-scale task.", "labels": [], "entities": [{"text": "English-Chinese translation tasks", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.7248044013977051}]}], "introductionContent": [{"text": "With the rapid development of large-scale parallel corpus, research on data-driven SMT has made good progress to the real world applications.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.8980510830879211}]}, {"text": "Currently, fora typical automatic translation task, the SMT system searches and exactly matches the input sentences with the phrases or rules in the models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9935286641120911}]}, {"text": "Obviously, if the following two conditions could be satisfied, namely: \u2022 the words in the parallel corpus are highly aligned so that the phrase alignment can be performed well; \u2022 the coverage of the input sentence by the parallel corpus is high; then the \"exact phrase match\" translation method could bring a good translation.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.7218213230371475}]}, {"text": "However, for some language pairs, it is not easy to obtain a huge amount of parallel data, so it is not that easy to satisfy these two conditions.", "labels": [], "entities": []}, {"text": "To alleviate this problem, paraphrase-enriched SMT systems have been proposed to show the effectiveness of incorporating paraphrase information.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.7788241505622864}]}, {"text": "In terms of the position at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories: \u2022 Translation model augmentation with paraphrases.", "labels": [], "entities": [{"text": "MT-pipeline", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.8017139434814453}, {"text": "Translation model augmentation", "start_pos": 149, "end_pos": 179, "type": "TASK", "confidence": 0.8906193772951762}]}, {"text": "Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases.", "labels": [], "entities": [{"text": "translation of unknown source words or phrases in the input sentences", "start_pos": 25, "end_pos": 94, "type": "TASK", "confidence": 0.8440807570110668}]}, {"text": "\u2022 Training corpus augmentation with paraphrases (.", "labels": [], "entities": []}, {"text": "Paraphrases are incorporated into the MT systems by expanding the training data.", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9695591330528259}]}, {"text": "\u2022 Word-lattice-based method with paraphrases ().", "labels": [], "entities": []}, {"text": "Instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.9817736744880676}]}, {"text": "Another directly related work is to use word lattices to deal with multi-source translation (, in which paraphrases are actually generated from the alignments of difference source sentences.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.7227106541395187}]}, {"text": "Comparing these three methods, the word-latticebased method has the least overheads because: \u2022 The translation model augmentation method has to re-run the whole MT pipeline once the inputs are changed, while the word-latticebased method only need to transform the new input sentences into word lattices.", "labels": [], "entities": []}, {"text": "\u2022 The training corpus augmentation method requires corpus-scale expansion, which drastically increases the computational complexity on large corpora, while the word-lattice-based method only deals with the development set and test set.", "labels": [], "entities": []}, {"text": "In (, it is also observed that the word-lattice-based method performed better than the translation model augmentation method on different scales and two different language pairs in several translation tasks.", "labels": [], "entities": []}, {"text": "Thus they concluded that the word-lattice-based method is preferable for this task.", "labels": [], "entities": []}, {"text": "However, there are still some drawbacks for the word-lattice-based method: \u2022 In the lattice construction processing, duplicated paths are created and fed into SMT decoders.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 159, "end_pos": 171, "type": "TASK", "confidence": 0.9094978868961334}]}, {"text": "This decreases the paraphrase capacity in the word lattices.", "labels": [], "entities": []}, {"text": "Note that we use the phrase \"paraphrase capacity\" to represent the amount of paraphrases that are actually built into the word lattices.", "labels": [], "entities": []}, {"text": "As presented in (, only a limited number of paraphrases are allowed to be used while others are pruned during the construction process, so duplicate paths actually decrease the number of paraphrases that contribute to the translation quality.", "labels": [], "entities": []}, {"text": "\u2022 The lattice decoding in SMT decoder have a very high computational complexity which makes the system less feasible in real time application.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9174482822418213}]}, {"text": "Therefore, in this paper, we use confusion networks (CNs) instead of word lattices to carry paraphrase information in the inputs for SMT decoders.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 133, "end_pos": 145, "type": "TASK", "confidence": 0.9390210807323456}]}, {"text": "CNs are constructed from the aforementioned word lattices, while duplicate paths are merged to increase paraphrase capacity (e.g. by admitting more nonduplicate paraphrases without increasing the input size).", "labels": [], "entities": []}, {"text": "Furthermore, much less computational complexity is required to perform CN decoding instead of lattice decoding in the SMT decoder.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.7667209804058075}]}, {"text": "We carried out experiments on small-, medium-and largescale English-Chinese translation tasks to compare against a baseline PBSMT system, the translation model augmentation of) method and the word-lattice-based method of ( to show the effectiveness of our novel approach.", "labels": [], "entities": []}, {"text": "The motivation of this work is to use CN as the compromise between speed and quality, which comes from previous studies in speech recognition and speech translation: in), word lattices are transformed into CNs to obtain compact representations of multiple aligned ASR hypotheses in speech understanding; in (, CNs are also adopted instead of word lattices as the source-side inputs for speech translation systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7349619418382645}, {"text": "speech translation", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.7149359285831451}, {"text": "speech understanding", "start_pos": 282, "end_pos": 302, "type": "TASK", "confidence": 0.6997726857662201}, {"text": "speech translation", "start_pos": 386, "end_pos": 404, "type": "TASK", "confidence": 0.7339389026165009}]}, {"text": "The main contribution of this paper is to show that this compromise also works for SMT systems incorporating sourcelanguage paraphrases in the inputs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9921640157699585}]}, {"text": "Regarding the use of paraphrases SMT system, there are still other two categories of work that are related to this paper: \u2022 Using paraphrases to improve system optimization (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.850021481513977}, {"text": "system optimization", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7305424213409424}]}, {"text": "With an EnglishEnglish MT system, this work utilises paraphrases to reduce the number of manually translated references that are needed in the parameter tuning process of SMT, while preserved a similar translation quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 171, "end_pos": 174, "type": "TASK", "confidence": 0.9672272205352783}]}, {"text": "\u2022 Using paraphrases to smooth translation models (.", "labels": [], "entities": []}, {"text": "Either cluster-based or example-based methods areproposed to obtain better estimation on phrase translation probabilities with paraphrases.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 89, "end_pos": 121, "type": "TASK", "confidence": 0.8132904171943665}]}, {"text": "The rest of this paper is organized as follows: In section 2, we present an overview of the wordlattice-based method and its drawbacks.", "labels": [], "entities": []}, {"text": "Section 3 proposes the CN-based method, including the building process and its application on paraphrases in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9612478613853455}]}, {"text": "Section 4 presents the experiments and results of the proposed method as well as discussions.", "labels": [], "entities": []}, {"text": "Conclusions and future work are then given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were carried out on three EnglishChinese translation tasks.", "labels": [], "entities": [{"text": "EnglishChinese translation tasks", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.8297591408093771}]}, {"text": "The training corpora comprise 20K, 200K and 2.1 million sentence pairs, where the former two corpora are derived from FBIS corpus 1 which is sentence-aligned by Champollion aligner), the latter corpus comes from HK parallel corpus, 2 ISI parallel corpus, 3 other news data and parallel dictionaries from LDC.", "labels": [], "entities": [{"text": "FBIS corpus 1", "start_pos": 118, "end_pos": 131, "type": "DATASET", "confidence": 0.8941888809204102}, {"text": "HK parallel corpus", "start_pos": 212, "end_pos": 230, "type": "DATASET", "confidence": 0.7858145038286845}, {"text": "LDC", "start_pos": 304, "end_pos": 307, "type": "DATASET", "confidence": 0.9430974721908569}]}, {"text": "The development set and the test set for the 20K and 200K corpora are randomly selected from the FBIS corpus, each of which contains 1,200 sentences, with one reference.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.9287898242473602}]}, {"text": "For the 2.1 million corpus, the NIST 2005 Chinese-English current set (1,082 sentences) with one reference is used as the development set, and NIST 2003 English-Chinese current set (1,859 sentences) with four references is used as the test set.", "labels": [], "entities": [{"text": "NIST 2005 Chinese-English current set", "start_pos": 32, "end_pos": 69, "type": "DATASET", "confidence": 0.9691344380378724}, {"text": "NIST 2003 English-Chinese current set", "start_pos": 143, "end_pos": 180, "type": "DATASET", "confidence": 0.9392316699028015}]}, {"text": "Three baseline systems are built for comparison: Moses PBSMT baseline system (, a realization of the translation model augmentation system described in) (named \"Para-Sub\" hereafter), and the word-lattice based system proposed in (.", "labels": [], "entities": [{"text": "Moses PBSMT baseline", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.8063532710075378}]}, {"text": "Word alignments on the parallel corpus are performed using GIZA++ with the \"grow-diag-final\" refinement.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6567430198192596}]}, {"text": "Maximum phrase length is set to 10 words and the parameters in the log-linear model are tuned by MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.813008725643158}]}, {"text": "All the language models are 5-gram built with the SRILM toolkit) on the monolingual part of the parallel corpora.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8151569664478302}]}, {"text": "The results are reported in BLEU () and TER () scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9992268085479736}, {"text": "TER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.995735764503479}]}, {"text": "compares the performance of four systems on three translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.9096913039684296}]}, {"text": "As can be observed from the Table, for 20K and 200K corpora, the word-lattice-based system accomplished the best results.", "labels": [], "entities": []}, {"text": "For the 20K corpus, the CN outperformed the baseline PBSMT by 0.31 absolute (2.15% relative) BLEU points and 1.5 absolute (1.99% relative) TER points.", "labels": [], "entities": [{"text": "20K corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8415296077728271}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9875839352607727}, {"text": "TER", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9942147135734558}]}, {"text": "For the 200K corpus, it still outperformed the \"Para-Sub\" by 0.06 absolute (0.26% relative) BLEU points and 0.15 absolute (0.23% relative) TER points.", "labels": [], "entities": [{"text": "200K corpus", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.9541244804859161}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9859517812728882}, {"text": "TER", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9949482679367065}]}, {"text": "Note that for the 2.1M corpus, although CN underperformed the best word lattice by an insignificant amount (0.06 absolute, 0.41% relative) in terms of BLEU points, it has the best performance in terms of TER points (0.22 absolute, 0.3% relative than word lattice).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9990110397338867}, {"text": "TER", "start_pos": 204, "end_pos": 207, "type": "METRIC", "confidence": 0.9976522326469421}]}, {"text": "Furthermore, the CN outperformed \"Para-Sub\" by 0.36 absolute (2.55% relative) BLEU points and 1.37 absolute (1.84% relative) TER points, and also beat the baseline PBSMT system by 0.45 absolute (3.21% relative) BLEU points and 1.82 absolute (2.43% relative) TER points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9974168539047241}, {"text": "TER", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9883577227592468}, {"text": "BLEU", "start_pos": 211, "end_pos": 215, "type": "METRIC", "confidence": 0.9974406957626343}, {"text": "TER", "start_pos": 258, "end_pos": 261, "type": "METRIC", "confidence": 0.9917733073234558}]}, {"text": "The paired 95% confidence interval of significant test between the \"Lattice\" and \"CN\" system is, which also suggests that the two system has a comparable performance in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9985289573669434}]}, {"text": "In, decoding time on test sets is reported to compare the computational efficiency of the baseline PBSMT, word-lattice-based and CNbased methods.", "labels": [], "entities": []}, {"text": "Note that word lattice construction time and CN building time (including word lattice construction and conversion from word lattices into CNs with the SRILM toolkit) are counted in the decoding time and illustrated in the table within parentheses respectively.", "labels": [], "entities": [{"text": "word lattice construction", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.6117318371931711}, {"text": "SRILM toolkit", "start_pos": 151, "end_pos": 164, "type": "DATASET", "confidence": 0.8679264187812805}]}, {"text": "Although both word-lattice-based and CN-based methods require longer decoding times than the baseline PB-SMT system, it is observed that compared with the word lattices, CNs reduced the decoding time significantly on three tasks, namely 52.06% for the 20K model, 75.75% for the 200K model and 78.88% for the 2.1M model.", "labels": [], "entities": []}, {"text": "It is also worth noting that the \"Para-Sub\" system has a similar decoding time with baseline PBSMT since only the translation: Decoding time comparison of PBSMT, word-lattice (\"Lattice\") and CN-based (\"CN\") methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison on PBSMT, \"Para-Sub\", word-lattice and CN-based methods.", "labels": [], "entities": []}]}