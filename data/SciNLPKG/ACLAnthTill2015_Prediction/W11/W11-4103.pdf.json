{"title": [{"text": "Query classification via Topic Models for an art image archive", "labels": [], "entities": [{"text": "Query classification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7907147705554962}]}], "abstractContent": [{"text": "In recent years, there has been an increasing amount of literature on query classification.", "labels": [], "entities": [{"text": "query classification", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8719472885131836}]}, {"text": "Click-through information has been shown to be a useful source for improving this task.", "labels": [], "entities": []}, {"text": "However, far too little attention has been paid to queries in very specific domains such as art, culture and history.", "labels": [], "entities": []}, {"text": "We propose an approach that exploits topic models built from a domain specific corpus as a mean to enrich both the query and the categories against which the query need to be classified.", "labels": [], "entities": []}, {"text": "We take an Art Library as the case study and show that topic model enrichment improves over the enrichment via click-through considerably .", "labels": [], "entities": []}], "introductionContent": [{"text": "\u2022 w: observed word \u2022 \u03b8: distribution of topic in documents \u2022 \u03c6: distribution of words generated from topic z Conversely, given a set of documents, we can discover a set of topics that are responsible for generating a document, and the distribution of words that belong to a topic.", "labels": [], "entities": []}, {"text": "Estimating these parameters for LDA is intractable.", "labels": [], "entities": [{"text": "LDA", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.7929770946502686}]}, {"text": "Different solutions for approximating estimation such as Variational Methods ( and Gibbs Sampling () can be used.", "labels": [], "entities": [{"text": "approximating estimation", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7961714565753937}]}, {"text": "Gibbs Sampling is an example of a Markov chain Monte Carlo with relatively simple algorithm for approximate inference in high dimensional models, with the first use for LDA reported in ().", "labels": [], "entities": [{"text": "Gibbs Sampling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.47674283385276794}, {"text": "LDA", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.6865767240524292}]}, {"text": "In our experiment, we have estimated the multinomial observations by unsupervised learning with GibbsLDA++ toolkit.", "labels": [], "entities": [{"text": "GibbsLDA++ toolkit", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.8801268537839254}]}, {"text": "Following the data enrichment approach in (, we have enriched the query and category with hidden topic.", "labels": [], "entities": []}, {"text": "In particular, given a probability \u03d1 m,k of document mover topic k, the corresponding weight w topic k ,m was determined by discretizing \u03d1 m,k using two parameters cut-off and scale: (1) We chose cut-off = 0.01, scale = 20 as to ensure that the number of topics assigned to a query/category does not exceed the number of original terms of that query/category, i.e., to keep a balance weight between topics enriched and original terms.", "labels": [], "entities": []}, {"text": "To discover the set of topics and the distribution of words per topic, we need to choose a universal data set.", "labels": [], "entities": []}, {"text": "Since we are interested in topics within a rather specific domain, we need to choose a data set that provides an appropriate vocabulary.", "labels": [], "entities": []}, {"text": "We have tried two options, a Topic Model built out of selected pages of Wikipedia and a Topic Model built out of BAL Catalogue.", "labels": [], "entities": [{"text": "BAL Catalogue", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9257953763008118}]}, {"text": "Wikipedia Topic Model Wikipedia is a rich source of data that has been widely exploited to extract knowledge in many different domains.", "labels": [], "entities": [{"text": "Wikipedia Topic Model Wikipedia", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.9040377736091614}]}, {"text": "We have used aversion of it, viz.", "labels": [], "entities": []}, {"text": "WaCKypedia (), 3 that contains around 3 million articles from Wikipedia segmented, normalized, POS-tagged and parsed.", "labels": [], "entities": [{"text": "WaCKypedia", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9442567825317383}]}, {"text": "In order to extract those pages that could provide a better model for our specific domain, we selected those pages that contain at least one content word of the BAL browse categories listed below.", "labels": [], "entities": []}, {"text": "For our vocabulary, we considered only words in the selected WaCKypedia pages that are either Nouns (N.*) or Verbs (VV.*) or Adjectives (J.*) after being lemmatized.", "labels": [], "entities": [{"text": "WaCKypedia pages", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9444470703601837}]}, {"text": "We obtain \u2248 14K documents, with a vocabulary of \u2248 200K words, out of which we computed 100 topics.", "labels": [], "entities": []}, {"text": "Examples of random topics are illustrated in.", "labels": [], "entities": []}, {"text": "We group together images that share the same sub-categories and consider each group of subcategory as a document.", "labels": [], "entities": []}, {"text": "We have 732 documents and \u2248 136K words, out of which we computed 100 topics.", "labels": [], "entities": []}, {"text": "Examples of topics estimated from this dataset are given in.", "labels": [], "entities": []}], "datasetContent": [{"text": "Let Q = {q 1 , q 2 , . .", "labels": [], "entities": []}, {"text": ", q N } be a set of N queries and C = {c 1 , c 2 , . .", "labels": [], "entities": []}, {"text": ", c M } a set of M categories.", "labels": [], "entities": []}, {"text": "We represent each query q i and category c j as the vectors \u2212 \u2192 q i = {w tq i } t\u2208V and \u2212 \u2192 c j = {w tc j } t\u2208V where V is the vocabulary that contains all terms in the corpus and w tq i , w tc i are the frequency in q i and c j , respectively, of each term tin the vocabulary.", "labels": [], "entities": []}, {"text": "We use the cosine similarity measure to assign categories to the queries.", "labels": [], "entities": [{"text": "cosine similarity measure", "start_pos": 11, "end_pos": 36, "type": "METRIC", "confidence": 0.7224670648574829}]}, {"text": "For each query q i , the cosine similarity between every pair q i , c j j=1..M is computed as: For each query, the top 3 categories with highest cosine similarities are returned.", "labels": [], "entities": [{"text": "M", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9484947323799133}]}, {"text": "The different query and category enrichment methods are spelled out in.", "labels": [], "entities": [{"text": "category enrichment", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.6699666678905487}]}, {"text": "To evaluate the effect of click-through information in query classification, we setup two different configurations: QR, where besides the terms contained in the top and sub-categories, V consists of terms appearing in the queries; QR-CT for which V consists also of terms in the title, keywords, description fields of the clicked images' meta-data.", "labels": [], "entities": [{"text": "query classification", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.8302375078201294}]}, {"text": "In the case of the classifiers exploiting topic models, both vocabulary is extended with the hidden topics too and both queries and categories are enriched with them as explained in section 3.", "labels": [], "entities": []}, {"text": "In particular, TM wiki is the classifier based on the model built with the hidden topics extracted from WaCKpe-dia, and TM BAL is the one based on the model built out of Bridgeman metadata.", "labels": [], "entities": [{"text": "TM BAL", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.45128555595874786}]}], "tableCaptions": [{"text": " Table 4: P, R and F measures -Evaluation", "labels": [], "entities": [{"text": "F", "start_pos": 19, "end_pos": 20, "type": "METRIC", "confidence": 0.9799124598503113}, {"text": "Evaluation", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.5817211270332336}]}, {"text": " Table 5: Results of query classification: number  of correct categories found (for 1,049 queries)", "labels": [], "entities": [{"text": "query classification", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8308299481868744}]}, {"text": " Table 6: Correct categories checked by the expert  for the 270 queries (using the click-through infor- mation)", "labels": [], "entities": [{"text": "Correct", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9587011337280273}]}]}