{"title": [{"text": "Multi-Word Unit Dependency Forest-based Translation Rule Extraction", "labels": [], "entities": [{"text": "Multi-Word Unit Dependency Forest-based Translation Rule Extraction", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.5679033739226205}]}], "abstractContent": [{"text": "Translation requires non-isomorphic transformation from the source to the target.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9640258550643921}]}, {"text": "However, non-isomorphism can be reduced by learning multi-word units (MWUs).", "labels": [], "entities": []}, {"text": "We present a novel way of representating sentence structure based on MWUs, which are not necessarily continuous word sequences.", "labels": [], "entities": [{"text": "representating sentence structure", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.8556631008783976}]}, {"text": "Our proposed method builds a simpler structure of MWUs than words using words as vertices of a dependency structure.", "labels": [], "entities": []}, {"text": "Unlike previous studies, we collect many alternative structures in a packed forest.", "labels": [], "entities": []}, {"text": "As an application of our proposed method, we extract translation rules inform of a source MWU-forest to the target string, and verify the rule coverage empirically.", "labels": [], "entities": []}, {"text": "As a consequence, we improve the rule coverage compare to a previous work, while retaining the linear asymptotic complexity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax is the hierarchical structure of a natural language sentence.", "labels": [], "entities": []}, {"text": "It is generally represented with tree structures using phrase structure grammar (PSG) or dependency grammar.", "labels": [], "entities": [{"text": "phrase structure grammar (PSG)", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.8131315608819326}]}, {"text": "Although the state-of-the-art statistical machine translation (SMT) paradigm is phrasebased SMT (PBSMT), many researchers have attempted to utilize syntax in SMT to overcome the weaknesses of PBSMT.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.7938537696997324}, {"text": "phrasebased SMT", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.5962479412555695}]}, {"text": "An emerging paradigm alternative to PBSMT is syntax-based SMT, which embeds the source and/or target syntax in its translation model (TM).", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8044450283050537}]}, {"text": "Utilizing syntax in TM has two advantages over PBSMT.", "labels": [], "entities": []}, {"text": "The first advantage is that syntax eases global reordering between the source and the target language.", "labels": [], "entities": []}, {"text": "shows that we need global reordering in a complex real situation, where a verbal phrase requires along distance movement.", "labels": [], "entities": [{"text": "global reordering", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.6897144019603729}]}, {"text": "PBSMT often fails to handle global reordering, for example, from subject-verb-object (SVO) to SOV transformation where V should be moved faraway from the original position in the source language.", "labels": [], "entities": [{"text": "SOV transformation", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7635624408721924}]}, {"text": "This is because of the two distance-based constraints in PBSMT: the distortion model cost and the distortion size limit.", "labels": [], "entities": [{"text": "distortion size limit", "start_pos": 98, "end_pos": 119, "type": "METRIC", "confidence": 0.8975677092870077}]}, {"text": "For the distortion model cost, PBSMT sets zero cost to the monotone translation and penalizes the distorted translations as the distortion grows larger.", "labels": [], "entities": []}, {"text": "For the distortion size limit, a phrase can only be moved from its original position within a limit.", "labels": [], "entities": []}, {"text": "Therefore, PBSMT fails to handle long distance reordering.", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.7455003261566162}]}, {"text": "Syntax-based SMT manages global reordering as structural transformation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9067071676254272}]}, {"text": "Because reordering occurs at the substructure level such as constituents or treelets in syntax-based SMT, the transformation of the sub-structure eventually yields the reordering of the whole sentence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.8393877744674683}]}, {"text": "The second advantage of using syntax in TM is that syntax guides us to discontinuous translation patterns.", "labels": [], "entities": [{"text": "TM", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9412909746170044}]}, {"text": "Because PBSMT regards only a continuous sequence of words as a translation pattern, it often fails to utilize many useful discontinuous translation patterns.", "labels": [], "entities": []}, {"text": "For example, two discontinuous source words correspond to a target word in.", "labels": [], "entities": []}, {"text": "In our inspection of the training corpus, a continuous word sequence often corresponds to a set of discontinuous words in the target language, or vice versa.", "labels": [], "entities": []}, {"text": "Discontinuous translation patterns frequently appear in many languages.", "labels": [], "entities": [{"text": "Discontinuous translation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5812133252620697}]}, {"text": "Syntax-based SMT overcomes the limitations of PBSMT because it finds discontinuous patterns along with the hier-: The maximum branching factor (BF) and depth factor (DF) in a dependency tree in our corpus archical structure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9003878831863403}, {"text": "maximum branching factor (BF)", "start_pos": 118, "end_pos": 147, "type": "METRIC", "confidence": 0.7158043732245764}, {"text": "depth factor (DF)", "start_pos": 152, "end_pos": 169, "type": "METRIC", "confidence": 0.933690881729126}]}, {"text": "For example, the two discontinuous source words have a head-dependent relation (.", "labels": [], "entities": []}, {"text": "Especially with the dependency tree, we can easily identify patterns that have non-projectivity (.", "labels": [], "entities": []}, {"text": "However, syntax-based patterns such as constituents or treelets do not sufficiently cover various useful patterns, even if we have the correct syntactic analysis.", "labels": [], "entities": []}, {"text": "For this reason, many researchers have proposed supplementary patterns such as an intra/inter constituent or sequence of treelets (.", "labels": [], "entities": []}, {"text": "Unlike PSG, DG does not include nonterminal symbols, which represent constituent information.", "labels": [], "entities": []}, {"text": "This makes DG simpler than PSG.", "labels": [], "entities": []}, {"text": "For instance, it directly associates syntatic role with the structure, but introduces a difficulty in syntax-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9095523953437805}]}, {"text": "The branching factor of a dependency tree becomes larger when ahead word dominates many dependents.", "labels": [], "entities": []}, {"text": "We observe that the maximum branching factor of an automatically parsed dependency tree ranges widely, while most trees have depth under a certain degree).", "labels": [], "entities": []}, {"text": "This indicates that we have a horizontally flat dependency tree structure.", "labels": [], "entities": []}, {"text": "The translation patterns extracted from the flat dependency tree are also likely to be flat.", "labels": [], "entities": []}, {"text": "Unfortunately, the flat patterns are less applicable at the decoding stage.", "labels": [], "entities": []}, {"text": "When one of the modifiers does not match, for instance, we fail to apply the translation pattern.", "labels": [], "entities": []}, {"text": "Therefore, we need a more generally applicable representation for syntax-based SMT using DG.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.8958855271339417}]}, {"text": "We propose a novel representation of DG that regards a set of words as a unit of the dependency relations, similar to.", "labels": [], "entities": []}, {"text": "Unlike their work, we consider many alternatives without predefined units, and construct a packed forest of the multi-word units (MWUs) from a dependency tree.", "labels": [], "entities": []}, {"text": "For brevity, we denote the forest based on MWUs as an MWU-forest.", "labels": [], "entities": []}, {"text": "Because all possible alternatives are exponentially many, we give an efficient algorithm that enumerates the k-best alternatives in section 3.", "labels": [], "entities": []}, {"text": "As an application, we extract translation patterns inform of a source MWU-forest to the target string in order to broaden the coverage of the extracted patterns for syntax-based SMT in section 4.", "labels": [], "entities": [{"text": "SMT", "start_pos": 178, "end_pos": 181, "type": "TASK", "confidence": 0.9067529439926147}]}, {"text": "We also report empirical results related to the usefulness of the extracted pattern in section 5.", "labels": [], "entities": []}, {"text": "The experimental results show that the MWU-forest representation gives more applicable translation patterns than the original word-based tree.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the training corpus provided for the DIALOG Task in IWSLT10 between Chinese and English . The corpus is a collection of 30,033 sentence pairs and consists of dialogs in travel situations (10,061) and parts of the BTEC corpus.", "labels": [], "entities": [{"text": "IWSLT10", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.7449594736099243}, {"text": "BTEC corpus", "start_pos": 221, "end_pos": 232, "type": "DATASET", "confidence": 0.9732921123504639}]}, {"text": "Details about the provided corpus are described in.", "labels": [], "entities": []}, {"text": "We used the Stanford Parser 1 to obtain word-level dependency structures of Chinese sentences, and GIZA++ 2 to obtain word alignments of the biligual corpus.", "labels": [], "entities": []}, {"text": "We extracted the SCFG-MWU from the biligual corpus with word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.6960284411907196}]}, {"text": "In order to investigate the coverage of the extracted rule, we counted the number of the recovered sentences, i.e. counted if the extracted rule for each sentence pair generates the target sentence by combining the extracted rules.", "labels": [], "entities": []}, {"text": "As we collected many alternatives in an MWU-forest, we wanted to determine the importance of each source fragment.", "labels": [], "entities": [{"text": "MWU-forest", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9241819381713867}]}, {"text": "penalized a rule \u03b3 by the posterior probability of its tree fragment lhs(\u03b3).", "labels": [], "entities": []}, {"text": "This posterior probability is also computed in the Inside-Outside fashion that we used in Algorithm 1.", "labels": [], "entities": []}, {"text": "Therefore, we regarded the fractional count of a rule \u03b3 as We prioritized the rule according to the fractional count.", "labels": [], "entities": []}, {"text": "The priority is used when we combine the rules to restore the target sentence using the extracted rule for each sentence.", "labels": [], "entities": []}, {"text": "We varied the maximum size of a vertex m, and the number of incoming hyperedges k. shows the emprical result.", "labels": [], "entities": [{"text": "emprical", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9549273252487183}]}, {"text": "shows that we need MWU to broaden the coverage of the extracted translation rules.", "labels": [], "entities": [{"text": "MWU", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9082272052764893}]}, {"text": "The rule coverage increases as the number of words in an MWU increases, and almost converges at m = 6.", "labels": [], "entities": [{"text": "coverage", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8908085823059082}]}, {"text": "Our proposed method recover around 75% of the sentences in the corpus when we properly restrict m and k.", "labels": [], "entities": []}, {"text": "This is a great improvement over, who reported around 60% of the rule coverage without the limitaion of the size of MWUs.", "labels": [], "entities": [{"text": "coverage", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.5258068442344666}]}, {"text": "They only considered the best decomposition of the dependency tree, while our proposed method collects many alternative MWUs into an MWUforest.", "labels": [], "entities": []}, {"text": "When we considered the best decomposition (k = 1), the rule coverage dropped to around 65%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.532139241695404}]}, {"text": "This can be viewed as an indirect comparison between and our proposed method in this corpus.", "labels": [], "entities": []}, {"text": "shows that the frequency of success and failure in the recovery depends on the length of the sentences.", "labels": [], "entities": []}, {"text": "As the length of sentences increase, the successful recovery occurs less frequently.", "labels": [], "entities": []}, {"text": "We investigated the reason of failure in the longer sentences.", "labels": [], "entities": []}, {"text": "As a result, the two main sources of the failure are the word alignment error and the dependency parsing error.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.6300481855869293}, {"text": "dependency parsing", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.6545974165201187}]}], "tableCaptions": [{"text": " Table 1: Statistics of the corresponding target words  for the continuous word sequences in the source lan- guage, or vice versa. C denotes consistent, O over- lapped, D discontinuous, and N null.", "labels": [], "entities": []}]}