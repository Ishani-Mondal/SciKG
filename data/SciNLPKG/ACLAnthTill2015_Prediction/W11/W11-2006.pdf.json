{"title": [{"text": "Toward Learning and Evaluation of Dialogue Policies with Text Examples", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a dialogue collection and enrichment framework that is designed to explore the learning and evaluation of dialogue policies for simple conversational characters using textual training data.", "labels": [], "entities": [{"text": "dialogue collection", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.777839332818985}]}, {"text": "To facilitate learning and evaluation, our framework enriches a collection of role-play dialogues with additional training data, including paraphrases of user utterances , and multiple independent judgments by external referees about the best policy response for the character at each point.", "labels": [], "entities": []}, {"text": "As a case study, we use this framework to train a policy fora limited domain tactical questioning character, reaching promising performance.", "labels": [], "entities": []}, {"text": "We also introduce an automatic policy evaluation metric that recognizes the validity of multiple conversational responses at each point in a dialogue.", "labels": [], "entities": []}, {"text": "We use this metric to explore the variability inhuman opinion about optimal policy decisions, and to automatically evaluate several learned policies in our example domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is a large class of potential users of dialogue systems technology who lack the background for many of the formal modeling tasks that typically are required in the construction of a dialogue system.", "labels": [], "entities": []}, {"text": "The problematic steps include annotating the meaning of user utterances in some semantic formalism, developing a formal representation of information state, writing detailed rules that govern dialogue management, and annotating the meaning of system utterances in support of language generation, among other tasks.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.7314861118793488}, {"text": "language generation", "start_pos": 275, "end_pos": 294, "type": "TASK", "confidence": 0.7481338679790497}]}, {"text": "In this paper, we explore data collection and machine learning techniques that enable the implementation of domain-specific conversational dialogue policies through a relatively small data collection effort, and without any formal modeling.", "labels": [], "entities": []}, {"text": "We present a case study, which serves to illustrate some of the possibilities in our framework.", "labels": [], "entities": []}, {"text": "In contrast to recent work on data-driven dialogue policy learning that learns dialogue behavior from existing data sources (), we address the task of authoring a dialogue policy from scratch with a specific purpose, task and scenario in mind.", "labels": [], "entities": [{"text": "dialogue policy learning", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7113288640975952}]}, {"text": "We examine the data collection, learning and evaluation steps.", "labels": [], "entities": [{"text": "data collection", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7700671851634979}]}, {"text": "The contributions of this work include a data collection and enrichment framework without formal modeling, and the creation of dialogue policies from the collected data.", "labels": [], "entities": [{"text": "data collection and enrichment", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.7607653886079788}]}, {"text": "We also propose a framework for evaluating learned policies.", "labels": [], "entities": []}, {"text": "We show, for the scenario in our case study, that these techniques deliver promising levels of performance, and point to possible future developments in data-driven dialogue policy creation and evaluation.", "labels": [], "entities": [{"text": "dialogue policy creation and evaluation", "start_pos": 165, "end_pos": 204, "type": "TASK", "confidence": 0.7457636713981628}]}], "datasetContent": [{"text": "multiple external referees  The weak agreement metric defined in the previous section can be used to measure the quality of automatically learned policies, and to provide insight into how a learned policy compares to humanlevel performance.", "labels": [], "entities": []}, {"text": "Because it recognizes the validity of multiple responses, the weak agreement metric can help distinguish true policy errors from policy choices that are consistent with the intuitions of at least some human referees about what the character should say.", "labels": [], "entities": []}, {"text": "In particular, given the choices made by five external referees for our 19 Amani dialogues, we can expect their choices to cover about 80% of the choices a sixth person would make for what Amani should say at each turn in these dialogues.", "labels": [], "entities": []}, {"text": "(I.e., we know that the weak agreement among a group of six human referees is about 80% for this Amani scenario.)", "labels": [], "entities": []}, {"text": "We proceed to rate the quality of an automatic policy by computing a one-vs-others version of weak agreement-intuitively treating our policy as if it were such a \"sixth person\", and comparing it to the other five.", "labels": [], "entities": []}, {"text": "Instead of computing the average weak agreement for referees randomly selected from an entire group, as in the previous section, to evaluate a policy, we compute its weak agreement compared to the combined set of human external referees, as follows.", "labels": [], "entities": []}, {"text": "For every system utterance u j in our set of role play dialogues, a given automatic policy P is used to select a response c j (corresponding to a dialogue act in the domain).", "labels": [], "entities": []}, {"text": "We then check for membership of c j in the set that contains only and all dialogue act choices c kj fork ranging from 1 to N , inclusive, where N is the number of external referees and c kj corresponds to the k th referee's choice for the j th utterance.", "labels": [], "entities": []}, {"text": "Another way to interpret this evaluation metric is to consider it a form of accuracy that computes the number of correct choices made by the policy divided by the total number of choices made by the policy, where a choice is considered \"correct\" if it matches any of the external referees' choices fora specific utterance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.998621940612793}]}, {"text": "For this reason, we refer to this evaluation-focused one-vs-all version of weak agreement as weak accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9851367473602295}]}, {"text": "Based on the definition above, an automatic policy with quality indistinguishable from that of a person choosing utterances for the Amani character would have a weak accuracy of about 80% or higher when measured using a set of five external referees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9989989399909973}]}, {"text": "We see then that this metric is far from perfect, since it cannot rank two policies with weak accuracy levels of, say, 80% and 90%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9979676604270935}]}, {"text": "It is also possible fora policy that results in dialogue behavior noticeably inferior to that of a human referee to be rated at the same weak accuracy value fora human referee (80%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9985302686691284}]}, {"text": "In practice, however, weak accuracy with five or six external referees has far greater power for discriminating between policies of varying quality, and ranking them correctly, than a naive version of accuracy, which corresponds to weak accuracy using a single referee.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9874182939529419}, {"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9981393814086914}, {"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9846718907356262}]}, {"text": "Furthermore, the addition of only a few more external referees would very likely increase the efficacy of the weak agreement metric.", "labels": [], "entities": []}, {"text": "Despite the shortcomings of weak accuracy as a metric for evaluation of quality of dialogue policies, it opens up a wide range of opportunities for development of learned policies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9975632429122925}]}, {"text": "Without an automated metric, development of such techniques can be only vaguely incremental, relying on either costly or, more likely, infrequent human evaluations with results that are difficult to optimize toward with current machine learning techniques.", "labels": [], "entities": []}, {"text": "The use of imperfect automated metrics in situations where ideal metrics are unavailable or are impractical to deploy is fairly common in natural language processing.", "labels": [], "entities": []}, {"text": "PARSEVAL (, commonly used for parser evaluation, and BLEU (), commonly used in machine translation, are two examples of well-known imperfect metrics that have been the subject of much criticism, but that are widely agreed to have been necessary for much of the progress enjoyed by their respective fields.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9533930420875549}, {"text": "parser evaluation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8860507905483246}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9986391663551331}, {"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7749515473842621}]}, {"text": "Unlike BLEU, however, which has been shown to correlate with certain types of human judgment on the quality of machine translation systems, our notion of weak accuracy has not yet been demonstrated to correlate with human judgments on the quality of dialogue policies, and as such it is only hypothesized to have this property.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9943727254867554}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9321607947349548}]}, {"text": "We leave this important step of validation as future work.", "labels": [], "entities": [{"text": "validation", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.9768868088722229}]}], "tableCaptions": []}