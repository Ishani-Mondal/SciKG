{"title": [{"text": "Punctuation: Making a Point in Unsupervised Dependency Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We show how punctuation can be used to improve unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.6665304899215698}]}, {"text": "Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.9914519190788269}]}, {"text": "However, approaches that naively include punctuation marks in the grammar (as if they were words) do not perform well with Klein and Manning's Dependency Model with Valence (DMV).", "labels": [], "entities": []}, {"text": "Instead , we split a sentence at punctuation and impose parsing restrictions over its fragments.", "labels": [], "entities": []}, {"text": "Our grammar inducer is trained on the Wall Street Journal (WSJ) and achieves 59.5% accuracy out-of-domain (Brown sentences with 100 or fewer words), more than 6% higher than the previous best results.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.9642050762971243}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9992604851722717}]}, {"text": "Further evaluation , using the 2006/7 CoNLL sets, reveals that punctuation aids grammar induction in 17 of 18 languages, for an overall average net gain of 1.3%.", "labels": [], "entities": [{"text": "2006/7 CoNLL sets", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.7664525985717774}, {"text": "grammar induction", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7040979564189911}]}, {"text": "Some of this improvement is from training, but more than half is from parsing with induced constraints, in inference.", "labels": [], "entities": []}, {"text": "Punctuation-aware decoding works with existing (even already-trained) parsing models and always increased accuracy in our experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9992371797561646}]}], "introductionContent": [{"text": "Unsupervised dependency parsing is a type of grammar induction -a central problem in computational linguistics.", "labels": [], "entities": [{"text": "Unsupervised dependency parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6800790031750997}, {"text": "grammar induction", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7288046330213547}]}, {"text": "It aims to uncover hidden relations between head words and their dependents in free-form text.", "labels": [], "entities": []}, {"text": "Despite decades of significant research efforts, the task still poses a challenge, as sentence structure is underdetermined by only raw, unannotated words.", "labels": [], "entities": []}, {"text": "Structure can be clearer in formatted text, which typically includes proper capitalization and punctuation (.", "labels": [], "entities": []}, {"text": "Raw word streams, such as utterances transcribed by speech recognizers, are often difficult even for humans).", "labels": [], "entities": []}, {"text": "Therefore, one would expect grammar inducers to exploit any available linguistic meta-data.", "labels": [], "entities": []}, {"text": "And yet in unsupervised dependency parsing, sentenceinternal punctuation has long been ignored.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7714669406414032}]}, {"text": "HTML is another kind of meta-data that is ordinarily stripped out in pre-processing.", "labels": [], "entities": []}, {"text": "However, recently demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors often match linguistic constituents: ..., whereas McCain is secure on the topic, Obama <a>[ VP worries about winning the pro-Israel vote]</a>.", "labels": [], "entities": [{"text": "hierarchical syntactic structure discovery", "start_pos": 70, "end_pos": 112, "type": "TASK", "confidence": 0.6813104748725891}]}, {"text": "We propose exploring punctuation's potential to aid grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7945263385772705}]}, {"text": "Consider a motivating example (all of our examples are from WSJ), in which all (six) marks align with constituent boundaries: This link between punctuation and constituent boundaries suggests that we could approximate parsing by treating inter-punctuation fragments independently.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9176837801933289}]}, {"text": "In training, our algorithm first parses each fragment separately, then parses the sequence of the resulting head words.", "labels": [], "entities": []}, {"text": "In inference, we use a better approximation that allows heads of fragments to be attached by arbitrary external words, e.g.: The: Top 99% of the lowest dominating non-terminals deriving complete inter-punctuation fragments in WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 226, "end_pos": 229, "type": "DATASET", "confidence": 0.9110414981842041}]}], "datasetContent": [{"text": "Our first experiment compares \"punctuation as constraints\" to the baseline systems.", "labels": [], "entities": []}, {"text": "We use default settings, as recommended by: loose in training; and sprawl in inference.", "labels": [], "entities": []}, {"text": "Evaluation is on Section 23 of WSJ (all sentence lengths).", "labels": [], "entities": [{"text": "Section 23 of WSJ", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.8021617978811264}]}, {"text": "To facilitate comparison with prior work, we also report accuracies against shorter sentences, with up to ten non-punctuation tokens (WSJ10 -see).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9851027727127075}, {"text": "WSJ10", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.7337284088134766}]}, {"text": "We find that both constrained regimes improve performance.", "labels": [], "entities": []}, {"text": "Constrained decoding alone increases the accuracy of a standardly-trained system from 52.0% to 54.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9996786117553711}]}, {"text": "And constrained training yields 55.6% -57.4% in combination with inference.", "labels": [], "entities": []}, {"text": "We were careful to use exactly the same data sets in both cases, not counting punctuation towards sentence lengths.", "labels": [], "entities": []}, {"text": "And we used forgiving scoring ( \u00a73.2) when evaluating these trees.", "labels": [], "entities": []}, {"text": "To get this particular number we forced punctuation to be tacked on, as a layer below the tree of words, to fairly compare systems (using the same initializer).", "labels": [], "entities": []}, {"text": "Since improved initialization strategies -both ours and \"ad-hoc harmonic\" initializer -rely on distances between tokens, they could be unfairly biased towards one approach or the other, if punctuation counted towards length.", "labels": [], "entities": [{"text": "initialization", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.9690932631492615}]}, {"text": "We also trained similar baselines without restrictions, allowing punctuation to appear anywhere in the tree (still with forgiving scoring -see \u00a73.2), using the uninformed uniform initializer (.", "labels": [], "entities": []}, {"text": "Disallowing punctuation as a parent of areal word made things worse, suggesting that not all marks belong near the leaves (sentence stops, semicolons, colons, etc. make more sense as roots and heads).", "labels": [], "entities": []}, {"text": "We tried the weighted initializer also without restrictions and repeated all experiments without scaffolding, on WSJ15 and WSJ45 alone, but treating punctuation as words never came within even 5% of (comparable) standard training.", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.9770545959472656}, {"text": "WSJ45", "start_pos": 123, "end_pos": 128, "type": "DATASET", "confidence": 0.8832558989524841}]}, {"text": "Punctuation, as words, reliably disrupted learning.: Directed accuracies on Section 23 of WSJ \u221e and WSJ10 for the supervised DMV, our baseline systems and the punctuation runs (all using the weighted initializer).", "labels": [], "entities": [{"text": "WSJ \u221e", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.7996001243591309}, {"text": "WSJ10", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.6598206162452698}]}, {"text": "These are multi-point increases, but they could disappear in a more accurate state-of-the-art system.", "labels": [], "entities": []}, {"text": "To test this hypothesis, we applied constrained decoding to a supervised system.", "labels": [], "entities": []}, {"text": "We found that this (ideal) instantiation of the DMV benefits as much or more than the unsupervised systems: accuracy increases from 69.8% to 73.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9996140599250793}]}, {"text": "Punctuation seems to capture the kinds of, perhaps long-distance, regularities that are not accessible to the model, possibly because of its unrealistic independence assumptions.", "labels": [], "entities": []}, {"text": "We next re-examined the choices of constraints.", "labels": [], "entities": []}, {"text": "Our full factorial analysis was similar, but significantly smaller, than Spitkovsky et al.'s (2010b): we excluded their larger-scale news and web data sets that are not publicly available.", "labels": [], "entities": []}, {"text": "Nevertheless, we still tried every meaningful combination of settings, testing both thread and tear (instead of strict, since it can't work with sentences containing sentenceinternal punctuation), in both training and inference.", "labels": [], "entities": []}, {"text": "We did not find better settings than loose for training, and sprawl for decoding, among our options.", "labels": [], "entities": []}, {"text": "A full analysis is omitted due to space constraints.", "labels": [], "entities": []}, {"text": "Our first observation is that constrained inference, using punctuation, is helpful and robust.", "labels": [], "entities": []}, {"text": "It boosted accuracy (on WSJ45) by approximately 1.5%, on average, with all settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9995056390762329}, {"text": "WSJ45", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8838370442390442}]}, {"text": "Indeed, sprawl was consistently (but only slightly, at 1.6%, on average) better than the rest.", "labels": [], "entities": [{"text": "sprawl", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9872229099273682}]}, {"text": "Second, constrained training hurt more often than it helped.", "labels": [], "entities": []}, {"text": "It degraded accuracy in all but one case, loose, where it gained approximately 0.4%, on average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9991517066955566}]}, {"text": "Both improvements are statistically significant: p \u2248 0.036 for training with loose; and p \u2248 5.6 \u00d7 10 \u221212 for decoding with sprawl.", "labels": [], "entities": []}, {"text": "The purpose of these experiments is to compare the punctuation-enhanced DMV with other, recent stateof-the-art systems.", "labels": [], "entities": []}, {"text": "We find that, lexicalized ( \u00a76), our approach performs better, by a wide margin; without lexicalization ( \u00a73.1), it was already better for longer, but not for shorter, sentences (see.", "labels": [], "entities": []}, {"text": "We trained a variant of our system without gold part-of-speech tags, using the unsupervised word clusters) computed by.", "labels": [], "entities": []}, {"text": "Accuracy decreased slightly, to 58.2% on Section 23 of WSJ (down only 0.2%).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9978063702583313}, {"text": "Section 23 of WSJ", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9148662388324738}]}, {"text": "This result improves over substantial performance degradations previously observed for unsupervised dependency parsing with induced word categories (  This final batch of experiments probes the generalization of our approach ( \u00a76) across languages.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7472303807735443}]}, {"text": "The data are from 2006/7 CoNLL shared tasks, where punctuation was identified by the organizers, who also furnished disjoint train/test splits.", "labels": [], "entities": [{"text": "2006/7 CoNLL shared tasks", "start_pos": 18, "end_pos": 43, "type": "DATASET", "confidence": 0.6291398306687673}]}, {"text": "We tested against all sentences in their evaluation sets.", "labels": [], "entities": []}, {"text": "7,8 The gains are not English-specific (see).", "labels": [], "entities": []}, {"text": "Every language improves with constrained decoding (more so without constrained training); and all but Italian benefit in combination.", "labels": [], "entities": []}, {"text": "Averaged across all eighteen languages, the net change inaccuracy is 1.3%.", "labels": [], "entities": [{"text": "change", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9348816275596619}, {"text": "inaccuracy", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.6461954116821289}]}, {"text": "After standard training, constrained decoding alone delivers a 0.7% gain, on average, never causing harm in any of our experiments.", "labels": [], "entities": []}, {"text": "These gains are statistically significant: p \u2248 1.59 \u00d7 10 \u22125 for constrained training; and p \u2248 4.27\u00d710 \u22127 for inference.", "labels": [], "entities": []}, {"text": "With the exception of Arabic '07, from which we discarded one sentence with 145 tokens.", "labels": [], "entities": [{"text": "Arabic '07", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.7804925044377645}]}, {"text": "We down-weighed languages appearing in both years by 50% in our analyses, and excluded Chinese entirely, since it had already been cutup at punctuation.", "labels": [], "entities": []}, {"text": "8 Note that punctuation was treated differently in the two years: in '06, it was always at the leaves of the dependency trees; in '07, it matched original annotations of the source treebanks.", "labels": [], "entities": []}, {"text": "For both, we used punctuation-insensitive scoring ( \u00a73.2).", "labels": [], "entities": []}, {"text": "We did not detect synergy between the two improvements.", "labels": [], "entities": []}, {"text": "However, note that without constrained training, \"full\" data sets do not help, on average, despite having more data and lexicalization.", "labels": [], "entities": []}, {"text": "Furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top 15 fragments of POS tag sequences in WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.895766019821167}]}, {"text": " Table 2: Top 99% of the lowest dominating non-terminals  deriving complete inter-punctuation fragments in WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9223959445953369}]}, {"text": " Table 3: Top 15 productions yielding punctuation-induced fragments in WSJ, viewed as constituents (left) and as de- pendencies (right). For constituents, we recursively expanded any internal nodes that did not align with the associated  fragmentation (underlined). For dependencies we dropped all daughters that fell entirely in the same region as their  mother (i.e., both inside a fragment, both to its left or both to its right), keeping only crossing attachments (just one).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.914204478263855}]}, {"text": " Table 4: Directed accuracies on Section 23 of WSJ \u221e and  WSJ10 for the supervised DMV, our baseline systems and  the punctuation runs (all using the weighted initializer).", "labels": [], "entities": [{"text": "WSJ \u221e", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.789320170879364}, {"text": "WSJ10", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.7972387671470642}]}, {"text": " Table 5: Accuracies on the out-of-domain Brown100 set  and Section 23 of WSJ \u221e and WSJ10, for the lexicalized  punctuation run and other recent state-of-the-art systems.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9696641564369202}, {"text": "Brown100 set", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9330913424491882}, {"text": "WSJ \u221e", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8492765128612518}, {"text": "WSJ10", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.667916476726532}]}, {"text": " Table 6: Multi-lingual evaluation for CoNLL sets, measured at all three stages of training, with and without constraints.", "labels": [], "entities": []}]}