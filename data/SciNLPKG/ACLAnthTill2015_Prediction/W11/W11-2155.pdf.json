{"title": [{"text": "Factored Translation with Unsupervised Word Clusters", "labels": [], "entities": [{"text": "Factored Translation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5219373852014542}]}], "abstractContent": [{"text": "Unsupervised word clustering algorithms-which form word clusters based on a measure of distributional similarity-have proven to be useful in providing beneficial features for various natural language processing tasks involving supervised learning.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7294913232326508}]}, {"text": "This work explores the utility of such word clusters as factors in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.7213704586029053}]}, {"text": "Although some of the language pairs in this work clearly benefit from the factor augmentation , there is no consistent improvement in translation accuracy across the board.", "labels": [], "entities": [{"text": "translation", "start_pos": 134, "end_pos": 145, "type": "TASK", "confidence": 0.9372537136077881}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.8786053657531738}]}, {"text": "For all language pairs, the word clusters clearly improve translation for some proportion of the sentences in the test set, but has a weak or even detrimental effect on the rest.", "labels": [], "entities": []}, {"text": "It is shown that if one could determine whether or not to use a factor when translating a given sentence, rather substantial improvements in precision could be achieved for all of the language pairs evaluated.", "labels": [], "entities": [{"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.999311089515686}]}, {"text": "While such an \"oracle\" method is not identified, evaluations indicate that unsupervised word cluster are most beneficial in sentences without unknown words.", "labels": [], "entities": []}, {"text": "1 Factored translation One can go far in terms of translation quality with plenty of bilingual text and a translation model that maps small chunks of tokens as they appear in the surface form, that is, the usual phrase-based statistical machine translation model.", "labels": [], "entities": [{"text": "Factored translation", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.7475129961967468}, {"text": "phrase-based statistical machine translation", "start_pos": 212, "end_pos": 256, "type": "TASK", "confidence": 0.5817688703536987}]}, {"text": "Yet even with a large parallel corpus, data sparsity is still an issue.", "labels": [], "entities": []}, {"text": "Factored translation models are an extension of phrase-based models which allow integration of additional word-level annotation into the model.", "labels": [], "entities": [{"text": "Factored translation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6567903161048889}]}, {"text": "Operating on more general representations, such as lemmas or some kind of stems, translation model can draw on richer statistics and to some degree offset the data sparsity problem.", "labels": [], "entities": [{"text": "translation", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.9871790409088135}]}, {"text": "the quality of the resulting clustering (quality will be defined later).", "labels": [], "entities": []}, {"text": "1 Note that the algorithm generates a hard clustering-each word belongs to exactly one cluster.", "labels": [], "entities": []}, {"text": "To define the quality of a clustering, we view the clustering in the context of a class-based bigram language model.", "labels": [], "entities": []}, {"text": "Given a clustering C that maps each word to a cluster, the class-based language model assigns a probability to the input text w 1 ,.", "labels": [], "entities": []}, {"text": ".. , w n , where the maximum-likelihood estimate of the model parameters (estimated with empirical counts) are used.", "labels": [], "entities": []}, {"text": "We define the quality of the clustering C to be the logarithm of this probability (see Figure 4-1 and Equation 4.1) normalized by the length of the text.", "labels": [], "entities": []}, {"text": "P (c i |c i\u22121) P (w i |c i) c i = C(w i) Figure 4-1: The class-based bigram language model, which defines the quality of a clustering, represented as a Bayesian network.", "labels": [], "entities": []}, {"text": "1 We use the term clustering to refer to a set of clusters.", "labels": [], "entities": []}, {"text": "44 Figure 1: Bayesian network illustrating the class-based language model that is used to define the quality of a clustering in the Brown algorithm [Liang, 2005] 2 Unsupervised word clusters Unsupervised word clusters owe their appeal perhaps mostly to the relative ease of obtaining them.", "labels": [], "entities": []}, {"text": "Obtaining regular morphological, syntactic or semantic analyses for tokens in a text relies on some sort of tagger, either based on manually crafted rules or trainable on an annotated corpus.", "labels": [], "entities": []}, {"text": "Both rule-crafting and corpus annotation are time-consuming and expensive processes, and might not be feasible fora small or resource-scarce language.", "labels": [], "entities": []}, {"text": "For unsupervised word clusters, on the other hand, one merely needs a large amount of raw (unanno-tated) text and some processing power.", "labels": [], "entities": []}, {"text": "Such clustering is thus particularly interesting for resource-scarce languages, and especially so if the clusters enable the training of more generalized translation models without more bilingual text.", "labels": [], "entities": []}, {"text": "The independence of annotated corpora or hand-crafted rules make unsupervised clusters interesting for languages rich in NLP resources too.", "labels": [], "entities": []}, {"text": "They offer away to exploit vast amounts of raw, unanno-tated, monolingual text, in a manner akin to the way language models profitably maybe trained on vast amounts of raw monolingual text.", "labels": [], "entities": []}, {"text": "With the broad coverage achievable from vast amounts of monolingual text, word clusters might help alleviate the problem of unknown words in translation.", "labels": [], "entities": []}, {"text": "It is imaginable that a word form otherwise unknown to the translation model belongs to 447", "labels": [], "entities": [{"text": "447", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.5819804668426514}]}], "introductionContent": [], "datasetContent": [{"text": "The baseline systems were setup in accordance with the guidelines on the shared task website.", "labels": [], "entities": []}, {"text": "That is, they were trained with grow-diag-final-and word alignment heuristics and msd-bidirectional-fe reordering.", "labels": [], "entities": [{"text": "word alignment heuristics", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.7816639840602875}]}, {"text": "Translation models were trained on a concatenation of the Europarl and News Commentary corpora, which were first tokenized, then filtered to sentence lengths of up to 40 tokens, and finally lowercased.", "labels": [], "entities": [{"text": "Europarl and News Commentary corpora", "start_pos": 58, "end_pos": 94, "type": "DATASET", "confidence": 0.826475715637207}]}, {"text": "5-gram language models were built using ngram-count on a concatenation of the Europarl corpora and the News Commentary corpora.", "labels": [], "entities": [{"text": "Europarl corpora", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9893437325954437}, {"text": "News Commentary corpora", "start_pos": 103, "end_pos": 126, "type": "DATASET", "confidence": 0.9302563269933065}]}, {"text": "3 As available at http://wortschatz.unileipzig.de/~cbiemann/software/unsupos.html 4 LCC refers to the Leipzig Corpora, available at http://corpora.uni-leipzig.de/.", "labels": [], "entities": [{"text": "Leipzig Corpora", "start_pos": 102, "end_pos": 117, "type": "DATASET", "confidence": 0.9360809028148651}]}, {"text": "Wortschatz refers to http://www.wortschatz.uni-leipzig.de/.", "labels": [], "entities": []}, {"text": "Medline is available at http://www.nlm.nih.gov/mesh/filelist.html.", "labels": [], "entities": [{"text": "Medline", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9439451098442078}]}, {"text": "5 A planned evaluation of a cluster count of 3200 was abandoned due to time constraints For the unsupervised word clusters, 5-gram language models were used as well, built from tagged versions of the same corpora.", "labels": [], "entities": []}, {"text": "All language models were binarised and loaded using KenLM.", "labels": [], "entities": [{"text": "KenLM", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.7594152688980103}]}, {"text": "Minimum error rate training (MERT) was used to optimise parameters on both baseline and factored models against the 2008 news test set, as suggested on the shared task website 6 . All phrase tables were filtered and binarised for the development and testing corpora during tuning and testing, respectively.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT)", "start_pos": 0, "end_pos": 34, "type": "METRIC", "confidence": 0.8408352988106864}, {"text": "2008 news test set", "start_pos": 116, "end_pos": 134, "type": "DATASET", "confidence": 0.814726248383522}]}, {"text": "Seeing that the preparation of the raw corpora, word clustering models, factored corpora, language models, as well as training, optimization and evaluation of the various models was a rather involved, yet repetitive process, we took a stab at making a GNU Makefile-based approach for automated handling (and parallelisation) of the whole dependency graph of subtasks.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7358169853687286}]}, {"text": "The ongoing effort, which shares some aspirations and abilities with the recently announced Experiment Management System (EMS), is publicly available 7 . lists BLEU scores for adding jUnsupos tags (uPOS), Brown clusters with 320 clusters (C320) or Brown clusters with 1000 clusters (C1000) as either an alignment factor, a two-sided translation factor or a source-sided translation factor.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9983986020088196}]}], "tableCaptions": [{"text": " Table 1: BLEU scores when using Brown Clusters with granularity 1000 (C1000), granularity 320 (C320)  and unsupervised part-of-speech tags (uPOS) as either an added alignment factor, a two-sided translation  factor or a source-sided translation factor", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9960417747497559}]}, {"text": " Table 2: BLEU scores under the assumption of an  oracle function indicating the optimal factor config- uration for each sentence", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988518953323364}]}, {"text": " Table 3: BLEU scores for the best overall factorisa- tion, Brown clusters (C=1000) as a two-sided trans- lation factor, on sentences with", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9970117807388306}]}]}