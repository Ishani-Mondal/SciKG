{"title": [{"text": "Text Simplification using Typed Dependencies: A Comparison of the Robustness of Different Generation Strategies", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7739650905132294}]}], "abstractContent": [{"text": "We present a framework for text simplification based on applying transformation rules to a typed dependency representation produced by the Stanford parser.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7806375622749329}]}, {"text": "We test two approaches to regeneration from typed dependencies: (a) gen-light, where the transformed dependency graphs are linearised using the word order and morphology of the original sentence, with any changes coded into the transformation rules, and (b) gen-heavy, where the Stanford dependencies are reduced to a DSyntS representation and sentences are generating formally using the RealPro surface realiser.", "labels": [], "entities": [{"text": "RealPro surface realiser", "start_pos": 388, "end_pos": 412, "type": "DATASET", "confidence": 0.9237361351648966}]}, {"text": "The main contribution of this paper is to compare the robustness of these approaches in the presence of parsing errors, using both a single parse and an n-best parse setting in an overgenerate and rank approach.", "labels": [], "entities": [{"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9591039419174194}]}, {"text": "We find that the gen-light approach is robust to parser error, particularly in the n-best parse setting.", "labels": [], "entities": []}, {"text": "On the other hand, parsing errors cause the realiser in the gen-heavy approach to order words and phrases in ways that are disliked by our evaluators.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present a system, REGENT, for text regeneration tasks such as text simplification, style modification or paraphrase.", "labels": [], "entities": [{"text": "REGENT", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.8751035928726196}, {"text": "text regeneration tasks", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.8347329298655192}, {"text": "text simplification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7515847682952881}, {"text": "style modification", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.710406094789505}]}, {"text": "Our system applies transformation rules specified in XML files, to a typed dependency representation obtained from the Stanford Parser).", "labels": [], "entities": []}, {"text": "There are currently rule files for simplifying coordination (of verb phrases and full clauses), subordination, apposition and relative clauses, as well as conversion of passive to active voice; for instance, simplifying: The original police inquiry, which led to Mulcaire being jailed in 2007, also discovered evidence that he has successfully intercepted voicemail messages belonging to Rebekah Brooks, who was editor of the Sun when Mulcaire was working exclusively for its Sunday stablemate.", "labels": [], "entities": [{"text": "the Sun", "start_pos": 422, "end_pos": 429, "type": "DATASET", "confidence": 0.563247486948967}, {"text": "Sunday stablemate", "start_pos": 476, "end_pos": 493, "type": "DATASET", "confidence": 0.9452690482139587}]}, {"text": "to: The original police inquiry led to Mulcaire being jailed in 2007.", "labels": [], "entities": [{"text": "Mulcaire", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9217652082443237}]}, {"text": "The police inquiry also discovered evidence that he has successfully intercepted voicemail messages belonging to Rebekah Brooks.", "labels": [], "entities": []}, {"text": "Rebekah Brooks was editor of the Sun.", "labels": [], "entities": [{"text": "editor of the Sun", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.7763042002916336}]}, {"text": "This was when Mulcaire was working exclusively for its Sunday stablemate.", "labels": [], "entities": [{"text": "Mulcaire", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.7074790596961975}, {"text": "Sunday stablemate", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.9296039640903473}]}, {"text": "The main aim of this paper is to describe and compare two methods for generating sentences from the transformed dependency graphs: 1.", "labels": [], "entities": []}, {"text": "gen-heavy: We use RealPro), a statistical realiser to generate, making all decisions related to morphology and word ordering.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 111, "end_pos": 124, "type": "TASK", "confidence": 0.7001084089279175}]}, {"text": "2. gen-light: We reuse word order and morphology from the original sentence, and specify any changes to these as part of each transformation rule.", "labels": [], "entities": []}, {"text": "Both options have pros and cons.", "labels": [], "entities": []}, {"text": "In the gen-light approach described in detail in and summarised in \u00a73.1, we can reuse information from the input sentence as much as possible, leading to very efficient generation.", "labels": [], "entities": []}, {"text": "The downside is that we need to encode some generation decisions within transfer rules, making them cumbersome to write and difficult to learn automatically.", "labels": [], "entities": []}, {"text": "A case can be made, particularly for the issue of subject-verb agreement, for such issues to be handled by a generator.", "labels": [], "entities": []}, {"text": "This would make the transfer rules simpler to write, and indeed easier to learn automatically in a supervised setting.", "labels": [], "entities": []}, {"text": "While many syntactic simplification rules are quite easy to formulate by hand, this might bean important consideration if we were trying to learn stylistic improvements or other general paraphrase rules from corpora.", "labels": [], "entities": []}, {"text": "To explore the feasibility of using a full surface realiser for text simplification, we implemented a module that converts the Stanford dependencies to a DSyntS representation, and used RealPro) to generate sentences.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7226609587669373}]}, {"text": "This module is briefly described in \u00a73.2, before we evaluate both approaches in \u00a74.", "labels": [], "entities": []}, {"text": "Summary: To summarise our findings, we find that that the gen-light approach is fairly robust to parsing errors, particularly when the n-best parses are used in an overgenerate-and-rank approach.", "labels": [], "entities": [{"text": "parsing", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.9664217233657837}]}, {"text": "However, the gen-heavy approach fares less well, since the process of applying transformation rules to an incorrect analysis and then generating with a statistical realiser often leads to garbled output.", "labels": [], "entities": []}, {"text": "The gen-heavy approach can be made slightly more robust by using the n-best parses, but the judges in our evaluation still find its word and phrase ordering decisions much less acceptable.", "labels": [], "entities": [{"text": "word and phrase ordering", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.5895154103636742}]}, {"text": "Based on our evaluation, we conclude that the preferred solution to regeneration tasks would use a gen-heavy approach for verb features (tense, mood, voice, agreement etc.) and argument ordering, while otherwise reusing word and phrase order from the input.", "labels": [], "entities": [{"text": "argument ordering", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.7290532886981964}]}], "datasetContent": [{"text": "In this paper we have proposed a framework for complex lexico-syntactic text regeneration.", "labels": [], "entities": [{"text": "complex lexico-syntactic text regeneration", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.6326390728354454}]}, {"text": "Our system, REGENT, comes with 63 rules for simplifying coordination, subordination, relative clauses, apposition and passive voice.", "labels": [], "entities": []}, {"text": "In addition, our system offers two generation options (gen-light and genheavy) in two settings (single and n-best parse).", "labels": [], "entities": []}, {"text": "We emphasise that our purpose in this paper is not to evaluate the simplification rules for their effect on comprehension for different categories of users, but only to test the framework for robustness in the face of parsing errors.", "labels": [], "entities": [{"text": "parsing", "start_pos": 218, "end_pos": 225, "type": "TASK", "confidence": 0.96451735496521}]}, {"text": "We will focus on comparing the four different system settings with respect to how many simplifications have been performed and whether these have been done correctly.", "labels": [], "entities": []}, {"text": "Specifically, we will not evaluate whether simplification is found to be useful to different categories of users.", "labels": [], "entities": []}, {"text": "With these narrow goals, we report results using: \u2022 Extent: The level of simplification achieved, based on the number of transforms performed and the average sentence length in the simplified text.", "labels": [], "entities": []}, {"text": "\u2022 Precision: The proportion of transformed sentences for which the rules have been applied accurately, so that the output is grammatical with (a) correct verb agreement and inflexion and (b) modifiers/complements appearing in acceptable orders.", "labels": [], "entities": [{"text": "Precision", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9970617890357971}]}, {"text": "Measuring precision as defined above is tricky.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9993060827255249}]}, {"text": "As a developer trying to evaluate the framework, the pertinent question is whether the transformation rules have been applied correctly.", "labels": [], "entities": []}, {"text": "This however requires knowledge of the transformation rules, which only the developer has.", "labels": [], "entities": []}, {"text": "However, we also need external unbiased judgements by testers not involved with the development of the system.", "labels": [], "entities": []}, {"text": "These would necessarily conflate issues arising from the quality of the transformation rules with issues arising from the parsing and generation aspects of the system.", "labels": [], "entities": []}, {"text": "We present developer test results in \u00a74.1, and an additional evaluation with external testers in \u00a74.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of automatic reformulations.", "labels": [], "entities": []}, {"text": " Table 2: Test results for four configurations of the sys- tem: gen-light and gen-heavy in single parse and 50- best parses modes. The columns report average sentence  length in words, average number of transformations per- formed on each input sentence, percentage of input sen- tences with at least one transformation, the correctness of  the transformations.", "labels": [], "entities": []}, {"text": " Table 3: Pairwise agreement on acceptability", "labels": [], "entities": [{"text": "acceptability", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8290354609489441}]}, {"text": " Table 4: The percentage of transformed sentences accept- able to the three raters (the developer and two judges) for  4 reformulations each of 50 sentences. The final column  treats a transformation as acceptable if at least 2 raters  find it acceptable.", "labels": [], "entities": []}]}