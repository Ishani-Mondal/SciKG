{"title": [], "abstractContent": [{"text": "This paper describes the submission of the National University of Singapore (NUS) to the Helping Our Own (HOO) Pilot Shared Task.", "labels": [], "entities": []}, {"text": "Our system targets spelling, article, and preposition errors in a sequential processing pipeline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Helping Our Own (HOO)) is anew shared task for automatic grammatical error correction, a task which has attracted increasing attention recently.", "labels": [], "entities": [{"text": "automatic grammatical error correction", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.6512518674135208}]}, {"text": "Instead of correcting errors in a general domain, e.g., essays written by second language learners of English, HOO focuses on papers written by non-native authors of English within the natural language processing community.", "labels": [], "entities": []}, {"text": "In this paper, we describe the participating system from the National University of Singapore (NUS).", "labels": [], "entities": [{"text": "National University of Singapore (NUS)", "start_pos": 61, "end_pos": 99, "type": "DATASET", "confidence": 0.8962561999048505}]}, {"text": "The system targets spelling, article, and preposition errors.", "labels": [], "entities": []}, {"text": "The core of our system is built on linear classification models and a large language model filter.", "labels": [], "entities": []}, {"text": "We present experimental results on the HOO development and test data.", "labels": [], "entities": [{"text": "HOO development", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.5715639591217041}]}, {"text": "The next section describes the system in more detail.", "labels": [], "entities": []}, {"text": "Section 3 describes the data sets used.", "labels": [], "entities": []}, {"text": "Section 4 reports experimental results on the HOO development and test data.", "labels": [], "entities": [{"text": "HOO development and test data", "start_pos": 46, "end_pos": 75, "type": "DATASET", "confidence": 0.5771995604038238}]}], "datasetContent": [{"text": "This section reports experimental results of our system on the HOO-HELDOUT and the HOO-TEST data set.", "labels": [], "entities": [{"text": "HOO-HELDOUT", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.935482919216156}, {"text": "HOO-TEST data set", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.9696003993352255}]}, {"text": "The parameters of the system are as follows.", "labels": [], "entities": []}, {"text": "The minimum length for spelling correction is four characters.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.9482065737247467}]}, {"text": "The language model filter for article and preposition correction uses a 5-gram language model built from the complete WEB 1T 5-GRAM CORPUS using RandLM).", "labels": [], "entities": [{"text": "article and preposition correction", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.8240256309509277}, {"text": "WEB 1T 5-GRAM CORPUS", "start_pos": 118, "end_pos": 138, "type": "DATASET", "confidence": 0.8654113113880157}, {"text": "RandLM", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.663140058517456}]}, {"text": "For spelling correction, the language model filter is built from the ACL-ANTHOLOGY data set.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9615383446216583}, {"text": "ACL-ANTHOLOGY data set", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.9516515334447225}]}, {"text": "The linear classifiers for article and preposition correction are trained on the CL-JOURNAL data set.", "labels": [], "entities": [{"text": "preposition correction", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6673689782619476}, {"text": "CL-JOURNAL data set", "start_pos": 81, "end_pos": 100, "type": "DATASET", "confidence": 0.9486432472864786}]}, {"text": "Threshold parameters are tuned on HOO-TUNE when testing on HOO-HELDOUT, and on the complete HOO development data when testing on HOO-TEST.", "labels": [], "entities": [{"text": "Threshold", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9900826811790466}, {"text": "HOO-TUNE", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8932129740715027}, {"text": "HOO-HELDOUT", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9460427165031433}, {"text": "HOO development data", "start_pos": 92, "end_pos": 112, "type": "DATASET", "confidence": 0.7045693000157675}, {"text": "HOO-TEST", "start_pos": 129, "end_pos": 137, "type": "DATASET", "confidence": 0.9672114253044128}]}, {"text": "We report micro-averaged detection, recognition, and correction F 1 scores as defined in the HOO overview paper.", "labels": [], "entities": [{"text": "correction F 1 scores", "start_pos": 53, "end_pos": 74, "type": "METRIC", "confidence": 0.9310940355062485}, {"text": "HOO overview paper", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.8543302416801453}]}, {"text": "The scores are computed over the entire test collection.", "labels": [], "entities": []}, {"text": "For individual error categories, the HOO overview paper only reports the \"percentage of: Overall F 1 scores with (wb) and without bonus (w/o b) on the HOO-HELDOUT data after pre-processing (PRE), spelling (SPEL), article (ART), and preposition correction (PREP).", "labels": [], "entities": [{"text": "HOO overview paper", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.605857233206431}, {"text": "F 1 scores", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9143012563387553}, {"text": "HOO-HELDOUT data", "start_pos": 151, "end_pos": 167, "type": "DATASET", "confidence": 0.7572483718395233}, {"text": "article (ART)", "start_pos": 213, "end_pos": 226, "type": "METRIC", "confidence": 0.9323304295539856}, {"text": "preposition correction (PREP)", "start_pos": 232, "end_pos": 261, "type": "METRIC", "confidence": 0.7260792136192322}]}, {"text": "Step  instances in each category that were detected, recognized and corrected\", but not precision or F 1 scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9995412826538086}, {"text": "F 1 scores", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9810444911321005}]}, {"text": "Computing precision and F 1 is complicated by the fact that the HOO submission format does not require a system to \"label\" each proposed correction with the intended error category.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9974568486213684}, {"text": "F 1", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9899953603744507}, {"text": "HOO submission format", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.6397597293059031}]}, {"text": "As we know which correction was produced by which processing step for our own system, we know which error category a correction belongs to.", "labels": [], "entities": []}, {"text": "Therefore, we can calculate micro-averaged precision, recall, and F 1 scores for spelling, article, and preposition errors individually by restricting the set of proposed edits and the set of gold corrections to a particular category.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9645866751670837}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9988576173782349}, {"text": "F 1 scores", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9913986722628275}]}, {"text": "show the overall detection, recognition, and correction F 1 scores after each processing step on the HOO-HELDOUT and HOO-TEST set, respectively.", "labels": [], "entities": [{"text": "correction F 1", "start_pos": 45, "end_pos": 59, "type": "METRIC", "confidence": 0.9378446539243063}, {"text": "HOO-HELDOUT", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.6520496010780334}, {"text": "HOO-TEST set", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.8243364691734314}]}, {"text": "Each processing step builds on the output of the previous step.", "labels": [], "entities": []}, {"text": "The single biggest improve-: Individual F 1 scores for each error category with (wb) and without bonus (w/o b) on the HOO-TEST data.", "labels": [], "entities": [{"text": "Individual F 1", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.8054316441218058}, {"text": "HOO-TEST data", "start_pos": 118, "end_pos": 131, "type": "DATASET", "confidence": 0.7734859883785248}]}, {"text": "ment in the score comes from the article correction step.", "labels": [], "entities": [{"text": "ment", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9578039646148682}, {"text": "article correction step", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.6187600791454315}]}, {"text": "The gap between the scores with and without bonus shows the large number of optional corrections in the HOO data.", "labels": [], "entities": [{"text": "HOO data", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.8326545059680939}]}, {"text": "show the detection, recognition, and correction F 1 scores for individual error categories on the HOO-HELDOUT and HOO-TEST set, respectively.", "labels": [], "entities": [{"text": "correction F 1", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.9356036384900411}, {"text": "HOO-HELDOUT", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.4833378195762634}, {"text": "HOO-TEST set", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.8054305016994476}]}], "tableCaptions": [{"text": " Table 1: Overview of the data sets.", "labels": [], "entities": []}, {"text": " Table 2: Overall F 1 scores with (wb) and without bonus  (w/o b) on the HOO-HELDOUT data after pre-processing  (PRE), spelling (SPEL), article (ART), and preposition  correction (PREP).", "labels": [], "entities": [{"text": "F 1", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9692275524139404}, {"text": "HOO-HELDOUT", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9089831113815308}, {"text": "spelling (SPEL)", "start_pos": 119, "end_pos": 134, "type": "METRIC", "confidence": 0.8703975826501846}, {"text": "article (ART)", "start_pos": 136, "end_pos": 149, "type": "METRIC", "confidence": 0.9276899695396423}, {"text": "preposition  correction (PREP)", "start_pos": 155, "end_pos": 185, "type": "METRIC", "confidence": 0.6855971038341522}]}, {"text": " Table 3: Overall F 1 scores with (wb) and without bonus  (w/o b) on the HOO-TEST data.", "labels": [], "entities": [{"text": "F 1", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9772041738033295}, {"text": "HOO-TEST data", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.8254479765892029}]}, {"text": " Table 4: Individual F 1 scores for each error category with  (wb) and without bonus (w/o b) on the HOO-HELDOUT  data.", "labels": [], "entities": [{"text": "Individual F 1 scores", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8596189916133881}, {"text": "HOO-HELDOUT  data", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.8767278492450714}]}, {"text": " Table 5: Individual F 1 scores for each error category with  (wb) and without bonus (w/o b) on the HOO-TEST data.", "labels": [], "entities": [{"text": "Individual F 1 scores", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8517285138368607}, {"text": "HOO-TEST data", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.8672291934490204}]}]}