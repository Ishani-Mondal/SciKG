{"title": [{"text": "Gender Attribution: Tracing Stylometric Evidence Beyond Topic and Genre", "labels": [], "entities": [{"text": "Gender Attribution", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8099495470523834}]}], "abstractContent": [{"text": "Sociolinguistic theories (e.g., Lakoff (1973)) postulate that women's language styles differ from that of men.", "labels": [], "entities": []}, {"text": "In this paper, we explore statistical techniques that can learn to identify the gender of authors in modern English text, such as web blogs and scientific papers.", "labels": [], "entities": []}, {"text": "Although recent work has shown the efficacy of statistical approaches to gender attribution, we conjecture that the reported performance might be overly optimistic due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7615934908390045}, {"text": "gender detection task", "start_pos": 240, "end_pos": 261, "type": "TASK", "confidence": 0.811319907506307}]}, {"text": "Our work is the first that consciously avoids gender bias in topics, thereby providing stronger evidence to gender-specific styles in language beyond topic.", "labels": [], "entities": []}, {"text": "In addition, our comparative study provides new insights into robustness of various stylometric techniques across topic and genre.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sociolinguistic theories (e.g.,) postulate that women's language styles differ from that of men with respect to various aspects of communication, such as discourse behavior, body language, lexical choices, and linguistic cues (e.g.,,,,,).", "labels": [], "entities": []}, {"text": "In this paper, we explore statistical techniques that can learn to identify the gender of authors in modern English text, such as web blogs and scientific papers, motivated by sociolinguistic theories for gender attribution.", "labels": [], "entities": []}, {"text": "There is abroad range of potential applications across computational linguistics and social science where statistical techniques for gender attribution can be useful: e.g., they can help understanding demographic characteristics of user-created web text today, which can provide new insight to social science as well as intelligent marketing and opinion mining.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7918570339679718}, {"text": "opinion mining", "start_pos": 346, "end_pos": 360, "type": "TASK", "confidence": 0.7733956575393677}]}, {"text": "Models for gender attribution can also help tracking changes to gender-specific styles in language over different domain and time.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.705163449048996}]}, {"text": "Gender detectors can be useful to guide the style of writing as well, if one needs to assume the style of a specific gender for imaginative writing.", "labels": [], "entities": []}, {"text": "Although some recent work has shown the efficacy of machine learning techniques to gender attribution (e.g.,,), we conjecture that the reported performance might be overly optimistic under scrutiny due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier.", "labels": [], "entities": [{"text": "gender detection task", "start_pos": 274, "end_pos": 295, "type": "TASK", "confidence": 0.8197224140167236}]}, {"text": "Indeed, recent research on web blogs reports that there is substantial gender bias in topics (e.g.,,) as well as in genre (e.g.,).", "labels": [], "entities": []}, {"text": "In order to address this concern, we perform the first comparative study of machine learning techniques for gender attribution after deliberately removing gender bias in topics and genre.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7745280563831329}]}, {"text": "Furthermore, making the task even more realistic (and challenging), we experiment with cross-topic and crossgenre gender attribution, and provide statistical evidence to gender-specific styles in language beyond topic and genre.", "labels": [], "entities": [{"text": "crossgenre gender attribution", "start_pos": 103, "end_pos": 132, "type": "TASK", "confidence": 0.5891035596529642}]}, {"text": "Five specific questions we aim to investigate are: Q1 Are there truly gender-specific characteristics in language? or are they confused with gender preferences in topics and genre?", "labels": [], "entities": []}, {"text": "Q2 Are there deep-syntactic patterns in women's language beyond words and shallow patterns?", "labels": [], "entities": []}, {"text": "Q3 Which stylometric analysis techniques are effective in detecting characteristics in women's language?", "labels": [], "entities": [{"text": "detecting characteristics in women's language", "start_pos": 58, "end_pos": 103, "type": "TASK", "confidence": 0.7651313742001852}]}, {"text": "Q4 Which stylometric analysis techniques are robust against domain change with respect to topics and genre?", "labels": [], "entities": []}, {"text": "Q5 Are there gender-specific language characteristics even in modern scientific text?", "labels": [], "entities": []}, {"text": "From our comparative study of various techniques for gender attribution, including two publicly available systems -Gender Genie 1 and Gender Guesser 2 we find that (1) despite strong evidence for deep syntactic structure that characterizes gender-specific language styles, such deep patterns are not as robust as shallow morphology-level patterns when faced with topic and genre change, and that (2) there are indeed gender-specific linguistic signals that go beyond topics and genre, even in modern and scientific literature.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7553825378417969}, {"text": "Gender Guesser", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.595048263669014}]}], "datasetContent": [{"text": "In this section, we describe how we prepared our dataset to avoid unwanted gender bias in topics and genre.", "labels": [], "entities": []}, {"text": "Much of previous work has focused on formal writings, such as English literature, newswire articles and the British Natural Corpus(BNC) (e.g.,), while recent studies expanded toward more informal writing such as web blogs (e.g.,).", "labels": [], "entities": [{"text": "British Natural Corpus(BNC)", "start_pos": 108, "end_pos": 135, "type": "DATASET", "confidence": 0.9618305961290995}]}, {"text": "In this work, we chose two very different and prominent genre electronically available today: web blogs and scientific papers.", "labels": [], "entities": []}, {"text": "Blogs: We downloaded blogs from popular blog sites for 7 distinctive topics: 3 education, travel, spirituality, entertainment, book reviews, history and politics.", "labels": [], "entities": []}, {"text": "Within each topic, we find 20 articles written by male authors, and additional 20 articles written by female authors.", "labels": [], "entities": []}, {"text": "We took the effort to match articles written by different gender even at the subtopic level.", "labels": [], "entities": []}, {"text": "For example, if we take a blog written about the TV show \"How I met your mother\" by a female author, then we also find a blog written by a male author on the same show.", "labels": [], "entities": []}, {"text": "Note that previous research on web blogs does not purposefully maintain balanced topics between gender, thereby benefiting from topic bias inadvertently.", "labels": [], "entities": []}, {"text": "From each blog, we keep the first 450 (+/-20) words preserving the sentence boundaries.", "labels": [], "entities": []}, {"text": "We plan to make this data publically available.", "labels": [], "entities": []}, {"text": "Scientific Papers: Scientific papers have not been studied in previous research on gender attribution.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.762000322341919}]}, {"text": "Scientific papers correspond to very formal writing where gender-specific language styles are not likely to be conspicuous (e.g., Janssen and Murachver).", "labels": [], "entities": []}, {"text": "For this dataset, we collected papers from the researchers in our own Natural Language Processing community.", "labels": [], "entities": []}, {"text": "We randomly selected 5 female and 5 male authors, and collected 20 papers from each author.", "labels": [], "entities": []}, {"text": "We tried to select these authors across a variety of subtopics within NLP research, so as to reduce potential topic-bias in gender even in research.", "labels": [], "entities": []}, {"text": "It is also worthwhile to mention that authors in our selection are highly established ones who have published over multiple subtopics in NLP.", "labels": [], "entities": []}, {"text": "Similarly as the blog dataset, we keep the first 450 (+/-20) words preserving the sentence boundaries.", "labels": [], "entities": []}, {"text": "Some papers are co-authored by researchers of mixed gender.", "labels": [], "entities": []}, {"text": "In those cases, we rely on the gender of the advisory person as she or he is likely to influence on the abstract and intro the most.", "labels": [], "entities": []}, {"text": "Note that our two datasets are created to specifically answer the following question: are there genderspecific characteristics in language beyond gender).", "labels": [], "entities": []}, {"text": "However, if they can still perform considerably better than random prediction, then it would prove that there is indeed gender-specific stylometric characteristics beyond topic and genre.", "labels": [], "entities": []}, {"text": "In what follows, we present five different experimental settings across two different dataset to compare in-domain and cross-domain performance of various techniques for gender attribution.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.7113458812236786}]}, {"text": "First we conduct two different experiments using the blog data in the order of increasing difficulty.", "labels": [], "entities": []}, {"text": "[Experiment-I: Balanced Topic] Using the web blog dataset introduced in Section 3, we perform gender attribution (classification) task on balanced topics.", "labels": [], "entities": [{"text": "gender attribution (classification)", "start_pos": 94, "end_pos": 129, "type": "TASK", "confidence": 0.7374263942241669}]}, {"text": "For each topic, 80% of the documents are used for training and remaining ones are used for testing, yielding 5-fold cross validation.", "labels": [], "entities": []}, {"text": "Both training and test data have balanced class distributions so that random guess would yield 50% of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9983687996864319}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "Note that the \"overall accuracy\" corresponds to the average across the five folds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.983665406703949}]}, {"text": "The PCFG model achieves prediction accuracy 64.1%, demonstrating statistical evidence to genderspecific characteristics in syntactic structure.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8491277098655701}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8323717713356018}]}, {"text": "The PCFG model outperforms two publicly available systems -Gender Genie and Gender Guesser, which are based on a fixed list of indicator words.", "labels": [], "entities": [{"text": "Gender Guesser", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.5923395156860352}]}, {"text": "The difference is statistically significant (p = 0.01 < 0.05)   using paired student's t-test.", "labels": [], "entities": []}, {"text": "Interestingly, the best performing approaches are character-level language models, performing substantially better (71.30% for n=2) than both the token-level language models (66.1% for n=2) and the PCFG model (64.10%).", "labels": [], "entities": []}, {"text": "The difference between CLM(n=2) and PCFG is statistically significant (p = 0.015 < 0.05) using paired student's t-test, while the difference between TLM(n=2) and PCFG is not.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.9044986367225647}]}, {"text": "We also experimented with the interpolated PCFG model following using various interpolation dataset, but we were notable to achieve a better result in our experiments.", "labels": [], "entities": []}, {"text": "We omit the results of interpolated PCFG models for brevity.", "labels": [], "entities": []}, {"text": "As will be seen in the following experiment (Experiment-II) using the Blog dataset as well, the performance of PCFG models is very close to that of unigram language models.", "labels": [], "entities": [{"text": "Blog dataset", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9772015511989594}]}, {"text": "As a result, one might wonder whether PCFG models are learning any useful syntactic pattern beyond terminal productions that can help discriminating gender-specific styles in language.", "labels": [], "entities": []}, {"text": "This question will be partially answered in the fourth experiment (Experiment-IV) using the Scientific Paper dataset, where PCFG models demonstrate considerably better performance over the unigram language models.", "labels": [], "entities": [{"text": "Scientific Paper dataset", "start_pos": 92, "end_pos": 116, "type": "DATASET", "confidence": 0.9695690075556437}]}, {"text": "Male  imented with ensemble methods that linearly combine the output of different classifiers, but we omit the results in, as we were notable to obtain consistently higher performance than the simple character-level language models in our dataset.", "labels": [], "entities": []}, {"text": "[Experiment-II: Cross-Topic] Next we perform cross-topic experiments using the same blog dataset, in order to quantify the robustness of different techniques against topic change.", "labels": [], "entities": []}, {"text": "We train on 6 topics, and test on the remaining 1 topic, making 7-fold cross validation.", "labels": [], "entities": []}, {"text": "The results are shown in, where the top one third shows the performance for all authors, the next one third shows the performance with respect to only female authors, the bottom one third shows the performance with respect to only male authors.", "labels": [], "entities": []}, {"text": "Again, the best performing approaches are based on character-level language models, achieving upto 68.3% inaccuracy.", "labels": [], "entities": []}, {"text": "PCFG models and token-level language models achieve substantially lower accuracy of 59.0% and 61.5% respectively.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9306547045707703}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9995563626289368}]}, {"text": "Per-gender analysis in reveals interesting insights into different approaches.", "labels": [], "entities": [{"text": "Per-gender analysis", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7909470796585083}]}, {"text": "In particular, we find that Gender Genie and Gender Guesser are biased toward male authors, attributing the majority authors as male.", "labels": [], "entities": [{"text": "Gender Genie", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.6648929417133331}, {"text": "Gender Guesser", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.5968140512704849}]}, {"text": "PCFG and ME on the other hand are biased toward female authors.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9777332544326782}, {"text": "ME", "start_pos": 9, "end_pos": 11, "type": "DATASET", "confidence": 0.49827417731285095}]}, {"text": "Both character-level and token-level language models show balanced distribution between gender.", "labels": [], "entities": []}, {"text": "We also experimented with ensemble methods, but omit the results as we were notable to obtain higher scores than simple character-level language models.", "labels": [], "entities": []}, {"text": "From these two experiments so far, we find that PCFG models and word-level language models are neither as effective, nor as robust as character-level language models for gender attribution.", "labels": [], "entities": []}, {"text": "Despite overall low performance of PCFG models, this result suggests that PCFG models are able to learn gender-specific syntactic patterns, albeit the signals from deep syntax seem much weaker than those of very shallow morphological patterns.", "labels": [], "entities": []}, {"text": "Next we present three different experiments using the scientific data, in the order of decreasing difficulty.", "labels": [], "entities": []}, {"text": "[Experiment-III: In this experiment, we challenge statistical techniques for gender attribution by changing both topics and genre across training and testing.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7498796284198761}]}, {"text": "To do so, we train models on the blog dataset and test on the scientific paper dataset.", "labels": [], "entities": [{"text": "blog dataset", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.7427798211574554}, {"text": "scientific paper dataset", "start_pos": 62, "end_pos": 86, "type": "DATASET", "confidence": 0.760638823111852}]}, {"text": "Notice that this is a dramatically harder task than the previous two experiments.", "labels": [], "entities": []}, {"text": "Note also that previous research thus far has not reported experiments such as this, or even like the previous one.", "labels": [], "entities": []}, {"text": "It is worthwhile to mention that our goal in this paper is not domain adaptation for gender attribution, but merely to quantify to what degree the gender-specific language styles can be traced across different topics and genre, and which techniques are robust against domain change.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7136503756046295}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Precisely as expected, the performance of all models drop significantly in this scenario.", "labels": [], "entities": []}, {"text": "The two baseline systems -Gender Genie and Gender Guesser, which are not designed for formal scientific writings also perform worse in this dataset.", "labels": [], "entities": [{"text": "Gender Genie", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.5538319200277328}, {"text": "Gender Guesser", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.5992877334356308}]}, {"text": "discussed in the next experiment will provide more insight into this by providing per-gender accuracy of these baseline systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9905139207839966}]}, {"text": "From this experiment, we find a rather surprising message: although the performance of most statistical approaches decreases significantly, notice that most approaches perform still better than random (50%) prediction, achieving upto 61.5% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 240, "end_pos": 248, "type": "METRIC", "confidence": 0.9978154897689819}]}, {"text": "Considering that the models are trained on drastically different topics and genre, this result suggests that there are indeed gender-specific linguistic signals beyond different topics and genre.", "labels": [], "entities": []}, {"text": "This is particularly interesting given that scientific papers correspond to very formal writing where genderspecific language styles are not likely to be conspicuous (e.g.,).", "labels": [], "entities": []}, {"text": "[Experiment-IV: Cross-Topic] Next we perform cross-topic experiment, only using the scientific paper dataset.", "labels": [], "entities": []}, {"text": "Because the stylistic difference in genre is significantly more prominent than the stylistic difference in topics, this should be a substantially easier task than the previous experiment.", "labels": [], "entities": []}, {"text": "Nevertheless, previous research to date has not attempted to evaluate gender attribution techniques across different topics.", "labels": [], "entities": [{"text": "gender attribution", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7276692986488342}]}, {"text": "Here we train on 4 authors per gender (8 authors in total), and test on the remaining 2 authors, making 5-fold cross validation.", "labels": [], "entities": []}, {"text": "As before, the class distributions are balanced in both training and test data.", "labels": [], "entities": []}, {"text": "The experimental results are shown in, where we report per-author, per-gender, and overall average accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9838151335716248}]}, {"text": "As expected, the overall performance increase dramatically, as models are trained on articles in the same genre.", "labels": [], "entities": []}, {"text": "It is interesting to see how Gender Genie and Gender Guesser are extremely biased toward male authors, achieving almost zero accuracy with respect to articles written by female authors.", "labels": [], "entities": [{"text": "Gender Genie", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.6735220849514008}, {"text": "Gender Guesser", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.6439906805753708}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.998499870300293}]}, {"text": "Here the best performing models are PCFG and CLM(n=3), both achieving 76.0% inaccuracy.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8693480491638184}]}, {"text": "Token-level language models on the other hand achieve significantly lower performance.", "labels": [], "entities": []}, {"text": "Remind that in the first two experiments based on the blog data, PCFG models and token-level language models performed similarly.", "labels": [], "entities": []}, {"text": "Given that, it is very interesting that PCFG models now perform just as good as character-level language models, while outperforming token-level language models significantly.", "labels": [], "entities": []}, {"text": "We conjecture following two reasons to explain this: \u2022 First, scientific papers use very formal language, thereby suppressing gender-specific lexical cues that are easier to detect (e.g., empty words such as \"lovely\", \"gorgeous\"  addresses the concern raised in Experiment-I & II as to whether the PCFG models are learning any syntactic pattern beyond terminal productions that are similar to unigram language models.", "labels": [], "entities": []}, {"text": "\u2022 Second, our dataset is constructed in such away that the training and test data do not share articles written by the same authors.", "labels": [], "entities": []}, {"text": "Furthermore, the authors are chosen so that the main research topics are substantially different from each other.", "labels": [], "entities": []}, {"text": "Therefore, token-based language models are likely to learn topical words and phrases, and suffer when the topics change dramatically between training and testing.", "labels": [], "entities": []}, {"text": "[Experiment-V: Balanced Topic] Finally, we present the conventional experimental setup, where topic distribution is balanced between training and test dataset.", "labels": [], "entities": []}, {"text": "This is not as interesting as the previous two scenarios, however, we include this experiment in order to provide a loose upper bound.", "labels": [], "entities": []}, {"text": "Because we choose each different author from each different sub-topic of research, we need to split articles by the same author into training and testing to ensure balanced topic distribution.", "labels": [], "entities": []}, {"text": "We select 80% of articles from each author as training data, and use the remaining 20% as test data, resulting in 5-fold cross validation.", "labels": [], "entities": []}, {"text": "This is the easiest task among the three experiments using the scientific paper data, hence the performance increases substantially.", "labels": [], "entities": []}, {"text": "As before, character-level language models perform the best, with CLM n=3 reaching extremely high accuracy of 91.50%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9991687536239624}]}, {"text": "All other statistical approaches perform very well achieving at least 85% or higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9973315000534058}]}, {"text": "Note that token-level language models perform very poorly in the previous experimental setting, while performing close to the top performer in this experiment.", "labels": [], "entities": []}, {"text": "We make the following two conclusions based on the last two experiments: \u2022 Token-level language models have the tendency of learning topics words, rather than just stylometric cues.", "labels": [], "entities": []}, {"text": "\u2022 When performing cross-topic gender attribution (as in Experiment-IV), PCFG models are more robust than token-level language models.", "labels": [], "entities": [{"text": "cross-topic gender attribution", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.617795060078303}]}], "tableCaptions": [{"text": " Table 1: Overall Accuracy of Topic-Balanced Gender Attribution on Blog Data (Experiment-I)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9630508422851562}, {"text": "Topic-Balanced Gender Attribution on Blog", "start_pos": 30, "end_pos": 71, "type": "TASK", "confidence": 0.7232414066791535}]}, {"text": " Table 2: Per-Topic & Per-Gender Accuracy of Cross-Topic Gender Attribution on Blog Data (Experiment-II)", "labels": [], "entities": [{"text": "Cross-Topic Gender Attribution on Blog", "start_pos": 45, "end_pos": 83, "type": "TASK", "confidence": 0.7174704253673554}]}, {"text": " Table 3: Overall Accuracy of Cross-Topic /Cross-Genre Gender Attribution on Scientific Papers (Experiment-III)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9588358998298645}, {"text": "Cross-Genre Gender Attribution on Scientific Papers", "start_pos": 43, "end_pos": 94, "type": "TASK", "confidence": 0.7290293127298355}]}, {"text": " Table 4: Per-Author Accuracy of Cross-Topic Gender Attribution for Scientific Papers (Experiment-IV)", "labels": [], "entities": [{"text": "Cross-Topic Gender Attribution", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.6185304323832194}]}, {"text": " Table 5: Overall Accuracy of Topic-Balanced Gender Attribution on Scientific Papers (Experiment-V)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9555853605270386}, {"text": "Topic-Balanced Gender Attribution on Scientific Papers", "start_pos": 30, "end_pos": 84, "type": "TASK", "confidence": 0.7636958956718445}]}]}