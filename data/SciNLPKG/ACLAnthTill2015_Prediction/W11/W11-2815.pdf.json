{"title": [{"text": "Combining symbolic and corpus-based approaches for the generation of successful referring expressions", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach to the generation of referring expressions (REs) which computes the unique RE that it predicts to be fastest for the hearer to resolve.", "labels": [], "entities": [{"text": "generation of referring expressions (REs)", "start_pos": 30, "end_pos": 71, "type": "TASK", "confidence": 0.6638061829975673}]}, {"text": "The system operates by learning a maximum entropy model for referential success from a corpus and using the model's weights as costs in a metric planning problem.", "labels": [], "entities": []}, {"text": "Our system outperforms the baselines both on predicted RE success and on similarity to human-produced successful REs.", "labels": [], "entities": []}, {"text": "A task-based evaluation in the context of the GIVE-2.5 Challenge on Generating Instructions in Virtual Environments verifies the higher RE success scores of the system.", "labels": [], "entities": [{"text": "RE success", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9111458659172058}]}], "introductionContent": [{"text": "The generation of referring expressions (REs) is one of the best-studied problems in natural language generation (NLG).", "labels": [], "entities": [{"text": "generation of referring expressions (REs)", "start_pos": 4, "end_pos": 45, "type": "TASK", "confidence": 0.8202197636876788}, {"text": "natural language generation (NLG)", "start_pos": 85, "end_pos": 118, "type": "TASK", "confidence": 0.8090823690096537}]}, {"text": "Traditional approaches have focused on defining the range of possible valid REs (e.g., as those REs that describe the target object uniquely) and on simple heuristics for choosing one valid RE (e.g., minimal REs).", "labels": [], "entities": []}, {"text": "Recently, the question of how to choose the best RE out of the possible ones has gained increasing attention (.", "labels": [], "entities": [{"text": "RE", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9042040109634399}]}, {"text": "This process has been accelerated by the systematic evaluation of RE generation systems in the context of RE generation challenges (.", "labels": [], "entities": [{"text": "RE generation", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.864145964384079}, {"text": "RE generation", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.8887748122215271}]}, {"text": "Almost all of these approaches optimize the humanlikeness of the NLG system, i.e. the similarity between system-generated REs and humangenerated REs from some corpus.", "labels": [], "entities": []}, {"text": "However, in order to be most helpful to the user, an NLG system should arguably produce REs that are easy to understand.", "labels": [], "entities": []}, {"text": "As show, these are not the same: In particular, the scores for humanlikeness and usefulness in task-based evaluations of systems participating in the TUNA RE generation challenge are not correlated.", "labels": [], "entities": [{"text": "TUNA RE generation challenge", "start_pos": 150, "end_pos": 178, "type": "TASK", "confidence": 0.7279053330421448}]}, {"text": "It would therefore be desirable to optimize a system directly for usefulness.", "labels": [], "entities": []}, {"text": "A second characteristic of most existing RE generation systems is that they are limited to generating single noun phrases in isolation.", "labels": [], "entities": [{"text": "RE generation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9842245578765869}]}, {"text": "By contrast, planning-based approaches generate REs in the context of an entire sentence or even discourse ( , and can therefore exploit and manipulate the linguistic and non-linguistic context in order to produce succinct REs).", "labels": [], "entities": []}, {"text": "However, these approaches have not been combined with corpus-based measures of humanlikeness or understandability of REs.", "labels": [], "entities": [{"text": "REs", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.7721919417381287}]}, {"text": "In this paper, we present the mSCRISP system, which extends the planning-based approach to NLG with a statistical model of RE understandability.", "labels": [], "entities": [{"text": "RE understandability", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.9104755818843842}]}, {"text": "mSCRISP uses a metric planner) to compute the best REs that refer uniquely to the target referent, and thus combines statistical and symbolic reasoning.", "labels": [], "entities": [{"text": "mSCRISP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.914896547794342}]}, {"text": "We obtain the cost model by training a maximum entropy (maxent) classifier on a corpus of human-generated instruction giving sessions () in which every RE can be automatically annotated with a measure of how easy it was for the hearer to resolve.", "labels": [], "entities": []}, {"text": "Although mSCRISP is in principle capable of generating complete in-struction discourses, we only evaluate its RE generation component here.", "labels": [], "entities": [{"text": "RE generation", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.6533066928386688}]}, {"text": "It turns out that mSCRISP generates more understandable REs than a purely symbolic baseline, according to our model's estimation of understandability.", "labels": [], "entities": []}, {"text": "Furthermore, mSCRISP generates REs that are more similar to high-quality human-generated REs than either the symbolic or a purely statistical baseline.", "labels": [], "entities": []}, {"text": "Finally, a full task-based evaluation in the context of the GIVE-2.5 Challenge on Generating Instructions in Virtual Environments 1) verifies the higher referential success of the system.", "labels": [], "entities": []}, {"text": "We first compare our model to earlier work in Section 2.", "labels": [], "entities": []}, {"text": "We then introduce the planning-based approach to NLG on which mSCRISP is based in Section 3.", "labels": [], "entities": [{"text": "mSCRISP", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.912665605545044}]}, {"text": "Section 4 lays out how we obtain a maximum entropy model of RE attribute preferences from our corpus, and Section 5 shows how we bring the two approaches together using metric planning.", "labels": [], "entities": []}, {"text": "We present the evaluation in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model against two baselines.", "labels": [], "entities": []}, {"text": "The MaxEnt baseline builds an RE by selecting all attributes a j for which v j (s) \u2264 0 fora given scene s.", "labels": [], "entities": [{"text": "MaxEnt baseline", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8422164916992188}, {"text": "RE", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.7911433577537537}]}, {"text": "This is a purely statistical model, which does not verify the applicability or discriminatory power of the attributes it selects, and thus makes no correctness or uniqueness guarantees.", "labels": [], "entities": []}, {"text": "The EqualCosts baseline is aversion of our mSCRISP model in: REs produced by a human IG, our model and the two baselines in the bottom-left room of. which all attribute costs are equal.", "labels": [], "entities": []}, {"text": "This is a purely symbolic model which always computes a correct and unique RE, but does this without any empirical guidance about expected successfulness.", "labels": [], "entities": [{"text": "RE", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9390072226524353}]}, {"text": "presents example REs that a human IG, our model and the two baselines issue for one of the buttons in the bottom-left room of.", "labels": [], "entities": []}, {"text": "As the IF is entering the room, they see from left to right a green button, a picture, and another green button.", "labels": [], "entities": []}, {"text": "All REs in this example are distinguishing.", "labels": [], "entities": [{"text": "REs", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7188149094581604}]}, {"text": "However, the human-produced RE, which favors the use of an absolute (\"green\") and a viewer-centered (\"on the left\") attribute over one pointing to the microlevel landmark (\"to the left of the picture\"), was not particularly successful in the scene: After hearing it, the IF spent time scanning the room further to the left before finally approaching the referent.", "labels": [], "entities": [{"text": "RE", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.8139528632164001}]}, {"text": "MaxEnt and mSCRISP generate a different RE, using a landmark, which they judge to be more successful.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8748223185539246}, {"text": "RE", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.933147132396698}]}, {"text": "By contrast, EqualCosts generates a correct but more complex RE.", "labels": [], "entities": []}, {"text": "To verify the model's performance in the context of real interactions with human IFs, we entered mSCRISP and the correct RE generating baseline EqualCosts as participating NLG systems for the 2011 edition of the GIVE Challenge ().", "labels": [], "entities": [{"text": "RE generating baseline EqualCosts", "start_pos": 121, "end_pos": 154, "type": "METRIC", "confidence": 0.8962885737419128}]}, {"text": "Both systems operate by first generating an RE (the first-attempt RE) fora given button target as soon as the IF is in the target's room and can seethe target.", "labels": [], "entities": [{"text": "RE", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9650254249572754}]}, {"text": "Subsequently, the systems issue follow-up REs at regular intervals until the IF responds with a manipulation actor navigates away from the target.", "labels": [], "entities": []}, {"text": "Follow-up REs may differ from first-attempt REs, especially for the mSCRISP system, which relies for its attribute selection on several dynamically changing context features of the scene (see).", "labels": [], "entities": []}, {"text": "Indeed, mSCRISP issues follow-up REs that are dif-  ferent from the original more often than the purely symbolic system (in 85% of the cases, as compared to only 59% for EqualCosts).", "labels": [], "entities": []}, {"text": "Follow-up REs are important for the GIVE task, yet the fact that they are issued regardless of whether the IF is on the right track or not poses a problem on automatic methods of assessing success.", "labels": [], "entities": [{"text": "GIVE task", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.8505280315876007}]}, {"text": "We therefore base our analysis only on first-attempt REs.", "labels": [], "entities": []}, {"text": "To control for the effect of rephrasing, we separately examine the subset of REs for which all follow-up REs were non-rephrasing, i.e. exactly the same as the original.", "labels": [], "entities": []}, {"text": "We conduct the analysis on the latest currently available snapshot of the challenge results, which contains 74 valid games for each of our two systems.", "labels": [], "entities": []}, {"text": "We first look into two metrics for referential success, as shown in.", "labels": [], "entities": []}, {"text": "In terms of resolution success, which represents the rate of REs whose intended referents have been correctly identified by the hearer (regardless of how fast), we find that mSCRISP significantly outperforms the baseline with a high success rate of 95%.", "labels": [], "entities": []}, {"text": "Though the results are measured on different datasets and are thus not directly comparable, it is interesting to note that this surpasses the 92% success rate of human IGs in the GIVE-2 corpus.", "labels": [], "entities": [{"text": "GIVE-2 corpus", "start_pos": 179, "end_pos": 192, "type": "DATASET", "confidence": 0.8899457454681396}]}, {"text": "The system's performance remains better than the baseline's, though not significantly so, in the nonrephrased RE dataset.", "labels": [], "entities": [{"text": "RE dataset", "start_pos": 110, "end_pos": 120, "type": "DATASET", "confidence": 0.7555957734584808}]}, {"text": "Turning to the metric of successfulness as defined in Subsection 4.2, we see that the two systems do not differ significantly when all first-attempt REs are considered.", "labels": [], "entities": []}, {"text": "However it is clear that rephrasing affects the hearer's response, since processing new REs takes additional time.", "labels": [], "entities": []}, {"text": "Examining the portion of non-rephrased first-attempt REs, we find that our model does generate REs that humans resolve significantly faster.", "labels": [], "entities": []}, {"text": "Finally, from the questionnaire data collected in the challenge, we consider a subjective metric of RE success as reported by the IFs in response to the post-task question \"I could easily identify the buttons the system described to me\".", "labels": [], "entities": [{"text": "RE success", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.8034811913967133}]}, {"text": "Although a Tukey's test does not find the difference to be statistically significant, it is worth mentioning that our model receives higher rates than the baseline with respect to this subjective metric, too.", "labels": [], "entities": []}, {"text": "The average scores for mSCRISP and EqualCosts are 38.59 and 16.42, respectively (on a scale of -100 to 100).", "labels": [], "entities": [{"text": "mSCRISP", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.8496574759483337}]}], "tableCaptions": [{"text": " Table 5: Average DICE coefficients across datasets. Dif- ferences to mSCRISP are significant at *p < 0.05,  ***p < 0.001 (paired t-tests).", "labels": [], "entities": [{"text": "Dif- ferences", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.9636351466178894}]}, {"text": " Table 6: Task-based evaluation results. Differences to  mSCRISP are significant at ***p < 0.001 (Pearson's \u03c7 2  test for resolution success rates; unpaired two-sample t- tests for the rest).", "labels": [], "entities": []}]}