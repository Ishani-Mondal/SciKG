{"title": [{"text": "Regression and Ranking based Optimisation for Sentence Level Machine Translation Evaluation", "labels": [], "entities": [{"text": "Sentence Level Machine Translation Evaluation", "start_pos": 46, "end_pos": 91, "type": "TASK", "confidence": 0.8957927107810975}]}], "abstractContent": [{"text": "Automatic evaluation metrics are fundamentally important for Machine Translation, allowing comparison of systems performance and efficient training.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.9031409323215485}]}, {"text": "Current evaluation met-rics fall into two classes: heuristic approaches, like BLEU, and those using supervised learning trained on human judgement data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9962033629417419}]}, {"text": "While many trained metrics provide a better match against human judgements, this comes at the cost of including lots of features, leading to unwieldy, non-portable and slow metrics.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew trained metric , ROSE, which only uses simple features that are easy portable and quick to compute.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9942246079444885}]}, {"text": "In addition, ROSE is sentence-based, as opposed to document-based, allowing it to be used in a wider range of settings.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.6438187956809998}]}, {"text": "Results show that ROSE performs well on many tasks, such as ranking system and syntactic constituents, with results competitive to BLEU.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.7816617488861084}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9828417301177979}]}, {"text": "Moreover, this still holds when ROSE is trained on human judgements of translations into a different language compared with that use in testing.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.6199010014533997}]}], "introductionContent": [{"text": "Human judgements of translation quality are very expensive.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8290600478649139}]}, {"text": "For this reason automatic MT evaluation metrics are used to as an approximation by comparing predicted translations to human authored references.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9254743158817291}]}, {"text": "An early MT evaluation metric, BLEU), is still the most commonly used metric in automatic machine translation evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9301348924636841}, {"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9456093907356262}, {"text": "machine translation evaluation", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.7969057361284891}]}, {"text": "However, several drawbacks have been stated by many researchers;), most notably that it omits recall (substituting this with a penalty for overly short output) and not being easily applied at the sentence level.", "labels": [], "entities": [{"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9952630996704102}]}, {"text": "Later heuristic metrics such as METEOR () and TER () account for both precision and recall, but their relative weights are difficult to determine manually.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9880115389823914}, {"text": "TER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9975650310516357}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9993891716003418}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9985886216163635}]}, {"text": "In contrast to heuristic metrics, trained metrics use supervised learning to model directly human judgements.", "labels": [], "entities": []}, {"text": "This allows the combination of different features and can better fit specific tasks, such as evaluation focusing more on fluency/adequacy/relative ranks or post editing effort.", "labels": [], "entities": []}, {"text": "Previous work includes approaches using classification), regression, and ranking.", "labels": [], "entities": []}, {"text": "Most of which achieved good results and better correlations with human judgments than heuristic baseline methods.", "labels": [], "entities": []}, {"text": "Overall automatic metrics must find a balance between several key issues: a) applicability to different sized texts (documents vs sentences), b) easy of portability to different languages, c) runtime requirements and d) correlation with human judgement data.", "labels": [], "entities": []}, {"text": "Previous work has typically ignored at least one of these issues, e.g., BLEU which applies only to documents (A), trained metrics) which tend to ignore B and C.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9984695315361023}]}, {"text": "This paper presents ROSE, a trained metric which is loosely based on BLEU, but seeks to further simplify its components such that it can be used for sentence level evaluation.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9855207800865173}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9891942739486694}, {"text": "sentence level evaluation", "start_pos": 149, "end_pos": 174, "type": "TASK", "confidence": 0.6479690670967102}]}, {"text": "This contrasts with BLEU which is defined overlarge documents, and must be coarsely approximated to allow sentence level application.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9953420162200928}]}, {"text": "The increased flexibility of ROSE allows the metric to be used in a wider range of situations, including during decoding.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9655476212501526}]}, {"text": "ROSE is a linear model with a small number of simple features, and is trained using regression or ranking against human judgement data.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.385722815990448}]}, {"text": "A benefit of using only simple features is that ROSE can be trivially ported between target languages, and that it can be run very quickly.", "labels": [], "entities": []}, {"text": "Features include precision and recall over different sized n-grams, and the difference in word counts between the candidate and the reference sentences, which is further divided into content word, function word and punctuation.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9993394017219543}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9983660578727722}]}, {"text": "An extended versions also includes features over Part of Speech (POS) sequences.", "labels": [], "entities": [{"text": "Part of Speech (POS) sequences", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.7537385651043483}]}, {"text": "The paper is structured as follows: Related work on metrics for statistical machine translation is described in Section 2.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.7131959597269694}]}, {"text": "Four variations of ROSE and their features will be introduced in Section 3.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.393052339553833}]}, {"text": "In section 4 we presents the result, showing how ROSE correlates well with human judgments on both system and sentence levels.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.7184579372406006}]}, {"text": "Conclusions are given at the end of the paper.", "labels": [], "entities": [{"text": "Conclusions", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9620361328125}]}], "datasetContent": [{"text": "Our experiments test ROSE performance on document level with three different Kernel functions: linear, polynomial and radial basis function.", "labels": [], "entities": []}, {"text": "Then we compare four variants of ROSE with BLEU on both sentence and system (document) level.", "labels": [], "entities": [{"text": "ROSE", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.7361164689064026}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9974817633628845}]}, {"text": "The BLEU version we used here is NIST Open MT Evaluation tool mteval version 13a, smoothing was disabled and except for the sentence level evaluation experiment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9968298077583313}, {"text": "NIST Open MT Evaluation tool mteval version 13a", "start_pos": 33, "end_pos": 80, "type": "DATASET", "confidence": 0.8682157397270203}, {"text": "sentence level evaluation", "start_pos": 124, "end_pos": 149, "type": "TASK", "confidence": 0.652217040459315}]}, {"text": "The system level evaluation procedure follows WMT08, which ranked each system submitted on WMT08 in three types of tasks: \u2022 Rank: Human judges candidate sentence rank in order of quality.", "labels": [], "entities": [{"text": "WMT08", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9457111358642578}, {"text": "WMT08", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.9814582467079163}]}, {"text": "On the document level, documents are ranked according to the proportion of candidate sentences in a document that are better than all of the candidates.", "labels": [], "entities": []}, {"text": "\u2022 Constituent: The constituent task is the same as for ranking but operates over chosen syntactic constituents.", "labels": [], "entities": []}, {"text": "\u2022 Yes/No: WMT08 Yes/No task is to let human judge decide whether the particular part of a sentence is acceptable or not.", "labels": [], "entities": [{"text": "WMT08", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.7785439491271973}]}, {"text": "Document level Yes/No ranks a document according to their number of YES sentences Spearman's rho correlation was used to measure the quality of the metrics on system level.", "labels": [], "entities": [{"text": "YES sentences Spearman's rho correlation", "start_pos": 68, "end_pos": 108, "type": "METRIC", "confidence": 0.5085031390190125}]}, {"text": "Four target languages (English, German, French and Spanish) were used in system level experiments.", "labels": [], "entities": []}, {"text": "ROSE-reg and ROSE-rank were tested in all target language sets, but ROSE-regpos was only tested in the into-English set as it requires a POS tagger.", "labels": [], "entities": [{"text": "ROSE-reg", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9559119343757629}, {"text": "ROSE-rank", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9516119956970215}]}, {"text": "On the sentence level, we compare sentences ranking that ranked by metrics against human ranking.", "labels": [], "entities": []}, {"text": "The evaluation quality was examined by Kendall's tau correlation, and tied results from human judges were excluded.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROSE Features. The first column is the feature  number. The dashed line separates the core features from  the POS extended features.", "labels": [], "entities": []}, {"text": " Table 3: ROSE-reg in with SVM kernel functions", "labels": [], "entities": [{"text": "ROSE-reg", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8672196865081787}]}, {"text": " Table 4: Sentence Level Evaluation", "labels": [], "entities": [{"text": "Sentence Level Evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.9554639657338461}]}, {"text": " Table 5: System Level evaluation that translation into En- glish", "labels": [], "entities": []}]}