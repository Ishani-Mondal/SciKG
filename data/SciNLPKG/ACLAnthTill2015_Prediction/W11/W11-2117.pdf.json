{"title": [{"text": "CMU System Combination in WMT 2011", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.4833942949771881}]}], "abstractContent": [{"text": "This paper describes our submissions, cmu-heafield-combo, to the ten tracks of the 2011 Workshop on Machine Transla-tion's system combination task.", "labels": [], "entities": []}, {"text": "We show how the combination scheme operates by flexibly aligning system outputs then searching a space constructed from the alignments.", "labels": [], "entities": []}, {"text": "Humans judged our combination the best on eight often tracks.", "labels": [], "entities": []}], "introductionContent": [{"text": "We participated in all ten tracks of the 2011 Workshop on Machine Translation system combination task as cmu-heafield-combo.", "labels": [], "entities": [{"text": "Machine Translation system combination task", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.8492638468742371}]}, {"text": "This uses a system combination scheme that builds on our prior work, especially with respect to language modeling and handling nonEnglish languages.", "labels": [], "entities": []}, {"text": "We present a summary of the system, describe improvements, list the data used (all of the constrained monolingual data), and present automatic results in anticipation of human evaluation by the workshop.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Counts of separate or compounded versions of  select words in the lowercased German monolingual data.  Compounding can be optional or biased in either way.", "labels": [], "entities": [{"text": "German monolingual data", "start_pos": 87, "end_pos": 110, "type": "DATASET", "confidence": 0.5776615639527639}, {"text": "Compounding", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.9281871318817139}]}, {"text": " Table 2: Automatic scores for our submissions. For com- parison, the top individual system by BLEU is shown  in the third row of each track. Test data and references  were preprocessed prior to scoring. Metrics are uncased  and METEOR 1.0 uses adequacy-fluency parameters. We  show improvement on all tasks except Haitian Creole- English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9966804385185242}, {"text": "METEOR", "start_pos": 229, "end_pos": 235, "type": "METRIC", "confidence": 0.9657373428344727}]}]}