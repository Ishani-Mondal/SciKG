{"title": [], "abstractContent": [{"text": "This paper presents a progressively challenging series of experiments that investigate clarification subdialogues to resolve the words in noisy transcriptions of user utterances.", "labels": [], "entities": []}, {"text": "We focus on user utterances where the user's specific intent requires little additional inference, given sufficient understanding of the form.", "labels": [], "entities": []}, {"text": "We learned decision-making strategies fora dialogue manager from run-time features of our spoken dialogue system and from observation of human wizards we had embedded within it.", "labels": [], "entities": []}, {"text": "Results show that noisy ASR can be resolved based on predictions from context about what a user might say, and that dialogue management strategies for clarifications of linguistic form benefit from access to features from spoken language understanding.", "labels": [], "entities": [{"text": "ASR", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9342599511146545}]}], "introductionContent": [{"text": "Utterances have literal meaning derived from their linguistic form, and pragmatic intent, the actions speakers aim to achieve through words.", "labels": [], "entities": []}, {"text": "Because the channel is usually not noisy enough to impede communication, misunderstandings that arise between adult human interlocutors are more often due to confusions about intent, rather than about words.", "labels": [], "entities": []}, {"text": "Between humans and machines, however, verbal interaction has a much higher rate of linguistic misunderstandings because the channel is noisy, and machines are not as adept at using spoken language.", "labels": [], "entities": []}, {"text": "It is difficult to arrive at accurate rates for misunderstandings of form versus intent inhuman conversation, because the two types cannot always be distinguished ().", "labels": [], "entities": []}, {"text": "However, one estimate of the rate of misunderstandings of literal meaning between humans, based on text transcripts of the British National Corpus, is in the low range of 4% (), compared with a 30% estimate for human-computer dialogue ().", "labels": [], "entities": [{"text": "misunderstandings of literal meaning between humans", "start_pos": 37, "end_pos": 88, "type": "TASK", "confidence": 0.7074927737315496}, {"text": "British National Corpus", "start_pos": 123, "end_pos": 146, "type": "DATASET", "confidence": 0.9165510733922323}]}, {"text": "The thesis of our work is that misunderstandings of linguistic form in human-machine dialogue are more effectively resolved through greater reliance on context, and through closer integration of spoken language understanding (SLU) with dialogue management (DM).", "labels": [], "entities": [{"text": "dialogue management (DM)", "start_pos": 236, "end_pos": 260, "type": "TASK", "confidence": 0.8472713828086853}]}, {"text": "We investigate these claims by focusing on noisy speech recognition for utterances where the user's specific intent requires little additional inference, given sufficient understanding of the form.", "labels": [], "entities": [{"text": "noisy speech recognition", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.728468676408132}]}, {"text": "This paper presents three experiments that progressively address SLU methods to compensate for poor automated speech recognition (ASR), and complementary DM strategies.", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.8358657836914063}]}, {"text": "In two of the experiments, human wizards are embedded in the spoken dialogue system while run-time SLU features are collected.", "labels": [], "entities": []}, {"text": "Many wizard-of-Oz investigations have addressed the noisy channel issue for SDS.", "labels": [], "entities": [{"text": "SDS", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9503876566886902}]}, {"text": "Like them, we study how human wizards solve the joint problem of interpreting users' words and inferring users' intents.", "labels": [], "entities": []}, {"text": "Our work differs in its exploration of the role context can play in the literal interpretation of noisy language.", "labels": [], "entities": []}, {"text": "We rely on knowledge in the backend database to propose candidate linguistic forms for noisy ASR.", "labels": [], "entities": [{"text": "ASR", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.911310613155365}]}, {"text": "Our principal results are that both wizards and our SDS can achieve high accuracy interpretations, indicating that predictions about what the user might be saying can play a significant role in resolving noise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9780396819114685}]}, {"text": "We show it is possible to achieve low rates of unresolved misunderstanding, even at word error rates (WER) as poor as 50%-70%.", "labels": [], "entities": [{"text": "word error rates (WER)", "start_pos": 84, "end_pos": 106, "type": "METRIC", "confidence": 0.8596602082252502}]}, {"text": "We achieve this through machine learned models of DM actions that combine standard DM features with a rich number and variety of SLU features.", "labels": [], "entities": []}, {"text": "The learned models predict DM actions to determine whether a reliable candidate interpretation exists fora noisy utterance, and if not, what action to take.", "labels": [], "entities": []}, {"text": "The results support an approach to DM design that integrates the two problems of understanding form and intent.", "labels": [], "entities": [{"text": "DM design", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.9865530431270599}]}, {"text": "The next sections present related work, our library domain and our baseline SDS architecture.", "labels": [], "entities": []}, {"text": "Subsequent sections discuss the SLU settings across the three experiments, and present the experimental designs and results, discussion and conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments reported here are an off-line pilot study to identify book titles under worst case recognition (Title Pilot), an embedded WOz study of a single turn exchange involving book requests by title (Turn Exchange), and an embedded WOz study of dialogues where users followed scenarios that included four books at a time (Full WOz).", "labels": [], "entities": []}, {"text": "To evaluate the impact of learned models of wizard actions from the Full WOz wizard data, we evaluated CheckItOut before and after the dialogue manager was enhanced with wizard models for specific actions.", "labels": [], "entities": [{"text": "Full WOz wizard data", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.7657904326915741}]}, {"text": "All three experiments use the full database for search.", "labels": [], "entities": []}, {"text": "To control for WER, the knowledge sources for speech recognition and semantic parsing vary across experiments.", "labels": [], "entities": [{"text": "WER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9353229999542236}, {"text": "speech recognition", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8175168633460999}, {"text": "semantic parsing", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.6979564726352692}]}, {"text": "For each experiment, indicates the acoustic model (AM) used, the number of hours of domain-specific spontaneous speech used for AM adaptation, the number of titles used to construct the language model (LM), the type of LM, the type of grammar rules in the Phoenix book title subgrammar, and average WER as measured by Levenstein word edit distance.", "labels": [], "entities": [{"text": "AM adaptation", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.9429814219474792}, {"text": "Phoenix book title subgrammar", "start_pos": 256, "end_pos": 285, "type": "DATASET", "confidence": 0.9014809280633926}, {"text": "WER", "start_pos": 299, "end_pos": 302, "type": "METRIC", "confidence": 0.9935405850410461}]}, {"text": "For the first two experiments, we used CMU's Open Source WSJ1 dictation AMs for wideband (16kHz) microphone (dictation) speech.", "labels": [], "entities": [{"text": "CMU", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9503386616706848}]}, {"text": "For Full WOz we adapted narrowband (8kHz) WSJ1 dictation speech with about eight hours of data collected from Turn Exchange and two hours of scripted spontaneous speech typical of CheckItOut dialogues.", "labels": [], "entities": [{"text": "Turn Exchange", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.9210592806339264}]}, {"text": "Logios is a CMU toolkit for generating a pseudocorpus from a Phoenix grammar.", "labels": [], "entities": []}, {"text": "It produces a set of strings generated by Phoenix production rules, which in turn are used to build an LM (Carnegie Mellon University Speech.", "labels": [], "entities": []}, {"text": "Before we explain the three rightmost columns in, we first briefly describe Phoenix, the Phoenix book title subgrammar, and how we combine title strings with a Logios pseudo-corpus.", "labels": [], "entities": [{"text": "Phoenix book title subgrammar", "start_pos": 89, "end_pos": 118, "type": "DATASET", "confidence": 0.8966305255889893}]}, {"text": "Phoenix is a context-free grammar (CFG) parser that produces one or more semantic frames per parse.", "labels": [], "entities": [{"text": "context-free grammar (CFG) parser", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.6269685576359431}]}, {"text": "A semantic frame has slots, where each slot is a concept with its own CFG productions (subgrammar).", "labels": [], "entities": []}, {"text": "To accommodate noisy ASR, the parser can skip words between frames or slots.", "labels": [], "entities": [{"text": "ASR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9891357421875}]}, {"text": "Phoenix is wellsuited for restricted domains, where a frame represents a particular type of subdialogue (e.g., ordering a plane ticket), and slots represent constrained concepts (e.g., departure city, destination city).", "labels": [], "entities": []}, {"text": "Phoenix is not well-suited for book titles, which have a rich vocabulary and syntax, and no obvious component slots.", "labels": [], "entities": []}, {"text": "The CFG rules for the Turn Exchange book title subgrammar consisted of a verbatim rule for each book title.", "labels": [], "entities": [{"text": "CFG", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9125999808311462}, {"text": "Turn Exchange book title subgrammar", "start_pos": 22, "end_pos": 57, "type": "DATASET", "confidence": 0.8549924492835999}]}, {"text": "Rules that consisted of a bag-of-words (BOW; i.e., unordered) for each title proved to be too unconstrained.", "labels": [], "entities": [{"text": "BOW", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9926867485046387}]}, {"text": "In Turn Exchange, interpretation of ASR consisted primarily of voice search; the highly constrained CFG rules (exact words inexact order) had little impact on performance.", "labels": [], "entities": [{"text": "interpretation of ASR", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7279862562815348}]}, {"text": "For baseline CheckItOut dialogues, and for Full WOz, we required more constrained grammar rules that would preserve Phoenix's robustness to noise.", "labels": [], "entities": [{"text": "CheckItOut dialogues", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.5792423188686371}]}, {"text": "To avoid the brittleness of exact string CFG rules, and the massive over-generation of BOW CFG rules, we wrote a transducer that mapped dependency parses of book titles to CFG rules.", "labels": [], "entities": [{"text": "mapped dependency parses of book titles", "start_pos": 129, "end_pos": 168, "type": "TASK", "confidence": 0.7567999760309855}]}, {"text": "When ASR words are skipped, book title parses can consist of multiple slots.", "labels": [], "entities": [{"text": "ASR words", "start_pos": 5, "end_pos": 14, "type": "TASK", "confidence": 0.9033956229686737}, {"text": "book title parses", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.6457004050413767}]}, {"text": "We used MICA, a broad-coverage dependency grammar ( consist largely of a first name slot followed by a last name slot.", "labels": [], "entities": []}, {"text": "The remaining portions of the Phoenix CheckItOut grammar consist of subgrammars for book request prefixes and affixes (e.g., \"I would like the book called\"), for confirmations and rejections, phone numbers, book catalogue numbers, and miscellaneous additional concepts.", "labels": [], "entities": [{"text": "Phoenix CheckItOut grammar", "start_pos": 30, "end_pos": 56, "type": "DATASET", "confidence": 0.8915607134501139}, {"text": "confirmations and rejections", "start_pos": 162, "end_pos": 190, "type": "TASK", "confidence": 0.8442973097165426}]}, {"text": "The set of subgrammars excluding the book title and author subgrammars (book requests, confirmations, and soon; the grammar shell) are the same for all experiments.", "labels": [], "entities": []}, {"text": "The MICA-based book title grammar also provides several features (e.g., number of slots in a parse) for machine learning.", "labels": [], "entities": [{"text": "MICA-based book title grammar", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.6627937704324722}, {"text": "machine learning", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.7752508223056793}]}, {"text": "The Title Pilot LM consisted of unigram frequencies of the 1400 word types from a random sample (without replacement) of 500 titles.", "labels": [], "entities": [{"text": "Title Pilot LM", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8124079505602518}]}, {"text": "For Turn Exchange, a trigram LM was constructed from 7,500 titles randomly selected from the 19,708 titles that remained after we eliminated one-word titles and titles with below average circulation.", "labels": [], "entities": [{"text": "Turn Exchange", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8171960115432739}]}, {"text": "For Full WOz, 3,000 books were randomly selected from the full book database (with no more than three titles by the same author, and no one-word titles).", "labels": [], "entities": [{"text": "Full WOz", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.6705188751220703}]}, {"text": "Logios was used on the grammar shell to generate an initial pseudo-corpus, which was combined with the book title and author strings to generate a full pseudocorpus for the trigram LM (denoted as \"Logios + book data\" in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SLU settings across experiments", "labels": [], "entities": []}, {"text": " Table 2: Performance of learned trees", "labels": [], "entities": []}]}