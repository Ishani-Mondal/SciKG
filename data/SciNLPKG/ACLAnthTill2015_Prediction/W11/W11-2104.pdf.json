{"title": [{"text": "Evaluate with Confidence Estimation: Machine ranking of translation outputs using grammatical features", "labels": [], "entities": [{"text": "Machine ranking of translation outputs", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.7617964267730712}]}], "abstractContent": [{"text": "We present a pilot study on an evaluation method which is able to rank translation outputs with no reference translation, given only their source sentence.", "labels": [], "entities": []}, {"text": "The system employs a statistical classifier trained upon existing human rankings, using several features derived from analysis of both the source and the target sentences.", "labels": [], "entities": []}, {"text": "Development experiments on one language pair showed that the method has considerably good correlation with human ranking when using features obtained from a PCFG parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic evaluation metrics for Machine Translation (MT) have mainly relied on analyzing both the MT output against (one or more) reference translations.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8656264185905457}]}, {"text": "Though, several paradigms in Machine Translation Research pose the need to estimate the quality through many translation outputs, when no reference translation is given (n-best rescoring of SMT systems, system combination etc.).", "labels": [], "entities": [{"text": "Machine Translation Research", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8881293932596842}, {"text": "SMT", "start_pos": 190, "end_pos": 193, "type": "TASK", "confidence": 0.9759544730186462}]}, {"text": "Such metrics have been known as Confidence Estimation metrics and quite a few projects have suggested solutions on this direction.", "labels": [], "entities": [{"text": "Confidence Estimation", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8191192150115967}]}, {"text": "With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 135, "end_pos": 137, "type": "TASK", "confidence": 0.9096481800079346}]}, {"text": "Our approach suggests building a Confidence Estimation metric using already existing human judgments.", "labels": [], "entities": [{"text": "Confidence Estimation", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.9084323346614838}]}, {"text": "This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 183, "end_pos": 220, "type": "TASK", "confidence": 0.7843203544616699}]}, {"text": "This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators' choices.", "labels": [], "entities": []}, {"text": "Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in).", "labels": [], "entities": []}, {"text": "Our proposition is similar, but works without reference translations.", "labels": [], "entities": []}, {"text": "We develop a solution of applying machine learning in order to build a statistical classifier that performs similar to the human ranking: it is trained to rank several MT outputs, given analysis of possible qualitative criteria on both the source and the target side of every given sentence.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 168, "end_pos": 178, "type": "TASK", "confidence": 0.8774586021900177}]}, {"text": "As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output.", "labels": [], "entities": []}], "datasetContent": [{"text": "A basic experiment was designed in order to determine the exact setup and the feature set of the metric prior to the shared task submission.", "labels": [], "entities": []}, {"text": "The classifiers for the task were learnt using the German-English testset of the WMT 2008 and 2010 (about 700 sentences) . For testing, the classifiers were used to perform ranking on a test set of 184 sentences which had been kept apart from the 2010 data, with the criterion that they do not contain contradictions among human judgments.", "labels": [], "entities": [{"text": "German-English testset of the WMT 2008", "start_pos": 51, "end_pos": 89, "type": "DATASET", "confidence": 0.8344153662522634}]}, {"text": "In order to allow further comparison with other evaluation metrics, we performed an extended experiment: we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement.", "labels": [], "entities": [{"text": "WMT 2008 and 2009 data", "start_pos": 141, "end_pos": 163, "type": "DATASET", "confidence": 0.940127718448639}, {"text": "WMT 2010 test set", "start_pos": 215, "end_pos": 232, "type": "DATASET", "confidence": 0.9336047172546387}]}, {"text": "In both experiments, tokenization was performed with the PUNKT tokenizer (; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit).", "labels": [], "entities": [{"text": "tokenization", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9788124561309814}]}, {"text": "The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set.", "labels": [], "entities": [{"text": "WMT 2011 Shared Task", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.584164172410965}, {"text": "2007 test set", "start_pos": 153, "end_pos": 166, "type": "DATASET", "confidence": 0.7636440197626749}]}, {"text": "As a PCFG parser, the Berkeley Parser () was preferred, due 1 data acquired from http://www.statmt.org/wmt11 to the possibility of easily obtaining complex internal statistics, including n-best trees.", "labels": [], "entities": []}, {"text": "Unfortunately, the time required for parsing leads to significant delays at the overall processing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9863578081130981}]}, {"text": "The machine learning algorithms were implemented with the Orange toolkit).", "labels": [], "entities": [{"text": "Orange toolkit", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.9430968761444092}]}], "tableCaptions": [{"text": " Table 1: System-level Spearman's rho and segment-level Kendall's tau correlation coefficients achieved on automatic  ranking (average absolute value)", "labels": [], "entities": [{"text": "segment-level Kendall's tau correlation", "start_pos": 42, "end_pos": 81, "type": "METRIC", "confidence": 0.47286252975463866}]}]}