{"title": [{"text": "Analysis of the Hindi Proposition Bank using Dependency Structure", "labels": [], "entities": [{"text": "Hindi Proposition Bank", "start_pos": 16, "end_pos": 38, "type": "DATASET", "confidence": 0.949098269144694}]}], "abstractContent": [{"text": "This paper makes two contributions.", "labels": [], "entities": []}, {"text": "First, we describe the Hindi Proposition Bank that contains annotations of predicate argument structures of verb predicates.", "labels": [], "entities": [{"text": "Hindi Proposition Bank", "start_pos": 23, "end_pos": 45, "type": "DATASET", "confidence": 0.8746834595998129}]}, {"text": "Unlike PropBanks inmost other languages, the Hind PropBank is annotated on top of dependency structure, the Hindi Dependency Treebank.", "labels": [], "entities": [{"text": "Hind PropBank", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9120336174964905}, {"text": "Hindi Dependency Treebank", "start_pos": 108, "end_pos": 133, "type": "DATASET", "confidence": 0.8923300306002299}]}, {"text": "We explore the similarities between dependency and predicate argument structures, so the PropBank annotation can be faster and more accurate.", "labels": [], "entities": []}, {"text": "Second , we present a probabilistic rule-based system that maps syntactic dependents to semantic arguments.", "labels": [], "entities": []}, {"text": "With simple rules, we classify about 47% of the entire PropBank arguments with over 90% confidence.", "labels": [], "entities": [{"text": "PropBank arguments", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.9188016653060913}]}, {"text": "These preliminary results are promising; they show how well these two frameworks are correlated.", "labels": [], "entities": []}, {"text": "This can also be used to speedup our annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Proposition Bank (from now on, PropBank) is a corpus in which the arguments of each verb predicate are annotated with their semantic roles).", "labels": [], "entities": []}, {"text": "PropBank annotation has been carried out in several languages; most of them are annotated on top of Penn Treebank style phrase structure.", "labels": [], "entities": [{"text": "Penn Treebank style phrase structure", "start_pos": 100, "end_pos": 136, "type": "DATASET", "confidence": 0.9501924395561219}]}, {"text": "However, a different grammatical analysis has been used for the Hindi PropBank annotation, dependency structure, which maybe particularly suited for the analysis of flexible word order languages such as Hindi.", "labels": [], "entities": [{"text": "Hindi PropBank annotation", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.8016747037569681}]}, {"text": "As a syntactic corpus, we use the Hindi Dependency.", "labels": [], "entities": [{"text": "Hindi Dependency", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9555816054344177}]}, {"text": "Using dependency structure has some advantages.", "labels": [], "entities": []}, {"text": "First, semantic arguments can be marked explicitly on the syntactic trees, so annotations of the predicate argument structure can be more consistent with the dependency structure.", "labels": [], "entities": []}, {"text": "Second, the Hindi Dependency Treebank provides a rich set of dependency relations that capture the syntactic-semantic information.", "labels": [], "entities": [{"text": "Hindi Dependency Treebank", "start_pos": 12, "end_pos": 37, "type": "DATASET", "confidence": 0.8874831597010294}]}, {"text": "This facilitates mappings between syntactic dependents and semantic arguments.", "labels": [], "entities": []}, {"text": "A successful mapping would reduce the annotation effort, improve the inter-annotator agreement, and guide a full fledged semantic role labeling task.", "labels": [], "entities": [{"text": "semantic role labeling task", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.6968418508768082}]}, {"text": "In this paper, we briefly describe our annotation work on the Hindi PropBank, and suggest mappings between syntactic and semantic arguments based on linguistic intuitions.", "labels": [], "entities": [{"text": "Hindi PropBank", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.90754234790802}]}, {"text": "We also present a probabilistic rule-based system that uses three types of rules to arrive at mappings between syntactic and semantic arguments.", "labels": [], "entities": []}, {"text": "Our experiments show some promising results; these mappings illustrate how well those two frameworks are correlated, and can also be used to speedup the PropBank annotation.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 153, "end_pos": 161, "type": "DATASET", "confidence": 0.9057615995407104}]}], "datasetContent": [{"text": "First, we evaluate how well our deterministic rule classifies the ARGM-PRX label.", "labels": [], "entities": []}, {"text": "Using the deterministic rule, we get a 94.46% precision and a 100% recall on ARGM-PRX.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9988190531730652}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9973686933517456}, {"text": "ARGM-PRX", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9561001062393188}]}, {"text": "The 100% recall is expected; the precision implies that about 5.5% of the time, light verb annotations in the HPB do not agree with the complex predicate annotations (pof relation) in the HDT (cf. Section 3.3).", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9932603240013123}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995192289352417}]}, {"text": "More analysis needs to be done to improve the precision of this rule.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.999561607837677}]}, {"text": "Next, we evaluate our empirically-derived rules with respect to the different thresholds set for P (pbrel i ).", "labels": [], "entities": []}, {"text": "In general, the higher the threshold is, the higher and lower the precision and recall become, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9995877146720886}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9990377426147461}]}, {"text": "shows comparisons between precision and recall with respect to different thresholds.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9994069337844849}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9990239143371582}]}, {"text": "Notice that a threshold of 1.0, meaning that using only rules with 100% confidence, does not give the highest precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.998913049697876}]}, {"text": "This is because the model with this high of a threshold overfits to the training data.", "labels": [], "entities": []}, {"text": "Rules that work well in the training data do not necessarily work as well on the test data.", "labels": [], "entities": []}, {"text": "We need to find a threshold that gives a high precision (so annotators do not get confused by the automatic output) while maintaining a good recall (so annotations can go faster).", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9979404807090759}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9988911747932434}]}, {"text": "With a threshold of 0.93 using features (lemma, voice, dependency label), we get a precision of 90.37%, a recall of 44.52%, and an F1-score of 59.65%.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9996782541275024}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9995658993721008}, {"text": "F1-score", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9997989535331726}]}, {"text": "shows accuracies for all PropBank labels achieved by a threshold of 0.92 using roleset ID's instead of predicate's lemmas.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9963158965110779}, {"text": "PropBank", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8793228268623352}]}, {"text": "Although the overall precision stays about the same, we get a noticeable improvement in the overall recall using roleset ID's.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9992330074310303}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9986616373062134}]}, {"text": "Note that some labels are missing in.", "labels": [], "entities": []}, {"text": "This is because either they do not occur in our current data (ARGC and ARGA) or we have not started annotating them properly yet (ARGM-MOD and ARGM-NEG).", "labels": [], "entities": [{"text": "ARGC", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9654372334480286}, {"text": "ARGA", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.7255079746246338}, {"text": "ARGM-MOD", "start_pos": 130, "end_pos": 138, "type": "DATASET", "confidence": 0.9117748141288757}, {"text": "ARGM-NEG", "start_pos": 143, "end_pos": 151, "type": "DATASET", "confidence": 0.8653872609138489}]}, {"text": "Finally, we evaluate the impact of the linguisticallymotivated rules.", "labels": [], "entities": []}, {"text": "shows accuracies achieved by the linguistically motivated rules applied after the empirically derived rules.", "labels": [], "entities": []}, {"text": "As expected, the linguistically motivated rules improve the recall of ARGN significantly, but bring a slight decrease in the precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9991406202316284}, {"text": "ARGN", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.42518821358680725}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.999006450176239}]}, {"text": "This shows that our linguistic intuitions are generally on the right track.", "labels": [], "entities": []}, {"text": "We may combine some of the empirically derived rules with linguistically motivated rules together in the frameset files so annotators can take advantage of both kinds of rules in the future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 7: Labeling accuracies achieved by the empirically de-", "labels": [], "entities": [{"text": "Labeling", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9620649814605713}]}, {"text": " Table 8: Labeling accuracies achieved by the linguistically", "labels": [], "entities": [{"text": "Labeling", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.95795077085495}]}]}