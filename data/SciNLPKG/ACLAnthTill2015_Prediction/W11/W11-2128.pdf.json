{"title": [{"text": "Filtering Antonymous, Trend-Contrasting, and Polarity-Dissimilar Distributional Paraphrases for Improving Statistical Machine Translation", "labels": [], "entities": [{"text": "Improving Statistical Machine Translation", "start_pos": 96, "end_pos": 137, "type": "TASK", "confidence": 0.8298041969537735}]}], "abstractContent": [{"text": "Paraphrases are useful for statistical machine translation (SMT) and natural language processing tasks.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.8169115682442983}]}, {"text": "Distributional paraphrase generation is independent of parallel texts and syntactic parses, and hence is suitable also for resource-poor languages, but tends to erroneously rank antonyms, trend-contrasting, and polarity-dissimilar candidates as good paraphrases.", "labels": [], "entities": [{"text": "Distributional paraphrase generation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7338333527247111}]}, {"text": "We present here a novel method for improving distributional paraphrasing by filtering out such candidates.", "labels": [], "entities": [{"text": "distributional paraphrasing", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6942671239376068}]}, {"text": "We evaluate it in simulated low and mid-resourced SMT tasks, translating from English to two quite different languages.", "labels": [], "entities": [{"text": "SMT tasks", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.9108469188213348}]}, {"text": "We show statistically significant gains in English-to-Chinese translation quality, up to 1 BLEU from non-filtered paraphrase-augmented models (1.6 BLEU from baseline).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9986156225204468}, {"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9912671446800232}]}, {"text": "We also show that yielding gains in translation to Arabic, a morphologically rich language, is not straightforward .", "labels": [], "entities": [{"text": "translation", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9724430441856384}]}], "introductionContent": [{"text": "Paraphrase recognition and generation has proven useful for various natural language processing (NLP) tasks, including statistical machine translation (SMT), information retrieval, query expansion, document summarization, and natural language generation.", "labels": [], "entities": [{"text": "Paraphrase recognition and generation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8539664447307587}, {"text": "statistical machine translation (SMT)", "start_pos": 119, "end_pos": 156, "type": "TASK", "confidence": 0.7940976619720459}, {"text": "information retrieval", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.8184753656387329}, {"text": "query expansion", "start_pos": 181, "end_pos": 196, "type": "TASK", "confidence": 0.7180276811122894}, {"text": "document summarization", "start_pos": 198, "end_pos": 220, "type": "TASK", "confidence": 0.6069532334804535}, {"text": "natural language generation", "start_pos": 226, "end_pos": 253, "type": "TASK", "confidence": 0.7019546627998352}]}, {"text": "We concentrate hereon phrase-level (as opposed to sentence-level) paraphrasing for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9919582605361938}]}, {"text": "Paraphrasing is useful for SMT as it increases translation coverage -a persistent problem, even in largescale systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9952552318572998}, {"text": "translation coverage", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.6265588700771332}]}, {"text": "Two common approaches are \"pivot\" and distributional paraphrasing.", "labels": [], "entities": []}, {"text": "Pivot paraphrasing translates phrases of interest to other languages and back.", "labels": [], "entities": []}, {"text": "It relies on parallel texts (or translation phrase tables) in various languages, which are typically scarce, and hence limit its applicability.", "labels": [], "entities": []}, {"text": "Distributional paraphrasing) generates paraphrases using a distributional semantic distance measure computed over a large monolingual corpus.", "labels": [], "entities": []}, {"text": "Monolingual corpora are relatively easy and inexpensive to collect, but distributional semantic distance measures are known to rank antonymous and polarity-dissimilar phrasal candidates high.", "labels": [], "entities": []}, {"text": "We therefore attempt to identify and filter out such illsuited paraphrase candidates.", "labels": [], "entities": []}, {"text": "A phrase pair may have a varying degree of antonymy, beyond the better-known complete opposites (hot / cold) and contradictions (did / did not), e.g., weaker contrasts (hot / cool), contrasting trends (covered / reduced coverage), or sentiment polarity (happy / sad).", "labels": [], "entities": []}, {"text": "Information extraction, opinion mining and sentiment analysis literature has been grappling with identifying such pairs), e.g., in order to distinguish positive and negative reviews or comments, or to detect contradictions.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.756097823381424}, {"text": "opinion mining", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7944997847080231}, {"text": "sentiment analysis", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9184825420379639}]}, {"text": "We transfer some of the insights, data and techniques to the area of paraphrasing and SMT.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 69, "end_pos": 81, "type": "TASK", "confidence": 0.9693720936775208}, {"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9829043745994568}]}, {"text": "We distributionally expand a small seed set of antonyms in an unsupervised manner, following.", "labels": [], "entities": []}, {"text": "We then present a method for filtering antonymous and polarity-dissimilar distributional paraphrases using the expanded antonymous list and a list of negators (e.g., cannot) and trenddecreasing words (reduced).", "labels": [], "entities": []}, {"text": "We evaluate the impact of our approach in a SMT setting, where non-baseline translation models are augmented with distributional paraphrases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9948090314865112}]}, {"text": "We show gains of up to 1 BLEU relative to non-filtered models (1.6 BLEU from non-augmented baselines) in English-Chinese models trained on small and medium-large size data, but lower to no gains in English-Arabic.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9980119466781616}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9883716702461243}]}, {"text": "The small training size simulates resource-poor languages.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: We describe distributional paraphrase generation in Section 2, antonym discovery in Section 3, and paraphrase-augmented SMT in Section 4.", "labels": [], "entities": [{"text": "distributional paraphrase generation", "start_pos": 60, "end_pos": 96, "type": "TASK", "confidence": 0.6408050855000814}, {"text": "antonym discovery", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.6491458564996719}, {"text": "paraphrase-augmented SMT", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.5878131091594696}]}, {"text": "We then report experimental results in Section 5, and discuss the implications in Section 6.", "labels": [], "entities": []}, {"text": "We survey related work in Section 7, and conclude with future work in Section 8.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1.  Augmenting SMT phrase tables with paraphrases of  OOV unigrams resulted in gains of 0.6-0.7 BLEU  points for both subset and full models, but TER  scores were worse (higher) for the full model. Aug- menting same models with same paraphrases filtered  for antonyms resulted in further gains of 1.6 and 1  BLEU points for both subset and full models, respec- tively, relative to the respective baselines. The TER  scores of the antonym filtered models were also as  good or better (lower) than those of the baselines.", "labels": [], "entities": [{"text": "SMT phrase", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.8617981374263763}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9965627789497375}, {"text": "TER", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9960592985153198}, {"text": "BLEU", "start_pos": 315, "end_pos": 319, "type": "METRIC", "confidence": 0.9967730641365051}, {"text": "TER", "start_pos": 418, "end_pos": 421, "type": "METRIC", "confidence": 0.9880046248435974}]}, {"text": " Table 1: English-Chinese scores. B/D = statistically significant", "labels": [], "entities": [{"text": "B/D", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8961319128672282}]}, {"text": " Table 2. On the MT05 test set, the 135k-sentence  aug-1gram model outperformed its baseline in both  BLEU and TER scores. The lemmatized variants  of the scores showed higher or same gains. Since", "labels": [], "entities": [{"text": "MT05 test set", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9088222980499268}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9988951086997986}, {"text": "TER", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9775822758674622}]}, {"text": " Table 2: English-Arabic translation scores and analysis for NIST MT05 and MEDAR test sets. B = statistically significant w.r.t.", "labels": [], "entities": [{"text": "NIST MT05", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.8229592144489288}, {"text": "MEDAR test sets", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.8807903726895651}, {"text": "B", "start_pos": 92, "end_pos": 93, "type": "METRIC", "confidence": 0.9932988882064819}]}]}