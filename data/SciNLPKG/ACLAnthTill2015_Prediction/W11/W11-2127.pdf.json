{"title": [{"text": "Fuzzy Syntactic Reordering for Phrase-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Fuzzy Syntactic Reordering", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8842996756235758}, {"text": "Phrase-based Statistical Machine Translation", "start_pos": 31, "end_pos": 75, "type": "TASK", "confidence": 0.7261374294757843}]}], "abstractContent": [{"text": "The quality of Arabic-English statistical machine translation often suffers as a result of standard phrase-based SMT systems' inability to perform long-range re-orderings, specifically those needed to translate VSO-ordered Arabic sentences.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6286708017190298}, {"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.8465836048126221}]}, {"text": "This problem is further exacerbated by the low performance of Arabic parsers on subject and subject span detection.", "labels": [], "entities": [{"text": "subject span detection", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.6008370618025461}]}, {"text": "In this paper, we present two parse \"fuzzi-fication\" techniques which allow the translation system to select among a range of possible S-V re-orderings.", "labels": [], "entities": []}, {"text": "With this approach, we demonstrate a 0.3-point improvement in BLEU score (69% of the maximum possible using gold parses), and a corresponding improvement in the percentage of syntactically well-formed subjects under a manual evaluation .", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9825185537338257}]}], "introductionContent": [{"text": "The question of how to effectively use phrase-based statistical machine translation (PSMT) to translate between language pairs which require long-range reordering has attracted a great deal of interest in recent years.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PSMT)", "start_pos": 39, "end_pos": 90, "type": "TASK", "confidence": 0.7071719595364162}]}, {"text": "The inability to capture long-range reordering behaviors is a weakness inherent in PSMT systems, which typically have only two mechanisms to control the reordering between source and target language: (1) distortion penalties, which penalize or forbid long-distance re-orderings in order to reduce the search space explored by the decoder, and (2) lexicalized reordering models, which capture the preferences of individual phrases to orient themselves monotonically, reversed with their preceding phrases or discontinuously.", "labels": [], "entities": []}, {"text": "Because both of these mechanisms work at the phrase level, they have proven very effective at capturing short-range reordering behaviors, but unable to describe long range movements; in fact, the distortion penalty effectively causes the translation system to not prefer long-range re-orderings, even when they are assigned significantly higher probability by the language model.", "labels": [], "entities": []}, {"text": "The problem is particularly acute in translating from Arabic to English: Arabic sentences frequently exhibit a VSO ordering (both VSO and SVO are permitted in Arabic), while English permits only an SVO order.", "labels": [], "entities": []}, {"text": "Past research has shown that verb anticipation and subject-span detection is a major source of error when translating from Arabic to).", "labels": [], "entities": [{"text": "verb anticipation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7406652271747589}, {"text": "subject-span detection", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.6741831004619598}]}, {"text": "Unable to perform long-range reordering, PSMT frequently produces English sentences in which verbs precede their subjects (sometimes with \"hallucinated\" pronouns in front of them) or do not appear at all.", "labels": [], "entities": [{"text": "PSMT", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.9618518352508545}]}, {"text": "Intuitively, better handling of these reorderings has the potential to improve both accuracy and fluency of translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9989268183708191}]}, {"text": "In this paper, we present two parse fuzzification techniques which allow the translation system to select among a range of possible S-V re-orderings.", "labels": [], "entities": []}, {"text": "With this approach, we demonstrate a 0.3-point improvement in BLEU score (69% of the maximum possible using gold parses), and a corresponding improvement in the percentage of syntactically wellformed subjects under a manual evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9827096462249756}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a review of research on this topic.", "labels": [], "entities": []}, {"text": "Section 3 motivates the approach discussed in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents the results of a set of machine translation experiments using the automatic metrics BLEU () and METEOR, and a manual-evaluation of subject integrity.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7679945230484009}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9358734488487244}, {"text": "METEOR", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9924331903457642}]}, {"text": "Section 6 discusses our conclusions and future plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the open-source Moses PSMT toolkit (.", "labels": [], "entities": []}, {"text": "Training data was a newswire (MSA-English) parallel text with 12M words on the Arabic side (LDC2007E103) 5 Sentences were reordered only for alignment, following the approach of.", "labels": [], "entities": [{"text": "alignment", "start_pos": 141, "end_pos": 150, "type": "TASK", "confidence": 0.9786983728408813}]}, {"text": "Parses were obtained using a publicly available parser for Arabic ( . GIZA++ was used for word alignment and phrase translations of up to 10 words are extracted in the Moses phrase table.", "labels": [], "entities": [{"text": "Parses", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8671846985816956}, {"text": "word alignment", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7859592139720917}]}, {"text": "The same baseline phrase table was used in all experiments.", "labels": [], "entities": []}, {"text": "The system's language model was trained both on the English portion of the training corpus and English Gigaword (.", "labels": [], "entities": []}, {"text": "We used a 5-gram language model with modified Kneser-Ney smoothing implemented using the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.8296308815479279}]}, {"text": "Feature weights were tuned with MERT to maximize BLEU on the NIST MT06 corpus.", "labels": [], "entities": [{"text": "MERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9957624077796936}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9995138645172119}, {"text": "NIST MT06 corpus", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8779450257619222}]}, {"text": "MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability.", "labels": [], "entities": [{"text": "MERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9479639530181885}, {"text": "MERT", "start_pos": 122, "end_pos": 126, "type": "TASK", "confidence": 0.7455717325210571}]}, {"text": "In the future, we plan to experiment with approachspecific optimization and to use recent published suggestions on controlling for optimizer instability).", "labels": [], "entities": [{"text": "approachspecific optimization", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7069900184869766}]}, {"text": "English data was tokenized using simple punctuation-based rules.", "labels": [], "entities": []}, {"text": "Arabic data was segmented with to the Arabic Treebank tokenization scheme () using the MADA+TOKAN morphological disambiguator and tokenizer).", "labels": [], "entities": [{"text": "Arabic Treebank", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.8410147428512573}]}, {"text": "The Arabic text was also Alif/Ya normalized.", "labels": [], "entities": []}, {"text": "MADAproduced Arabic lemmas were used for word alignment.", "labels": [], "entities": [{"text": "MADAproduced Arabic lemmas", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.8367975552876791}, {"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8147684037685394}]}, {"text": "We compare four settings with predicted parses (as opposed to the gold parse experiments discussed in Section 3): \u2022 BASE An un-reordered test set; \u2022 FORCE A test set which forced reordering on matrix verbs; \u2022 OPT A test set with fuzzification through optional reordering on matrix verbs; and \u2022 SPAN A test set with fuzzification through optional reordering on matrix verbs and through fuzzification of the subject span according to the algorithm shown in Section 4.2.", "labels": [], "entities": [{"text": "BASE", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.997017502784729}, {"text": "FORCE", "start_pos": 149, "end_pos": 154, "type": "METRIC", "confidence": 0.9973657727241516}]}, {"text": "Each reordering corpus used Moses' lattice input format () (including the baselines, which had only one path).", "labels": [], "entities": []}, {"text": "Results are presented in terms of the standard BLEU metric (), METEOR metric (Banerjee and) and a manual evaluation targeting subject span translation correctness.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.972949206829071}, {"text": "METEOR metric", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9800560176372528}, {"text": "subject span translation correctness", "start_pos": 126, "end_pos": 162, "type": "TASK", "confidence": 0.5902262032032013}]}, {"text": "Both OPT and SPAN showed a statistically significant improvement in BLEU score over BASE and FORCE above the 95% level.", "labels": [], "entities": [{"text": "OPT", "start_pos": 5, "end_pos": 8, "type": "DATASET", "confidence": 0.8865088820457458}, {"text": "SPAN", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.7157066464424133}, {"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9663747549057007}, {"text": "BASE", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9883118271827698}, {"text": "FORCE", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9970308542251587}]}, {"text": "Statistical significance is computed using paired bootstrap resampling.", "labels": [], "entities": []}, {"text": "The difference between OPT and SPAN, however, was not statistically significant.", "labels": [], "entities": [{"text": "OPT", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8296461701393127}]}, {"text": "The relatively small difference in BLEU score between the baseline and gold reordering (Section 3: baseline 47.13 and optional reordering 47.55) suggests that we should expect at most a modest increase in BLEU from improving the predicted trees.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9783215522766113}, {"text": "BLEU", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.9992331266403198}]}, {"text": "The first key observation in these results is that with a noisy parser, translation quality actually goes down with forced reordering-the opposite of what was observed in the gold experiment.", "labels": [], "entities": [{"text": "translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9314188957214355}]}, {"text": "By introducing either optional reordering or complete fuzzification, however, BLEU score increases .3 past the baseline to achieve nearly three quarters of the gain obtained by optional reordering using the gold parse (Section 3: baseline 47.13 and optional reordering 47.55).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9780114889144897}]}, {"text": "In other words, it is possible to compensate for the parser noisiness without actually attempting to correct spans: simply allowing the translation system to fallback on an un-reordered input leads to a significant gain in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 223, "end_pos": 227, "type": "METRIC", "confidence": 0.9974156618118286}]}, {"text": "One possible explanation for this fact is that we only ever correct for parses on the right-hand sidethe left sides are virtually always correct.", "labels": [], "entities": []}, {"text": "Thus, when we perform any reordering, even if the subject span is not entirely perfect, we guarantee that we bring at least one word from the sentence (and usually more) into alignment where it was out of alignment before; this obviously leads to better BLEU n-gram scores along that boundary.", "labels": [], "entities": [{"text": "BLEU n-gram scores", "start_pos": 254, "end_pos": 272, "type": "METRIC", "confidence": 0.928748607635498}]}, {"text": "The general trend in these results is confirmed by the results of a METEOR analysis, also provided in Tab.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9396876096725464}, {"text": "Tab.", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.9762479066848755}]}, {"text": "2. Again, both the OPT and SPAN systems result exhibit comparable performance, and demonstrate an improvement over the baseline.", "labels": [], "entities": [{"text": "OPT", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.7842189073562622}]}, {"text": "The second observation is that introducing span fuzzification did not improve oversimple optional reordering.", "labels": [], "entities": []}, {"text": "There area several reasons this could be happening: \u2022 The increased fluency and introduction of unseen phrases cancel each other out.", "labels": [], "entities": []}, {"text": "\u2022 All the gains that come from reordering occur at the left; the presence or absence of correct words at the right end is less important.", "labels": [], "entities": []}, {"text": "\u2022 Better sentences are proposed during the translation process, but they are not selected during the final filtering stage.", "labels": [], "entities": [{"text": "translation process", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.9044977724552155}]}, {"text": "\u2022 The sentences being output are actually better, but the improvement is not captured by the automatic evaluation.", "labels": [], "entities": []}, {"text": "Further experiments will be necessary to determine whether any of the first three possibilities is the case.", "labels": [], "entities": []}, {"text": "We next consider the fourth possibility in more detail.", "labels": [], "entities": []}, {"text": "We additionally conducted a manual evaluation to examine how subject quality differed in fuzzified vs. unfuzzified parses.", "labels": [], "entities": []}, {"text": "Each sentence examined was assigned one of the six labels below.", "labels": [], "entities": []}, {"text": "Examples are with respect to the reference sentence \"Recep Tayyip Erdogan announced that Turkey is strong.\"", "labels": [], "entities": []}, {"text": "\u2022 MM: both verb and subject missing.", "labels": [], "entities": []}, {"text": "\"Turkey is strong.\"", "labels": [], "entities": [{"text": "Turkey", "start_pos": 1, "end_pos": 7, "type": "DATASET", "confidence": 0.953423261642456}]}, {"text": "\u2022 MV: verb missing.", "labels": [], "entities": [{"text": "MV", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9237624406814575}, {"text": "verb missing", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.6093907356262207}]}, {"text": "\"Recep Tayyip Erdogan Turkey is strong.\"", "labels": [], "entities": [{"text": "Recep Tayyip Erdogan Turkey", "start_pos": 1, "end_pos": 28, "type": "DATASET", "confidence": 0.7735124826431274}]}, {"text": "\u2022 MS: subject missing.", "labels": [], "entities": []}, {"text": "\"announced that Turkey is strong.\"", "labels": [], "entities": [{"text": "Turkey", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8089563250541687}]}, {"text": "\u2022 SO: subject overlaps with verb.", "labels": [], "entities": []}, {"text": "\"Recep announced Tayyip Erdogan Turkey is strong.\"", "labels": [], "entities": [{"text": "Recep announced Tayyip Erdogan Turkey", "start_pos": 1, "end_pos": 38, "type": "DATASET", "confidence": 0.8618361115455627}]}, {"text": "\u2022 SI: verb precedes subject (as in Arabic).", "labels": [], "entities": []}, {"text": "\"announced Recep Tayyip Erdogan that Turkey is strong.\"", "labels": [], "entities": []}, {"text": "\u2022 C: verb follows subject (as in English), i.e. the correct ordering.", "labels": [], "entities": []}, {"text": "\"Recep Tayyip Erdogan announced that Turkey is strong.\"", "labels": [], "entities": []}, {"text": "We also include in this category sentences where the English reference contains no verb (e.g. in newspaper headlines).", "labels": [], "entities": []}, {"text": "System MM MS MV SI SO C M* S* C BASE 8: Subject integrity analysis results.", "labels": [], "entities": [{"text": "BASE 8", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.963172972202301}, {"text": "Subject integrity analysis", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.8191331624984741}]}, {"text": "All numbers are %s.", "labels": [], "entities": []}, {"text": "By grouping some of these categories together, we obtained the following label scheme: \u2022 M*: MM, MV or MS, i.e. verb or subject is missing.", "labels": [], "entities": []}, {"text": "\u2022 S*: SO or SI, i.e. word order is incorrect.", "labels": [], "entities": []}, {"text": "\u2022 C: as above.", "labels": [], "entities": []}, {"text": "280 sentences selected randomly from our test set were evaluated, generating 461 unique output sentences.", "labels": [], "entities": []}, {"text": "Annotation was performed by two English speakers, with 40 input sentences (68 unique outputs) annotated by both authors to collect agreement statistics.", "labels": [], "entities": []}, {"text": "For the complete label scheme, the annotators agreed on 86.8% of labels, with Cohen's \u03ba = 0.811.", "labels": [], "entities": []}, {"text": "For the simple label scheme, the annotators agreed on 92.6% of labels, with \u03ba = .883.", "labels": [], "entities": []}, {"text": "Results for the BASE, OPT and SPAN systems are shown in.", "labels": [], "entities": [{"text": "BASE", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.47447070479393005}, {"text": "OPT", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.7028369307518005}]}, {"text": "Each annotator's labels were assigned a weight of .5 in the section that was jointly annotated.", "labels": [], "entities": []}, {"text": "Again, both the OPT and SPAN systems display statistically significant improvements over the baseline system (p < 0.001).", "labels": [], "entities": [{"text": "OPT", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8627380132675171}]}, {"text": "While the SPAN system consistently displays better results than the OPT system, the significance is low (p < .3).", "labels": [], "entities": [{"text": "SPAN", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.7435047030448914}, {"text": "significance", "start_pos": 84, "end_pos": 96, "type": "METRIC", "confidence": 0.9957091808319092}]}, {"text": "Statistical significance was measured using the McNemar test of statistical significance.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.702714741230011}]}, {"text": "These results thus agree with the BLEU score in indicating that the OPT and SPAN systems are substantially better than the baseline, but statistically indistinguishable from each other.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9992949962615967}, {"text": "OPT", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.8824455142021179}]}, {"text": "They further indicate that most of the improvements in the OPT system come from preventing dropped subjects or verbs, while the improvements in the SPAN system result in roughly equal proportion from preventing word-dropping and ensuring correct ordering.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of span errors", "labels": [], "entities": [{"text": "span errors", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.8491272926330566}]}, {"text": " Table 1. We see that the 26.4%  of subjects with incorrect spans are roughly equally  divided between subjects that are too short and sub- jects that are too long.", "labels": [], "entities": []}, {"text": " Table 1: Distribution of span errors in NIST MT05", "labels": [], "entities": [{"text": "MT05", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.7150206565856934}]}, {"text": " Table 2: Automatic evaluation results", "labels": [], "entities": []}, {"text": " Table 3: Subject integrity analysis results. All numbers are %s.", "labels": [], "entities": [{"text": "Subject integrity analysis", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.9082069794336954}]}]}