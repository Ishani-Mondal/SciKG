{"title": [{"text": "A Collaborative Annotation between Human Annotators and a Statistical Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe anew interactive annotation scheme between a human annotator who carries out simplified annotations on CFG trees, and a statistical parser that converts the human annotations automatically into a richly annotated HPSG treebank.", "labels": [], "entities": [{"text": "HPSG treebank", "start_pos": 225, "end_pos": 238, "type": "DATASET", "confidence": 0.963058590888977}]}, {"text": "In order to check the proposed scheme's effectiveness, we performed automatic pseudo-annotations that emulate the system's idealized behavior and measured the performance of the parser trained on those annotations.", "labels": [], "entities": []}, {"text": "In addition, we implemented a prototype system and conducted manual annotation experiments on a small test set.", "labels": [], "entities": []}], "introductionContent": [{"text": "On the basis of the success of the research on the corpus-based development in NLP, the demand fora variety of corpora has increased, for use as both a training resource and an evaluation data-set.", "labels": [], "entities": []}, {"text": "However, the development of a richly annotated corpus such as an HPSG treebank is not an easy task, since the traditional two-step annotation, in which a parser first generates the candidates and then an annotator checks each candidate, needs intensive efforts even for well-trained annotators).", "labels": [], "entities": [{"text": "HPSG treebank", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9748049676418304}]}, {"text": "Among many NLP problems, adapting a parser for out-domain texts, which is usually referred to as domain adaptation problem, is one of the most remarkable problems.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7021583914756775}]}, {"text": "The main cause of this problem is the lack of corpora in that domain.", "labels": [], "entities": []}, {"text": "Because it is difficult to prepare a sufficient corpus for each domain without reducing the annotation cost, research on annotation methodologies has been intensively studied.", "labels": [], "entities": []}, {"text": "There has been a number of research projects to efficiently develop richly annotated corpora with the help of parsers, one of which is called a discriminant-based treebanking.", "labels": [], "entities": []}, {"text": "In discriminant-based treebanking, the annotation process consists of two steps: a parser first generates the parse trees, which are annotation candidates, and then a human annotator selects the most plausible one.", "labels": [], "entities": []}, {"text": "One of the most important characteristics of this methodology is to use easily-understandable questions called discriminants for picking up the final annotation results.", "labels": [], "entities": []}, {"text": "Human annotators can perform annotations simply by answering those questions without closely examining the whole tree.", "labels": [], "entities": []}, {"text": "Although this approach has been successful in breaking down the difficult annotations into a set of easy questions, specific knowledge about the grammar, especially in the case of a deep grammar, is still required for an annotator.", "labels": [], "entities": []}, {"text": "This would be the bottleneck to reduce the cost of annotator training and can restrict the size of annotations.", "labels": [], "entities": []}, {"text": "Interactive predictive parsing) is another approach of annotations, which focuses on CFG trees.", "labels": [], "entities": [{"text": "Interactive predictive parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6986549496650696}]}, {"text": "In this system, an annotator revises the currently proposed CFG tree until he or she gets the correct tree by using a simple graphical user interface.", "labels": [], "entities": [{"text": "CFG tree", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.928872138261795}]}, {"text": "Although our target product is a more richly annotated treebanks, the interface of CFG can be useful to develop deep annotations such as HPSG features by cooperating with a statistical deep parser.", "labels": [], "entities": []}, {"text": "Since CFG is easier to understand than HPSG, it can re-duce the cost of annotator training; non-experts can perform annotations without decent training.", "labels": [], "entities": [{"text": "CFG", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.8163548111915588}]}, {"text": "As a result, crowd-sourcing or similar approach can be adopted and the annotation process would be accelerated.", "labels": [], "entities": []}, {"text": "Before conducting manual annotation, we simulated the annotation procedure for validating our system.", "labels": [], "entities": []}, {"text": "In order to check whether the CFG-based annotations can lead to sufficiently accurate HPSG annotations, several HPSG treebanks were created with various qualities of CFG and evaluated by their HPSG qualities.", "labels": [], "entities": [{"text": "HPSG treebanks", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.8444694876670837}]}, {"text": "We further conducted manual annotation experiments by two human annotators to evaluate the efficiency of the annotation system and the accuracy of the resulting annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9992293119430542}]}, {"text": "The causes of annotation errors were analyzed and future direction of the further development is discussed.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments shown below, we evaluate the accuracy of an annotation result (i.e., an HPSG derivation on a sentence) by evaluating the accuracy of the semantic description produced by the derivation, as well as a more traditional metrics such as labeled bracketing accuracy of the tree structure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9983217120170593}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9983618855476379}, {"text": "accuracy", "start_pos": 270, "end_pos": 278, "type": "METRIC", "confidence": 0.6058019995689392}]}, {"text": "Specifically, we used labeled and unlabeled precision/recall/F-score of the predicate-argument dependencies and the labeled brackets compared against a gold-standard annotation obtained by using the Enju's treebank conversion tool.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9823468923568726}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.6642534136772156}, {"text": "F-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.7055959701538086}, {"text": "Enju's treebank conversion", "start_pos": 199, "end_pos": 225, "type": "DATASET", "confidence": 0.9460816830396652}]}, {"text": "A predicateargument dependency is represented as a tuple of w p , w a , r, where w p is the predicate word, w a is the argument word, and r is the label of the predicate-argument relation, such as verb-ARG1 (semantic subject of a verb) and prep-MOD (modi-fiee of a prepositional phrase).", "labels": [], "entities": []}, {"text": "As for the bracketing accuracies, the label of a bracket is obtained by projecting the sign corresponding to the phrase into a simple phrasal labels such as S, NP, and VP.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus and experimental data information (s. l.  means \"sentence length.\")", "labels": [], "entities": []}, {"text": " Table 2: The number of \"in-chart\" and \"out-chart\" sentences (total / 1-40 length)", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of the automatic annotation sets. Each cell has the score of CFG F1 / Lex. Acc. / Dep. F1.", "labels": [], "entities": [{"text": "CFG", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.825577974319458}, {"text": "F1 / Lex. Acc. / Dep. F1", "start_pos": 86, "end_pos": 110, "type": "METRIC", "confidence": 0.6209115505218505}]}, {"text": " Table 7: Automatic evaluation of the annotation results  (LP / LR / F1)", "labels": [], "entities": [{"text": "LP / LR / F1)", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.848237931728363}]}, {"text": " Table 8: Automatic evaluation of the annotation results  (LP/LR/F1); in-chart sentences (left-column) and out- chart sentences (right column) both from Brown", "labels": [], "entities": [{"text": "LP/LR/F1)", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.787558396657308}]}, {"text": " Table 4: HPSG agreement of SL-full for \"in-chart\" data (EM means \"Exact Match.\")", "labels": [], "entities": []}, {"text": " Table 5: Domain Adaptation Results", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8844011127948761}]}]}