{"title": [{"text": "A New Sentence Compression Dataset and Its Use in an Abstractive Generate-and-Rank Sentence Compressor", "labels": [], "entities": []}], "abstractContent": [{"text": "Sentence compression has attracted much interest in recent years, but most sentence com-pressors are extractive, i.e., they only delete words.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9265209436416626}]}, {"text": "There is alack of appropriate datasets to train and evaluate abstractive sentence com-pressors, i.e., methods that apart from deleting words can also rephrase expressions.", "labels": [], "entities": []}, {"text": "We present anew dataset that contains candidate extractive and abstractive compressions of source sentences.", "labels": [], "entities": []}, {"text": "The candidate compressions are annotated with human judgements for grammaticality and meaning preservation.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.808407336473465}]}, {"text": "We discuss how the dataset was created, and how it can be used in generate-and-rank ab-stractive sentence compressors.", "labels": [], "entities": []}, {"text": "We also report experimental results with a novel abstrac-tive sentence compressor that uses the dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression is the task of producing a shorter form of a grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source).", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9365787506103516}]}, {"text": "Sentence compression is useful in many applications, such as text summarization ( and subtitle generation).", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9260799288749695}, {"text": "text summarization", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7992634177207947}, {"text": "subtitle generation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.792273759841919}]}, {"text": "Methods for sentence compression can be divided in two categories: extractive methods produce compressions by only removing words, whereas abstractive methods may additionally rephrase expressions of the source sentence.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7306215316057205}]}, {"text": "Extractive methods are generally simpler and have dominated the sentence compression literature.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7999298274517059}]}, {"text": "Abstractive methods, however, can in principle produce shorter compressions that convey the same information as longer extractive ones.", "labels": [], "entities": []}, {"text": "Furthermore, humans produce mostly abstractive compressions; hence, abstractive compressors may generate more natural outputs.", "labels": [], "entities": []}, {"text": "When evaluating extractive methods, it suffices to have a single human gold extractive compression per source sentence, because it has been shown that measuring the similarity (as F 1 -measure of dependencies) between the dependency tree of the gold compression and that of a machine-generated compression correlates well with human judgements ().", "labels": [], "entities": [{"text": "F 1 -measure of dependencies", "start_pos": 180, "end_pos": 208, "type": "METRIC", "confidence": 0.8480992615222931}]}, {"text": "With abstractive methods, however, there is a much wider range of acceptable abstractive compressions of each source sentence, to the extent that a single gold compression per source is insufficient.", "labels": [], "entities": []}, {"text": "Indeed, to the best of our knowledge no measure to compare a machine-generated abstractive compression to a single human gold compression has been shown to correlate well with human judgements.", "labels": [], "entities": []}, {"text": "One might attempt to provide multiple human gold abstractive compressions per source sentence and employ measures from machine translation, for example BLEU (), to compare each machine-generated compression to all the corresponding gold ones.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9965842962265015}]}, {"text": "However, a large number of gold compressions would be necessary to capture all (or at least most) of the acceptable shorter rephras-ings of the source sentences, and it is questionable if human judges could provide (or even think of) all the acceptable rephrasings.", "labels": [], "entities": []}, {"text": "In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they cannot cope sufficiently well with paraphrases), which play a central role in abstractive sentence compression ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.756846010684967}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9950383305549622}, {"text": "abstractive sentence compression", "start_pos": 187, "end_pos": 219, "type": "TASK", "confidence": 0.6161524256070455}]}, {"text": "Although it is difficult to construct datasets for end-to-end automatic evaluation of abstractive sentence compression methods, it is possible to construct datasets to evaluate the ranking components of generate-and-rank abstractive sentence compressors, i.e., compressors that first generate a large set of candidate abstractive (and possibly also extractive) compressions of the source and then rank them to select the best one.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.6786515315373739}]}, {"text": "In previous work, we presented a generateand-rank extractive sentence compressor, hereafter called GA-EXTR, which achieved state-of-the art results.", "labels": [], "entities": []}, {"text": "We aim to construct a similar abstractive generate-and-rank sentence compressor.", "labels": [], "entities": []}, {"text": "As part of this endeavour, we needed a dataset to automatically test (and train) several alternative ranking components.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a dataset of this kind, which we also make publicly available.", "labels": [], "entities": []}, {"text": "The dataset consists of pairs of source sentences and candidate extractive or abstractive compressions.", "labels": [], "entities": []}, {"text": "The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules () to the best extractive compressions of GA-EXTR.", "labels": [], "entities": []}, {"text": "Each pair (source and candidate compression) was then scored by a human judge for grammaticality and meaning preservation.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7769388258457184}]}, {"text": "We discuss how the dataset was constructed and how we established upper and lower performance boundaries for ranking components of compressors that may use it.", "labels": [], "entities": []}, {"text": "We also present the 1 Ways to extend n-gram measures to account for paraphrases have been proposed (), but they require accurate paraphrase recognizers), which are not yet available; or they assume that the same paraphrase generation resources, for example paraphrasing rules, that some abstractive sentence compressors (including ours) use always produce acceptable paraphrases, which is not the case as discussed below.", "labels": [], "entities": []}, {"text": "The new dataset and GA-EXTR are freely available from http://nlp.cs.aueb.gr/software.html.", "labels": [], "entities": [{"text": "GA-EXTR", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.8591644763946533}]}, {"text": "current version of our abstractive sentence compressor, and we discuss how its ranking component was improved by performing experiments on the dataset.", "labels": [], "entities": []}, {"text": "Section 2 below summarizes prior work on abstractive sentence compression.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.6140045523643494}]}, {"text": "Section 3 discusses the dataset we constructed.", "labels": [], "entities": []}, {"text": "Section 4 describes our abstractive sentence compressor.", "labels": [], "entities": []}, {"text": "Section 5 presents our experimental results, and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "To construct the new dataset, we used source sentences from the 570 pairs of Cohn and Lapata (Section 2).", "labels": [], "entities": []}, {"text": "This way a human gold abstractive compression is also available for each source sentence, though we do not currently use the gold compressions in our experiments.", "labels": [], "entities": []}, {"text": "We actually used only 346 of the 570 source sentences of Cohn and Lapata, reserving the remaining 224 for further experiments.", "labels": [], "entities": []}, {"text": "To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below.", "labels": [], "entities": []}, {"text": "We decided to apply paraphrasing rules to extractive compressions, because we noticed that most of the 42 human abstractive compressions of the 50 sample pairs from Cohn and Lapata's dataset that we initially considered (Section 2) could be produced from the corresponding source sentences by first deleting words and then us- The 346 sources are from 19 randomly selected articles among the 30 that Cohn and Lapata drew source sentences from.", "labels": [], "entities": [{"text": "extractive compressions", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7317638397216797}]}, {"text": "ing shorter paraphrases, as in the following example.", "labels": [], "entities": []}, {"text": "source: Constraints on recruiting are constraints on safety and have to be removed.", "labels": [], "entities": [{"text": "recruiting", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.9624502658843994}, {"text": "safety", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.960411012172699}]}, {"text": "extractive: Constraints on recruiting have to be removed.", "labels": [], "entities": []}, {"text": "abstractive: Recruiting constraints must be removed.", "labels": [], "entities": []}, {"text": "To assess the performance of SVR-BASE, SVR-PMI, and SVR-PMI-LDA, we trained the three SVR-based ranking components on the training part of our dataset, and we evaluated them on the test part.", "labels": [], "entities": []}, {"text": "We repeated the experiments for 81 different \u03b3 values to obtain average GM scores at different average compression rates (Section 3.5).", "labels": [], "entities": [{"text": "GM", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.921196699142456}]}, {"text": "The resulting curves of the three SVR-based ranking components are included in (left diagram).", "labels": [], "entities": []}, {"text": "Overall, SVR-PMI-LDA performed better than SVR-PMI and SVR-BASE, since it achieved the best average GM scores throughout the range of average compression rates.", "labels": [], "entities": [{"text": "SVR-PMI", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8847249150276184}, {"text": "GM", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.960369884967804}]}, {"text": "In general, SVR-PMI also performed better than SVR-BASE, though the average GM score of SVR-BASE was sometimes higher.", "labels": [], "entities": [{"text": "SVR-PMI", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.4632132351398468}, {"text": "SVR-BASE", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.7802606225013733}, {"text": "GM score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9847373962402344}]}, {"text": "All three SVR-based ranking components performed better than the random baseline, but worse than the oracle; hence, there is scope for further improvements in the ranking components, which is also why we believe other researchers may wish to experiment with our dataset.", "labels": [], "entities": []}, {"text": "The oracle selected abstractive (as opposed to simply extractive) candidates for 20 (13%) to 30 (19%, depending on \u03b3) of the 158 source sentences of the test part; the same applies to the SVR-based ranking components.", "labels": [], "entities": []}, {"text": "Hence, good abstractive candidates (or at least better than the corresponding extractive ones) are present in the dataset.", "labels": [], "entities": []}, {"text": "Humans, however, produce mostly abstractive compressions, as already discussed; the fact that the oracle (which uses human judgements) does not select abstractive candidates more frequently maybe an indication that more or better abstractive candidates are needed.", "labels": [], "entities": []}, {"text": "We plan to investigate alternative methods to produce more abstractive candidates.", "labels": [], "entities": []}, {"text": "For example, one could translate each source to multiple pivot languages and back to the original language by using multiple commercial machine translation engines instead of, or in addition to applying paraphrasing source generated Gillette was considered a leading financial analyst on the beverage industry -one who also had an expert palate for wine tasting.", "labels": [], "entities": []}, {"text": "Gillette was seen as a leading financial analyst on the beverage industry -one who also had an expert palate.", "labels": [], "entities": []}, {"text": "Nearly 200,000 lawsuits were brought by women who said they suffered injuries ranging from minor inflammation to infertility and in some cases, death.", "labels": [], "entities": []}, {"text": "Lawsuits were made by women who said they suffered injuries ranging from inflammation to infertility in some cases, death.", "labels": [], "entities": []}, {"text": "Marcello Mastroianni, the witty, affable and darkly handsome Italian actor who sprang on international consciousness in Federico Fellini's 1960 classic \"La Dolce Vita,\" died Wednesday at his Paris home.", "labels": [], "entities": [{"text": "international consciousness in Federico Fellini's 1960 classic \"La Dolce Vita", "start_pos": 89, "end_pos": 166, "type": "TASK", "confidence": 0.6711014434695244}]}, {"text": "Marcello Mastroianni died Wednesday at his home.", "labels": [], "entities": []}, {"text": "A pioneer in laparoscopy, he held over 30 patents for medical instruments used in abdominal surgery such as tubal ligations.", "labels": [], "entities": []}, {"text": "He held over 30 patents for the medical tools used in abdominal surgery.", "labels": [], "entities": [{"text": "abdominal surgery", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7843868434429169}]}, {"text": "LOS ANGELES -James Arnold Doolittle, a Los Angeles dance impresario who brought names such as Joffrey and Baryshnikov to local dance stages and ensured that a high-profile \"Nutcracker Suite\" was presented here every Christmas, has died.", "labels": [], "entities": [{"text": "LOS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9730817675590515}, {"text": "ANGELES", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.6119338274002075}]}, {"text": "James Arnold Doolittle, a Los Angeles dance impresario is dead.", "labels": [], "entities": []}, {"text": "After working as a cashier fora British filmmaker in Rome, he joined an amateur theatrical group at the University of Rome, where he was taking some classes.", "labels": [], "entities": [{"text": "British filmmaker", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.893145352602005}]}, {"text": "After working as a cashier fora British filmmaker in Rome, he joined an amateur group at the University of Rome, where he was using some classes.", "labels": [], "entities": [{"text": "British filmmaker", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.8926731944084167}]}, {"text": "He was a 1953 graduate of the Johns Hopkins Medical School and after completing his residency in gynecology and surgery, traveled to Denmark where he joined the staff of the National Cancer Center there.", "labels": [], "entities": [{"text": "National Cancer Center", "start_pos": 174, "end_pos": 196, "type": "DATASET", "confidence": 0.9323273499806722}]}, {"text": "He was a graduate of the Johns Hopkins Medical School and traveled to Denmark where he joined a member of the National Cancer Center there.", "labels": [], "entities": [{"text": "National Cancer Center", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.9455249706904093}]}, {"text": "Mastroianni, a comic but also suave and romantic leading man in some 120 motion pictures, had suffered from pancreatic cancer.", "labels": [], "entities": []}, {"text": "Mastroianni, a leading man in some 120 motion pictures, had subjected to cancer. rules.", "labels": [], "entities": []}, {"text": "An approach of this kind has been proposed for sentence paraphrasing (.", "labels": [], "entities": [{"text": "sentence paraphrasing", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7460943162441254}]}, {"text": "The right diagram of shows how the performance of SVR-PMI-LDA is affected when using 33% or 63% of the training s i , c i pairs.", "labels": [], "entities": []}, {"text": "As more examples are used, the performance improves, suggesting that better results could be obtained by using more training data.", "labels": [], "entities": []}, {"text": "Finally, shows examples of good and bad compressions the abstractive compressor produced with SVR-PMI-LDA.", "labels": [], "entities": [{"text": "SVR-PMI-LDA", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.916472852230072}]}], "tableCaptions": [{"text": " Table 1: Distribution of GM scores (grammaticality plus meaning preservation) in our dataset.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.6490777134895325}]}]}