{"title": [], "abstractContent": [{"text": "We present the first work on applying statistical techniques to unrestricted Quanti-fier Scope Disambiguation (QSD), where there is no restriction on the type or the number of quantifiers in the sentence.", "labels": [], "entities": [{"text": "unrestricted Quanti-fier Scope Disambiguation (QSD)", "start_pos": 64, "end_pos": 115, "type": "TASK", "confidence": 0.6991543684686933}]}, {"text": "We formulate unrestricted QSD as learning to build a Directed Acyclic Graph (DAG) and define evaluation metrics based on the properties of DAGs.", "labels": [], "entities": []}, {"text": "Previous work on statistical scope disambiguation is very limited , only considering sentences with two explicitly quantified noun phrases (NPs).", "labels": [], "entities": [{"text": "statistical scope disambiguation", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.6809824705123901}]}, {"text": "In addition, they only handle a restricted list of quantifiers.", "labels": [], "entities": []}, {"text": "In our system, all NPs, explicitly quantified or not (e.g. definites, bare singulars/plurals, etc.), are considered for possible scope interactions.", "labels": [], "entities": []}, {"text": "We present early results on applying a simple model to a small corpus.", "labels": [], "entities": []}, {"text": "The preliminary results are encouraging, and we hope will motivate further research in this area.", "labels": [], "entities": []}, {"text": "1 Introduction There are at least two interpretations for the following sentence: (1) Every line ends with a digit.", "labels": [], "entities": []}, {"text": "In one reading, there is a unique digit (say 2) at the end of all lines.", "labels": [], "entities": []}, {"text": "This is the case where the quanti-fier A outscopes (aka having wide-scope over) the quantifier Every.", "labels": [], "entities": []}, {"text": "The other case is the one in which Every has wide-scope (or alternatively A has narrow scope), and represents the reading in which different lines could possibly end with distinct digits.", "labels": [], "entities": []}, {"text": "This phenomenon is known as quantifier scope ambiguity.", "labels": [], "entities": [{"text": "quantifier scope ambiguity", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7030101219813029}]}, {"text": "Shortly after the first efforts to build natural language understanding systems, Quantifier Scope Disambiguation (QSD) was realized to be very difficult.", "labels": [], "entities": [{"text": "Quantifier Scope Disambiguation (QSD)", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.721366822719574}]}, {"text": "Woods (1978) was one of the first to suggest away to get around this problem.", "labels": [], "entities": []}, {"text": "He presented a framework for scope-underspecified semantic representation.", "labels": [], "entities": [{"text": "scope-underspecified semantic representation", "start_pos": 29, "end_pos": 73, "type": "TASK", "confidence": 0.6443597177664439}]}, {"text": "He suggests representing the Logical Form (LF) of the above sentence as: (2) <Every x Line> <A y Digit> Ends-with(x, y) in which, the relative scope of the quantifiers is underspecified.", "labels": [], "entities": []}, {"text": "Since then scope underspecifica-tion has been the most popular way to deal with quantifier scope ambiguity in deep language understanding systems (e.g. Boxer (Bos 2004), TRAINS (Allen et al.", "labels": [], "entities": []}, {"text": "2007), BLUE (Clark and Harrison 2008), and DELPH-IN 1).", "labels": [], "entities": [{"text": "BLUE", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9852425456047058}, {"text": "Clark and Harrison 2008)", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.7047574162483216}, {"text": "DELPH-IN", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9702205061912537}]}, {"text": "Scope under-specification works in practice, only because many NLP applications (e.g. machine translation) could be achieved without quantifier scope disambigua-tion.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7643926441669464}]}, {"text": "QSD on the other hand, is critical for many other NLP tasks such as question answering systems , dialogue systems and computing entailment.", "labels": [], "entities": [{"text": "question answering", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9086245894432068}]}, {"text": "Almost all efforts in the 80s and 90s on QSD adopt heuristics based on the lexical properties of the quantifiers, syntactic/semantic properties of the sentences, and discourse/pragmatic cues (VanLehn 1 http://www.delph-in.net/ 51", "labels": [], "entities": []}], "introductionContent": [{"text": "There are at least two interpretations for the following sentence: (1) Every line ends with a digit.", "labels": [], "entities": []}, {"text": "In one reading, there is a unique digit (say 2) at the end of all lines.", "labels": [], "entities": []}, {"text": "This is the case where the quantifier A outscopes (aka having wide-scope over) the quantifier Every.", "labels": [], "entities": []}, {"text": "The other case is the one in which Every has wide-scope (or alternatively A has narrow-scope), and represents the reading in which different lines could possibly end with distinct digits.", "labels": [], "entities": []}, {"text": "This phenomenon is known as quantifier scope ambiguity.", "labels": [], "entities": [{"text": "quantifier scope ambiguity", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7030101219813029}]}, {"text": "Shortly after the first efforts to build natural language understanding systems, Quantifier Scope Disambiguation (QSD) was realized to be very difficult. was one of the first to suggest away to get around this problem.", "labels": [], "entities": [{"text": "Quantifier Scope Disambiguation (QSD)", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.7342216422160467}]}, {"text": "He presented a framework for scope-underspecified semantic representation.", "labels": [], "entities": [{"text": "scope-underspecified semantic representation", "start_pos": 29, "end_pos": 73, "type": "TASK", "confidence": 0.6443597177664439}]}, {"text": "He suggests representing the Logical Form (LF) of the above sentence as: (2) <Every x Line> <A y Digit> Ends-with(x, y) in which, the relative scope of the quantifiers is underspecified.", "labels": [], "entities": []}, {"text": "Since then scope underspecification has been the most popular way to deal with quantifier scope ambiguity in deep language understanding systems (e.g. Boxer), TRAINS (), BLUE, and DELPH-IN 1 ).", "labels": [], "entities": [{"text": "BLUE", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.8039668202400208}]}, {"text": "Scope underspecification works in practice, only because many NLP applications (e.g. machine translation) could be achieved without quantifier scope disambiguation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7710084617137909}]}, {"text": "QSD on the other hand, is critical for many other NLP tasks such as question answering systems, dialogue systems and computing entailment.", "labels": [], "entities": [{"text": "question answering", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9086245894432068}]}, {"text": "Almost all efforts in the 80s and 90s on QSD adopt heuristics based on the lexical properties of the quantifiers, syntactic/semantic properties of the sentences, and discourse/pragmatic cues.", "labels": [], "entities": []}, {"text": "For example, it is widely known that in English, the quantifier each tends to have the widest scope.", "labels": [], "entities": []}, {"text": "Also, the subject of a sentence often outscopes the direct object.", "labels": [], "entities": []}, {"text": "In cases where these heuristics conflict, (manually) weighted preference rules are adopted to resolve the conflict (.", "labels": [], "entities": []}, {"text": "In the last decade there has been some effort to apply statistical and machine learning (ML) techniques to QSD.", "labels": [], "entities": [{"text": "statistical and machine learning (ML)", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.6937766884054456}]}, {"text": "All the previous efforts, however, suffer from the following two limitations (see section 2 for details): \u2022 They only allow scoping two NPs per sentence.", "labels": [], "entities": []}, {"text": "\u2022 The NPs must be explicitly quantified (e.g. they ignore definites or bare singulars/plurals), and the quantifiers are restricted to a predefined list.", "labels": [], "entities": []}, {"text": "In this paper, we present the first work on applying statistical techniques to unrestricted QSD, where we put no restriction on the type or the number of NPs to be scoped in a sentence.", "labels": [], "entities": []}, {"text": "In fact, every two NPs, explicitly quantified or not (including definites, indefinites, bare singulars/plurals, pronouns, etc.), are examined for possible scope interactions.", "labels": [], "entities": []}, {"text": "Scoping only two quantifiers per sentence, the previous work defines QSD as a single classification task (e.g. 0 where the first quantifier has widescope, and 1 otherwise).", "labels": [], "entities": []}, {"text": "As a result standard metrics for classification tasks are used for evaluation purposes.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.903370589017868}]}, {"text": "We formalize the unrestricted form of QSD as learning to build a DAG over the set of NP chunks in the sentence.", "labels": [], "entities": []}, {"text": "We define accuracy, precision and recall metrics based on the properties of DAGs for evaluation purposes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992550015449524}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9990078806877136}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9984819293022156}]}, {"text": "We report the application of our model to a small corpus.", "labels": [], "entities": []}, {"text": "As seen later, the early results are promising and shall motivate further research on applying ML techniques to unrestricted QSD.", "labels": [], "entities": [{"text": "ML", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9760401248931885}]}, {"text": "In fact, they set a baseline for future work in this area.", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows.", "labels": [], "entities": []}, {"text": "Section (2) reviews the related work.", "labels": [], "entities": []}, {"text": "In (3) we briefly describe our corpus.", "labels": [], "entities": []}, {"text": "We formalize the problem of quantifier scope disambiguation for multiple quantifiers in section (4) and define some evaluation metrics in (5).", "labels": [], "entities": [{"text": "quantifier scope disambiguation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.6932312250137329}]}, {"text": "(6) presents our model including the kinds of features we have used.", "labels": [], "entities": []}, {"text": "We present our experiments in (7) and give a discussion of the results in. summarizes the current work and gives some directions for the future work.", "labels": [], "entities": []}, {"text": "2 Allen (1995) discusses some of these heuristics and gives an algorithm to incorporate those for scoping while parsing.", "labels": [], "entities": []}], "datasetContent": [{"text": "Intuitively the similarity of two QSDs, given fora sentence S, can be defined as the ratio of the chunk pairs that have the same label in both QSDs to the total number of pairs.", "labels": [], "entities": []}, {"text": "For example, consider the 6 Given a DAG G=(V, E), node u is said to immediately dominate node v if and only if (u,v) \u2208 E.", "labels": [], "entities": []}, {"text": "\"dominates\" is the reflexive transitive closure of \"immediately dominates\".", "labels": [], "entities": []}, {"text": "The nodes u and v of the DAG G are said to be disjoint if neither u dominates v nor v dominates u.", "labels": [], "entities": []}, {"text": "(a) (b).", "labels": [], "entities": []}, {"text": "Scopings represented as DAGs two DAGs in.", "labels": [], "entities": []}, {"text": "Although looking different, both DAGs define the same partial order (i.e. QSD).", "labels": [], "entities": []}, {"text": "This is because the partial order represented by a DAG G corresponds to the transitive closure (TC) of G.", "labels": [], "entities": []}, {"text": "100 sentences from the corpus were picked at random as the development set, in order to study the relevant features and their contribution to QSD.", "labels": [], "entities": [{"text": "QSD", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.6849242448806763}]}, {"text": "The rest of the corpus (400 sentences) was then used to do a 5 fold cross validation.", "labels": [], "entities": []}, {"text": "We used SVM Multiclass from SVM-light toolkit (Joachims 1999) as the classifier.", "labels": [], "entities": [{"text": "SVM Multiclass from SVM-light toolkit (Joachims 1999)", "start_pos": 8, "end_pos": 61, "type": "DATASET", "confidence": 0.8348998228708903}]}, {"text": "Before giving the results, we define a baseline.", "labels": [], "entities": []}, {"text": "HS03 use the most frequent label as the baseline and the similarity metric given in definition (5) to evaluate the performance.", "labels": [], "entities": [{"text": "HS03", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9499374628067017}, {"text": "similarity", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9629353284835815}]}, {"text": "Since more than 61% of the labels in their corpus is NI, the baseline system (that leaves every sentence unscoped) has the accuracy above 61%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9993330836296082}]}, {"text": "In our corpus, the majority class is WS containing around 35% of the samples.", "labels": [], "entities": []}, {"text": "NS and NI each contain 34% and 31% of the samples respectively.", "labels": [], "entities": [{"text": "NS", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8149390816688538}]}, {"text": "This means that there is a slight tendency for having scope preference in chronological order.", "labels": [], "entities": []}, {"text": "Therefore, the linear order of the chunks (i.e. from left to right) defines a reasonable baseline.", "labels": [], "entities": []}, {"text": "The results of our experiments are shown in table 1.", "labels": [], "entities": []}, {"text": "The table lists the parameters P, R, and F-score 9 for our SVM-MC model vs. the baseline system.", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9757733345031738}]}, {"text": "For each system, two sets of metrics have been reported: TC-based and TR-based.", "labels": [], "entities": [{"text": "TC-based", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9427043795585632}, {"text": "TR-based", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9379746913909912}]}, {"text": "lists the sentence-level accuracy of the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9913123250007629}]}, {"text": "We computed two metrics for sentencelevel accuracy: Acc and Acc-EZ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9450883269309998}, {"text": "Acc", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9963811039924622}, {"text": "Acc-EZ", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9848145246505737}]}, {"text": "In calculating Acc, a sentence is considered correct if all the labels (including NI) exactly match the gold standard labels.", "labels": [], "entities": [{"text": "Acc", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9006497859954834}]}, {"text": "However, this is an unnecessarily tough metric.", "labels": [], "entities": []}, {"text": "As mentioned before (footnote 8), in practice the output of the system for the samples labeled NI is not important; all we care is that all outscoping (i.e. WS/NS) relations are recovered correctly.", "labels": [], "entities": []}, {"text": "In other words, in practice, the system's recall is the most important parameter.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9992309808731079}]}, {"text": "Regarding this fact, we define Acc-EZ as the percentage of sentences with 100% recall (ignoring the value of precision).", "labels": [], "entities": [{"text": "Acc-EZ", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9971535205841064}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9882789850234985}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9991440773010254}]}, {"text": "In order to compare our system with that of HS03, we applied our model unmodified to their corpus using the same set-up, a 10-fold cross validation.", "labels": [], "entities": [{"text": "HS03", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.9409494996070862}]}, {"text": "However, since their corpus is not annotated with DG, we translated our dependency features to the properties of the Penn Treebank's phrase structure trees.3) lists the accuracy 9 F-score is defined as F=2PR/(P+R). of their best model, their baseline, and our SVM-MC model.", "labels": [], "entities": [{"text": "Penn Treebank's phrase structure trees.3", "start_pos": 117, "end_pos": 157, "type": "DATASET", "confidence": 0.9646445711453756}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9995238780975342}, {"text": "F-score", "start_pos": 180, "end_pos": 187, "type": "METRIC", "confidence": 0.9892483949661255}, {"text": "F", "start_pos": 202, "end_pos": 203, "type": "METRIC", "confidence": 0.9796282052993774}]}, {"text": "As seen in this table, their model outperforms ours.", "labels": [], "entities": []}, {"text": "This, however, is not surprising.", "labels": [], "entities": []}, {"text": "First, although we trained our model on their corpus, the feature engineering of our model was done based on our own development set.", "labels": [], "entities": []}, {"text": "Second, since our corpus is not annotated with phrase structure trees, our model does not use any of their features that can only be extracted from phrase structure trees.", "labels": [], "entities": []}, {"text": "It remains for future work to incorporate the features extracted from phrase structure trees (which is not already encoded in DG) and evaluate the performance of the model on either corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Comparison with HS04 system on their dataset", "labels": [], "entities": []}, {"text": " Table 2. Sentence level accuracy", "labels": [], "entities": [{"text": "Sentence level accuracy", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.587170014778773}]}]}