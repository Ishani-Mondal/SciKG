{"title": [{"text": "Overview of the Entity Relations (REL) supporting task of BioNLP Shared Task 2011", "labels": [], "entities": [{"text": "BioNLP Shared Task 2011", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.6141322702169418}]}], "abstractContent": [{"text": "This paper presents the Entity Relations (REL) task, a supporting task of the BioNLP Shared Task 2011.", "labels": [], "entities": [{"text": "Entity Relations (REL) task", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6722636967897415}, {"text": "BioNLP Shared Task 2011", "start_pos": 78, "end_pos": 101, "type": "DATASET", "confidence": 0.5372571721673012}]}, {"text": "The task concerns the extraction of two types of part-of relations between a gene/protein and an associated entity.", "labels": [], "entities": []}, {"text": "Four teams submitted final results for the REL task, with the highest-performing system achieving 57.7% F-score.", "labels": [], "entities": [{"text": "REL task", "start_pos": 43, "end_pos": 51, "type": "TASK", "confidence": 0.7333811819553375}, {"text": "F-score", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9961511492729187}]}, {"text": "While experiments suggest use of the data can help improve event extraction performance, the task data has so far received only limited use in support of event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7638249397277832}, {"text": "event extraction", "start_pos": 154, "end_pos": 170, "type": "TASK", "confidence": 0.7723639607429504}]}, {"text": "The REL task continues as an open challenge, with all resources available from the shared task website.", "labels": [], "entities": [{"text": "REL task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.7493001520633698}]}], "introductionContent": [{"text": "The BioNLP Shared Task 2011 (BioNLP ST'11)), the follow-up event to the BioNLP'09 Shared Task ( ), was organized from August 2010 (sample data release) to March 2011.", "labels": [], "entities": [{"text": "BioNLP Shared Task 2011 (BioNLP ST'11))", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.5700474604964256}]}, {"text": "The shared task was divided into two stages, with supporting tasks carried out before the main tasks.", "labels": [], "entities": []}, {"text": "The motivation for this task setup drew in part from analysis of the results of the previous shared task, which suggested that events that involve coreference or entity relations represent particular challenges for extraction.", "labels": [], "entities": []}, {"text": "To help address these challenges and encourage modular extraction approaches, increased sharing of successful solutions, and an efficient division of labor, the two were separated into independent supporting tasks on Coreference (CO) ) and Entity Relations in BioNLP ST'11.", "labels": [], "entities": []}, {"text": "This paper presents the Entity Relations (REL) supporting task.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of the REL task is relation-based and uses the standard precision/recall/F 1 -score metrics.", "labels": [], "entities": [{"text": "REL task", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.8740232288837433}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9984647035598755}, {"text": "recall/F 1 -score metrics", "start_pos": 81, "end_pos": 106, "type": "METRIC", "confidence": 0.8759368998663766}]}, {"text": "Similarly to the BioNLP'09 ST and most of the 2011 main tasks, the REL task relaxes the equality criteria for matching text-bound annotations: fora submission entity to match an entity in the gold reference annotation, it is sufficient that the span of the submitted entity (i.e. its start and end positions in text) is entirely contained within the span of the gold annotation.", "labels": [], "entities": [{"text": "BioNLP'09 ST", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8892283737659454}, {"text": "REL task", "start_pos": 67, "end_pos": 75, "type": "TASK", "confidence": 0.6274450421333313}]}, {"text": "This corresponds largely to the approximate span matching criterion of the 2009 task ( ), although the REL criterion is slightly stricter in not involving testing against an extension of the gold entity span.", "labels": [], "entities": [{"text": "REL", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9931938052177429}]}, {"text": "Relation matching is exact: fora submitted relation to match a gold one, both its type and the related entities must match.", "labels": [], "entities": []}, {"text": "summarizes the participating groups and approaches.", "labels": [], "entities": []}, {"text": "We find a remarkable number of similarities between the approaches of the systems, with all four utilizing full parsing and a dependency representation of the syntactic analysis, and the three highest-ranking further specifically the phrase structure parser of with the biomedical domain model of, converted into Stanford Dependency form using the Stanford tools).", "labels": [], "entities": [{"text": "phrase structure parser", "start_pos": 234, "end_pos": 257, "type": "TASK", "confidence": 0.6949683825174967}]}, {"text": "These specific choices may perhaps be influenced by the success of systems building on them in the 2009 shared task (e.g. Only the VIBGhent system makes use of resources external to those provided for the task, extracting specific semantic entity types from the GENIA corpus as well as inducing word similarities from a large unannotated corpus of PubMed abstracts.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 262, "end_pos": 274, "type": "DATASET", "confidence": 0.9221042096614838}]}, {"text": "shows the results of the REL task.", "labels": [], "entities": [{"text": "REL task", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.6030850410461426}]}, {"text": "We find that the four systems diverge substantially in terms of overall performance, with all pairs of systems of neighboring ranks showing differences approaching or exceeding 10% points in F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 191, "end_pos": 198, "type": "METRIC", "confidence": 0.9954830408096313}]}, {"text": "While three of the systems notably favor precision over recall, VIBGhent shows a decided preference for recall, suggesting a different approach from UTurku in design details despite the substantial similarities in overall system architecture.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9983896017074585}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9969141483306885}, {"text": "VIBGhent", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.7992169857025146}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9986225366592407}]}, {"text": "The highest-performing system, UTurku, shows an F-score in the general range of state-of-the-art results in the main event extraction task, which could betaken as an indication that the reliability of REL task analyses created with presently available methods may not be high enough for direct use as a building block for the main tasks.", "labels": [], "entities": [{"text": "UTurku", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.7824094891548157}, {"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9989582300186157}, {"text": "main event extraction task", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.7768747508525848}]}, {"text": "However, the emphasis of the UTurku system on precision is encouraging for such applications: nearly 70% of the entity-relation pairs that the system predicts are correct.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9979735016822815}]}, {"text": "The two topranking systems show similar precision and recall results for the two relation types.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9994896650314331}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9993283748626709}]}, {"text": "The submission of HCMUS shows a decided advantage for PROTEIN-COMPONENT relation extraction as tentatively predicted from the relative numbers of training examples (Section 3 and), but their rule-based approach suggests training data size is likely not the decisive factor.", "labels": [], "entities": [{"text": "HCMUS", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.8644047975540161}, {"text": "PROTEIN-COMPONENT relation extraction", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.7479451100031534}]}, {"text": "While the limited amount of data available prevents strong conclusions from being drawn, overall the lack of correlation between training data size and extraction performance suggests that performance may not be primarily limited by the size of the available training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: REL dataset statistics.", "labels": [], "entities": [{"text": "REL dataset", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.7315766662359238}]}, {"text": " Table 2: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural  Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer,  McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary", "labels": [], "entities": [{"text": "BI", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9785196185112}]}, {"text": " Table 3: Primary evaluation results for the REL task. Results given as recall / precision / F-score.", "labels": [], "entities": [{"text": "REL task", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.8867671191692352}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9995014667510986}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.8275575041770935}, {"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.8947423696517944}]}]}