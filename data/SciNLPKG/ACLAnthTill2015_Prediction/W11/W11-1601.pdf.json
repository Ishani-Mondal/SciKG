{"title": [{"text": "Learning to Simplify Sentences Using Wikipedia", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we examine the sentence simplification problem as an English-to-English translation problem, utilizing a corpus of 137K aligned sentence pairs extracted by aligning English Wikipedia and Simple En-glish Wikipedia.", "labels": [], "entities": [{"text": "sentence simplification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7319196462631226}, {"text": "English-to-English translation", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.70575250685215}]}, {"text": "This data set contains the full range of transformation operations including rewording, reordering, insertion and deletion.", "labels": [], "entities": []}, {"text": "We introduce anew translation model for text simplification that extends a phrase-based machine translation approach to include phrasal deletion.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7783012986183167}, {"text": "phrase-based machine translation", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.6366091072559357}]}, {"text": "Evaluated based on three metrics that compare against a human reference (BLEU, word-F1 and SSA) our new approach performs significantly better than two text compression techniques (including T3) and the phrase-based translation system without deletion.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.998002827167511}, {"text": "text compression", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.7310517728328705}, {"text": "phrase-based translation", "start_pos": 203, "end_pos": 227, "type": "TASK", "confidence": 0.7367392182350159}]}], "introductionContent": [{"text": "In this paper we examine the sentence simplification problem: given an English sentence we aim to produce a simplified version of that sentence with simpler vocabulary and sentence structure while preserving the main ideas in the original sentence.", "labels": [], "entities": [{"text": "sentence simplification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7121740132570267}]}, {"text": "The definition what a \"simple\" sentence is can vary and represents a spectrum of complexity and readability.", "labels": [], "entities": []}, {"text": "For concreteness, we use Simple English Wikipedia 1 as our archetype of simplified English.", "labels": [], "entities": [{"text": "Simple English Wikipedia 1", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.7076169550418854}]}, {"text": "Simple English Wikipedia articles represent a simplified version of traditional English Wikipedia articles.", "labels": [], "entities": []}, {"text": "The main Simple English 1 http://simple.wikipedia.org Wikipedia page outlines general guidelines for creating simple articles: \u2022 Use Basic English vocabulary and shorter sentences.", "labels": [], "entities": []}, {"text": "This allows people to understand normally complex terms or phrases.", "labels": [], "entities": []}, {"text": "\u2022 Simple does not mean short.", "labels": [], "entities": [{"text": "Simple", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9878214001655579}, {"text": "short", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9474345445632935}]}, {"text": "Writing in Simple English means that simple words are used.", "labels": [], "entities": []}, {"text": "It does not mean readers want basic information.", "labels": [], "entities": []}, {"text": "Articles do not have to be short to be simple; expand articles, add details, but use basic vocabulary.", "labels": [], "entities": []}, {"text": "The data set we examine contains aligned sentence pairs of English Wikipedia 2 with Simple English Wikipedia (.", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.723738819360733}]}, {"text": "We view the simplification problem as an English-to-English translation problem: given aligned sentence pairs consisting of a normal, unsimplified sentence and a simplified version of that sentence, the goal is to learn a sentence simplification system to \"translate\" from normal English to simplified English.", "labels": [], "entities": [{"text": "English-to-English translation", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.714390754699707}]}, {"text": "This setup has been successfully employed in a number of text-to-text applications including machine translation, paraphrasing) and text compression (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7504920959472656}, {"text": "text compression", "start_pos": 132, "end_pos": 148, "type": "TASK", "confidence": 0.795446902513504}]}, {"text": "shows example sentence pairs from the aligned data set.", "labels": [], "entities": []}, {"text": "One of the challenges of text simplification is that, unlike text compression where the emphasis is often on word deletion, text simplifica-a.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7000766694545746}, {"text": "text compression", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7147684097290039}]}, {"text": "Normal: Greene agreed that she could earn more by breaking away from 20th Century Fox.", "labels": [], "entities": [{"text": "20th Century Fox", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.6261676152547201}]}, {"text": "Simple: Greene agreed that she could earn more by leaving 20th Century Fox. b. Normal: The crust and underlying relatively rigid mantle makeup the lithosphere.", "labels": [], "entities": []}, {"text": "Simple: The crust and mantle makeup the lithosphere. c. Normal: They established themselves here and called that port Menestheus's port.", "labels": [], "entities": []}, {"text": "Simple: They called the port Menestheus's port. d. Normal: Heat engines are often confused with the cycles they attempt to mimic.", "labels": [], "entities": []}, {"text": "Simple: Real heat engines are often confused with the ideal engines or cycles they attempt to mimic. e. Normal: In 1962 , Steinbeck received the Nobel Prize for Literature.", "labels": [], "entities": [{"text": "Steinbeck received the Nobel Prize for Literature", "start_pos": 122, "end_pos": 171, "type": "TASK", "confidence": 0.5876907067639487}]}, {"text": "Simple: Steinbeck won the Nobel Prize in Literature in 1962.", "labels": [], "entities": [{"text": "Steinbeck won the Nobel Prize in Literature in 1962", "start_pos": 8, "end_pos": 59, "type": "TASK", "confidence": 0.6361890501446195}]}, {"text": "tion involves the full range of transformation operations: deletion: \"underlying relatively rigid\" in b., \"established themselves here and\" inc. and the comma ind.", "labels": [], "entities": []}, {"text": "rewording: \"breaking away from\" \u2192 \"leaving\" in a. and \"received\" \u2192 \"won\" in e. reordering: in e.", "labels": [], "entities": []}, {"text": "\"in 1962\" moves from the beginning of the sentence to the end.", "labels": [], "entities": []}, {"text": "insertion: \"ideal engines or\" ind.", "labels": [], "entities": [{"text": "insertion", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9442986249923706}]}, {"text": "Motivated by the need to model all of these different transformations, we chose to extend a statistical phrase-based translation system ().", "labels": [], "entities": [{"text": "statistical phrase-based translation", "start_pos": 92, "end_pos": 128, "type": "TASK", "confidence": 0.5608925422032675}]}, {"text": "In particular, we added phrasal deletion to the probabilistic translation model.", "labels": [], "entities": []}, {"text": "This addition broadens the deletion capabilities of the system since the base model only allows for deletion within a phrase.", "labels": [], "entities": []}, {"text": "As Kauchak and Coster (2011) point out, deletion is a frequently occurring phenomena in the simplification data.", "labels": [], "entities": []}, {"text": "There area number of benefits of text simplification research.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8337189853191376}]}, {"text": "Much of the current text data available including Wikipedia, news articles and most web pages are written with an average adult reader as the target audience.", "labels": [], "entities": []}, {"text": "Text simplification can make this data available to a broader range of audiences including children, language learners, the elderly, the hearing impaired and people with aphasia or cognitive disabilities.", "labels": [], "entities": []}, {"text": "Text simplification has also been shown to improve the performance of other natural language processing applications including semantic role labeling and relation extraction ().", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.7074218193689982}, {"text": "relation extraction", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.8716329634189606}]}], "datasetContent": [{"text": "We compared five different approaches on the text simplification task: none: Does no simplification.", "labels": [], "entities": [{"text": "text simplification task", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.8593915104866028}]}, {"text": "Outputs the normal, unsimplified sentence.", "labels": [], "entities": []}, {"text": "K & M: Noisy-channel sentence compression system described in.", "labels": [], "entities": [{"text": "K & M", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.780938466389974}, {"text": "Noisy-channel sentence compression", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.6035259366035461}]}, {"text": "Moses: Phrase-based, machine translation approach (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.720781609416008}]}, {"text": "Moses+Del: Our approach described in Section 4 which is a phrase-based approach with the addition of phrasal deletion.", "labels": [], "entities": []}, {"text": "From the aligned data set of 137K sentence pairs, we used 124K for training and 1,300 for testing with the remaining 12K sentences used during development.", "labels": [], "entities": []}, {"text": "We trained the n-gram language model used by the last four systems on the simple side of the training data.", "labels": [], "entities": []}, {"text": "3 T3 requires parsed data which we generated using the Stanford parser (.", "labels": [], "entities": []}, {"text": "Both Moses and Moses+Del were trained using the default Moses parameters and we used the last 500 sentence pairs from the training set to optimize the hyper-parameters of the log-linear model for both Moses variants.", "labels": [], "entities": []}, {"text": "T3 was run with the default parameters.", "labels": [], "entities": [{"text": "T3", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9428780674934387}]}, {"text": "Due to runtime and memory issues, we were unable to run T3 on the full data set.", "labels": [], "entities": []}, {"text": "We therefore present results for T3 trained on the largest training set that completed successfully, the first 30K sentence pairs.", "labels": [], "entities": []}, {"text": "This still represents a significantly larger training set than T3 has been run on previously.", "labels": [], "entities": []}, {"text": "For comparison, we also provide results below for Moses+Del trained on the same 30K sentences.", "labels": [], "entities": []}, {"text": "Since there is no standard way of evaluating text simplification, we provide results for three different automatic methods, all of which compare the system's output to a reference simplification.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7059899270534515}]}, {"text": "We used BLEU (), which is the weighted mean of n-gram precisions with a penalty for brevity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9992212057113647}]}, {"text": "It has been used extensively in machine translation and has been shown to correlate well with human performance judgements.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8279297351837158}]}, {"text": "We also adopt two automatic measures that have been used to evaluate text compression that compare the system's output to a reference translation  (): simple string accuracy measure (a normalized version of edit distance, abbreviated SSA) and F1 score calculated over words.", "labels": [], "entities": [{"text": "text compression", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7427915036678314}, {"text": "string accuracy measure", "start_pos": 158, "end_pos": 181, "type": "METRIC", "confidence": 0.7021400034427643}, {"text": "F1 score", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.9891339242458344}]}, {"text": "We calculated F1 over words instead of grammatical relations (subject, direct/indirect object, etc.) since finding the relation correspondence between the system output and the reference is a non-trivial task for simplification data where reordering, insertions and lexical changes can occur.", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.998042106628418}]}, {"text": "Clarke and Lapata showed a moderate correlation with human judgement for SSA and a strong correlation for the F1 measure.", "labels": [], "entities": [{"text": "SSA", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9274283647537231}, {"text": "F1 measure", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9855446815490723}]}, {"text": "To measure whether the difference between system performance is statistically significant, we use bootstrap resampling with 100 samples with the ttest).", "labels": [], "entities": []}, {"text": "shows the results on the test set for the different evaluation measures.", "labels": [], "entities": []}, {"text": "All three of the evaluation metrics rank the five systems in the same order with Moses+Del performing best.", "labels": [], "entities": []}, {"text": "All differences between the systems are statistically significant for all metrics at the p = 0.01 level.", "labels": [], "entities": []}, {"text": "One of the challenges for the sentence simplification problem is that, like sentence compression, not making any changes to the system produces reasonable results (contrast this with machine translation).", "labels": [], "entities": [{"text": "sentence simplification", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7329060137271881}, {"text": "sentence compression", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7239327877759933}, {"text": "machine translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.7520900070667267}]}, {"text": "In the test set, 30% of the simple sentences were the same as the corresponding normal sentence.", "labels": [], "entities": []}, {"text": "Because of this, we see that not making any changes (none) performs fairly well.", "labels": [], "entities": []}, {"text": "It is, however, important to leave these sentences in the test set, since not all sentences need simplification and systems should be able to handle these sentences appropriately.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance of the five approaches on the test  data. All differences in performance are statistically sig- nificant. * -T3 was only trained on 30K sentence pairs  for performance reasons.", "labels": [], "entities": []}, {"text": " Table 5: BLEU scores for Moses and Moses+Del on sen- tences where the system made a change. \"correct change\"  shows the score where a change was made by the system  as well as in the reference and \"incorrect change\" where  a change was made by the system, but not the reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992119073867798}]}]}