{"title": [{"text": "The RWTH System Combination System for WMT 2011", "labels": [], "entities": [{"text": "WMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.89784836769104}]}], "abstractContent": [{"text": "RWTH participated in the System Combination task of the Sixth Workshop on Statistical Machine Translation (WMT 2011).", "labels": [], "entities": [{"text": "RWTH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9302890300750732}, {"text": "System Combination task of the Sixth Workshop on Statistical Machine Translation (WMT 2011)", "start_pos": 25, "end_pos": 116, "type": "TASK", "confidence": 0.7139052132765452}]}, {"text": "For three language pairs, we combined 6 to 14 systems into a single consensus translation.", "labels": [], "entities": []}, {"text": "A three-level meta-combination scheme combining six different system combination setups with three different engines was applied on the French-English language pair.", "labels": [], "entities": []}, {"text": "Depending on the language pair, improvements versus the best single system are in the range of +1.9% and +2.5% abs.", "labels": [], "entities": []}, {"text": "on BLEU, and between \u22121.8% and \u22122.4% abs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9613628387451172}]}, {"text": "Novel techniques compared with RWTH's submission to WMT 2010 include two additional system combination engines, an additional word alignment technique, meta combination, and additional optimization techniques.", "labels": [], "entities": [{"text": "RWTH's submission to WMT 2010", "start_pos": 31, "end_pos": 60, "type": "DATASET", "confidence": 0.8120338221391042}, {"text": "word alignment", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.7559832334518433}, {"text": "meta combination", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.6740086227655411}]}], "introductionContent": [{"text": "RWTH's main approach to System Combination (SC) for Machine Translation (MT) is a refined version of the ROVER approach in Automatic Speech Recognition (ASR), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses.", "labels": [], "entities": [{"text": "RWTH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8905655145645142}, {"text": "System Combination (SC) for Machine Translation (MT)", "start_pos": 24, "end_pos": 76, "type": "TASK", "confidence": 0.7487950216640126}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 123, "end_pos": 157, "type": "TASK", "confidence": 0.8198997875054678}]}, {"text": "The basic concept of the approach has been described by.", "labels": [], "entities": []}, {"text": "Several improvements have been added later (.", "labels": [], "entities": []}, {"text": "This approach includes an enhanced alignment and reordering framework.", "labels": [], "entities": []}, {"text": "In contrast to existing approaches (Jayaraman and, the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment.", "labels": [], "entities": []}, {"text": "Majority voting on the generated lattice is performed using prior weights for each system as well as other statistical models such as a special n-gram language model.", "labels": [], "entities": [{"text": "Majority voting", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7935046553611755}]}, {"text": "True casing is considered a separate step in RWTH's approach, which also takes the input hypotheses into account.", "labels": [], "entities": [{"text": "RWTH", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.6738738417625427}]}, {"text": "The pipeline, and consequently the description of the main pipeline given in this paper, is based on our pipeline for WMT 2010, with extensions as described.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.8768049776554108}]}, {"text": "When necessary, we denote this pipeline as Align-toLattice, or A2L . For the French-English task, we used two additional system combination engines for the first time: The first one uses the same alignments as A2L, but generates lattices in the OpenFST framework ().", "labels": [], "entities": []}, {"text": "The OpenFST decoder (fstshortestpath) is then used to find the best path (consensus translation) in this lattice.", "labels": [], "entities": []}, {"text": "Analogously, we call this engine A2FST . The second additional engine, which we call SCUNC, uses a TER-based alignment, similar to the approach by.", "labels": [], "entities": [{"text": "A2FST", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.7215304970741272}, {"text": "TER-based", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9661018252372742}]}, {"text": "Instead of a lattice rescoring, finding the consensus translation is considered a per-node classification problem: For each slot, which one is the \"correct\" one (i.e. will give the \"best\" output)?", "labels": [], "entities": []}, {"text": "This approach is inspired by iROVER (. Consensus translations from different settings of these approaches could then be combined again by an additional application of system combination -which we refer to as meta combination).", "labels": [], "entities": []}, {"text": "These three approaches are described in more detail in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3 we describe how we tuned the parameters and decisions of our system combination approaches for WMT 2011.", "labels": [], "entities": [{"text": "WMT 2011", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.6858698427677155}]}, {"text": "Section 4 then lists our experimental setup as well as the experimental results we obtained on the WMT 2011 system combination track.", "labels": [], "entities": [{"text": "WMT 2011 system combination track", "start_pos": 99, "end_pos": 132, "type": "DATASET", "confidence": 0.9370533108711243}]}, {"text": "We conclude this paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each language pair in WMT 2011 had its own set of systems, so we selected and tuned separately for each language pair . Due to time constraints, we only participated in tasks with English as the target language.", "labels": [], "entities": [{"text": "WMT 2011", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8630090951919556}]}, {"text": "In preliminary experiments, it turned out that System Combination was notable to get a better result than the best single system on the Czech-English task.", "labels": [], "entities": [{"text": "System Combination", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7967877686023712}]}, {"text": "Consequently, we focused on the language pairs French-English, GermanEnglish, and Spanish-English.", "labels": [], "entities": []}, {"text": "We split the available tuning data documentwise into a 609-line TUNE set (for tuning), and a 394-line DEV set (to verify tuning results).", "labels": [], "entities": [{"text": "TUNE", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9756381511688232}]}, {"text": "More statistics on these sets can be found in.", "labels": [], "entities": []}, {"text": "Unfortunately, late in the evaluation campaign it turned out that the quality of several reference sentences used in TUNE and DEV was rather low: Many reference sentences contained spelling errors, a few dozen lines even contained French phrases or sentences within or after the English text.", "labels": [], "entities": [{"text": "TUNE", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.7167422771453857}, {"text": "DEV", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.5672219395637512}]}, {"text": "We corrected many of these errors manually in the references.", "labels": [], "entities": []}, {"text": "In total 101 of 690 lines (16.6%) in TUNE and 58 of 394 lines (14.7%) in DEV were affected by this.", "labels": [], "entities": [{"text": "TUNE", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9371117353439331}, {"text": "DEV", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9253843426704407}]}, {"text": "While it was too late to rerun all of the optimization runs, we re-optimized at least a few final systems.", "labels": [], "entities": []}, {"text": "All scores within this section were calculated on the corrected reference translations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus and Task statistics.", "labels": [], "entities": []}, {"text": " Table 3: Results for FR-EN.  TUNE  DEV  BLEU TER BLEU TER", "labels": [], "entities": [{"text": "FR-EN", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.43066105246543884}, {"text": "TUNE  DEV  BLEU TER BLEU TER", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.6443747778733572}]}, {"text": " Table 4: Results for DE-EN.  TUNE  DEV  BLEU TER BLEU TER", "labels": [], "entities": [{"text": "DE-EN.  TUNE  DEV  BLEU TER BLEU TER", "start_pos": 22, "end_pos": 58, "type": "METRIC", "confidence": 0.7260709926486015}]}, {"text": " Table 5: Results for ES-EN.  TUNE  DEV  BLEU TER BLEU TER", "labels": [], "entities": [{"text": "ES-EN", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.6899681687355042}, {"text": "TUNE  DEV  BLEU TER BLEU TER", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.6680201987425486}]}]}