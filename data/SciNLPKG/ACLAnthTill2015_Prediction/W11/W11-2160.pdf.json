{"title": [{"text": "Joshua 3.0: Syntax-based Machine Translation with the Thrax Grammar Extractor", "labels": [], "entities": [{"text": "Syntax-based Machine Translation", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.6738363405068716}]}], "abstractContent": [{"text": "We present progress on Joshua, an open-source decoder for hierarchical and syntax-based machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7413079142570496}]}, {"text": "The main focus is describing Thrax, a flexible, open source synchronous context-free grammar ex-tractor.", "labels": [], "entities": []}, {"text": "Thrax extracts both hierarchical (Chi-ang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars.", "labels": [], "entities": [{"text": "syntax-augmented machine translation", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.6308100521564484}]}, {"text": "It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars , feature functions, and output formats.", "labels": [], "entities": []}], "introductionContent": [{"text": "Joshua is an open-source 1 toolkit for hierarchical machine translation of human languages.", "labels": [], "entities": [{"text": "hierarchical machine translation of human languages", "start_pos": 39, "end_pos": 90, "type": "TASK", "confidence": 0.7999664694070816}]}, {"text": "The original version of Joshua () was a reimplementation of the Python-based Hiero machinetranslation system; it was later extended ( to support richer formalisms, such as SAMT ().", "labels": [], "entities": []}, {"text": "The main focus of this paper is to describe this past year's work in developing Thrax, an open-source grammar extractor for Hiero and SAMT grammars.", "labels": [], "entities": [{"text": "Hiero and SAMT grammars", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.6347874850034714}]}, {"text": "Grammar extraction has shown itself to be something of a black art, with decoding performance depending crucially on a variety of features and options that are not always clearly described in papers.", "labels": [], "entities": [{"text": "Grammar extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.849567174911499}]}, {"text": "This hindered direct comparison both between and within grammatical formalisms.", "labels": [], "entities": []}, {"text": "Thrax standardizes Joshua's grammar ex-1 http://github.com/joshua-decoder/joshua traction procedures by providing a flexible and configurable means of specifying these settings.", "labels": [], "entities": []}, {"text": "Section 3 presents a systematic comparison of the two grammars using identical feature sets.", "labels": [], "entities": []}, {"text": "In addition, Joshua now includes a single parameterized script that implements the entire MT pipeline, from data preparation to evaluation.", "labels": [], "entities": [{"text": "MT pipeline", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.8784018754959106}, {"text": "data preparation", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.6801924109458923}]}, {"text": "This script is built on top of a module called CachePipe.", "labels": [], "entities": []}, {"text": "CachePipe is a simple wrapper around shell commands that uses SHA-1 hashes and explicitlyprovided lists of dependencies to determine whether a command needs to be run, saving time both in running and debugging machine translation pipelines.", "labels": [], "entities": []}], "datasetContent": [{"text": "We built systems for six language pairs for the WMT 2011 shared task: cz-en, en-cz, de-en, en-de, fr-en, and en-fr.", "labels": [], "entities": [{"text": "WMT 2011 shared task", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.591942086815834}]}, {"text": "For each language pair, we built both SAMT and hiero grammars.", "labels": [], "entities": [{"text": "SAMT", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.6715529561042786}]}, {"text": "4 contains the results on the complete WMT 2011 test set.", "labels": [], "entities": [{"text": "WMT 2011 test set", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.8536290973424911}]}, {"text": "To train the translation models, we used the provided Europarl and news commentary data.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9634098410606384}, {"text": "Europarl and news commentary data", "start_pos": 54, "end_pos": 87, "type": "DATASET", "confidence": 0.7795235574245453}]}, {"text": "For czen and en-cz, we also used sections of the CzEng parallel corpus (Bojar and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2009).", "labels": [], "entities": [{"text": "Bojar and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2009)", "start_pos": 72, "end_pos": 130, "type": "DATASET", "confidence": 0.8011400103569031}]}, {"text": "The parallel data was subsampled using Joshua's builtin subsampler to select sentences with n-grams relevant to the tuning and test set.", "labels": [], "entities": []}, {"text": "We used SRILM to train a 5-gram language model with Kneser-Ney smoothing using the appropriate side of the parallel data.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.6682841181755066}]}, {"text": "For the English LM, we also used English Gigaword Fourth Edition.", "labels": [], "entities": [{"text": "English Gigaword Fourth Edition", "start_pos": 33, "end_pos": 64, "type": "DATASET", "confidence": 0.8835873007774353}]}, {"text": "Before extracting an SCFG with Thrax, we used the provided Perl scripts to tokenize and normalize 3 fr=French, cz=Czech, de=German, en=English.", "labels": [], "entities": []}, {"text": "Except for fr-en and en-fr.", "labels": [], "entities": []}, {"text": "We were unable to decode with SAMT grammars for these language pairs due to their large size.", "labels": [], "entities": []}, {"text": "We have since resolved this issue and will have scores for the final version of the paper.: Single-reference BLEU-4 scores.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9693098664283752}]}, {"text": "We also removed any sentences longer than 50 tokens (after tokenization).", "labels": [], "entities": []}, {"text": "For SAMT grammar extraction, we parsed the English training data using the Berkeley Parser () with the provided Treebank-trained grammar.", "labels": [], "entities": [{"text": "SAMT grammar extraction", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9361472924550375}]}, {"text": "We tuned the model weights against the WMT08 test set (news-test2008) using Z-MERT, an implementation of minimum error-rate training included with Joshua.", "labels": [], "entities": [{"text": "WMT08 test set", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9662828842798868}]}, {"text": "We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking ().", "labels": [], "entities": [{"text": "Minimum Bayes Risk reranking", "start_pos": 129, "end_pos": 157, "type": "METRIC", "confidence": 0.6808901876211166}]}, {"text": "shows an example derivation with an SAMT grammar.", "labels": [], "entities": []}, {"text": "To re-case the 1-best test set output, we trained a true-case 5-gram language model using the same LM training data as before, and used an SCFG translation model to translate from the lowercased to true-case output.", "labels": [], "entities": []}, {"text": "The translation model used rules limited to five tokens in length, and contained no hierarchical rules.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.962406575679779}]}], "tableCaptions": [{"text": " Table 2: Training data size after subsampling.", "labels": [], "entities": []}, {"text": " Table 3: Single-reference BLEU-4 scores.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9733539819717407}]}]}