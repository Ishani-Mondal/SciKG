{"title": [{"text": "CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes", "labels": [], "entities": [{"text": "CoNLL-2011", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.837764322757721}, {"text": "Modeling Unrestricted Coreference", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.8378998041152954}]}], "abstractContent": [{"text": "The CoNLL-2011 shared task involved predicting coreference using OntoNotes data.", "labels": [], "entities": [{"text": "predicting coreference", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8308224380016327}, {"text": "OntoNotes data", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.8918443322181702}]}, {"text": "Resources in this field have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities.", "labels": [], "entities": [{"text": "noun phrase coreference", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.6726935903231303}]}, {"text": "OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types.", "labels": [], "entities": []}, {"text": "OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8240493535995483}]}, {"text": "This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information , and evaluation criteria, and presents and discusses the results achieved by the participating systems.", "labels": [], "entities": []}, {"text": "Having a standard test set and evaluation parameters, all based on anew resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.", "labels": [], "entities": []}], "introductionContent": [{"text": "The importance of coreference resolution for the entity/event detection task, namely identifying all mentions of entities and events in text and clustering them into equivalence classes, has been well recognized in the natural language processing community.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.9251162707805634}, {"text": "entity/event detection task", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7449718713760376}]}, {"text": "Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data.", "labels": [], "entities": [{"text": "Automatic identification of coreferring entities and events in text", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8496089908811781}]}, {"text": "Early work on corpus-based coreference resolution dates back to the mid-90s by where they experimented with using decision trees and hand-written rules.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.9112965166568756}]}, {"text": "A systematic study was then conducted using decision trees by.", "labels": [], "entities": []}, {"text": "Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward.", "labels": [], "entities": [{"text": "language processing", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7295966148376465}, {"text": "coreference resolution", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.9604343473911285}]}, {"text": "Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited.", "labels": [], "entities": []}, {"text": "Researchers continued finding novel ways of exploiting ontologies such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9730929136276245}]}, {"text": "Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia ().", "labels": [], "entities": []}, {"text": "In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs' distance, etc.", "labels": [], "entities": []}, {"text": "A better idea of the progress in the field can be obtained by reading recent survey articles and tutorials) dedicated to this subject.", "labels": [], "entities": []}, {"text": "Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC).", "labels": [], "entities": [{"text": "Message Understanding Conferences (MUC)", "start_pos": 69, "end_pos": 108, "type": "TASK", "confidence": 0.8209266861279806}]}, {"text": "These corpora were tagged with coreferring entities identified by noun phrases in the text.", "labels": [], "entities": []}, {"text": "The de facto standard datasets for current coreference studies are the MUC) and the ACE 1 (G.) corpora.", "labels": [], "entities": [{"text": "MUC)", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.8998956084251404}, {"text": "ACE 1 (G.) corpora", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.8561324973901113}]}, {"text": "The MUC corpora coverall noun phrases in text, but represent small training and test sets.", "labels": [], "entities": [{"text": "MUC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.776202917098999}]}, {"text": "The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities.", "labels": [], "entities": [{"text": "ACE corpora", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9209239184856415}]}, {"text": "They are also less consistent, in terms of inter-annotator agreement (ITA).", "labels": [], "entities": []}, {"text": "This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models.", "labels": [], "entities": [{"text": "reliability", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.960968554019928}]}, {"text": "The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past).", "labels": [], "entities": []}, {"text": "There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation -both of which seek to take information access technology to the next level -we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7334236949682236}, {"text": "question answering", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.8619163930416107}, {"text": "entity and event identification", "start_pos": 352, "end_pos": 383, "type": "TASK", "confidence": 0.597526490688324}]}, {"text": "Identification and encoding of richer knowledge -possibly linked to knowledge sources -and development of learning algorithms that would effectively incorporate them is a necessary next step towards improving the current state of the art.", "labels": [], "entities": []}, {"text": "The computational learning community, in general, is also witnessing a move towards evaluations based on joint inference, with the two previous CoNLL tasks () devoted to joint learning of syntactic and semantic dependencies.", "labels": [], "entities": []}, {"text": "A principle ingredient for joint learning is the presence of multiple layers of semantic information.", "labels": [], "entities": []}, {"text": "One fundamental question still remains, and that is -what would it take to improve the state of the art in coreference resolution that has not been attempted so far?", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.9820943176746368}]}, {"text": "Many different algorithms have been tried in the past 15 years, but one thing that is still lacking is a corpus comprehensively tagged on a large scale with consistent, multiple layers of semantic information.", "labels": [], "entities": []}, {"text": "One of the many goals of the OntoNotes project 2 () is to explore whether it can fill this void and help push the progress further -not only in coreference, but with the various layers of semantics that it tries to capture.", "labels": [], "entities": []}, {"text": "As one of its layers, it has created a corpus for general anaphoric coreference that cov-ers entities and events not limited to noun phrases or a limited set of entity types.", "labels": [], "entities": [{"text": "general anaphoric coreference", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6327812075614929}]}, {"text": "A small portion of this corpus from the newswire and broadcast news genres (\u223c120k) was recently used fora SEMEVAL task (.", "labels": [], "entities": [{"text": "SEMEVAL task", "start_pos": 106, "end_pos": 118, "type": "TASK", "confidence": 0.8120164275169373}]}, {"text": "As mentioned earlier, the coreference layer in OntoNotes constitutes just one part of a multi-layered, integrated annotation of shallow semantic structure in text with high interannotator agreement, which also provides a unique opportunity for performing joint inference over a substantial body of data.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents an overview of the OntoNotes corpus.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.7883048355579376}]}, {"text": "Section 3 describes the coreference annotation in OntoNotes.", "labels": [], "entities": []}, {"text": "Section 4 then describes the shared task, including the data provided and the evaluation criteria.", "labels": [], "entities": []}, {"text": "Sections 5 and 6 then describe the participating system results and analyze the approaches, and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the evaluation criteria used.", "labels": [], "entities": []}, {"text": "Unlike for propositions, word sense and named entities, where it is simply a matter of counting the correct answers, or for parsing, where there are several established metrics, evaluating the accuracy of coreference continues to be contentious.", "labels": [], "entities": [{"text": "parsing", "start_pos": 124, "end_pos": 131, "type": "TASK", "confidence": 0.9717227220535278}, {"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9940420985221863}]}, {"text": "Various al-  In order to determine the best performing system in the shared task, we needed to associate a single number with each system.", "labels": [], "entities": []}, {"text": "This could have been one of the metrics above, or some combination of more than one of them.", "labels": [], "entities": []}, {"text": "The choice was not simple, and while we consulted various researchers in the field, hoping fora strong consensus, their conclusion seemed to be that each metric had its pros and cons.", "labels": [], "entities": []}, {"text": "We settled on the MELA metric by, which takes a weighted average of three metrics: MUC, B-CUBED, and CEAF.", "labels": [], "entities": [{"text": "MELA", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.6913835406303406}, {"text": "MUC", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.5125556588172913}, {"text": "B-CUBED", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9904629588127136}, {"text": "CEAF", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.6323990225791931}]}, {"text": "The rationale for the combination is that each of the three metrics represents a different important dimension, the MUC measure being based on links, the B-CUBED based on mentions, and the CEAF based on entities.", "labels": [], "entities": [{"text": "MUC measure", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.8713635802268982}, {"text": "B-CUBED", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9920185208320618}, {"text": "CEAF", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.7782934308052063}]}, {"text": "For a given task, a weighted average of the three might be optimal, but since we don't have an end task in mind, we decided to use the unweighted mean of the three metrics as the score on which the winning system was judged.", "labels": [], "entities": []}, {"text": "We decided to use CEAF e instead of CEAF m .", "labels": [], "entities": [{"text": "CEAF e", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.6495586037635803}]}], "tableCaptions": [{"text": " Table 1: Inter Annotator and Adjudicator agreement for  the Coreference Layer in OntoNotes measured in terms  of the MUC score.", "labels": [], "entities": [{"text": "MUC score", "start_pos": 118, "end_pos": 127, "type": "DATASET", "confidence": 0.7794775068759918}]}, {"text": " Table 2: Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets. The  numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.  Each part was considered a separate document for evaluation purposes.", "labels": [], "entities": [{"text": "OntoNotes data", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.9270723462104797}, {"text": "MUC and ACE data sets", "start_pos": 82, "end_pos": 103, "type": "DATASET", "confidence": 0.8003109931945801}, {"text": "OntoNotes corpus", "start_pos": 141, "end_pos": 157, "type": "DATASET", "confidence": 0.8441810309886932}]}, {"text": " Table 3: Distribution of mentions in the data by their syn- tactic category.", "labels": [], "entities": []}, {"text": " Table 4: Number of entities, links and mentions in the  OntoNotes 4.0 data.", "labels": [], "entities": [{"text": "OntoNotes 4.0 data", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.8853480418523153}]}, {"text": " Table 6: Word sense polysemy over verb and noun lem- mas in OntoNotes", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.7852781414985657}]}, {"text": " Table 5: Parser performance on the CoNLL-2011 test set", "labels": [], "entities": [{"text": "Parser", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8983009457588196}, {"text": "CoNLL-2011 test set", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9768395026524862}]}, {"text": " Table 8: Performance on the propositions and framesets in the CoNLL-2011 test set.", "labels": [], "entities": [{"text": "CoNLL-2011 test set", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.9707896908124288}]}, {"text": " Table 7: Word sense performance over both verbs and  nouns in the CoNLL-2011 test set", "labels": [], "entities": [{"text": "CoNLL-2011 test set", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.9591493407885233}]}, {"text": " Table 9: Frameset polysemy across lemmas", "labels": [], "entities": []}, {"text": " Table 10: Named Entity performance on the CoNLL- 2011 test set", "labels": [], "entities": [{"text": "CoNLL- 2011 test set", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.9588006734848022}]}, {"text": " Table 11: Format of the .conll file used on the shared task", "labels": [], "entities": []}, {"text": " Table 12: Performance of systems in the official, closed", "labels": [], "entities": []}, {"text": " Table 13: Performance of systems in the official, open", "labels": [], "entities": []}, {"text": " Table 14: Performance of systems in the supplementary closed", "labels": [], "entities": []}, {"text": " Table 16: Performance of systems in the supplementary, closed", "labels": [], "entities": []}, {"text": " Table 17: Performance of systems in the supplementary, open", "labels": [], "entities": []}, {"text": " Table 18: Head word based  performance of systems in the official, closed", "labels": [], "entities": []}, {"text": " Table 21: Detailed look at the performance per genre for  the official, open track using predicted information. MD  represents MENTION DETECTION; BCUB represents B- CUBED; C m represents CEAF m ; C e represents CEAF e and  O represents the OFFICIAL score.", "labels": [], "entities": [{"text": "MENTION DETECTION", "start_pos": 128, "end_pos": 145, "type": "METRIC", "confidence": 0.7620558738708496}, {"text": "BCUB", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.990450918674469}, {"text": "B- CUBED", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.8360888759295145}, {"text": "OFFICIAL score", "start_pos": 241, "end_pos": 255, "type": "METRIC", "confidence": 0.9221500158309937}]}]}