{"title": [{"text": "Evaluating sentence compression: Pitfalls and suggested remedies", "labels": [], "entities": [{"text": "Evaluating sentence compression", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.763165553410848}]}], "abstractContent": [{"text": "This work surveys existing evaluation methodologies for the task of sentence compression, identifies their shortcomings, and proposes alternatives.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7432016730308533}]}, {"text": "In particular, we examine the problems of evaluating paraphrastic compression and comparing the output of different models.", "labels": [], "entities": [{"text": "paraphrastic compression", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6995788067579269}]}, {"text": "We demonstrate that compression rate is a strong predictor of compression quality and that perceived improvement over other models is often aside effect of producing longer output.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 20, "end_pos": 36, "type": "METRIC", "confidence": 0.9402965307235718}]}], "introductionContent": [{"text": "Sentence compression is the natural language generation (NLG) task of automatically shortening sentences.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9140394330024719}, {"text": "natural language generation (NLG)", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.8236156205336252}]}, {"text": "Because good compressions should be grammatical and retain important meaning, they must be evaluated along these two dimensions.", "labels": [], "entities": []}, {"text": "Evaluation is a difficult problem for NLG, and many of the problems identified in this work are relevant for other generation tasks.", "labels": [], "entities": []}, {"text": "Shared tasks are popular in many areas as away to compare system performance in an unbiased manner.", "labels": [], "entities": []}, {"text": "Unlike other tasks, such as machine translation, there is no shared-task evaluation for compression, even though some compression systems are indirectly evaluated as apart of DUC.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8235869109630585}]}, {"text": "The benefits of shared-task evaluation have been discussed before (e.g., and), and they include comparing systems fairly under the same conditions.", "labels": [], "entities": []}, {"text": "One difficulty in evaluating compression systems fairly is that an unbiased automatic metric is hard to define.", "labels": [], "entities": []}, {"text": "Automatic evaluation relies on a comparison to a single gold standard at a predetermined length, which greatly limits the types of compressions that can be fairly judged.", "labels": [], "entities": []}, {"text": "As we will discuss in Section 2.1.1, automatic evaluation assumes that deletions are independent, considers only a single gold standard, and cannot handle compressions with paraphrasing.", "labels": [], "entities": []}, {"text": "Like for most areas in NLG, human evaluation is preferable.", "labels": [], "entities": []}, {"text": "However, as we discuss in Section 2.2, there are some subtleties to appropriate experiment design, which can give misleading results if not handled properly.", "labels": [], "entities": []}, {"text": "This work identifies the shortcomings of widely practiced evaluation methodologies and proposes alternatives.", "labels": [], "entities": []}, {"text": "We report on the effect of compression rate on perceived quality and suggest ways to control for this dependency when evaluating across different systems.", "labels": [], "entities": []}, {"text": "In this work we: \u2022 highlight the importance of comparing systems with similar compression rates, \u2022 argue that comparisons in many previous publications are invalid, \u2022 provide suggestions for unbiased evaluation.", "labels": [], "entities": []}, {"text": "While many may find this discussion intuitive, these points are not addressed in much of the existing research, and therefore it is crucial to enumerate them in order to improve the scientific validity of the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Automatic evaluation operates under three often incorrect assumptions: Deletions are independent.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7420749664306641}]}, {"text": "The dependency structure of a sentence maybe unaltered when dependent words are not deleted as a unit.", "labels": [], "entities": []}, {"text": "Examples of words that should be treated as a single unit include negations and negative polarity items or certain multi-word phrases (such as deleting Latin and leaving America).", "labels": [], "entities": []}, {"text": "F1 treats all deletions equally, when in fact errors of this type may dramatically alter the meaning or the grammaticality of a sentence and should be penalized more than less serious errors, such as deleting an article.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9838889837265015}]}, {"text": "The gold standard is the single best compression.", "labels": [], "entities": [{"text": "compression", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9819376468658447}]}, {"text": "Automatic evaluation considers a single gold-standard compression.", "labels": [], "entities": []}, {"text": "This ignores the possibility of different length compressions and equally good compressions of the same length, where multiple non-overlapping deletions are acceptable.", "labels": [], "entities": []}, {"text": "For an example, see.", "labels": [], "entities": []}, {"text": "Having multiple gold standards would provide references at different compression lengths and reflect different deletion choices (see Section 3).", "labels": [], "entities": []}, {"text": "Since no large corpus with multiple gold standards exists to our knowledge, systems could instead report the quality of compressions at several different compression rates, as Nomoto (2008) did.", "labels": [], "entities": []}, {"text": "Alternatively, systems could evaluate compressions that are of a similar length as the gold standard compression, to fix a length for the purpose of evaluation.", "labels": [], "entities": []}, {"text": "Output length is controlled for evaluation in some other areas, notably DUC.", "labels": [], "entities": [{"text": "Output length", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8922466337680817}, {"text": "DUC", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.7233337163925171}]}, {"text": "Systems compress by deletion and not substitution.", "labels": [], "entities": []}, {"text": "More recent approaches to compression introduce reordering and paraphrase operations (e.g., dencies) while there are over 50 Stanford Dependencies ().,).", "labels": [], "entities": []}, {"text": "For paraphrastic compressions, manual evaluation alone reliably determines the compression quality.", "labels": [], "entities": [{"text": "paraphrastic compressions", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7226141691207886}]}, {"text": "Because automatic evaluation metrics compare shortened sentences to extractive gold standards, they cannot be applied to paraphrastic compression.", "labels": [], "entities": [{"text": "paraphrastic compression", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.7727012038230896}]}, {"text": "To apply automatic techniques to substitutionbased compression, one would need a gold-standard set of paraphrastic compressions.", "labels": [], "entities": [{"text": "substitutionbased compression", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7508808970451355}]}, {"text": "created an abstractive corpus, which contains word reordering and paraphrasing in addition to deletion.", "labels": [], "entities": []}, {"text": "Unfortunately, this corpus is small (575 sentences) and only includes one possible compression for each sentence.", "labels": [], "entities": []}, {"text": "Other alternatives include deriving such corpora from existing corpora of multi-reference translations.", "labels": [], "entities": []}, {"text": "The longest reference translation can be paired with the shortest reference to represent along sentence and corresponding paraphrased goldstandard compression.", "labels": [], "entities": []}, {"text": "Similar to machine translation or summarization, automatic translation of paraphrastic compressions would require multiple references to capture allowable variation, since there are often many equally valid ways of compressing an input.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7774962782859802}, {"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7498765587806702}, {"text": "automatic translation of paraphrastic compressions", "start_pos": 49, "end_pos": 99, "type": "TASK", "confidence": 0.6956605792045594}]}, {"text": "ROUGE or BLEU could be applied to a set of multiplereference compressions, although BLEU is not without its own shortcomings).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9524902105331421}, {"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9943649768829346}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9839762449264526}]}, {"text": "One benefit of both ROUGE and BLEU is that they are based on n-gram recall and precision (respectively) instead of word-error rate, so reordering and word substitutions can be evaluated.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9931219220161438}, {"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.99736088514328}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9753445982933044}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9974305033683777}, {"text": "word substitutions", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.7265042960643768}]}, {"text": "used BLEU for evaluation in the context of headline generation, which uses rewording and is related to sentence compression.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9968247413635254}, {"text": "headline generation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8339519500732422}, {"text": "sentence compression", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.7497868239879608}]}, {"text": "Alternatively, manual evalation can be adapted from other NLG domains, such as the techniques described in the following section.", "labels": [], "entities": []}, {"text": "In order to determine semantic and syntactic suitability, manual evaluation is preferable over automatic techniques whenever possible.", "labels": [], "entities": []}, {"text": "The most widely practiced manual evaluation methodology was first used by.", "labels": [], "entities": []}, {"text": "Judges grade each compressed sentence against the original and make two separate decisions: how grammatical is the compression and how much of the meaning from the original sentence is preserved.", "labels": [], "entities": []}, {"text": "Decisions are rated along a 5-point scale).", "labels": [], "entities": []}, {"text": "Most compression systems consider sentences out of context (a few exceptions exist, e.g.,,, and).", "labels": [], "entities": []}, {"text": "Contextual cues and discourse structure may not be a factor to consider if the sentences are generated for use out of context.", "labels": [], "entities": []}, {"text": "An example of a context-aware approach considered the summaries formed by shortened sentences and evaluated the compression systems based on how well people could answer questions about the original document from the summaries.", "labels": [], "entities": [{"text": "summaries formed by shortened sentences", "start_pos": 54, "end_pos": 93, "type": "TASK", "confidence": 0.8718902945518494}]}, {"text": "This technique has been used before for evaluating summarization and text comprehension ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9492977261543274}]}, {"text": "Grammar judgments decrease when the compression is presented alongside the original sentence.", "labels": [], "entities": []}, {"text": "shows that the mean grammar rating for the same compressions is on average about 0.3 points higher when the compression is judged in isolation.", "labels": [], "entities": [{"text": "grammar rating", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.8836280703544617}]}, {"text": "Researchers should be careful to state when grammar is judged on compressions lacking reference sentences.", "labels": [], "entities": []}, {"text": "Another factor is the group of judges.", "labels": [], "entities": []}, {"text": "Obviously different studies will rely on different judges, so whenever possible the sentences from an existing model should be re-evaluated alongside the new model.", "labels": [], "entities": []}, {"text": "The \"McD\" entries in represent a set of sentences generated from the exact same model evaluated by two different sets of judges.", "labels": [], "entities": []}, {"text": "The mean grammar and meaning ratings in each evaluation setup differ by 0.5-0.7 points.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Three acceptable compressions of a sentence created by different annotators (the first is the original).", "labels": [], "entities": []}, {"text": " Table 2: Mean quality ratings of two competing mod- els once the compression rates have been standardized,  and as reported in the original work (denoted  *  ). There  is no significant improvement, but the numerically better  model changes.", "labels": [], "entities": []}]}