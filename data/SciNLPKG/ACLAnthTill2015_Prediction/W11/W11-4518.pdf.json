{"title": [{"text": "Part-of-Speech Tagging of Portuguese Using Hidden Markov Models with Character Language Model Emissions", "labels": [], "entities": [{"text": "Part-of-Speech Tagging of Portuguese", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.841788649559021}]}], "abstractContent": [{"text": "This paper presents a probabilistic approach for POS tagging that combines HMMs and character language models being applied to Portuguese texts.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9484325647354126}]}, {"text": "In this approach, the emission probabilities for each hidden state in a HMM are estimated by a proper character language model.", "labels": [], "entities": []}, {"text": "The tagger built has been trained and tested on Bosque, a subset of Floresta Sint\u00e1(c)tica treebank, reaching 96.2% accuracy with a 39-tag tagset and 92.0% with a 257-tag tagset extended with inflexion information.", "labels": [], "entities": [{"text": "Bosque", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.5669350028038025}, {"text": "Floresta Sint\u00e1(c)tica treebank", "start_pos": 68, "end_pos": 98, "type": "DATASET", "confidence": 0.8939286981310163}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9993817806243896}]}], "introductionContent": [{"text": "Some advances in part-of-speech (POS) tagging techniques still have not been fully explored for other languages than English.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.6136857986450195}]}, {"text": "Works in this field with Portuguese include Bick's rule-based tagger, with over 99% accuracy, which is a very impressive performance.", "labels": [], "entities": [{"text": "Bick's rule-based tagger", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.4340856671333313}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.997184693813324}]}, {"text": "However, probabilistic approaches applied to Portuguese have not reached the same levels achieved with English (96-97%) yet.", "labels": [], "entities": []}, {"text": "In this paper we present the results obtained using a probabilistic approach that combines hidden Markov models (HMMs) and character language models for POS tagging of Portuguese.", "labels": [], "entities": [{"text": "POS tagging of Portuguese", "start_pos": 153, "end_pos": 178, "type": "TASK", "confidence": 0.8627138286828995}]}, {"text": "The remainder of this paper is structured as follows: in Section 2 the background for the addressed problem is discussed; in Section 3 we introduce the approach taken in this work; Section 4 presents the corpus used for training and testing the tagger; in Section 5 we define the experiment setup; Section 6 displays the key results; in Section 7 conclusions and future works are discussed.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of this work was to evaluate the presented approach for POS tagging regarding accuracy (number of correctly tagged tokens / total number of tokens).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9273602366447449}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.999071478843689}]}, {"text": "In order to do so we have randomly partitioned the corpus sentences in two fixed sets: a training set containing approximately 90% of the corpus tokens and a test set containing the remaining tokens.", "labels": [], "entities": []}, {"text": "The first was used for training, testing and tuning the models during development with 10-fold cross-validation, and then the second (unseen during development) was used for testing the final models (trained on the entire training set).", "labels": [], "entities": []}, {"text": "Since tagsets may differ between works we firstly defined a lower-bound baseline for comparison purposes.", "labels": [], "entities": []}, {"text": "This baseline was a unigram POS tagger.", "labels": [], "entities": []}, {"text": "As stated in Section 3, our approach presents HMMs with character language model emissions and those language models are based on n-grams.", "labels": [], "entities": []}, {"text": "So, there was one parameter to be tuned in our model: max n-gram size for HMM emissions.", "labels": [], "entities": [{"text": "max n-gram size", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.9235124389330546}]}, {"text": "This tuning was done by varying max n-gram size on a range of values and, for each value, training and testing a corresponding model.", "labels": [], "entities": []}, {"text": "This range has the lower bound 1 (unigram) and some upper bound defined by max length of words in the language (setting max n-gram sizes greater than max word length would be useless).", "labels": [], "entities": []}, {"text": "Additionally, confusion matrices were built on each test.", "labels": [], "entities": []}, {"text": "They were helpful for error analysis, revealing most common sources of tagging mistakes.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.6674410551786423}]}], "tableCaptions": []}