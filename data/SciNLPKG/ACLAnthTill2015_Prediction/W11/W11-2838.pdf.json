{"title": [{"text": "Helping Our Own: The HOO 2011 Pilot Shared Task", "labels": [], "entities": [{"text": "HOO 2011 Pilot Shared Task", "start_pos": 21, "end_pos": 47, "type": "DATASET", "confidence": 0.705579137802124}]}], "abstractContent": [{"text": "The aim of the Helping Our Own (HOO) Shared Task is to promote the development of automated tools and techniques that can assist authors in the writing task, with a specific focus on writing within the natural language processing community.", "labels": [], "entities": []}, {"text": "This paper reports on the results of a pilot run of the shared task, in which six teams participated.", "labels": [], "entities": []}, {"text": "We describe the nature of the task and the data used, report on the results achieved, and discuss some of the things we learned that will guide future versions of the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Helping Our Own (HOO) Shared Task aims to promote the development of automated tools and techniques that can assist authors in the writing task.", "labels": [], "entities": []}, {"text": "The task focusses specifically on writing within the natural language processing community, on the grounds that content matter familiar to Shared Task participants will be more engaging than content matter from another discipline.", "labels": [], "entities": []}, {"text": "In addition, the ACL Anthology ( provides us with a large and freely-available collection of material in the appropriate domain and genre that can be used, for example, for language modelling; obtaining similar material in other disciplines is more difficult and potentially costly.", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9631454944610596}, {"text": "language modelling", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.8496525287628174}]}, {"text": "A broader discussion of the background to the HOO task can be found in (.", "labels": [], "entities": [{"text": "HOO task", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.7794070541858673}]}, {"text": "In this first pilot round of the task, we focussed on errors and infelicities introduced into text by non-native speakers (NNSs) of English.", "labels": [], "entities": []}, {"text": "While there are few native speakers who would not also have something to gain from the kinds of technologies we would like to see developed, the generally higher density of errors in texts authored by NNSs makes annotation of this material much more cost efficient than the annotation of native-speaker text.", "labels": [], "entities": []}, {"text": "The focus on English texts is for purely pragmatic reasons; obviously one could in principle pursue the goals discussed here for other languages too.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the development and test data that was provided to participants.", "labels": [], "entities": []}, {"text": "Then, in Section 3 we describe the approach taken to evaluation.", "labels": [], "entities": []}, {"text": "In Section 4, we summarise the results of the submissions from each of the six participating teams.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we make some observations on lessons learned and comment on plans for the future.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each team was allowed to submit up to 10 distinct 'runs', so that they could provide alternative outputs.", "labels": [], "entities": []}, {"text": "Evaluation then proceeds by comparing the set of gold-standard edit structures fora fragment with the set of edit structures corresponding to the participating team's output fora single run for that fragment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Best run scores for Detection, 'No Bonus' condition", "labels": [], "entities": [{"text": "Detection", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.49158230423927307}]}, {"text": " Table 3: Best run scores for Recognition, 'No Bonus' condition", "labels": [], "entities": [{"text": "Recognition", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.789604902267456}]}, {"text": " Table 4: Best run scores for Correction, 'No Bonus' condition", "labels": [], "entities": [{"text": "Correction", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9948849081993103}]}, {"text": " Table 5: Best run scores for Detection, 'Bonus' condition", "labels": [], "entities": [{"text": "Detection", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.536680281162262}]}, {"text": " Table 6: Best run scores for Recognition 'Bonus' condition", "labels": [], "entities": [{"text": "Recognition", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.5522782206535339}]}, {"text": " Table 7: Best run scores for Correction, 'Bonus' condition", "labels": [], "entities": [{"text": "Correction", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9948141574859619}]}, {"text": " Table 8: Edits by Type", "labels": [], "entities": []}, {"text": " Table 9: Best run scores for Article errors", "labels": [], "entities": []}, {"text": " Table 10: Best run scores for Punctuation errors", "labels": [], "entities": [{"text": "Punctuation errors", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7755860388278961}]}, {"text": " Table 11: Best run scores for Preposition errors", "labels": [], "entities": [{"text": "Preposition errors", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9080224633216858}]}, {"text": " Table 12: Best run scores for Noun errors", "labels": [], "entities": []}, {"text": " Table 13: Best run scores for Verb errors", "labels": [], "entities": [{"text": "Verb", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.4970579147338867}]}, {"text": " Table 14: Best run scores for Compound Change errors", "labels": [], "entities": [{"text": "Compound Change", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.6768937557935715}]}, {"text": " Table 15: Best run scores for Adjective errors", "labels": [], "entities": [{"text": "Adjective errors", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.9356294274330139}]}, {"text": " Table 16: Best run scores for Adverb errors", "labels": [], "entities": []}, {"text": " Table 17: Best run scores for Conjunction errors", "labels": [], "entities": [{"text": "Conjunction", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.6908213496208191}]}, {"text": " Table 18: Best run scores for Anaphor errors", "labels": [], "entities": [{"text": "Anaphor", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9633612632751465}]}, {"text": " Table 19: Best run scores for Spelling errors", "labels": [], "entities": [{"text": "Spelling errors", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.9108094274997711}]}, {"text": " Table 20: Best run scores for Quantifier errors", "labels": [], "entities": [{"text": "Quantifier errors", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8142510950565338}]}]}