{"title": [{"text": "High-Order Sequence Modeling for Language Learner Error Detection", "labels": [], "entities": [{"text": "High-Order Sequence Modeling", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.66525799036026}, {"text": "Language Learner Error Detection", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6695017069578171}]}], "abstractContent": [{"text": "We address the problem of detecting Eng-lish language learner errors by using a dis-criminative high-order sequence model.", "labels": [], "entities": [{"text": "detecting Eng-lish language learner errors", "start_pos": 26, "end_pos": 68, "type": "TASK", "confidence": 0.6451121866703033}]}, {"text": "Unlike most work in error-detection, this method is agnostic as to specific error types, thus potentially allowing for higher recall across different error types.", "labels": [], "entities": [{"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9984763264656067}]}, {"text": "The approach integrates features from many sources into the error-detection model, ranging from language model-based features to linguistic analysis features.", "labels": [], "entities": []}, {"text": "Evaluation results on a large annotated corpus of learner writing indicate the feasibility of our approach on a realistic, noisy and inherently skewed set of data.", "labels": [], "entities": []}, {"text": "High-order models consistently outperform low-order models in our experiments.", "labels": [], "entities": []}, {"text": "Error analysis on the output shows that the calculation of precision on the test set represents a lower bound on the real system performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9968549013137817}]}], "introductionContent": [{"text": "Systems for automatic detection and correction of errors in native writing have been developed for many decades.", "labels": [], "entities": [{"text": "automatic detection and correction of errors in native writing", "start_pos": 12, "end_pos": 74, "type": "TASK", "confidence": 0.7925053735574087}]}, {"text": "Early in the development of these systems, the approach was exclusively based on knowledge engineering.", "labels": [], "entities": []}, {"text": "Hand-crafted grammars would analyze a sentence and would contain special mechanisms for rule or constraint relaxation that allow ungrammatical sentences to produce a parse, while at the same time indicating that a grammatical error is present.", "labels": [], "entities": []}, {"text": "More recently, datadriven methods have assumed prominence and there has been an emerging area of research into the challenge of detecting and correcting errors in learner language (for an overview see).", "labels": [], "entities": [{"text": "detecting and correcting errors in learner language", "start_pos": 128, "end_pos": 179, "type": "TASK", "confidence": 0.7935536418642316}]}, {"text": "Data-driven methods offer the familiar set of advantages: they can be more flexible than a manually maintained set of rules and they tend to cope better with noisy input.", "labels": [], "entities": []}, {"text": "Drawbacks include the inability to handle linguistically more complex errors that involve long distance dependencies such as subject-verb agreement.", "labels": [], "entities": []}, {"text": "Learner errors as a target for error detection and correction pose a particular challenge but also offer some unique opportunities.", "labels": [], "entities": [{"text": "error detection and correction", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.7273354232311249}]}, {"text": "The challenge lies in the density of errors (much higher than in native writing), the variety of errors (a superset of typical native errors) and the generally more non-idiomatic writing.", "labels": [], "entities": []}, {"text": "On the other hand, the availability of annotated corpora, often comprised of manually corrected learner essays or scripts, provides a big advantage for the evaluation and training of data-driven systems.", "labels": [], "entities": []}, {"text": "Data-driven systems for English learner error detection and correction typically target a specific set of error types and contain a machine learned component for each error type.", "labels": [], "entities": [{"text": "English learner error detection and correction", "start_pos": 24, "end_pos": 70, "type": "TASK", "confidence": 0.7269563575585684}]}, {"text": "For example, such a system may have a classifier that determines the correct choice of preposition given the lexical and syntactic part-of-speech (POS) context and hence can aid the learner with the notoriously difficult problem of identifying an appropriate preposition.", "labels": [], "entities": []}, {"text": "Similarly, a classifier can be used to predict the correct choice of article in a given context.", "labels": [], "entities": []}, {"text": "Such targeted systems have the advantage that they often achieve relatively high precision at, of course, the cost of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9986960291862488}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9958789348602295}]}, {"text": "However, while there area few major learner error categories, such as prepositions and articles, there is also along tail of content word and other errors that is not amenable to a targeted approach.", "labels": [], "entities": []}, {"text": "In this paper, we depart from the error-specific paradigm and explore a sequence modeling approach to general error detection in learner writing.", "labels": [], "entities": [{"text": "general error detection", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.6308933595816294}]}, {"text": "This approach is completely agnostic as to the error type.", "labels": [], "entities": []}, {"text": "It attempts to predict the location of an error in a sentence based on observations gathered from a supervised training phase on an errorannotated learner corpus.", "labels": [], "entities": []}, {"text": "Features used here are based on an n-gram language model, POS tags, simple string features that indicate token length and capitalization, and linguistic analysis by a constituency parser.", "labels": [], "entities": []}, {"text": "We train and evaluate the method on a sizeable subset of the corpus.", "labels": [], "entities": []}, {"text": "We show the contribution of the different feature types and perform a manual error analysis to pinpoint shortcomings of the system and to get a more accurate idea of the system's precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.997775137424469}]}], "datasetContent": [{"text": "An obvious question that arises is how much training data we need for an error detection sequence model, i.e. how does performance degrade as we decrease the amount of training data from the 50K error-annotated sentences that were used in the previous experiments.", "labels": [], "entities": [{"text": "error detection sequence", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.7437112629413605}]}, {"text": "To this end we produced random subsets of the training data in 20% increments.", "labels": [], "entities": []}, {"text": "For each of these training sets, we determined the resulting F1 score by first performing parameter tuning on the development set and then measuring precision and recall of the best model on the test set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9862604141235352}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.999297022819519}, {"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9976338148117065}]}, {"text": "Results are shown in: at 20% of training data, precision starts to increase at the cost of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9997116923332214}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9990288019180298}]}, {"text": "At 80% of the training data, recall starts to trend up as well.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996751546859741}]}, {"text": "This upward trend of both precision and recall indicates that increasing the amount of training data is likely to further improve results.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.999678373336792}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9995425939559937}]}], "tableCaptions": [{"text": " Table 1: Error types in the CLC.", "labels": [], "entities": []}]}