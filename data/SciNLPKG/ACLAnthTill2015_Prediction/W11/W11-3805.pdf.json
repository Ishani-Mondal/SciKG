{"title": [{"text": "Discontinuous Data-Oriented Parsing: A mildly context-sensitive all-fragments grammar", "labels": [], "entities": [{"text": "Discontinuous Data-Oriented Parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6586198806762695}]}], "abstractContent": [{"text": "Recent advances in parsing technology have made treebank parsing with discontinuous constituents possible, with parser output of competitive quality (Kallmeyer and Maier, 2010).", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9870097637176514}, {"text": "treebank parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.5249733328819275}]}, {"text": "We apply Data-Oriented Parsing (DOP) to a grammar formalism that allows for discontinuous trees (LCFRS).", "labels": [], "entities": []}, {"text": "Decisions during parsing are conditioned on all possible fragments, resulting in improved performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9599961638450623}]}, {"text": "Despite the fact that both DOP and disconti-nuity present formidable challenges in terms of computational complexity, the model is reasonably efficient, and surpasses the state of the art in discontinuous parsing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language phenomena are inherently not context-free, or call for structural descriptions that cannot be produced by a context-free grammar.", "labels": [], "entities": []}, {"text": "Examples are extraposition, cross-serial dependencies and WH-inversion.", "labels": [], "entities": []}, {"text": "However, relaxing the context-freeness assumption comes at the peril of combinatorial explosion.", "labels": [], "entities": []}, {"text": "This work aims to transcend two limitations associated with probabilistic context-free grammars.", "labels": [], "entities": []}, {"text": "First in the sense of the representations produced by the parser, which allow constituents with gaps in their yields (see).", "labels": [], "entities": []}, {"text": "Building on , we parse with a mildly context-sensitive grammar (LCFRS) that can be read off directly from a treebank annotated with discontinuous constituents.", "labels": [], "entities": []}, {"text": "Secondly, the statistical dependencies in our generative model are derived from arbitrarily large frag- ments from the corpus.", "labels": [], "entities": []}, {"text": "We employ a Data-Oriented Parsing (DOP) model: a probabilistic tree-substitution grammar that employs probabilities derived from the frequencies of all connected fragments in the treebank).", "labels": [], "entities": []}, {"text": "We generalize the DOP model to support discontinuity.", "labels": [], "entities": []}, {"text": "This allows us to model complex constructions such as NP kann man VVINF.", "labels": [], "entities": [{"text": "VVINF", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.48030850291252136}]}], "datasetContent": [{"text": "We evaluate on version 2 of the German Negra treebank (.", "labels": [], "entities": [{"text": "German Negra treebank", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.7902880311012268}]}, {"text": "Results are for models based on splits of 90% training data and 10% test data.", "labels": [], "entities": []}, {"text": "The setup follows  as much as possible.", "labels": [], "entities": []}, {"text": "The parser is presented with (gold) part-of-speech tags from the treebank.", "labels": [], "entities": []}, {"text": "The DOP model, however, does exploit its knowledge of lexical dependencies by using fragments with terminals.", "labels": [], "entities": []}, {"text": "Ina pre-processing step, function labels are discarded and all punctuation is lowered to the best matching constituent.", "labels": [], "entities": []}, {"text": "Heads are marked using the head finding rules for the Negra corpus used by the Stanford parser, after which trees are binarized head-outward (.", "labels": [], "entities": [{"text": "head finding", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.7610595226287842}, {"text": "Negra corpus", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9567122161388397}]}, {"text": "The markovization setting is v=1 (i.e., no parent annotation), and h \u2208 {1, 2, \u221e}, dictated by efficiency concerns.", "labels": [], "entities": []}, {"text": "Lower values for h give better performance because they allow more flat structures to be covered through re-combinations of parts of different constituents.", "labels": [], "entities": []}, {"text": "However, this also greatly increases the number of possible edges which have to be explored.", "labels": [], "entities": []}, {"text": "For this reason we had to increase the value of h for parsing longer sentences, at the cost of decreased performance and coverage.", "labels": [], "entities": [{"text": "parsing longer sentences", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.9264475504557291}, {"text": "coverage", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9844915270805359}]}, {"text": "With these settings the grammar has a fan-out of 5 for the grammar of up to 15 word sentences, and a fan-out of 7 for the other two.", "labels": [], "entities": []}, {"text": "lists the size of the training & test corpora and their grammars for the respective length restrictions.", "labels": [], "entities": []}, {"text": "Note that  apply the length restriction before the 90-10 split, but the difference is not more than 12 sentences.", "labels": [], "entities": []}, {"text": "Evaluation is performed using a generalization of the PARSEVAL measures, which compares bracketings of the form A, a where a is the yield described by a tuple of intervals; we used Maier's publicly available implementation.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8199689388275146}]}, {"text": "We use PARSEVAL, in spite of its serious shortcomings, to enable comparison with previous work.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9205227494239807}]}, {"text": "Unparsed sentences are assigned a baseline parse with all tags directly under the root node.", "labels": [], "entities": []}, {"text": "Our model performs consistently better than previous results on discontinuous parsing; see table 2 for the results, including comparisons to previous work.", "labels": [], "entities": []}, {"text": "plots the time required to parse sentences of different lengths with v=1 h=2, showing a strikingly steep curve, which makes clear why parsing sentences longer than 25 words was not feasible with these settings.", "labels": [], "entities": []}, {"text": "The coarse-to-fine inference appears to work rather well, apparently displaying a linear observed time complexity on the DOP grammar; unfortunately exhaustive parsing with the coarse grammar forms a bottleneck.", "labels": [], "entities": []}, {"text": "The total time to parse was 1, 16 and 52 hours for 15, 25, and 30 words respectively, using about 4 GB of memory.", "labels": [], "entities": [{"text": "parse", "start_pos": 18, "end_pos": 23, "type": "TASK", "confidence": 0.9694740772247314}]}], "tableCaptions": [{"text": " Table 1: Number of sentences and rules.", "labels": [], "entities": []}, {"text": " Table 2: Results for discontinuous parsing on the Negra treebank.", "labels": [], "entities": [{"text": "discontinuous parsing", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.5487989187240601}, {"text": "Negra treebank", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.971709132194519}]}]}