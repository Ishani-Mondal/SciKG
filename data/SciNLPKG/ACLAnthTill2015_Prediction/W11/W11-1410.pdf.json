{"title": [{"text": "Developing Methodology for Korean Particle Error Detection", "labels": [], "entities": [{"text": "Korean Particle Error Detection", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.6625515520572662}]}], "abstractContent": [{"text": "We further work on detecting errors in post-positional particle usage by learners of Korean by improving the training data and developing a complete pipeline of particle selection.", "labels": [], "entities": [{"text": "particle selection", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.6920207440853119}]}, {"text": "We improve the data by filtering non-Korean data and sampling instances to better match the particle distribution.", "labels": [], "entities": []}, {"text": "Our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization.", "labels": [], "entities": [{"text": "system optimization", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.6857662051916122}]}], "introductionContent": [{"text": "A growing area of research in analyzing learner language is to detect errors in function words, namely categories such as prepositions and articles (see.", "labels": [], "entities": []}, {"text": "This work has mostly been for English, and there are issues, such as greater morphological complexity, in moving to other languages (see, e.g.,.", "labels": [], "entities": []}, {"text": "Our goal is to build a machine learning system for detecting errors in postpositional particles in Korean, a significant source of learner errors (.", "labels": [], "entities": []}, {"text": "Korean postpositional particles are morphemes that attach to a preceding nominal to indicate a range of linguistic functions, including grammatical functions, e.g., subject and object; semantic roles; and discourse functions.", "labels": [], "entities": []}, {"text": "In (1), for instance, ka marks the subject (function) and agent (semantic role).", "labels": [], "entities": []}, {"text": "Similar to English prepositions, particles can also have modifier functions, adding meanings of time, location, instrument, possession, and so forth.", "labels": [], "entities": []}, {"text": "We use the Yale Romanization scheme for writing Korean.", "labels": [], "entities": [{"text": "Yale Romanization scheme", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9488350749015808}]}, {"text": "We treat the task of particle error detection as one of particle selection, and we use machine learning because it has proven effective in similar tasks for other languages (e.g.,.", "labels": [], "entities": [{"text": "particle error detection", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.7287215987841288}, {"text": "particle selection", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7085393667221069}]}, {"text": "Training on a corpus of well-formed Korean, we predict which particle should appear after a given nominal; if this is different from the learner's, we have detected an error.", "labels": [], "entities": []}, {"text": "Using a machine learner has the advantage of being able to perform well without a researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.", "labels": [], "entities": []}, {"text": "We build from in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf..", "labels": [], "entities": [{"text": "English preposition error detection", "start_pos": 112, "end_pos": 147, "type": "TASK", "confidence": 0.6134635135531425}]}, {"text": "As the task is understudied, the work is preliminary, but it nonetheless is able to highlight the primary areas of focus for future work.", "labels": [], "entities": []}, {"text": "Secondly, we improve upon the training data, in particular doing a better job of selecting relevant instances for the machine learner.", "labels": [], "entities": []}, {"text": "Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains).", "labels": [], "entities": []}], "datasetContent": [{"text": "We have obtained an annotated corpus of 25 essays from heritage intermediate learners, with 299 sentences and 2515 ecels (2676 ecels after correcting spacing errors).", "labels": [], "entities": []}, {"text": "There are 1138 nominals, with 93 particle errors (5 added particles, 35 omissions, 53 substitutions)-in other words, less than 10% of particles are errors.", "labels": [], "entities": []}, {"text": "There are 979 particles after correction.", "labels": [], "entities": []}, {"text": "We focus on 38 particles that intermediate students can be reasonably expected to use.", "labels": [], "entities": []}, {"text": "A particle is one of three types (cf.): 1) case markers, 2) adverbials (cf. prepositions), and 3) auxiliary particles.", "labels": [], "entities": []}, {"text": "4 gives the results for the entire system on the test corpus, with separate results for each category of particle, as well as the concatenation of the three (All).", "labels": [], "entities": []}, {"text": "The accuracy presented here is in terms of only the particle in question, as opposed to the full form of root+particle(s).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994552731513977}]}, {"text": "Step 2 is presented in 2 ways: Classified, meaning that all of the instances classified as needing a particle by step 1 are processed, or Gold, in which we rely on the annotation to determine particle presence.", "labels": [], "entities": [{"text": "Gold", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9568957090377808}]}, {"text": "It is not surprising, then, that Gold experiments are more accurate than Classified experiments, due to step 1 errors and also preprocessing issues, discussed next.", "labels": [], "entities": []}, {"text": "Step 1 Step: Accuracy for step 1 (particle presence) & step 2 (particle selection), with number (#) of instances Preprocessing For the particles we examine, there are 135 mis-segmented nominals.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9976847171783447}, {"text": "particle selection", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7030404806137085}, {"text": "Preprocessing", "start_pos": 113, "end_pos": 126, "type": "METRIC", "confidence": 0.9210065603256226}]}, {"text": "The problem is more conspicuous if we look at the entire corpus: the tagger identifies 1547 nominal roots, but there are only 1138.", "labels": [], "entities": []}, {"text": "Some are errors in segmentation, i.e., mis-identifying the proper root of the ecel, and some are problems with tagging the root, e.g., a nominal mistagged as a verb.", "labels": [], "entities": []}, {"text": "provides results divided by cases with only correctly pre-processed ecels and where the target ecel has been mis-handled by the tagger.", "labels": [], "entities": []}, {"text": "This checks whether the system particle is correct, ignoring whether the whole form is correct; if full-form accuracy is considered, we have noway to get the 135 inaccurate cases correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9831200838088989}]}, {"text": "Error detection While our goal now is to establish a starting point, the ultimate, on-going goal of Full corpus details will be made available at: http:// cl.indiana.edu/ \u02dc particles/.", "labels": [], "entities": [{"text": "Error detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7780258357524872}]}, {"text": "Step 1 Step   this work is to develop a robust system for automatically detecting errors in learner data.", "labels": [], "entities": []}, {"text": "Thus, it is necessary to measure our performance at actually finding the erroneous instances extracted from our test corpus.", "labels": [], "entities": []}, {"text": "provides results for step 2 in terms of our ability to detect erroneous instances.", "labels": [], "entities": []}, {"text": "We report precision and recall, calculated as in.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.998865008354187}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.999534010887146}]}], "tableCaptions": [{"text": " Table 1: Step 1 (particle presence) results with filters", "labels": [], "entities": []}, {"text": " Table 2: Step 1 (presence) results with instance sampling", "labels": [], "entities": []}, {"text": " Table 3: Accuracy for step 1 (particle presence) & step 2  (particle selection), with number (#) of instances", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984839558601379}, {"text": "particle selection", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7273057699203491}]}, {"text": " Table 4: Overall accuracy divided by accurate and inac- curate preprocessing", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9992610812187195}, {"text": "accurate", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9973315000534058}]}, {"text": " Table 5: Error detection (using Gold step 1)", "labels": [], "entities": [{"text": "Error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8968685269355774}]}]}