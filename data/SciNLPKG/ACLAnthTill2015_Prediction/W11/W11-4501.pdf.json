{"title": [{"text": "Multi-Document Discourse Parsing Using Traditional and Hierarchical Machine Learning", "labels": [], "entities": [{"text": "Multi-Document Discourse Parsing", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7043097416559855}]}], "abstractContent": [{"text": "Multi-document handling is essential today, when many documents on the same topic are produced, especially considering the Web.", "labels": [], "entities": [{"text": "Multi-document handling", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8218773901462555}]}, {"text": "Both readers and computer applications can benefit from a discourse analysis of this multi-document content, since it demonstrates clearly the relations among portions of these documents.", "labels": [], "entities": []}, {"text": "This work aims to identify such relations automatically using machine learning techniques.", "labels": [], "entities": []}, {"text": "Particularly, this work focuses on the identification of relations predicted by the Cross-document Structure Theory (CST).", "labels": [], "entities": [{"text": "identification of relations predicted", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.8506508320569992}, {"text": "Cross-document Structure Theory (CST)", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.7985758731762568}]}, {"text": "The obtained results improve the state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the electronic media, there are many sources reporting the same topic from the same or different perspectives.", "labels": [], "entities": []}, {"text": "Online newspapers are an example: the same event is reported on different news portals.", "labels": [], "entities": []}, {"text": "In general, these documents are produced soon after the event and, subsequently, other documents are generated to update the news.", "labels": [], "entities": []}, {"text": "Therefore, readers interested on a current event will find an endless number of texts, and it will be crucial to pick just a few to read.", "labels": [], "entities": []}, {"text": "This requires a great effort on the part of readers.", "labels": [], "entities": []}, {"text": "Since these texts are produced by different sources at different moments, they may contain contradictory or redundant portions.", "labels": [], "entities": []}, {"text": "For instance, the two sentences below, S1 and S2, from different documents, are contradictory regarding the number of bombs in an attack, but both also present overlapping information (that there was a bomb): S1: The downtown Public Finance Department building was hit by three homemade bombs.", "labels": [], "entities": []}, {"text": "S2: The Public Finance Department was also hit by a bomb.", "labels": [], "entities": [{"text": "Public Finance Department", "start_pos": 8, "end_pos": 33, "type": "DATASET", "confidence": 0.5626608431339264}]}, {"text": "It is believed that, when readers know how the parts of multiple documents are related, they can, for example, ignore redundancy, find contradictions, and understand the temporal evolution of a factor event, which would allow them to approach the information in which they are interested in a more organized way.", "labels": [], "entities": []}, {"text": "In another vein, this type of knowledge might also be useful for several computer applications, such as web browsers and automatic summarizers, which would have more information available to produce their results and meet the users' needs more efficiently.", "labels": [], "entities": []}, {"text": "Some theories or models on multi-document relationships have been proposed for this purpose.", "labels": [], "entities": []}, {"text": "One of the most used is the Crossdocument Structure Theory (CST).", "labels": [], "entities": [{"text": "Crossdocument Structure Theory (CST)", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.8110321561495463}]}, {"text": "In this work, we propose to investigate the automatic identification of the relations among portions of several texts suggested by the CST, developing an automated multidocument parser.", "labels": [], "entities": []}, {"text": "We explore, in particular, the use of traditional (flat) and hierarchical machine learning techniques in this task, using a corpus of news texts written in Brazilian Portuguese, already annotated according to CST, which allows applying machine learning techniques and testing them.", "labels": [], "entities": [{"text": "CST", "start_pos": 209, "end_pos": 212, "type": "DATASET", "confidence": 0.9563432931900024}]}, {"text": "The results obtained are that the performance of some classifiers improved the state of the art.", "labels": [], "entities": []}, {"text": "It is also demonstrated that the task in question can be characterized as a hierarchical problem.", "labels": [], "entities": []}, {"text": "In Section 2, related work on multi-document parsing is briefly presented.", "labels": [], "entities": [{"text": "multi-document parsing", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7625119090080261}]}, {"text": "In Section 3, we describe the proposed architecture for the multi-document parser and discuss the methodology for identifying multi-document relations, introducing the experiments carried outwith machine learning.", "labels": [], "entities": []}, {"text": "Finally, Section 4 provides conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Results from the multi-class classifier for content relations  Relation  Precision Coverage  F-Measure  Subsumption  0.485  0.560  0.520  Elaboration  0.386  0.382  0.384  Overlap  0.486  0.503  0.494  Follow-up  0.317  0.287  0.301  Historical background  0.243  0.221  0.231  Identity  0.941  0.941  0.941  Equivalence  0.207  0.154  0.176  Average  0.438  0.435  0.435", "labels": [], "entities": [{"text": "Relation  Precision Coverage  F-Measure  Subsumption", "start_pos": 73, "end_pos": 125, "type": "TASK", "confidence": 0.7156833052635193}, {"text": "Equivalence  0.207  0.154  0.176  Average  0.438  0.435  0.435", "start_pos": 319, "end_pos": 381, "type": "METRIC", "confidence": 0.591851320117712}]}, {"text": " Table 4. Binary Classifiers  Attribution classifier  Follow-up classifier  Relation  Precision Coverage F-Measure  Relation  Precision Coverage F-Measure  Other  0.950  0.976  0.963  Other  0.814  0.887  0.848  Attribution  0.600  0.413  0.489  Follow-up  0.668  0.529  0.590  Average  0.775  0.694  0.726  Average  0.741  0.708  0.719", "labels": [], "entities": [{"text": "Relation  Precision Coverage F-Measure  Relation  Precision Coverage F-Measure  Other  0.950  0.976  0.963  Other  0.814  0.887  0.848  Attribution  0.600  0.413  0.489  Follow-up  0.668  0.529  0.590  Average  0.775  0.694  0.726  Average  0.741  0.708  0.719", "start_pos": 76, "end_pos": 336, "type": "METRIC", "confidence": 0.7888732794672251}]}, {"text": " Table 5. General accuracies for unbalanced and balanced data  Classification strategy  Unbalanced data  Balanced data  Multi-class  40.2%  72.5%  Hierarchical  35.3%  75.6%  Binary  27.5%  72.4%", "labels": [], "entities": []}]}