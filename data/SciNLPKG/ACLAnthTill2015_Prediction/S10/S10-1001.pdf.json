{"title": [{"text": "SemEval-2010 Task 1: Coreference Resolution in Multiple Languages", "labels": [], "entities": [{"text": "SemEval-2010 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.848513275384903}, {"text": "Coreference Resolution", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.9745443165302277}]}], "abstractContent": [{"text": "This paper presents the SemEval-2010 task on Coreference Resolution in Multiple Languages.", "labels": [], "entities": [{"text": "SemEval-2010 task", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8595491051673889}, {"text": "Coreference Resolution", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.938004344701767}]}, {"text": "The goal was to evaluate and compare automatic coreference resolution systems for six different languages (Catalan, Dutch, English, German, Italian, and Spanish) in four evaluation settings and using four different metrics.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.8805995881557465}]}, {"text": "Such a rich scenario had the potential to provide insight into key issues concerning corefer-ence resolution: (i) the portability of systems across languages, (ii) the relevance of different levels of linguistic information, and (iii) the behavior of scoring metrics.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.9190462529659271}]}], "introductionContent": [{"text": "The task of coreference resolution, defined as the identification of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.969120442867279}]}], "datasetContent": [{"text": "In order to address our goal of studying the effect of different levels of linguistic information (preprocessing) on solving coreference relations, the test was divided into four evaluation settings that differed along two dimensions.", "labels": [], "entities": []}, {"text": "Only in the gold-standard setting were participants allowed to use the gold-standard columns, including the last one (of the test dataset) with true mention boundaries.", "labels": [], "entities": []}, {"text": "In the regular setting, they were allowed to use only the automatically predicted columns.", "labels": [], "entities": []}, {"text": "Obtaining better results in the gold setting would provide evidence for the relevance of using high-quality preprocessing information.", "labels": [], "entities": []}, {"text": "Since not all columns were available for all six languages, the gold setting was only possible for Catalan, English, German, and Spanish.", "labels": [], "entities": []}, {"text": "In the closed setting, systems had to be built strictly with the information provided in the task datasets.", "labels": [], "entities": []}, {"text": "In contrast, there was no restriction on the resources that participants could utilize in the open setting: systems could be developed using any external tools and resources to predict the preprocessing information, e.g., WordNet, Wikipedia, etc.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 222, "end_pos": 229, "type": "DATASET", "confidence": 0.947557270526886}]}, {"text": "The only requirement was to use tools that had not been developed with the annotations of the test set.", "labels": [], "entities": []}, {"text": "This setting provided an open door into tools or resources that improve performance.", "labels": [], "entities": []}, {"text": "Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures.", "labels": [], "entities": [{"text": "coreference resolution evaluation", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.9695916374524435}]}, {"text": "The task scorer provides results in the two mentionbased metrics B 3 (Bagga and  Regarding languages, English concentrates the most participants (fifteen entries), followed by German (eight), Catalan and Spanish (seven each), Italian (five), and Dutch (three).", "labels": [], "entities": []}, {"text": "The number of languages addressed by each system ranges from one (Corry) to six (UBIU and SUCRE); BART and RelaxCor addressed three languages, and TANL-1 five.", "labels": [], "entities": [{"text": "Corry", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.9235599040985107}, {"text": "BART", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.5986114740371704}]}, {"text": "The best overall results are obtained for English followed by German, then Catalan, Spanish and Italian, and finally Dutch.", "labels": [], "entities": []}, {"text": "Apart from differences between corpora, there are other factors that might explain this ranking: (i) the fact that most of the systems were originally developed for English, and (ii) differences in corpus size (German having the largest corpus, and Dutch the smallest).", "labels": [], "entities": []}, {"text": "Regarding systems, there are no clear \"winners.\"", "labels": [], "entities": []}, {"text": "Note that no language-setting was addressed by all six systems.", "labels": [], "entities": []}, {"text": "The BART system, for instance, is either on its own or competing against a single system.", "labels": [], "entities": [{"text": "BART system", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8271679580211639}]}, {"text": "It emerges from partial comparisons that SUCRE performs the best in closed\u00d7regular for English, German, and Italian, although it never outperforms the CEAF or B 3 singleton baseline.", "labels": [], "entities": [{"text": "SUCRE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.824789822101593}, {"text": "CEAF", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.9351614713668823}]}, {"text": "While SUCRE always obtains the best scores according to MUC and BLANC, RelaxCor and TANL-1 usually win based on CEAF and B 3 . The Corry system presents three variants optimized for CEAF (Corry-C), MUC (Corry-M), and BLANC (Corry-B).", "labels": [], "entities": [{"text": "BLANC", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.989230215549469}, {"text": "RelaxCor", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.765338122844696}, {"text": "TANL-1", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9112002849578857}, {"text": "CEAF", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.5861398577690125}, {"text": "CEAF", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.8578869104385376}, {"text": "BLANC", "start_pos": 217, "end_pos": 222, "type": "METRIC", "confidence": 0.9976119995117188}]}, {"text": "Their results are consistent with the bias introduced in the optimization (see English:open\u00d7gold).", "labels": [], "entities": []}, {"text": "Depending on the evaluation metric then, the rankings of systems vary with considerable score differences.", "labels": [], "entities": []}, {"text": "There is a significant positive correlation between CEAF and B 3 (Pearson's r = 0.91, p < 0.01), and a significant lack of correlation between CEAF and MUC in terms of recall (Pearson's r = 0.44, p < 0.01).", "labels": [], "entities": [{"text": "B 3 (Pearson's r = 0.91", "start_pos": 61, "end_pos": 84, "type": "METRIC", "confidence": 0.7587841525673866}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9989901185035706}]}, {"text": "This fact stresses the importance of defining appropriate metrics (or a combination of them) for coreference evaluation.", "labels": [], "entities": [{"text": "coreference evaluation", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.9642682075500488}]}, {"text": "Finally, regarding evaluation settings, the results in the gold setting are significantly better than those in the regular.", "labels": [], "entities": []}, {"text": "However, this might be a direct effect of the mention recognition task.", "labels": [], "entities": [{"text": "mention recognition task", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.9168007771174113}]}, {"text": "Mention recognition in the regular setting falls more than 20 F 1 points with respect to the gold setting (where correct mention boundaries were given).", "labels": [], "entities": [{"text": "Mention recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6974315792322159}, {"text": "F 1", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.988094836473465}]}, {"text": "As for the open versus closed setting, there is only one system, RelaxCor for English, that addressed the two.", "labels": [], "entities": []}, {"text": "As expected, results show a slight improvement from closed\u00d7gold to open\u00d7gold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Format of the coreference annotations  (corresponding to example", "labels": [], "entities": []}, {"text": " Table 5: Official results of the participating systems for all languages, settings, and metrics.", "labels": [], "entities": []}]}