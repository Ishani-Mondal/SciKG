{"title": [{"text": "Cambridge: Parser Evaluation using Textual Entailment by Grammatical Relation Comparison", "labels": [], "entities": [{"text": "Parser Evaluation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7903316617012024}, {"text": "Textual Entailment by Grammatical Relation Comparison", "start_pos": 35, "end_pos": 88, "type": "TASK", "confidence": 0.7018434107303619}]}], "abstractContent": [{"text": "This paper describes the Cambridge submission to the SemEval-2010 Parser Evaluation using Textual Entailment (PETE) task.", "labels": [], "entities": [{"text": "SemEval-2010 Parser Evaluation using Textual Entailment (PETE) task", "start_pos": 53, "end_pos": 120, "type": "TASK", "confidence": 0.8488201856613159}]}, {"text": "We used a simple definition of en-tailment, parsing both T and H with the C&C parser and checking whether the core grammatical relations (subject and object) produced for H were a subset of those for T.", "labels": [], "entities": []}, {"text": "This simple system achieved the top score for the task out of those systems submitted.", "labels": [], "entities": []}, {"text": "We analyze the errors made by the system and the potential role of the task in parser evaluation.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.9169764816761017}]}], "introductionContent": [{"text": "SemEval-2010 Task 12, Parser Evaluation using Textual Entailment (PETE), was designed as anew, formalism-independent type of parser evaluation scheme.", "labels": [], "entities": [{"text": "Parser Evaluation using Textual Entailment (PETE)", "start_pos": 22, "end_pos": 71, "type": "TASK", "confidence": 0.7644036337733269}]}, {"text": "The task is broadly Recognizing Textual Entailment (RTE), but unlike typical RTE tasks, its intention is to focus on purely syntactic entailments, assuming no background knowledge or reasoning ability.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.7608695824941}]}, {"text": "For example, given a text (T) The man with the hat was tired., the hypothesis (H) The man was tired. is entailed, but The hat was tired. is not.", "labels": [], "entities": []}, {"text": "A correct decision on whether H is entailed can be used as a diagnostic for the parser's analysis of (some aspect of) T.", "labels": [], "entities": []}, {"text": "By requiring only a binary decision on the entailment, instead of a full syntactic analysis, a parser can be evaluated while its underlying formalism remains a \"black box\".", "labels": [], "entities": []}, {"text": "Our system had two components: a parser, and an entailment system which decided whether T entails H based on the parser's output.", "labels": [], "entities": []}, {"text": "We distinguish two types of evaluation.", "labels": [], "entities": []}, {"text": "Task evaluation, i.e. the official task scoring, indicates whether the entailment decisions -made by the parser and entailment system together -tally with the gold standard dataset.", "labels": [], "entities": []}, {"text": "Entailment system evaluation, on the other hand, indicates whether the entailment system is an appropriate parser evaluation tool.", "labels": [], "entities": [{"text": "Entailment system evaluation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5965094467004141}]}, {"text": "In the PETE task the parser is not evaluated directly on the dataset, since the entailment system acts as intermediary.", "labels": [], "entities": [{"text": "PETE task", "start_pos": 7, "end_pos": 16, "type": "TASK", "confidence": 0.6284905076026917}]}, {"text": "Therefore, for PETE to be a viable parser evaluation scheme, each parser must be coupled with an entailment system which accurately reflects the parser's analysis of the data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now consider whether our entailment system was an appropriate tool for evaluating the C&C parser on the PETE dataset.", "labels": [], "entities": [{"text": "PETE dataset", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9531637132167816}]}, {"text": "It is easy to imagine a poor entailment system that makes incorrect guesses in spite of good parser output, or conversely one that uses additional reasoning to supplement the parser's analysis.", "labels": [], "entities": []}, {"text": "To bean appropriate parser evaluation tool, the entailment system must decide whether the information in H is also contained in the parse of T, without \"introducing\" or \"correcting\" any errors.", "labels": [], "entities": []}, {"text": "Assuming our GR-based approach is valid, then given gold-standard GRs for T and H, we expect an appropriate entailment system to result in 100% accuracy on the task evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9991961121559143}]}, {"text": "To perform this oracle experiment we annotated the development data with gold-standard GRs.", "labels": [], "entities": []}, {"text": "Using our entailment system with the gold GRs we achieved 90.9% task accuracy.", "labels": [], "entities": [{"text": "GRs", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.7710400819778442}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9746277928352356}]}, {"text": "Six incorrect entailment decisions were made, of which one was on the arguably extrasyntactic entailment discussed in Section 4.", "labels": [], "entities": []}, {"text": "Three errors were due to transformations between T and H which changed the GR label or head.", "labels": [], "entities": []}, {"text": "For example, consider Occasionally, the children find steamed, whole-wheat grains for cereal which they call \"buckshot\".", "labels": [], "entities": []}, {"text": "In T, steamed is a prenominal adjective, with grains as its head; while in H, it is a passive, with grains as its subject.", "labels": [], "entities": []}, {"text": "The entailment system did not account for this transformation, although in principle it could have.", "labels": [], "entities": []}, {"text": "The other two errors occurred because GRs involving a non-core relation or a pronoun introduced in H, both of which our system ignored, were crucial for the correct entailment decision.", "labels": [], "entities": []}, {"text": "shows that with automaticallygenerated GRs, four errors on the task evaluation were attributable to the entailment system.", "labels": [], "entities": []}, {"text": "Three of these were also found in the oracle experiment.", "labels": [], "entities": []}, {"text": "The fourth resulted from a POS change between T and H for There was the revolution in Tibet which we pretended did not exist.", "labels": [], "entities": [{"text": "POS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9782711863517761}, {"text": "T", "start_pos": 46, "end_pos": 47, "type": "METRIC", "confidence": 0.9638441205024719}]}, {"text": "\u21d2 The pretended did not exist..", "labels": [], "entities": []}, {"text": "The crucial GR was (nsubj exist pretended) in grs(H), but the entailment system ignored it because the lemmatizer did not give pretend as the lemma for pretended as a noun.", "labels": [], "entities": [{"text": "GR", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9975473284721375}]}, {"text": "This type of error might be prevented by answering NO if the POS of any word changes between T and H, but the implementation is non-trivial since word indices may also change.", "labels": [], "entities": [{"text": "NO", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9025022983551025}, {"text": "POS", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9650884866714478}]}, {"text": "There were eight POS changes in the development data, most of which did not result in errors.", "labels": [], "entities": []}, {"text": "We also observed two cases where the entailment system \"corrected\" parser errors, yielding a correct entailment decision despite the parser's incorrect analysis of T.", "labels": [], "entities": []}, {"text": "When compared with a manual analysis of whether T entailed H based on automatically-generated GRs, the entailment system achieved 89.4% overall accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.998866081237793}]}], "tableCaptions": [{"text": " Table 1: Results on the test data.", "labels": [], "entities": []}, {"text": " Table 2: Results on the development data.", "labels": [], "entities": []}, {"text": " Table 3: Error breakdown on the development  data. FN: false negative, FP: false positive.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9951938986778259}, {"text": "FN", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9690557718276978}, {"text": "FP", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9515591859817505}]}]}