{"title": [{"text": "RACAI: Unsupervised WSD experiments @ SemEval-2, Task #17", "labels": [], "entities": [{"text": "RACAI", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5901790857315063}, {"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9312655329704285}]}], "abstractContent": [{"text": "This paper documents the participation of the Research Institute for Artificial Intelligence of the Romanian Academy (RACAI) to the Task 17-All-words Word Sense Disambiguation on a Specific Domain, of the SemEval-2 competition.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.6427537898222605}, {"text": "SemEval-2 competition", "start_pos": 205, "end_pos": 226, "type": "TASK", "confidence": 0.8901346027851105}]}, {"text": "We describe three unsupervised WSD systems that make extensive use of the Prince-ton WordNet (WN) structure and WordNet Domains in order to perform the disambigua-tion.", "labels": [], "entities": [{"text": "Prince-ton WordNet (WN) structure", "start_pos": 74, "end_pos": 107, "type": "DATASET", "confidence": 0.6691633562246958}]}, {"text": "The best of them has been ranked the 12 th by the task organizers out of 29 judged runs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Referring to the last SemEval (SemEval-1, () and to our recent work, unsupervised Word Sense Disambiguation (WSD) is still at the bottom of WSD systems ranking with a significant loss in performance when compared to supervised approaches.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6607212275266647}]}, {"text": "With Task #17 @ SemEval-2, this observation is (probably 1 ) reinforced but another issue is re-brought to light: the difficulty of supervised WSD systems to adapt to a given domain ().", "labels": [], "entities": []}, {"text": "With general scores lower with at least 3% than 3 years ago in Task #17 @ SemEval-1 which was a supposedly harder task (general, no particular domain WSD was required for all words), we observe that supervised WSD is certainly more difficult to implement in areal world application.", "labels": [], "entities": []}, {"text": "Our unsupervised WSD approach benefited from the specification of this year's Task #17 which was a domain-limited WSD, meaning that the disambiguation would be applied to content words drawn from a specific domain: the surrounding environment.", "labels": [], "entities": [{"text": "WSD", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9561976194381714}]}, {"text": "We worked under the assumption that a term of the given domain would have the same meaning with all its occurrences throughout the text.", "labels": [], "entities": []}, {"text": "This hypothesis has been put forth by as the \"one sense per discourse\" hypothesis (OSPD for short).", "labels": [], "entities": []}, {"text": "The task organizers offered a set of background documents with no sense annotations to the competitors who want to train/tune their systems using data from the same domain as the official test set.", "labels": [], "entities": []}, {"text": "Working with the OSPD hypothesis, we set off to construct/test domain specific WSD models from/on this corpus using the WordNet Domains ().", "labels": [], "entities": [{"text": "WordNet Domains", "start_pos": 120, "end_pos": 135, "type": "DATASET", "confidence": 0.9275216460227966}]}, {"text": "For testing purposes, we have constructed an inhouse gold standard from this corpus that comprises of 1601 occurrences of 204 terms of the \"surrounding environment\" domain that have been automatically extracted with the highest confidence.", "labels": [], "entities": []}, {"text": "We have observed that our gold standard (which has been independently annotated by 3 annotators but on non-overlapping sections which led to having no inter-annotator agreement scores) obeys the OSPD hypothesis which we think that is appropriate to domainlimited WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 263, "end_pos": 266, "type": "TASK", "confidence": 0.653861403465271}]}, {"text": "In what follows, we will briefly acknowledge the usage of WordNet Domains in WSD, we will then describe the construction of the corpus of the background documents including here the creation of an in-house gold standard, we will then briefly describe our three WSD algorithms and finally we will conclude with a discussion on the ranking of our runs among the 29 evaluated by the task organizers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: RACAI systems results (accuracy) on the  RACAI test set", "labels": [], "entities": [{"text": "RACAI", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.8414624333381653}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9989136457443237}, {"text": "RACAI test set", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.8595114151636759}]}, {"text": " Table 4: RACAI systems results (accuracy) on the  SEMEVAL test set", "labels": [], "entities": [{"text": "RACAI", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.8573206067085266}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9991374015808105}, {"text": "SEMEVAL test set", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.8857514262199402}]}]}