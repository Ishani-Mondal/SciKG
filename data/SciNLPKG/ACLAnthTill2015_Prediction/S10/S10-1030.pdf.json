{"title": [{"text": "DERIUNLP: A Context Based Approach to Automatic Keyphrase Extraction", "labels": [], "entities": [{"text": "DERIUNLP", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9027349948883057}, {"text": "Keyphrase Extraction", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7101099789142609}]}], "abstractContent": [{"text": "The DERI UNLP team participated in the SemEval 2010 Task #5 with an unsuper-vised system that automatically extracts keyphrases from scientific articles.", "labels": [], "entities": [{"text": "DERI UNLP team", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8165210684140524}, {"text": "SemEval 2010 Task #", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8328856527805328}]}, {"text": "Our approach does not only consider a general description of a term to select keyphrase candidates but also context information in the form of \"skill types\".", "labels": [], "entities": []}, {"text": "Even though our system analyses only a limited set of candidates, it is still able to outperform baseline unsupervised and supervised approaches .", "labels": [], "entities": []}], "introductionContent": [{"text": "Keyphrases provide users overwhelmed by the richness of information currently available with useful insight into document content but at the same time they area valuable input fora variety of NLP applications such as summarization, clustering and searching.", "labels": [], "entities": [{"text": "summarization", "start_pos": 217, "end_pos": 230, "type": "TASK", "confidence": 0.9884006977081299}]}, {"text": "The SemEval 2010 competition included a task targeting the Automatic Keyphrase Extraction from Scientific Articles (.", "labels": [], "entities": [{"text": "SemEval 2010 competition", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8336254358291626}, {"text": "Keyphrase Extraction from Scientific Articles", "start_pos": 69, "end_pos": 114, "type": "TASK", "confidence": 0.8331296980381012}]}, {"text": "Given a set of scientific articles participants are required to assign to each document keyphrases extracted from text.", "labels": [], "entities": []}, {"text": "We participated in this task with an unsupervised approach for keyphrase extraction that does not only consider a general description of a term to select candidates but also takes into consideration context information.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.857986330986023}]}, {"text": "The larger context of our work is the extraction of expertise topics for Expertise Mining.", "labels": [], "entities": [{"text": "Expertise Mining", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7736902832984924}]}, {"text": "Expertise Mining is the task of automatically extracting expertise topics and expertise profiles from a collection of documents.", "labels": [], "entities": [{"text": "Expertise Mining", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7225616127252579}]}, {"text": "Even though the Expertise Mining task and the Keyphrase Extraction task are essentially different, it is important to assess the keyphraseness of extracted expertise topics, i.e., their ability to represent the content of a document.", "labels": [], "entities": [{"text": "Expertise Mining task", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7746500174204508}, {"text": "Keyphrase Extraction task", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.8259141047795614}]}, {"text": "Here we will report only relevant findings for the Keyphrase Extraction task, focusing on the overlapping aspects of the two aforementioned tasks.", "labels": [], "entities": [{"text": "Keyphrase Extraction task", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.9275002678235372}]}, {"text": "After giving an overview of related work in section 2 we introduce skill types and present our candidate selection method in section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes the features used for ranking and filtering the candidate keyphrases and Section 5 presents our results before we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The SemEval task organizers provided two sets of scientific articles, a set of 144 documents for training and a set of 100 documents for testing.", "labels": [], "entities": []}, {"text": "No information was provided about the scientific domain of the articles but at least some of them are from Computer Science.", "labels": [], "entities": []}, {"text": "The average length of the articles is between 6 and 8 pages including tables and pictures.", "labels": [], "entities": []}, {"text": "Three sets of answers were provided: author-assigned keyphrases, reader-assigned keyphrases and combined keyphrases (combination of the first two sets).", "labels": [], "entities": []}, {"text": "The participants were asked to assign a number of exactly 15 keyphrases per document.", "labels": [], "entities": []}, {"text": "All reader-assigned keyphrases are extracted from the papers, whereas some of the authorassigned keyphrases do not occur explicitly in the text.", "labels": [], "entities": []}, {"text": "Two alternations of keyphrase are accepted: A of B / BA and A's B.", "labels": [], "entities": [{"text": "BA", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.4706036150455475}, {"text": "B", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.7328607439994812}]}, {"text": "In case that the semantics changes due to the alternation, the alternation is not included in the answer set.", "labels": [], "entities": []}, {"text": "The traditional evaluation metric was followed, matching the extracted keyphrases with the keyphrases in the answer sets and calculating precision, recall and Fscore.", "labels": [], "entities": [{"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9996248483657837}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.999553382396698}, {"text": "Fscore", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9980371594429016}]}, {"text": "In both tables the column labels start with a number which stands for the top 5, 10 or 15 candidates.", "labels": [], "entities": []}, {"text": "The characters P, R, F mean micro-averaged precision, recall and F-scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9359781742095947}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9996132254600525}, {"text": "F-scores", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9926549196243286}]}, {"text": "For baselines, 1, 2, 3 grams were used as candidates and TF-IDF as features.", "labels": [], "entities": []}, {"text": "In the keyphrases extracted by our system are compared with keyphrases extracted by an unsupervised method that ranks the candidates based on TF-IDF scores and two supervised methods using Naive Bayes (NB) and maximum entropy(ME) in WEKA 2 . Our performance is well above the baseline in all cases.", "labels": [], "entities": [{"text": "maximum entropy(ME)", "start_pos": 210, "end_pos": 229, "type": "METRIC", "confidence": 0.6935818135738373}, {"text": "WEKA", "start_pos": 233, "end_pos": 237, "type": "DATASET", "confidence": 0.7324428558349609}]}, {"text": "To show the contribution of skill types we included the results fora baseline version of our system (DUB) that does not rank the candidates using the normalized collection frequency in the context of a skill type F n i but the overall collection frequency (i.e., the number of occurrences of a keyphrase in the corpus).", "labels": [], "entities": []}, {"text": "The significantly increased results compared to our baseline version show the effectiveness of skill types for keyphrase candidate ranking.", "labels": [], "entities": []}, {"text": "presents our results in comparison with results of other participants.", "labels": [], "entities": []}, {"text": "Even though our system considers in the first stage a significantly limited set of candidates the results are very close to the average results of other participants.", "labels": [], "entities": []}, {"text": "Our system performed 8th best out of 19 participants for top 15 keyphrases, 10th best for top 10 keyphrases and 13th best for top 5 keyphrases, which indicates that our approach could be improved by using a more sophisticated ranking method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline and DERIUNLP Performance aver Combined Keywords", "labels": [], "entities": [{"text": "DERIUNLP", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9992982149124146}]}, {"text": " Table 2: Performance over Combined Keywords", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9671510457992554}]}]}