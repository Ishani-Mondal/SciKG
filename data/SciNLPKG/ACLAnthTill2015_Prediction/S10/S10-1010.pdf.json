{"title": [], "abstractContent": [{"text": "Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations , the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 206, "end_pos": 222, "type": "TASK", "confidence": 0.7238927483558655}, {"text": "temporal relation extraction", "start_pos": 227, "end_pos": 255, "type": "TASK", "confidence": 0.6779546439647675}]}, {"text": "Manually annotated data were provided for six languages: Chinese, En-glish, French, Italian, Korean and Spanish.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within a text.", "labels": [], "entities": [{"text": "identification of all temporal referring expressions, events and temporal relations within a text", "start_pos": 57, "end_pos": 154, "type": "TASK", "confidence": 0.6275160227503095}]}, {"text": "However, addressing this aim is beyond the scope of an evaluation challenge and a more modest approach is appropriate.", "labels": [], "entities": []}, {"text": "The 2007 SemEval task, TempEval-1 (, was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.8437178134918213}]}, {"text": "TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we first introduce the data that we are dealing with.", "labels": [], "entities": []}, {"text": "Which gets us in a position to present the list of task introduced by TempEval-2, including some motivation as to why we feel that it is a good idea to split up temporal relation classification into sub tasks.", "labels": [], "entities": [{"text": "temporal relation classification", "start_pos": 161, "end_pos": 193, "type": "TASK", "confidence": 0.6914903322855631}]}, {"text": "We proceed by shortly describing the data resources and their creation, followed by the performance of the systems that participated in the tasks.", "labels": [], "entities": []}, {"text": "The Semeval-2007 task was actually known simply as TempEval, but here we use Tempeval-1 to avoid confusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the extents of events and time expressions (tasks A and B), precision, recall and the f1-measure are used as evaluation metrics, using the following formulas: Where tp is the number of tokens that are part of an extent in both key and response, fp is the number of tokens that are part of an extent in the response but not in the key, and fn is the number of tokens that are part of an extent in the key but not in the response.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9994924068450928}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9994627833366394}]}, {"text": "For attributes of events and time expressions (the second part of tasks A and B) and for relation types (tasks C through F) we use an even simpler metric: the number of correct answers divided by the number of answers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Task A results for English", "labels": [], "entities": [{"text": "A", "start_pos": 15, "end_pos": 16, "type": "METRIC", "confidence": 0.886040985584259}]}, {"text": " Table 4: Event extent results", "labels": [], "entities": [{"text": "Event extent", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.5948198586702347}]}, {"text": " Table 5: Event attribute results", "labels": [], "entities": []}, {"text": " Table 6: Results for relation tasks", "labels": [], "entities": [{"text": "relation tasks", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.9333910644054413}]}, {"text": " Table 7: Percentage not classified", "labels": [], "entities": []}]}