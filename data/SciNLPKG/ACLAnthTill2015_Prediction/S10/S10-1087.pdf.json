{"title": [{"text": "IIITH: Domain Specific Word Sense Disambiguation", "labels": [], "entities": [{"text": "Domain Specific Word Sense Disambiguation", "start_pos": 7, "end_pos": 48, "type": "TASK", "confidence": 0.6208232820034028}]}], "abstractContent": [{"text": "We describe two systems that participated in SemEval-2010 task 17 (All-words Word Sense Disambiguation on a Specific Domain) and were ranked in the third and fourth positions in the formal evaluation.", "labels": [], "entities": [{"text": "SemEval-2010 task 17 (All-words Word Sense Disambiguation on a Specific Domain)", "start_pos": 45, "end_pos": 124, "type": "TASK", "confidence": 0.7666576298383566}]}, {"text": "Domain adaptation techniques using the background documents released in the task were used to assign ranking scores to the words and their senses.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7241881042718887}]}, {"text": "The test data was disambiguated using the Personalized PageRank algorithm which was applied to a graph constructed from the whole of WordNet in which nodes are initialized with ranking scores of words and their senses.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.9594156742095947}]}, {"text": "In the competition, our systems achieved comparable accuracy of 53.4 and 52.2, which outperforms the most frequent sense baseline (50.5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9996681213378906}]}], "introductionContent": [{"text": "The senses in WordNet are ordered according to their frequency in a manually tagged corpus, SemCor (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.9344455003738403}]}, {"text": "Senses that do not occur in SemCor are ordered arbitrarily after those senses of the word that have occurred.", "labels": [], "entities": []}, {"text": "It is known from the results of SENSEVAL2 () and SENSEVAL3 () that first sense heuristic outperforms many WSD systems (see).", "labels": [], "entities": []}, {"text": "The first sense baseline's strong performance is due to the skewed frequency distribution of word senses.", "labels": [], "entities": []}, {"text": "WordNet sense distributions based on SemCor are clearly useful, however in a given domain these distributions may not hold true.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.912577211856842}]}, {"text": "For example, the first sense for \"bank\" in WordNet refers to \"sloping land beside a body of river\" and the second to \"financial institution\", but in the domain of \"finance\" the \"financial institution\" sense would be expected to be more likely than the \"sloping land beside a body of river\" sense.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9650325179100037}]}, {"text": "Unfortunately, it is not feasible to produce large manually senseannotated corpora for every domain of interest.", "labels": [], "entities": []}, {"text": "propose a method to predict sense distributions from raw corpora and use this as a first sense heuristic for tagging text with the predominant sense.", "labels": [], "entities": []}, {"text": "Rather than assigning predominant sense in every case, our approach aims to use these sense distributions collected from domain specific corpora as a knowledge source and combine this with information from the context.", "labels": [], "entities": []}, {"text": "Our approach focuses on the strong influence of domain for WSD () and the benefits of focusing on words salient to the domain ().", "labels": [], "entities": [{"text": "WSD", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.803047776222229}]}, {"text": "Words are assigned a ranking score based on its keyness (salience) in the given domain.", "labels": [], "entities": []}, {"text": "We use these word scores as another knowledge source.", "labels": [], "entities": []}, {"text": "Graph based methods have been shown to produce state-of-the-art performance for unsupervised word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.7070687810579935}]}, {"text": "These approaches use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular lexical knowledge base (LKB), such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 176, "end_pos": 183, "type": "DATASET", "confidence": 0.9575522541999817}]}, {"text": "These graphbased algorithms are appealing because they take into account information drawn from the entire graph as well as from the given context, making them superior to other approaches that rely only on local information individually derived for each word.", "labels": [], "entities": []}, {"text": "Our approach uses the Personalized PageRank algorithm) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9466037154197693}]}, {"text": "We also combine domain-specific information from the knowledge sources, like sense distribution scores and keyword ranking scores, into the graph thus personalizing the graph for the given domain.", "labels": [], "entities": []}, {"text": "In section 2, we describe domain sense ranking.", "labels": [], "entities": [{"text": "domain sense ranking", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7253902157147726}]}, {"text": "Domain keyword ranking is described in Section 3.", "labels": [], "entities": [{"text": "Domain keyword ranking", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6560004949569702}]}, {"text": "Graph construction and personalized page rank are described in Section 4.", "labels": [], "entities": [{"text": "Graph construction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7051252573728561}]}, {"text": "Evaluation results over the SemEval data are provided in Section 5.", "labels": [], "entities": [{"text": "SemEval data", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.7731409966945648}]}, {"text": "propose a method for finding predominant senses from raw text.", "labels": [], "entities": []}, {"text": "The method uses a thesaurus acquired from automatically parsed text based on the method described by.", "labels": [], "entities": []}, {"text": "This provides the top k nearest neighbours for each target word w, along with the distributional similarity score between the target word and each neighbour.", "labels": [], "entities": [{"text": "distributional similarity score", "start_pos": 82, "end_pos": 113, "type": "METRIC", "confidence": 0.824872354666392}]}, {"text": "The senses of a word ware each assigned a score by summing over the distributional similarity scores of its neighbours.", "labels": [], "entities": []}, {"text": "These are weighted by a semantic similarity score (using WordNet Similarity score) between the sense of wand the sense of the neighbour that maximizes the semantic similarity score.", "labels": [], "entities": []}], "datasetContent": [{"text": "Tool: We used UKB tool 3 (Agirre and Soroa, 2009) which provides an implementation of personalized PageRank.", "labels": [], "entities": [{"text": "UKB tool 3 (Agirre and Soroa, 2009)", "start_pos": 14, "end_pos": 49, "type": "DATASET", "confidence": 0.9217630505561829}]}, {"text": "We modified it to incorporate our methods of graph initialization.", "labels": [], "entities": [{"text": "graph initialization", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.6500366628170013}]}, {"text": "The LKB used in our experiments is WordNet3.0 + Gloss which is provided in the tool.", "labels": [], "entities": [{"text": "WordNet3.0 + Gloss", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.7106742858886719}]}, {"text": "More details of the tools used can be found in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8655664920806885}]}, {"text": "Normalizations: Sense ranking scores (srs) and keyword ranking scores (krs) have diverse ranges.", "labels": [], "entities": [{"text": "keyword ranking scores (krs)", "start_pos": 47, "end_pos": 75, "type": "METRIC", "confidence": 0.6682940373818079}]}, {"text": "We found srs generally in the range between 0 to: Evaluation results on English test data of SemEval-2010 Task-17.", "labels": [], "entities": [{"text": "English test data of SemEval-2010 Task-17", "start_pos": 72, "end_pos": 113, "type": "DATASET", "confidence": 0.8347959617773691}]}, {"text": "* represents the system which we submitted to SemEval and is ranked 3rd in public evaluation.", "labels": [], "entities": []}, {"text": "Test data released for this task is disambiguated using IIITH1 and IIITH2 systems.", "labels": [], "entities": [{"text": "IIITH1", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.8849536180496216}]}, {"text": "As described in Section 2, IIITH1 and IIITH2 systems differ in the way the sense ranking scores are computed.", "labels": [], "entities": [{"text": "IIITH1", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.6558457612991333}]}, {"text": "Here we project only the results of IIITH1 since IIITH1 performed slightly better than IIITH2 in all the above settings.", "labels": [], "entities": [{"text": "IIITH1", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.675207257270813}]}, {"text": "Results of 1 st sense system provided by the organizers which assigns first sense computed from the annotations in hand-labeled corpora is also presented.", "labels": [], "entities": []}, {"text": "Additionally, we also present the results of Predominant Sense Heuristic (PSH) which assigns every word w with the sense ws j (ws j \u2208 senses(w)) which has the highest value of srs(ws j ) computed in Section 2 similar to ().", "labels": [], "entities": []}, {"text": "We used TreeTagger to Part of Speech tag the test data.", "labels": [], "entities": [{"text": "Part of Speech tag the test data", "start_pos": 22, "end_pos": 54, "type": "DATASET", "confidence": 0.5319126120635441}]}, {"text": "POS information was used to discard irrelevant senses.", "labels": [], "entities": []}, {"text": "Due to POS tagging errors, our precision values were not equal to recall values.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.6707907915115356}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9991067051887512}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9977854490280151}]}, {"text": "In the competition, we submitted IIITH1 and IIITH2 systems with CS + SRS + PPR graph initialization.", "labels": [], "entities": []}, {"text": "IIITH1 and IIIH2 gave performances of 53.4 % and 52.2 % precision respectively.", "labels": [], "entities": [{"text": "IIITH1", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9686282277107239}, {"text": "IIIH2", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8409899473190308}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9996662139892578}]}, {"text": "In our later experiments, we found CS + KRS + SRS + PPR has given the best performance of 53.6 % precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9987877011299133}]}, {"text": "From the results, it can be seen when srs information is incorporated in the graph, precision improved by 11.1% compared to PPR in unsupervised graph initialization and by 3.19% compared to CS + PPR in semi-supervised graph initialization.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9997367262840271}]}, {"text": "Also little improvements are seen when krs information is added.", "labels": [], "entities": []}, {"text": "This shows that domain specific information like sense ranking scores and keyword ranking scores play a major role in domain specific WSD.", "labels": [], "entities": [{"text": "domain specific WSD", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.5061715741952261}]}, {"text": "The difference between the results in unsupervised and semi-supervised graph initializations maybe attributed to the additional information the semi-supervised graph is having i.e. the sense distribution knowledge of non-domain specific words (common words).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results on English test data of SemEval-2010 Task-17. * represents the system which  we submitted to SemEval and is ranked 3rd in public evaluation.", "labels": [], "entities": [{"text": "English test data of SemEval-2010 Task-17", "start_pos": 32, "end_pos": 73, "type": "DATASET", "confidence": 0.7956031113862991}]}]}