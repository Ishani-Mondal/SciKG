{"title": [{"text": "MARS: A Specialized RTE System for Parser Evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our participation in the the SemEval-2010 Task #12, Parser Evaluation using Textual Entail-ment.", "labels": [], "entities": [{"text": "SemEval-2010 Task #12", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8021234571933746}, {"text": "Parser Evaluation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.8249331414699554}]}, {"text": "Our system incorporated two dependency parsers, one semantic role labeler, and a deep parser based on hand-crafted grammars.", "labels": [], "entities": []}, {"text": "The shortest path algorithm is applied on the graph representation of the parser outputs.", "labels": [], "entities": []}, {"text": "Then, different types of features are extracted and the entail-ment recognition is casted into a machine-learning-based classification task.", "labels": [], "entities": [{"text": "entail-ment recognition", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7278545796871185}]}, {"text": "The best setting of the system achieves 66.78% of accuracy, which ranks the 3rd place.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9994091987609863}]}], "introductionContent": [{"text": "The SemEval-2010 Task #12, Parser Evaluation using Textual Entailment (PETE), is an interesting task connecting two areas of research, parsing and recognizing textual entailment (RTE) ().", "labels": [], "entities": [{"text": "Parser Evaluation using Textual Entailment (PETE)", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.668215699493885}, {"text": "parsing and recognizing textual entailment (RTE)", "start_pos": 135, "end_pos": 183, "type": "TASK", "confidence": 0.5933786518871784}]}, {"text": "The former is usually concerned with syntactic analysis in specific linguistic frameworks, while the latter is believed to involve more semantic aspects of the human languages.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7344009280204773}]}, {"text": "However, no clear-cut boundary can be drawn between syntax and semantics for both tasks.", "labels": [], "entities": []}, {"text": "In recent years, the parsing community has been reaching beyond what was usually accepted as syntactic structures.", "labels": [], "entities": [{"text": "parsing community", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8856491148471832}]}, {"text": "Many deep linguistic frameworks allow the construction of semantic representations in parallel to the syntactic structure.", "labels": [], "entities": []}, {"text": "Meanwhile, data-driven shallow semantic parsers (or semantic role labelers) are another popular type of extension to enrich the information in the parser outputs.", "labels": [], "entities": []}, {"text": "Although entailment is described as a semantic relation, RTE, in practice, covers linguistic phenomena at various levels, from surface text to the meaning, even to the context and discourse.", "labels": [], "entities": [{"text": "RTE", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9695031046867371}]}, {"text": "One proposal of solving the problem is to deal with different cases of entailment using different specialized RTE modules ().", "labels": [], "entities": []}, {"text": "Then, the PETE data can be naturally classified into the syntactic and shallow semantic categories.", "labels": [], "entities": [{"text": "PETE data", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.7041697055101395}]}, {"text": "By participating in this shared task, we aim to investigate whether different parsing outputs leads to different RTE accuracy, and on the contrary, whether the \"application\"-based evaluation provides insights to the parser comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.7700383067131042}]}, {"text": "Further, we investigate if strict grammaticality checking with a linguistic grammar is helpful in this task.", "labels": [], "entities": []}], "datasetContent": [{"text": "As we mentioned in the preprocessing section (Section 2.1), we utilize the open source dependency parsers, MSTParser 4 and MaltParser 5 , our own semantic role labeler (, and the PET HPSG parser 6 . For the shortest path algorithm, we use the jGraphT package 7 ; and for the machine learning toolkit, we use the UniverSVM Enlightened by, we exclude some dependency relations like \"CONJ\", \"COORD\", \"APPO\", etc., heuristically, since inmost of the cases, they will not change the relationship between the two words at both ends of the path.", "labels": [], "entities": []}, {"text": "means whether T has the corresponding paths (using the same word pairs found in H); Dir is whether the direction of the path T the same as H; Multi?", "labels": [], "entities": [{"text": "Dir", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.982893168926239}]}, {"text": "adds a prefix, m , to the Rel Pair features, if the T-path is longer than one dependency relation; Dep Same?", "labels": [], "entities": []}, {"text": "checks whether the two dependency types are the same, i.e. syntactic and semantic dependencies; Rel Sim?", "labels": [], "entities": [{"text": "Rel Sim?", "start_pos": 96, "end_pos": 104, "type": "TASK", "confidence": 0.6605115532875061}]}, {"text": "only occurs when two semantic dependencies are compared, meaning whether they have the same prefixes, e.g. C-, AM-, etc.; Rel Same?", "labels": [], "entities": []}, {"text": "checks whether the two dependency relations are the same; and Rel Pair simple concatenates the two relation labels together.", "labels": [], "entities": []}, {"text": "Notice that, the first seven feature types all contain boolean values, and for the last one, we make it boolean as well, by observing whether that pair of dependency labels appear or not.", "labels": [], "entities": []}, {"text": "package 8 . We test different dependency graphs and feature sets as mentioned before, and the results are shown in: Experiment results of our system with different settings.", "labels": [], "entities": []}, {"text": "First of all, in almost all the cases, the grammaticality checking based on HPSG parsing is helpful, if we compare each pair of results at the two rows, +GC and -GC.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.8175862729549408}]}, {"text": "In all cases, the joint graph representation achieves better results.", "labels": [], "entities": []}, {"text": "This indicates that features extracted from both syntactic dependency and shallow semantic dependency are useful for the entailment recognition.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.8700105547904968}]}, {"text": "For the MaltParser case, the semantic features show great importance.", "labels": [], "entities": []}, {"text": "Notice that the performance of the whole system does not necessarily reflect the performance of the parser itself, since it also depends on our entailment modules.", "labels": [], "entities": []}, {"text": "In all, the best setting of our system ranks the 3rd place in the evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experiment results of our system with  different settings.", "labels": [], "entities": []}]}