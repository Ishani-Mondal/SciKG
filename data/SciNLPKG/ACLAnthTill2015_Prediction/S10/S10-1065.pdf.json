{"title": [{"text": "VENSES++: Adapting a deep semantic processing system to the identification of null instantiations", "labels": [], "entities": [{"text": "identification of null instantiations", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.7593913227319717}]}], "abstractContent": [{"text": "The system to spot INIs, DNIs and their antecedents is an adaptation of VENSES, a system for semantic evaluation that has been used for RTE challenges in the last 6 years.", "labels": [], "entities": [{"text": "VENSES", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.7418140769004822}, {"text": "RTE challenges", "start_pos": 136, "end_pos": 150, "type": "TASK", "confidence": 0.9324443936347961}]}, {"text": "In the following we will briefly describe the system and then the additions we made to cope with the new task.", "labels": [], "entities": []}, {"text": "In particular, we will discuss how we mapped the VENSES analysis to the representation of frame information in order to identify null instantia-tions in the text.", "labels": [], "entities": [{"text": "VENSES", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.5556728839874268}]}], "introductionContent": [{"text": "The SemEval-2010 task for linking events and their participants in discourse) introduced anew issue w.r.t. the SemEval-2007 task \"Frame Semantic Structure Extraction\" (, in that it focused on linking local semantic argument structures across sentence boundaries.", "labels": [], "entities": [{"text": "Frame Semantic Structure Extraction", "start_pos": 130, "end_pos": 165, "type": "TASK", "confidence": 0.7292613834142685}]}, {"text": "Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (, then the identification of locally uninstantiated roles (NIs).", "labels": [], "entities": []}, {"text": "If these roles are indefinite (INI), they have to be marked as such and no antecedent has to be found.", "labels": [], "entities": [{"text": "INI", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9481691718101501}]}, {"text": "On the contrary, if they are definite (DNI), their coreferents have to be found in the wider discourse context.", "labels": [], "entities": []}, {"text": "The challenge comprised two tasks, namely the full task (semantic role recognition and labelling + NI linking) and the NIs only task, i.e. the identification of null instantiations and their referents given a test set with gold standard local semantic argument structure.", "labels": [], "entities": [{"text": "semantic role recognition", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.6533040106296539}]}, {"text": "We took part to the NIs only task by modifying the VENSES system for deep semantic processing and entailment recognition).", "labels": [], "entities": [{"text": "VENSES", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.7948983907699585}, {"text": "entailment recognition", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.7805083394050598}]}, {"text": "In our approach, we assume that the identification of null instantiations is a complex task requiring different levels of semantic knowledge and several processing steps.", "labels": [], "entities": [{"text": "identification of null instantiations", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.8700765520334244}]}, {"text": "For this reason, we believe that the rich analysis performed by the pipeline architecture of VENSES is particularly suitable for the task, also due to the small amount of training data available and the heterogeneity of NI phenomena.", "labels": [], "entities": [{"text": "VENSES", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.9004393219947815}]}], "datasetContent": [{"text": "The SemEval test data comprise two annotated documents extracted from Conan Doyle's novels.", "labels": [], "entities": [{"text": "SemEval test data", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7896572947502136}]}, {"text": "We report some statistics about the test data with gold standard annotation and a comparison with our system output in The amount of NIs detected by our system is much lower than the gold standard one, particularly for INIs.", "labels": [], "entities": []}, {"text": "This depends partly on the fact that no specific strategy for INI detection with nominal predicates has been devised so far, as described in Section 3.2.", "labels": [], "entities": [{"text": "INI detection", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9725055694580078}]}, {"text": "Another problem is that a lot of DNIs in the gold standard don't get resolved, while our system always looks fora re-ferent in case of DNIs and if it is not found, the procedure fails.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9303261041641235}]}, {"text": "The issue of detecting which DNIs are liable not to have an explicit antecedent remains an open problem.", "labels": [], "entities": [{"text": "detecting which DNIs", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7881359259287516}]}, {"text": "In general, suggest to treat the DNI identification and binding as a coreference resolution task.", "labels": [], "entities": [{"text": "DNI identification and binding", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8961115628480911}, {"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.8603297173976898}]}, {"text": "However, the only information available is in fact the label of the missing FE.", "labels": [], "entities": [{"text": "FE", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9141690135002136}]}, {"text": "The authors propose to obtain information about the likely fillers of a missing FE from annotated data sets, but the task showed that this procedure could be successful only in case all FE labels are semantically well identifiable: in fact many FE labels are devoid of any specific associated meaning.", "labels": [], "entities": []}, {"text": "Furthermore, lexical fillers of a given semantic role in the FrameNet data sets can be as diverse as possible.", "labels": [], "entities": [{"text": "FrameNet data sets", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.9517915844917297}]}, {"text": "For example, a complete search in the FrameNet database for the FE Charges will reveal heads like \"possession, innocent, actions\", where the significant portion of text addressed by the FE would be in the specification -i.e. \"possession of a gun\" etc.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.9476737678050995}, {"text": "FE Charges", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.8867209255695343}, {"text": "FE", "start_pos": 186, "end_pos": 188, "type": "DATASET", "confidence": 0.8765987157821655}]}, {"text": "Only in case of highly specialized FEs there will be some help in the semantic characterization of a possible antecedent.", "labels": [], "entities": [{"text": "semantic characterization", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7658379375934601}]}, {"text": "Another open issue is the notion of context where the antecedent should be searched for, which is lacking an appropriate definition.", "labels": [], "entities": []}, {"text": "If we take into account our system results on Text 1, we notice that only 3 DNIs have been identified and linked to the correct antecedent, while the overall amount of exact matches including INIs is 7.", "labels": [], "entities": [{"text": "Text 1", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.9098628163337708}, {"text": "INIs", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.8917788863182068}]}, {"text": "However, in 21 other cases the system correctly identifies a null instantiated role and assigns the right FE label, but it either detects an INI instead of a DNI (and vice-versa), or it finds the wrong antecedent for the DNI.", "labels": [], "entities": [{"text": "FE label", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9748788475990295}]}, {"text": "A similar performance is achieved on Text 2: no DNI has been linked to the correct antecedent, and in only 8 cases there is an exact match between the INIs identified by the system and those in the gold standard.", "labels": [], "entities": []}, {"text": "However, in 18 cases a null instantiation is detected and assigned the correct FE label, even if either the referent or the definiteness label is wrong.", "labels": [], "entities": [{"text": "FE label", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9768895208835602}]}, {"text": "Some evaluation metrics taking into account the different information layers conveyed by the system would help highlighting such differences and pointing out the NI identification steps that need to be consolidated.", "labels": [], "entities": [{"text": "NI identification", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.7997079491615295}]}], "tableCaptions": [{"text": " Table 1: Comparison between gold standard and  system output", "labels": [], "entities": []}]}