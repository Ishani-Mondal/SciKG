{"title": [{"text": "KX: A flexible system for Keyphrase eXtraction", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present KX, a system for key-phrase extraction developed at FBK-IRST, which exploits basic linguistic annotation combined with simple statistical measures to select a list of weighted keywords from a document.", "labels": [], "entities": [{"text": "key-phrase extraction", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7214011251926422}, {"text": "FBK-IRST", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9806593060493469}]}, {"text": "The system is flexible in that it offers to the user the possibility of setting parameters such as frequency thresholds for col-location extraction and indicators for key-phrase relevance, as well as it allows for domain adaptation exploiting a corpus of documents in an unsupervised way.", "labels": [], "entities": [{"text": "domain adaptation exploiting a corpus of documents", "start_pos": 214, "end_pos": 264, "type": "TASK", "confidence": 0.8579624380384173}]}, {"text": "KX is also easily adaptable to new languages in that it requires only a PoS-Tagger to derive lexical patterns.", "labels": [], "entities": []}, {"text": "In the SemEval task 5 \"Automatic Key-phrase Extraction from Scientific Articles\", KX performance achieved satisfactory results both in finding reader-assigned keywords and in the combined keywords subtask.", "labels": [], "entities": [{"text": "SemEval task 5 \"Automatic Key-phrase Extraction from Scientific Articles", "start_pos": 7, "end_pos": 79, "type": "TASK", "confidence": 0.734460636973381}]}], "introductionContent": [{"text": "Keyphrases are expressions, either single words or phrases, describing the most important concepts of a document.", "labels": [], "entities": []}, {"text": "As such, a list of keyphrases provides an approximate but useful characterization of the content of a text and can be used in a number of interesting ways both for human and automatic processing.", "labels": [], "entities": []}, {"text": "For example, keyphrases provide a sort of quick summary of a document.", "labels": [], "entities": []}, {"text": "This can be exploited not only in automatic summarization tasks, but also to enable quick topic search over a number of documents indexed according to their keywords, which is more precise and efficient than full-text search.", "labels": [], "entities": [{"text": "summarization", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.8732529282569885}]}, {"text": "Once the keywords of a document collection are known, they can also be used to calculate semantic similarity between documents and to cluster the texts according to such similarity ().", "labels": [], "entities": []}, {"text": "Also, keyword extraction can be used as an intermediate step for automatic sense extraction).", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8150834143161774}, {"text": "automatic sense extraction", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.6375108957290649}]}, {"text": "For these reasons, the keyphrase extraction task proposed at SemEval 2010 raised much attention among NLP researchers, with 20 groups participating to the competition.", "labels": [], "entities": [{"text": "keyphrase extraction task", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.8969777425130209}, {"text": "SemEval 2010", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.7188328206539154}]}, {"text": "In this framework, we presented the KX system, specifically tuned to identify keyphrases in scientific articles.", "labels": [], "entities": []}, {"text": "In particular, the challenge comprised two subtasks: the extraction of reader-assigned and of author-assigned keyphrases in scientific articles from the ACM digital library.", "labels": [], "entities": [{"text": "extraction of reader-assigned and of author-assigned keyphrases in scientific articles", "start_pos": 57, "end_pos": 143, "type": "TASK", "confidence": 0.6525517702102661}, {"text": "ACM digital library", "start_pos": 153, "end_pos": 172, "type": "DATASET", "confidence": 0.711560328801473}]}, {"text": "The former are assigned to the articles by annotators, who can choose only keyphrases that occur in the document, while author-assigned keyphrases are not necessarily included in the text.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the SemEval task, 144 training files were made available before the test data release.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.9327806830406189}]}, {"text": "We split them into a training/development set of 100 documents and a test set of 44 documents, in order to find the best parameter combination.", "labels": [], "entities": []}, {"text": "Keyphrase assignment is a subjective task and criteria for keyphrase identification depend on the domain and on the goal for which the keyphrases are needed.", "labels": [], "entities": [{"text": "Keyphrase assignment", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9070384204387665}, {"text": "keyphrase identification", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7877011001110077}]}, {"text": "For example in scientific articles longer keyphrases are often more informative than shorter ones, so the parameters for boosting longer concepts are particularly relevant.", "labels": [], "entities": []}, {"text": "We first tested all parameters in isolation to compute the improvement over the frequencybased baseline.", "labels": [], "entities": []}, {"text": "F1 is computed as the harmonic mean of precision and recall over the 15 top-ranked keyphrases after stemming.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9969923496246338}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9992364645004272}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9983518123626709}]}, {"text": "We report the combined F1, as computed by the task scorer in order to combine reader-assigned and author-assigned keyword sets.", "labels": [], "entities": [{"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9959204196929932}]}, {"text": "The parameter scoring the highest improvement over the baseline is IDF.", "labels": [], "entities": [{"text": "IDF", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9992907047271729}]}, {"text": "Also the parameters boosting longer keyphrases and those that occur at the beginning of the text are effective.", "labels": [], "entities": []}, {"text": "Note that the LongConcBoost parameter achieves better results in the first version, which has a higher impact on the re-ranking.", "labels": [], "entities": [{"text": "LongConcBoost", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.7485353350639343}]}, {"text": "Surprisingly, using a domain corpus to extract information about multiword terms, as described in Section 2, steps 1 -3, does not achieve any improvement.", "labels": [], "entities": []}, {"text": "This means that KX can better recognize keyphrases in single documents without any corpus reference.", "labels": [], "entities": []}, {"text": "Besides, the best setting for MinDoc, the minimum number of multiword occurrences in the current document (see Section 2) is 4.", "labels": [], "entities": []}, {"text": "We tested the CorpusColloc parameter using two different reference corpora: one contained the 100 articles of the training set (CorpusColloc small), while the other (CorpusColloc big) included both the 100 training articles and the 200 scientific publications of the NUS Keyphrase Corpus.", "labels": [], "entities": [{"text": "CorpusColloc", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.8791323304176331}, {"text": "NUS Keyphrase Corpus", "start_pos": 267, "end_pos": 287, "type": "DATASET", "confidence": 0.9682090679804484}]}, {"text": "The performance is worse using the larger corpus than the smaller one, and in both cases it is below the baseline obtained without any reference corpus.", "labels": [], "entities": []}, {"text": "In the bottom row of, the best parameter combination is reported with the score obtained over the development set.", "labels": [], "entities": []}, {"text": "The improvement over the baseline reaches 11.99 F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9896830320358276}]}, {"text": "In the SemEval task, the system was run on the test set (100 articles) with the best performing parameter combination described in the previous section.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.9228972792625427}]}, {"text": "The results obtained over the 15 topranked keyphrases are reported in In the competition, the F1 score over readerassigned keyphrases was ranked 3 rd out of 20 participants, while the combined measure achieved the 7 th best result out of 20.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9812508821487427}]}], "tableCaptions": [{"text": " Table 1.  F1 is computed as the harmonic mean of preci- sion and recall over the 15 top-ranked key- phrases after stemming. We report the combined  F1, as computed by the task scorer in order to  combine reader-assigned and author-assigned  keyword sets.", "labels": [], "entities": [{"text": "F1", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9990766048431396}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9957311749458313}, {"text": "F1", "start_pos": 149, "end_pos": 151, "type": "METRIC", "confidence": 0.9880917072296143}]}, {"text": " Table 1: Parameter performance over development set", "labels": [], "entities": [{"text": "Parameter", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9665254950523376}]}, {"text": " Table 2: System performance over test set", "labels": [], "entities": []}]}