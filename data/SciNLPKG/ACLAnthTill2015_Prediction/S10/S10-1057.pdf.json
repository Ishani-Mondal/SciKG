{"title": [{"text": "UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources", "labels": [], "entities": [{"text": "UTD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7173317074775696}]}], "abstractContent": [{"text": "This paper describes our system for SemEval-2010 Task 8 on multi-way classification of semantic relations between nominals.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.884321371714274}, {"text": "multi-way classification of semantic relations between nominals", "start_pos": 59, "end_pos": 122, "type": "TASK", "confidence": 0.8103292967591967}]}, {"text": "First, the type of semantic relation is classified.", "labels": [], "entities": []}, {"text": "Then a relation type-specific classifier determines the relation direction.", "labels": [], "entities": []}, {"text": "Classification is performed using SVM classifiers and a number of features that capture the context, semantic role affiliation, and possible pre-existing relations of the nominals.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.946276843547821}]}, {"text": "This approach achieved an F1 score of 82.19% and an accuracy of 77.92%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9883365035057068}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9998428821563721}]}], "introductionContent": [{"text": "SemEval-2010 Task 8 evaluated the multi-way classification of semantic relations between nominals in a sentence (.", "labels": [], "entities": [{"text": "multi-way classification of semantic relations between nominals in a sentence", "start_pos": 34, "end_pos": 111, "type": "TASK", "confidence": 0.7736793100833893}]}, {"text": "Given two nominals embedded in a sentence, the task requires identifying which of the following nine semantic relations holds between the nominals: Cause-Effect, Instrument-Agency, Product-Producer, Content-Container, EntityOrigin, Entity-Destination, Component-Whole, Member-Collection, Message-Topic, or Other if no other relation is appropriate.", "labels": [], "entities": []}, {"text": "For instance, the following sentence provides an example of the Entity-Destination relation: \"A small [piece] E1 of rock landed into the .\" The two nominals given for this sentence are E 1 (piece) and E 2 (trunk).", "labels": [], "entities": []}, {"text": "This is an EntityDestination relation because the piece of rock originated from outside of the trunk, but ended up there.", "labels": [], "entities": []}, {"text": "Finally, the direction of the relation is (E 1 ,E 2 ) because E 1 , the piece, is the Entity and E 2 , the trunk, is the Destination.", "labels": [], "entities": []}, {"text": "Analysis of the training data revealed three major classes of knowledge required for recognizing semantic relations: (i) examples that require background knowledge of an existing relation between the nominals (e.g., example 5884 below), (ii) examples using background knowledge regarding the typical role of one of the nominals (e.g., example 3402), and (iii) examples that require contextual cues to disambiguate the role between the nominals (e.g., example 5710).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Overall and individual relation scores on  the test set, along with precision and recall", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9997901320457458}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9992629885673523}]}, {"text": " Table 3: Scores obtained for various sets of fea- tures on the training set. The bottom portion of  the table shows the best combination containing 1  to 8 feature sets", "labels": [], "entities": []}]}