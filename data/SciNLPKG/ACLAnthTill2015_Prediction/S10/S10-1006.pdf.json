{"title": [{"text": "SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals", "labels": [], "entities": [{"text": "Multi-Way Classification of Semantic Relations Between Pairs of Nominals", "start_pos": 21, "end_pos": 93, "type": "TASK", "confidence": 0.8436072932349311}]}], "abstractContent": [{"text": "SemEval-2 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals.", "labels": [], "entities": [{"text": "Multi-way classification of semantic relations between pairs of nominals", "start_pos": 28, "end_pos": 100, "type": "TASK", "confidence": 0.8593174682723151}]}, {"text": "The task was designed to compare different approaches to semantic relation classification and to provide a standard testbed for future research.", "labels": [], "entities": [{"text": "semantic relation classification", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.8201957543691}]}, {"text": "This paper defines the task, describes the training and test data and the process of their creation, lists the participating systems (10 teams, 28 runs), and discusses their results.", "labels": [], "entities": []}], "introductionContent": [{"text": "SemEval-2010 Task 8 focused on semantic relations between pairs of nominals.", "labels": [], "entities": []}, {"text": "For example, tea and ginseng are in an ENTITY-ORIGIN relation in \"The cup contained tea from dried ginseng.\".", "labels": [], "entities": [{"text": "ENTITY-ORIGIN", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9953852295875549}]}, {"text": "The automatic recognition of semantic relations has many applications, such as information extraction, document summarization, machine translation, or construction of thesauri and semantic networks.", "labels": [], "entities": [{"text": "automatic recognition of semantic relations", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.7669129967689514}, {"text": "information extraction", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.8373921811580658}, {"text": "document summarization", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.6982628703117371}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7715169489383698}]}, {"text": "It can also facilitate auxiliary tasks such as word sense disambiguation, language modeling, paraphrasing, and recognizing textual entailment.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.6981250643730164}, {"text": "language modeling", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7545511722564697}, {"text": "recognizing textual entailment", "start_pos": 111, "end_pos": 141, "type": "TASK", "confidence": 0.8142929474512736}]}, {"text": "Our goal was to create a testbed for automatic classification of semantic relations.", "labels": [], "entities": [{"text": "automatic classification of semantic relations", "start_pos": 37, "end_pos": 83, "type": "TASK", "confidence": 0.7355130076408386}]}, {"text": "In developing the task we met several challenges: selecting a suitable set of relations, specifying the annotation procedure, and deciding on the details of the task itself.", "labels": [], "entities": []}, {"text": "They are discussed briefly in Section 2; see also, which includes a survey of related work.", "labels": [], "entities": []}, {"text": "The direct predecessor of Task 8 was Classification of semantic relations between nominals, Task 4 at SemEval-1 (), which had a separate binary-labeled dataset for each of seven relations.", "labels": [], "entities": [{"text": "Classification of semantic relations between nominals", "start_pos": 37, "end_pos": 90, "type": "TASK", "confidence": 0.896360864241918}]}, {"text": "We have defined SemEval-2010 Task 8 as a multi-way classification task in which the label for each example must be chosen from the complete set often relations and the mapping from nouns to argument slots is not provided in advance.", "labels": [], "entities": []}, {"text": "We also provide more data: 10,717 annotated examples, compared to 1,529 in SemEval-1 Task 4.", "labels": [], "entities": [{"text": "SemEval-1 Task 4", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7546517054239908}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: F 1 -Score of all submitted systems on the test dataset as a function of training data: TD1=1000,  TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results  marked with  *  were submitted after the deadline. The best-performing run for each participant is italicized.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9820377826690674}, {"text": "TD4", "start_pos": 192, "end_pos": 195, "type": "DATASET", "confidence": 0.9802823066711426}]}]}