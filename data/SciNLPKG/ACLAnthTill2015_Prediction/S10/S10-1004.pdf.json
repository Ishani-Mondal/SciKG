{"title": [{"text": "SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific Articles", "labels": [], "entities": [{"text": "SemEval-2010 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8432237207889557}, {"text": "Automatic Keyphrase Extraction from Scientific Articles", "start_pos": 21, "end_pos": 76, "type": "TASK", "confidence": 0.7210370649894079}]}], "abstractContent": [{"text": "This paper describes Task 5 of the Workshop on Semantic Evaluation 2010 (SemEval-2010).", "labels": [], "entities": [{"text": "Semantic Evaluation 2010 (SemEval-2010)", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.69169353445371}]}, {"text": "Systems are to automatically assign keyphrases or keywords to given scientific articles.", "labels": [], "entities": []}, {"text": "The participating systems were evaluated by matching their extracted keyphrases against manually assigned ones.", "labels": [], "entities": []}, {"text": "We present the overall ranking of the submitted systems and discuss our findings to suggest future directions for this task.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Traditionally, automatic keyphrase extraction systems have been assessed using the proportion of top-N candidates that exactly match the goldstandard keyphrases).", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8340173065662384}]}, {"text": "In some cases, inexact matches, or near-misses, have also been considered.", "labels": [], "entities": []}, {"text": "Some have suggested treating semanticallysimilar keyphrases as correct based on similarities computed over a large corpus (Jarmasz and Barriere, 2004; Mihalcea and Tarau, 2004), or using semantic relations defined in a thesaurus ().", "labels": [], "entities": []}, {"text": "Zesch and Gurevych (2009) compute near-misses using an ngram based approach relative to the gold standard.", "labels": [], "entities": []}, {"text": "For our shared task, we follow the traditional exact match evaluation metric.", "labels": [], "entities": []}, {"text": "That is, we match the keyphrases in the answer set with those the systems provide, and calculate micro-averaged precision, recall and F-score (\u03b2 = 1).", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9439589381217957}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9997528195381165}, {"text": "F-score", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.9994044303894043}]}, {"text": "In the evaluation, we check the performance over the top 5, 10 and 15 candidates returned by each system.", "labels": [], "entities": []}, {"text": "We rank the participating systems by F-score over the top 15 candidates.", "labels": [], "entities": [{"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9984087347984314}]}, {"text": "Participants were required to extract existing phrases from the documents.", "labels": [], "entities": []}, {"text": "Since it is theoretically possible to retrieve authorassigned keyphrases from the original PDF articles, we evaluate the participating systems over the independently-generated and held-out readerassigned keyphrases, as well as the combined set of keyphrases (author-and reader-assigned).", "labels": [], "entities": []}, {"text": "All keyphrases in the answer set are stemmed using the English Porter stemmer for both the training and test dataset.", "labels": [], "entities": []}, {"text": "We computed a TF\u00d7IDF n-gram based baseline using both supervised and unsupervised learning systems.", "labels": [], "entities": []}, {"text": "We use 1, 2, 3-grams as keyphrase candidates, used Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB) and Maximum Entropy (ME) classifiers to learn two supervised baseline systems based on the keyphrase candidates and gold-standard annotations for the training documents.", "labels": [], "entities": [{"text": "Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB) and Maximum Entropy (ME)", "start_pos": 51, "end_pos": 99, "type": "METRIC", "confidence": 0.7360217650731404}]}, {"text": "In total, there are three baselines: two supervised and one unsupervised.", "labels": [], "entities": []}, {"text": "The performance of the baselines is presented in, where R indicates reader-assigned keyphrases and C indicates combined (both author-and readerassigned) keyphrases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents per topic in the  trial, training and test datasets, across the four  ACM document classifications", "labels": [], "entities": []}, {"text": " Table 2: Number of author-and reader-assigned  keyphrases in the different datasets", "labels": [], "entities": []}, {"text": " Table 3: Baseline keyphrase extraction performance for one unsupervised (TF\u00d7IDF) and two supervised  (NB and ME) systems", "labels": [], "entities": [{"text": "Baseline keyphrase extraction", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6198248962561289}]}, {"text": " Table 4: Performance of the submitted systems over the combined author-and reader-assigned keywords,  ranked by F-score", "labels": [], "entities": [{"text": "F-score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.9957014918327332}]}, {"text": " Table 5: Performance of the submitted systems over the reader-assigned keywords, ranked by F-score", "labels": [], "entities": [{"text": "F-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9962582588195801}]}, {"text": " Table 6: Performance of the submitted systems over the author-assigned keywords, ranked by F-score", "labels": [], "entities": [{"text": "F-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9962185025215149}]}]}