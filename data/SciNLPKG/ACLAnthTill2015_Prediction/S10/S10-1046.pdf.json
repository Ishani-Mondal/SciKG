{"title": [{"text": "TUD: semantic relatedness for relation classification", "labels": [], "entities": [{"text": "relation classification", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8856921195983887}]}], "abstractContent": [{"text": "In this paper, we describe the system submitted by the team TUD to Task 8 at SemEval 2010.", "labels": [], "entities": [{"text": "TUD to Task 8 at SemEval 2010", "start_pos": 60, "end_pos": 89, "type": "DATASET", "confidence": 0.6150467949254173}]}, {"text": "The challenge focused on the identification of semantic relations between pairs of nominals in sentences collected from the web.", "labels": [], "entities": [{"text": "identification of semantic relations between pairs of nominals in sentences", "start_pos": 29, "end_pos": 104, "type": "TASK", "confidence": 0.8016614437103271}]}, {"text": "We applied maximum entropy classification using both lexical and syntactic features to describe the nominals and their context.", "labels": [], "entities": [{"text": "maximum entropy classification", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.6707988182703654}]}, {"text": "In addition , we experimented with features describing the semantic relatedness (SR) between the target nominals and a set of clue words characteristic to the relations.", "labels": [], "entities": [{"text": "semantic relatedness (SR)", "start_pos": 59, "end_pos": 84, "type": "METRIC", "confidence": 0.6647436916828156}]}, {"text": "Our best submission with SR features achieved 69.23% macro-averaged F-measure, providing 8.73% improvement over our base-line system.", "labels": [], "entities": [{"text": "SR", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.8176238536834717}, {"text": "F-measure", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9562799334526062}]}, {"text": "Thus, we think SR can serve as a natural way to incorporate external knowledge to relation classification.", "labels": [], "entities": [{"text": "SR", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9800353646278381}, {"text": "relation classification", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.8634814918041229}]}], "introductionContent": [{"text": "Automatic extraction of typed semantic relations between sentence constituents is an important step towards deep semantic analysis and understanding the semantic content of natural language texts.", "labels": [], "entities": [{"text": "Automatic extraction of typed semantic relations between sentence constituents", "start_pos": 0, "end_pos": 78, "type": "TASK", "confidence": 0.7564567062589858}, {"text": "deep semantic analysis", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.6801785628000895}, {"text": "understanding the semantic content of natural language texts", "start_pos": 135, "end_pos": 195, "type": "TASK", "confidence": 0.6188324615359306}]}, {"text": "Identification of relations between a nominal and the main verb, and between pairs of nominals are important steps for the extraction of structured semantic information from text, and can benefit various applications ranging from Information Extraction and Information Retrieval to Machine Translation or Question Answering.", "labels": [], "entities": [{"text": "extraction of structured semantic information from text", "start_pos": 123, "end_pos": 178, "type": "TASK", "confidence": 0.799792902810233}, {"text": "Information Extraction", "start_pos": 230, "end_pos": 252, "type": "TASK", "confidence": 0.7721539437770844}, {"text": "Information Retrieval", "start_pos": 257, "end_pos": 278, "type": "TASK", "confidence": 0.7802025973796844}, {"text": "Machine Translation", "start_pos": 282, "end_pos": 301, "type": "TASK", "confidence": 0.8424019515514374}, {"text": "Question Answering", "start_pos": 305, "end_pos": 323, "type": "TASK", "confidence": 0.8430427014827728}]}, {"text": "The Multi-Way Classification of Semantic Relations Between Pairs of Nominals challenge) focused on the identification of specific relation types between nominals (nouns or base noun phrases) in natural language sentences collected from the web.", "labels": [], "entities": [{"text": "Multi-Way Classification of Semantic Relations Between Pairs of Nominals", "start_pos": 4, "end_pos": 76, "type": "TASK", "confidence": 0.7965505189365811}, {"text": "identification of specific relation types between nominals (nouns or base noun phrases) in natural language sentences", "start_pos": 103, "end_pos": 220, "type": "TASK", "confidence": 0.7273151609632704}]}, {"text": "The main task of the challenge was to identify and classify instances of 9 abstract semantic relations between noun phrases, i.e. Cause-Effect, InstrumentAgency, Product-Producer, Content-Container, Entity-Origin, Entity-Destination, ComponentWhole, Member-Collection, Message-Topic.", "labels": [], "entities": []}, {"text": "That is, given two nominals (e1 and e2) in a sentence, systems had to decide whether relation(e1,e2), relation(e2,e1) holds for one of the relation types or the nominals' relation is other (falls to a category not listed above or they are unrelated).", "labels": [], "entities": []}, {"text": "In this sense, the challenge was an important pilot task towards large scale semantic processing of text.", "labels": [], "entities": []}, {"text": "In this paper, we describe the system we submitted to Semeval 2010, Task 8.", "labels": [], "entities": [{"text": "Semeval 2010, Task 8", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.6718218684196472}]}, {"text": "We applied maximum entropy classification to the problem using both lexical and contextual features to describe the nominals themselves and their context (i.e. the sentence).", "labels": [], "entities": [{"text": "maximum entropy classification", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.6558634042739868}]}, {"text": "In addition, we experimented with features exploiting the strength of association between the target nominals and a predefined set of clue words characteristic to the nine relation types.", "labels": [], "entities": []}, {"text": "In order to measure the semantic relatedness (SR) of targets and clues, we used the Explicit Semantic Analysis () SR measure (based on Wikipedia, Wiktionary and WordNet).", "labels": [], "entities": [{"text": "semantic relatedness (SR)", "start_pos": 24, "end_pos": 49, "type": "METRIC", "confidence": 0.5414400219917297}, {"text": "Explicit Semantic Analysis () SR measure", "start_pos": 84, "end_pos": 124, "type": "METRIC", "confidence": 0.6271785298983256}, {"text": "WordNet", "start_pos": 161, "end_pos": 168, "type": "DATASET", "confidence": 0.8919979929924011}]}, {"text": "Our best submission, benefiting from SR features, achieved 69.23% macro-averaged Fmeasure for the 9 relation types used.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9107731580734253}]}, {"text": "Providing 8.73% improvement over our baseline system, we found the SR-based features to be beneficial for the classification of semantic relations.", "labels": [], "entities": [{"text": "classification of semantic relations", "start_pos": 110, "end_pos": 146, "type": "TASK", "confidence": 0.8289374262094498}]}], "datasetContent": [{"text": "Feature set In our system, we used both lexical (1-3) and contextual features (4-8) to describe the nominals and their context (i.e. the sentence).", "labels": [], "entities": []}, {"text": "Additionally, we experimented with a set of features (9) that exploit the co-occurrence statistics of the nominals and a set of clue words chosen manually, examining the relation definitions and examples provided by the organizers.", "labels": [], "entities": []}, {"text": "The clues characterize the relations addressed in the task (e.g. cargo, goods, content, box, bottle characterize the Content-Container relation) . Each feature type was distinguished from the others using a prefix.", "labels": [], "entities": []}, {"text": "All but the semantic relatedness features we used were binary, denoting whether a specific word, lemma, POS tag, etc. is found in the example sentence, or not.", "labels": [], "entities": []}, {"text": "SR features were real valued, scaled to for each clue word separately (on train, and the same scaling factores were applied on the test data).", "labels": [], "entities": [{"text": "SR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9417738914489746}]}, {"text": "The feature types used: 1.", "labels": [], "entities": []}, {"text": "Token: word unigrams in the sentence in their inflected form.", "labels": [], "entities": []}, {"text": "2. Lemma: word uni-and bigrams in the sentence in their lemmatized form.", "labels": [], "entities": []}, {"text": "3. Target Nouns: the syntactic head words of the target nouns.", "labels": [], "entities": []}, {"text": "4. POS: the part of speech uni-and biand trigrams in the sentence.", "labels": [], "entities": [{"text": "POS", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.8910732269287109}]}, {"text": "5. Between POS: the part of speech sequence between the target nouns.", "labels": [], "entities": []}, {"text": "6. Dependency Path: the dependency path (syntactic relations and directions) between the target nouns.", "labels": [], "entities": []}, {"text": "The whole path constituted a single feature.", "labels": [], "entities": []}, {"text": "7. Target Distance: the distance between the target nouns (in tokens).", "labels": [], "entities": [{"text": "Target Distance", "start_pos": 3, "end_pos": 18, "type": "METRIC", "confidence": 0.8871786594390869}]}, {"text": "8. Sentence Length: the length of the sentence (in tokens).", "labels": [], "entities": []}, {"text": "9. Semantic Relatedness: the semantic relatedness scores measuring the strength of association between the target nominals and the set of clue words we collected.", "labels": [], "entities": [{"text": "Semantic Relatedness", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7571017146110535}]}, {"text": "In order to measure the semantic relatedness (SR) of targets and clues, we used the Explicit Semantic Analysis (ESA) SR measure.", "labels": [], "entities": []}, {"text": "Feature selection In order to discard uninformative features automatically, we performed feature selection on the binary features.", "labels": [], "entities": []}, {"text": "We kept features that satisfied the following three conditions: where f req(x) denotes the frequency of feature x observed in the training dataset, y denotes a class label, p denotes the highest posterior probability (for feature x) over the nine relations (undirected) and the other class.", "labels": [], "entities": []}, {"text": "Finally, t 1 , t 2 are filtering thresholds chosen arbitrarily.", "labels": [], "entities": []}, {"text": "We used t 1 = 0.25 for all features but the dependency path, where we: Performance of different learning methods on train (10-fold).", "labels": [], "entities": []}, {"text": "used t 1 = 0.2.", "labels": [], "entities": []}, {"text": "We set the thresold t 2 to 1.9 for lexical features (i.e. token and lemma features), to 0.3 for dependency path features and to 0.9 for all other features.", "labels": [], "entities": []}, {"text": "All parameters for the feature selection process were chosen manually (crossvalidating the parameters was omitted due to lack of time during the challenge development period).", "labels": [], "entities": [{"text": "feature selection process", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.8322359919548035}]}, {"text": "The higher t 2 value for lexical features was motivated by the aim to avoid overfitting, and the lower thresholds for dependency-based features by the hypothesis that these can be most efficient to determine the direction of relationships (c.f. we disregarded direction during feature selection).", "labels": [], "entities": []}, {"text": "As the numeric SR features were all bound to clue words selected specifically for the task, we did not perform any feature selection for that feature type.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of different learning meth- ods on train (10-fold).", "labels": [], "entities": []}, {"text": " Table 2: Performance of 4 submissions on train (10-fold) and test.", "labels": [], "entities": []}, {"text": " Table 3: Prediction error statistics.", "labels": [], "entities": [{"text": "Prediction error", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6254624426364899}]}]}