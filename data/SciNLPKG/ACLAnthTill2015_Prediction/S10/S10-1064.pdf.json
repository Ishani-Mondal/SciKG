{"title": [{"text": "CityU-DAC: Disambiguating Sentiment-Ambiguous Adjectives within Context", "labels": [], "entities": [{"text": "CityU-DAC", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8650856018066406}]}], "abstractContent": [{"text": "This paper describes our system participating in task 18 of SemEval-2010, i.e. disambiguating Sentiment-Ambiguous Adjectives (SAAs).", "labels": [], "entities": []}, {"text": "To disambiguating SAAs, we compare the machine learning-based and lexicon-based methods in our submissions: 1) Maximum entropy is used to train classifiers based on the annotated Chinese data from the NTCIR opinion analysis tasks, and the clause-level and sentence-level classifiers are compared; 2) For the lexicon-based method, we first classify the adjectives into two classes: intensifiers (i.e. adjectives intensifying the intensity of context) and suppressors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs' contextual polarity based on a sentiment lexicon.", "labels": [], "entities": [{"text": "NTCIR opinion analysis tasks", "start_pos": 201, "end_pos": 229, "type": "TASK", "confidence": 0.6609838753938675}]}, {"text": "The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.998950183391571}]}], "introductionContent": [{"text": "In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9695407748222351}]}, {"text": "It has many applications such as social media monitoring, market research, and public relations.", "labels": [], "entities": [{"text": "social media monitoring", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6691976388295492}, {"text": "market research", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.7427235245704651}]}, {"text": "Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context.", "labels": [], "entities": []}, {"text": "Such words can be called dynamic sentiment-ambiguous adjectives (SAAs).", "labels": [], "entities": [{"text": "dynamic sentiment-ambiguous adjectives (SAAs)", "start_pos": 25, "end_pos": 70, "type": "TASK", "confidence": 0.6556443174680074}]}, {"text": "However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work.", "labels": [], "entities": [{"text": "SAAs", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.9617756009101868}, {"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9490594863891602}]}, {"text": "presents an approach of combining collocation information and SVM to disambiguate SAAs, in which the collocationbased method was first used to disambiguate adjectives within the context of collocation (i.e. a sub-sentence marked by comma), and then the SVM algorithm was explored for those instances not covered by the collocation-based method.", "labels": [], "entities": []}, {"text": "According to their experiments, their supervised algorithm achieves encouraging performance.", "labels": [], "entities": []}, {"text": "The task 18 at SemEval-2010 is intended to create a benchmark dataset for disambiguating SAAs.", "labels": [], "entities": []}, {"text": "Given only 100 trial sentences, but not provided with any official training data, participants are required to tackle this problem data by unsupervised approaches or use their own training data.", "labels": [], "entities": []}, {"text": "The task consists of 14 SAAs, which are all high-frequency words in Mandarin Chinese.", "labels": [], "entities": []}, {"text": "They are \u5927|big, \u5c0f|small, \u591a|many, \u5c11 |few, \u9ad8|high, \u4f4e|low, \u539a|thick, \u8584|thin, \u6df1|deep, \u6d45|shallow, \u91cd|heavy, \u8f7b|light, \u5de8\u5927|huge, \u91cd\u5927 |grave.", "labels": [], "entities": []}, {"text": "This task deals with Chinese SAAs, but the disambiguating techniques should be language-independent.", "labels": [], "entities": []}, {"text": "Please refer to () for more descriptions of the task.", "labels": [], "entities": []}, {"text": "In our participating system, the annotated Chinese data from the NTCIR opinion analysis tasks is used as training data with the help of a combined sentiment lexicon.", "labels": [], "entities": [{"text": "NTCIR opinion analysis tasks", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.656159907579422}]}, {"text": "A machine learning-based method (namely maximum entropy) and the lexicon-based method are compared in our submissions.", "labels": [], "entities": []}, {"text": "The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the context of SAAs.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9988659620285034}]}, {"text": "In Section 2, we briefly describe data preparation of sentiment lexicon and training data.", "labels": [], "entities": [{"text": "data preparation of sentiment lexicon", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.6572695434093475}]}, {"text": "Our approaches for disambiguating SAAs are given in Section 3.", "labels": [], "entities": [{"text": "disambiguating SAAs", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8374241292476654}]}, {"text": "The experiment and results are presented in Section 4, followed by a conclusion in Section 5.", "labels": [], "entities": []}, {"text": "), which were manually marked in the political news data by trained annotators . Sentimentbearing items marked with the SENTIMENT_KW tag (SKPI), including only positive and negative items but not neutral ones, were also automatically extracted from the Chinese sample data of NTCIR-6 OAPT (.", "labels": [], "entities": [{"text": "SENTIMENT_KW tag (SKPI)", "start_pos": 120, "end_pos": 143, "type": "METRIC", "confidence": 0.9406638145446777}, {"text": "Chinese sample data of NTCIR-6 OAPT", "start_pos": 253, "end_pos": 288, "type": "DATASET", "confidence": 0.8229782283306122}]}, {"text": "All these polar item lexicons were combined, and the combined polar item lexicon consists of 13,437 positive items and 18,365 negative items, a total of 31,802 items.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset contains two parts: some sentences were extracted from Chinese Gigaword (LDC corpus: LDC2005T14), and other sentences were gathered through the search engine like Google.", "labels": [], "entities": [{"text": "Chinese Gigaword (LDC corpus: LDC2005T14)", "start_pos": 67, "end_pos": 108, "type": "DATASET", "confidence": 0.9222622662782669}]}, {"text": "Firstly, these sentences were automatically segmented and POS-tagged, and then the ambiguous adjectives were manually annotated with the correct sentiment polarity within the sentence context.", "labels": [], "entities": []}, {"text": "Two annotators annotated the sentences double blindly, and the third annotator checks the annotation.", "labels": [], "entities": []}, {"text": "All the data of 2,917 sentences is provided as the test set, and evaluation is performed in terms of micro accuracy and macro accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.5612061023712158}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.8480290174484253}]}, {"text": "We submitted 4 runs: run 1 is based on the sentence-level MaxEnt classifier; run 2 on the clause-level MaxEnt classifier; run 3 is got by combining the lexicon-based method and the sentence-level MaxEnt classifier; and run 4 by combining the lexicon-based method and the clause-level MaxEnt classifier.", "labels": [], "entities": []}, {"text": "The official scores for the 4 runs are shown in., we can observe that: 1) Compared the highest scores achieved by other teams, the performance of maximum entropy (run 1 and 2) is not quite high due to little training data; 2) By integrating the lexicon-based method and maximum entropy (run 3 and 4), we improve the accuracy by considering the context of SAAs; 3) The sentence-level maximum entropy classifier shows better macro accuracy, and clause-level one better micro accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 316, "end_pos": 324, "type": "METRIC", "confidence": 0.9987597465515137}, {"text": "accuracy", "start_pos": 429, "end_pos": 437, "type": "METRIC", "confidence": 0.9481832981109619}, {"text": "accuracy", "start_pos": 473, "end_pos": 481, "type": "METRIC", "confidence": 0.5437353849411011}]}, {"text": "In addition to the official scores, we also evaluate the performance of the lexicon-based method alone.", "labels": [], "entities": []}, {"text": "The micro and macro accuracy are respectively 0.847 and 0.835665, showing that the lexicon-based method is more accurate than the maximum entropy algorithm (run 1 and 2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9420270919799805}]}, {"text": "But it only covers 1,436 (49%) of 2,917 test instances.", "labels": [], "entities": []}, {"text": "Because the data from the NTCIR opinion analysis task is not specifically annotated for this task, and the manually checked clauses are less than 600, the performance of our system is not quite high compared to the highest performance achieved by other teams.", "labels": [], "entities": [{"text": "NTCIR opinion analysis task", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7119953632354736}]}], "tableCaptions": [{"text": " Table 2. Results of 4 Runs  Run Micro Acc. (%) Macro Acc. (%)  1  61.98  67.89  2  62.63  60.85  3  71.55  75.54  4  72.47  69.80  From", "labels": [], "entities": [{"text": "Acc", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.8495010733604431}, {"text": "Acc.", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.8653231263160706}]}]}