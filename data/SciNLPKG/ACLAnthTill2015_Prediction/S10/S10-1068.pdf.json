{"title": [{"text": "372:Comparing the Benefit of Different Dependency Parsers for Textu- al Entailment Using Syntactic Constraints Only", "labels": [], "entities": [{"text": "Textu- al Entailment", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.5141576454043388}]}], "abstractContent": [{"text": "We compare several state of the art dependency parsers with our own parser based on a linear classification technique.", "labels": [], "entities": []}, {"text": "Our primary goal is therefore to use syntactic information only, in order to keep the comparison of the parsers as fair as possible.", "labels": [], "entities": []}, {"text": "We demonstrate, that despite the inferior result using the standard evaluation metrics for parsers like UAS or LAS on standard test data, our system achieves comparable results when used in an application, such as the SemEv-al-2 #12 evaluation exercise PETE.", "labels": [], "entities": [{"text": "SemEv-al-2 #12 evaluation exercise PETE", "start_pos": 218, "end_pos": 257, "type": "TASK", "confidence": 0.6589783132076263}]}, {"text": "Our submission achieved the 4 th position out of 19 participating systems.", "labels": [], "entities": []}, {"text": "However, since it only uses a linear classifier it works 17-20 times faster than other state of the parsers, as for instance MaltParser or Stanford Parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parsing is the process of mapping sentences to their syntactic representations.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9365499019622803}]}, {"text": "These representations can be used by computers for performing many interesting natural language processing tasks, such as question answering or information extraction.", "labels": [], "entities": [{"text": "question answering", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.8770205676555634}, {"text": "information extraction", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.8201193511486053}]}, {"text": "In recent years a lot of parsers have been developed for this purpose.", "labels": [], "entities": []}, {"text": "Avery interesting and important issue is the comparison between a large number of such parsing systems.", "labels": [], "entities": []}, {"text": "The most widespread method is to evaluate the number of correctly recognized units according to a certain gold standard.", "labels": [], "entities": []}, {"text": "For dependency-based units unlabeled or labeled attachment scores (percentage of correctly classified dependency relations, either with or without the dependency relation type) are usually used (cf.).", "labels": [], "entities": []}, {"text": "However, parsing is very rarely a goal in itself.", "labels": [], "entities": [{"text": "parsing", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.9822607636451721}]}, {"text": "In most cases it is a necessary preprocessing step fora certain application.", "labels": [], "entities": []}, {"text": "Therefore it is usually not the best option to decide which parser suits one's goals best by purely looking on its performance on some standard test data set.", "labels": [], "entities": []}, {"text": "It is rather more sensible to analyse whether the parser is able to recognise those syntactic units or relations, which are most relevant for one's application.", "labels": [], "entities": []}, {"text": "The shared task #12 PETE in the SemEval-2010 Evaluation Exercises on Semantic Evaluation) involved recognizing textual entailments (RTE).", "labels": [], "entities": [{"text": "PETE", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9950472116470337}, {"text": "SemEval-2010 Evaluation Exercises on Semantic Evaluation", "start_pos": 32, "end_pos": 88, "type": "TASK", "confidence": 0.7429197430610657}, {"text": "recognizing textual entailments (RTE)", "start_pos": 99, "end_pos": 136, "type": "TASK", "confidence": 0.7104588250319163}]}, {"text": "RTE is a binary classification task, whose goal is to determine, whether fora pair of texts T and H the meaning of H is contained in T ().", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8265522122383118}, {"text": "binary classification task", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7753554582595825}]}, {"text": "This task can be very complex depending on the properties of these texts.", "labels": [], "entities": []}, {"text": "However, for the data, released by the organisers of PETE, only the syntactic information should be sufficient to reliably perform this task.", "labels": [], "entities": [{"text": "PETE", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.68285071849823}]}, {"text": "Thus it offers an ideal setting for evaluating the performance of different parsers.", "labels": [], "entities": []}, {"text": "To our mind evaluation of parsers via RTE is a very good additional possibility, besides the usual evaluation metrics, since inmost cases the main thing in real-word applications is to recognize the primary units, such as the subject, the predicate, the objects, as well as their modifiers, rather than the other subordinate relations.", "labels": [], "entities": []}, {"text": "We have been developing our own a multilingual dependency parser (called MDParser), which is based on linear classification 1 . Whereas the system is quite fast because the classification is linear, it usually achieves inferior results (using UAS/LAS evaluation metrics) in comparison to other parsers, which for example use kernel-based classification or other more sophisticated methods.", "labels": [], "entities": []}, {"text": "Therefore the PETE shared task was a perfect opportunity for us to investigate whether the inferior result of our parser is also relevant for its applicability in a concrete task.", "labels": [], "entities": []}, {"text": "We have compared our system with three state of the art parsers made available on the PETE web page: MaltParser, MiniPar and StandfordParser.", "labels": [], "entities": [{"text": "PETE web page", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.9538647532463074}, {"text": "MaltParser", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.8245464563369751}, {"text": "StandfordParser", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.9337325096130371}]}, {"text": "We have achieved the total score of 0.6545 (200/301 correct answers on the test data), which is the 4 th rank out of 19 submissions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}