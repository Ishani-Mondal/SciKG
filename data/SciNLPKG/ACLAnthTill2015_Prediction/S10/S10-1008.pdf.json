{"title": [{"text": "SemEval-2010 Task 10: Linking Events and Their Participants in Discourse", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe the SemEval-2010 shared task on \"Linking Events and Their Participants in Discourse\".", "labels": [], "entities": [{"text": "SemEval-2010 shared task", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7820491393407186}, {"text": "Linking Events and Their Participants in Discourse", "start_pos": 45, "end_pos": 95, "type": "TASK", "confidence": 0.8098485469818115}]}, {"text": "This task is an extension to the classical semantic role labeling task.", "labels": [], "entities": [{"text": "semantic role labeling task", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.724091112613678}]}, {"text": "While semantic role labeling is traditionally viewed as a sentence-internal task, local semantic argument structures clearly interact with each other in a larger context, e.g., by sharing references to specific discourse entities or events.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.6362574696540833}]}, {"text": "In the shared task we looked atone particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist).", "labels": [], "entities": []}, {"text": "This task is potentially beneficial fora number of NLP applications , such as information extraction, question answering or text summarization.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.8820052444934845}, {"text": "question answering", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.919110506772995}, {"text": "text summarization", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.773625373840332}]}], "introductionContent": [{"text": "Semantic role labeling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate ().", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8621059954166412}]}, {"text": "Semantic roles describe the function of the participants in an event.", "labels": [], "entities": []}, {"text": "Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc.", "labels": [], "entities": []}, {"text": "However, semantic role labeling as it is currently defined misses a lot of information due to the fact that it is viewed as a sentence-internal task.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.7079445719718933}]}, {"text": "Hence, relations between different local semantic argument structures are disregarded.", "labels": [], "entities": []}, {"text": "This view of SRL as a sentence-internal task is partly due to the fact that large-scale manual annotation projects such as FrameNet and PropBank 2 typically present their annotations lexicographically by lemma rather than by source text.", "labels": [], "entities": [{"text": "SRL", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9891391396522522}]}, {"text": "It is clear that there is an interplay between local argument structure and the surrounding discourse.", "labels": [], "entities": []}, {"text": "In early work, discussed filling null complements from context by using knowledge about individual predicates and tendencies of referential chaining across sentences.", "labels": [], "entities": [{"text": "filling null complements from context", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8353041052818299}, {"text": "referential chaining across sentences", "start_pos": 128, "end_pos": 165, "type": "TASK", "confidence": 0.8029461354017258}]}, {"text": "But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semantic relations between the predicates involved.", "labels": [], "entities": []}, {"text": "Two notable exceptions are and.", "labels": [], "entities": []}, {"text": "analyse a short newspaper article and discuss how frame semantics could benefit discourse processing but without making concrete suggestions of how to model this.", "labels": [], "entities": []}, {"text": "provide a detailed analysis of the links between the local semantic argument structures in a short text; however their system is not fully implemented either.", "labels": [], "entities": []}, {"text": "With the shared task, we aimed to make a first step towards taking SRL beyond the domain of individual sentences by linking local semantic argument structures to the wider discourse context.", "labels": [], "entities": [{"text": "SRL", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9944281578063965}]}, {"text": "The task addresses the problem of finding fillers for roles which are neither instantiated as direct dependents of our target predicates nor displaced through long-distance dependency or coinstantiation constructions.", "labels": [], "entities": []}, {"text": "Often a referent for an uninstantiated role can be found in the wider context, i.e. in preceding or following sentences.", "labels": [], "entities": []}, {"text": "An example is given in (1), where the CHARGES role (ARG2 in PropBank) of cleared is left empty but can be linked to murder in the previous sentence.", "labels": [], "entities": [{"text": "CHARGES", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.8452907800674438}, {"text": "ARG2", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.5851184725761414}]}, {"text": "(1) Ina lengthy court case the defendant was tried for murder.", "labels": [], "entities": []}, {"text": "In the end, he was cleared.", "labels": [], "entities": []}, {"text": "Another very rich example is provided by (2), where, for instance, the experiencer and the object of jealousy are not overtly expressed as dependents of the noun jealousy but can be inferred to be Watson and the speaker, Holmes, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "As noted above, we allowed participants to address three different tasks: SRL only, NI only, full task.", "labels": [], "entities": [{"text": "SRL", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.7167255282402039}]}, {"text": "For role recognition and labeling we used a standard evaluation set-up, i.e., accuracy for role labeling and precision, recall, F-Score for role recognition.", "labels": [], "entities": [{"text": "role recognition", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.9119917154312134}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9996408224105835}, {"text": "role labeling", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.7343939542770386}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9994550347328186}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9996317625045776}, {"text": "F-Score", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.9976564645767212}, {"text": "role recognition", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.8558421432971954}]}, {"text": "The NI linkings were evaluated slightly differently.", "labels": [], "entities": [{"text": "NI linkings", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.4837879240512848}]}, {"text": "In the gold standard, we identified referents for null instantiations in the discourse context.", "labels": [], "entities": []}, {"text": "In some cases, more than one referent might be appropriate, e.g., because the omitted argument refers to an entity that is mentioned multiple times in the context.", "labels": [], "entities": []}, {"text": "In this case, a system is given credit if the NI is linked to any of these expressions.", "labels": [], "entities": []}, {"text": "To achieve this we create equivalence sets for the referents of NIs (by annotating coreference chains).", "labels": [], "entities": []}, {"text": "If the NI is linked to any item in the equivalence set, the link is counted as a true positive.", "labels": [], "entities": []}, {"text": "We can then define NI linking precision as the number of all true positive links divided by the number of links made by a system, and NI linking recall as the number of true positive links divided by the number of links between an NI and its equivalence set in the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.7133735418319702}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.7059082984924316}]}, {"text": "NI linking F-Score is then the harmonic mean between NI linking precision and recall.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.47211694717407227}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.7360415458679199}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9966481328010559}]}, {"text": "Since it may sometimes be difficult to determine the correct extent of the filler of an NI, we score an automatic annotation as correct if it includes the head of the gold standard filler in the predicted filler.", "labels": [], "entities": []}, {"text": "However, in order to not favor systems which link NIs to very large spans of text to maximize the likelihood of linking to a correct referent, we introduce a second evaluation measure, which computes the overlap (Dice coefficient) between the words in the predicted filler (P) of an NI and the words in the gold standard one (G): Example illustrates this point.", "labels": [], "entities": [{"text": "Dice coefficient)", "start_pos": 213, "end_pos": 230, "type": "METRIC", "confidence": 0.9777357578277588}]}, {"text": "The verb won in the second sentence evokes the Finish competition frame whose COMPETITION role is omitted.", "labels": [], "entities": [{"text": "COMPETITION", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.9170235991477966}]}, {"text": "From the context it is clear that the competition role is semantically filled by their first TV debate (head: debate) and last night's debate (head: debate) in the previous sentences.", "labels": [], "entities": []}, {"text": "These two expressions form the equivalence set for the COMPETITION role in the last sentence.", "labels": [], "entities": []}, {"text": "Any system that would predict a linkage to a filler that covers the head of either of these two expressions would score a true positive for this NI.", "labels": [], "entities": []}, {"text": "However, a system that linked to last night's debate would have an NI linking overlap of 1 (i.e., 2*3/(3+3)) while a system linking the whole second sentence Last night's debate was eagerly anticipated to the NI would have an overlap of 0.67 (i.e., 2*3/(6+3))", "labels": [], "entities": [{"text": "NI linking overlap", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.9234055479367574}, {"text": "NI", "start_pos": 209, "end_pos": 211, "type": "DATASET", "confidence": 0.9064812064170837}]}], "tableCaptions": [{"text": " Table 1: Statistics for the provided data sets", "labels": [], "entities": []}, {"text": " Table 2: Shalmaneser (SHA), SEMAFOR (SEM)  and CLR performance on the SRL task (across  both chapters)", "labels": [], "entities": [{"text": "SRL task", "start_pos": 71, "end_pos": 79, "type": "TASK", "confidence": 0.9223784506320953}]}]}