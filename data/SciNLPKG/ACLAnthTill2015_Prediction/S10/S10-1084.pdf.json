{"title": [{"text": "RALI: Automatic weighting of text window distances", "labels": [], "entities": [{"text": "RALI", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.47124558687210083}]}], "abstractContent": [{"text": "Systems using text windows to model word contexts have mostly been using fixed-sized windows and uniform weights.", "labels": [], "entities": []}, {"text": "The window size is often selected by trial and error to maximize task results.", "labels": [], "entities": []}, {"text": "We propose a non-supervised method for selecting weights for each window distance, effectively removing the need to limit window sizes, by maximizing the mutual generation of two sets of samples of the same word.", "labels": [], "entities": []}, {"text": "Experiments on Semeval Word Sense Disambiguation tasks showed considerable improvements.", "labels": [], "entities": [{"text": "Semeval Word Sense Disambiguation tasks", "start_pos": 15, "end_pos": 54, "type": "TASK", "confidence": 0.8523446321487427}]}], "introductionContent": [{"text": "The meaning of a word can be defined by the words that accompany it in the text.", "labels": [], "entities": []}, {"text": "This is the principle often used in previous studies on Word Sense Disambiguation (WSD).", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.7873905797799429}]}, {"text": "In general, the accompanying words form a context vector of the target word, or a probability distribution of the context words.", "labels": [], "entities": []}, {"text": "For example, under the unigram bag-ofword assumption, this means building p(x|t) = count(x,t) \u2211 x count , where count(x, t) is the count of co-occurrences of word x with the target word t under a certain criterion.", "labels": [], "entities": []}, {"text": "In most studies, x and t should co-occur within a window of up to k words or sentences.", "labels": [], "entities": []}, {"text": "The bounds are usually selected as to maximize system performance.", "labels": [], "entities": []}, {"text": "Occurrences inside the window usually weight the same without regard to their position.", "labels": [], "entities": []}, {"text": "Indeed, a word closer to the target word usually has a greater semantic constraint on the target word than a more distant word.", "labels": [], "entities": []}, {"text": "Some studies have also proposed decaying factors to decrease the importance of more distant words in the context vector.", "labels": [], "entities": []}, {"text": "However, the decaying functions are defined manually.", "labels": [], "entities": []}, {"text": "It is unclear that the functions defined can capture the true impact of the context words on the target word.", "labels": [], "entities": []}, {"text": "In this paper, we propose an unsupervised method to automatically learn the optimal weight of a word according to its distance to the target word.", "labels": [], "entities": []}, {"text": "The general idea used to determine such weight is that, if we randomly determine two sets of texts containing the target word, the resulting probability distributions for its context words in the two sets should be similar.", "labels": [], "entities": []}, {"text": "Therefore, the weights of context words at different distance are determined so as to maximize the mutual generation probabilities of two sets of samples.", "labels": [], "entities": []}, {"text": "Experimentation on Semeval-2007 English and Semeval-2010 Japanese lexical sample task data shows that improvements can automatically be attained on simple Naive Bayes (NB) systems in comparison to the best manually selected fixed window system.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: example uses of text windows and related work are presented in Section 2.", "labels": [], "entities": []}, {"text": "Our method is presented in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 and 5, we show experimental results on English and Japanese WSD.", "labels": [], "entities": [{"text": "English and Japanese WSD", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.44843199104070663}]}, {"text": "We conclude in Section 6 with discussion and further possible extensions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (.", "labels": [], "entities": [{"text": "WSD tasks", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.8995906710624695}, {"text": "English Lexical Sample (ELS)", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6380885591109594}]}, {"text": "It consists of a selected set of polysemous words, contained within passages where a sense taken from a sense inventory is manually annotated.", "labels": [], "entities": []}, {"text": "The task is to create supervised classifiers maximizing accuracy on test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9977160692214966}]}, {"text": "Since there are only 50 words and instances are few, we judged there was not enough data to compute weights.", "labels": [], "entities": []}, {"text": "Instead, we used the AP Newswire corpus of the TREC collection (.", "labels": [], "entities": [{"text": "AP Newswire corpus of the TREC collection", "start_pos": 21, "end_pos": 62, "type": "DATASET", "confidence": 0.891616702079773}]}, {"text": "Words were stemmed with the Porter stemmer and text windows were grouped for all words.", "labels": [], "entities": []}, {"text": "For simplicity and efficiency, windows to the right and to the left were considered independent, and we only kept words with between 30 and 1000 windows.", "labels": [], "entities": []}, {"text": "Also, only windows with a size of 100, which was considered big enough without any doubt, were kept.", "labels": [], "entities": []}, {"text": "A stop list of the top 10 frequent words was used, but place holders were left in the windows to preserve the distances.", "labels": [], "entities": []}, {"text": "Multiple consecutive stop words (ex: \"of the\") were merged, and the target word, being the same for all samples of a set, was ignored.", "labels": [], "entities": []}, {"text": "This results in 32,650 sets containing 5,870,604 windows.", "labels": [], "entities": []}, {"text": "In, we can seethe resulting weight curve.", "labels": [], "entities": []}, {"text": "Since the curve converges, words over the 100th distance were assigned the minimum weight found in the curve.", "labels": [], "entities": []}, {"text": "From this we constructed NB models whose class priors used an absolute discounting of 0.5.", "labels": [], "entities": []}, {"text": "The collection language model used the concatenation of the AP collection and the Semeval data.", "labels": [], "entities": [{"text": "AP collection", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8994086384773254}, {"text": "Semeval data", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.8069095611572266}]}, {"text": "As the unstemmed target word is an important feature it was added to the models.", "labels": [], "entities": []}, {"text": "It's weight was chosen to be 0.7 by maximizing accuracy on one-held-out cross-validation of the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9996228218078613}]}, {"text": "The results are listed in.", "labels": [], "entities": []}, {"text": "Japanese WSD The Semeval-2010 Japanese WSD task ( consists of 50 polysemous words for which examples were taken from the BC-CWJ tagged corpus.", "labels": [], "entities": [{"text": "Semeval-2010 Japanese WSD task", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.7686910182237625}, {"text": "BC-CWJ tagged corpus", "start_pos": 121, "end_pos": 141, "type": "DATASET", "confidence": 0.8282514611879984}]}, {"text": "It was manually segmented, tagged, and annotated with senses taken from the Iwanami Kokugo dictionary.", "labels": [], "entities": [{"text": "Iwanami Kokugo dictionary", "start_pos": 76, "end_pos": 101, "type": "DATASET", "confidence": 0.9439704219500223}]}, {"text": "The task is identical to the ELS of the previous experiment.", "labels": [], "entities": [{"text": "ELS", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9550178050994873}]}, {"text": "Since the data was again insufficient to compute curves, we used the Mainichi-2005 corpus of NTCIR-8.", "labels": [], "entities": [{"text": "Mainichi-2005 corpus of NTCIR-8", "start_pos": 69, "end_pos": 100, "type": "DATASET", "confidence": 0.889255553483963}]}, {"text": "We tried to reproduce the same kind of segmentation as the training data by using the Chasen parser with UniDic.", "labels": [], "entities": []}, {"text": "For the corpus and Semeval data, conjugations (setsuzoku-to, jod\u00f4-shi, etc.), particles (all jo-shi), symbols (blanks, kig\u00f4, etc.), and numbers were stripped.", "labels": [], "entities": [{"text": "Semeval data", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.8191672563552856}]}, {"text": "When a base-form reading was present (for verbs and adjectives), the token was replaced by the Kanjis (chinese characters) in the word writing concatenated with the base-form reading.", "labels": [], "entities": []}, {"text": "This treatment is somewhat equivalent to the stemming+stop list of the ELS tasks.", "labels": [], "entities": [{"text": "ELS tasks", "start_pos": 71, "end_pos": 80, "type": "TASK", "confidence": 0.6710896193981171}]}, {"text": "The resulting curve can be seen in.", "labels": [], "entities": []}, {"text": "The NB models are the same as in the previous experiments.", "labels": [], "entities": [{"text": "NB", "start_pos": 4, "end_pos": 6, "type": "DATASET", "confidence": 0.7622550129890442}]}, {"text": "Target words were again added the same way as in the ELS task.", "labels": [], "entities": [{"text": "ELS task", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.8814792931079865}]}, {"text": "The best fixed window model was found to have a window size of 1 with a target word weight of 0.6 and used manual Dirichlet smoothing with a pseudo-count of 110.", "labels": [], "entities": []}, {"text": "We submited two systems with the following settings: RALI-1 used manual Dirichlet smoothing and 0.9 for the target word.", "labels": [], "entities": [{"text": "RALI-1", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.6405879855155945}]}, {"text": "RALI-2 used auto-: WSD accuracy on Semeval-2010 JWSD As we can see, the results are not significantly different from the best uniform model.", "labels": [], "entities": [{"text": "RALI-2", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6361073851585388}, {"text": "WSD", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7133996486663818}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.7842350006103516}, {"text": "Semeval-2010", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8159583210945129}, {"text": "JWSD", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.5201537013053894}]}, {"text": "This maybe due to differences in the segmentation parameters of our external corpus.", "labels": [], "entities": []}, {"text": "Another reason could be that the systems use almost the same weights: the best fixed window had size 1, and the Japanese curve is steeper than the English one.", "labels": [], "entities": []}, {"text": "This steeper curve can be explained by the grammatical structure of the Japanese language.", "labels": [], "entities": []}, {"text": "While English can be considered a SubjectVerb-Complement language, Japanese is considered Subject-Complement-Verb.", "labels": [], "entities": []}, {"text": "Verbs are mostly found at the end of the sentence, far from their subject, and vice versa.", "labels": [], "entities": [{"text": "Verbs", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9400953650474548}]}, {"text": "The window distance is therefore less useful in Japanese than in English since it has more non-local dependencies.", "labels": [], "entities": []}, {"text": "These results show that the curves work as expected even in different languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WSD accuracy on Semeval-2007 ELC", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7379223108291626}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.7675241827964783}, {"text": "ELC", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.6622481942176819}]}, {"text": " Table 2: WSD accuracy on Semeval-2010 JWSD", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7204784154891968}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.792031466960907}, {"text": "JWSD", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.5879255533218384}]}]}