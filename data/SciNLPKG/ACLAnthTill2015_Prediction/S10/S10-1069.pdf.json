{"title": [{"text": "SCHWA: PETE using CCG Dependencies with the C&C Parser", "labels": [], "entities": [{"text": "PETE", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.7487828731536865}, {"text": "C&C Parser", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.8561467528343201}]}], "abstractContent": [{"text": "This paper describes the SCHWA system entered by the University of Sydney in Se-mEval 2010 Task 12-Parser Evaluation using Textual Entailments (Yuret et al., 2010).", "labels": [], "entities": [{"text": "Se-mEval 2010 Task 12-Parser Evaluation", "start_pos": 77, "end_pos": 116, "type": "TASK", "confidence": 0.5514740526676178}]}, {"text": "Our system achieved an overall accuracy of 70% in the task evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9996925592422485}]}, {"text": "We used the C&C parser to build CCG dependency parses of the truth and hypothesis sentences.", "labels": [], "entities": []}, {"text": "We then used partial match heuristics to determine whether the system should predict entailment.", "labels": [], "entities": []}, {"text": "Heuristics were used because the dependencies generated by the parser are construction specific , making full compatibility unlikely.", "labels": [], "entities": []}, {"text": "We also manually annotated the development set with CCG analyses, establishing an upper bound for our entailment system of 87%.", "labels": [], "entities": []}], "introductionContent": [{"text": "The SemEval 2010 Parser Evaluation using Textual Entailments (PETE) task attempts to address the long-standing problems in parser evaluation caused by the diversity of syntactic formalisms and analyses in use.", "labels": [], "entities": [{"text": "SemEval 2010 Parser Evaluation using Textual Entailments (PETE) task", "start_pos": 4, "end_pos": 72, "type": "TASK", "confidence": 0.8402065472169356}, {"text": "parser evaluation", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.8457086980342865}]}, {"text": "The task investigates the feasibility of a minimalist extrinsic evaluationthat of detecting textual entailment between a truth sentence and a hypothesis sentence.", "labels": [], "entities": []}, {"text": "It is extrinsic in the sense that it evaluates parsers on a task, rather than a direct comparison of their output against some gold standard.", "labels": [], "entities": []}, {"text": "However, it requires only minimal task-specific logic, and the proposed entailments are designed to be inferrable based on syntactic information alone.", "labels": [], "entities": []}, {"text": "Our system used the C&C parser, which uses the Combinatory Categorial Grammar formalism).", "labels": [], "entities": []}, {"text": "We used the CCGbank-style dependency output of the parser, which is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child.", "labels": [], "entities": []}, {"text": "We divided the dependency graphs of the truth and hypothesis sentences into predicates that consisted of ahead word and its immediate children.", "labels": [], "entities": []}, {"text": "For instance, the parser's analysis of the sentence Totals include only vehicle sales reported in period might produce predicates like include(Totals, sales), only(include), and reported(sales).", "labels": [], "entities": []}, {"text": "If at least one such predicate matches in the two parses, we predict entailment.", "labels": [], "entities": []}, {"text": "We consider a single predicate match sufficient for entailment because the lexical categories and slots that constitute our dependency labels are often different in the hypothesis sentence due to the generation process used in the task.", "labels": [], "entities": []}, {"text": "The single predicate heuristic gives us an overall accuracy of 70% on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9995477795600891}]}, {"text": "Our precision and recall over the test set was 68% and 80% respectively giving an F-score of 74%.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9998291730880737}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9997492432594299}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9997466206550598}]}, {"text": "To investigate how many of the errors were due to parse failures, and how many were failures of our entailment recognition process, we manually annotated the 66 development truth sentences with gold standard CCG derivations.", "labels": [], "entities": [{"text": "entailment recognition process", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.7521421015262604}]}, {"text": "This established an upper bound of 87% F-score for our approach.", "labels": [], "entities": [{"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9994328618049622}]}, {"text": "This upper bound suggests that there is still work to be done before the system allows transparent evaluation of the parser.", "labels": [], "entities": []}, {"text": "However, crossframework parser evaluation is a difficult problem: previous attempts to evaluate the C&C parser on grammatical relations and Penn Treebank-trees have also produced upper bounds between 80 and 90% F-score.", "labels": [], "entities": [{"text": "crossframework parser evaluation", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.8774158557256063}, {"text": "Penn Treebank-trees", "start_pos": 140, "end_pos": 159, "type": "DATASET", "confidence": 0.9904362559318542}, {"text": "F-score", "start_pos": 211, "end_pos": 218, "type": "METRIC", "confidence": 0.9952113032341003}]}, {"text": "Our PETE system was much easier to produce than either of these previous attempts at cross-framework parser evaluation, suggesting that this maybe a promising approach to a difficult problem.: An example CCG derivation, showing how the categories assigned to words are combined to form a sentence.", "labels": [], "entities": [{"text": "cross-framework parser evaluation", "start_pos": 85, "end_pos": 118, "type": "TASK", "confidence": 0.7119751572608948}]}, {"text": "The arrows indicate the direction of application.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Final results over the test set", "labels": [], "entities": []}, {"text": " Table 1. Our overall accuracy was 70%, and per- formance over YES entailments was roughly 20%  higher than accuracy over NO entailments. This", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.998400866985321}, {"text": "YES entailments", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.42647549510002136}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9986739158630371}]}]}