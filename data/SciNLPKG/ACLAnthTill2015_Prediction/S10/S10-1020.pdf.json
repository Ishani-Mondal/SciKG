{"title": [{"text": "Corry: A System for Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9754408597946167}]}], "abstractContent": [{"text": "Corry is a system for coreference resolution in English.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.974056750535965}]}, {"text": "It supports both local (Soon et al. (2001)-style) and global (Integer Linear Programming, Denis and Baldridge (2007)-style) models of coreference.", "labels": [], "entities": []}, {"text": "Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.", "labels": [], "entities": [{"text": "Corry", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8684516549110413}]}, {"text": "Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al., 2010), optimizing Corry's performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) and CEAF (Luo, 2005).", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.8929437398910522}, {"text": "Coreference Resolution", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.9110790193080902}, {"text": "BLANC", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.9853702783584595}, {"text": "MUC", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.5494586825370789}, {"text": "CEAF", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.4852660000324249}]}, {"text": "Corry runs have shown the best performance level among all the systems in their track for the corresponding metric.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corry is a system for coreference resolution in English.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.974056750535965}]}, {"text": "It supports both local (-style) and global-style) models of coreference.", "labels": [], "entities": []}, {"text": "The backbone of the system is a family of SVM classifiers for pairs of mentions: each mention type receives its own classifier.", "labels": [], "entities": []}, {"text": "A separate anaphoricity classifier is learned for the ILP setting.", "labels": [], "entities": []}, {"text": "Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.", "labels": [], "entities": [{"text": "Corry", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8684516549110413}]}, {"text": "Corry has only participated in the \"open\" setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing and NE-tagging (), Wordnet for semantic classes and the U.S. census data for assigning gender values to person names.", "labels": [], "entities": [{"text": "Corry", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8966456651687622}, {"text": "U.S. census data", "start_pos": 224, "end_pos": 240, "type": "DATASET", "confidence": 0.6397788723309835}, {"text": "assigning gender values to person names", "start_pos": 245, "end_pos": 284, "type": "TASK", "confidence": 0.8091849585374197}]}, {"text": "Three runs have been submitted for the SemEval task 1 on Coreference Resolution, optimizing Corry's performance for BLANC, MUC and CEAF.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.904237687587738}, {"text": "Coreference Resolution", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.9003162980079651}, {"text": "BLANC", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.7219791412353516}, {"text": "MUC", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.6137343049049377}, {"text": "CEAF", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.6870079040527344}]}, {"text": "The runs differ with respect to the model (local for BLANC, global for MUC and CEAF) and the definition of mention types.", "labels": [], "entities": [{"text": "BLANC", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.7531759142875671}, {"text": "MUC", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8452764749526978}, {"text": "CEAF", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.4419541656970978}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: System scores for the gold/regular open setting. The best F-score for each metric shown in bold.", "labels": [], "entities": [{"text": "F-score", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9906942248344421}]}]}