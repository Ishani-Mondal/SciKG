{"title": [{"text": "TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval-2", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6574183106422424}, {"text": "Evaluating CRFs", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7501009702682495}, {"text": "TempEval-2", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.6607144474983215}]}], "abstractContent": [{"text": "This paper presents TIPSem, a system to extract temporal information from natural language texts for English and Spanish.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.5713918805122375}]}, {"text": "TIPSem, learns CRF models from training data.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8071091175079346}]}, {"text": "Although the used features include different language analysis levels, the approach is focused on semantic information.", "labels": [], "entities": []}, {"text": "For Spanish, TIPSem achieved the best F1 score in all the tasks.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.8141701817512512}, {"text": "F1 score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9867976605892181}]}, {"text": "For English, it obtained the best F1 in tasks B (events) and D (event-dct links); and was among the best systems in the rest.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9993699193000793}]}], "introductionContent": [{"text": "The automatic treatment of time expressions, events and their relations over natural language text consists of making temporal elements explicit through a system that identifies and annotates them following a standard scheme.", "labels": [], "entities": []}, {"text": "This information is crucial for other natural language processing (NLP) areas, such as summarization or question answering.", "labels": [], "entities": [{"text": "summarization", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.987883985042572}, {"text": "question answering", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.8285159766674042}]}, {"text": "The relevance of temporal information has been reflected in specialized conferences (  and evaluation forums.", "labels": [], "entities": []}, {"text": "We present a system to tackle the six different tasks related to multilingual temporal information treatment proposed in TempEval-2.", "labels": [], "entities": [{"text": "multilingual temporal information treatment", "start_pos": 65, "end_pos": 108, "type": "TASK", "confidence": 0.6309768334031105}]}, {"text": "Particularly, in this evaluation exercise,) is adopted as temporal annotation scheme.", "labels": [], "entities": []}, {"text": "In this manner, the tasks require participating systems to automatically annotate different TimeML elements.", "labels": [], "entities": []}, {"text": "Firstly, task A consists of determining the extent of time expressions as defined by the TimeML TIMEX3 tag, as well as the attributes \"type\" and \"value\".", "labels": [], "entities": [{"text": "TimeML TIMEX3 tag", "start_pos": 89, "end_pos": 106, "type": "DATASET", "confidence": 0.8269410928090414}]}, {"text": "Secondly, task B addresses the recognition and classification of events as defined by TimeML EVENT tag.", "labels": [], "entities": [{"text": "recognition and classification of events", "start_pos": 31, "end_pos": 71, "type": "TASK", "confidence": 0.8316421747207642}, {"text": "TimeML EVENT tag", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.8158969084421793}]}, {"text": "Finally, tasks C to F comprise the categorization of different temporal links (TimeML LINKs).", "labels": [], "entities": [{"text": "TimeML LINKs", "start_pos": 79, "end_pos": 91, "type": "METRIC", "confidence": 0.5902827978134155}]}, {"text": "illustrates the TimeML elements in a sentence.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8741340041160583}]}, {"text": "In the context of TempEval-2, we tackle all tasks for English and Spanish with a data-driven system.", "labels": [], "entities": []}, {"text": "This consists of CRF models inferred from lexical, syntactic and semantic information of given training data.", "labels": [], "entities": []}, {"text": "Our main approach, TIPSem (Temporal Information Processing based on Semantic information), is focused on semantic roles and semantic networks.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.8624285459518433}]}, {"text": "Furthermore, we present a secondary approach, TIPSem-B (TIPSemBaseline), which contrary to the former does not consider semantic information.", "labels": [], "entities": []}, {"text": "The main objectives of this paper are (1) evaluating the performance of TIPSem comparing it to other participating systems and (2) measuring the contribution of semantic information to different TempEval-2 tasks though the comparison between our systems: TIPSem and TIPSem-B.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Our approach to address the TempEval-2 tasks is motivated in Section 2 and described in Section 3.", "labels": [], "entities": [{"text": "TempEval-2 tasks", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.6101242303848267}]}, {"text": "The results obtained in the evaluation are shown and analyzed in Section 4.", "labels": [], "entities": []}, {"text": "Finally, conclusions are drawn in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The test corpus consists of 17K words for English and 10K words for Spanish, in which approximately a half part correspond to tasks A and B, and the other half to tasks C, D, E and F.", "labels": [], "entities": []}, {"text": "The performance is measured using precision, recall and F \u03b2=1 metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9997628331184387}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.999744713306427}, {"text": "F \u03b2=1", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9768618196249008}]}, {"text": "A scoring script is provided.", "labels": [], "entities": []}, {"text": "This counts correct instances at token level for tasks A and B, and at temporal link level for the rest.", "labels": [], "entities": []}, {"text": "Next subsections show the results obtained by TIPSem system in each one of the TempEval-2 tasks for English (EN) and Spanish (ES).", "labels": [], "entities": []}, {"text": "Moreover, a final subsection illustrates the F \u03b2=1 results in three comparative graphs.", "labels": [], "entities": [{"text": "F \u03b2=1", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.957171306014061}]}, {"text": "In tasks A and B, precision, recall and F \u03b2=1 are given.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9997989535331726}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.999733030796051}, {"text": "F \u03b2", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9722786843776703}]}, {"text": "In tasks C to E, links tasks precision, recall and F \u03b2=1 are the same because our system does not consider the NONE value.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9983705878257751}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9995943903923035}, {"text": "F \u03b2", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9859164357185364}, {"text": "NONE", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.8477577567100525}]}, {"text": "Hence, only F \u03b2=1 is given.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9834520816802979}]}, {"text": "Tasks E and F were not considered for Spanish in TempEval-2 evaluation and thus Spanish is excluded from those subsections.", "labels": [], "entities": []}, {"text": "For each task, scores in which our system obtained the first place in the evaluation exercise are in bold.", "labels": [], "entities": []}, {"text": "Furthermore, in all cases the best score obtained by participating systems is reported.", "labels": [], "entities": []}, {"text": "Finally, the influence of semantic information in terms of improvement is indicated and analyzed through the comparison with TIPSem-B system, which exclude the features related with semantics.", "labels": [], "entities": [{"text": "TIPSem-B", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.5670849680900574}]}, {"text": "As shown in results, TIPSem obtains the best results for Spanish in all measures except for \"value\" attribute, in which the best system obtained a 0.83.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9042158722877502}]}, {"text": "Another system obtained the same recall (0.87) but a lower precision (0.90), and thus a F \u03b2=1 of (0.88) below TIPSem score (0.91).", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9994445443153381}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9981268048286438}, {"text": "F \u03b2", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9883497655391693}, {"text": "TIPSem score", "start_pos": 110, "end_pos": 122, "type": "METRIC", "confidence": 0.9045585095882416}]}, {"text": "For English, our main approach obtained the best precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9987329840660095}]}, {"text": "However, another system obtained the best recall (0.91).", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9996615648269653}]}, {"text": "The best F \u03b2=1 was 0.86.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9766381084918976}]}, {"text": "Regarding type attribute, TIPSem obtained values closer to best system (0.98).", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.6254481077194214}]}, {"text": "Finally, in normalization, which is the only attribute that is not annotated by a purely data-driven process, best system surpassed TIPSem in 0.20.", "labels": [], "entities": [{"text": "normalization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9522043466567993}, {"text": "TIPSem", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.8518103361129761}]}], "tableCaptions": [{"text": " Table 1 shows the results obtained by our ap- proaches in TIMEX3 recognition, typing and ISO  8601 normalization (value).", "labels": [], "entities": [{"text": "TIMEX3 recognition", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.6830803602933884}]}, {"text": " Table 2: Task B -English and Spanish", "labels": [], "entities": []}, {"text": " Table 3: Task C -English and Spanish", "labels": [], "entities": []}, {"text": " Table 4: Task D -English and Spanish", "labels": [], "entities": []}, {"text": " Table 5: Task E -English", "labels": [], "entities": []}]}