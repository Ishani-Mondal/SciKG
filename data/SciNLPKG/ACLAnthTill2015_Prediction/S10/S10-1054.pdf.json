{"title": [{"text": "UBA: Using Automatic Translation and Wikipedia for Cross-Lingual Lexical Substitution", "labels": [], "entities": [{"text": "Automatic Translation", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6992164552211761}, {"text": "Cross-Lingual Lexical Substitution", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.7008397579193115}]}], "abstractContent": [{"text": "This paper presents the participation of the University of Bari (UBA) at the SemEval-2010 Cross-Lingual Lexical Substitution Task.", "labels": [], "entities": [{"text": "SemEval-2010 Cross-Lingual Lexical Substitution Task", "start_pos": 77, "end_pos": 129, "type": "TASK", "confidence": 0.8663439393043518}]}, {"text": "The goal of the task is to substitute a word in a language L s , which occurs in a particular context, by providing the best synonyms in a different language Lt which fit in that context.", "labels": [], "entities": []}, {"text": "This task has a strict relation with the task of automatic machine translation, but there are some differences: Cross-lingual lexical substitution targets one word at a time and the main goal is to find as many good translations as possible for the given target word.", "labels": [], "entities": [{"text": "automatic machine translation", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.6665298044681549}, {"text": "Cross-lingual lexical substitution", "start_pos": 112, "end_pos": 146, "type": "TASK", "confidence": 0.7312153180440267}]}, {"text": "Moreover, there are some connections with Word Sense Disambiguation (WSD) algorithms.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7497929980357488}]}, {"text": "Indeed, understanding the meaning of the target word is necessary to find the best substitutions.", "labels": [], "entities": []}, {"text": "An important aspect of this kind of task is the possibility of finding synonyms without using a particular sense inventory or a specific parallel corpus, thus allowing the participation of unsupervised approaches.", "labels": [], "entities": []}, {"text": "UBA proposes two systems: the former is based on an automatic translation system which exploits Google Translator, the latter is based on a parallel corpus approach which relies on Wikipedia in order to find the best substitutions.", "labels": [], "entities": [{"text": "UBA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9022508859634399}]}], "introductionContent": [{"text": "The goal of the Cross-Lingual Lexical Substitution (CLLS) task is to substitute a word in a language L s , which occurs in a particular context, by providing the best substitutions in a different language Lt . In SemEval-2010 the source language L sis English, while the target language Lt is Spanish.", "labels": [], "entities": [{"text": "Cross-Lingual Lexical Substitution (CLLS) task", "start_pos": 16, "end_pos": 62, "type": "TASK", "confidence": 0.8143643523965564}]}, {"text": "Clearly, this task is related to Lexical Substitution (LS) which consists in selecting an alternative word fora given one in a particular context by preserving its meaning.", "labels": [], "entities": [{"text": "Lexical Substitution (LS)", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.869680917263031}]}, {"text": "The main difference between the LS task and the CLLS one is that in LS source and target languages are the same.", "labels": [], "entities": []}, {"text": "CLLS is not a easy task since neither a list of candidate words nor a specific parallel corpus are supplied by the organizers.", "labels": [], "entities": []}, {"text": "However, this opens the possibility of using several knowledge sources, instead of a single one fixed by the task organizers.", "labels": [], "entities": []}, {"text": "Therefore, the system must identify a set of candidate words in Lt and then select only those words which fit the context.", "labels": [], "entities": []}, {"text": "From another point of view, the crosslingual nature of the task allows to exploit automatic machine translation methods, hence the goal is to find as many good translations as possible for the given target word.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7474832534790039}]}, {"text": "A thorough description of the task can be found in (.", "labels": [], "entities": []}, {"text": "To easily understand the task, an example follows.", "labels": [], "entities": []}, {"text": "Consider the sentence: During the siege, George Robertson had appointed Shuja-ul-Mulk , who was a bright boy only 12 years old and the youngest surviving son of Aman-ul-Mulk, as the ruler of Chitral.", "labels": [], "entities": []}, {"text": "In the previous sentence the target word is \"bright\".", "labels": [], "entities": []}, {"text": "Taking into account the meaning of the word \"bright\" in this particular context, the best substitutions in Spanish are: \"inteligente\", \"brillante\" and \"listo\".", "labels": [], "entities": []}, {"text": "We propose two systems to tackle the problem of CLLS: the first is based on an automatic translation system which exploits the API of Google Translator 1 , the second is based on a parallel corpus approach which relies on Wikipedia.", "labels": [], "entities": []}, {"text": "In particular, in the second approach we use a structured version of Wikipedia called.", "labels": [], "entities": []}, {"text": "Both systems adopt several lexical resources to select the list of possible substitutions fora given word.", "labels": [], "entities": []}, {"text": "Specifically, we use three different dictionaries: Google Dictionary, Babylon Dictionary and Spanishdict.", "labels": [], "entities": [{"text": "Google Dictionary", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9567656517028809}, {"text": "Babylon Dictionary", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.8812350630760193}, {"text": "Spanishdict", "start_pos": 93, "end_pos": 104, "type": "DATASET", "confidence": 0.9306779503822327}]}, {"text": "Then, we combine the dictionaries into a single one, as described in Section 2.1.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 describes the strategy we adopted to tackle the CLLS task, while results of an experimental session we carried out in order to evaluate the proposed approaches are presented in Section 3.", "labels": [], "entities": [{"text": "CLLS task", "start_pos": 93, "end_pos": 102, "type": "TASK", "confidence": 0.7235418558120728}]}, {"text": "Conclusions are discussed in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the evaluation is to measure the systems' ability to find correct Spanish substitutions fora given word.", "labels": [], "entities": []}, {"text": "The dataset supplied by the organizers contains 1,000 instances in XML format.", "labels": [], "entities": []}, {"text": "Moreover, the organizers provide trial data composed by 300 instances to help the participants during the development of their systems.", "labels": [], "entities": []}, {"text": "The systems are evaluated using two scoring types: best scores the best guessed substitution, while out-of-ten (oot) scores the best 10 guessed substitutions.", "labels": [], "entities": []}, {"text": "For each scoring type, precision (P) and recall (R) are computed.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9504485577344894}, {"text": "recall (R)", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9730565696954727}]}, {"text": "Mode precision (Pmode) and mode recall (R-mode) calculate precision and recall against the substitution chosen by the majority of the annotators (if there is a majority), respectively.", "labels": [], "entities": [{"text": "Mode precision (Pmode)", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.8848238348960876}, {"text": "mode recall (R-mode", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.7846243530511856}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9985936284065247}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9991845488548279}]}, {"text": "Details about evaluation and scoring types are provided in the task guidelines.", "labels": [], "entities": []}, {"text": "Results of the evaluation using trial data are reported in.", "labels": [], "entities": []}, {"text": "Our systems are tagged as UBA-T and UBA-W, which denote unibaTranslate and unibaWiki, respectively.", "labels": [], "entities": []}, {"text": "Systems marked as BL-1 and BL-2 are the two baselines provided by the organizers.", "labels": [], "entities": [{"text": "BL-1", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9757798910140991}, {"text": "BL-2", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9373082518577576}]}, {"text": "The baselines use Spanishdict dictionary to retrieve candidates.", "labels": [], "entities": []}, {"text": "The system BL-1 ranks the candidates according to the order returned on the online query page, while the BL-2 rank is based on candidate frequencies in the Spanish Wikipedia.", "labels": [], "entities": [{"text": "BL-1", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.8972618579864502}, {"text": "BL-2 rank", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9722903966903687}]}, {"text": "Results obtained using trial data show that our systems are able to overcome the baselines.", "labels": [], "entities": []}, {"text": "Only the best score achieved by UBA-W is below BL-1.", "labels": [], "entities": [{"text": "UBA-W", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9100046753883362}, {"text": "BL-1", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9934982061386108}]}, {"text": "Moreover, our strategy based on Wikipedia (UBA-W) works better than the one proposed by the organizers.", "labels": [], "entities": [{"text": "Wikipedia (UBA-W)", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.8462850004434586}]}, {"text": "Results of the evaluation using test data are reported in, which include all the participants.", "labels": [], "entities": []}, {"text": "Results show that UBA-T obtains the highest recall using best scoring strategy.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9995972514152527}]}, {"text": "Moreover, both systems UBA-T and UBA-W achieve the highest R-mode and P-mode using oot scoring strategy.", "labels": [], "entities": [{"text": "UBA-W", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8199718594551086}, {"text": "R-mode", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9639366865158081}]}, {"text": "It is worthwhile to point out that the presence of duplicates affect recall (R) and precision (P), but not R-mode and P-mode.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.925779864192009}, {"text": "precision (P)", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.9438700973987579}]}, {"text": "For this reason some systems, such as SWAT-E, obtain very high recall (R) and low R-mode using oot scoring.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9730837345123291}, {"text": "R-mode", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9887828230857849}]}, {"text": "Duplicates are not produced by our systems, but we performed an a posteriori experiment in which duplicates are allowed.", "labels": [], "entities": []}, {"text": "In that experiment, the first candidate provided by UBA-T has been duplicated ten times in the results.", "labels": [], "entities": []}, {"text": "Using that strategy, UBA-T achieves a recall (and precision) equal to 271.51.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9997288584709167}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9982853531837463}]}, {"text": "This experiment proves that also our system is able to obtain the highest recall when duplicates are allowed into the results.", "labels": [], "entities": [{"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9993554949760437}]}, {"text": "Moreover, it is important to underline here that we do not know how other participants generate duplicates in their results.", "labels": [], "entities": []}, {"text": "We adopted a trivial strategy to introduce duplicates.", "labels": [], "entities": []}, {"text": "Finally, reports some statistics about UBA-T and the number of times (N) the candi-date translation is taken from Spanish WordNet (Spanish WN) or multiword expressions (Multiword exp.).", "labels": [], "entities": []}, {"text": "The number of instances in which the candidate is a correct substitution is reported in column C.", "labels": [], "entities": []}, {"text": "Analyzing the results we note that most errors are due to part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6935976892709732}]}, {"text": "For example, given the following sentence: S en : You will still be responsible for the shipping and handling fees, and for the cost of returning the merchandise.", "labels": [], "entities": []}, {"text": "S es : Usted seguira siendo responsable de los gastos de envio y manipulacion y, para los gastos de devolucion de la mercancia.", "labels": [], "entities": []}, {"text": "where the target word is the verb return.", "labels": [], "entities": []}, {"text": "In this case the verb is used as noun and the algorithm suggests correctly devolucion (noun) as substitution instead of devolver (verb).", "labels": [], "entities": []}, {"text": "The gold standard provided by the organizers contains devolver as substitution and there is no match between devolucion and devolver during the scoring.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 and Table 2. Our systems  are tagged as UBA-T and UBA-W, which de- note unibaTranslate and unibaWiki, re- spectively. Systems marked as BL-1 and BL-2  are the two baselines provided by the organiz- ers. The baselines use Spanishdict dictionary to  retrieve candidates. The system BL-1 ranks the  candidates according to the order returned on the  online query page, while the BL-2 rank is based on  candidate frequencies in the Spanish Wikipedia.", "labels": [], "entities": [{"text": "BL-1", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9563714861869812}, {"text": "BL-2", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.8677425980567932}, {"text": "BL-2", "start_pos": 385, "end_pos": 389, "type": "METRIC", "confidence": 0.9661467671394348}]}, {"text": " Table 1: best results (trial data)", "labels": [], "entities": []}, {"text": " Table 2: oot results (trial data)", "labels": [], "entities": []}, {"text": " Table 3: best results (test data)", "labels": [], "entities": []}, {"text": " Table 4: oot results (test data)", "labels": [], "entities": []}]}