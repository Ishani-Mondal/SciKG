{"title": [{"text": "GPLSI-IXA: Using Semantic Classes to Acquire Monosemous Training Examples from Domain Texts", "labels": [], "entities": [{"text": "GPLSI-IXA", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8109127879142761}]}], "abstractContent": [{"text": "This paper summarizes our participation in task #17 of SemEval-2 (All-words WSD on a specific domain) using a supervised class-based Word Sense Disam-biguation system.", "labels": [], "entities": []}, {"text": "Basically, we use Support Vector Machines (SVM) as learning algorithm and a set of simple features to build three different models.", "labels": [], "entities": []}, {"text": "Each model considers a different training corpus: Sem-Cor (SC), examples from monosemous words extracted automatically from background data (BG), and both SC and BG (SCBG).", "labels": [], "entities": [{"text": "BG", "start_pos": 162, "end_pos": 164, "type": "METRIC", "confidence": 0.9536538124084473}]}, {"text": "Our system explodes the monosemous words appearing as members of a particular WordNet semantic class to automatically acquire class-based annotated examples from the domain text.", "labels": [], "entities": []}, {"text": "We use the class-based examples gathered from the domain corpus to adapt our traditional system trained on SemCor.", "labels": [], "entities": []}, {"text": "The evaluation reveal that the best results are achieved training with SemCor and the background examples from monosemous words, obtaining results above the first sense baseline and the fifth best position in the competition rank.", "labels": [], "entities": []}], "introductionContent": [{"text": "As empirically demonstrated by the last SensEval and SemEval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed.", "labels": [], "entities": []}, {"text": "In fact, supervised word-based WSD systems are very dependent of the corpora used for training and testing the system ().", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9033612608909607}]}, {"text": "One possible reason could be the use of inappropriate level of abstraction.", "labels": [], "entities": []}, {"text": "Most supervised systems simply model each polysemous word as a classification problem where each class corresponds to a particular synset of the word.", "labels": [], "entities": []}, {"text": "But, WordNet (WN) has been widely criticized for being a sense repository that often provides too fine-grained sense distinctions for higher level applications like Machine Translation or Question & Answering.", "labels": [], "entities": [{"text": "WordNet (WN)", "start_pos": 5, "end_pos": 17, "type": "DATASET", "confidence": 0.7580339908599854}, {"text": "Machine Translation", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.8359833359718323}, {"text": "Question & Answering", "start_pos": 188, "end_pos": 208, "type": "TASK", "confidence": 0.7123296062151591}]}, {"text": "In fact, WSD at this level of granularity has resisted all attempts of inferring robust broad-coverage models.", "labels": [], "entities": [{"text": "WSD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.8549418449401855}]}, {"text": "It seems that many word-sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word-sense annotated examples.", "labels": [], "entities": []}, {"text": "Thus, some research has been focused on deriving different word-sense groupings to overcome the fine-grained distinctions of WN,,,,) and (.", "labels": [], "entities": []}, {"text": "That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.", "labels": [], "entities": []}, {"text": "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (,,),,) and ().", "labels": [], "entities": []}, {"text": "That is, grouping senses of different words into the same explicit and comprehensive semantic class.", "labels": [], "entities": []}, {"text": "Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse-grained sense distinctions.", "labels": [], "entities": [{"text": "Lexicographical Files of WN", "start_pos": 47, "end_pos": 74, "type": "DATASET", "confidence": 0.8738230615854263}]}, {"text": "We suspect that selecting the appropriate level of abstraction could be on between both levels.", "labels": [], "entities": []}, {"text": "Thus, we use the semantic classes modeled by the Basic Level Concepts 1 (BLC) (.", "labels": [], "entities": []}, {"text": "Our previous research using BLC empirically demonstrated that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform class-based Word Sense Disambiguation (WSD) ().", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 189, "end_pos": 220, "type": "TASK", "confidence": 0.7635509918133417}]}, {"text": "Now, we also show that class-based WSD allows to successfully incorporate monosemous examples from the domain text.", "labels": [], "entities": [{"text": "WSD", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8410868048667908}]}, {"text": "In fact, the robustness of our class-based WSD approach is shown by our system that just uses the SemCor examples (SC).", "labels": [], "entities": []}, {"text": "It performs without any kind of domain adaptation as the Most Frequent Sense (MFS) baseline.", "labels": [], "entities": []}, {"text": "This paper describes our participation in.", "labels": [], "entities": []}, {"text": "In section 2 semantic classes used and selection algorithm used to obtain them automatically from WordNet are described.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9601998329162598}]}, {"text": "In section 3 the technique employed to extract monosemous examples from background data is described.", "labels": [], "entities": []}, {"text": "Section 4 explains the general approach of our system, and the experiments designed, and finally, in section 5, the results and some analysis are shown.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Most frequent BLC-20 semantic classes on WordNet 3.0", "labels": [], "entities": [{"text": "WordNet 3.0", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8684757351875305}]}, {"text": " Table 3: Most frequent monosemic words in BG", "labels": [], "entities": [{"text": "BG", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.4934879243373871}]}, {"text": " Table 4: Results of task#17", "labels": [], "entities": []}]}