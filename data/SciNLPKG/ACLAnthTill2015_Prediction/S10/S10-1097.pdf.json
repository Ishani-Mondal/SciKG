{"title": [{"text": "Twitter Based System: Using Twitter for Disambiguating Sentiment Ambiguous Adjectives", "labels": [], "entities": [{"text": "Disambiguating Sentiment Ambiguous Adjectives", "start_pos": 40, "end_pos": 85, "type": "TASK", "confidence": 0.8411015719175339}]}], "abstractContent": [{"text": "In this paper, we describe our system which participated in the SemEval 2010 task of disambiguating sentiment ambiguous adjectives for Chinese.", "labels": [], "entities": [{"text": "SemEval 2010 task", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8181382218996683}]}, {"text": "Our system uses text messages from Twitter, a popular microblogging platform, for building a dataset of emotional texts.", "labels": [], "entities": []}, {"text": "Using the built dataset, the system classifies the meaning of adjectives into positive or negative sentiment polarity according to the given context.", "labels": [], "entities": []}, {"text": "Our approach is fully automatic.", "labels": [], "entities": []}, {"text": "It does not require any additional hand-built language resources and it is language independent .", "labels": [], "entities": []}], "introductionContent": [{"text": "The dataset of the SemEval task ( consists of short texts in Chinese containing target adjectives whose sentiments need to be disambiguated in the given contexts.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.9365943968296051}]}, {"text": "Those adjectives are: big, small, many, few, high, low, thick, thin, deep, shallow, heavy, light, huge, grave.", "labels": [], "entities": []}, {"text": "Disambiguating sentiment ambiguous adjectives is a challenging task for NLP.", "labels": [], "entities": [{"text": "Disambiguating sentiment ambiguous adjectives", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8811801671981812}]}, {"text": "Previous studies were mostly focused on word sense disambiguation rather than sentiment disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.758397380510966}, {"text": "sentiment disambiguation", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8062374889850616}]}, {"text": "Although both problems look similar, the latter is more challenging in our opinion because impregnated with more subjectivity.", "labels": [], "entities": []}, {"text": "In order to solve the task, one has to deal not only with the semantics of the context, but also with the psychological aspects of human perception of emotions from the written text.", "labels": [], "entities": []}, {"text": "In our approach, we use Twitter 1 microblogging platform to retrieve emotional messages and form two sets of texts: messages with positive emotions and those with negative ones (Pak and Paroubek, 1 http://twitter.com 2010).", "labels": [], "entities": []}, {"text": "We use emoticons 2 as indicators of an emotion) to automatically classify texts into positive or negative sets.", "labels": [], "entities": []}, {"text": "The reason we use Twitter is because it allows us to collect the data with minimal supervision efforts.", "labels": [], "entities": []}, {"text": "It provides an API 3 which makes the data retrieval process much more easier then Web based search or other resources.", "labels": [], "entities": [{"text": "data retrieval", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7211634814739227}]}, {"text": "After the dataset of emotional texts has been obtained, we build a classifier based on n-grams Na\u00a8\u0131veNa\u00a8\u0131ve Bayes approach.", "labels": [], "entities": []}, {"text": "We tested two approaches to build a sentiment classifier: 1.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7792590260505676}]}, {"text": "In the first one, we collected Chinese texts from Twitter and used them to train a classifier to annotate the test dataset.", "labels": [], "entities": []}, {"text": "2. In the second one, we used machine translator to translate the dataset from Chinese to English and annotated it using collected English texts from Twitter as the training data.", "labels": [], "entities": []}, {"text": "We have made the second approach because we were able to collect much more of English texts from Twitter than Chinese ones and we wanted to test the impact of machine translation on the performance of our classifier.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7006208002567291}]}, {"text": "We have experimented with Google Translate and Yahoo Babelfish . Google Translate yielded better results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we used two datasets: atrial dataset containing 100 sentences in Chinese and First, we compared the performance of our method when using Google Translate and Yahoo Babelfish for translating the trial dataset.", "labels": [], "entities": []}, {"text": "The results for micro and macro accuracy are shown in Graphs 1 and 2 respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9863994121551514}]}, {"text": "The x-axis represents a context window-size, equal to a number of words on both sides of the target adjective.", "labels": [], "entities": []}, {"text": "The yaxis shows accuracy values.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9997068047523499}]}, {"text": "From the graphs we see that Google Translate provides better results, therefore it was chosen when annotating the test dataset.", "labels": [], "entities": []}, {"text": "Next, we studied the impact of the context window size on micro and macro accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9860383868217468}]}, {"text": "The impact of the size of the context window on the accuracy of the classifier trained on Chinese texts is depicted in Graph 3 and for the classifier trained on English texts with translated test dataset in Graph 4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9992672801017761}]}, {"text": "The second approach achieves better results.", "labels": [], "entities": []}, {"text": "We were able to obtain 64% of macro and 61% of micro accuracy when using the second approach but only 63% of macro and 61% of micro accuracy when using the first approach.", "labels": [], "entities": [{"text": "micro", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9226293563842773}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.6048120856285095}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.5865344405174255}]}, {"text": "Another observation from the graphs is that Chinese requires a smaller size of a context window to obtain the best performance.", "labels": [], "entities": []}, {"text": "For the first approach, a window size of 8 words gave the best macro accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9774417877197266}]}, {"text": "For the second approach, we obtained the highest accuracy with a window size of 22 words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9991952776908875}]}], "tableCaptions": []}