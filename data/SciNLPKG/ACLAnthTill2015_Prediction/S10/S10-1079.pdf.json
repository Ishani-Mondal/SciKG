{"title": [{"text": "UoY: Graphs of Unambiguous Vertices for Word Sense Induction and Disambiguation", "labels": [], "entities": [{"text": "UoY", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8410390019416809}, {"text": "Word Sense Induction and Disambiguation", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.7207167983055115}]}], "abstractContent": [{"text": "This paper presents an unsupervised graph-based method for automatic word sense induction and disambiguation.", "labels": [], "entities": [{"text": "automatic word sense induction", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6578818187117577}]}, {"text": "The innovative part of our method is the assignment of either a word or a word pair to each vertex of the constructed graph.", "labels": [], "entities": []}, {"text": "Word senses are induced by clustering the constructed graph.", "labels": [], "entities": []}, {"text": "In the disambiguation stage, each induced cluster is scored according to the number of its vertices found in the context of the target word.", "labels": [], "entities": []}, {"text": "Our system participated in SemEval-2010 word sense induction and disambiguation task.", "labels": [], "entities": [{"text": "SemEval-2010 word sense induction", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.8398187160491943}]}], "introductionContent": [{"text": "There exists significant evidence that word sense disambiguation is important fora variety of natural language processing tasks: machine translation, information retrieval, grammatical analysis, speech and text processing.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.7620296676953634}, {"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7480393648147583}, {"text": "information retrieval", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.7662950158119202}, {"text": "grammatical analysis", "start_pos": 173, "end_pos": 193, "type": "TASK", "confidence": 0.7380481064319611}, {"text": "speech and text processing", "start_pos": 195, "end_pos": 221, "type": "TASK", "confidence": 0.6259234920144081}]}, {"text": "However, the \"fixed-list\" of senses paradigm, where the senses of a target word is a closed list of definitions coming from a standard dictionary), was long ago abandoned.", "labels": [], "entities": []}, {"text": "The reason is that sense lists, such as WordNet, miss many senses, especially domainspecific ones).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9626790285110474}]}, {"text": "The missing concepts are not recognised.", "labels": [], "entities": []}, {"text": "Moreover, senses cannot be easily related to their use in context.", "labels": [], "entities": []}, {"text": "Word sense induction methods can be divided into vector-space models and graph based ones.", "labels": [], "entities": [{"text": "Word sense induction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6764684716860453}]}, {"text": "Ina vector-space model, each context of a target word is represented as a feature vector, e.g. frequency of cooccurring words).", "labels": [], "entities": []}, {"text": "Context vectors are clustered and the resulting clusters represent the induced senses.", "labels": [], "entities": []}, {"text": "Recently, graph-based methods have been employed for word sense induction.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8571111361185709}]}, {"text": "Typically, graph-based methods represent each context word of the target word as a vertex.", "labels": [], "entities": []}, {"text": "Two vertices are connected via an edge if they cooccur in one or more instances.", "labels": [], "entities": []}, {"text": "Once the cooccurrence graph has been constructed, different graph clustering algorithms are applied to partition the graph.", "labels": [], "entities": []}, {"text": "Each cluster (partition) consists of a set of words that are semantically related to the particular sense).", "labels": [], "entities": []}, {"text": "The potential advantage of graph-based methods is that they can combine both local and global cooccurrence information ().", "labels": [], "entities": []}, {"text": "presented a graph-based approach that represents pairs of words as vertices instead of single words.", "labels": [], "entities": []}, {"text": "They claimed that single words might appear with more than one senses of the target word, while they hypothesize that a pair of words is unambiguous.", "labels": [], "entities": []}, {"text": "Hard-clustering the graph will potentially identify less conflating senses of the target word.", "labels": [], "entities": []}, {"text": "In this paper, we relax the above hypothesis because in some cases a single word is unambiguous.", "labels": [], "entities": []}, {"text": "We present a method that generates two-word vertices only when a single word vertex is unambiguous.", "labels": [], "entities": []}, {"text": "If the word is judged as unambiguous, then it is represented as a single-word vertex.", "labels": [], "entities": []}, {"text": "Otherwise, it is represented as a pair-of-words vertex.", "labels": [], "entities": []}, {"text": "The approach of achieved good results in both evaluation settings of the SemEval-2007 task.", "labels": [], "entities": [{"text": "SemEval-2007 task", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7237348854541779}]}, {"text": "A test instance is disambiguated towards one of the induced senses if one or more pairs of words representing that sense cooccur in the test instance.", "labels": [], "entities": []}, {"text": "This creates a sparsity problem, because a cooccurrence of two words is generally less likely than the occurrence of a single word.", "labels": [], "entities": []}, {"text": "We expect our approach to address the data sparsity problem without conflating the induced senses.", "labels": [], "entities": []}, {"text": "1 shows an example showing how the sense induction algorithm works: The left side of part I shows the context nouns of four snippets containing the target noun \"chip\".", "labels": [], "entities": [{"text": "sense induction", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8158416450023651}]}, {"text": "The most relevant of these nouns are represented as single word vertices (part II).", "labels": [], "entities": []}, {"text": "Note that \"customer\" was not judged to be significantly relevant.", "labels": [], "entities": []}, {"text": "In addition, the system introduced several vertices representing pairs of nouns.", "labels": [], "entities": []}, {"text": "For example, note the vertex \"company potato\".", "labels": [], "entities": [{"text": "company potato", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.9250008463859558}]}, {"text": "The set of sentences containing the context word \"company\" was judged as very different from the set of sentences containing \"company\" and \"potato\".", "labels": [], "entities": []}, {"text": "Thus, our system hypothesizes that probably \"company\" and \"company potato\" are relevant to different senses of \"chip\", and allows them to be clustered accordingly.", "labels": [], "entities": []}, {"text": "Vertices whose content nouns or pairs of nouns cooccur in some snippet are connected with an edge (part III and right side of part I).", "labels": [], "entities": []}, {"text": "Edge weights depend upon the conditional probabilities of the occurrence frequencies of the vertex contents in a large corpus, e.g. w 2,6 in part III.", "labels": [], "entities": []}, {"text": "Hardclustering the graph produces the induced senses of \"chip\": (a) potato crisp, and (b) microchip.", "labels": [], "entities": []}, {"text": "In the following subsections, the system is described in detail.", "labels": [], "entities": []}, {"text": "shows a block diagram overview of the sense induction system.", "labels": [], "entities": []}, {"text": "It consists of three main components: (a) corpus preprocessing, (b) graph construction, and (c) clustering.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7739897072315216}]}, {"text": "Ina number of different stages, the system uses a reference corpus to count occurrences of word or word pairs.", "labels": [], "entities": []}, {"text": "It is chosen to be large because frequencies of words in a large corpus are more significant statistically.", "labels": [], "entities": []}, {"text": "Ideally we would use the web or another large repository, but for the purposes of the SemEval-2010 task we used the union of all snippets of all target words.", "labels": [], "entities": [{"text": "SemEval-2010 task", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.8097445666790009}]}], "datasetContent": [{"text": "Three different measures, V-Measure, F-Score, and supervised recall on word sense disambiguation task, were used for evaluation.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9937424063682556}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9425607919692993}, {"text": "word sense disambiguation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.5887386997540792}]}, {"text": "V-Measure and F-Score are unsupervised.", "labels": [], "entities": [{"text": "V-Measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9532890319824219}, {"text": "F-Score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9984086155891418}]}, {"text": "Supervised recall was measured on two different data splits.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9745076894760132}]}, {"text": "shows the performance of our system, UoY, for all measures and in comparison with the best, worst and average performing system and the random and most frequent sense (MFS) baselines.", "labels": [], "entities": []}, {"text": "Results are shown for all words, and nouns and verbs only.: Summary of results (%).", "labels": [], "entities": [{"text": "Summary", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9764518737792969}]}, {"text": "V-Msr: VMeasure, F-Sc: F-Score, S-R X : Supervised recall under data split: X% training, (100-X)% test shows the ranks of UoY for all evaluation categories.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9336915612220764}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9790438413619995}]}, {"text": "Our system was generally very highly ranked.", "labels": [], "entities": []}, {"text": "It outperformed the random baseline in all cases and the MFS baseline in measures but F-Score.", "labels": [], "entities": [{"text": "MFS baseline", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.7255556881427765}, {"text": "F-Score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.988603949546814}]}, {"text": "No participant system managed to achive higher F-Score than the MFS baseline.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.997948944568634}, {"text": "MFS baseline", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.8651036024093628}]}, {"text": "The main disadvantage of the system seems to be the large number of induced senses.", "labels": [], "entities": []}, {"text": "The reasons are data sparcity and tuning on nouns, that might have led to parameters that induce more senses.", "labels": [], "entities": []}, {"text": "However, the system performs best among systems that produce comparable numbers of clusters.", "labels": [], "entities": []}, {"text": "shows the number of senses of UoY and the gold-standard.", "labels": [], "entities": []}, {"text": "UoY produces significantly more senses than the gold-standard, especially for nouns, while for verbs figures are similar.", "labels": [], "entities": [{"text": "UoY", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7119216322898865}]}, {"text": "The system achieves low F-Scores, because this measure favours fewer induced senses.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9993446469306946}]}, {"text": "Moreover, we observe that most scores are lower for verbs than nouns.", "labels": [], "entities": []}, {"text": "This is probably because parameters are tuned on nouns and because in general nouns appear with more senses than verbs, allowing our system to adapt better.", "labels": [], "entities": []}, {"text": "As an overall conclusion, each evaluation measure is more or less biased towards small or large numbers of induced senses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of results (%). V-Msr: V- Measure, F-Sc: F-Score, S-R X : Supervised recall  under data split: X% training, (100-X)% test", "labels": [], "entities": [{"text": "V- Measure", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.8387232422828674}, {"text": "F-Score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.602968156337738}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9799144864082336}]}, {"text": " Table 2: Ranks of UoY (out of 26 systems)", "labels": [], "entities": []}, {"text": " Table 3: Number of senses", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9823545217514038}]}]}