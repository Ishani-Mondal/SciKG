{"title": [{"text": "Duluth-WSI: SenseClusters Applied to the Sense Induction Task of SemEval-2", "labels": [], "entities": [{"text": "Sense Induction Task", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7770026326179504}, {"text": "SemEval-2", "start_pos": 65, "end_pos": 74, "type": "TASK", "confidence": 0.5087364315986633}]}], "abstractContent": [{"text": "The Duluth-WSI systems in SemEval-2 built word co-occurrence matrices from the task test data to create a second order co-occurrence representation of those test instances.", "labels": [], "entities": []}, {"text": "The senses of words were induced by clustering these instances, where the number of clusters was automatically predicted.", "labels": [], "entities": []}, {"text": "The Duluth-Mix system was a variation of WSI that used the combination of training and test data to create the co-occurrence matrix.", "labels": [], "entities": []}, {"text": "The Duluth-R system was a series of random baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Duluth systems in the sense induction task of SemEval-2 ( were based on SenseClusters (v1.01), a freely available open source software package which relies on the premise that words with similar meanings will occur in similar contexts ().", "labels": [], "entities": [{"text": "sense induction task", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.805701494216919}]}, {"text": "The data for the sense induction task included 100 ambiguous words made up of 50 nouns and 50 verbs.", "labels": [], "entities": [{"text": "sense induction task", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.9207248091697693}]}, {"text": "There were a total of 8,915 test instances and 879,807 training instances provided.", "labels": [], "entities": []}, {"text": "Note that neither the training nor the test data was sense tagged.", "labels": [], "entities": []}, {"text": "The training data was made available as a resource for participants, with the understanding that system evaluation would be done on the test instances only.", "labels": [], "entities": []}, {"text": "The organizers held back a gold standard annotation of the test data that was only used for evaluation.", "labels": [], "entities": []}, {"text": "Five Duluth-WSI systems participated in this task, six Duluth-Mix systems, and five Duluth Random systems.", "labels": [], "entities": []}, {"text": "The WSI and Mix systems almost always represented the test instances using second order co-occurrences, where each word in a test instance is replaced by a vector that shows the words with which it co-occurs.", "labels": [], "entities": [{"text": "WSI", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7790210247039795}]}, {"text": "The word vectors that makeup a test instance are averaged together to makeup anew representation for that instance.", "labels": [], "entities": []}, {"text": "All the test instances fora word are clustered, and the number of senses is automatically predicted by either the PK2 measure or Adapted Gap Statistic ().", "labels": [], "entities": []}, {"text": "In the Duluth systems the co-occurrence matrices are either based on order-dependent bigrams or unordered pairs of words, both of which can be separated by up to some given number of intervening words.", "labels": [], "entities": []}, {"text": "Bigrams are used to preserve distinctions between collocations such as cat house and house cat, whereas co-occurrences do not consider order and would treat these two as being equivalent.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each participating system was scored by three different evaluation methods: the V-measure, the supervised recall measure, and the paired F-score (.", "labels": [], "entities": [{"text": "recall measure", "start_pos": 106, "end_pos": 120, "type": "METRIC", "confidence": 0.963647723197937}, {"text": "F-score", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.8972567915916443}]}, {"text": "The results of the evaluation are in some sense confusing -a system that ranks near the top according to one measure may rank at the bottom or middle of another.", "labels": [], "entities": []}, {"text": "There was not any single system that did well according to all of the different measures.", "labels": [], "entities": []}, {"text": "The situation is so extreme that in some cases a system would perform near the top in one measure, and then below random baselines in another.", "labels": [], "entities": []}, {"text": "These stark differences suggest areal need for continued development of other methods for evaluating unsupervised sense induction.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.6823542416095734}]}, {"text": "One minimum expectation of an evaluation measure is that it should expose and identify random baselines by giving them low scores that clearly distinguish them from actual participating systems.", "labels": [], "entities": []}, {"text": "The scores of all the evaluation measures used in this task when applied to different random baseline systems are summarized in.", "labels": [], "entities": []}, {"text": "These include a number of post-evaluation random clustering systems, which are referred to as post-R1k, where k is the number of random clusters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation of Random Systems", "labels": [], "entities": []}]}