{"title": [{"text": "SemEval-2010 Task 12: Parser Evaluation using Textual Entailments", "labels": [], "entities": [{"text": "SemEval-2010 Task 12", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8749652703603109}, {"text": "Parser Evaluation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9069197177886963}]}], "abstractContent": [{"text": "Parser Evaluation using Textual Entail-ments (PETE) is a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evaluation.", "labels": [], "entities": [{"text": "Parser Evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7843220233917236}, {"text": "SemEval-2010 Evaluation Exercises on Semantic Evaluation", "start_pos": 76, "end_pos": 132, "type": "TASK", "confidence": 0.7705271442731222}]}, {"text": "The task involves recognizing textual entailments based on syntactic information alone.", "labels": [], "entities": [{"text": "recognizing textual entailments", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.7714323997497559}]}, {"text": "PETE introduces anew parser evaluation scheme that is formalism independent, less prone to annotation error, and focused on semantically relevant distinctions.", "labels": [], "entities": [{"text": "PETE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9217087626457214}]}], "introductionContent": [{"text": "Parser Evaluation using Textual Entailments (PETE) is a shared task that involves recognizing textual entailments based on syntactic information alone.", "labels": [], "entities": [{"text": "Parser Evaluation using Textual Entailments (PETE)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8054763227701187}]}, {"text": "Given two text fragments called \"text\" and \"hypothesis\", textual entailment recognition is the task of determining whether the meaning of the hypothesis is entailed (can be inferred) from the text.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.7605124413967133}]}, {"text": "In contrast with general RTE tasks () the PETE task focuses on syntactic entailments: Text: The man with the hat was tired.", "labels": [], "entities": [{"text": "RTE tasks", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.8125185966491699}]}, {"text": "Hypothesis-1: The man was tired.", "labels": [], "entities": []}, {"text": "(yes) Hypothesis-2: The hat was tired.", "labels": [], "entities": []}, {"text": "(no) PETE is an evaluation scheme based on a natural human linguistic competence (i.e. the ability to comprehend sentences and answer simple yes/no questions about them).", "labels": [], "entities": [{"text": "PETE", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9535087943077087}]}, {"text": "We believe systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.", "labels": [], "entities": []}, {"text": "The PARSEVAL measures introduced nearly two decades ago) still dominate the field of parser evaluation.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9254817366600037}, {"text": "parser evaluation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.9612083733081818}]}, {"text": "These methods compare phrase-structure bracketings produced by the parser with bracketings in the annotated corpus, or \"treebank\".", "labels": [], "entities": [{"text": "phrase-structure bracketings", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.739671140909195}]}, {"text": "Parser evaluation using short textual entailments has the following advantages compared to treebank based evaluation.", "labels": [], "entities": [{"text": "Parser evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9080464839935303}]}, {"text": "Consistency: Recognizing syntactic entailments is a more natural task for people than treebank annotation.", "labels": [], "entities": [{"text": "Recognizing syntactic entailments", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8192660609881083}]}, {"text": "Focusing on a natural human competence makes it practical to collect high quality evaluation data from untrained annotators.", "labels": [], "entities": []}, {"text": "The PETE dataset was annotated by untrained Amazon Mechanical Turk workers at an insignificant cost and each annotation is based on the unanimous agreement of at least three workers.", "labels": [], "entities": [{"text": "PETE dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8237126171588898}, {"text": "Amazon Mechanical Turk", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.8781784772872925}]}, {"text": "In contrast, of the 36306 constituent strings that appear multiple times in the Penn Treebank, 5646 (15%) have multiple conflicting annotations.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9927258789539337}]}, {"text": "If indicative of the general level of inconsistency, 15% is a very high number given that the state of the art parsers claim f-scores above 90%).", "labels": [], "entities": []}, {"text": "Relevance: PETE automatically focuses attention on semantically relevant phenomena rather than differences in annotation style or linguistic convention.", "labels": [], "entities": []}, {"text": "Whether a phrase is tagged ADJP vs ADVP rarely affects semantic interpretation.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7126404345035553}]}, {"text": "Attaching the wrong subject to a verb or the wrong prepositional phrase to a noun changes the meaning of the sentence.", "labels": [], "entities": []}, {"text": "Standard treebank based evaluation metrics do not distinguish between semantically relevant and irrelevant errors ().", "labels": [], "entities": []}, {"text": "In PETE semantically relevant differences lead to different entailments, semantically irrelevant differences do not.", "labels": [], "entities": []}, {"text": "Framework independence: Entailment recognition is a formalism independent task.", "labels": [], "entities": [{"text": "Entailment recognition", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.8845148384571075}]}, {"text": "A common evaluation method for parsers that do not use the Penn Treebank formalism is to automatically convert the Penn Treebank to the appropriate formalism and to perform treebank based evaluation (.", "labels": [], "entities": [{"text": "Penn Treebank formalism", "start_pos": 59, "end_pos": 82, "type": "DATASET", "confidence": 0.9768937627474467}, {"text": "Penn Treebank", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9918281733989716}]}, {"text": "The inevitable conversion errors compound the already mentioned problems of treebank based evaluation.", "labels": [], "entities": []}, {"text": "In addition, manually designed treebanks do not naturally lend themselves to unsupervised parser evaluation.", "labels": [], "entities": []}, {"text": "Unlike treebank based evaluation, PETE can compare phrase structure parsers, dependency parsers, unsupervised parsers and other approaches on an equal footing.", "labels": [], "entities": [{"text": "phrase structure parsers", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.7040260831514994}, {"text": "dependency parsers", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.6776128858327866}]}, {"text": "PETE was inspired by earlier work on representations of grammatical dependency, proposed for ease of use by end users and suitable for parser evaluation.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.8558064997196198}]}, {"text": "These include the grammatical relations (GR) by), the PARC representation (, and Stanford typed dependencies (SD)) (See (Bos and others, 2008) for other proposals).", "labels": [], "entities": [{"text": "PARC representation", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.8262777328491211}]}, {"text": "Each use a set of binary relations between words in a sentence as the primary unit of representation.", "labels": [], "entities": []}, {"text": "They share some common motivations: usability by people who are not (computational) linguists and suitability for relation extraction applications.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8906996846199036}]}, {"text": "Here is an example sentence and its SD representation: Bell, based in Los Angeles, makes and distributes electronic, computer and building products.", "labels": [], "entities": [{"text": "Bell", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9359760880470276}]}, {"text": "nsubj(makes-8, Bell-1) nsubj(distributes-10, Bell-1) partmod(Bell-1, based-3) nn(Angeles-6, Los-5) prep-in(based-3, Angeles-6) conj-and(makes-8, distributes-10) amod(products-16, electronic-11) conj-and(electronic-11, computer-13) amod(products-16, computer-13) conj-and(electronic-11, building-15) amod(products-16, building-15) dobj(makes-8, products-16) PETE goes one step further by translating most of these dependencies into natural language entailments.", "labels": [], "entities": []}, {"text": "Someone is based in Los Angeles.", "labels": [], "entities": []}, {"text": "PETE has some advantages over representations based on grammatical relations.", "labels": [], "entities": [{"text": "PETE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5407781600952148}]}, {"text": "For example SD defines 55 relations organized in a hierarchy, and it maybe non-trivial fora non-linguist to understand the difference between ccomp (clausal complement with internal subject) and xcomp (clausal complement with external subject) or between nsubj (nominal subject) and xsubj (controlling subject).", "labels": [], "entities": []}, {"text": "In fact it could be argued that proposals like SD replace one artificial annotation formalism with another and no two such proposals agree on the ideal set of binary relations to use.", "labels": [], "entities": []}, {"text": "In contrast, untrained annotators have no difficulty unanimously agreeing on the validity of most PETE type entailments.", "labels": [], "entities": [{"text": "PETE type entailments", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.5951740940411886}]}, {"text": "However there are also significant challenges associated with an evaluation scheme like PETE.", "labels": [], "entities": [{"text": "PETE", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.6886481642723083}]}, {"text": "It is not always clear how to convert certain relations into grammatical hypothesis sentences without including most of the original sentence in the hypothesis.", "labels": [], "entities": []}, {"text": "Including too much of the sentence in the hypothesis would increase the chances of getting the right answer with the wrong parse.", "labels": [], "entities": []}, {"text": "Grammatical hypothesis sentences are especially difficult to construct when a (negative) entailment is based on a bad parse of the sentence.", "labels": [], "entities": []}, {"text": "Introducing dummy words like \"someone\" or \"something\" alleviates part of the problem but does not help in the case of clausal complements.", "labels": [], "entities": []}, {"text": "In summary, PETE makes the annotation phase more practical and consistent but shifts the difficulty to the entailment creation phase.", "labels": [], "entities": [{"text": "PETE", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.4180748760700226}]}, {"text": "PETE gets closer to an extrinsic evaluation by focusing on semantically relevant, application oriented differences that can be expressed in natural language sentences.", "labels": [], "entities": []}, {"text": "This makes the evaluation procedure indirect: a parser developer has to write an extension that can handle entailment questions.", "labels": [], "entities": []}, {"text": "However, given the simplicity of the entailments, the complexity of such an extension is comparable to one that extracts grammatical relations.", "labels": [], "entities": []}, {"text": "The balance of what is being evaluated is also important.", "labels": [], "entities": []}, {"text": "A treebank based evaluation scheme may mix semantically relevant and irrelevant mistakes, but at least it covers every sentence at a uniform level of detail.", "labels": [], "entities": []}, {"text": "In this evaluation, we focused on sentences and relations where state of the art parsers disagree.", "labels": [], "entities": []}, {"text": "We hope this methodology will uncover weaknesses that the next generation systems can focus on.", "labels": [], "entities": []}, {"text": "The remaining sections will go into more detail about these challenges and the solutions we have chosen to implement.", "labels": [], "entities": []}, {"text": "Section 2 explains the method followed to create the PETE dataset.", "labels": [], "entities": [{"text": "PETE dataset", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9005737602710724}]}, {"text": "Sec-tion 3 evaluates the baseline systems the task organizers created by implementing simple entailment extensions for several state of the art parsers.", "labels": [], "entities": []}, {"text": "Section 4 presents the participating systems, their methods and results.", "labels": [], "entities": []}, {"text": "Section 5 summarizes our contribution.", "labels": [], "entities": []}], "datasetContent": [{"text": "To generate the entailments for the PETE task we followed the following three steps: 1.", "labels": [], "entities": [{"text": "PETE task", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.6839913725852966}]}, {"text": "Identify syntactic dependencies that are challenging to state of the art parsers.", "labels": [], "entities": []}, {"text": "2. Construct short entailment sentences that paraphrase those dependencies.", "labels": [], "entities": []}, {"text": "3. Identify the subset of the entailments with high inter-annotator agreement.", "labels": [], "entities": []}, {"text": "The final dataset contained 367 entailments which were randomly divided into a 66 sentence development test and a 301 sentence test set.", "labels": [], "entities": []}, {"text": "52% of the entailments in the test set were positive.", "labels": [], "entities": []}, {"text": "Approximately half of the final entailments were from the Unbounded Dependency Corpus, a third were from the Brown section of the Penn Treebank, and the remaining were from the Charniak sentences.", "labels": [], "entities": [{"text": "Unbounded Dependency Corpus", "start_pos": 58, "end_pos": 85, "type": "DATASET", "confidence": 0.8459912339846293}, {"text": "Brown section of the Penn Treebank", "start_pos": 109, "end_pos": 143, "type": "DATASET", "confidence": 0.9206402798493704}]}, {"text": "lists the most frequent grammatical relations encountered in the entailments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Most frequent grammatical relations en- countered in the entailments.", "labels": [], "entities": []}, {"text": " Table 2: Baseline systems: The second column  gives the performance on the PETE test set, the  third column gives the unlabeled attachment score  on section 23 of the Penn Treebank.", "labels": [], "entities": [{"text": "PETE test set", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9098629156748453}, {"text": "Penn Treebank", "start_pos": 168, "end_pos": 181, "type": "DATASET", "confidence": 0.943021684885025}]}, {"text": " Table 3: Participating systems and their scores. The system identifier consists of the participant ID,  system ID, and the system name given by the participant. Accuracy gives the percentage of correct  entailments. Precision, Recall and F1 are calculated for positive entailments.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9986503720283508}, {"text": "Precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9989295601844788}, {"text": "Recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9963416457176208}, {"text": "F1", "start_pos": 239, "end_pos": 241, "type": "METRIC", "confidence": 0.9986870884895325}]}]}