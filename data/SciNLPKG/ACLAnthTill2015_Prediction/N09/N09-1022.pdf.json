{"title": [{"text": "Japanese Query Alteration Based on Semantic Similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a unified approach to web search query alterations in Japanese that is not limited to particular character types or ortho-graphic similarity between a query and its alteration candidate.", "labels": [], "entities": [{"text": "web search query alterations", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.632380947470665}]}, {"text": "Our model is based on previous work on English query correction, but makes some crucial improvements: (1) we augment the query-candidate list to include orthographically dissimilar but semantically similar pairs; and (2) we use kernel-based lexical semantic similarity to avoid the problem of data sparseness in computing query-candidate similarity.", "labels": [], "entities": [{"text": "English query correction", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6534446875254313}]}, {"text": "We also propose an efficient method for generating query-candidate pairs for model training and testing.", "labels": [], "entities": []}, {"text": "We show that the proposed method achieves about 80% accuracy on the query alteration task, improving over previously proposed methods that use semantic similarity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9989261031150818}, {"text": "query alteration task", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7665047844250997}]}], "introductionContent": [{"text": "Web search query correction is an important problem to solve for robust information retrieval given how pervasive errors are in search queries: it is said that more than 10% of web search queries contain errors ().", "labels": [], "entities": [{"text": "Web search query correction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7261010855436325}, {"text": "information retrieval", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.7800976037979126}]}, {"text": "English query correction has been an area of active research in recent years, building on previous work on generalpurpose spelling correction.", "labels": [], "entities": [{"text": "English query correction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6012038787206014}, {"text": "generalpurpose spelling correction", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.5935414334138235}]}, {"text": "However, there has been little investigation of query correction in languages other than English.", "labels": [], "entities": [{"text": "query correction", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.9048623442649841}]}, {"text": "In this paper, we address the issue of query correction, and more generally, query alteration in Japanese.", "labels": [], "entities": [{"text": "query correction", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8278512954711914}, {"text": "query alteration", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.7595742344856262}]}, {"text": "Japanese poses particular challenges to the query correction task due to its complex writing system, summarized in: Japanese character types and spelling variants main character types, including two types of kana (phonetic alphabet -hiragana and katakana), kanji (ideographic -characters represent meaning) and Roman alphabet; a word can be legitimately spelled in multiple ways, combining any of these character sets.", "labels": [], "entities": [{"text": "query correction", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.8638730049133301}]}, {"text": "For example, the word for 'protein' can be spelled as (all in hiragana), (katakana+kanji), (all in kanji) or (hiragana+kanji), all pronounced in the same way (tanpakushitsu).", "labels": [], "entities": []}, {"text": "Some examples of these spelling variants are shown in with the prefix Sp: as is observed from the figure, spelling variation occurs within and across different character types.", "labels": [], "entities": []}, {"text": "Resolving these variants will be essential not only for information retrieval but practically for all NLP tasks.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.8044518232345581}]}, {"text": "A particularly prolific source of spelling variations in Japanese is katakana.", "labels": [], "entities": []}, {"text": "Katakana characters are used to transliterate words from English and other foreign languages, and as such, the variations in the source language pronunciation as well as the ambiguity in sound adaptation are reflected in the katakana spelling.", "labels": [], "entities": []}, {"text": "For example, report that at least six distinct transliterations of the word 'spaghetti' (, , etc.) are attested in the newspaper corpus they studied.", "labels": [], "entities": []}, {"text": "Normalizing katakana spelling variations has been the subject of research by itself).", "labels": [], "entities": [{"text": "Normalizing katakana spelling variations", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8631391525268555}]}, {"text": "Similarly, English-to-katakana transliteration (e.g., 'fedex' as fedekkusu in) and katakana-to-English back-transliteration (e.g., back into 'fedex') have also been studied extensively, as it is an essential component for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 222, "end_pos": 241, "type": "TASK", "confidence": 0.7899162471294403}]}, {"text": "To our knowledge, however, there has been no work that addresses spelling variation in Japanese generally.", "labels": [], "entities": [{"text": "spelling variation in Japanese", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.8675715774297714}]}, {"text": "In this paper, we propose a general approach to query correction/alteration in Japanese.", "labels": [], "entities": [{"text": "query correction/alteration", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.8730550408363342}]}, {"text": "Our goal is to find precise re-write candidates fora query, be it a correction of a spelling error, normalization of a spelling variant, or finding a strict synonym including abbreviations (e.g., MS 'Microsoft', prefixed by Abbr in) and true synonyms (e.g., (translation of 'seat') (transliteration of 'seat', indicated by Syn in) 2 . Our method is based on previous work on English query correction in that we use both spelling and semantic similarity between a query and its alteration candidate, but is more general in that we include alteration candidates that are not similar to the original query in spelling.", "labels": [], "entities": [{"text": "English query correction", "start_pos": 375, "end_pos": 399, "type": "TASK", "confidence": 0.5781053801377615}]}, {"text": "In computing semantic similarity, we adopt a kernel-based method (), which improves the accuracy of the query alteration results over previously proposed methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.998674750328064}]}, {"text": "We also introduce a novel approach to creating a dataset of query and alteration candidate pairs efficiently and reliably from query session logs.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all the experiments conducted in this paper, we used a subset of the Japanese search query logs submitted to Live Search (www.live.com) in November and December of 2007.", "labels": [], "entities": [{"text": "Japanese search query logs submitted to Live Search (www.live.com)", "start_pos": 73, "end_pos": 139, "type": "DATASET", "confidence": 0.8891512101346796}]}, {"text": "Queries submitted less than eight times were deleted.", "labels": [], "entities": []}, {"text": "The query log we used contained 83,080,257 tokens and 1,038,499 unique queries.", "labels": [], "entities": []}, {"text": "Models of query correction in previous work were trained and evaluated using manually created querycandidate pairs.", "labels": [], "entities": [{"text": "query correction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8866842687129974}]}, {"text": "That is, human annotators were given a set of queries and were asked to provide a correction for each query when it needed to be rewritten.", "labels": [], "entities": []}, {"text": "As point out, however, this method is seriously flawed in that the intention of the original query is completely lost to the annotator, without which the correction is often impossible: it is not clear if gogle should be corrected to google or goggle, or neither -gogle maybe a brand new product name.", "labels": [], "entities": []}, {"text": "Cucerzan and Brill therefore performed a second evaluation, where the test data was drawn by sampling the query logs for successive queries (q 1 , q 2 ) by the same user where the edit distance between q 1 and q 2 are within a certain threshold, which are then submitted to annotators for generating the correction.", "labels": [], "entities": []}, {"text": "While this method makes the annotation more reliable by relying on user (rather than annotator) reformulation, the task is still overly difficult: going back to the example in Section 1, it is unclear which spelling of 'protein' produces the best search results -it can only be empirically determined.", "labels": [], "entities": []}, {"text": "Their method also eliminates all pairs of candidates that are not orthographically similar.", "labels": [], "entities": []}, {"text": "We have therefore improved their method in the following manner, making the process more automated and thus more reliable.", "labels": [], "entities": []}, {"text": "We first collected a subset of the query log that contains only those pairs (q 1 , q 2 ) that are issued successively by the same user, q 2 is issued within 3 minutes of q 1 , and q 2 resulted in a click of the resulting page while q 1 did not.", "labels": [], "entities": []}, {"text": "The last condition adds the evidence that q 2 was a better formulation than q 1 . We then ranked the collected query pairs using loglikelihood ratio (LLR), which measures the dependence between q 1 and q 2 within the context of web queries ().", "labels": [], "entities": [{"text": "loglikelihood ratio (LLR)", "start_pos": 129, "end_pos": 154, "type": "METRIC", "confidence": 0.8095111966133117}]}, {"text": "We randomly sampled 10,000 query pairs with LLR \u2265 200, and submitted them to annotators, who only confirm or reject a query pair as being synonymous.", "labels": [], "entities": []}, {"text": "For example, q 1 = nikon and q 2 = canon are related but not synonymous, while we are reasonably sure q 1 = ipot and q 2 = ipod are synonymous, given that this pair has a high LLR value.", "labels": [], "entities": []}, {"text": "This verification process is extremely fast and consistent across annotators: it takes less than 1 hour to go through 1,000 query pairs, and the inter-annotator agreement rate of two annotators on 2,000 query pairs was 95.7%.", "labels": [], "entities": []}, {"text": "We annotated 10,000 query pairs consisting of alphanumerical and kana characters in this manner.", "labels": [], "entities": []}, {"text": "After rejecting non-synonymous pairs and those which do not co-occur with any context patterns, 6,489 pairs remained, and we used 1,243 pairs for testing, 628 as a development set, and 4,618 for training the maximum entropy model.", "labels": [], "entities": []}, {"text": "The performance of query alteration was evaluated based on the following measures ().", "labels": [], "entities": [{"text": "query alteration", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7578851878643036}]}, {"text": "The input queries, correct suggestions, and outputs were matched in a case-insensitive manner.", "labels": [], "entities": []}, {"text": "\u2022 Accuracy: The number of correct outputs generated by the system divided by the total number of queries in the test set; \u2022 Recall: The number of correct suggestions for altered queries divided by the total number of altered queries in the test set; \u2022 Precision: The number of correct suggestions for altered queries divided by the total number of alterations made by the system.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9986063838005066}, {"text": "Recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9885557293891907}]}, {"text": "The parameters for the kernels, namely, \u03b2, \u03b3, and \u03b4, are tuned using the development set.", "labels": [], "entities": []}, {"text": "The finally employed values are: \u03b2 = 0.3 for\u02c6Kfor\u02c6 for\u02c6K, \u02dc K, and\u02c6Kand\u02c6 and\u02c6K + , \u03b2 = 0.2 for\u02dcKfor\u02dc for\u02dcK + , \u03b3 = 0.2 and \u03b4 = 0.4 for\u02c6Kfor\u02c6 for\u02c6K + , and \u03b3 = 0.35 and \u03b4 = 0.7 for\u02dcKfor\u02dc for\u02dcK + . In the source channel model, we manually scaled the language probability by a factor of 0.1 to alleviate the bias toward highly frequent candidates.", "labels": [], "entities": []}, {"text": "As the initial candidate set C 0 , top-50 instances were selected by the source channel model, and 100 patterns were extracted as P 0 by the Tchai iteration after removing generic patterns, which we detected simply by rejecting those which induced more than 200 unique instances.", "labels": [], "entities": []}, {"text": "Finally top-30 instances were induced using P 0 to create C 1 . Generic instances were not removed in this process because they may still be alterations of input query q.", "labels": [], "entities": []}, {"text": "The maximum size of P 1 was set to 2,000, after removing unreliable patterns with reliability smaller than 0.0001.", "labels": [], "entities": [{"text": "P 1", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8442951738834381}, {"text": "reliability", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.9674721956253052}]}, {"text": "SC is the source channel model, while the others are maximum entropy (ME) models with different features.", "labels": [], "entities": []}, {"text": "ME-NoSim uses the same features as SC, but considerably outperforms SC in all three measures, confirming the superiority of the ME approach.", "labels": [], "entities": []}, {"text": "Decomposing the three edit distance functions into three separate features in the ME model may also explain the better result.", "labels": [], "entities": []}, {"text": "All the ME approaches outperformed SC inaccuracy with a statistically significant difference (p < 0.0001 on McNemar's test).", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.8169206579526266}]}], "tableCaptions": [{"text": " Table 1: Performance results (%)  Model  Accuracy Recall Precision  SC  71.12  39.29  45.09  ME-NoSim  74.58  44.58  52.52  ME-Cos  74.18  45.84  50.70  ME-vN  74.34  45.59  52.16  ME-Exp  73.61  44.84  50.57  ME-vN+  75.06  44.33  53.01  ME-Exp+  75.14  44.08  53.52", "labels": [], "entities": [{"text": "Accuracy Recall Precision  SC", "start_pos": 42, "end_pos": 71, "type": "METRIC", "confidence": 0.8367154151201248}]}, {"text": " Table 2: Performance with the multiple reference model  Model  Accuracy Recall Precision  SC  75.30  48.61  55.78  ME-NoSim  79.49  56.17  66.17  ME-Cos  79.32  58.19  64.35  ME-vN  79.24  57.18  65.42  ME-Exp  78.52  56.42  63.64  ME-vN+  79.89  55.67  66.57  ME-Exp+  79.81  54.91  66.67", "labels": [], "entities": [{"text": "Accuracy Recall Precision  SC", "start_pos": 64, "end_pos": 93, "type": "METRIC", "confidence": 0.823456734418869}]}]}