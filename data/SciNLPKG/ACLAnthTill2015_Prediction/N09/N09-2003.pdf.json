{"title": [{"text": "Efficient Extraction of Oracle-best Translations from Hypergraphs", "labels": [], "entities": [{"text": "Efficient Extraction of Oracle-best Translations from Hypergraphs", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.6832544888768878}]}], "abstractContent": [{"text": "Hypergraphs are used in several syntax-inspired methods of machine translation to compactly encode exponentially many translation hypotheses.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7521220445632935}]}, {"text": "The hypotheses closest to given reference translations therefore cannot be found via brute force, particularly for popular measures of closeness such as BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9603018164634705}]}, {"text": "We develop a dynamic program for extracting the so called oracle-best hypothesis from a hyper-graph by viewing it as the problem of finding the most likely hypothesis under an n-gram language model trained from only the reference translations.", "labels": [], "entities": []}, {"text": "We further identify and remove massive redundancies in the dynamic program state due to the sparsity of n-grams present in the reference translations, resulting in a very efficient program.", "labels": [], "entities": []}, {"text": "We present run-time statistics for this program, and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components.", "labels": [], "entities": []}], "introductionContent": [{"text": "A hypergraph, as demonstrated by, is a compact data-structure that can encode an exponential number of hypotheses generated by a regular phrase-based machine translation (MT) system (e.g.,) or a syntaxbased MT system (e.g.,).", "labels": [], "entities": [{"text": "phrase-based machine translation (MT)", "start_pos": 137, "end_pos": 174, "type": "TASK", "confidence": 0.7969416777292887}]}, {"text": "While the hypergraph represents a very large set of translations, it is quite possible that some desired translations (e.g., the reference translations) are not contained in the hypergraph, due to pruning or inherent deficiency of the translation model.", "labels": [], "entities": []}, {"text": "In this case, one is often required to find the translation(s) in the hypergraph that are most similar to the desired translations, with similarity computed via some automatic metric such as BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9980514049530029}]}, {"text": "Such maximally similar translations will be called oraclebest translations, and the process of extracting them oracle extraction.", "labels": [], "entities": []}, {"text": "Oracle extraction is a nontrivial task because computing the similarity of anyone hypothesis requires information scattered over many items in the hypergraph, and the exponentially large number of hypotheses makes a brute-force linear search intractable.", "labels": [], "entities": [{"text": "Oracle extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9350056946277618}]}, {"text": "Therefore, efficient algorithms that can exploit the structure of the hypergraph are required.", "labels": [], "entities": []}, {"text": "We present an efficient oracle extraction algorithm, which involves two key ideas.", "labels": [], "entities": [{"text": "oracle extraction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7853268086910248}]}, {"text": "Firstly, we view the oracle extraction as a bottom-up model scoring process on a hypergraph, where the model is \"trained\" on the reference translation(s).", "labels": [], "entities": [{"text": "oracle extraction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7575730681419373}]}, {"text": "This is similar to the algorithm proposed fora lattice by.", "labels": [], "entities": []}, {"text": "Their algorithm, however, requires maintaining a separate dynamic programming state for each distinguished sequence of \"state\" words and the number of such sequences can be huge, making the search very slow.", "labels": [], "entities": []}, {"text": "Secondly, therefore, we present a novel look-ahead technique, called equivalent oracle-state maintenance, to merge multiple states that are equivalent for similarity computation.", "labels": [], "entities": [{"text": "equivalent oracle-state maintenance", "start_pos": 69, "end_pos": 104, "type": "TASK", "confidence": 0.6501014232635498}]}, {"text": "Our experiments show that the equivalent oraclestate maintenance technique significantly speeds up (more than 40 times) the oracle extraction.", "labels": [], "entities": [{"text": "oracle extraction", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7933048903942108}]}, {"text": "Efficient oracle extraction has at least three important applications in machine translation.", "labels": [], "entities": [{"text": "Efficient oracle extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6104597647984823}, {"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7623262703418732}]}, {"text": "Discriminative Training: In discriminative training, the objective is to tune the model parameters, e.g. weights of a perceptron model or conditional random field, such that the reference translations are preferred over competitors.", "labels": [], "entities": []}, {"text": "However, the reference translations may not be reachable by the translation system, in which case the oracle-best hypotheses should be substituted in training.", "labels": [], "entities": []}, {"text": "System Combination: Ina typical system combination task, e.g., each component system produces a set of translations, which are then grafted to form a confusion network.", "labels": [], "entities": []}, {"text": "The confusion network is then rescored, often employing additional (language) models, to select the final translation.", "labels": [], "entities": []}, {"text": "When measuring the goodness of a hypothesis in the confusion network, one requires its score under each component system.", "labels": [], "entities": []}, {"text": "However, some translations in the confusion network may not be reachable by some component systems, in which case a system's score for the most similar reachable translation serves as a good approximation.", "labels": [], "entities": []}, {"text": "Multi-source Translation: Ina multi-source translation task) the input is given in multiple source languages.", "labels": [], "entities": [{"text": "Multi-source Translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.765961766242981}]}, {"text": "This leads to a situation analogous to system combination, except that each component translation system now corresponds to a specific source language.", "labels": [], "entities": [{"text": "system combination", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6974189430475235}]}], "datasetContent": [{"text": "We report experimental results on a Chinese to English task, fora system that is trained using a similar pipeline and data resource as in. reports the goodness of the oracle-best hypotheses on three standard data sets.", "labels": [], "entities": []}, {"text": "The highest achievable BLEU score in a hypergraph is clearly much higher than in the 500-best unique strings.", "labels": [], "entities": [{"text": "achievable", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9684417843818665}, {"text": "BLEU score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.8867329359054565}]}, {"text": "This shows that a hypergraph provides a much better basis, e.g., for reranking than an n-best list.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example computation when items A and B are  combined by a rule to produce item C. | r| is the approxi- mated reference length as described in the text.", "labels": [], "entities": []}, {"text": " Table 3: Baseline and oracle-best 4-gram BLEU scores  with 4 references for NIST Chinese-English MT datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9945800304412842}, {"text": "NIST Chinese-English MT datasets", "start_pos": 77, "end_pos": 109, "type": "DATASET", "confidence": 0.8754835277795792}]}, {"text": " Table 4: BLEU scores after discriminative hypergraph- reranking. Only the language model (LM) or the transla- tion model (TM) or both (LM+TM) may be discrimina- tively trained to prefer the oracle-best hypotheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983464479446411}]}]}