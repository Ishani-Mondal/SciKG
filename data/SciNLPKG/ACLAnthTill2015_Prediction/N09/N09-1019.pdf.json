{"title": [{"text": "Structured Generative Models for Unsupervised Named-Entity Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a generative model for clustering named entities which also models named entity internal structure, clustering related words by role.", "labels": [], "entities": [{"text": "clustering named entities", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.8687286575635275}]}, {"text": "The model is entirely unsupervised; it uses features from the named entity itself and its syntactic context, and coreference information from an unsupervised pronoun re-solver.", "labels": [], "entities": []}, {"text": "The model scores 86% on the MUC-7 named-entity dataset.", "labels": [], "entities": [{"text": "MUC-7 named-entity dataset", "start_pos": 28, "end_pos": 54, "type": "DATASET", "confidence": 0.8579168518384298}]}, {"text": "To our knowledge, this is the best reported score fora fully unsuper-vised model, and the best score fora genera-tive model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity clustering is a classic task in NLP, and one for which both supervised and semi-supervised systems have excellent performance.", "labels": [], "entities": [{"text": "Named entity clustering", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6789775391419729}]}, {"text": "In this paper, we describe a fully unsupervised system (using no \"seed rules\" or initial heuristics); to our knowledge this is the best such system reported on the MUC-7 dataset.", "labels": [], "entities": [{"text": "MUC-7 dataset", "start_pos": 164, "end_pos": 177, "type": "DATASET", "confidence": 0.9842934906482697}]}, {"text": "In addition, the model clusters the words which appear in named entities, discovering groups of words with similar roles such as first names and types of organization.", "labels": [], "entities": []}, {"text": "Finally, the model defines a notion of consistency between different references to the same entity; this component of the model yields a significant increase in performance.", "labels": [], "entities": []}, {"text": "The main motivation for our system is the recent success of unsupervised generative models for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.973611980676651}]}, {"text": "The model of incorporated a latent variable for named entity class.", "labels": [], "entities": []}, {"text": "They report a named entity score of 61.2 percent, well above the baseline of 46.4, but still far behind existing named-entity systems.", "labels": [], "entities": []}, {"text": "We suspect that better models for named entities could aid in the coreference task.", "labels": [], "entities": [{"text": "coreference task", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.9028565883636475}]}, {"text": "The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess.", "labels": [], "entities": []}, {"text": "To perform joint inference, however, requires an unsupervised generative model for named entities.", "labels": [], "entities": []}, {"text": "As far as we know, this work is the best such model.", "labels": [], "entities": []}, {"text": "Named entities also pose another problem with the coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (\"Ford Motor Co.\", \"Ford\"), while erroneously merging others: (\"Ford Motor Co.\", \"Lockheed Martin Co.\").", "labels": [], "entities": []}, {"text": "showed that better features for matching named entities-exact string match and an \"alias detector\" looking for acronyms, abbreviations and name variants-improve the model's performance substantially.", "labels": [], "entities": []}, {"text": "Yet building an alias detector is nontrivial).", "labels": [], "entities": []}, {"text": "English speakers know that \"President Clinton\" is the same person as \"Bill Clinton\" , not \"President Bush\".", "labels": [], "entities": []}, {"text": "But this cannot be implemented by simple substring matching.", "labels": [], "entities": []}, {"text": "It requires some concept of the role of each word in the string.", "labels": [], "entities": []}, {"text": "Our model attempts to learn this role information by clustering the words within named entities.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on the named entity dataset from MUC-7, using the training set as development data and the formal test set as test data.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.504533052444458}]}, {"text": "The development set has 4936 named entities, of which 1575 (31.9%) are locations, 2096 (42.5%) are organizations and 1265 (25.6%) people.", "labels": [], "entities": []}, {"text": "The test set has 4069 named entities, 1321 (32.5%) locations, 1862 (45.8%) organizations and 876 (21.5%) people 5 . We use a baseline which gives all named entities the same label; this label is mapped to \"organization\".", "labels": [], "entities": []}, {"text": "In most of our experiments, we use an input file of 40000 lines.", "labels": [], "entities": []}, {"text": "For dev experiments, the labeled data contributes 1585 merged examples; for test experiments, only 1320.", "labels": [], "entities": []}, {"text": "The remaining lines are derived Expansions that used only the middle three symbols x got a prior of .005, expansions whose outermost symbol was NE 0,4 x got .0025, and so forth.", "labels": [], "entities": []}, {"text": "This is not so important for our final system, which has only 5 symbols, but was designed during development to handle systems with up to 10 symbols.", "labels": [], "entities": []}, {"text": "10 entities are labeled location|organization; since this fraction of the dataset is insignificant we score them as wrong.", "labels": [], "entities": []}, {"text": "using the process described in section 3.4 from 5 million words of NANC.", "labels": [], "entities": [{"text": "NANC", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9279240369796753}]}, {"text": "To evaluate our results, we map our three induced labels to their corresponding gold label, then count the overlap; as stated, this mapping is predictably encoded in the prior when we use the pronoun features.", "labels": [], "entities": []}, {"text": "Our experimental results are shown in.", "labels": [], "entities": []}, {"text": "All models perform above baseline, and all features contribute significantly to the final result.", "labels": [], "entities": []}, {"text": "Test results for our final model are shown in.", "labels": [], "entities": []}, {"text": "A confusion matrix for our highest-likelihood test solution is shown as.", "labels": [], "entities": []}, {"text": "The highest confusion class is \"organization\", which is confused most often with \"location\" but also with \"person\".", "labels": [], "entities": []}, {"text": "\"location\" is likewise confused with \"organization\".", "labels": [], "entities": []}, {"text": "\"person\" is the easiest class to identify-we believe this explains the slight decline in performance from dev to test, since dev has proportionally more people.", "labels": [], "entities": []}, {"text": "Our mapping from grammar symbols to words appears in; the learned prepositional and modifier information is in.", "labels": [], "entities": []}, {"text": "Overall the results are good, but not perfect; for instance, the P ers states are mostly interpretable as a sequence of title -first name -middle name or initial -last name - last name or post-title (similar to).", "labels": [], "entities": []}, {"text": "The organization symbols tend to put nationalities and other modifiers first, and end with institutional types like \"inc.\" or \"center\", although there is a similar (but smaller) cluster of types at Org 2 , suggesting the system has incorrectly found two analyses for these names.", "labels": [], "entities": []}, {"text": "Location symbols seem to put entities with a single, non-analyzable name into Loc 2 , and use symbols 0, 1 and 3 for compound names.", "labels": [], "entities": []}, {"text": "Loc 4 has been recruited for time expressions, since our NANC dataset includes many of these, but we failed to account for them in the model.", "labels": [], "entities": [{"text": "NANC dataset", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9633607864379883}]}, {"text": "Since they appear in a single class here, we are optimistic that they could be clustered separately if another class and some appropriate features were added to the prior.", "labels": [], "entities": []}, {"text": "Some errors do appear (\"supreme court\" and \"house\" as locations, \"minister\" and \"chairman\" as middle names, \"newt gingrich\" as a multiword phrase).", "labels": [], "entities": []}, {"text": "The table also reveals an unforeseen issue with the parser: it tends to analyze the dateline beginning a news story along with the following NP (\"WASHINGTON Bill Clinton said...\").", "labels": [], "entities": [{"text": "dateline", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9591502547264099}, {"text": "NP", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.8536016345024109}, {"text": "WASHINGTON", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.8400612473487854}]}, {"text": "Thus common datelines (\"washington\", \"new york\" and \"los angeles\") appear instate 0 for each class.", "labels": [], "entities": [{"text": "datelines", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9640686511993408}]}], "tableCaptions": [{"text": " Table 1: Accuracy of various models on development  data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.995209276676178}]}, {"text": " Table 2: Accuracy of the final model on test data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976487755775452}]}, {"text": " Table 4. Overall the results  are good, but not perfect; for instance, the P ers  states are mostly interpretable as a sequence of ti- tle -first name -middle name or initial -last name -", "labels": [], "entities": []}]}