{"title": [], "abstractContent": [{"text": "We present a shallow approach to the sentence ordering problem.", "labels": [], "entities": [{"text": "sentence ordering problem", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.8142763574918112}]}, {"text": "The employed features are based on discourse entities, shallow syntactic analysis, and temporal precedence relations retrieved from VerbOcean.", "labels": [], "entities": [{"text": "VerbOcean", "start_pos": 132, "end_pos": 141, "type": "DATASET", "confidence": 0.9494407176971436}]}, {"text": "We show that these relatively simple features perform well in a machine learning algorithm on datasets containing sequences of events, and that the resulting models achieve optimal performance with small amounts of training data.", "labels": [], "entities": []}, {"text": "The model does not yet perform well on datasets describing the consequences of events, such as the destructions after an earthquake.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence ordering is a problem in many natural language processing tasks.", "labels": [], "entities": [{"text": "Sentence ordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9325829148292542}, {"text": "natural language processing tasks", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.7079530954360962}]}, {"text": "While it has, historically, mainly been considered a challenging problem in (concept-to-text) language generation tasks, more recently, the issue has also generated interest within summarization research).", "labels": [], "entities": [{"text": "concept-to-text) language generation tasks", "start_pos": 77, "end_pos": 119, "type": "TASK", "confidence": 0.7108117163181304}, {"text": "summarization", "start_pos": 181, "end_pos": 194, "type": "TASK", "confidence": 0.983385443687439}]}, {"text": "In the spirit of the latter, this paper investigates the following questions: (1) Does the topic of the text influence the factors that are important to sentence ordering?", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 153, "end_pos": 170, "type": "TASK", "confidence": 0.7375678569078445}]}, {"text": "(2) Which factors are most important for determining coherent sentence orderings?", "labels": [], "entities": [{"text": "determining coherent sentence orderings", "start_pos": 41, "end_pos": 80, "type": "TASK", "confidence": 0.7012919485569}]}, {"text": "(3) How much performance is gained when using deeper knowledge resources?", "labels": [], "entities": []}, {"text": "Past research has investigated a wide range of aspects pertaining to the ordering of sentences in text.", "labels": [], "entities": [{"text": "ordering of sentences in text", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.7914020419120789}]}, {"text": "The most prominent approaches include: (1) temporal ordering in terms of publication date, (2) temporal ordering in terms of textual cues in sentences (), (3) the topic of the sentences, (4) coherence theories (), e.g., Centering Theory, (5) content models (), and (6) ordering(s) in the underlying documents in the case of summarisation (;).", "labels": [], "entities": []}], "datasetContent": [{"text": "This section introduces the datasets used for the experiments, describes the experiments, and discusses our main findings.", "labels": [], "entities": []}, {"text": "The three datasets used for the automatic evaluation in this paper are based on human-generated texts.", "labels": [], "entities": []}, {"text": "The first two are the earthquake and accident datasets used by.", "labels": [], "entities": []}, {"text": "Each of these sets consists of 100 datasets in the training and test sets, respectively, as well as 20 random permutations for each text.", "labels": [], "entities": []}, {"text": "The third dataset is similar to the first two in that it contains original texts and random permutations.", "labels": [], "entities": []}, {"text": "In contrast to the other two sources, however, this dataset is based on the human summaries from DUC 2005.", "labels": [], "entities": [{"text": "human summaries from DUC 2005", "start_pos": 76, "end_pos": 105, "type": "DATASET", "confidence": 0.6431402325630188}]}, {"text": "It comprises 300 human summaries on 50 document sets, resulting in a total of 6,000 pairwise rankings split into training and test sets.", "labels": [], "entities": []}, {"text": "The source furthermore differs from's datasets in that the content of each text is not based on one individual event (an earthquake or accident), but on more complex topics followed over a period of time (e.g., the espionage case between GM and VW along with the various actions taken to resolve it).", "labels": [], "entities": [{"text": "GM and VW", "start_pos": 238, "end_pos": 247, "type": "DATASET", "confidence": 0.6859587033589681}]}, {"text": "Since the different document sets cover completely different topics the third dataset will mainly be used to evaluate the topic-independent properties of our model.", "labels": [], "entities": []}, {"text": "In the first part of this experiment, we consider the problem of the granularity of the syntactic units to be used.", "labels": [], "entities": []}, {"text": "That is, does it make a difference whether we use the words in the sentence, the words in the noun groups, the words in the verb groups, or the words in the respective heads of the groups to determine coherence?", "labels": [], "entities": []}, {"text": "(The units are obtained by processing the documents using the LT-TTT2 tools); the lemmatizer used by LT-TTT2 is morpha).)", "labels": [], "entities": [{"text": "LT-TTT2", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9372339844703674}, {"text": "LT-TTT2", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9374836087226868}]}, {"text": "We also consider whether lemmatization is beneficial in each of the granularities.", "labels": [], "entities": []}, {"text": "The results -presented in -indicate that considering only the heads of the verb and noun groups separately provides the best performance.", "labels": [], "entities": []}, {"text": "In particular, the heads outperform the whole groups, and the heads separately also outperform noun and verb group heads together.", "labels": [], "entities": []}, {"text": "As for the question of whether lemmatization provides better results, one needs to distinguish the case of noun and verb groups.", "labels": [], "entities": []}, {"text": "For noun groups, lemmatization improves performance, which can mostly be attributed to singular and plural forms.", "labels": [], "entities": []}, {"text": "In the case of verb groups, however, the lemmatized version yields worse results than the surface forms, a fact mainly explained by the tense and modality properties of verbs.", "labels": [], "entities": []}, {"text": "Given the appropriate unit of granularity, we can consider the impact of semantic relations between surface realizations on coherence.", "labels": [], "entities": []}, {"text": "For these experiments we use the synonym, hypernym, hyponym, and antonym relations in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9622212052345276}]}, {"text": "The rationale for the consideration of semantic relations lies in the fact that the frequent use of the same words is usually deemed bad writing style.", "labels": [], "entities": []}, {"text": "One therefore tends to observe the use of semantically similar terms in neighboring sentences.", "labels": [], "entities": []}, {"text": "The results of using semantic relations for coherence rating are provided in Table 3.", "labels": [], "entities": []}, {"text": "Synonym detection improves performance, while the other units provide poorer performance.", "labels": [], "entities": [{"text": "Synonym detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7777161300182343}]}, {"text": "This suggests that the hypernym and hyponym relations tend to over-generalize in the semantics.", "labels": [], "entities": []}, {"text": "As stated, the ultimate goal of the models presented in this paper is the application of sentence ordering to automatically generated summaries.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.719331756234169}]}, {"text": "It is, in this regard, important to distinguish coherence as studied in Experiment 1 and coherence in the context of automatic summarization.", "labels": [], "entities": []}, {"text": "Namely, for newswire summarization systems, the topics of the documents are  The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from unknown at the time of training.", "labels": [], "entities": []}, {"text": "As a result, model performance on out-of-domain texts is important for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9850073456764221}]}, {"text": "Experiment 2 seeks to evaluate how well our model performs in such cases.", "labels": [], "entities": []}, {"text": "To this end, we carryout two sets of tests.", "labels": [], "entities": []}, {"text": "First, we crosstrain the models between the accident and earthquake datasets to determine system performance in unseen domains.", "labels": [], "entities": []}, {"text": "Second, we use the dataset based on the DUC 2005 model summaries to investigate whether our model's performance on unseen topics reaches a plateau after training on a particular number of different topics.", "labels": [], "entities": [{"text": "DUC 2005 model summaries", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.9800500720739365}]}, {"text": "Surprisingly, the results are rather good, when compared to the poor results in part of the previous experiment.", "labels": [], "entities": []}, {"text": "In fact, model performance is nearly independent of the training topic.", "labels": [], "entities": []}, {"text": "Nevertheless, the results on the earthquake test set indicate that our model is missing essential components for the correct prediction of sentence orderings on this set.", "labels": [], "entities": [{"text": "earthquake test set", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.7516418298085531}, {"text": "prediction of sentence orderings", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.6467596217989922}]}, {"text": "When compared to the results obtained by and, it would appear that direct sentenceto-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset.", "labels": [], "entities": [{"text": "Barzilay and Lapata baseline score", "start_pos": 126, "end_pos": 160, "type": "DATASET", "confidence": 0.6690807998180389}]}, {"text": "The final experimental setup applies the best -indicate that very little training data (both regarding the number of pairs and the number of different topics) is needed.", "labels": [], "entities": []}, {"text": "Unfortunately, they also suggest that the DUC summaries are more similar to the earthquake than to the accident dataset.", "labels": [], "entities": [{"text": "DUC summaries", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9463820457458496}]}], "tableCaptions": [{"text": " Table 1: Number of pairwise rankings in the training and  test sets for the three datasets", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.958369255065918}]}, {"text": " Table 2: Performance with respect to the syntactic unit  of processing of the training datasets. Accuracy is the  fraction of correctly ranked pairs of documents over the  total number of pairs. (?Heads sentence? is the heads of  NGs and VGs.)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9987749457359314}]}, {"text": " Table 3: The impact of WordNet on sentence ordering  accuracy", "labels": [], "entities": [{"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9371472001075745}, {"text": "sentence ordering", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7710328996181488}]}, {"text": " Table 4: The impact of the VerbOcean ?happens-before?  temporal precedence relation on accuracy on the training  datasets", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9965684413909912}]}, {"text": " Table 5: Effect of longer range relations on coherence  accuracy", "labels": [], "entities": []}, {"text": " Table 6: Comparison of the developed model with other  state-of-the-art systems. Coreference+Syntax+Salience+  and Coreference?Syntax+Salience+ are the Barzilay and  Lapata (2008) model, HMM-based Content Models is the  Barzilay and Lee (2004) paper and Latent Semantic Anal- ysis is the Barzilay and Lapata (2008) implementation of  Peter W. Foltz and Landauer", "labels": [], "entities": []}, {"text": " Table 7:  Cross-Training between Accident and  Earthquake datasets.", "labels": [], "entities": [{"text": "Earthquake datasets", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.8087406158447266}]}, {"text": " Table 8: Accuracy on 20 test topics (2,700 pairs) with  respect to the number of topics used for training using  the model Chunk+Temporal-WordNet+LongRange+", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9977222084999084}, {"text": "LongRange", "start_pos": 147, "end_pos": 156, "type": "DATASET", "confidence": 0.7406752109527588}]}]}