{"title": [{"text": "Adding More Languages Improves Unsupervised Multilingual Part-of-Speech Tagging: A Bayesian Non-Parametric Approach", "labels": [], "entities": [{"text": "Adding More Languages Improves Unsupervised Multilingual Part-of-Speech Tagging", "start_pos": 0, "end_pos": 79, "type": "TASK", "confidence": 0.6087553575634956}]}], "abstractContent": [{"text": "We investigate the problem of unsupervised part-of-speech tagging when raw parallel data is available in a large number of languages.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6938439905643463}]}, {"text": "Patterns of ambiguity vary greatly across languages and therefore even unannotated multilingual data can serve as a learning signal.", "labels": [], "entities": []}, {"text": "We propose a non-parametric Bayesian model that connects related tagging decisions across languages through the use of multilingual latent variables.", "labels": [], "entities": []}, {"text": "Our experiments show that performance improves steadily as the number of languages increases.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we investigate the problem of unsupervised part-of-speech tagging when unannotated parallel data is available in a large number of languages.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6926202923059464}]}, {"text": "Our goal is to develop a fully joint multilingual model that scales well and shows improved performance for individual languages as the total number of languages increases.", "labels": [], "entities": []}, {"text": "Languages exhibit ambiguity at multiple levels, making unsupervised induction of their underlying structure a difficult task.", "labels": [], "entities": []}, {"text": "However, sources of linguistic ambiguity vary across languages.", "labels": [], "entities": []}, {"text": "For example, the word fish in English can be used as either a verb or a noun.", "labels": [], "entities": []}, {"text": "In French, however, the noun poisson (fish) is entirely distinct from the verbal form p\u00eacher (to fish).", "labels": [], "entities": []}, {"text": "Previous work has leveraged this idea by building models for unsupervised learning from aligned bilingual data.", "labels": [], "entities": []}, {"text": "However, aligned data is often available for many languages.", "labels": [], "entities": []}, {"text": "The benefits of bilingual learning vary markedly depending on which pair of languages is selected, and without labeled data it is unclear how to determine which supplementary language is most helpful.", "labels": [], "entities": []}, {"text": "In this paper, we show that it is possible to leverage all aligned languages simultaneously, achieving accuracy that inmost cases outperforms even optimally chosen bilingual pairings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9994243383407593}]}, {"text": "Even in expressing the same meaning, languages take different syntactic routes, leading to variation in part-of-speech sequences.", "labels": [], "entities": []}, {"text": "Therefore, an effective multilingual model must accurately model common linguistic structure, yet remain flexible to the idiosyncrasies of each language.", "labels": [], "entities": []}, {"text": "This tension only becomes stronger as additional languages are added to the mix.", "labels": [], "entities": []}, {"text": "From a computational standpoint, the main challenge is to ensure that the model scales well as the number of languages increases.", "labels": [], "entities": []}, {"text": "Care must betaken to avoid an exponential increase in the parameter space as well as the time complexity of inference procedure.", "labels": [], "entities": []}, {"text": "We propose a non-parametric Bayesian model for joint multilingual tagging.", "labels": [], "entities": [{"text": "joint multilingual tagging", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.5781001647313436}]}, {"text": "The topology of our model connects tagging decisions within a language as well as across languages.", "labels": [], "entities": []}, {"text": "The model scales linearly with the number of languages, allowing us to incorporate as many as are available.", "labels": [], "entities": []}, {"text": "For each language, the model contains an HMM-like substructure and connects these substructures to one another by means of cross-lingual latent variables.", "labels": [], "entities": []}, {"text": "These variables, which we refer to as superlingual tags, capture repeated multilingual patterns and thus reduce the overall uncertainty in tagging decisions.", "labels": [], "entities": []}, {"text": "We evaluate our model on a parallel corpus of eight languages.", "labels": [], "entities": []}, {"text": "The model is trained once using all languages, and its performance is tested separately for each on a held-out monolingual test set.", "labels": [], "entities": []}, {"text": "When a complete tag lexicon is provided, our unsupervised model achieves an average accuracy of 95%, in comparison to 91% for an unsupervised monolingual Bayesian HMM and 97.4% for its supervised counterpart.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9995195865631104}]}, {"text": "Thus, on average, the gap between unsupervised and supervised monolingual performance is cut by nearly two thirds.", "labels": [], "entities": []}, {"text": "We also examined scenarios where the tag lexicon is reduced in size.", "labels": [], "entities": []}, {"text": "In all cases, the multilingual model yielded substantial performance gains.", "labels": [], "entities": []}, {"text": "Finally, we examined the performance of our model when trained on all possible subsets of the eight languages.", "labels": [], "entities": []}, {"text": "We found that performance improves steadily as the number of available languages increases.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our model in an unsupervised framework where only raw parallel text is available for each of the languages.", "labels": [], "entities": []}, {"text": "In addition, we assume that for each language a tag dictionary is available that covers some subset of words in the text.", "labels": [], "entities": []}, {"text": "The task is to learn an independent tagger for each language that can annotate non-parallel raw text using the learned parameters.", "labels": [], "entities": []}, {"text": "All reported results are on non-parallel monolingual test data.", "labels": [], "entities": []}, {"text": "Data For our experiments we use the MultextEast parallel corpus) which has been used before for multilingual learning).", "labels": [], "entities": [{"text": "MultextEast parallel corpus", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.8482839465141296}]}, {"text": "The tagged portion of the corpus includes a 100,000 word English text, Orwell's novel \"Nineteen Eighty Four\", and its translation into seven languages: Bulgarian, Czech, Estonian, Hungarian, Romanian, Slovene and Serbian.", "labels": [], "entities": []}, {"text": "The corpus also includes a tag lexicon for each of these languages.", "labels": [], "entities": []}, {"text": "We use the first 3/4 of the text for learning and the last 1/4 as held-out non-parallel test data.", "labels": [], "entities": []}, {"text": "The corpus provides sentence level alignments.", "labels": [], "entities": [{"text": "sentence level alignments", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.6432085831960043}]}, {"text": "To obtain word level alignments, we run GIZA++) on all 28 pairings of the 8 languages.", "labels": [], "entities": [{"text": "word level alignments", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6228529512882233}]}, {"text": "Since we want each latent superlingual variable to span as many languages as possible, we aggregate the pairwise lexical alignments into larger sets of aligned words.", "labels": [], "entities": []}, {"text": "These sets of aligned words are generated as a preprocessing step.", "labels": [], "entities": []}, {"text": "During sampling they remain fixed and are treated as observed data.", "labels": [], "entities": []}, {"text": "We use the set of 14 basic part-of-speech tags provided by the corpus.", "labels": [], "entities": []}, {"text": "In our first experiment, we assume that a complete tag lexicon is available, so that for each word, its set of possible parts-of-speech is known ahead of time.", "labels": [], "entities": []}, {"text": "In this setting, the average number of possible tags per token is 1.39.", "labels": [], "entities": []}, {"text": "We also experimented with incomplete tag dictionaries, where entries are only available for words appearing more than five or ten times in the corpus.", "labels": [], "entities": []}, {"text": "For other words, the entire tagset of 14 tags is considered.", "labels": [], "entities": []}, {"text": "In these two scenarios, the average per-token tag ambi-: Tagging accuracy for Bulgarian, Czech, English, Estonian, Hungarian, Romanian, Slovene, and Serbian.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9855988025665283}]}, {"text": "In the first scenario, a complete tag lexicon is available for all the words.", "labels": [], "entities": []}, {"text": "In the other two scenarios the tag lexicon only includes words that appear more than five or ten times.", "labels": [], "entities": []}, {"text": "Results are given fora monolingual Bayesian HMM), a bilingual model (, and the multilingual model presented here.", "labels": [], "entities": []}, {"text": "In the case of the bilingual model, we present both the average accuracy overall pairings as well as the result from the best performing pairing for each language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9990274906158447}]}, {"text": "The best results for each language in each scenario are given in boldface.", "labels": [], "entities": []}, {"text": "guity is 4.65 and 5.58, respectively.", "labels": [], "entities": [{"text": "guity", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9861414432525635}]}, {"text": "Training and testing In the full lexicon experiment, each word is initialized with a random part-of-speech tag from its dictionary entry.", "labels": [], "entities": []}, {"text": "In the two reduced lexicon experiments, we initialize the tags with the result of our monolingual baseline (see below) to reduce sampling time.", "labels": [], "entities": []}, {"text": "In both cases, we begin with 14 superlingual tag values -corresponding to the parts-of-speech -and initially assign them based on the most common initial part-ofspeech of words in each alignment.", "labels": [], "entities": []}, {"text": "We run our Gibbs sampler for 1,000 iterations, and store the conditional tag probabilities for the last 100 iterations.", "labels": [], "entities": []}, {"text": "We then approximate marginal tag probabilities on the training data using Equation 4 and predict the highest probability tags.", "labels": [], "entities": []}, {"text": "Finally, we compute maximum likelihood transition and emission probabilities using these tag counts, and then apply smoothed viterbi decoding to each held-out monolingual test set.", "labels": [], "entities": [{"text": "maximum likelihood transition and emission probabilities", "start_pos": 20, "end_pos": 76, "type": "METRIC", "confidence": 0.7463014225165049}]}, {"text": "All reported results are averaged over five runs of the sampler.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tagging accuracy for Bulgarian, Czech, English, Estonian, Hungarian, Romanian, Slovene, and Serbian. In  the first scenario, a complete tag lexicon is available for all the words. In the other two scenarios the tag lexicon  only includes words that appear more than five or ten times. Results are given for a monolingual Bayesian HMM", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9902713894844055}]}]}