{"title": [{"text": "Anchored Speech Recognition for Question Answering", "labels": [], "entities": [{"text": "Anchored Speech Recognition", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7162751952807108}, {"text": "Question Answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.790229320526123}]}], "abstractContent": [{"text": "In this paper, we propose a novel question answering system that searches for responses from spoken documents such as broadcast news stories and conversations.", "labels": [], "entities": [{"text": "question answering", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7867201566696167}]}, {"text": "We propose a novel two-step approach, which we refer to as anchored speech recognition, to improve the speech recognition of the sentence that supports the answer.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7425379157066345}]}, {"text": "In the first step, the sentence that is highly likely to contain the answer is retrieved among the spoken data that has been transcribed using a generic automatic speech recognition (ASR) system.", "labels": [], "entities": [{"text": "generic automatic speech recognition (ASR)", "start_pos": 145, "end_pos": 187, "type": "TASK", "confidence": 0.7497070516858783}]}, {"text": "This candidate sentence is then re-recognized in the second step by constraining the ASR search space using the lexical information in the question.", "labels": [], "entities": [{"text": "ASR search", "start_pos": 85, "end_pos": 95, "type": "TASK", "confidence": 0.8045051693916321}]}, {"text": "Our analysis showed that ASR errors caused a 35% degradation in the performance of the question answering system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9647590517997742}, {"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8045377731323242}]}, {"text": "Experiments with the proposed anchored recognition approach indicated a significant improvement in the performance of the question answering module, recovering 30% of the answers erroneous due to ASR.", "labels": [], "entities": [{"text": "anchored recognition", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7452180683612823}, {"text": "question answering module", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.834922174612681}, {"text": "ASR", "start_pos": 196, "end_pos": 199, "type": "TASK", "confidence": 0.9031514525413513}]}], "introductionContent": [{"text": "In this paper, we focus on finding answers to user questions from spoken documents, such as broadcast news stories and conversations.", "labels": [], "entities": []}, {"text": "Ina typical question answering system, the user query is first processed by an information retrieval (IR) system, which finds out the most relevant documents among massive document collections.", "labels": [], "entities": [{"text": "question answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7398113310337067}, {"text": "information retrieval (IR)", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.6939098119735718}]}, {"text": "Each sentence in these relevant documents is processed to determine whether or not it answers user questions.", "labels": [], "entities": []}, {"text": "Once a candidate sentence is determined, it is further processed to extract the exact answer.", "labels": [], "entities": []}, {"text": "Answering factoid questions (i.e., questions like \"What is the capital of France?\") using web makes use of the redundancy of information).", "labels": [], "entities": []}, {"text": "However, when the document collection is not large and when the queries are complex, as in the task we focus on in this paper, more sophisticated syntactic, semantic, and contextual processing of documents and queries is performed to extractor construct the answer.", "labels": [], "entities": []}, {"text": "Although much of the work on question answering has been focused on written texts, many emerging systems also enable either spoken queries or spoken document collections ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8953575789928436}]}, {"text": "The work we describe in this paper also uses spoken data collections to answer user questions but our focus is on improving speech recognition quality of the documents by making use of the wording in the queries.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.6781279146671295}]}, {"text": "Consider the following example: Manual transcription: We understand from Greek officials here that it was a Russian-made rocket which is available in many countries but certainly not a weapon used by the Greek military ASR transcription: to stand firm greek officials here that he was a a russian made rocket uh which is available in many countries but certainly not a weapon used by he great moments Question: What is certainly not a weapon used by the Greek military?", "labels": [], "entities": [{"text": "ASR", "start_pos": 219, "end_pos": 222, "type": "TASK", "confidence": 0.8225401043891907}]}, {"text": "Answer: a Russian-made rocket Answering such questions requires as good ASR transcriptions as possible.", "labels": [], "entities": [{"text": "Answering", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.8220602869987488}, {"text": "ASR transcriptions", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8027978539466858}]}, {"text": "In many cases, though, there is one generic ASR system and a generic language model to use.", "labels": [], "entities": [{"text": "ASR", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9743194580078125}]}, {"text": "The approach proposed in this paper attempts to improve the ASR performance by re-recognizing the candidate sentence using lexical information from the given question.", "labels": [], "entities": [{"text": "ASR", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9940162897109985}]}, {"text": "The motiva-tion is that the question and the candidate sentence should share some common words, and therefore the words of the answer sentence can be estimated from the given question.", "labels": [], "entities": []}, {"text": "For example, given a factoid question such as: \"What is the tallest building in the world?\", the sentence containing its answer is highly likely to include word sequences such as: \"The tallest building in the world is NAME\" or \"NAME, the highest building in the world, ...\", where NAME is the exact answer.", "labels": [], "entities": []}, {"text": "Once the sentence supporting the answer is located, it is re-recognized such that the candidate answer is constrained to include parts of the question word sequence.", "labels": [], "entities": []}, {"text": "To achieve this, a word network is formed to match the answer sentence to the given question.", "labels": [], "entities": []}, {"text": "Since the question words are taken as a basis to re-recognize the best-candidate sentence, the question acts as an anchor, and therefore, we call this approach anchored recognition.", "labels": [], "entities": [{"text": "anchored recognition", "start_pos": 160, "end_pos": 180, "type": "TASK", "confidence": 0.7616755366325378}]}, {"text": "In this work, we restrict out attention to questions about the subject, the object and the locative, temporal, and causative arguments.", "labels": [], "entities": []}, {"text": "For instance, the followings are the questions of interest for the sentence Obama invited Clinton to the White House to discuss the recent developments: Who invited Clinton to the White House?", "labels": [], "entities": []}, {"text": "Who did Obama invite to the White House?", "labels": [], "entities": []}, {"text": "Why did Obama invite Clinton to the White House?", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments using a set of questions and broadcast audio documents released by LDC for the DARPA-funded GALE project Phase 3.", "labels": [], "entities": [{"text": "LDC", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9208824038505554}, {"text": "GALE project Phase 3", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.5619051158428192}]}, {"text": "In this dataset we have 482 questions (177 subject, 160 object, 73 temporal argument, 49 locative argument, and 23 reason) from 90 documents.", "labels": [], "entities": []}, {"text": "The ASR word error rate (WER) for the sentences from which the questions are constructed is 37% with respect to noisy closed captions.", "labels": [], "entities": [{"text": "ASR word error rate (WER)", "start_pos": 4, "end_pos": 29, "type": "METRIC", "confidence": 0.8484664644513812}]}, {"text": "To factor out IR noise we assumed that the target document is given.", "labels": [], "entities": [{"text": "IR", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9511427283287048}]}, {"text": "presents the performance of the sentence extraction system using manual and automatic transcriptions.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7638584673404694}]}, {"text": "As seen, the system is almost perfect when there is no noise, however performance degrades about 12% with the ASR output.", "labels": [], "entities": [{"text": "ASR", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.6354289054870605}]}, {"text": "The next set of experiments demonstrate the performance of the answer extraction system when the correct sentence is given using both automatic and manual transcriptions.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.8410594463348389}]}, {"text": "As seen from, the answer extraction performance degrades by about 35% relative using the ASR output.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8815346658229828}, {"text": "ASR", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.7719777822494507}]}, {"text": "However, using the anchored recognition approach, this improves to 23%, reducing the effect of the ASR noise significantly 1 by more than 30% relative.", "labels": [], "entities": [{"text": "anchored recognition", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.6872998476028442}, {"text": "ASR", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.8297240734100342}]}, {"text": "This is shown in the last column of this table, demonstrating the use of the proposed approach.", "labels": [], "entities": []}, {"text": "We observe that the WER of the sentences for which we now get corrected answers is reduced from 45% to 28% with this approach, a reduction of 37% relative.", "labels": [], "entities": [{"text": "WER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9985052347183228}]}], "tableCaptions": [{"text": " Table 1: Performance figures for the sentence extraction  system using automatic and manual transcriptions.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8423561453819275}]}, {"text": " Table 2: Performance figures for the answer extraction  system using automatic and manual transcriptions com- pared with anchored recognition outputs.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.9374428391456604}]}]}