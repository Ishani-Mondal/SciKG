{"title": [{"text": "Estimating and Exploiting the Entropy of Sense Distributions", "labels": [], "entities": []}], "abstractContent": [{"text": "Word sense distributions are usually skewed.", "labels": [], "entities": []}, {"text": "Predicting the extent of the skew can help a word sense disambiguation (WSD) system determine whether to consider evidence from the local context or apply the simple yet effective heuristic of using the first (most frequent) sense.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.7881009727716446}]}, {"text": "In this paper, we propose a method to estimate the entropy of a sense distribution to boost the precision of a first sense heuristic by restricting its application to words with lower entropy.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9991030693054199}]}, {"text": "We show on two standard datasets that automatic prediction of entropy can increase the performance of an automatic first sense heuristic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense distributions are typically skewed and WSD systems do best when they exploit this tendency.", "labels": [], "entities": []}, {"text": "This is usually done by estimating the most frequent sense (MFS) for each word from a training corpus and using that sense as a back-off strategy fora word when there is no convincing evidence from the context.", "labels": [], "entities": [{"text": "frequent sense (MFS)", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.8192386269569397}]}, {"text": "This is known as the MFS heuristic 1 and is very powerful since sense distributions are usually skewed.", "labels": [], "entities": []}, {"text": "The heuristic becomes particularly hard to beat for words with highly skewed sense distributions ().", "labels": [], "entities": []}, {"text": "Although the MFS can be estimated from tagged corpora, there are always cases where there is insufficient data, or where the data is inappropriate, for example because it comes from a very different domain.", "labels": [], "entities": [{"text": "MFS", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9032085537910461}]}, {"text": "This has motivated some recent work attempting to estimate the distributions automatically (.", "labels": [], "entities": []}, {"text": "This paper examines the case for determining the skew of a word sense distribution by estimating entropy and then using this to increase the precision of an unsupervised first sense heuristic by restricting application to those words where the system can automatically detect that it has the most chance.", "labels": [], "entities": [{"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9990795850753784}]}, {"text": "We use a method based on that proposed by as this approach does not require hand-labelled corpora.", "labels": [], "entities": []}, {"text": "The method could easily be adapted to other methods for predicing predominant sense.", "labels": [], "entities": [{"text": "predicing predominant sense", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.8184158404668173}]}], "datasetContent": [{"text": "We conducted two experiments to evaluate the benefit of using our estimate of entropy to restrict application of the MFS heuristic.", "labels": [], "entities": [{"text": "MFS heuristic", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.7516650855541229}]}, {"text": "The two experiments are conducted on the polysemous nouns in SemCor and the nouns in the SENSEVAL-2 English all words task (we will refer to this as SE2-EAW).", "labels": [], "entities": [{"text": "SENSEVAL-2 English all words task", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.5767333626747131}]}, {"text": "The SE2-EAW task provides a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II ( gives the results.", "labels": [], "entities": [{"text": "Penn Treebank II", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.9907984137535095}]}, {"text": "The most frequent sense (MFS) from SE2-EAW itself provides the upper-bound (UB).", "labels": [], "entities": [{"text": "upper-bound (UB)", "start_pos": 63, "end_pos": 79, "type": "METRIC", "confidence": 0.8466019332408905}]}, {"text": "We also compare performance with the Semcor MFS (SC).", "labels": [], "entities": []}, {"text": "Performance is close to the Semcor MFS while not relying on any manual tagging.", "labels": [], "entities": [{"text": "Semcor MFS", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.8301282823085785}]}, {"text": "As before, precision increases significantly for words with low estimated entropy, and the gains over the random baseline are higher compared to the gains including all words.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9994774460792542}]}], "tableCaptions": [{"text": " Table 1: First sense heuristic on SemCor", "labels": [], "entities": [{"text": "SemCor", "start_pos": 35, "end_pos": 41, "type": "TASK", "confidence": 0.7429894804954529}]}, {"text": " Table 2: Precision (P) of equation 1 on SemCor with re- spect to frequency and polysemy", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9220454096794128}]}, {"text": " Table 3: First sense heuristic on SE2-EAW", "labels": [], "entities": [{"text": "SE2-EAW", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.7113050222396851}]}]}