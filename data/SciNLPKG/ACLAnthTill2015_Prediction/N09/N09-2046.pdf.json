{"title": [{"text": "Improving SCL Model for Sentiment-Transfer Learning", "labels": [], "entities": [{"text": "Improving SCL", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8271515667438507}, {"text": "Sentiment-Transfer Learning", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9452747702598572}]}], "abstractContent": [{"text": "In recent years, Structural Correspondence Learning (SCL) is becoming one of the most promising techniques for sentiment-transfer learning.", "labels": [], "entities": [{"text": "Structural Correspondence Learning (SCL)", "start_pos": 17, "end_pos": 57, "type": "TASK", "confidence": 0.7457406421502432}, {"text": "sentiment-transfer learning", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.9382498562335968}]}, {"text": "However, SCL model treats each feature as well as each instance by an equivalent-weight strategy.", "labels": [], "entities": []}, {"text": "To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances.", "labels": [], "entities": []}, {"text": "More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature.", "labels": [], "entities": []}, {"text": "The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the community of sentiment analysis, transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9425386488437653}]}, {"text": "Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and soon ().", "labels": [], "entities": [{"text": "classifiers adaptation", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.8589484691619873}, {"text": "generalizable features detection", "start_pos": 116, "end_pos": 148, "type": "TASK", "confidence": 0.6080214083194733}]}, {"text": "Among these techniques, SCL (Structural Correspondence Learning)) is regarded as a promising method to tackle transfer-learning problem.", "labels": [], "entities": []}, {"text": "The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features).", "labels": [], "entities": []}, {"text": "Pivot features behave similarly in both domains.", "labels": [], "entities": []}, {"text": "If non-pivot features from different domains are correlated with many of the same pivot features, then we assume them to be corresponded with each other, and treat them similarly when training a sentiment classifier.", "labels": [], "entities": []}, {"text": "However, SCL model treats each feature as well as each instance by an equivalent-weight strategy.", "labels": [], "entities": []}, {"text": "From the perspective of feature, this strategy fails to overcome the adverse influence of highfrequency domain-specific (HFDS) features.", "labels": [], "entities": []}, {"text": "For example, the words \"stock\" or \"market\" occurs frequently inmost of stock reviews, so these nonsentiment features tend to have a strong correspondence with pivot features.", "labels": [], "entities": []}, {"text": "As a result, the representative ability of the other sentiment features will inevitably be weakened to some degree.", "labels": [], "entities": []}, {"text": "To address this issue, we proposed Frequently Exclusively-occurring Entropy (FEE) to pick out HFDS features, and proposed a feature-weighted SCL model (FW-SCL) to adjust the influence of HFDS features in building correspondence.", "labels": [], "entities": [{"text": "Frequently Exclusively-occurring Entropy (FEE)", "start_pos": 35, "end_pos": 81, "type": "METRIC", "confidence": 0.9427036941051483}]}, {"text": "The main idea of FW-SCL is to assign a smaller weight to HFDS features so that the adverse influence of HFDS features can be decreased.", "labels": [], "entities": [{"text": "FW-SCL", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.859687864780426}]}, {"text": "From the other perspective, the equivalentweight strategy of SCL model ignores the labels (\"positive\" or \"negative\") of labeled instances.", "labels": [], "entities": []}, {"text": "Obviously, this is not a good idea.", "labels": [], "entities": []}, {"text": "In fact, positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than that built on negative instances; and vice versa.", "labels": [], "entities": []}, {"text": "Consequently, utilization of labels of instances and pivot features can decrease the adverse influence of some co-occurrences, such as co-occurrences involved with positive pivot features and negative instances, or involved with negative pivot features and positive instances.", "labels": [], "entities": []}, {"text": "In order to take into account the labels of labeled instances, we proposed an instanceweighted SCL model (IW-SCL), which assigns a larger weight to instances with the same label as the involved pivot feature.", "labels": [], "entities": []}, {"text": "In this time, we obtain a combined model: feature-weighted and instanceweighted SCL model (FWIW-SCL).", "labels": [], "entities": [{"text": "FWIW-SCL", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.5040920376777649}]}, {"text": "For the sake of convenience, we simplify \"FWIW-SCL\" as \"W-SCL\" in the rest of this paper.", "labels": [], "entities": [{"text": "FWIW-SCL", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.5435499548912048}]}], "datasetContent": [{"text": "We collected three Chinese domain-specific datasets: Education Reviews (Edu, from http://blog.sohu.com/learning/), Stock Reviews (Sto, from http://blog.sohu.com/stock/) and Computer Reviews (Comp, from http://detail.zol.com.cn/).", "labels": [], "entities": []}, {"text": "All of these datasets are annotated by three linguists.", "labels": [], "entities": []}, {"text": "We use ICTCLAS (a Chinese text POS tool, http://ictclas.org/) to parse Chinese words.", "labels": [], "entities": [{"text": "parse Chinese words", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8340495030085245}]}, {"text": "The dataset Edu includes 1,012 negative reviews and 254 positive reviews.", "labels": [], "entities": [{"text": "Edu", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.8391951322555542}]}, {"text": "The average size of reviews is about 600 words.", "labels": [], "entities": []}, {"text": "The dataset Sto consists of 683 negative reviews and 364 positive reviews.", "labels": [], "entities": [{"text": "Sto", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.7450499534606934}]}, {"text": "The average length of reviews is about 460 terms.", "labels": [], "entities": []}, {"text": "The dataset Comp contains 390 negative reviews and 544 positive reviews.", "labels": [], "entities": [{"text": "Comp", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.49433472752571106}]}, {"text": "The average length of reviews is about 120 words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A simple example for FEE", "labels": [], "entities": [{"text": "FEE", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.42669230699539185}]}, {"text": " Table 2: Accuracy of different methods", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99860018491745}]}]}