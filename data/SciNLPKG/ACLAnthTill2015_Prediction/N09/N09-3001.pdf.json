{"title": [{"text": "Classifier Combination Techniques Applied to Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.9631950557231903}]}], "abstractContent": [{"text": "This paper examines the applicability of clas-sifier combination approaches such as bagging and boosting for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.957000732421875}]}, {"text": "To the best of our knowledge, this is the first effort that utilizes such techniques for corefer-ence resolution.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.9049781262874603}]}, {"text": "In this paper, we provide experimental evidence which indicates that the accuracy of the coreference engine can potentially be increased by use of bagging and boosting methods, without any additional features or training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9993212223052979}]}, {"text": "We implement and evaluate combination techniques at the mention, entity and document level, and also address issues like entity alignment, that are specific to coreference resolution.", "labels": [], "entities": [{"text": "entity alignment", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.7160568535327911}, {"text": "coreference resolution", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.9627392292022705}]}], "introductionContent": [{"text": "Coreference resolution is the task of partitioning a set of mentions (i.e. person, organization and location) into entities.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9244304597377777}, {"text": "partitioning a set of mentions (i.e. person, organization and location)", "start_pos": 38, "end_pos": 109, "type": "TASK", "confidence": 0.6678502055314871}]}, {"text": "A mention is an instance of textual reference to an object, which can be either named (e.g. Barack Obama), nominal (e.g. the president) or pronominal (e.g. he, his, it).", "labels": [], "entities": []}, {"text": "An entity is an aggregate of all the mentions (of any level) which refer to one conceptual entity.", "labels": [], "entities": []}, {"text": "For example, in the following sentence: John said Mary was his sister.", "labels": [], "entities": [{"text": "John said Mary was his sister", "start_pos": 40, "end_pos": 69, "type": "DATASET", "confidence": 0.8801884651184082}]}, {"text": "there are four mentions: John, Mary, his, and sister.", "labels": [], "entities": []}, {"text": "John and his belong to the one entity since they refer to the same person; Mary and sister both refer to another person entity.", "labels": [], "entities": []}, {"text": "Furthermore, John and Mary are named mentions, sister is a nominal mention and his is a pronominal mention.", "labels": [], "entities": []}, {"text": "In this paper, we present a potential approach for improving the performance of coreference resolution by using classifier combination techniques such as bagging and boosting.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.9662995636463165}]}, {"text": "To the best of our knowledge, this is the first effort that utilizes classifier combination for improving coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.9601881206035614}]}, {"text": "Combination methods have been applied to many problems in natural-language processing (NLP).", "labels": [], "entities": [{"text": "natural-language processing (NLP)", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.7287094593048096}]}, {"text": "Examples include the ROVER system for speech recognition, the Multi-Engine Machine Translation (MEMT) system (Jayaraman and, and part-of-speech tagging ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.8081235587596893}, {"text": "Multi-Engine Machine Translation (MEMT)", "start_pos": 62, "end_pos": 101, "type": "TASK", "confidence": 0.754032701253891}, {"text": "part-of-speech tagging", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.8080564439296722}]}, {"text": "Most of these techniques have shown a considerable improvement over the performance of a single classifier and, therefore, lead us to consider implementing such a multipleclassifier system for coreference resolution as well.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 193, "end_pos": 215, "type": "TASK", "confidence": 0.9605194330215454}]}, {"text": "Using classifier combination techniques one can potentially achieve a classification accuracy that is superior to that of the single best classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9071244597434998}]}, {"text": "This is based on the assumption that the errors made by each of the classifiers are not identical, and therefore if we intelligently combine multiple classifier outputs, we maybe able to correct some of these errors.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: \u2022 Demonstrating the potential for improvement in the baseline -By implementing a system that behaves like an oracle, we have shown that the output of the combination of multiple classifiers has the potential to be significantly higher inaccuracy than any of the individual classifiers.", "labels": [], "entities": []}, {"text": "\u2022 Adapting traditional bagging techniques -Multiple classifiers, generated using bagging techniques, were combined using an entity-level sum 1 rule and mention-level majority voting.", "labels": [], "entities": []}, {"text": "\u2022 Implementing a document-level boosting algorithm -A boosting algorithm was implemented in which a coreference resolution classifier was iteratively trained using a re-weighted training set, where the reweighting was done at the document level.", "labels": [], "entities": [{"text": "coreference resolution classifier", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.8556578954060873}]}, {"text": "\u2022 Addressing the problem of entity alignmentIn order to apply combination techniques to multiple classifiers, we need to address entityalignment issues, explained later in this paper.", "labels": [], "entities": [{"text": "entity alignmentIn", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7503319084644318}]}, {"text": "The baseline coreference system we use is similar to the one described by).", "labels": [], "entities": []}, {"text": "In such a system, mentions are processed sequentially, and at each step, a mention is either linked to one of existing entities, or used to create anew entity.", "labels": [], "entities": []}, {"text": "At the end of this process, each possible partition of the mentions corresponds to a unique sequence of link or creation actions, each of which is scored by a statistical model.", "labels": [], "entities": []}, {"text": "The one with the highest score is output as the final coreference result.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the general setup used to conduct the experiments and presents an evaluation of the combination techniques that were implemented.", "labels": [], "entities": []}, {"text": "The coreference resolution system used in our experiments makes use of a Maximum Entropy model which has lexical, syntactical, semantic and discourse features (Luo et al.,, which consists of 599 documents from rich and diversified sources.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9036622941493988}]}, {"text": "We reserve the last 16% documents of each source as the test set, and use the rest of the documents as the training set.", "labels": [], "entities": []}, {"text": "The ACE 2005 data split is tabulated in.", "labels": [], "entities": [{"text": "ACE 2005 data split", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9809515029191971}]}, {"text": "Bagging A total of 15 classifiers (C 1 to C 15 ) were generated, 12 of which were obtained by sampling the training set and the remaining 3 by sampling the feature set.", "labels": [], "entities": []}, {"text": "We also make use of the baseline classifier C 0 . The accuracy of C 0 to C 15 has been summarized in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.999285876750946}]}, {"text": "The agreement between the classifiers' output was found to be in the range of 93% to 95%.", "labels": [], "entities": []}, {"text": "In this paper, the metric used to compute the accuracy of the coreference resolution is the Constrained Entity-Alignment F-Measure (CEAF) () with the entity-pair similarity measure in Equation 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9991024732589722}, {"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.9500767886638641}, {"text": "Constrained Entity-Alignment F-Measure (CEAF)", "start_pos": 92, "end_pos": 137, "type": "METRIC", "confidence": 0.7746838430563608}]}, {"text": "To conduct the oracle experiment, we train 1 to 15 classifiers and align their output to the gold standard.", "labels": [], "entities": []}, {"text": "For all entities aligned with a gold entity, we pick the one with the highest score as the output.", "labels": [], "entities": []}, {"text": "We measure the performance for varying number of classifiers, and the result is plotted in.", "labels": [], "entities": []}, {"text": "First, we observe a steady and significant increase in CEAF for every additional classifier, because additional classifiers can only improve the alignment score.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9880369901657104}, {"text": "alignment score", "start_pos": 145, "end_pos": 160, "type": "METRIC", "confidence": 0.9470539689064026}]}, {"text": "Second, we note that the oracle accuracy is 87.58% fora single input classifier C 1 , i.e. an absolute gain of 9% compared to C 0 . This is because the availability of gold entities makes it possible to remove many false-alarm entities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9841337203979492}]}, {"text": "Finally, the oracle accuracy when all 15 classifiers are used as input is 94.59%, a 16.06% absolute improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9912189841270447}]}, {"text": "This experiment helps us to understand the performance bound of combining multiple classifiers and the contribution of every additional classifier.", "labels": [], "entities": []}, {"text": "how much performance gain can be attained if the gold standard is not available.", "labels": [], "entities": []}, {"text": "To answer this question, we replace the gold standard with one of the classifiers C 1 to C 15 , and align the classifiers.", "labels": [], "entities": []}, {"text": "This is done in around robin fashion as described in Section 2.3.", "labels": [], "entities": []}, {"text": "The best performance of this procedure is 77.93%.", "labels": [], "entities": []}, {"text": "The sum-rule combination output had an accuracy of 78.65% with a slightly different baseline of 78.81%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9997140765190125}, {"text": "baseline", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9472121000289917}]}, {"text": "These techniques do not yield a statistically significant increase in CEAF but this is not surprising as C 1 to C 15 are highly correlated.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9018077254295349}]}, {"text": "This experiment is conducted to evaluate the mention-level majority voting technique.", "labels": [], "entities": [{"text": "mention-level majority voting", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6161838372548422}]}, {"text": "The results are not statistically better than the baseline, but they give us valuable insight into the working of the combination technique.", "labels": [], "entities": []}, {"text": "The example in shows a single entity-alignment level for the baseline C 0 and 3 classifiers C 1 , C 2 , and C 3 and the combination output by mention-level majority voting.", "labels": [], "entities": []}, {"text": "The mentions are denoted by the notation 'EntityID -MentionID', for example 7-10 is the mention with EntityID=7 and MentionID=10.", "labels": [], "entities": []}, {"text": "Here, we use the EntityID in the gold file.", "labels": [], "entities": [{"text": "EntityID", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8848563432693481}]}, {"text": "The mentions with EntityID=7 are \"correct\" i.e. they belong in this entity, and the others are \"wrong\" i.e. they do not belong in this entity.", "labels": [], "entities": []}, {"text": "The aligned mentions are of four types: \u2022 Type I mentions -These mentions have a highest voting count of 2 or more at the same entity-level alignment and hence appear in the output.", "labels": [], "entities": []}, {"text": "\u2022 Type II mentions -These mentions have a highest voting count of 1.", "labels": [], "entities": []}, {"text": "But they are present in more than one input classifier and there is a tie between the mention counts at different entitylevel alignments.", "labels": [], "entities": []}, {"text": "The rule to break the tie is that mentions are included if they are also seen in the full system C 0 . As can been seen, this rule brings incorrect mentions such as 7-61, 7-63, 7-64, but it also admits In the oracle, the gold standard helps to remove entities with false-alarm mentions, whereas the full system output is noisy and it is not strong enough to reliably remove undesired mentions.", "labels": [], "entities": []}, {"text": "\u2022 Type III mentions -There is only one mention 20-66 which is of this type.", "labels": [], "entities": []}, {"text": "It is selected in the combination output since it is present in C 2 and the baseline C 0 , although it has been rejected as a false-alarm in C 1 and C 3 . \u2022 Type IV mentions -These false-alarm mentions (relative to C 0 ) are rejected in the output.", "labels": [], "entities": []}, {"text": "As can be seen, this correctly rejects mentions such as 15-22 and 20-68, but it also rejects correct mentions 7-18, 7-19 and 7-30.", "labels": [], "entities": []}, {"text": "In summary, the current implementation of this technique has a limited ability to distinguish correct mentions from wrong ones due to the noisy nature of C 0 which is used for alignment.", "labels": [], "entities": []}, {"text": "We also observe that mentions spread across different alignments often have low-count and they are often tied in count.", "labels": [], "entities": [{"text": "count", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9628468751907349}]}, {"text": "Therefore, it is important to set a minimum threshold for accepting these low-count majority votes and also investigate better tie-breaking techniques.", "labels": [], "entities": []}, {"text": "Document-level Boosting This experiment is conducted to evaluate the document-level boosting technique.", "labels": [], "entities": [{"text": "document-level boosting", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.6888771504163742}]}, {"text": "shows the results with the ratio of the number of training documents to the number of test documents equal to 80:20, F-measure threshold F thresh = 74% and percentile threshold P thresh = 25%.", "labels": [], "entities": [{"text": "F-measure threshold F thresh", "start_pos": 117, "end_pos": 145, "type": "METRIC", "confidence": 0.9652195870876312}, {"text": "percentile threshold P thresh", "start_pos": 156, "end_pos": 185, "type": "METRIC", "confidence": 0.8423104733228683}]}, {"text": "The accuracy increases by 0.7%, relative to the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997616410255432}]}, {"text": "Due to computational complexity considerations, we used fixed values for the parameters.", "labels": [], "entities": []}, {"text": "Therefore, these values maybe suboptimal and may not correspond to the best possible increase inaccuracy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of ACE 2005 data", "labels": [], "entities": [{"text": "ACE 2005 data", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.8905232747395834}]}, {"text": " Table 2: Accuracy of generated and baseline classifiers", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9952215552330017}]}, {"text": " Table 3: Results of document-level boosting", "labels": [], "entities": [{"text": "document-level boosting", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7521041333675385}]}]}