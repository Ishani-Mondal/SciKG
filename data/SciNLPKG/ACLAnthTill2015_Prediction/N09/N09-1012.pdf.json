{"title": [{"text": "Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing", "labels": [], "entities": [{"text": "Improving Unsupervised Dependency Parsing", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8831221610307693}, {"text": "Smoothing", "start_pos": 67, "end_pos": 76, "type": "TASK", "confidence": 0.768936038017273}]}], "abstractContent": [{"text": "Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.", "labels": [], "entities": []}, {"text": "Traditionally, the unsupervised models have been kept simple due to tractabil-ity and data sparsity concerns.", "labels": [], "entities": []}, {"text": "In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.", "labels": [], "entities": []}, {"text": "Our model produces state-of-the-art results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.687249556183815}]}], "introductionContent": [{"text": "The last decade has seen great strides in statistical natural language parsing.", "labels": [], "entities": [{"text": "statistical natural language parsing", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.734828270971775}]}, {"text": "Supervised and semisupervised methods now provide highly accurate parsers fora number of languages, but require training from corpora hand-annotated with parse trees.", "labels": [], "entities": []}, {"text": "Unfortunately, manually annotating corpora with parse trees is expensive and time consuming so for languages and domains with minimal resources it is valuable to study methods for parsing without requiring annotated sentences.", "labels": [], "entities": []}, {"text": "In this work, we focus on unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7402994930744171}]}, {"text": "Our goal is to produce a directed graph of dependency relations (e.g.) where each edge indicates a head-argument relation.", "labels": [], "entities": []}, {"text": "Since the task is unsupervised, we are not given any examples of correct dependency graphs and only take words and their parts of speech as input.", "labels": [], "entities": []}, {"text": "Most of the recent work in this area) has focused on variants of the The big dog barks Dependency Model with Valence (DMV) by.", "labels": [], "entities": []}, {"text": "DMV was the first unsupervised dependency grammar induction system to achieve accuracy above a right-branching baseline.", "labels": [], "entities": [{"text": "DMV", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9098398685455322}, {"text": "dependency grammar induction", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7062522371610006}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9980742931365967}]}, {"text": "However, DMV is notable to capture some of the more complex aspects of language.", "labels": [], "entities": []}, {"text": "Borrowing some ideas from the supervised parsing literature, we present two new models: Extended Valence Grammar (EVG) and its lexicalized extension (L-EVG).", "labels": [], "entities": [{"text": "supervised parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.5977298319339752}]}, {"text": "The primary difference between EVG and DMV is that DMV uses valence information to determine the number of arguments ahead takes but not their categories.", "labels": [], "entities": []}, {"text": "In contrast, EVG allows different distributions over arguments for different valence slots.", "labels": [], "entities": []}, {"text": "L-EVG extends EVG by conditioning on lexical information as well.", "labels": [], "entities": []}, {"text": "This allows L-EVG to potentially capture subcategorizations.", "labels": [], "entities": []}, {"text": "The downside of adding additional conditioning events is that we introduce data sparsity problems.", "labels": [], "entities": []}, {"text": "Incorporating more valence and lexical information increases the number of parameters to estimate.", "labels": [], "entities": []}, {"text": "A common solution to data sparsity in supervised parsing is to add smoothing.", "labels": [], "entities": [{"text": "supervised parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.4984274059534073}]}, {"text": "We show that smoothing can be employed in an unsupervised fashion as well, and show that mixing DMV, EVG, and L-EVG together produces state-ofthe-art results on this task.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.9624677300453186}]}, {"text": "To our knowledge, this is the first time that grammars with differing levels of detail have been successfully combined for unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7250482141971588}]}, {"text": "A brief overview of the paper follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss the relevant background.", "labels": [], "entities": []}, {"text": "Section 3 presents how we will extend DMV with additional features.", "labels": [], "entities": []}, {"text": "We describe smoothing in an unsupervised context in Section 4.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.9654349088668823}]}, {"text": "In Section 5, we discuss search issues.", "labels": [], "entities": []}, {"text": "We present our experiments in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Most likely arguments given valence and direc- tion, according to smoothing distribution P (arg|dir, val)  in EVG smoothed-skip-head model with lowest free en- ergy.", "labels": [], "entities": [{"text": "direc- tion", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9203315178553263}, {"text": "EVG", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.9147383570671082}]}]}