{"title": [{"text": "Domain Adaptation with Artificial Data for Semantic Parsing of Speech", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6728622317314148}, {"text": "Semantic Parsing of Speech", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.789943277835846}]}], "abstractContent": [{"text": "We adapt a semantic role parser to the domain of goal-directed speech by creating an artificial treebank from an existing text tree-bank.", "labels": [], "entities": []}, {"text": "We use a three-component model that includes distributional models from both target and source domains.", "labels": [], "entities": []}, {"text": "We show that we improve the parser's performance on utterances collected from human-machine dialogues by training on the artificially created data without loss of performance on the text treebank.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the quality of natural language parsing improves and the sophistication of natural language understanding applications increases, there are several domains where parsing, and especially semantic parsing, could be useful.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.665567954381307}, {"text": "semantic parsing", "start_pos": 189, "end_pos": 205, "type": "TASK", "confidence": 0.7305740863084793}]}, {"text": "This is particularly true in adaptive systems for spoken language understanding, where complex utterances need to be translated into shallow semantic representation, such as dialogue acts.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.7514864802360535}]}, {"text": "The domain on which we are working is goaldirected system-driven dialogues, where a system helps the user to fulfil a certain goal, e.g. booking a hotel room.", "labels": [], "entities": []}, {"text": "Typically, users respond with short answers to questions posed by the system.", "labels": [], "entities": []}, {"text": "For example In the South is an answer to the question Where would you like the hotel to be?", "labels": [], "entities": []}, {"text": "Parsing helps identifying the components (In the South is a PP) and semantic roles identify the PP as a locative, yielding the following slot-value pair for the dialogue act: area=South.", "labels": [], "entities": []}, {"text": "A PP such as in time is not identified as a locative, whereas keyword-spotting techniques as those currently used in dialogue systems may produce area=South and area=time indifferently.", "labels": [], "entities": []}, {"text": "Statistical syntactic and semantic parsers need treebanks.", "labels": [], "entities": []}, {"text": "Current available data is lacking in one or more respects: Syntactic/semantic treebanks are developed on text, while treebanks of speech corpora are not semantically annotated (e.g. Switchboard).", "labels": [], "entities": []}, {"text": "Moreover, the available human-human speech treebanks do not exhibit the same properties as the system-driven speech on which we are focusing, in particular in their proportion of non-sentential utterances (NSUs), utterances that are not full sentences.", "labels": [], "entities": []}, {"text": "Ina corpus study of a subset of the human-human dialogues in the BNC, found that only 9% of the total utterances are NSUs, whereas we find 44% in our system-driven data.", "labels": [], "entities": [{"text": "BNC", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9386303424835205}]}, {"text": "We illustrate a technique to adapt an existing semantic parser trained on merged Penn Treebank/PropBank data to goal-directed system-driven dialogue by artificial data generation.", "labels": [], "entities": [{"text": "Penn Treebank/PropBank data", "start_pos": 81, "end_pos": 108, "type": "DATASET", "confidence": 0.9626670241355896}]}, {"text": "Our main contribution lies in the framework used to generate artificial data for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7229045033454895}]}, {"text": "We mimic the distributions over parse structures in the target domain by combining the text treebank data and the artificially created NSUs, using a three-component model.", "labels": [], "entities": []}, {"text": "The first component is a hand-crafted model of NSUs.", "labels": [], "entities": [{"text": "NSUs", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.8228226900100708}]}, {"text": "The second component describes the distribution overfull sentences and types of NSUs as found in a minimally annotated subset of the target domain.", "labels": [], "entities": []}, {"text": "The third component describes the distribution over the internal parse structure of the generated data and is taken from the source domain.", "labels": [], "entities": []}, {"text": "Our approach differs from most approaches to domain adaptation, which require some training on fully annotated target data (), whereas we use minimally annotated target data only to help determine the distributions in the artificially created data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7845582962036133}]}, {"text": "It also differs from previ-ous work in domain adaptation by, where similar proportions of ungrammatical and grammatical data are combined to train a parser on ungrammatical written text, and by, who use interpolation between two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7290475964546204}]}, {"text": "Whereas much previous work on parsing speech has focused on speech repairs, e.g., we focus on parsing NSUs.", "labels": [], "entities": [{"text": "parsing speech", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.903514176607132}, {"text": "speech repairs", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7141325920820236}, {"text": "parsing NSUs", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.846124529838562}]}], "datasetContent": [{"text": "We trained three parsing models on both the original non-augmented merged Penn Treebank/Propbank corpus and the artificially generated augmented treebank including NSUs.", "labels": [], "entities": [{"text": "Penn Treebank/Propbank corpus", "start_pos": 74, "end_pos": 103, "type": "DATASET", "confidence": 0.9472294092178345}]}, {"text": "We ran a contrastive experiment to examine the usefulness of the threecomponent model by training two versions of the augmented model: One with and one without the target component.", "labels": [], "entities": []}, {"text": "These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors : the TownInfo test set.", "labels": [], "entities": [{"text": "TownInfo corpus", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.992168128490448}, {"text": "TownInfo test set", "start_pos": 202, "end_pos": 219, "type": "DATASET", "confidence": 0.9938262899716696}]}, {"text": "The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus.", "labels": [], "entities": [{"text": "Penn Treebank/Propbank corpus", "start_pos": 133, "end_pos": 162, "type": "DATASET", "confidence": 0.9496741652488708}]}, {"text": "We will refer to this test set as the non-augmented test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of types of NSUs and full sentences  in the TownInfo development set.", "labels": [], "entities": [{"text": "TownInfo development set", "start_pos": 67, "end_pos": 91, "type": "DATASET", "confidence": 0.972211996714274}]}, {"text": " Table 2: Recall, precision, and F-measure for the two test  sets, trained on non-augmented data and data augmented  with and without the target distribution component.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9899525046348572}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9995360374450684}, {"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995484948158264}]}]}