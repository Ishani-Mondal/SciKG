{"title": [], "abstractContent": [{"text": "The tree-transducer grammars that arise in current syntactic machine translation systems are large, flat, and highly lexicalized.", "labels": [], "entities": [{"text": "syntactic machine translation", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.7130951285362244}]}, {"text": "We address the problem of parsing efficiently with such grammars in three ways.", "labels": [], "entities": []}, {"text": "First, we present a pair of grammar transformations that admit an efficient cubic-time CKY-style parsing algorithm despite leaving most of the grammar in n-ary form.", "labels": [], "entities": [{"text": "CKY-style parsing", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.5980028212070465}]}, {"text": "Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices.", "labels": [], "entities": []}, {"text": "Finally, we describe a two-pass coarse-to-fine parsing approach that prunes the search space using predictions from a subset of the original grammar.", "labels": [], "entities": []}, {"text": "In all, parsing time reduces by 81%.", "labels": [], "entities": [{"text": "parsing", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9787084460258484}, {"text": "time", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.6068823337554932}]}, {"text": "We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time.", "labels": [], "entities": []}, {"text": "The resulting translations improve by 1.3 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9995059967041016}]}], "introductionContent": [{"text": "Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model.", "labels": [], "entities": [{"text": "syntactic machine translation", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.7107589145501455}]}, {"text": "Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking, local search (, and coarse-to-fine pruning (.", "labels": [], "entities": [{"text": "language model integration", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6494146585464478}]}, {"text": "Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars to general tree-to-string transducers () and have increased in size by including more synchronous tree fragments (;.", "labels": [], "entities": [{"text": "translation grammars", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9477778375148773}]}, {"text": "As a result of these trends, the syntactic component of machine translation decoding can now account fora substantial portion of total decoding time.", "labels": [], "entities": [{"text": "machine translation decoding", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.7950385610262553}]}, {"text": "In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat n-ary rules with many adjacent non-terminals, as in Figure 1.", "labels": [], "entities": [{"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9732879400253296}]}, {"text": "These grammars are sufficiently complex that the purely syntactic pass of our multi-pass decoder is the compute-time bottleneck under some conditions.", "labels": [], "entities": []}, {"text": "Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis.", "labels": [], "entities": [{"text": "MT grammars", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.9448722302913666}, {"text": "syntactic analysis", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.7455130815505981}]}, {"text": "There are several good reasons.", "labels": [], "entities": []}, {"text": "The most important is that MT grammars must do both analysis and generation.", "labels": [], "entities": [{"text": "MT grammars", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9661971926689148}]}, {"text": "To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized.", "labels": [], "entities": []}, {"text": "Second, syntax diverges between languages, and each divergence expands the minimal domain of translation rules, so rules are large and flat.", "labels": [], "entities": []}, {"text": "Finally, we see most rules very few times, so it is challenging to subcategorize non-terminals to the degree done in analytic parsing.", "labels": [], "entities": [{"text": "analytic parsing", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7789486348628998}]}, {"text": "This paper develops encodings, algorithms, and pruning strategies for such grammars.", "labels": [], "entities": []}, {"text": "We first investigate the qualitative properties of MT grammars, then present a sequence of parsing methods adapted to their broad characteristics.", "labels": [], "entities": [{"text": "MT grammars", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9575038552284241}]}, {"text": "We give normal forms which are more appropriate than Chomsky normal form, leaving the rules mostly flat.", "labels": [], "entities": []}, {"text": "We then describe a CKY-like algorithm which applies such rules efficiently, working directly over the n-ary forms in cubic time.", "labels": [], "entities": []}, {"text": "We show how thoughtful Figure 1: (a) A synchronous transducer rule has coindexed non-terminals on the source and target side.", "labels": [], "entities": []}, {"text": "Internal grammatical structure of the target side has been omitted.", "labels": [], "entities": []}, {"text": "(b) The source-side projection of the rule is a monolingual source-language rule with target-side grammar symbols.", "labels": [], "entities": []}, {"text": "(c) A training sentence pair is annotated with a target-side parse tree and a word alignment, which license this rule to be extracted.", "labels": [], "entities": []}, {"text": "binarization can further increase parsing speed, and we present anew coarse-to-fine scheme that uses rule subsets rather than symbol clustering to build a coarse grammar projection.", "labels": [], "entities": [{"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9695586562156677}]}, {"text": "These techniques reduce parsing time by 81% in aggregate.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9805681109428406}]}, {"text": "Finally, we demonstrate that we can accelerate forest-based reranking with a language model by pruning with information from the parsing pass.", "labels": [], "entities": []}, {"text": "This approach enables a 100-fold increase in maximum beam size, improving translation quality by 1.3 BLEU while decreasing total decoding time.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9984563589096069}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Coarse-to-fine pruning speeds up parsing time  with minimal effect on either model score or translation  quality. The coarse grammar built using symbol subsets  outperforms clustering grammar symbols, reducing pars- ing time by 52%. These experiments do not include a  language model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9689472913742065}]}, {"text": " Table 3: Time in minutes and performance for 300 sentences. We used a trigram language model trained on 220  million words of English text. The no pruning baseline used a fix beam size for forest-based language model reranking.", "labels": [], "entities": []}]}