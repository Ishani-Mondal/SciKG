{"title": [{"text": "A Discriminative Latent Variable Chinese Segmenter with Hybrid Word/Character Information", "labels": [], "entities": [{"text": "Discriminative Latent Variable Chinese Segmenter", "start_pos": 2, "end_pos": 50, "type": "TASK", "confidence": 0.5946966707706451}]}], "abstractContent": [{"text": "Conventional approaches to Chinese word segmentation treat the problem as a character-based tagging task.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6219275494416555}, {"text": "character-based tagging task", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.6787578463554382}]}, {"text": "Recently, semi-Markov models have been applied to the problem, incorporating features based on complete words.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative, a latent variable model, which uses hybrid information based on both word sequences and character sequences.", "labels": [], "entities": []}, {"text": "We argue that the use of latent variables can help capture long range dependencies and improve the recall on segmenting long words, e.g., named-entities.", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9993960857391357}]}, {"text": "Experimental results show that this is indeed the case.", "labels": [], "entities": []}, {"text": "With this improvement, evaluations on the data of the second SIGHAN CWS bakeoff show that our system is competitive with the best ones in the literature.", "labels": [], "entities": [{"text": "SIGHAN CWS bakeoff", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.6856007973353068}]}], "introductionContent": [{"text": "For most natural language processing tasks, words are the basic units to process.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.7439511567354202}]}, {"text": "Since Chinese sentences are written as continuous sequences of characters, segmenting a character sequence into a word sequence is the first step for most Chinese processing applications.", "labels": [], "entities": []}, {"text": "In this paper, we study the problem of Chinese word segmentation (CWS), which aims to find these basic units (words 1 ) fora given sentence in Chinese.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7809784412384033}]}, {"text": "Chinese character sequences are normally ambiguous, and out-of-vocabulary (OOV) words area major source of the ambiguity.", "labels": [], "entities": []}, {"text": "Typical examples of OOV words include named entities (e.g., organization names, person names, and location names).", "labels": [], "entities": []}, {"text": "Those named entities maybe very long, and a difficult case occurs when along word W (|W | \u2265 4) consists of some words which can be separate words on their own; in such cases an automatic segmenter may split the OOV word into individual words.", "labels": [], "entities": []}, {"text": "For example, (Computer Committee of International Federation of Automatic Control) is one of the organization names in the Microsoft Research corpus.", "labels": [], "entities": [{"text": "Microsoft Research corpus", "start_pos": 123, "end_pos": 148, "type": "DATASET", "confidence": 0.9271714886029562}]}, {"text": "Its length is 13 and it contains more than 6 individual words, but it should be treated as a single word.", "labels": [], "entities": []}, {"text": "Proper recognition of long OOV words are meaningful not only for word segmentation, but also fora variety of other purposes, e.g., full-text indexing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7317555844783783}, {"text": "full-text indexing", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.7223659157752991}]}, {"text": "However, as is illustrated, recognizing long words (without sacrificing the performance on short words) is challenging.", "labels": [], "entities": []}, {"text": "Conventional approaches to Chinese word segmentation treat the problem as a character-based la-beling task.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6355492969353994}]}, {"text": "Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Label i = B), middle or end of a multi-character word (Label i = C).", "labels": [], "entities": []}, {"text": "A popular discriminative model that have been used for this task is the conditional random fields (CRFs), starting with the model of.", "labels": [], "entities": []}, {"text": "In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff)), two of the highest scoring systems in the closed track competition were based on a CRF model (.", "labels": [], "entities": [{"text": "International Chinese Word Segmentation Bakeoff", "start_pos": 14, "end_pos": 61, "type": "TASK", "confidence": 0.7311072468757629}]}, {"text": "While the CRF model is quite effective compared with other models designed for CWS, it maybe limited by its restrictive independence assumptions on non-adjacent labels.", "labels": [], "entities": []}, {"text": "Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order).", "labels": [], "entities": []}, {"text": "To address this difficulty, a choice is to relax the Markov assumption by using the semi-Markov conditional random field model (semi-CRF) ().", "labels": [], "entities": []}, {"text": "Despite the theoretical advantage of semi-CRFs over CRFs, however, some previous studies) exploring the use of a semi-CRF for Chinese word segmentation did not find significant gains over the CRF ones.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.6318588455518087}]}, {"text": "As discussed in, the reason maybe that despite the greater representational power of the semi-CRF, there are some valuable features that could be more naturally expressed in a character-based labeling model.", "labels": [], "entities": []}, {"text": "For example, on a CRF model, one might use the feature \"the current character xi is X and the current label Label i is C\".", "labels": [], "entities": []}, {"text": "This feature maybe helpful in CWS for generalizing to new words.", "labels": [], "entities": [{"text": "CWS", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9336041808128357}, {"text": "generalizing to new words", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8632416278123856}]}, {"text": "For example, it may rule out certain word boundaries if X were a character that normally occurs only as a suffix but that combines freely with some other basic forms to create new words.", "labels": [], "entities": []}, {"text": "This type of features is slightly less natural in a semi-CRF, since in that case local features \u03d5(y i , y i+1 , x) are defined on pairs of adjacent words.", "labels": [], "entities": []}, {"text": "That is to say, information about which characters are not on boundaries is only implicit.", "labels": [], "entities": []}, {"text": "Notably, except the hybrid Markov/semi-Markov system in Andrew (2006) 2 , no other studies using the semi-CRF () experimented with features of segmenting non-boundaries.", "labels": [], "entities": [{"text": "Andrew (2006) 2", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.8665821790695191}]}, {"text": "In this paper, instead of using semi-Markov models, we describe an alternative, a latent variable model, to learn long range dependencies in Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.6227979063987732}]}, {"text": "We use the discriminative probabilistic latent variable models (DPLVMs), which use latent variables to carry additional information that may not be expressed by those original labels, and therefore try to build more complicated or longer dependencies.", "labels": [], "entities": []}, {"text": "This is especially meaningful in CWS, because the used labels are quite coarse: Label(y) \u2208 {B, C}, where B signifies beginning a word and C signifies the continuation of a word.", "labels": [], "entities": []}, {"text": "3 For example, by using DPLVM, the aforementioned feature may turn to \"the current character xi is X, Label i = C, and LatentV ariable i = LV \".", "labels": [], "entities": []}, {"text": "The current latent variable LV may strongly depend on the previous one or many latent variables, and therefore we can model the long range dependencies which may not be captured by those very coarse labels.", "labels": [], "entities": []}, {"text": "Also, since character and word information have their different advantages in CWS, in our latent variable model, we use hybrid information based on both character and word sequences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the data provided by the second International Chinese Word Segmentation Bakeoff to test our approaches described in the previous sections.", "labels": [], "entities": [{"text": "International Chinese Word Segmentation Bakeoff", "start_pos": 40, "end_pos": 87, "type": "DATASET", "confidence": 0.8345137119293213}]}, {"text": "The data contains three corpora from different sources: Microsoft Research Asia (MSR), City University of Hong Kong (CU), and Peking University (PKU).", "labels": [], "entities": [{"text": "Microsoft Research Asia (MSR)", "start_pos": 56, "end_pos": 85, "type": "DATASET", "confidence": 0.9105791449546814}]}, {"text": "Since the purpose of this work is to evaluate the proposed latent variable model, we did not use extra resources such as common surnames, lexicons, parts-of-speech, and semantics.", "labels": [], "entities": []}, {"text": "For the generation of word-based features, we extracted a word list from the training data as the vocabulary.", "labels": [], "entities": []}, {"text": "Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score (F ) defined by 2P R/(P + R), recall of OOV words (R-oov).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.9662144184112549}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9987388253211975}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9991414546966553}, {"text": "F-score (F )", "start_pos": 260, "end_pos": 272, "type": "METRIC", "confidence": 0.9418613910675049}, {"text": "recall", "start_pos": 298, "end_pos": 304, "type": "METRIC", "confidence": 0.998753547668457}]}, {"text": "For more detailed information on the corpora and these metrics, refer to Emerson (2005).", "labels": [], "entities": []}, {"text": "Three training and test corpora were used in the test, including the MSR Corpus, the CU Corpus, and the We have tried stochastic gradient decent, as described previously.", "labels": [], "entities": [{"text": "MSR Corpus", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.9547011852264404}, {"text": "CU Corpus", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.9532980024814606}]}, {"text": "It is possible to try other stochastic learning methods, e.g., stochastic meta decent (  PKU Corpus (see for details).", "labels": [], "entities": [{"text": "PKU Corpus", "start_pos": 89, "end_pos": 99, "type": "DATASET", "confidence": 0.8404771685600281}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The results are grouped into three sub-tables according to different corpora.", "labels": [], "entities": []}, {"text": "Each row represents a CWS model.", "labels": [], "entities": []}, {"text": "For each group, the rows marked by * represent our models with hybrid word/character information.", "labels": [], "entities": []}, {"text": "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; A06 represents the semi-CRF model in Andrew (2006) , which was also used in (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure subword CRF model and the confidence-based combination of CRF and rule-based models, respectively (); ZC07 represents the word-based perceptron model in; T05 represents the CRF model in; C05 represents the system in Chen et al.", "labels": [], "entities": [{"text": "Second International Chinese Word Segmentation Bakeoff", "start_pos": 41, "end_pos": 95, "type": "TASK", "confidence": 0.764044443766276}]}, {"text": "It is a hybrid Markov/semi-Markov CRF model which outperforms conventional semi-CRF models).", "labels": [], "entities": []}, {"text": "However, in general, as discussed in, it is essentially still a semi-CRF model..", "labels": [], "entities": []}, {"text": "The best F-score and recall of OOV words of each group is shown in bold.", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9956804513931274}, {"text": "recall of OOV", "start_pos": 21, "end_pos": 34, "type": "METRIC", "confidence": 0.738241950670878}]}, {"text": "As is shown in the table, we achieved the best F-score in two out of the three corpora.", "labels": [], "entities": [{"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9986590147018433}]}, {"text": "We also achieved the best recall rate of OOV words on those two corpora.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.971411794424057}]}, {"text": "Both of the MSR and PKU Corpus use simplified Chinese, while the CU Corpus uses the traditional Chinese.", "labels": [], "entities": [{"text": "MSR", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.9533082246780396}, {"text": "PKU Corpus", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9657676815986633}, {"text": "CU Corpus", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9662197530269623}]}, {"text": "On the MSR Corpus, the DPLVM model reduced more than 10% error rate over the CRF model using exactly the same feature set.", "labels": [], "entities": [{"text": "MSR Corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9200983047485352}, {"text": "error rate", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9682632982730865}]}, {"text": "We also compared our DPLVM model with the semi-CRF models in and, and demonstrate that the DPLVM model achieved slightly better performance than the semi-CRF models. and only reported the results on the MSR Corpus.", "labels": [], "entities": [{"text": "MSR Corpus", "start_pos": 203, "end_pos": 213, "type": "DATASET", "confidence": 0.9688929915428162}]}, {"text": "In summary, tests for the Second International Chinese Word Segmentation Bakeoff showed competitive results for our method compared with the best results in the literature.", "labels": [], "entities": [{"text": "Second International Chinese Word Segmentation Bakeoff", "start_pos": 26, "end_pos": 80, "type": "TASK", "confidence": 0.8056252400080363}]}, {"text": "Our discriminative latent variable models achieved the best F-scores on the MSR Corpus (97.3%) and PKU Corpus (95.2%); the latent variable models also achieved the best recalls of OOV words over those two corpora.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9977741837501526}, {"text": "MSR Corpus", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9545333087444305}, {"text": "PKU Corpus", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.9457134902477264}, {"text": "recalls", "start_pos": 169, "end_pos": 176, "type": "METRIC", "confidence": 0.9966335892677307}]}, {"text": "We will analyze the results by varying the word-length in the following subsection.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results from DPLVMs, CRFs, semi-CRFs, and  other systems.", "labels": [], "entities": []}]}