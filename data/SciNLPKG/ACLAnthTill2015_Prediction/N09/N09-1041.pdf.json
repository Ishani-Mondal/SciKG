{"title": [{"text": "Exploring Content Models for Multi-Document Summarization", "labels": [], "entities": [{"text": "Multi-Document Summarization", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.686233639717102}]}], "abstractContent": [{"text": "We present an exploration of generative prob-abilistic models for multi-document summa-rization.", "labels": [], "entities": []}, {"text": "Beginning with a simple word frequency based model (Nenkova and Vander-wende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 212, "end_pos": 217, "type": "METRIC", "confidence": 0.9556214213371277}]}, {"text": "Our final model, HIERSUM, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.", "labels": [], "entities": []}, {"text": "At the task of producing generic DUC-style summaries, HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.880698561668396}, {"text": "ROUGE", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.980215847492218}]}, {"text": "(2007)'s state-of-the-art discriminative system.", "labels": [], "entities": []}, {"text": "We also explore HIERSUM's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation.", "labels": [], "entities": [{"text": "content discovery", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7277670055627823}]}], "introductionContent": [{"text": "Over the past several years, there has been much interest in the task of multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6393765807151794}]}, {"text": "In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary.", "labels": [], "entities": [{"text": "common Document Understanding Conference (DUC)", "start_pos": 7, "end_pos": 53, "type": "TASK", "confidence": 0.7053501648562295}]}, {"text": "To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.", "labels": [], "entities": []}, {"text": "There are several approaches to modeling document content: simple word frequency-based methods), graph-based approaches), as well as more linguistically motivated techniques.", "labels": [], "entities": []}, {"text": "Another strand of work (, has explored the use of structured probabilistic topic models to represent document content.", "labels": [], "entities": []}, {"text": "However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization.", "labels": [], "entities": [{"text": "generic multi-document summarization", "start_pos": 116, "end_pos": 152, "type": "TASK", "confidence": 0.5175908704598745}]}, {"text": "In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models ( can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2).", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.5022169947624207}]}, {"text": "We also contend that they provide convenient building blocks for adding more structure to a summarization model.", "labels": [], "entities": []}, {"text": "In particular, we utilize a variation of the hierarchical LDA topic model () to discover multiple specific 'subtopics' within a document set.", "labels": [], "entities": []}, {"text": "The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.8039543032646179}, {"text": "summaries", "start_pos": 89, "end_pos": 98, "type": "TASK", "confidence": 0.9593625664710999}]}], "datasetContent": [{"text": "The task we will consider is extractive multidocument summarization.", "labels": [], "entities": [{"text": "extractive multidocument summarization", "start_pos": 29, "end_pos": 67, "type": "TASK", "confidence": 0.5833171208699545}]}, {"text": "In this task we assume a document collection D consisting of documents D 1 , . .", "labels": [], "entities": []}, {"text": ", D n describing the same (or closely related) narrative set of events.", "labels": [], "entities": []}, {"text": "Our task will be to propose a summary S consisting of sentences in D totaling at most L words.", "labels": [], "entities": []}, {"text": "Here as in much extractive summarization, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1).", "labels": [], "entities": []}, {"text": "The most prevalent example of this data setting is document clusters found on news aggregator sites.", "labels": [], "entities": []}, {"text": "For model development we will utilize the DUC 2006 evaluation set 4 consisting of 50 document sets each with 25 documents; final evaluation will utilize the DUC 2007 evaluation set (section 5).", "labels": [], "entities": [{"text": "DUC 2006 evaluation set", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.9604170173406601}, {"text": "DUC 2007 evaluation set", "start_pos": 157, "end_pos": 180, "type": "DATASET", "confidence": 0.9731371104717255}]}, {"text": "Automated evaluation will utilize the standard DUC evaluation metric ROUGE) which represents recall over various n-grams statistics from a system-generated summary against a set of humangenerated peer summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.7675466537475586}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9970812201499939}]}, {"text": "We compute ROUGE scores with and without stop words removed from peer and proposed summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9718001484870911}]}, {"text": "In particular, we utilize R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams) . We present R-2 without stop words in the running text, but full development results are presented in table 1.", "labels": [], "entities": []}, {"text": "Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling).", "labels": [], "entities": [{"text": "significance", "start_pos": 67, "end_pos": 79, "type": "METRIC", "confidence": 0.9488938450813293}]}, {"text": "In addition to presenting automated results, we also present a user evaluation in section 5.2.", "labels": [], "entities": []}, {"text": "We present formal experiments on the DUC 2007 data main summarization task, proposing a general summary of at most 250 words 22 which will be evaluated automatically and manually in order to simulate as much as possible the DUC evaluation environment.", "labels": [], "entities": [{"text": "DUC 2007 data main summarization task", "start_pos": 37, "end_pos": 74, "type": "DATASET", "confidence": 0.8725671172142029}]}, {"text": "DUC 2007 consists of 45 document sets, each consisting of 25 documents and 4 human reference summaries.", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.977344810962677}]}, {"text": "We primarily evaluate the HIERSUM model, extracting a single summary from the general content distribution using the KLSUM criterion (see section 3.2).", "labels": [], "entities": [{"text": "KLSUM", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.7520820498466492}]}, {"text": "Although the differences in ROUGE between HIERSUM and TOPICSUM were minimal, we found HIERSUM summary quality to be stronger.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9948222637176514}, {"text": "TOPICSUM", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.5729977488517761}, {"text": "HIERSUM summary quality", "start_pos": 86, "end_pos": 109, "type": "METRIC", "confidence": 0.4938770929972331}]}, {"text": "In order to provide a reference for ROUGE and manual evaluation results, we compare against PYTHY, a state-of-the-art supervised sentence extraction summarization system.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9198412299156189}, {"text": "sentence extraction summarization", "start_pos": 129, "end_pos": 162, "type": "TASK", "confidence": 0.8031733632087708}]}, {"text": "PYTHY uses humangenerated summaries in order to train a sentence ranking system which discriminatively maximizes System ROUGE w/o stop ROUGE w/ stop R-1 R-2 R-SU4 R-1 R-2 R-SU4   As PYTHY utilizes a sentence simplification component, which we do not, we also compare against PYTHY without sentence simplification.", "labels": [], "entities": [{"text": "PYTHY", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9132184386253357}]}, {"text": "ROUGE results comparing variants of HIERSUM and PYTHY are given in table 3.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9875970482826233}, {"text": "HIERSUM", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.8757602572441101}, {"text": "PYTHY", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.8245749473571777}]}, {"text": "The HIERSUM system as described in section 3.4 yields 7.3 R-2 without stop words, falling significantly short of the 8.7 that PYTHY without simplification yields.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9490400552749634}]}, {"text": "Note that R-2 is a measure of bigram recall and HIERSUM does not represent bigrams whereas PYTHY includes several bigram and higher order n-gram statistics.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9662573933601379}, {"text": "HIERSUM", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9494917988777161}]}, {"text": "In order to put HIERSUM and PYTHY on equalfooting with respect to R-2, we instead ran HIER-SUM with each sentence consisting of a bag of bigrams instead of unigrams.", "labels": [], "entities": [{"text": "PYTHY", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.7854290008544922}]}, {"text": "All the details of the model remain the same.", "labels": [], "entities": []}, {"text": "Once a general content distribution over bigrams has been determined by hierarchical topic modeling, the KLSUM criterion is used as before to extract a summary.", "labels": [], "entities": [{"text": "KLSUM", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.747370183467865}]}, {"text": "This system, labeled HIERSUM bigram in table 3, yields 9.3 R-2 without stop words, significantly outperforming HIERSUM unigram.", "labels": [], "entities": []}, {"text": "This model outperforms PYTHY with and without sentence simplification, but not with statistical significance.", "labels": [], "entities": []}, {"text": "We conclude that both PYTHY variants and HIERSUM bigram are comparable with respect to ROUGE performance.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.8102979063987732}, {"text": "ROUGE", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9888626337051392}]}, {"text": "Note that by doing topic modeling in this way over bigrams, our model becomes degenerate as it can generate inconsistent bags of bigrams.", "labels": [], "entities": []}, {"text": "Future work may look at topic models over n-grams as suggested by  In order to obtain a more accurate measure of summary quality, we performed a simple user study.", "labels": [], "entities": []}, {"text": "For each document set in the DUC 2007 collection, a user was given a reference summary, a PYTHY summary, and a HIERSUM summary; 25 note that the original documents in the set were not provided to the user, only a reference summary.", "labels": [], "entities": [{"text": "DUC 2007 collection", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9643016258875529}, {"text": "PYTHY summary", "start_pos": 90, "end_pos": 103, "type": "METRIC", "confidence": 0.9546127021312714}, {"text": "HIERSUM summary", "start_pos": 111, "end_pos": 126, "type": "METRIC", "confidence": 0.9717564284801483}]}, {"text": "For this experiment we use the bigram variant of HIERSUM and compare it to PYTHY without simplification so both systems have the same set of possible output summaries.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.7910696864128113}, {"text": "PYTHY", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.6700726747512817}]}, {"text": "The reference summary for each document set was selected according to highest R-2 without stop words against the remaining peer summaries.", "labels": [], "entities": [{"text": "R-2", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9805867671966553}]}, {"text": "Users were presented with 4 questions drawn from the DUC manual evaluation guidelines: (1) Overall quality: Which summary was better overall?", "labels": [], "entities": [{"text": "DUC manual evaluation", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.8884959022204081}]}, {"text": "(2) Non-Redundancy: Which summary was less redundant?", "labels": [], "entities": []}, {"text": "(3) Coherence: Which summary was more coherent?", "labels": [], "entities": []}, {"text": "(4) Focus: Which summary was more: Using HIERSUM to organize content of document set into topics (see section 6.1).", "labels": [], "entities": []}, {"text": "The sidebar gives key phrases salient in each of the specific content topics in HIERSUM (see section 3.4).", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.8263278007507324}]}, {"text": "When a topic is clicked in the right sidebar, the mainframe displays an extractive 'topical summary' with links into document set articles.", "labels": [], "entities": []}, {"text": "Ideally, a user could use this interface to quickly find content in a document collection that matches their interest.", "labels": [], "entities": []}, {"text": "focused in its content, not conveying irrelevant details?", "labels": [], "entities": []}, {"text": "The study had 16 users and each was asked to compare five summary pairs, although some did fewer.", "labels": [], "entities": []}, {"text": "A total of 69 preferences were solicited.", "labels": [], "entities": []}, {"text": "Document collections presented to users were randomly selected from those evaluated fewest.", "labels": [], "entities": []}, {"text": "As seen in table 5.2, HIERSUM outperforms PYTHY under all questions.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9953306913375854}, {"text": "PYTHY", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.5260055661201477}]}, {"text": "All results are statistically significant as judged by a simple pairwise t-test with 95% confidence.", "labels": [], "entities": []}, {"text": "It is safe to conclude that users in this study strongly preferred the HIER-SUM summaries over the PYTHY summaries.", "labels": [], "entities": [{"text": "HIER-SUM summaries", "start_pos": 71, "end_pos": 89, "type": "DATASET", "confidence": 0.7053196281194687}, {"text": "PYTHY summaries", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.9472025632858276}]}], "tableCaptions": [{"text": " Table 1: ROUGE results on DUC2006 for models pre- sented in section 3. Results in bold represent results sta- tistically significantly different from SUMBASIC in the  appropriate metric.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9870016574859619}, {"text": "DUC2006", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9239629507064819}]}, {"text": " Table 2: Example summarization output for systems compared in section 5.2. (a), (b), and (c) represent the first two  sentences output from PYTHY, HIERSUM, and reference summary respectively. In (d), we present the most frequent  non-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries.  Note that many content words in the reference summary absent from PYTHY's proposal are present in HIERSUM's.", "labels": [], "entities": [{"text": "summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.974745512008667}, {"text": "PYTHY", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.8897236585617065}, {"text": "PYTHY and HIERSUM summaries", "start_pos": 308, "end_pos": 335, "type": "DATASET", "confidence": 0.6794038340449333}]}, {"text": " Table 3: Formal ROUGE experiment results on DUC 2007 document set collection (see section 5.1). While HIER- SUM unigram underperforms both PYTHY systems in statistical significance (for R-2 and RU-4 with and without stop  words), HIERSUM bigram's performance is comparable and statistically no worse.", "labels": [], "entities": [{"text": "DUC 2007 document set collection", "start_pos": 45, "end_pos": 77, "type": "DATASET", "confidence": 0.9652915835380554}]}]}