{"title": [{"text": "Reverse Revision and Linear Tree Combination for Dependency Parsing", "labels": [], "entities": [{"text": "Reverse Revision", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7891889810562134}, {"text": "Parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.7641740441322327}]}], "abstractContent": [], "introductionContent": [{"text": "Deterministic transition-based Shift/Reduce dependency parsers make often mistakes in the analysis of long span dependencies.", "labels": [], "entities": [{"text": "Shift/Reduce dependency parsers", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6838770866394043}]}, {"text": "address this accuracy drop by using abeam search instead of a greedy algorithm for predicting the next parser transition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9988048076629639}]}, {"text": "We propose a parsing method that allows reducing several of these errors, although maintaining a quasi linear complexity.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9779905080795288}]}, {"text": "The method consists in two steps: first the sentence is parsed by a deterministic Shift/Reduce parser, then a second deterministic Shift/Reduce parser analyzes the sentence in reverse using additional features extracted from the parse trees produced by the first parser.", "labels": [], "entities": []}, {"text": "Right-to-left parsing has been used as part of ensemble-based parsers.", "labels": [], "entities": [{"text": "Right-to-left parsing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.516542375087738}]}, {"text": "instead use hints from one parse as features in a second parse, exploiting the complementary properties of graph-based parsers) and transition-based dependency parsers).", "labels": [], "entities": []}, {"text": "Also our method uses input from a previous parser but only uses parsers of a single type, deterministic transition-based Shift/Reduce, maintaining an overall linear complexity.", "labels": [], "entities": []}, {"text": "In fact both the ensemble parsers and the stacking solution of NivreMcDonald involve the computation of the maximum spanning tree (MST) of a graph, which require algorithms of quadratic time complexity (e.g.).", "labels": [], "entities": []}, {"text": "We introduce an alternative linear combination method.", "labels": [], "entities": []}, {"text": "The algorithm is greedy and works by combining the trees top down.", "labels": [], "entities": []}, {"text": "We tested it on the dependency trees produced by three parsers, a Leftto-Right (LR ), a Right-to-Left (RL ) and a stacked Right-to-Left parser, or Reverse Revision parser (Rev2 ).", "labels": [], "entities": []}, {"text": "The experiments show that in practice its output often outperforms the results produced by calculating the MST.", "labels": [], "entities": [{"text": "MST", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.506303608417511}]}], "datasetContent": [{"text": "In the reported experiments we used DeSR (Attardi at al., 2007), a freely available implementation of a transition-based parser.", "labels": [], "entities": [{"text": "DeSR (Attardi at al., 2007)", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.7789722681045532}]}, {"text": "The parser processes input tokens advancing on the input with Shift actions and accumulates processed tokens on a stack with Reduce actions.", "labels": [], "entities": []}, {"text": "The parsing algorithm is fully deterministic and linear.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.976894199848175}]}, {"text": "For the LR parser and the Rev2 parser we employed an SVM classifier while a Maximum Entropy classifier, with lower accuracy, was used to create the training set for the Rev2 parser.", "labels": [], "entities": [{"text": "Rev2", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.943731963634491}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9980600476264954}]}, {"text": "The reason for this appears to be that the output of a low accuracy parser with many errors provides a better source of learning to the stacked parser.", "labels": [], "entities": []}, {"text": "The Rev2 parser exploits the same basic set of features as in the LR parser plus the additional features extracted from the output of the LR parser listed in, where: PHLEMMA is the lemma of the predicted head, PHPOS is the Part of Speech of the predicted head, PDEP is the predicted dependency label of a token to its predicted head, PHDIST indicates whether a token is located before or after Feature Tokens PHHLEMMA w0 w1 PHDEP w0 w1 PHPOS s0 w0 w1 PHLEMMA s0 w0 w1 PDEP s0 w0 w1 PHDIST s0 w0 w1 its predicted head, PHHLEMMA is the lemma of the predicted grandparent and PHDEP is the predicted dependency label of the predicted head of a token to the predicted grandparent.", "labels": [], "entities": [{"text": "PHLEMMA", "start_pos": 166, "end_pos": 173, "type": "METRIC", "confidence": 0.9259673953056335}, {"text": "PHDEP", "start_pos": 573, "end_pos": 578, "type": "METRIC", "confidence": 0.9076935648918152}]}, {"text": "s 0 refers to a token on top of the stack, w i refers to word at the i-th relative position with respect to the current word and parsing direction.", "labels": [], "entities": []}, {"text": "This feature model was used for all languages in our tests.", "labels": [], "entities": []}, {"text": "We present experiments and comparative error analysis on three representative languages from the CoNLL 2007 shared task (Nivre at al., 2007): Italian, Czech and English.", "labels": [], "entities": [{"text": "CoNLL 2007 shared task (Nivre at al., 2007)", "start_pos": 97, "end_pos": 140, "type": "DATASET", "confidence": 0.8498282324184071}]}, {"text": "We also report an evaluation on all thirteen languages of the CoNLL-X shared task), for comparison with the results by. shows the Labeled Attachment Score (LAS), for the Left-to-right parser (LR ), Right-toLeft (RL ), Reverse Revision parser (Rev2 ), linear parser combination (Comb) and MST parser combination (CombMST). and 2 present the accuracies of the LR and Rev2 parsers for English relative to the dependency length and the length of sentences, respectively.", "labels": [], "entities": [{"text": "Labeled Attachment Score (LAS)", "start_pos": 130, "end_pos": 160, "type": "METRIC", "confidence": 0.8413535753885905}]}, {"text": "For Czech and Italian the RL parser achieves higher accuracy than the LR parser and the Rev2 parser even higher.", "labels": [], "entities": [{"text": "RL parser", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.6852551996707916}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9993414282798767}, {"text": "Rev2 parser", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.6189133375883102}]}, {"text": "The error analysis for Czech showed that the Rev2 parser improves over the LR parser everywhere except in the Recall for dependencies of length between 10 and 14.", "labels": [], "entities": [{"text": "error", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9541995525360107}, {"text": "Czech", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.9509982466697693}, {"text": "Recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.6944512724876404}]}, {"text": "Such an improvement has positive impact on the analysis of sentences longer than 10 tokens, like for Italian.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Additional features used in training the Revision  parser.", "labels": [], "entities": [{"text": "Revision  parser", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.5678905695676804}]}, {"text": " Table 2: LAS for selected CoNLL 2007 languages.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9380133152008057}, {"text": "CoNLL 2007 languages", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.9372672438621521}]}, {"text": " Table 3: Labeled attachment scores for CoNLL-X corpora.", "labels": [], "entities": []}]}