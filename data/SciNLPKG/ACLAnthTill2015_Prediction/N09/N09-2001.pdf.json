{"title": [{"text": "Cohesive Constraints in A Beam Search Phrase-based Decoder", "labels": [], "entities": []}], "abstractContent": [{"text": "Cohesive constraints allow the phrase-based decoder to employ arbitrary, non-syntactic phrases, and encourage it to translate those phrases in an order that respects the source dependency tree structure.", "labels": [], "entities": []}, {"text": "We present extensions of the cohesive constraints, such as exhaustive interruption count and rich interruption check.", "labels": [], "entities": [{"text": "exhaustive interruption count", "start_pos": 59, "end_pos": 88, "type": "METRIC", "confidence": 0.8533508578936259}, {"text": "rich interruption check", "start_pos": 93, "end_pos": 116, "type": "METRIC", "confidence": 0.6872249841690063}]}, {"text": "We show that the cohesion-enhanced de-coder significantly outperforms the standard phrase-based decoder on English\u2192Spanish.", "labels": [], "entities": []}, {"text": "Improvements between 0.5 and 1.2 BLEU point are obtained on English\u2192Iraqi system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9990654587745667}, {"text": "English\u2192Iraqi system", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.6854230910539627}]}], "introductionContent": [{"text": "Phrase-based machine translation is driven by a phrasal translation model, which relates phrases (contiguous segments of words) in the source to phrases in the target.", "labels": [], "entities": [{"text": "Phrase-based machine translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8452305396397909}]}, {"text": "This translation model can be derived from a wordaligned bitext.", "labels": [], "entities": []}, {"text": "Translation candidates are scored according to a linear model combining several informative feature functions.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9546121954917908}]}, {"text": "Crucially, this model incorporates translation model scores and n-gram language model scores.", "labels": [], "entities": []}, {"text": "The component features are weighted to minimize a translation error criterion on a development set.", "labels": [], "entities": []}, {"text": "Decoding the source sentence takes the form of abeam search through the translation space, with intermediate states corresponding to partial translations.", "labels": [], "entities": []}, {"text": "The decoding process advances by extending a state with the translation of a source phrase, until each source word has been translated exactly once.", "labels": [], "entities": []}, {"text": "Re-ordering occurs when the source phrase to be translated does not immediately follow the previously translated phrase.", "labels": [], "entities": []}, {"text": "This is penalized with a discriminatively-trained distortion penalty.", "labels": [], "entities": [{"text": "distortion", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9250898361206055}]}, {"text": "In order to calculate the current translation score, each state can be represented by a triple: \u2022 A coverage vector HC indicates which source words have already been translated.", "labels": [], "entities": []}, {"text": "\u2022 A span \u00af f indicates the last source phrase translated to create this state.", "labels": [], "entities": [{"text": "A span \u00af f", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9335878044366837}]}, {"text": "\u2022 A target word sequence stores context needed by the target language model.", "labels": [], "entities": []}, {"text": "As cohesion concerns only movement in the source, we can completely ignore the language model context, making state effectively an ( \u00af f , HC ) tuple.", "labels": [], "entities": []}, {"text": "To enforce cohesion during the state expansion process, cohesive phrasal decoding has been proposed in).", "labels": [], "entities": []}, {"text": "The cohesionenhanced decoder enforces the following constraint: once the decoder begins translating any part of a source subtree, it must coverall the words under that subtree before it can translate anything outside of it.", "labels": [], "entities": []}, {"text": "This notion can be applied to any projective tree structure, but we use dependency trees, which have been shown to demonstrate greater cross-lingual cohesion than other structures).", "labels": [], "entities": []}, {"text": "We use a tree data structure to store the dependency tree.", "labels": [], "entities": []}, {"text": "Each node in the tree contains surface word form, word position, parent position, dependency type and POS tag.", "labels": [], "entities": []}, {"text": "We use T to stand for our dependency tree, and T (n) to stand for the subtree rooted at node n.", "labels": [], "entities": []}, {"text": "Each subtree T (n) covers a span of contiguous source words; for subspan \u00af f covered by T (n), we say \u00af f \u2208 T (n).", "labels": [], "entities": []}, {"text": "Cohesion is checked as we extend a state ( \u00af f h , HC h ) with the translation of \u00af f h+1 , creating anew state ( \u00af f h+1 , HC h+1 ).", "labels": [], "entities": []}, {"text": "Algorithm 1 presents the cohesion check described by.", "labels": [], "entities": []}, {"text": "Line 2 selects focal points, based on the last translated phrase.", "labels": [], "entities": []}, {"text": "Line 4 climbs from each focal point to find the largest subtree that needs to be completed before the translation process can move elsewhere in the tree.", "labels": [], "entities": []}, {"text": "Line 5 checks each such subtree for completion.", "labels": [], "entities": []}, {"text": "Since there area constant number of focal points (always 2) and the tree climb and completion checks are both linear in the size of the source, the entire check can be shown to take linear time.", "labels": [], "entities": [{"text": "completion", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9748585224151611}]}, {"text": "The selection of only two focal points is motivated by a \"violation free\" assumption.", "labels": [], "entities": []}, {"text": "If one assumes that the Algorithm 1 Interruption Check (Coh1) Input: Source tree T , previous phrase \u00af f h , current phrase \u00af f h+1 , coverage vector HC 1: Interruption \u2190 F alse 2: F \u2190 the left and right-most tokens of \u00af f h 3: for each off \u2208 F do 4: Climb the dependency tree from f until you reach the highest node n such that \u00af f h+1 / \u2208 T (n).", "labels": [], "entities": []}], "datasetContent": [{"text": "We built baseline systems using GIZA++, Moses' phrase extraction with grow-diag-finalend heuristic (), a standard phrasebased decoder, the SRI LM toolkit), the suffix-array language model), a distance-based word reordering model", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7832934558391571}, {"text": "SRI LM toolkit", "start_pos": 139, "end_pos": 153, "type": "DATASET", "confidence": 0.8302480181058248}]}], "tableCaptions": [{"text": " Table 2 shows results in  lowercase BLEU and bold type is used to indicate high- est scores. An italic text indicates the score is statistically  significant better than the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9976733326911926}]}, {"text": " Table 2: Scores of baseline and cohesion-enhanced systems on  English\u2192Iraqi and English\u2192Spanish systems", "labels": [], "entities": []}]}