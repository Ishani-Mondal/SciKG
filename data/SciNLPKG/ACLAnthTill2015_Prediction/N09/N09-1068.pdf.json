{"title": [], "abstractContent": [{"text": "Multi-task learning is the problem of maximizing the performance of a system across a number of related tasks.", "labels": [], "entities": [{"text": "Multi-task learning", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8335873484611511}]}, {"text": "When applied to multiple domains for the same task, it is similar to domain adaptation, but symmetric, rather than limited to improving performance on a target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7198190987110138}]}, {"text": "We present a more principled, better performing model for this problem, based on the use of a hierarchical Bayesian prior.", "labels": [], "entities": []}, {"text": "Each domain has its own domain-specific parameter for each feature but, rather than a constant prior over these parameters, the model instead links them via a hierarchical Bayesian global prior.", "labels": [], "entities": []}, {"text": "This prior encourages the features to have similar weights across domains, unless there is good evidence to the contrary.", "labels": [], "entities": []}, {"text": "We show that the method of (Daum\u00e9 III, 2007), which was presented as a simple \"prepro-cessing step,\" is actually equivalent, except our representation explicitly separates hyper-parameters which were tied in his work.", "labels": [], "entities": [{"text": "Daum\u00e9 III, 2007)", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.93937908411026}]}, {"text": "We demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and (Daum\u00e9 III, 2007) within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 215, "end_pos": 239, "type": "TASK", "confidence": 0.6308977603912354}]}], "introductionContent": [{"text": "The goal of multi-task learning is to improve performance on a set of related tasks, when provided with (potentially varying quantities of) annotated data for each of the tasks.", "labels": [], "entities": []}, {"text": "It is very closely related to domain adaptation, afar more common task in the natural language processing community, but with two primary differences.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7609375715255737}]}, {"text": "Firstly, in domain adaptation the different tasks are actually just different domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7120419442653656}]}, {"text": "Secondly, in multi-task learning the focus is on improving performance across all tasks, while in domain adaptation there is a distinction between source data and target data, and the goal is to improve performance on the target data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7438706159591675}]}, {"text": "In the present work we focus on domain adaptation, but like the multi-task setting, we wish to improve performance across all domains and not a single target domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8007088303565979}]}, {"text": "The word domain is used here somewhat loosely: it may refer to a topical domain or to distinctions that linguists might term mode (speech versus writing) or register (formal written prose versus SMS communications).", "labels": [], "entities": []}, {"text": "For example, one may have a large amount of parsed newswire, and want to use it to augment a much smaller amount of parsed e-mail, to build a higher quality parser for e-mail data.", "labels": [], "entities": []}, {"text": "We also consider the extension to the task where the annotation is not the same, but is consistent, across domains (that is, some domains maybe annotated with more information than others).", "labels": [], "entities": []}, {"text": "This problem is important because it is omnipresent in real life natural language processing tasks.", "labels": [], "entities": []}, {"text": "Annotated data is expensive to produce and limited in quantity.", "labels": [], "entities": []}, {"text": "Typically, one may begin with a considerable amount of annotated newswire data, some annotated speech data, and a little annotated e-mail data.", "labels": [], "entities": []}, {"text": "It would be most desirable if the aggregated training data could be used to improve the performance of a system on each of these domains.", "labels": [], "entities": []}, {"text": "From the baseline of building separate systems for each domain, the obvious first attempt at domain adaptation is to build a system from the union of the training data, and we will refer to this as a second baseline.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7567584812641144}]}, {"text": "In this paper we propose a more principled, formal model of domain adaptation, which not only outperforms previous work, but maintains attractive performance characteristics in terms of training and testing speed.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7575890719890594}]}, {"text": "We also show that the domain adaptation work of, which is presented as an ad-hoc \"preprocessing step,\" is actually equivalent to our formal model.", "labels": [], "entities": []}, {"text": "However, our representation of the model conceptually separates some of the hyperparameters which are not separated in, and we found that setting these hyperparameters with different values from one another was critical for improving performance.", "labels": [], "entities": []}, {"text": "We apply our model to two tasks, named entity recognition, using a linear chain conditional random field (CRF), and dependency parsing, using a discriminative, chart-based model.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.73995441198349}, {"text": "dependency parsing", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8129448592662811}]}, {"text": "In both cases, we find that our model improves performance over both baselines and prior work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared the same four domain adaptation models for dependency parsing as we did for the named entity experiments, once again setting \u03c3 = 1.0 and \u03c3 d = 0.1.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8509070873260498}]}, {"text": "Unlike the named entity experiments however, there were no label set discrepencies between the domains, so only one version of each domain adaptation model was necessary, instead of the two versions in that section.", "labels": [], "entities": []}, {"text": "Our full dependency parsing results can be found in.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.6041520088911057}]}, {"text": "Firstly, we found that DAUME07, which had outperformed the ALL DATA baseline for the sequence modeling task, performed worse than the Specifically, all the other domains use the \"new\" Penn Treebank annotation style, whereas the WSJ data is still in the \"traditional\" annotation style, familiar from the past decade's work in Penn Treebank parsing.", "labels": [], "entities": [{"text": "sequence modeling task", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.8342533111572266}, {"text": "Penn Treebank", "start_pos": 184, "end_pos": 197, "type": "DATASET", "confidence": 0.9884081780910492}, {"text": "WSJ data", "start_pos": 228, "end_pos": 236, "type": "DATASET", "confidence": 0.9546382427215576}, {"text": "Penn Treebank", "start_pos": 325, "end_pos": 338, "type": "DATASET", "confidence": 0.9832231402397156}]}, {"text": "The major changes are in hyphenation and NP structure.", "labels": [], "entities": []}, {"text": "In the new annotation style, many hyphenated words are separated into multiple tokens, with anew part-of-speech tag given to the hyphens, and leftwardbranching structure inside noun phrases is indicated by use of anew NML phrasal category.", "labels": [], "entities": []}, {"text": "The treatment of hyphenated words, in particular, makes the two annotation styles inconsistent, and so we could notwork with all the data together.", "labels": [], "entities": []}, {"text": "baseline here, indicating that the transfer of information between domains in the more structurally complicated task is inherently more difficult.", "labels": [], "entities": []}, {"text": "Our model's gains over the ALL DATA baseline are quite small, but we tested their significance using a sentence-level paired t-test (over all of the data combined) and found them to be significant at p < 10 \u22125 . We are unsure why some domains improved while others did not.", "labels": [], "entities": [{"text": "ALL DATA baseline", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.5762423078219095}]}, {"text": "It is not simply a consequence of training set size, but maybe due to qualities of the domains themselves.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of words in the training and test sets for  each of the named entity recognition datasets.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7592299282550812}]}, {"text": " Table 2: Named entity recognition results for each of the  models. With the exception of the TARGET ONLY model,  all three datasets were combined when training each of  the models.", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6093393564224243}, {"text": "TARGET", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.8452930450439453}, {"text": "ONLY", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.6615728139877319}]}, {"text": " Table 3: Dependency parsing results for each of the domain adaptation models. Performance is measured as unlabeled  attachment accuracy.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8002015948295593}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.8679683804512024}]}]}