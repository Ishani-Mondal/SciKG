{"title": [{"text": "Using Citations to Generate Surveys of Scientific Paradigms", "labels": [], "entities": []}], "abstractContent": [{"text": "The number of research publications in various disciplines is growing exponentially.", "labels": [], "entities": []}, {"text": "Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material.", "labels": [], "entities": []}, {"text": "In this paper we present the first steps in producing an automatically generated , readily consumable, technical survey.", "labels": [], "entities": []}, {"text": "Specifically we explore the combination of citation information and summarization techniques.", "labels": [], "entities": [{"text": "summarization", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.9810841679573059}]}, {"text": "Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role.", "labels": [], "entities": [{"text": "summarization", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.9826396703720093}]}], "introductionContent": [{"text": "In today's rapidly expanding disciplines, scientists and scholars are constantly faced with the daunting task of keeping up with knowledge in their field.", "labels": [], "entities": []}, {"text": "In addition, the increasingly interconnected nature of real-world tasks often requires experts in one discipline to rapidly learn about other areas in a short amount of time.", "labels": [], "entities": []}, {"text": "Cross-disciplinary research requires scientists in areas such as linguistics, biology, and sociology to learn about computational approaches and applications, e.g., computational linguistics, biological modeling, social networks.", "labels": [], "entities": []}, {"text": "Authors of journal articles and books must write accurate surveys of previous work, ranging from short summaries of related research to in-depth historical notes.", "labels": [], "entities": []}, {"text": "Interdisciplinary review panels are often called upon to review proposals in a wide range of areas, some of which maybe unfamiliar to panelists.", "labels": [], "entities": []}, {"text": "Thus, they must learn about anew discipline \"on the fly\" in order to relate their own expertise to the proposal.", "labels": [], "entities": []}, {"text": "Our goal is to effectively serve these needs by combining two currently available technologies: (1) bibliometric lexical link mining that exploits the structure of citations and relations among citations; and (2) summarization techniques that exploit the content of the material in both the citing and cited papers.", "labels": [], "entities": [{"text": "bibliometric lexical link mining", "start_pos": 100, "end_pos": 132, "type": "TASK", "confidence": 0.6201955452561378}, {"text": "summarization", "start_pos": 213, "end_pos": 226, "type": "TASK", "confidence": 0.9811536073684692}]}, {"text": "It is generally agreed upon that manually written abstracts are good summaries of individual papers.", "labels": [], "entities": []}, {"text": "More recently, argue that citation texts are useful in creating a summary of the important contributions of a research paper.", "labels": [], "entities": []}, {"text": "The citation text of a target paper is the set of sentences in other technical papers that explicitly refer to it).", "labels": [], "entities": []}, {"text": "However, argues that using citation text directly is not suitable for document summarization.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.5579449832439423}]}, {"text": "In this paper, we compare and contrast the usefulness of abstracts and of citation text in automatically generating a technical survey on a given topic from multiple research papers.", "labels": [], "entities": []}, {"text": "The next section provides the background for this work, including the primary features of a technical survey and also the types of input that are used in our study (full papers, abstracts, and citation texts).", "labels": [], "entities": []}, {"text": "Following this, we describe related work and point out the advances of our work over previous work.", "labels": [], "entities": []}, {"text": "We then describe how citation texts are used as anew input for multidocument summarization to produce surveys of a given technical area.", "labels": [], "entities": []}, {"text": "We apply four different summarization techniques to data in the ACL Anthol-ogy and evaluate our results using both automatic (ROUGE) and human-mediated (nugget-based pyramid) measures.", "labels": [], "entities": [{"text": "ACL Anthol-ogy", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.8378128409385681}]}, {"text": "We observe that, as expected, abstracts are useful in survey creation, but, notably, we also conclude that citation texts have crucial surveyworthy information not present in (or at least, not easily extractable from) abstracts.", "labels": [], "entities": [{"text": "survey creation", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.8349188268184662}]}, {"text": "We further discover that abstracts are author-biased and thus complementary to the broader perspective inherent in citation texts; these differences enable the use of a range of different levels and types of information in the survey-the extent of which is subject to survey length restrictions (if any).", "labels": [], "entities": []}], "datasetContent": [{"text": "We automatically generated surveys for both QA and DP from three different types of documents: (1) full papers from the QA and DP sets-QA and DP full papers (PA), (2) only the abstracts of the QA and DP papers-QA and DP abstracts (AB), and (3) the citation texts corresponding to the QA and DP papers-QA and DP citations texts (CT).", "labels": [], "entities": []}, {"text": "We generated twenty four (4x3x2) surveys, each of length 250 words, by applying Trimmer, LexRank, C-LexRank and C-RR on the three data types (citation texts, abstracts, and full papers) for both QA and DP.", "labels": [], "entities": [{"text": "Trimmer", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.7330955862998962}, {"text": "LexRank", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.8635871410369873}]}, {"text": "shows a fragment of one of the surveys automatically generated from QA citation texts.)", "labels": [], "entities": [{"text": "QA citation texts", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.7924505273501078}]}, {"text": "We created six (3x2) additional 250-word surveys by randomly choosing sentences from the citation texts, abstracts, and full papers of QA and DP.", "labels": [], "entities": []}, {"text": "We will refer to them as random surveys.", "labels": [], "entities": []}, {"text": "Our goal was to determine if citation texts do indeed have useful information that one will want to put in a survey and if so, how much of this information is not available in the original papers and their abstracts.", "labels": [], "entities": []}, {"text": "For this we evaluated each of the automatically generated surveys using two separate approaches: nugget-based pyramid evaluation and ROUGE (described in the two subsections below).", "labels": [], "entities": [{"text": "nugget-based pyramid evaluation", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.5819397767384847}, {"text": "ROUGE", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9976516366004944}]}, {"text": "Two sets of gold standard data were manually created from the QA and DP citation texts and abstracts, respectively: 2 (1) We asked two impartial judges to identify important nuggets of information worth including in a survey.", "labels": [], "entities": [{"text": "QA and DP citation texts", "start_pos": 62, "end_pos": 86, "type": "DATASET", "confidence": 0.7294649600982666}]}, {"text": "(2) We asked four fluent speakers of English to create 250-word surveys of the datasets.", "labels": [], "entities": []}, {"text": "Then we determined how well the different automatically generated surveys perform against these gold standards.", "labels": [], "entities": []}, {"text": "If the citation texts have only redundant information with respect to the abstracts and original papers, then the surveys of citation texts will not perform better than others.", "labels": [], "entities": []}, {"text": "For our first approach we used a nugget-based evaluation methodology.", "labels": [], "entities": []}, {"text": "We asked three impartial annotators (knowledgeable in NLP but not affiliated with the project) to review the citation texts and/or abstract sets for each of the papers in the QA and DP sets and manually extract prioritized lists of 2-8 \"nuggets,\" or main contributions, supplied by each paper.", "labels": [], "entities": []}, {"text": "Each nugget was assigned a weight based on the frequency with which it was listed by annotators as well as the priority it was assigned in each case.", "labels": [], "entities": []}, {"text": "Our automatically generated surveys were then scored based on the number and weight of the nuggets that they covered.", "labels": [], "entities": []}, {"text": "This evaluation approach is similar to the one adopted by, but adapted here for use in the multi-document case.", "labels": [], "entities": []}, {"text": "The annotators had two distinct tasks for the QA set, and one for the DP set: (1) extract nuggets for each of the 10 QA papers, based only on the citation texts for those papers; (2) extract nuggets for each of the 10 QA papers, based only on the abstracts of those papers; and (3) extract nuggets for each of the 16 DP papers, based only on the citation texts for those papers.", "labels": [], "entities": []}, {"text": "We obtained a weight for each nugget by reversing its priority out of 8 (e.g., a nugget listed with priority 1 was assigned a weight of 8) and summing the weights over each listing of that nugget.", "labels": [], "entities": []}, {"text": "To evaluate a given survey, we counted the number and weight of nuggets that it covered.", "labels": [], "entities": []}, {"text": "Nuggets were detected via the combined use of annotatorprovided regular expressions and careful human review.", "labels": [], "entities": []}, {"text": "Recall was calculated by dividing the combined weight of covered nuggets by the combined weight of all nuggets in the nugget set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9803867936134338}]}, {"text": "Precision was calculated by dividing the number of distinct nuggets covered in a survey by the number of sentences constituting that survey, with a cap of 1.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9417085647583008}]}, {"text": "Fmeasure, the weighted harmonic mean of precision and recall, was calculated with a beta value of 3 in order to assign the greatest weight to recall.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.947070837020874}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9982308745384216}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9975778460502625}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9793452024459839}]}, {"text": "Recall is favored because it rewards surveys that include highly weighted (important) facts, rather than just a We first experimented using only the QA set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9504716396331787}]}, {"text": "Then to show that the results apply to other datasets, we asked human annotators for gold standard data on the DP citation texts.", "labels": [], "entities": [{"text": "DP citation texts", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.7377456625302633}]}, {"text": "Additional experiments on DP abstracts were not pursued because this would have required additional human annotation effort to establish a point we had already made with the QA set, i.e., that abstracts are useful for survey creation.", "labels": [], "entities": [{"text": "QA set", "start_pos": 174, "end_pos": 180, "type": "DATASET", "confidence": 0.7970231771469116}, {"text": "survey creation", "start_pos": 218, "end_pos": 233, "type": "TASK", "confidence": 0.7245425879955292}]}, {"text": "Results obtained with other weighting schemes that ignored priority ratings and multiple mentions of a nugget by a single annotator showed the same trends as the ones shown by the selected weighting scheme, but the latter was a stronger distinguisher among the four systems.: Pyramid F-measure scores of human-created surveys of QA and DP data.", "labels": [], "entities": []}, {"text": "The surveys are evaluated using nuggets drawn from QA citation texts (QA-CT), QA abstracts (QA-AB), and DP citation texts (DP-CT).", "labels": [], "entities": []}, {"text": "gives the F-measure values of the 250-word surveys manually generated by humans.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9980646967887878}]}, {"text": "The surveys were evaluated using the nuggets drawn from the QA citation texts, QA abstracts, and DP citation texts.", "labels": [], "entities": [{"text": "QA citation texts", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.867869238058726}, {"text": "DP citation texts", "start_pos": 97, "end_pos": 114, "type": "DATASET", "confidence": 0.7659376660982767}]}, {"text": "The average of their scores (listed in the rightmost column) maybe considered a good score to aim for by the automatic summarization methods.", "labels": [], "entities": [{"text": "summarization", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.9362517595291138}]}, {"text": "gives the F-measure values of the surveys generated by the four automatic summarizers, evaluated using nuggets drawn from the QA citation texts, QA abstracts, and DP citation texts.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9865038394927979}, {"text": "QA citation texts", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.8395334680875143}, {"text": "DP citation texts", "start_pos": 163, "end_pos": 180, "type": "DATASET", "confidence": 0.7072100043296814}]}, {"text": "The table also includes results for the baseline random summaries.", "labels": [], "entities": []}, {"text": "When we used the nuggets from the abstracts set for evaluation, the surveys created from abstracts scored higher than the corresponding surveys created from citation texts and papers.", "labels": [], "entities": []}, {"text": "Further, the best surveys generated from citation texts outscored the best surveys generated from papers.", "labels": [], "entities": []}, {"text": "When we used the nuggets from citation sets for evaluation, the best automatic surveys generated from citation texts outperform those generated from abstracts and full papers.", "labels": [], "entities": []}, {"text": "All these pyramid results demonstrate that citation texts can contain useful information that is not available in the abstracts or the original papers, and that abstracts can contain useful information that is not available in the citation texts or full papers.", "labels": [], "entities": []}, {"text": "Among the various automatic summarizers, Trimmer performed best at this task, in two cases exceeding the average human performance.", "labels": [], "entities": [{"text": "summarizers", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.8751745820045471}, {"text": "Trimmer", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.8675493597984314}]}, {"text": "Note also that the random summarizer outscored the automatic summarizers in cases where the nuggets were taken from a source different from that used to generate the survey.", "labels": [], "entities": []}, {"text": "However, one or two summarizers still tended to do well.", "labels": [], "entities": []}, {"text": "This indicates a difficulty in ex-: Pyramid F-measure scores of automatic surveys of QA and DP data.", "labels": [], "entities": []}, {"text": "The surveys are evaluated using nuggets drawn from QA citation texts (QA-CT), QA abstracts (QA-AB), and DP citation texts (DP-CT).", "labels": [], "entities": []}, {"text": "* LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences).: ROUGE-2 scores obtained for each of the manually created surveys by using the other three as reference.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 2, "end_pos": 9, "type": "DATASET", "confidence": 0.9668602347373962}, {"text": "DP-PA dataset", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9579776227474213}, {"text": "ROUGE-2", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.998420238494873}]}, {"text": "ROUGE-1 and ROUGE-L followed similar patterns.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7113862037658691}, {"text": "ROUGE-L", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.841413140296936}]}, {"text": "tracting the overlapping survey-worthy information across the two sources.", "labels": [], "entities": []}, {"text": "presents ROUGE scores) of each of human-generated 250-word surveys against each other.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 9, "end_pos": 21, "type": "METRIC", "confidence": 0.9639150202274323}]}, {"text": "The average (last column) is what the automatic surveys can aim for.", "labels": [], "entities": []}, {"text": "We then evaluated each of the random surveys and those generated by the four summarization systems against the references.", "labels": [], "entities": []}, {"text": "lists ROUGE scores of surveys when the manually created 250-word survey of the QA citation texts, survey of the QA abstracts, and the survey of the DP citation texts, were used as gold standard.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9427276849746704}, {"text": "QA citation texts", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.7554182211558024}, {"text": "DP citation texts", "start_pos": 148, "end_pos": 165, "type": "DATASET", "confidence": 0.7707798878351847}]}, {"text": "When we use manually created citation text surveys as reference, then the surveys generated from citation texts obtained significantly better ROUGE scores than the surveys generated from abstracts and full papers (p < 0.05).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.997925877571106}]}, {"text": "This shows that crucial survey-worthy information present in citation texts is not available, or hard to extract, from abstracts and papers alone.", "labels": [], "entities": []}, {"text": "Further, the surveys generated from abstracts performed significantly better than those generated from the full papers (p < 0.05).", "labels": [], "entities": []}, {"text": "This shows that abstracts and citation texts are generally denser in survey worthy information than full papers.", "labels": [], "entities": []}, {"text": "When we use manually created abstract surveys as reference, then the surveys generated from abstracts obtained significantly better ROUGE scores than the surveys generated from citation texts and full papers (p < 0.05).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.9960695505142212}]}, {"text": "Further, and more importantly, the surveys generated from citation texts performed significantly better than those generated from the full papers (p < 0.05)].", "labels": [], "entities": []}, {"text": "Again, this shows that abstracts and citation texts are richer in survey-worthy information.", "labels": [], "entities": []}, {"text": "These results also show that abstracts of papers and citation texts have some overlapping information (RESULT 2 and RESULT 4), but they also have a significant amount of unique survey-worthy information (RESULT 1 and RESULT 3).", "labels": [], "entities": []}, {"text": "Among the automatic summarizers, C-LexRank and LexRank perform best.", "labels": [], "entities": [{"text": "summarizers", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.8842149376869202}, {"text": "LexRank", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9468791484832764}]}, {"text": "This is unlike the results found through the nugget-evaluation method, where Trimmer performed best.", "labels": [], "entities": []}, {"text": "This suggests that Trim-: ROUGE-2 scores of automatic surveys of QA and DP data.", "labels": [], "entities": [{"text": "Trim-:", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.8254626194636027}, {"text": "ROUGE-2", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.5555905103683472}]}, {"text": "The surveys are evaluated by using human references created from QA citation texts (QA-CT), QA abstracts (QA-AB), and DP citation texts (DP-CT).", "labels": [], "entities": []}, {"text": "These results are obtained after Jack-knifing the human references so that the values can be compared to those in.", "labels": [], "entities": []}, {"text": "* LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences).", "labels": [], "entities": [{"text": "LexRank", "start_pos": 2, "end_pos": 9, "type": "DATASET", "confidence": 0.9683890342712402}, {"text": "DP full papers set", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.9187088757753372}]}, {"text": "mer is better at identifying more useful nuggets of information, but C-LexRank and LexRank are better at producing unigrams and bigrams expected in a survey.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9673622846603394}]}, {"text": "To some extent this maybe due to the fact that Trimmer uses smaller (trimmed) fragments of source sentences in its summaries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pyramid F-measure scores of human-created  surveys of QA and DP data. The surveys are evaluated  using nuggets drawn from QA citation texts (QA-CT),  QA abstracts (QA-AB), and DP citation texts (DP-CT).", "labels": [], "entities": []}, {"text": " Table 3: Pyramid F-measure scores of automatic surveys of QA and DP data. The surveys are evaluated using nuggets  drawn from QA citation texts (QA-CT), QA abstracts (QA-AB), and DP citation texts (DP-CT).  * LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences).", "labels": [], "entities": [{"text": "LexRank", "start_pos": 210, "end_pos": 217, "type": "DATASET", "confidence": 0.952040433883667}, {"text": "DP-PA dataset", "start_pos": 273, "end_pos": 286, "type": "DATASET", "confidence": 0.9511341750621796}]}, {"text": " Table 4: ROUGE-2 scores obtained for each of the manu- ally created surveys by using the other three as reference.  ROUGE-1 and ROUGE-L followed similar patterns.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9334847927093506}]}, {"text": " Table 5: ROUGE-2 scores of automatic surveys of QA and DP data. The surveys are evaluated by using human  references created from QA citation texts (QA-CT), QA abstracts (QA-AB), and DP citation texts (DP-CT). These  results are obtained after Jack-knifing the human references so that the values can be compared to those in", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8516898155212402}]}, {"text": " Table 4.  * LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences).", "labels": [], "entities": [{"text": "LexRank", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9575023055076599}, {"text": "DP full papers set", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.9092320948839188}]}]}