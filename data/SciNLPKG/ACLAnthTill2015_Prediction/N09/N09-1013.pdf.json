{"title": [{"text": "Context-Dependent Alignment Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Context-Dependent Alignment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7961479425430298}, {"text": "Statistical Machine Translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.843987762928009}]}], "abstractContent": [{"text": "We introduce alignment models for Machine Translation that take into account the context of a source word when determining its translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.85020512342453}]}, {"text": "Since the use of these contexts alone causes data sparsity problems , we develop a decision tree algorithm for clustering the contexts based on optimisation of the EM auxiliary function.", "labels": [], "entities": []}, {"text": "We show that our context-dependent models lead to an improvement in alignment quality, and an increase in translation quality when the alignments are used in Arabic-English and Chinese-English translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Alignment modelling for Statistical Machine Translation (SMT) is the task of determining translational correspondences between the words in pairs of sentences in parallel text.", "labels": [], "entities": [{"text": "Alignment modelling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9381773173809052}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.8413028915723165}, {"text": "translational correspondences between the words in pairs of sentences in parallel text", "start_pos": 89, "end_pos": 175, "type": "TASK", "confidence": 0.6487467736005783}]}, {"text": "Given a source language word sequence f J 1 and a target language word sequence e I 1 , we model the translation probability as P(e I 1 |f J 1 ) and introduce a hidden variable a I 1 representing a mapping from the target word positions to source word positions such that e i is aligned to f ai . Then P(e I 1 |f j 1 ) = \ud97b\udf59 a I 1 P(e I 1 , a I 1 |f j 1 ) (.", "labels": [], "entities": []}, {"text": "Previous work on statistical alignment modelling has not taken into account the source word context when determining translations of that word.", "labels": [], "entities": [{"text": "statistical alignment modelling", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.8726688226064047}]}, {"text": "It is intuitive that a word in one context, with a particular part-of-speech and particular words surrounding it, may translate differently when in a different context.", "labels": [], "entities": []}, {"text": "We aim to take advantage of this information to provide a better estimate of the word's translation.", "labels": [], "entities": [{"text": "word's translation", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7010642488797506}]}, {"text": "The challenge of incorporating context information is maintaining computational tractability of estimation and alignment, and we develop algorithms to overcome this.", "labels": [], "entities": [{"text": "alignment", "start_pos": 111, "end_pos": 120, "type": "TASK", "confidence": 0.6659106016159058}]}, {"text": "The development of efficient estimation procedures for context-dependent acoustic models revolutionised the field of Automatic Speech Recognition (ASR).", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 117, "end_pos": 151, "type": "TASK", "confidence": 0.816999355951945}]}, {"text": "Clustering is used extensively for improving parameter estimation of triphone (and higher order) acoustic models, enabling robust estimation of parameters and reducing the computation required for recognition.", "labels": [], "entities": []}, {"text": "introduce a binary treegrowing procedure for clustering Gaussian models for triphone contexts based on the value of a likelihood ratio.", "labels": [], "entities": []}, {"text": "We adopt a similar approach to estimate contextdependent translation probabilities.", "labels": [], "entities": []}, {"text": "We focus on alignment with IBM Model 1 and HMMs.", "labels": [], "entities": [{"text": "IBM Model 1 and HMMs", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.8471566557884216}]}, {"text": "HMMs are commonly used to generate alignments from which state of the art SMT systems are built.", "labels": [], "entities": [{"text": "SMT", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9909231066703796}]}, {"text": "Model 1 is used as an intermediate step in the creation of more powerful alignment models, such as HMMs and further IBM models.", "labels": [], "entities": []}, {"text": "In addition, it is used in SMT as a feature in Minimum Error Training () and for rescoring lattices of translation hypotheses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9954322576522827}, {"text": "rescoring lattices of translation hypotheses", "start_pos": 81, "end_pos": 125, "type": "TASK", "confidence": 0.7210378766059875}]}, {"text": "It is also used for lexically-weighted phrase extraction) and sentence segmentation of parallel text prior to machine translation.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7031248211860657}, {"text": "sentence segmentation of parallel text", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.8325521886348725}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.6529942750930786}]}], "datasetContent": [{"text": "Our models were built using the MTTK toolkit).", "labels": [], "entities": [{"text": "MTTK toolkit", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8925155401229858}]}, {"text": "Decision tree clustering was implemented and the process parallelised to enable thousands of decision trees to be built.", "labels": [], "entities": [{"text": "Decision tree clustering", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7502981225649515}]}, {"text": "Our context-dependent (CD) Model 1 models trained on context-annotated data were compared to the baseline context-independent (CI) models trained on untagged data.", "labels": [], "entities": []}, {"text": "The models were trained using data allowed for the NIST 08 Arabic-English evaluation 1 , excluding the UN collections, comprising 300k parallel sentence pairs, a total of 8.4M words of Arabic and 9.5M words of English.", "labels": [], "entities": [{"text": "NIST 08 Arabic-English evaluation 1", "start_pos": 51, "end_pos": 86, "type": "DATASET", "confidence": 0.9160259604454041}, {"text": "UN collections", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.7984749674797058}]}, {"text": "The Arabic language incorporates into its words several prefixes and suffixes which determine grammatical features such as gender, number, person and voice.", "labels": [], "entities": []}, {"text": "The MADA toolkit) was used to perform Arabic morphological word decomposition and part-of-speech tagging.", "labels": [], "entities": [{"text": "Arabic morphological word decomposition", "start_pos": 38, "end_pos": 77, "type": "TASK", "confidence": 0.5870553031563759}, {"text": "part-of-speech tagging", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7309898436069489}]}, {"text": "It determines the best analysis for each word in a sentence and splits word prefixes and suffixes, based on the alternative analyses provided by BAMA).", "labels": [], "entities": [{"text": "BAMA", "start_pos": 145, "end_pos": 149, "type": "DATASET", "confidence": 0.9154028296470642}]}, {"text": "We use tokenisation scheme 'D2', which splits certain prefixes and has been reported to improve machine translation performance).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7853488028049469}]}, {"text": "The alignment models are trained on this processed data, and the prefixes and suffixes are treated as words in their own right; in particular their contexts are examined and clustered.", "labels": [], "entities": []}, {"text": "The TnT tagger), used as distributed with its model trained on the Wall Street Journal portion of the Penn treebank, was used to obtain part-of-speech tags for the English side of the parallel text.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn treebank", "start_pos": 67, "end_pos": 115, "type": "DATASET", "confidence": 0.9528030902147293}]}, {"text": "gives a complete list of part-of-speech tags produced.", "labels": [], "entities": []}, {"text": "No morphological analysis is performed for English.", "labels": [], "entities": []}, {"text": "Automatic word alignments were compared to a manually-aligned corpus made up of the IBM ArabicEnglish Word Alignment Corpus () and the word alignment corpora LDC2006E86 and LDC2006E93.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.715028315782547}, {"text": "IBM ArabicEnglish Word Alignment Corpus", "start_pos": 84, "end_pos": 123, "type": "DATASET", "confidence": 0.6661244869232178}]}, {"text": "This contains 28k parallel text sentences pairs: 724k words of Arabic and 847k words of English.", "labels": [], "entities": []}, {"text": "The alignment links were modified to reflect the MADA tokenisation; after modification, there are 946k word-toword alignment links.", "labels": [], "entities": [{"text": "MADA tokenisation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6022853553295135}]}, {"text": "Alignment quality was evaluated by computing Alignment Error Rate (AER)) relative to the manual alignments.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9136481881141663}, {"text": "Alignment Error Rate (AER))", "start_pos": 45, "end_pos": 72, "type": "METRIC", "confidence": 0.9658251504103342}]}, {"text": "Since the links supplied contain only 'sure' links and no 'possible' links, we use the following formula for computing AER given reference alignment links Sand hypothesised alignment links A:  We have shown that the context-dependent models produce a decrease in AER measured on manually-aligned data; we wish to show this improved model performance leads to an increase in translation quality, measured by BLEU score ().", "labels": [], "entities": [{"text": "AER", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.8600520491600037}, {"text": "AER", "start_pos": 263, "end_pos": 266, "type": "METRIC", "confidence": 0.9941970109939575}, {"text": "BLEU score", "start_pos": 407, "end_pos": 417, "type": "METRIC", "confidence": 0.980188637971878}]}, {"text": "In addition to the Arabic systems already evaluated by AER, we also report results fora Chinese-English translation system.", "labels": [], "entities": [{"text": "AER", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.5749104022979736}]}, {"text": "Alignment models were evaluated by aligning the training data using the models in each translation direc-tion.", "labels": [], "entities": []}, {"text": "HiFST, a WFST-based hierarchical translation system described in (, was trained on the union of these alignments.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9748979806900024}, {"text": "WFST-based hierarchical translation", "start_pos": 9, "end_pos": 44, "type": "TASK", "confidence": 0.6218979358673096}]}, {"text": "MET was carried out using a development set, and the BLEU score evaluated on two test sets.", "labels": [], "entities": [{"text": "MET", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7162270545959473}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9989280104637146}]}, {"text": "Decoding used a 4-gram language model estimated from the English side of the entire MT08 parallel text, and a 965M word subset of monolingual data from the English Gigaword Third Edition.", "labels": [], "entities": [{"text": "MT08 parallel text", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.9156680107116699}, {"text": "English Gigaword Third Edition", "start_pos": 156, "end_pos": 186, "type": "DATASET", "confidence": 0.7930683493614197}]}, {"text": "For both Arabic and English, the CD HMM models were evaluated as follows.", "labels": [], "entities": []}, {"text": "Iteration 5 of the CI HMM was used to produce alignments for the parallel text training data: these were used to train the baseline system.", "labels": [], "entities": [{"text": "CI HMM", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.9225344657897949}]}, {"text": "The same data is aligned using CD HMMs after two further iterations of training and a second WFST-based translation system built from these alignments.", "labels": [], "entities": [{"text": "WFST-based translation", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.7447468340396881}]}, {"text": "The models are evaluated by comparing BLEU scores with those of the baseline model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9990878105163574}]}], "tableCaptions": [{"text": " Table 1: Most frequent root node context questions", "labels": [], "entities": []}, {"text": " Table 2: Words [number (percentage)] with context-dependent  translation for varying T imp", "labels": [], "entities": []}, {"text": " Table 3: Comparison, using BLEU score, of the CD HMM with  the baseline CI HMM", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9859260320663452}]}]}