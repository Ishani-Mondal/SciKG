{"title": [{"text": "Solving the \"Who's Mark Johnson\" Puzzle: Information Extraction Based Cross Document Coreference", "labels": [], "entities": [{"text": "Who's Mark Johnson\" Puzzle", "start_pos": 13, "end_pos": 39, "type": "DATASET", "confidence": 0.7303926249345144}]}], "abstractContent": [{"text": "Cross Document Coreference (CDC) is the problem of resolving the underlying identity of entities across multiple documents and is a major step for document understanding.", "labels": [], "entities": [{"text": "Cross Document Coreference (CDC)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8387685318787893}, {"text": "document understanding", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.6920178383588791}]}, {"text": "We develop a framework to efficiently determine the identity of a person based on extracted information, which includes unary properties such as gender and title, as well as binary relationships with other named entities such as co-occurrence and geo-locations.", "labels": [], "entities": []}, {"text": "At the heart of our approach is a suite of similarity functions (specialists) for matching relationships and a relational density-based clustering algorithm that delineates name clusters based on pairwise similarity.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our methods on the WePS benchmark datasets and point out future research directions.", "labels": [], "entities": [{"text": "WePS benchmark datasets", "start_pos": 55, "end_pos": 78, "type": "DATASET", "confidence": 0.9701110124588013}]}], "introductionContent": [{"text": "The explosive growth of web data offers users both the opportunity and the challenge to discover and integrate information from disparate sources.", "labels": [], "entities": []}, {"text": "As alluded to in the title, a search query of the common name \"Mark Johnson\" refers to as many as 70 namesakes in the top 100 search results from the Yahoo!", "labels": [], "entities": []}, {"text": "search engine, only one of whom is the Brown University professor and co-author of an ACL 2006 paper (see experiments).", "labels": [], "entities": []}, {"text": "Cross document coreference (CDC) () is a distinct technology that consolidates named entities across documents according to their real referents.", "labels": [], "entities": [{"text": "Cross document coreference (CDC)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7790234684944153}]}, {"text": "Despite the variety of styles and content in different text, CDC can break the boundaries of documents and cluster those mentions referring to the same * Contact author: jhuang@ist.psu.edu Mark Johnson.", "labels": [], "entities": []}, {"text": "As unambiguous person references are key to many tasks, e.g. social network analysis, this work focuses on person named entities.", "labels": [], "entities": [{"text": "social network analysis", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.6355692446231842}]}, {"text": "The method can be later extended to organizations.", "labels": [], "entities": []}, {"text": "We highlight the key differences between our proposed CDC system with past person name search systems.", "labels": [], "entities": []}, {"text": "First, we seek to transcend the simple bag of words approaches in earlier CDC work by leveraging state-of-the-art information extraction (IE) tools for disambiguation.", "labels": [], "entities": [{"text": "state-of-the-art information extraction (IE)", "start_pos": 97, "end_pos": 141, "type": "TASK", "confidence": 0.8021020988623301}]}, {"text": "The main advantage is that our IE based approach has access to accurate information such as a person's work titles, geo-locations, relationships and other attributes.", "labels": [], "entities": [{"text": "IE", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9638951420783997}]}, {"text": "Traditional IR approaches, on the other hand, may naively use the terms in a document which can significantly hamper accuracy.", "labels": [], "entities": [{"text": "IR", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.962196946144104}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9907246232032776}]}, {"text": "For instance, an article about Hillary Clinton may contain references to journalists, politicians who make comments about her.", "labels": [], "entities": []}, {"text": "Even with careful word selection, such textual features may still confuse the disambiguation system about the true identity of the person.", "labels": [], "entities": []}, {"text": "The information extraction process in our work can thus be regarded as an intelligent feature selection step for disambiguation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8047685325145721}]}, {"text": "Second, after coreferencing, our system not only yields clusters of documents, but also structured information which is highly useful for automated document understanding and data mining.", "labels": [], "entities": [{"text": "automated document understanding", "start_pos": 138, "end_pos": 170, "type": "TASK", "confidence": 0.605710635582606}, {"text": "data mining", "start_pos": 175, "end_pos": 186, "type": "TASK", "confidence": 0.7673725783824921}]}, {"text": "We review related work on CDC next and describe our approach in Section 3.", "labels": [], "entities": []}, {"text": "The methods are evaluated on benchmark datasets in Section 4.", "labels": [], "entities": []}, {"text": "We discuss directions for future improvement in Section 5 and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first formally define the evaluation metrics, followed by the introduction to the benchmark test sets and the system's performance.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our method using the standard purity and inverse purity clustering metrics.", "labels": [], "entities": []}, {"text": "Leta set of clusters C = {C 1 , ..., C s } denote the system's output and a set of categories D = {D 1 , ..., D t } be the gold standard.", "labels": [], "entities": []}, {"text": "Both C and Dare partitions of the WDC chains {w 1 , ..., w n } (n = i |C i | = j |D j |).", "labels": [], "entities": []}, {"text": "First, the precision of a cluster Ci w.r.t. a category D j is defined as, Purity is defined as the weighted average of the maximum precision achieved by the clusters on one of the categories, Hence purity penalizes putting noise WDC chains in a cluster.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.996743381023407}, {"text": "Purity", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9894227385520935}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9739812016487122}]}, {"text": "Trivially, the maximum purity (i.e. 1) can be achieved by making one cluster per WDC chain (referred to as the one-in-one baseline).", "labels": [], "entities": []}, {"text": "Reversing the role of clusters and categories, Inverse purity(C, D) def = Purity(D, C).", "labels": [], "entities": [{"text": "Inverse purity", "start_pos": 47, "end_pos": 61, "type": "METRIC", "confidence": 0.9362285137176514}]}, {"text": "Inverse Purity penalizes splitting WDC chains belonging to the same category into different clusters.", "labels": [], "entities": []}, {"text": "The maximum inverse purity can be achieved by putting all chain in one cluster (all-in-one baseline).", "labels": [], "entities": [{"text": "inverse purity", "start_pos": 12, "end_pos": 26, "type": "METRIC", "confidence": 0.8501248359680176}]}, {"text": "Purity and inverse purity are similar to the precision and recall measures commonly used in information retrieval.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9812063574790955}, {"text": "inverse purity", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9036129415035248}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9992430210113525}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9798422455787659}]}, {"text": "There is a tradeoff relationship between the two and their harmonic mean F 0.5 is used for performance evaluation.", "labels": [], "entities": [{"text": "harmonic mean F 0.5", "start_pos": 59, "end_pos": 78, "type": "METRIC", "confidence": 0.69753497838974}]}, {"text": "We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS hereafter) (.", "labels": [], "entities": [{"text": "ACL SemEval-2007 web person search task", "start_pos": 69, "end_pos": 108, "type": "DATASET", "confidence": 0.8734693626562754}]}, {"text": "The test collection consists of three sets of documents for 10 different names, sampled from the English Wikipedia (famous people), participants of the ACL 2006 conference (computer scientists) and common names from the US Census data, respectively.", "labels": [], "entities": [{"text": "US Census data", "start_pos": 220, "end_pos": 234, "type": "DATASET", "confidence": 0.7185412347316742}]}, {"text": "For each ambiguous name, the top 100 documents retrieved from the Yahoo!", "labels": [], "entities": []}, {"text": "Search API were annotated by human annotators according to the actual entity of the name.", "labels": [], "entities": []}, {"text": "This yields on average 45 different real world entities per set and about 3k documents in total.", "labels": [], "entities": []}, {"text": "We note that the annotation in WePS makes the simplifying assumption that each document refers to only one real world person among the namesakes in question.", "labels": [], "entities": [{"text": "WePS", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8970073461532593}]}, {"text": "The CDC task in the perspective of this paper, however, is to merge the WDC chains rather than documents.", "labels": [], "entities": []}, {"text": "Hence in our evaluation, we adopt the document label to annotate the WDC chain from the document that corresponds to the person name search query.", "labels": [], "entities": []}, {"text": "Despite the difference, the results of the one-in-one and all-in-one baselines are almost identical to those reported in the WePS evaluation (F 0.5 = 0.61, 0.40 respectively).", "labels": [], "entities": [{"text": "WePS evaluation", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.9223071336746216}, {"text": "F 0.5", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.9775357246398926}]}, {"text": "Hence the performance reported here is comparable to the official evaluation results).", "labels": [], "entities": []}, {"text": "We computed the similarity features from the WDC chains extracted from the WePS training data and subsampled the non-coreferent pairs to generate a.", "labels": [], "entities": [{"text": "WePS training data", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.9248109658559164}]}, {"text": "The F 0.5 score of our CDC system (AT-CDC) is 0.708, comparable to the test results of the first tier systems in the official evaluation.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9842232267061869}]}, {"text": "The two baselines are also included.", "labels": [], "entities": []}, {"text": "Because the test set is very ambiguous (on average only two documents per real world entity), the one-in-one baseline has relatively high F 0.5 score.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 138, "end_pos": 149, "type": "METRIC", "confidence": 0.9893117149670919}]}, {"text": "The Wikipedia, ACL06 and US Census sets have on average 56, 31 and 50 entities per name respectively.", "labels": [], "entities": [{"text": "US Census sets", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9220929741859436}]}, {"text": "We notice that as the data set becomes more ambiguous, purity decreases implying it's harder for the system to discard irrelevant documents from a cluster.", "labels": [], "entities": [{"text": "purity", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9987943172454834}]}, {"text": "The other case is true for inverse purity.", "labels": [], "entities": []}, {"text": "In particular, we are interested in how the coreference performance changes with the number of entities per name (which can be viewed as the ambiguity level of a data set).", "labels": [], "entities": []}, {"text": "We observe that in general the harmonic mean of the purity is fairly stable across different number of entities per dataset (generally within the band between 0.6 and 0.8).", "labels": [], "entities": []}, {"text": "This is important because the system's performance does not vary greatly with the underlying data characteristics.", "labels": [], "entities": []}, {"text": "There is a particular name (with only one underlying referent) that appears to bean outlier in performance in.", "labels": [], "entities": []}, {"text": "After examining the extraction results, we notice that the extracted relationships refer to the same person's employment, coauthors and geolocations.", "labels": [], "entities": []}, {"text": "The generated CDC clusters correctly reflect the different aspects of the person but the system is unable to link them together due to the lack of information for merging.", "labels": [], "entities": []}, {"text": "This motivates us to further improve performance in future work.", "labels": [], "entities": []}, {"text": "shows how the coreference performance  changes with different density parameter . We observe that as we increase the size of the neighborhood, inverse purity increases indicating that more correct coreference decisions are made.", "labels": [], "entities": [{"text": "inverse purity", "start_pos": 143, "end_pos": 157, "type": "METRIC", "confidence": 0.715543121099472}]}, {"text": "On the other hand, purity decreases as more noise WDC chains appear in clusters.", "labels": [], "entities": [{"text": "purity", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9928883910179138}]}, {"text": "Due to this tradeoff relationship, the F score is fairly stable with a wide range of values and hence the density parameter is rather insensitive (compared to, say, the number of clusters K).", "labels": [], "entities": [{"text": "F score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9862589836120605}]}], "tableCaptions": [{"text": " Table 1: Cross document coreference performance  (macro-averaged scores, I-Pur denotes inverse purity).", "labels": [], "entities": [{"text": "inverse purity", "start_pos": 88, "end_pos": 102, "type": "METRIC", "confidence": 0.8306035995483398}]}, {"text": " Table 1. The F 0.5 score of our  CDC system (AT-CDC) is 0.708, comparable to the  test results of the first tier systems in the official  evaluation. The two baselines are also included.  Because the test set is very ambiguous (on average  only two documents per real world entity), the  one-in-one baseline has relatively high F 0.5 score.  The Wikipedia, ACL06 and US Census sets  have on average 56, 31 and 50 entities per name  respectively. We notice that as the data set becomes  more ambiguous, purity decreases implying  it's harder for the system to discard irrelevant  documents from a cluster. The other case is true  for inverse purity. In particular, we are interested in  how the coreference performance changes with the  number of entities per name (which can be viewed  as the ambiguity level of a data set). This is shown  in", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9786843061447144}, {"text": "F 0.5 score", "start_pos": 329, "end_pos": 340, "type": "METRIC", "confidence": 0.9755143523216248}, {"text": "US Census sets", "start_pos": 368, "end_pos": 382, "type": "DATASET", "confidence": 0.8980288505554199}, {"text": "purity", "start_pos": 503, "end_pos": 509, "type": "METRIC", "confidence": 0.9871169924736023}]}]}