{"title": [{"text": "Sentence Realisation from Bag of Words with dependency constraints", "labels": [], "entities": [{"text": "Sentence Realisation from Bag of Words", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8656650483608246}]}], "abstractContent": [{"text": "In this paper, we present five models for sentence realisation from a bag-of-words containing minimal syntactic information.", "labels": [], "entities": [{"text": "sentence realisation", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7671535611152649}]}, {"text": "It has a large variety of applications ranging from Machine Translation to Dialogue systems.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8306103348731995}]}, {"text": "Our models employ simple and efficient techniques based on n-gram Language modeling.", "labels": [], "entities": []}, {"text": "We evaluated the models by comparing the synthesized sentences with reference sentences using the standard BLEU metric(Papineni et al., 2001).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.997793436050415}]}, {"text": "We obtained higher results (BLEU score of 0.8156) when compared to the state-of-art results.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9833073318004608}]}, {"text": "In future , we plan to incorporate our sentence realiser in Machine Translation and observe its effect on the translation accuracies.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.6953172981739044}]}], "introductionContent": [{"text": "In applications such as Machine Translation (MT) and Dialogue Systems, sentence realisation is a major step.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.8599795937538147}, {"text": "sentence realisation", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.850677490234375}]}, {"text": "Sentence realisation involves generating a well-formed sentence from a bag of lexical items.", "labels": [], "entities": [{"text": "Sentence realisation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9460380971431732}]}, {"text": "These lexical items maybe syntactically related to each other.", "labels": [], "entities": []}, {"text": "The level of syntactic information attached to the lexical items might vary with application.", "labels": [], "entities": []}, {"text": "In order to appeal to the wide range of applications that use sentence realisation, our experiments assume only basic syntactic information, such as unlabeled dependency relationships between the lexical items.", "labels": [], "entities": [{"text": "sentence realisation", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.7275466620922089}]}, {"text": "In this paper, we present different models for sentence realisation.", "labels": [], "entities": [{"text": "sentence realisation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8007596433162689}]}, {"text": "These models consider a bag of words with unlabelled dependency relations as input and apply simple n-gram language modeling techniques to get a well-formed sentence.", "labels": [], "entities": []}, {"text": "We now present the role of a sentence realiser in the task of MT.", "labels": [], "entities": [{"text": "sentence realiser", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7358180284500122}, {"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9923213124275208}]}, {"text": "In transfer-based approaches for MT 1 (, the source sentence is first analyzed by a parser (a phrase-structure or a dependency-based parser).", "labels": [], "entities": [{"text": "MT 1", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.960561603307724}]}, {"text": "Then the source lexical items are transferred to the target language using a bi-lingual dictionary.", "labels": [], "entities": []}, {"text": "The target language sentence is finally realised by applying transfer-rules that map the grammar of both the languages.", "labels": [], "entities": []}, {"text": "Generally, these transfer rules make use of rich analysis on the source side such as dependency labels etc.", "labels": [], "entities": []}, {"text": "The accuracy of having such rich analysis (dependency labeling ) is low and hence, might affect the performance of the sentence realiser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999397873878479}]}, {"text": "Also, the approach of manually constructing transfer rules is costly, especially for divergent language pairs such as English and Hindi or English and Japanese.", "labels": [], "entities": []}, {"text": "Our models can be used in this scenario, providing a robust alternative to the transfer rules.", "labels": [], "entities": []}, {"text": "A sentence realiser can also be used in the framework of a two-step statistical machine translation.", "labels": [], "entities": [{"text": "sentence realiser", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7328500151634216}, {"text": "statistical machine translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6206429600715637}]}, {"text": "In the two-step framework, the semantic transfer and sentence realisation are decoupled into independent modules.", "labels": [], "entities": [{"text": "semantic transfer", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7172973155975342}, {"text": "sentence realisation", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.6988673806190491}]}, {"text": "This provides an opporunity to develop simple and efficient modules for each of the steps.", "labels": [], "entities": []}, {"text": "The model for Global Lexical Selection and Sentence Re-construction ( ) is one such approach.", "labels": [], "entities": [{"text": "Global Lexical Selection and Sentence Re-construction", "start_pos": 14, "end_pos": 67, "type": "TASK", "confidence": 0.6506687899430593}]}, {"text": "In this approach, discriminative techniques are used to first transfer semantic information of the source sentence by looking at the source sentence globally, this obtaining a accurate bag-of-words in the target language.", "labels": [], "entities": []}, {"text": "The words in the bag might be attached with mild syntactic information (ie., the words they modify).", "labels": [], "entities": []}, {"text": "We propose models that take this information as input and produce the target sentence.", "labels": [], "entities": []}, {"text": "We can also use our sentence realiser as an ordering module in other approaches such as, where the goal is to order an unordered bag (of treelets in this case) with dependency links.", "labels": [], "entities": []}, {"text": "In Natural Language Generation applications such as Dialogue systems etc, the set of concepts and the dependencies between the concepts is obtained first which is known as text planning.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6736848950386047}, {"text": "text planning", "start_pos": 172, "end_pos": 185, "type": "TASK", "confidence": 0.7646259963512421}]}, {"text": "These concepts are then realized into words resulting in a bag of words with syntactic relations).", "labels": [], "entities": []}, {"text": "This is known as sentence planning.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8024766743183136}]}, {"text": "In the end, the surface string can be obtained by our models.", "labels": [], "entities": []}, {"text": "In this paper, we do not test our models with any of the applications mentioned above.", "labels": [], "entities": []}, {"text": "However, we plan to test our models with these applications, especially on the two-stage statistical MT approach using the bag-of-words obtained by Global Lexical Selection ( ,.", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9214743971824646}]}, {"text": "Here, we test our models independent of any application, by beginning with a given bag-of-words (with dependency links).", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows.", "labels": [], "entities": []}, {"text": "We give an overview of the related work in section 2.", "labels": [], "entities": []}, {"text": "In section 3, we talk about the effect of dependency constraints and gives details of the experimental setup in section 4.", "labels": [], "entities": []}, {"text": "In section 5, we describe about the experiments that have been conducted.", "labels": [], "entities": []}, {"text": "In section 6, our experimental results are presented.", "labels": [], "entities": []}, {"text": "In section 7, we talk about the possible future work and we conclude with section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments, we use the WSJ portion of the Penn tree bank (, using the standard train/development/test splits, viz 39,832 sentences from 2-21 sections, 2416 sentences from section 23 for testing and 1,700 sentences from section 22 for development.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.9394553303718567}, {"text": "Penn tree bank", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9214602907498678}]}, {"text": "The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn treebank using head percolation rules used in, which do not contain any order information.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.9901280105113983}]}, {"text": "We also use the provided part-ofspeech tags in some experiments.", "labels": [], "entities": []}, {"text": "Ina typical application, the input to the sentence realiser is noisy.", "labels": [], "entities": []}, {"text": "To test the robustness of our models in such scenarios, we also conduct experiments with noisy input data.", "labels": [], "entities": []}, {"text": "We parse the test data with an unlabelled projective dependency parser () and drop the order information to obtain the input to our sentence realiser.", "labels": [], "entities": []}, {"text": "However we still use the correct bag of words.", "labels": [], "entities": []}, {"text": "We propose to test this aspect in future by plugging our sentence realiser in Machine Translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.688812330365181}]}, {"text": "shows the number of nodes having a particular number of children in the test data.", "labels": [], "entities": []}, {"text": "The task here is to realise a well formed sentence from a bag of words with dependency constraints (unordered dependency tree) for which we propose five models using n-gram based Language modeling techinque.", "labels": [], "entities": []}, {"text": "We train the language models of order 3 using Good-Turning smoothing on the training data of Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.957772433757782}]}, {"text": "Although the results of the proposed models are much higher when compared to other methods, the major constraint with our models is the computational complexity, which is O(n!).", "labels": [], "entities": [{"text": "O", "start_pos": 171, "end_pos": 172, "type": "METRIC", "confidence": 0.9879698753356934}]}, {"text": "However, our approach is still tractable because of the low values of n.", "labels": [], "entities": []}, {"text": "We plan to reduce the search space complexity by using Viterbi search (, and examine the drop in results because of that.", "labels": [], "entities": []}, {"text": "The models proposed in paper, consider only the locally best phrases (local to the sub-tree) at every step.", "labels": [], "entities": []}, {"text": "In order to retain the globally best possibilities at every step, we plan to use beam search, where we retain K-best best phrases for every sub-tree.", "labels": [], "entities": []}, {"text": "Also, the goal is to test the approach for morphologically-rich languages such as Hindi.", "labels": [], "entities": []}, {"text": "Also, it would require us to expand our features set.", "labels": [], "entities": []}, {"text": "We also plan to test the factored models.", "labels": [], "entities": []}, {"text": "The most important experiment that we plan to perform is to test our system in the context of MT, where the input is more real and noisy.", "labels": [], "entities": [{"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9686824679374695}]}, {"text": "To train more robust language models, we plan to use the much larger data on a web scale.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of nodes having a particular  number of children in the test data", "labels": [], "entities": []}, {"text": " Table 2: The results of Model 1-5", "labels": [], "entities": []}]}