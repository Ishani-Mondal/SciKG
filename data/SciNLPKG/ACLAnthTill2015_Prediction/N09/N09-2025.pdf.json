{"title": [{"text": "Learning Combination Features with L 1 Regularization", "labels": [], "entities": [{"text": "Regularization", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.6430671811103821}]}], "abstractContent": [{"text": "When linear classifiers cannot successfully classify data, we often add combination features , which are products of several original features.", "labels": [], "entities": []}, {"text": "The searching for effective combination features, namely feature engineering, requires domain-specific knowledge and hard work.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7819118797779083}]}, {"text": "We present herein an efficient algorithm for learning an L 1 regularized logistic regression model with combination features.", "labels": [], "entities": []}, {"text": "We propose to use the grafting algorithm with efficient computation of gradients.", "labels": [], "entities": []}, {"text": "This enables us to find optimal weights efficiently without enumerating all combination features.", "labels": [], "entities": []}, {"text": "By using L 1 regularization, the result we obtain is very compact and achieves very efficient inference.", "labels": [], "entities": []}, {"text": "In experiments with NLP tasks, we show that the proposed method can extract effective combination features, and achieve high performance with very few features.", "labels": [], "entities": []}], "introductionContent": [{"text": "A linear classifier is a fundamental tool for many NLP applications, including logistic regression models (LR), in that its score is based on a linear combination of features and their weights,.", "labels": [], "entities": []}, {"text": "Although a linear classifier is very simple, it can achieve high performance on many NLP tasks, partly because many problems are described with very high-dimensional data, and high dimensional weight vectors are effective in discriminating among examples.", "labels": [], "entities": []}, {"text": "However, when an original problem cannot be handled linearly, combination features are often added to the feature set, where combination features are products of several original features.", "labels": [], "entities": []}, {"text": "Examples of combination features are, word pairs in document classification, or part-of-speech pairs of head and modifier words in a dependency analysis task.", "labels": [], "entities": [{"text": "document classification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7386362254619598}, {"text": "dependency analysis task", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.7919236024220785}]}, {"text": "However, the task of determining effective combination features, namely feature engineering, requires domain-specific knowledge and hard work.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7610155940055847}]}, {"text": "Such a non-linear phenomenon can be implicitly captured by using the kernel trick.", "labels": [], "entities": []}, {"text": "However, its computational cost is very high, not only during training but also at inference time.", "labels": [], "entities": []}, {"text": "Moreover, the model is not interpretable, in that effective features are not represented explicitly.", "labels": [], "entities": []}, {"text": "Many kernels methods assume an L 2 regularizer, in that many features are equally relevant to the tasks).", "labels": [], "entities": []}, {"text": "There have been several studies to find efficient ways to obtain (combination) features.", "labels": [], "entities": []}, {"text": "In the context of boosting, have proposed a method to extract complex features that is similar to the item set mining algorithm.", "labels": [], "entities": [{"text": "boosting", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.965914249420166}, {"text": "item set mining", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.6901577115058899}]}, {"text": "In the context of L 1 regularization.,, and Tsuda (2007) have also proposed methods by which effective features are extracted from huge sets of feature candidates.", "labels": [], "entities": []}, {"text": "However, their methods are still very computationally expensive, and we cannot directly apply this kind of method to a large-scale NLP problem.", "labels": [], "entities": []}, {"text": "In the present paper, we propose a novel algorithm for learning of an L 1 regularized LR with combination features.", "labels": [], "entities": []}, {"text": "In our algorithm, we can exclusively extract effective combination features without enumerating all of the candidate features.", "labels": [], "entities": []}, {"text": "Our method relies on a grafting algorithm, which incrementally adds features like boosting, but it can converge to the global optimum.", "labels": [], "entities": []}, {"text": "We use L 1 regularization because we can obtain a sparse parameter vector, for which many of the parameter values are exactly zero.", "labels": [], "entities": []}, {"text": "In other words, learning with L 1 regularization naturally has an intrinsic effect of feature selection, which results in an efficient and interpretable inference with almost the same performance as L 2 regularization (.", "labels": [], "entities": []}, {"text": "The heart of our algorithm is away to find a feature that has the largest gradient value of likelihood from among the huge set of candidates.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9295291900634766}]}, {"text": "To solve this problem, we propose an example-wise algorithm with filtering.", "labels": [], "entities": []}, {"text": "This algorithm is very simple and easy to implement, but effective in practice.", "labels": [], "entities": []}, {"text": "We applied the proposed methods to NLP tasks, and found that our methods can achieve the same high performance as kernel methods, whereas the number of active combination features is relatively small, such as several thousands.", "labels": [], "entities": []}], "datasetContent": [{"text": "To measure the effectiveness of the proposed method (called L 1 -Comb), we conducted experiments on the dependency analysis task, and the document classification task.", "labels": [], "entities": [{"text": "dependency analysis task", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.8476778467496237}, {"text": "document classification task", "start_pos": 138, "end_pos": 166, "type": "TASK", "confidence": 0.8328282237052917}]}, {"text": "In all experiments, the parameter C was tuned using the development data set.", "labels": [], "entities": [{"text": "development data set", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.7888941764831543}]}, {"text": "In the first experiment, we performed Japanese dependency analysis.", "labels": [], "entities": [{"text": "Japanese dependency analysis", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.5973343054453532}]}, {"text": "We used the Kyoto Text Corpus (Version 3.0), Jan. 1, 3-8 as the training data, Jan. 10 as the development data, and Jan. 9 as the test data so that the result could be compared to those from previous studies) 2 . We used the shift-reduce dependency algorithm).", "labels": [], "entities": [{"text": "Kyoto Text Corpus", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.9844627777735392}]}, {"text": "The number of training events was 11, 3332, each of which consisted of two word positions as inputs, and y = {0, 1} as an output indicating the dependency relation.", "labels": [], "entities": []}, {"text": "For the training data, the number of original features was 78570, and the number of combination features of degrees 2 and 3 was 5787361, and 169430335, respectively.", "labels": [], "entities": []}, {"text": "Note that we need not see all of them using our algorithm.", "labels": [], "entities": []}, {"text": "In all experiments, combination features of degrees 2 and 3 (the products of two or three original features) were used.", "labels": [], "entities": []}, {"text": "We compared our methods using LR with L 1 regularization using original features (L 1 -Original), SVM with a 3rd-polynomial Kernel, LR with L 2 regularization using combination features with up to 3 combinations (L 2 -Comb3), and an averaged perceptron with original features (Ave. Perceptron).", "labels": [], "entities": []}, {"text": "shows the result of the Japanese dependency task.", "labels": [], "entities": []}, {"text": "The accuracy result indicates that the accuracy was improved with automatically extracted combination features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997109770774841}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9997150301933289}]}, {"text": "In the column of active features, the number of active features is listed.", "labels": [], "entities": []}, {"text": "This indicates that L 1 regularization automatically selects very few effective features.", "labels": [], "entities": []}, {"text": "Note that, in training, L 1 -Comb used around 100 MB, while L 2 -Comb3 used more than 30 GB.", "labels": [], "entities": []}, {"text": "The most time consuming part for L 1 -Comb was the optimization of the L 1 -LR problem.", "labels": [], "entities": []}, {"text": "Examples of extracted combination features include POS pairs of head and modifiers, such as Head/Noun-Modifier/Noun, and combinations of distance features with the POS of head.", "labels": [], "entities": []}, {"text": "For the second experiment, we performed the document classification task using the Tech-TC-300 data set ( . We used the tf-idf scores as feature values.", "labels": [], "entities": [{"text": "document classification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.8167905211448669}, {"text": "Tech-TC-300 data set", "start_pos": 83, "end_pos": 103, "type": "DATASET", "confidence": 0.9784816304842631}]}, {"text": "We did not filter out any words beforehand.", "labels": [], "entities": []}, {"text": "The Tech-TC-300 data set consists of 295 binary classification tasks.", "labels": [], "entities": [{"text": "Tech-TC-300 data set", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9731094241142273}]}, {"text": "We divided each document set into a training and a test set.", "labels": [], "entities": []}, {"text": "The ratio of the test set to the training set was 1 : 4.", "labels": [], "entities": []}, {"text": "The average number of features for tasks was 25, 389.", "labels": [], "entities": []}, {"text": "shows the results for L 1 -LR with combination features and SVM with linear kernel . The results indicate that the combination features are effective.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performance of the Japanese dependency  task on the Test set. The active features column shows  the number of nonzero weight features.", "labels": [], "entities": [{"text": "Test set", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.8836458921432495}]}, {"text": " Table 2: Document classification results for the Tech-TC- 300 data set. The column F 2 shows the average of F 2  scores for each method of classification.", "labels": [], "entities": [{"text": "Document classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7552992105484009}, {"text": "Tech-TC- 300 data set", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.9745869755744934}, {"text": "F 2  scores", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.9806924859682719}]}]}