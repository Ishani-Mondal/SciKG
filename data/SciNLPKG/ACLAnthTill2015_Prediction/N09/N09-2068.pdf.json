{"title": [{"text": "Score Distribution Based Term Specific Thresholding for Spoken Term Detection", "labels": [], "entities": [{"text": "Spoken Term Detection", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.8348676562309265}]}], "abstractContent": [{"text": "The spoken term detection (STD) task aims to return relevant segments from a spoken archive that contain the query terms.", "labels": [], "entities": [{"text": "spoken term detection (STD)", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8024323980013529}]}, {"text": "This paper focuses on the decision stage of an STD system.", "labels": [], "entities": [{"text": "STD system", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.9155521988868713}]}, {"text": "We propose a term specific threshold-ing (TST) method that uses per query posterior score distributions.", "labels": [], "entities": []}, {"text": "The STD system described in this paper indexes word-level lattices produced by an LVCSR system using Weighted Finite State Transducers (WFSTs).", "labels": [], "entities": [{"text": "STD", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9390996098518372}]}, {"text": "The target application is a sign dictionary where precision is more important than recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9989799857139587}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9984118938446045}]}, {"text": "Experiments compare the performance of different thresholding techniques.", "labels": [], "entities": []}, {"text": "The proposed approach increases the maximum precision attainable by the system.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9990115165710449}]}], "introductionContent": [{"text": "The availability of vast multimedia archives calls for solutions to efficiently search this data.", "labels": [], "entities": []}, {"text": "Multimedia content also enables interesting applications which utilize multiple modalities, such as speech and video.", "labels": [], "entities": []}, {"text": "Spoken term detection (STD) is a subfield of speech retrieval, which locates occurrences of a query in a spoken archive.", "labels": [], "entities": [{"text": "Spoken term detection (STD)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8849962751070658}, {"text": "speech retrieval", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7089860737323761}]}, {"text": "In this work, STD is used as a tool to segment and retrieve the signs in news videos for the hearing impaired based on speech information.", "labels": [], "entities": [{"text": "STD", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9550814032554626}]}, {"text": "After the location of the query is extracted with STD, the sign video corresponding to that time interval is displayed to the user.", "labels": [], "entities": []}, {"text": "In addition to being used as a sign language dictionary this approach can also be used to automatically create annotated sign databases that can be utilized for training sign recognizers (.", "labels": [], "entities": [{"text": "training sign recognizers", "start_pos": 161, "end_pos": 186, "type": "TASK", "confidence": 0.6782795290152231}]}, {"text": "For these applications the precision of the system is more important than its recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993312358856201}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9985939860343933}]}, {"text": "The classical STD approach consists of converting the speech to word transcripts using large vocabulary continuous speech recognition (LVCSR) tools and extending classical information retrieval techniques to word transcripts.", "labels": [], "entities": [{"text": "STD", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9775813817977905}, {"text": "large vocabulary continuous speech recognition (LVCSR)", "start_pos": 87, "end_pos": 141, "type": "TASK", "confidence": 0.7542549893260002}]}, {"text": "However, retrieval performance is highly dependent on the recognition errors.", "labels": [], "entities": []}, {"text": "In this context, lattice indexing provides a means of reducing the effect of recognition errors by incorporating alternative transcriptions in a probabilistic framework.", "labels": [], "entities": [{"text": "lattice indexing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7383264899253845}]}, {"text": "A system using lattices can also return the posterior probability of a query as a detection score.", "labels": [], "entities": []}, {"text": "Various operating points can be obtained by comparing the detection scores to a threshold.", "labels": [], "entities": []}, {"text": "In addition to using a global detection threshold, choosing term specific thresholds that optimize the STD evaluation metric known as Term-Weighted Value (TWV) was recently proposed.", "labels": [], "entities": [{"text": "STD evaluation", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.8396681547164917}, {"text": "Term-Weighted Value (TWV)", "start_pos": 134, "end_pos": 159, "type": "METRIC", "confidence": 0.7916609644889832}]}, {"text": "A similar approach which trains a neural network mapping various features to the target classes was used in ().", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we explain the methods used for spoken term detection.", "labels": [], "entities": [{"text": "spoken term detection", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7716598510742188}]}, {"text": "These include the indexing and search framework based on WFSTs and the detection framework based on posterior score distributions.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9408706426620483}]}, {"text": "In Section 3 we describe our experimental setup and present the results.", "labels": [], "entities": []}, {"text": "Finally, in Section 4 we summarize our contributions and discuss possible future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use precision and recall as the primary evaluation metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993865489959717}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9981551766395569}]}, {"text": "For a set of queries q k , k = 1, . .", "labels": [], "entities": []}, {"text": ", Q, where: R(q k ): Number of occurences of query q k , A(q k ): Total no. of retrieved documents for q k , C(q k ): No. of correctly retrieved documents for q k . We obtain a precision/recall curve by changing the free parameter associated with each thresholding method to simulate different decision cost settings.", "labels": [], "entities": [{"text": "R", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9511507153511047}, {"text": "A", "start_pos": 57, "end_pos": 58, "type": "METRIC", "confidence": 0.9847773313522339}, {"text": "precision", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9987589120864868}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9332724213600159}]}, {"text": "Right end of these curves fall into the high precision region which is the main concern in our application.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9858638644218445}]}, {"text": "For the case of global thresholding (GT), the same threshold \u03b8 is used for all queries.", "labels": [], "entities": [{"text": "global thresholding (GT)", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7187265813350677}]}, {"text": "TWV based term specific thresholding (TWV-TST) aims to maximize the TWV metric introduced during NIST 2006 STD Evaluations).", "labels": [], "entities": [{"text": "TWV based term specific thresholding", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5184674501419068}, {"text": "NIST 2006 STD Evaluations", "start_pos": 97, "end_pos": 122, "type": "DATASET", "confidence": 0.9081633388996124}]}, {"text": "where T is the total duration of the speech archive and \u03b2 is a weight assigned to false alarms that is proportional to the prior probability of occurence of a specific term and its cost-value ratio.", "labels": [], "entities": []}, {"text": "This method sets individual thresholds for each query term considering per query expected counts and the tuning parameter \u03b2.", "labels": [], "entities": []}, {"text": "In the proposed method \u03b1 plays the same role as \u03b2 and allows us to control the decision threshold for different cost settings.", "labels": [], "entities": []}, {"text": "compares GT, TWV-TST, and the proposed method that utilizes score distributions to derive an optimal decision threshold.", "labels": [], "entities": []}, {"text": "For GT and TWT-TST, last precision/recall point in the figure corresponds to the limit threshold value which is 1.0.", "labels": [], "entities": [{"text": "TWT-TST", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8371195197105408}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9913228750228882}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9657535552978516}]}, {"text": "Both the TWV-TST and the proposed method outperform GT over the entire region of interest.", "labels": [], "entities": [{"text": "TWV-TST", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.8629899024963379}]}, {"text": "While TWV-TST provides better performance around the knees of the curves, proposed method achieves higher maximum precision values which coincides with the primary objective of our application.", "labels": [], "entities": [{"text": "TWV-TST", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.4519713819026947}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9416607618331909}]}, {"text": "also provides a curve of what happens when the correct class labels are used to estimate the parameters of the exponential mixture model in a supervised manner instead of using EM.", "labels": [], "entities": []}, {"text": "This curve provides an upper bound on the performance of the proposed method.", "labels": [], "entities": []}], "tableCaptions": []}