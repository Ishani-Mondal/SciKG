{"title": [{"text": "Faster MT Decoding through Pervasive Laziness", "labels": [], "entities": [{"text": "MT Decoding", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9782256484031677}, {"text": "Pervasive Laziness", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7819104194641113}]}], "abstractContent": [{"text": "Syntax-based MT systems have proven effective-the models are compelling and show good room for improvement.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9852184653282166}]}, {"text": "However, decoding involves a slow search.", "labels": [], "entities": []}, {"text": "We present anew lazy-search method that obtains significant speedups over a strong baseline, with no loss in Bleu.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.757390022277832}]}], "introductionContent": [{"text": "Syntax-based string-to-tree MT systems have proven effective-the models are compelling and show good room for improvement.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9024409055709839}]}, {"text": "However, slow decoding hinders research, as most experiments involve heavy parameter tuning, which involves heavy decoding.", "labels": [], "entities": []}, {"text": "In this paper, we present anew method to improve decoding performance, obtaining a significant speedup over a strong baseline with no loss in Bleu.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.8762165307998657}]}, {"text": "In scenarios where fast decoding is more important than optimal Bleu, we obtain better Bleu for the same time investment.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9885467290878296}, {"text": "Bleu", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9833995699882507}]}, {"text": "Our baseline is a full-scale syntax-based MT system with 245m tree-transducer rules of the kind described in (), 192 English non-terminal symbols, an integrated 5-gram language model (LM), and a decoder that uses state-of-the-art cube pruning . A sample translation rule is: S(x0:NP x1:VP) \u2194 x1:VP x0:NP In CKY string-to-tree decoding, we attack spans of the input string from shortest to longest.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9451618194580078}]}, {"text": "We populate each span with a set of edges.", "labels": [], "entities": []}, {"text": "An edge contains a English non-terminal (NT) symbol (NP, VP, etc), border words for LM combination, pointers to child edges, and a score.", "labels": [], "entities": []}, {"text": "The score is a sum of (1) the left-child edge score, (2) the right-child edge score, (3) the score of the translation rule that combined them, and (4) the target-string LM score.", "labels": [], "entities": []}, {"text": "In this paper, we are only concerned with what happens when constructing edges fora single span.", "labels": [], "entities": []}, {"text": "The naive algorithm works like this: for each split point k for each edge A in span for each edge B in span for each rule R with RHS = AB create new edge for span [i,j] delete all but 1000-best edges The last step provides a necessary beam.", "labels": [], "entities": [{"text": "RHS = AB", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.774480938911438}]}, {"text": "Without it, edges proliferate beyond available memory and time.", "labels": [], "entities": []}, {"text": "But even with the beam, the naive algorithm fails, because enumerating all <A,B,R> triples at each span is too time consuming.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}