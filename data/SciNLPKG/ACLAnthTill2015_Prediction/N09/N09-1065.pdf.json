{"title": [{"text": "Graph-Cut-Based Anaphoricity Determination for Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.9799983203411102}]}], "abstractContent": [{"text": "Recent work has shown that explicitly identifying and filtering non-anaphoric mentions prior to coreference resolution can improve the performance of a coreference system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.9183072149753571}]}, {"text": "We present a novel approach to this task of anaphoricity determination based on graph cuts, and demonstrate its superiority to competing approaches by comparing their effectiveness in improving a learning-based coref-erence system on the ACE data sets.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7840188443660736}, {"text": "ACE data sets", "start_pos": 238, "end_pos": 251, "type": "DATASET", "confidence": 0.9839010437329611}]}], "introductionContent": [{"text": "Coreference resolution is the problem of identifying which noun phrases (NPs, or mentions) refer to the same real-world entity in a text or dialogue.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.919516921043396}, {"text": "identifying which noun phrases (NPs, or mentions) refer to the same real-world entity in a text or dialogue", "start_pos": 41, "end_pos": 148, "type": "TASK", "confidence": 0.5685641098590124}]}, {"text": "According to, coreference resolution can be decomposed into two complementary tasks: \" identifying what a text potentially makes available for anaphoric reference and (2) constraining the candidate set of a given anaphoric expression down to one possible choice.\"", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.9732413291931152}]}, {"text": "The first task is nowadays typically formulated as an anaphoricity determination task, which aims to classify whether a given mention is anaphoric or not.", "labels": [], "entities": []}, {"text": "Knowledge of anaphoricity could improve the precision of a coreference system, since non-anaphoric mentions do not have an antecedent and therefore do not need to be resolved.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9988716244697571}]}, {"text": "Previous work on anaphoricity determination can be broadly divided into two categories (see for an overview).", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.8668293654918671}]}, {"text": "Research in the first category aims to identify specific types of nonanaphoric phrases, with some identifying pleonastic it (using heuristics [e.g.,,,], supervised approaches [e.g.,,,], and distributional methods [e.g.,]), and others identifying non-anaphoric definite descriptions (using rule-based techniques [e.g.,] and unsupervised techniques [e.g.,).", "labels": [], "entities": []}, {"text": "On the other hand, research in the second category focuses on (1) determining the anaphoricity of all types of mentions, and (2) using the resulting anaphoricity information to improve coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.9611680507659912}]}, {"text": "For instance, train an anaphoricity classifier to determine whether a mention is anaphoric, and let an independentlytrained coreference system resolve only those mentions that are classified as anaphoric.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, they report that using anaphoricity information adversely affects the performance of their coreference system, as a result of an overly conservative anaphoricity classifier that misclassifies many anaphoric mentions as non-anaphoric.", "labels": [], "entities": []}, {"text": "One solution to this problem is to use anaphoricity information as soft constraints rather than as hard constraints for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.9494863450527191}]}, {"text": "For instance, when searching for the best partition of a set of mentions, combines the probabilities returned by an anaphoricity model and a coreference model to score a coreference partition, such that a partition is penalized whenever an anaphoric mention is resolved.", "labels": [], "entities": []}, {"text": "Another, arguably more popular, solution is to \"improve\" the output of the anaphoricity classifier by exploiting the dependency between anaphoricity determination and coreference resolu-tion.", "labels": [], "entities": []}, {"text": "For instance, noting that Ng and Cardie's anaphoricity classifier is too conservative, Ng (2004) first parameterizes their classifier such that its conservativeness can be varied, and then tunes this parameter so that the performance of the coreference system is maximized.", "labels": [], "entities": []}, {"text": "As another example, Denis and and perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output of the anaphoricity classifier and that of the coreference classifier.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.7401285767555237}, {"text": "coreference resolution", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.9487451314926147}]}, {"text": "While this ILP approach and approach to improving the output of an anaphoricity classifier both result in increased coreference performance, they have complementary strengths and weaknesses.", "labels": [], "entities": []}, {"text": "Specifically, Ng's approach can directly optimize the desired coreference evaluation metric, but by treating the coreference system as a black box during the optimization process, it does not exploit the potentially useful pairwise probabilities provided by the coreference classifier.", "labels": [], "entities": []}, {"text": "On the other hand, the ILP approach does exploit such pairwise probabilities, but optimizes an objective function that does not necessarily have any correlation with the desired evaluation metric.", "labels": [], "entities": []}, {"text": "Our goals in this paper are two-fold.", "labels": [], "entities": []}, {"text": "First, motivated in part by previous work, we propose a graphcut-based approach to anaphoricity determination that combines the strengths of Ng's approach and the ILP approach, by exploiting pairwise coreference probabilities when co-ordinating anaphoricity and coreference decisions, and at the same time allowing direct optimization of the desired coreference evaluation metric.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.839713841676712}]}, {"text": "Second, we compare our cut-based approach with the five aforementioned approaches to anaphoricity determination (namely,,,,, and) in terms of their effectiveness in improving a learning-based coreference system.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.716689795255661}]}, {"text": "To our knowledge, there has been no attempt to perform a comparative evaluation of existing approaches to anaphoricity determination.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 106, "end_pos": 132, "type": "TASK", "confidence": 0.8110983371734619}]}, {"text": "It is worth noting, in particular, that,, and evaluate their approaches on true mentions extracted from the answer keys.", "labels": [], "entities": []}, {"text": "Since true mentions are composed of all the NPs involved in coreference relations but only a subset of the singleton NPs (i.e., NPs that are not coreferent with any other NPs) in a text, evaluating the utility of anaphoricity determination on true mentions to some extent defeats the purpose of performing anaphoricity determination, which precisely aims to identify non-anaphoric mentions.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 306, "end_pos": 332, "type": "TASK", "confidence": 0.7161277234554291}]}, {"text": "Hence, we hope that our evaluation on mentions extracted using an NP chunker can reveal their comparative strengths and weaknesses.", "labels": [], "entities": []}, {"text": "We perform our evaluation on three ACE coreference data sets using two commonly-used scoring programs.", "labels": [], "entities": [{"text": "ACE coreference data sets", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.8073534220457077}]}, {"text": "Experimental results show that (1) employing our cut-based approach to anaphoricity determination yields a coreference system that achieves the best performance for all six dataset/scoring-program combinations, and (2) among the five existing approaches, none performs consistently better than the others.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7525627613067627}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our learning-based coreference system.", "labels": [], "entities": []}, {"text": "In Section 3, we give an overview of the five baseline approaches to anaphoricity determination.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.8202658891677856}]}, {"text": "Section 4 provides the details of our graph-cut-based approach.", "labels": [], "entities": []}, {"text": "Finally, we present evaluation results in Section 5 and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation, we use the ACE Phase II coreference corpus, which is composed of three sections: Broadcast News (BNEWS), Newspaper (NPAPER), and Newswire (NWIRE).", "labels": [], "entities": [{"text": "ACE Phase II coreference corpus", "start_pos": 27, "end_pos": 58, "type": "DATASET", "confidence": 0.7519264936447143}]}, {"text": "Each section is in turn composed of a training set and a test set.", "labels": [], "entities": []}, {"text": "For each section, we train an anaphoricity model, PA , and a coreference model, PC , on the training set, and evaluate PC (when used in combination with different approaches to anaphoricity determination) on the test set.", "labels": [], "entities": [{"text": "PA", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9717079401016235}, {"text": "PC", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9670788049697876}]}, {"text": "As noted before, the mentions used are extracted automatically using an in-house NP chunker.", "labels": [], "entities": []}, {"text": "Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two coreference scoring programs: the MUC scorer ( and the CEAF scorer ().", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9534901976585388}, {"text": "precision (P)", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9450829476118088}, {"text": "F-measure (F)", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9683055877685547}, {"text": "MUC scorer", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.5701981484889984}, {"text": "CEAF scorer", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.6579770147800446}]}], "tableCaptions": [{"text": " Table 1: MUC scores for the three ACE data sets. F-scores that represent statistically significant gains and drops with  respect to the \"No Anaphoricity\" baseline are marked with an asterisk (*) and a dagger ( \u2020), respectively.", "labels": [], "entities": [{"text": "MUC", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5590572357177734}, {"text": "ACE data sets", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9693950215975443}, {"text": "F-scores", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9874722361564636}]}, {"text": " Table 2: CEAF scores for the three ACE data sets. F-scores that represent statistically significant gains and drops with  respect to the \"No Anaphoricity\" baseline are marked with an asterisk (*) and a dagger ( \u2020), respectively.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.5187468528747559}, {"text": "ACE data sets", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9547436436017355}, {"text": "F-scores", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9876311421394348}]}]}