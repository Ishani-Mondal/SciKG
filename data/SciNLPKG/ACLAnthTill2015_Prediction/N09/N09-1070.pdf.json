{"title": [{"text": "Unsupervised Approaches for Automatic Keyword Extraction Using Meeting Transcripts", "labels": [], "entities": [{"text": "Keyword Extraction", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7045947760343552}]}], "abstractContent": [{"text": "This paper explores several unsupervised approaches to automatic keyword extraction using meeting transcripts.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.723568931221962}]}, {"text": "In the TFIDF (term frequency, inverse document frequency) weighting framework, we incorporated part-of-speech (POS) information, word clustering, and sentence salience score.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.7822252511978149}, {"text": "word clustering", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.7531819343566895}]}, {"text": "We also evaluated a graph-based approach that measures the importance of a word based on its connection with other sentences or words.", "labels": [], "entities": []}, {"text": "The system performance is evaluated in different ways, including comparison to human annotated keywords using F-measure and a weighted score relative to the oracle system performance, as well as a novel alternative human evaluation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9636369347572327}]}, {"text": "Our results have shown that the simple un-supervised TFIDF approach performs reasonably well, and the additional information from POS and sentence score helps keyword extraction.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.8613481819629669}]}, {"text": "However, the graph method is less effective for this domain.", "labels": [], "entities": []}, {"text": "Experiments were also performed using speech recognition output and we observed degradation and different patterns compared to human transcripts.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7451030611991882}]}], "introductionContent": [{"text": "Keywords in a document provide important information about the content of the document.", "labels": [], "entities": []}, {"text": "They can help users search through information more efficiently or decide whether to read a document.", "labels": [], "entities": []}, {"text": "They can also be used fora variety of language processing tasks such as text categorization and information retrieval.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7657306790351868}, {"text": "information retrieval", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.8289891183376312}]}, {"text": "However, most documents do not provide keywords.", "labels": [], "entities": []}, {"text": "This is especially true for spoken documents.", "labels": [], "entities": []}, {"text": "Current speech recognition system performance has improved significantly, but there is no rich structural information such as topics and keywords in the transcriptions.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.77554851770401}]}, {"text": "Therefore, there is a need to automatically generate keywords for the large amount of written or spoken documents available now.", "labels": [], "entities": []}, {"text": "There have been many efforts toward keyword extraction for text domain.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8000823557376862}]}, {"text": "In contrast, there is less work on speech transcripts.", "labels": [], "entities": []}, {"text": "In this paper we focus on one speech genre -the multiparty meeting domain.", "labels": [], "entities": []}, {"text": "Meeting speech is significantly different from written text and most other speech data.", "labels": [], "entities": []}, {"text": "For example, there are typically multiple participants in a meeting, the discussion is not well organized, and the speech is spontaneous and contains disfluencies and ill-formed sentences.", "labels": [], "entities": []}, {"text": "It is thus questionable whether we can adopt approaches that have been shown before to perform well in written text for automatic keyword extraction in meeting transcripts.", "labels": [], "entities": [{"text": "automatic keyword extraction", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.6705895165602366}]}, {"text": "In this paper, we evaluate several different keyword extraction algorithms using the transcripts of the ICSI meeting corpus.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7255311459302902}, {"text": "ICSI meeting corpus", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.9444342652956644}]}, {"text": "Starting from the simple TFIDF baseline, we introduce knowledge sources based on POS filtering, word clustering, and sentence salience score.", "labels": [], "entities": [{"text": "TFIDF baseline", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.784947007894516}, {"text": "POS filtering", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.7204677760601044}, {"text": "word clustering", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.7655711472034454}]}, {"text": "In addition, we also investigate a graph-based algorithm in order to leverage more global information and reinforcement from summary sentences.", "labels": [], "entities": []}, {"text": "We used different performance measurements: comparing to human annotated keywords using individual F-measures and a weighted score relative to the oracle system performance, and conducting novel human evaluation.", "labels": [], "entities": []}, {"text": "Experiments were conducted using both the human transcripts and the speech recognition (ASR) out-put.", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.8219963788986206}]}, {"text": "Overall the TFIDF based framework seems to work well for this domain, and the additional knowledge sources help improve system performance.", "labels": [], "entities": []}, {"text": "The graph-based approach yielded worse results, especially for the ASR condition, suggesting further investigation for this task.", "labels": [], "entities": [{"text": "ASR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9270585775375366}]}], "datasetContent": [{"text": "Using the approaches described above, we computed weights for the words and then picked the top five words as the keywords fora topic.", "labels": [], "entities": []}, {"text": "We chose five keywords since this is the number of keywords that human annotators used as a guideline, and it also yielded good performance in the development set.", "labels": [], "entities": []}, {"text": "To evaluate system performance, in this section we use human annotated keywords as references, and compare the system output to them.", "labels": [], "entities": []}, {"text": "The first metric we use is F-measure, which has been widely used for this task and other detection tasks.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9966592788696289}]}, {"text": "We compare the system output with respect to each human annotation, and calculate the maximum and the average F-scores.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9929260015487671}]}, {"text": "Note that our keyword evaluation is wordbased.", "labels": [], "entities": []}, {"text": "When human annotators choose key phrases (containing more than one word), we split them into words and measure the matching words.", "labels": [], "entities": []}, {"text": "Therefore, when the system only generates five keywords, the upper bound of the recall rate may not be 100%.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9924669861793518}]}, {"text": "In (, a lenient metric is used which accounts for some inflection of words.", "labels": [], "entities": []}, {"text": "Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments.", "labels": [], "entities": []}, {"text": "The second metric we use is similar to Pyramid (), which has been used for summarization evaluation.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.9630546569824219}]}, {"text": "Instead of comparing the system output with each individual human annotation, the method creates a \"pyramid\" using all the human annotated keywords, and then compares system output to this pyramid.", "labels": [], "entities": []}, {"text": "The pyramid consists of all the annotated keywords at different levels.", "labels": [], "entities": []}, {"text": "Each keyword has a score based on how many annotators have selected this one.", "labels": [], "entities": []}, {"text": "The higher the score, the higher up the keyword will be in the pyramid.", "labels": [], "entities": []}, {"text": "Then we calculate an oracle score that a system can obtain when generating k keywords.", "labels": [], "entities": []}, {"text": "This is done by selecting keywords in the decreasing order in terms of the pyramid levels until we obtain k keywords.", "labels": [], "entities": []}, {"text": "Finally for the system hypothesized k keywords, we compute its score by adding the scores of the keywords that match those in the pyramid.", "labels": [], "entities": []}, {"text": "The system's performance is measured using the relative performance of the system's pyramid scores divided by the oracle score.", "labels": [], "entities": []}, {"text": "shows the results using human transcripts for different methods on the 21 test meetings (139 topic segments in total).", "labels": [], "entities": []}, {"text": "For comparison, we also show results using the supervised approach as in (, which is the average of the 21-fold cross validation.", "labels": [], "entities": []}, {"text": "We only show the maximum F-measure with respect to individual annotations, since the average scores show similar trend.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9979203343391418}]}, {"text": "In addition, the weighted relative scores already accounts for the different annotation and human agreement.", "labels": [], "entities": []}, {"text": "We notice that for the TFIDF framework, adding POS information slightly helps the basic TFIDF method.", "labels": [], "entities": []}, {"text": "In all the meetings, our statistics show that adding POS filtering removed 2.3% of human annotated keywords from the word candidates; therefore, this does not have a significant negative impact on the upper bound recall rate, but helps eliminate unlikely keyword candidates.", "labels": [], "entities": [{"text": "POS filtering", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.6608404815196991}, {"text": "recall rate", "start_pos": 213, "end_pos": 224, "type": "METRIC", "confidence": 0.9714434444904327}]}, {"text": "Using word clustering does not yield a performance gain, most likely because of the clustering technique we used -it does clustering simply based on word co-occurrence and does not capture semantic similarity properly.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.764809787273407}]}, {"text": "Given the disagreement among human annotators, one question we need to answer is whether Fmeasure or even the weighted relative scores compared with human annotations are appropriate metrics to evaluate system-generated keywords.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.5100380182266235}]}, {"text": "For example, precision measures among the systemgenerated keywords how many are correct.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.999517560005188}]}, {"text": "However, this does not measure if the unmatched systemgenerated keywords are bad or acceptable.", "labels": [], "entities": []}, {"text": "We therefore performed a small scale human evaluation.", "labels": [], "entities": []}, {"text": "We selected four topic segments from four different meetings, and gave output from different systems to five human subjects.", "labels": [], "entities": []}, {"text": "The subjects ranged in age from 22 to 63, and all but one had only basic knowledge of computers.", "labels": [], "entities": []}, {"text": "We first asked the eval-uators to read the entire topic transcript, and then presented them with the system-generated keywords (randomly ordered by different systems).", "labels": [], "entities": []}, {"text": "For comparison, the keywords annotated by our three human annotators were also included without revealing which sets of keywords were generated by a human and which by a computer.", "labels": [], "entities": []}, {"text": "Because there was such disagreement between annotators regarding what made good keywords, we instead asked our evaluators to mark any words that were definitely not keywords.", "labels": [], "entities": []}, {"text": "Systems that produced more of these rejected words (such as \"basically\" or \"mmm-hm\") are assumed to be worse than those containing fewer rejected words.", "labels": [], "entities": []}, {"text": "We then measured the percentage of rejected keywords for each system/annotator.", "labels": [], "entities": []}, {"text": "The results are shown in  Note this rejection rate is highly related to the recall/precision measure in the sense that it measures how many keywords are acceptable (or rejected) among the system generated ones.", "labels": [], "entities": [{"text": "recall/precision measure", "start_pos": 76, "end_pos": 100, "type": "METRIC", "confidence": 0.85767862200737}]}, {"text": "However, instead of comparing to a fixed set of human annotated keywords (e.g., five) and using that as a gold standard to compute recall/precision, in this evaluation, the human evaluator may have a larger set of acceptable keywords in their mind.", "labels": [], "entities": [{"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9971004128456116}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9389976859092712}]}, {"text": "We also measured the human evaluator agreement regarding the accepted or bad keywords.", "labels": [], "entities": []}, {"text": "We found that the agreement on a bad keyword among five, four, and three human evaluator is 10.1%, 14.8%, and 10.1% respectively.", "labels": [], "entities": []}, {"text": "This suggests that humans are more likely to agree on a bad keyword selection compared to agreement on the selected keywords, as discussed in Section 3 (even though the data sets in these two analysis are not the same).", "labels": [], "entities": []}, {"text": "Another observation from the human evaluation is that sometimes a person rejects a keyword from one system output, but accepts that on the list from another system.", "labels": [], "entities": []}, {"text": "We are not sure yet whether this is the inconsistency from human evaluators or whether the judgment is based on a word's occurrence with other provided keywords and thus some kind of semantic coherence.", "labels": [], "entities": []}, {"text": "Further investigation on human evaluation is still needed.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7473414540290833}]}], "tableCaptions": [{"text": " Table 1: Keyword extraction results using human tran- scripts compared to human annotations.", "labels": [], "entities": [{"text": "Keyword extraction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8488661646842957}]}, {"text": " Table 1. We find that in gen- eral, there is a performance degradation compared", "labels": [], "entities": []}, {"text": " Table 2: Keyword extraction results using ASR output.", "labels": [], "entities": [{"text": "Keyword extraction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8459013402462006}, {"text": "ASR", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8403924107551575}]}, {"text": " Table 3. Not surprisingly, the  human annotations rank at the top. Overall, we find  human evaluation results to be consistent with the  automatic evaluation metrics in terms of the ranking  of different systems.", "labels": [], "entities": []}, {"text": " Table 3: Human evaluation results: percentage of the re- jected keywords by human evaluators for different sys- tems/annotators.", "labels": [], "entities": []}]}