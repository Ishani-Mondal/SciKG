{"title": [{"text": "Joint Parsing and Named Entity Recognition", "labels": [], "entities": [{"text": "Joint Parsing and Named Entity Recognition", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7078401943047842}]}], "abstractContent": [{"text": "For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser).", "labels": [], "entities": [{"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9273733794689178}]}, {"text": "This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system.", "labels": [], "entities": []}, {"text": "We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9776929020881653}, {"text": "named entity recognition", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6217538217703501}]}, {"text": "Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree.", "labels": [], "entities": []}, {"text": "The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of up to 1.36% absolute F1 for parsing, and up to 9.0% F1 for named entity recognition.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 148, "end_pos": 164, "type": "DATASET", "confidence": 0.8757763206958771}, {"text": "F1", "start_pos": 212, "end_pos": 214, "type": "METRIC", "confidence": 0.9324918389320374}, {"text": "parsing", "start_pos": 219, "end_pos": 226, "type": "TASK", "confidence": 0.9659740924835205}, {"text": "F1", "start_pos": 243, "end_pos": 245, "type": "METRIC", "confidence": 0.99964439868927}, {"text": "named entity recognition", "start_pos": 250, "end_pos": 274, "type": "TASK", "confidence": 0.6712076266606649}]}], "introductionContent": [{"text": "In order to build high quality systems for complex NLP tasks, such as question answering and textual entailment, it is essential to first have high quality systems for lower level tasks.", "labels": [], "entities": [{"text": "question answering and textual entailment", "start_pos": 70, "end_pos": 111, "type": "TASK", "confidence": 0.6801074028015137}]}, {"text": "A good (deep analysis) question answering system requires the data to first be annotated with several types of information: parse trees, named entities, word sense disambiguation, etc.", "labels": [], "entities": [{"text": "question answering", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7395023107528687}, {"text": "word sense disambiguation", "start_pos": 153, "end_pos": 178, "type": "TASK", "confidence": 0.6061737338701884}]}, {"text": "However, having high performing, lowlevel systems is not enough; the assertions of the various levels of annotation must be consistent with one another.", "labels": [], "entities": []}, {"text": "When a named entity span has crossing brackets with the spans in the parse tree it is usually impossible to effectively combine these pieces of information, and system performance suffers.", "labels": [], "entities": []}, {"text": "But, unfortunately, it is still common practice to cobble together independent systems for the various types of annotation, and there is no guarantee that their outputs will be consistent.", "labels": [], "entities": []}, {"text": "This paper begins to address this problem by building a joint model of both parsing and named entity recognition.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9638556241989136}, {"text": "named entity recognition", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.5561206142107645}]}, {"text": "Vapnik has observed) that \"one should solve the problem directly and never solve a more general problem as an intermediate step,\" implying that building a joint model of two phenomena is more likely to harm performance on the individual tasks than to help it.", "labels": [], "entities": []}, {"text": "Indeed, it has proven very difficult to build a joint model of parsing and semantic role labeling, either with PCFG trees) or with dependency trees.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.5601400633653005}]}, {"text": "The CoNLL 2008 shared task () was intended to be about joint dependency parsing and semantic role labeling, but the top performing systems decoupled the tasks and outperformed the systems which attempted to learn them jointly.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8693360388278961}, {"text": "joint dependency parsing", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6904449065526327}, {"text": "semantic role labeling", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.6295500000317892}]}, {"text": "Despite these earlier results, we found that combining parsing and named entity recognition modestly improved performance on both tasks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9693301320075989}, {"text": "named entity recognition", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6008298496405283}]}, {"text": "Our joint model produces an output which has consistent parse structure and named entity spans, and does a better job at both tasks than separate models with the same features.", "labels": [], "entities": []}, {"text": "We first present the joint, discriminative model that we use, which is a feature-based CRF-CFG parser operating over tree structures augmented with NER information.", "labels": [], "entities": []}, {"text": "We then discuss in detail how we make use of the recently developed OntoNotes corpus both for training and testing the model, and then finally present the performance of the model and some discussion of what causes its superior performance, and how the model relates to prior work.: An example of a (sub)tree which is modified for input to our learning algorithm.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.8321229517459869}]}, {"text": "Starting from the normalized tree discussed in section 4.1, anew NamedEntity node is added, so that the named entity corresponds to a single phrasal node.", "labels": [], "entities": []}, {"text": "That node, and its descendents, have their labels augmented with the type of named entity.", "labels": [], "entities": []}, {"text": "The * on the NamedEntity node indicates that it is the root of the named entity.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran our model on six of the OntoNotes datasets described in Section 4, 4 using sentences of length 40 and under (approximately 200,000 annotated English words, considerably smaller than the Penn Treebank ().", "labels": [], "entities": [{"text": "OntoNotes datasets", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.8991509675979614}, {"text": "Penn Treebank", "start_pos": 193, "end_pos": 206, "type": "DATASET", "confidence": 0.9959662556648254}]}, {"text": "For each dataset, we aimed for roughly a 75% train / 25% test split.", "labels": [], "entities": []}, {"text": "See for the the files used to train and test, along with the number of sentences in each.", "labels": [], "entities": []}, {"text": "For comparison, we also trained the parser without the named entity information (and omitted the NamedEntity nodes), and a linear chain CRF using just the named entity information.", "labels": [], "entities": []}, {"text": "Both the baseline parser and CRF were trained using the exact same features as the joint model, and all were optimized using stochastic gradient descent.", "labels": [], "entities": []}, {"text": "The full results can be found in.", "labels": [], "entities": []}, {"text": "Parse trees were scored using evalB (the extra NamedEntity nodes were ignored when computing evalB for the joint model), and named entities were scored using entity F-measure (as in the.", "labels": [], "entities": []}, {"text": "While the main benefit of our joint model is the ability to get a consistent output over both types of annotations, we also found that modeling the parse These datasets all consistently use the new conventions for treebank annotation, while the seventh WSJ portion is currently still annotated in the original 1990s style, and so we left the WSJ portion aside.", "labels": [], "entities": []}, {"text": "Sometimes the parser would be unable to parse a sentence (less than 2% of sentences), due to restrictions in part of speech tags.", "labels": [], "entities": [{"text": "parse a sentence", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.8649878899256388}]}, {"text": "Because the underlying grammar (ignoring the additional named entity information) was the same for both the joint and baseline parsers, it is the case that whenever a sentence is unparseable by either the baseline or joint parser it is in fact unparsable by both of them, and would affect the parse scores of both models equally.", "labels": [], "entities": []}, {"text": "However, the CRF is able to named entity tag any sentence, so these unparsable sentences had an effect on the named entity score.", "labels": [], "entities": []}, {"text": "To combat this, we fell back on the baseline CRF model to get named entity tags for unparsable sentences. and named entities jointly resulted in improved performance on both.", "labels": [], "entities": []}, {"text": "When looking at these numbers, it is important to keep in mind that the sizes of the training and test sets are significantly smaller than the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 143, "end_pos": 156, "type": "DATASET", "confidence": 0.996912032365799}]}, {"text": "The largest of the six datasets, CNN, has about one seventh the amount of training data as the Penn Treebank, and the smallest, MNB, has around 500 sentences from which to train.", "labels": [], "entities": [{"text": "CNN", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9814230799674988}, {"text": "Penn Treebank", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.9944728910923004}, {"text": "MNB", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.9686964154243469}]}, {"text": "Parse performance was improved by the joint model for five of the six datasets, by up to 1.36%.", "labels": [], "entities": [{"text": "Parse", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9765586853027344}]}, {"text": "Looking at the parsing improvements on a per-label basis, the largest gains came from improved identication of NML consituents, from an F-score of 45.9% to 57.0% (on all the data combined, fora total of 420 NML constituents).", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9687827229499817}, {"text": "F-score", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.998645007610321}]}, {"text": "This label was added in the new treebank annotation conventions, so as to identify internal left-branching structure inside previously flat NPs.", "labels": [], "entities": []}, {"text": "To our surprise, performance on NPs only increased by 1%, though over 12, 949 constituents, for the largest improvement in absolute terms.", "labels": [], "entities": []}, {"text": "The second largest gain was on PPs, where we improved by 1.7% over 3, 775 constituents.", "labels": [], "entities": []}, {"text": "We tested the significance of our results (on all the data combined) using Dan Bikel's randomized parsing evaluation comparator and found that both the precision and recall gains were significant at p \u2264 0.01.", "labels": [], "entities": [{"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9994314312934875}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9915353059768677}]}, {"text": "Much greater improvements in performance were seen on named entity recognition, where most of the domains saw improvements in the range of 3-4%, with performance on the VOA data improving by nearly 9%, which is a 45% reduction in error.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.7052478194236755}, {"text": "VOA data", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.9035593569278717}, {"text": "error", "start_pos": 230, "end_pos": 235, "type": "METRIC", "confidence": 0.9865015149116516}]}, {"text": "There was no clear trend in terms of precision versus recall, or the different entity types.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9993054866790771}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9992377758026123}]}, {"text": "The first place to look for improvements is with the boundaries for named entities.", "labels": [], "entities": []}, {"text": "Once again looking at all of the data combined, in the baseline model there were 203 entities where part of the entity was found, but one or both boundaries were incorrectly identified.", "labels": [], "entities": []}, {"text": "The joint model corrected 72 of those entities, while incorrectly identifying the boundaries of 37 entities which had previously been correctly identified.", "labels": [], "entities": []}, {"text": "In the baseline NER model, there were 243 entities for which the boundaries were correctly identified, but the type of entity was incorrect.", "labels": [], "entities": [{"text": "NER", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9304924607276917}]}, {"text": "The joint model corrected 80 of them, while changing the labels of 39 entities which had previously been correctly identified.", "labels": [], "entities": []}, {"text": "Additionally, 190 entities were found which the baseline model had missed entirely, and 68 enti-  ties were lost.", "labels": [], "entities": []}, {"text": "We tested the statistical significance of the gains (of all the data combined) using the same sentence-level, stratified shuffling technique as Bikel's parse comparator and found that both precision and recall gains were significant at p < 10 \u22124 . An example from the data where the joint model helped improve both parse structure and named entity recognition is shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9988203644752502}, {"text": "recall", "start_pos": 203, "end_pos": 209, "type": "METRIC", "confidence": 0.9863892197608948}, {"text": "named entity recognition", "start_pos": 335, "end_pos": 359, "type": "TASK", "confidence": 0.6210799713929495}]}, {"text": "The output from the individual models is shown in part (a), with the output from the named entity recognizer shown in brackets on the words at leaves of the parse.", "labels": [], "entities": []}, {"text": "The output from the joint model is shown in part (b), with the named entity information encoded within the parse.", "labels": [], "entities": []}, {"text": "In this example, the named entity Egyptian Islamic Jihad helped the parser to get its surrounding context correct, because it is improbable to attach a PP headed by with to an organization.", "labels": [], "entities": [{"text": "Egyptian Islamic Jihad", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.8736418684323629}]}, {"text": "At the same time, the surrounding context helped the joint model correctly identify Egyptian Islamic Jihad as an organization and not a person.", "labels": [], "entities": []}, {"text": "The baseline parser also incorrectly added an extra level of structure to the person name Osama Bin Laden, while the joint model found the correct structure.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training and test set sizes for the six datasets in  sentences. The file ranges refer to the numbers within the  names of the original OntoNotes files.", "labels": [], "entities": [{"text": "OntoNotes files", "start_pos": 145, "end_pos": 160, "type": "DATASET", "confidence": 0.8891083300113678}]}, {"text": " Table 2: Full parse and NER results for the six datasets. Parse trees were evaluated using evalB, and named entities  were scored using macro-averaged F-measure (conlleval).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9025328755378723}]}]}