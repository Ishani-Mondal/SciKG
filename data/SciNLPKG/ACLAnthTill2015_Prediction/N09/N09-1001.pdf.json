{"title": [{"text": "Subjectivity Recognition on Word Senses via Semi-supervised Mincuts", "labels": [], "entities": [{"text": "Subjectivity Recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9224018752574921}]}], "abstractContent": [{"text": "We supplement WordNet entries with information on the subjectivity of its word senses.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.956887423992157}]}, {"text": "Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.", "labels": [], "entities": []}, {"text": "The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short.", "labels": [], "entities": []}, {"text": "We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.", "labels": [], "entities": []}, {"text": "The experimental results show that it outper-forms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9863920509815216}]}, {"text": "In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief ( ).", "labels": [], "entities": []}, {"text": "Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed.", "labels": [], "entities": [{"text": "identification of subjective content", "start_pos": 38, "end_pos": 74, "type": "TASK", "confidence": 0.8836843818426132}]}, {"text": "Automatic identification of subjective content often relies on word indicators, such as unigrams () or predetermined sentiment lexica ( ).", "labels": [], "entities": [{"text": "Automatic identification of subjective content", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8211654782295227}]}, {"text": "Thus, the word positive in the sentence \"This deal is a positive development for our company.\" gives a strong indication that the sentence contains a favourable opinion.", "labels": [], "entities": []}, {"text": "However, such word-based indicators can be misleading for two reasons.", "labels": [], "entities": []}, {"text": "First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications ().", "labels": [], "entities": []}, {"text": "Second, different word senses of a single word can actually be of different subjectivity or polarity.", "labels": [], "entities": []}, {"text": "A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.", "labels": [], "entities": []}, {"text": "We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 206, "end_pos": 213, "type": "DATASET", "confidence": 0.9494817852973938}]}, {"text": "This is important as the problem of subjectivity-ambiguity is frequent: We ( find that over 30% of words in our dataset are subjectivity-ambiguous.", "labels": [], "entities": []}, {"text": "Information on subjectivity of senses can also improve other tasks such as word sense disambiguation).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.677748829126358}]}, {"text": "Moreover, show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use.", "labels": [], "entities": []}, {"text": "We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.", "labels": [], "entities": []}, {"text": "Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9782387316226959}]}, {"text": "In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.", "labels": [], "entities": []}, {"text": "Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets.", "labels": [], "entities": [{"text": "subjectivity recognition of word senses", "start_pos": 54, "end_pos": 93, "type": "TASK", "confidence": 0.8255674719810486}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses previous work.", "labels": [], "entities": []}, {"text": "Section 3 describes our proposed semi-supervised minimum cut framework in detail.", "labels": [], "entities": []}, {"text": "Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct the experiments on two different gold standard datasets.", "labels": [], "entities": [{"text": "gold standard datasets", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.7255597114562988}]}, {"text": "One is the Micro-WNOp corpus,  We compare to a baseline that assigns the most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&Mihalcea's dataset respectively.", "labels": [], "entities": [{"text": "Micro-WNOp corpus", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.9425413608551025}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9990580677986145}, {"text": "MicroWNOp", "start_pos": 170, "end_pos": 179, "type": "DATASET", "confidence": 0.9749694466590881}]}, {"text": "We use the McNemar test at the significance level of 5% for significance statements.", "labels": [], "entities": [{"text": "McNemar test", "start_pos": 11, "end_pos": 23, "type": "METRIC", "confidence": 0.6751871109008789}, {"text": "significance level", "start_pos": 31, "end_pos": 49, "type": "METRIC", "confidence": 0.975072592496872}]}, {"text": "All evaluations are carried out by 10-fold cross-validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Relation weights (Method 2)  Method  #Same #Different Weight  Antonym  2,808  309  0.90  Similar-to  6,887  1,614  0.81  Derived-from  4,630  947  0.83  Direct-Hypernym  71,915 8,600  0.89  Direct-Hyponym  71,915 8,600  0.89  Attribute  350  109  0.76  Also-see  1,037  337  0.75  Extended-Antonym 6,917  1,651  0.81  Domain  4,387  892  0.83  Domain-member  4,387  892  0.83", "labels": [], "entities": []}, {"text": " Table 2: Results of SVM and Mincuts with different settings of feature", "labels": [], "entities": []}, {"text": " Table 3: Accuracy for Different Part-Of-Speech  Method  Noun  Adjective Adverb Verb  Baseline 76.9% 61.1%  77.4%  72.6%  SVM  81.4% 63.4%  83.9% 75.1%  Mincut  88.6% 78.9%  77.4%  84.0%", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9938690066337585}]}, {"text": " Table 4: Accuracy with different sizes of labeled data  # labeled data SVM  Mincuts  100  69.1% 72.2%  200  72.6% 78.9%  400  74.4% 82.7%  600  75.5% 83.7%  800  76.0% 84.1%  900  75.6% 84.8%  954 (all)  75.3% 84.6%", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.984115481376648}]}, {"text": " Table 6: Accuracy with different sizes of unlabeled data  (random selection)  # unlabeled data Accuracy  0  75.9%  200  76.5%  500  78.6%  1000  80.2%  2000  82.8%  3000  84.0%  3220  84.6%", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.995711088180542}, {"text": "Accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.988831639289856}]}, {"text": " Table 5: Accuracy with different sizes of unlabeled data from WordNet relation  Relation  # unlabeled data Accuracy  {\u2205}  0  75.3%  {similar-to}  418  79.1%  {similar-to, antonym}  514  79.5%  {similar-to, antonym, direct-hypernym, direct- hyponym}  2,721  84.4%", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9935165643692017}, {"text": "WordNet relation  Relation", "start_pos": 63, "end_pos": 89, "type": "DATASET", "confidence": 0.8886979222297668}, {"text": "Accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9936287999153137}]}]}