{"title": [{"text": "Exploiting Named Entity Classes in CCG Surface Realization", "labels": [], "entities": [{"text": "CCG Surface Realization", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.5400738815466563}]}], "abstractContent": [{"text": "This paper describes how named entity (NE) classes can be used to improve broad coverage surface realization with the OpenCCG re-alizer.", "labels": [], "entities": [{"text": "broad coverage surface realization", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.6116847097873688}]}, {"text": "Our experiments indicate that collapsing certain multi-word NEs and interpolating a language model where NEs are replaced by their class labels yields the largest quality increase , with 4-grams adding a small additional boost.", "labels": [], "entities": []}, {"text": "Substantial further benefit is obtained by including class information in the hyper-tagging (supertagging for realization) component of the system, yielding a state-of-the-art BLEU score of 0.8173 on Section 23 of the CCGbank.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9987989664077759}, {"text": "CCGbank", "start_pos": 218, "end_pos": 225, "type": "DATASET", "confidence": 0.5065181255340576}]}, {"text": "A targeted manual evaluation confirms that the BLEU score increase corresponds to a significant rise in fluency.", "labels": [], "entities": [{"text": "BLEU score increase", "start_pos": 47, "end_pos": 66, "type": "METRIC", "confidence": 0.9803762038548788}]}], "introductionContent": [{"text": "have recently shown that better handling of named entities (NEs) in broad coverage surface realization with LFG can lead to substantial improvements in BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9972039461135864}]}, {"text": "In this paper, we confirm that better NE handling can likewise improve broad coverage surface realization with CCG, even when employing a more restrictive notion of named entities that better matches traditional realization practice.", "labels": [], "entities": [{"text": "NE handling", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9500689506530762}, {"text": "broad coverage surface realization", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.6062965169548988}]}, {"text": "Going beyond, we additionally show that NE classes can be used to improve realization quality through better language models and better hypertagging (supertagging for realization) models, yielding a state-of-the-art BLEU score of 0.8173 on Section 23 of the CCGbank.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9989317059516907}, {"text": "CCGbank", "start_pos": 258, "end_pos": 265, "type": "DATASET", "confidence": 0.6668115258216858}]}, {"text": "A question addressed neither by Hogan et al. nor anyone else working on broad coverage surface realization recently is whether reported increases in BLEU scores actually correspond to observable improvements in quality.", "labels": [], "entities": [{"text": "broad coverage surface realization", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.6439506486058235}, {"text": "BLEU scores", "start_pos": 149, "end_pos": 160, "type": "METRIC", "confidence": 0.9761747121810913}]}, {"text": "We view this situation as problematic, not only because have shown that BLEU does not always rank competing systems in accord with human judgments, but also because surface realization scores are typically much higher than those in MT-where BLEU's performance has been repeatedly assessed-even when using just one reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9217360019683838}, {"text": "MT-where BLEU", "start_pos": 232, "end_pos": 245, "type": "TASK", "confidence": 0.7659029364585876}]}, {"text": "Thus, in this paper, we present a targeted manual evaluation confirming that our BLEU score increase corresponds to a significant rise in fluency, a practice we encourage others to adopt.", "labels": [], "entities": [{"text": "BLEU score increase", "start_pos": 81, "end_pos": 100, "type": "METRIC", "confidence": 0.9832075238227844}]}], "datasetContent": [{"text": "As indicates, the hypertagging model does worse in terms of per-logical predication accuracy & per-whole-graph accuracy on the collapsed corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.7909697890281677}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9285758137702942}]}, {"text": "To some extent this is not surprising, as collapsing eliminates many easy tagging cases; however, a full explanation is still under investigation.", "labels": [], "entities": []}, {"text": "Note that class information does improve performance somewhat on the collapsed corpus.", "labels": [], "entities": []}, {"text": "For a both the original CCGbank and the collapsed corpus, we extracted a section 02-21 lexicogrammars and used it to derive LFs for the development and test sections.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9810312390327454}]}, {"text": "We used the language models in   collapsed corpus, we also tried a class-based hypertagging model.", "labels": [], "entities": []}, {"text": "Hypertagger \u03b2-values were set for each corpus and for each hypertagging model such that the predicted tags per pred was the same at each level.", "labels": [], "entities": []}, {"text": "BLEU scores were calculated after removing the underscores between collapsed NEs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9710083603858948}]}, {"text": "While the language models employing NE classes certainly improve some examples, others are made worse, and some are just changed to different, but equally acceptable paraphrases.", "labels": [], "entities": []}, {"text": "For this reason, we carried out a targeted manual evaluation to confirm the BLEU results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9985307455062866}]}], "tableCaptions": [{"text": " Table 1: Legend for Experimental Conditions", "labels": [], "entities": []}, {"text": " Table 2: Hypertagger testing on Section 00 of the uncol- lapsed corpus (1896 LFs & 38104 predicates) & partially  collapsed corpus (1895 LFs & 35370 predicates)", "labels": [], "entities": []}, {"text": " Table 3: Section 00 blind testing results", "labels": [], "entities": []}, {"text": " Table 4: Section 23 results: LM+HT baseline on origi- nal corpus (97.8% coverage), LM4C+HTC best case on  collapsed corpus (94.8% coverage)", "labels": [], "entities": []}]}