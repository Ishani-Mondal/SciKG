{"title": [], "abstractContent": [{"text": "In (Chen, 2009), we show that fora variety of language models belonging to the exponential family, the test set cross-entropy of a model can be accurately predicted from its training set cross-entropy and its parameter values.", "labels": [], "entities": []}, {"text": "In this work, we show how this relationship can be used to motivate two heuristics for \"shrinking\" the size of a language model to improve its performance.", "labels": [], "entities": []}, {"text": "We use the first heuristic to develop a novel class-based language model that outperforms a baseline word trigram model by 28% in perplexity and 1.9% absolute in speech recognition word-error rate on Wall Street Journal data.", "labels": [], "entities": [{"text": "Wall Street Journal data", "start_pos": 200, "end_pos": 224, "type": "DATASET", "confidence": 0.9830145984888077}]}, {"text": "We use the second heuristic to motivate a regularized version of minimum discrimination information models and show that this method outperforms other techniques for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.7220783829689026}]}], "introductionContent": [{"text": "An exponential model p \u039b (y|x) is a model with a set of features {f 1 (x, y), . .", "labels": [], "entities": []}, {"text": ", f F (x, y)} and equal number of parameters \u039b = {\u03bb 1 , . .", "labels": [], "entities": []}, {"text": ", \u03bb F } where and where Z \u039b (x) is a normalization factor.", "labels": [], "entities": []}, {"text": "In, we show that for many types of exponential language models, if a training and test set are drawn from the same distribution, we have where H test denotes test set cross-entropy; H train denotes training set cross-entropy; Dis the number of events in the training data; the\u02dc\u03bbthe\u02dc the\u02dc\u03bb i are regularized parameter estimates; and \u03b3 is a constant independent of domain, training set size, and model type.", "labels": [], "entities": []}, {"text": "This relationship is strongest if the\u02dc\u039bthe\u02dc the\u02dc\u039b = { \u02dc \u03bb i } are estimated using 1 + 2 2 regularization ().", "labels": [], "entities": []}, {"text": "In 1 + 2 2 regularization, parameters are chosen to optimize for some \u03b1 and \u03c3.", "labels": [], "entities": []}, {"text": "With (\u03b1 = 0.5, \u03c3 2 = 6) and taking \u03b3 = 0.938, test set cross-entropy can be predicted with eq.", "labels": [], "entities": []}, {"text": "(2) fora wide range of models with a mean error of a few hundredths of a nat, equivalent to a few percent in perplexity.", "labels": [], "entities": []}, {"text": "In this paper, we show how eq.", "labels": [], "entities": []}, {"text": "(2) can be applied to improve language model performance.", "labels": [], "entities": []}, {"text": "First, we use eq.", "labels": [], "entities": []}, {"text": "(2) to analyze backoff features in exponential n-gram models.", "labels": [], "entities": []}, {"text": "We find that backoff features improve test set performance by reducing the \"size\" of a model 1 D F i=1 | \u02dc \u03bb i | rather than by improving training set performance.", "labels": [], "entities": []}, {"text": "This suggests the following principle for improving exponential language models: if a model can be \"shrunk\" without increasing its training set cross-entropy, test set cross-entropy should improve.", "labels": [], "entities": []}, {"text": "We apply this idea to motivate two language models: a novel class-based language model and regularized minimum discrimination information (MDI) models.", "labels": [], "entities": []}, {"text": "We show how these models outperform other models in both perplexity and word-error rate on Wall Street Journal (WSJ) data.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) data", "start_pos": 91, "end_pos": 121, "type": "DATASET", "confidence": 0.9514097145625523}]}, {"text": "The organization of this paper is as follows: In Section 2, we analyze the use of backoff features in n-gram models to motivate a heuristic for model   If we denote bigrams as w j\u22121 w j , each column contains the\u02dc\u03bb the\u02dc the\u02dc\u03bb i 's corresponding to all bigrams with a particular w j . The '\u00d7' marks represent the average | \u02dc \u03bb i | in each column; this average includes history words for which no feature exists or for which\u02dc\u03bbwhich\u02dc which\u02dc\u03bb i = 0.", "labels": [], "entities": []}, {"text": "class-based model and discuss MDI domain adaptation, and compare these methods against other techniques on WSJ data.", "labels": [], "entities": [{"text": "MDI domain adaptation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.7898982961972555}, {"text": "WSJ data", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.920838862657547}]}, {"text": "Finally, in Sections 5 and 6 we discuss related work and conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Various statistics for letter trigram models built  on a 1k-word training set. H eval is the cross-entropy of  the evaluation data; H pred is the predicted test set cross- entropy according to eq. (2); and H train is the training  set cross-entropy. The evaluation data is drawn from the  same distribution as the training; H values are in nats.", "labels": [], "entities": []}, {"text": " Table 3: WSJ perplexity results. The best performance for each training set for each model type is highlighted in bold.", "labels": [], "entities": [{"text": "WSJ perplexity", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6514240205287933}]}, {"text": " Table 4: WSJ lattice rescoring results; all values are word-error rates. The best performance for each training set size  for each model type is highlighted in bold. Each 0.1% in error rate corresponds to about 47 errors.", "labels": [], "entities": [{"text": "WSJ lattice rescoring", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6118540366490682}]}, {"text": " Table 5: Various statistics for WSJ trigram models, with  and without a Broadcast News prior model. The first col- umn is the size of the in-domain training set in sentences.", "labels": [], "entities": [{"text": "WSJ trigram", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.5594115257263184}, {"text": "Broadcast News prior", "start_pos": 73, "end_pos": 93, "type": "DATASET", "confidence": 0.8945652445157369}]}, {"text": " Table 6: WSJ perplexity and lattice rescoring results for  domain adaptation models. Values on the left are perplex- ities and values on the right are word-error rates.", "labels": [], "entities": []}]}