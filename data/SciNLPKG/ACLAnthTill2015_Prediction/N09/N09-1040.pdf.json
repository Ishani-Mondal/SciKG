{"title": [{"text": "Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion", "labels": [], "entities": [{"text": "Hierarchical Text Segmentation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7020434339841207}]}], "abstractContent": [{"text": "This paper presents a novel unsupervised method for hierarchical topic segmentation.", "labels": [], "entities": [{"text": "hierarchical topic segmentation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.7296968102455139}]}, {"text": "Lexical cohesion-the workhorse of unsu-pervised linear segmentation-is treated as a multi-scale phenomenon, and formalized in a Bayesian setting.", "labels": [], "entities": []}, {"text": "Each word token is modeled as a draw from a pyramid of latent topic models, where the structure of the pyramid is constrained to induce a hierarchical segmentation.", "labels": [], "entities": []}, {"text": "Inference takes the form of a coordinate-ascent algorithm, iterating between two steps: a novel dynamic program for obtaining the globally-optimal hierarchical segmentation, and collapsed variational Bayesian inference over the hidden variables.", "labels": [], "entities": []}, {"text": "The resulting system is fast and accurate, and compares well against heuristic alternatives.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recovering structural organization from unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs.", "labels": [], "entities": [{"text": "Recovering structural organization from unformatted texts or transcripts", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.890541210770607}, {"text": "natural language processing", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.6521040797233582}]}, {"text": "In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion -the idea that topically-coherent segments display consistent lexical distributions.", "labels": [], "entities": []}, {"text": "However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (.", "labels": [], "entities": []}, {"text": "This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation.", "labels": [], "entities": [{"text": "hierarchical topic segmentation", "start_pos": 126, "end_pos": 157, "type": "TASK", "confidence": 0.6266322731971741}]}, {"text": "The idea of multi-scale cohesion is illustrated by the following two examples, drawn from the Wikipedia entry for the city of Buenos Aires.", "labels": [], "entities": []}, {"text": "There are over 150 city bus lines called Colectivos ...", "labels": [], "entities": []}, {"text": "Colectivos in Buenos Aires do not have a fixed timetable, but run from 4 to several per hour, depending on the bus line and time of the day.", "labels": [], "entities": []}, {"text": "The Buenos Aires metro has six lines, 74 stations, and 52.3 km of track.", "labels": [], "entities": []}, {"text": "An expansion program is underway to extend existing lines into the outer neighborhoods.", "labels": [], "entities": []}, {"text": "Track length is expected to reach 89 km...", "labels": [], "entities": []}, {"text": "The two sections are both part of a high-level segment on transportation.", "labels": [], "entities": []}, {"text": "Words in bold are characteristic of the subsections (buses and trains, respectively), and do not occur elsewhere in the transportation section; words in italics occur throughout the high-level section, but not elsewhere in the article.", "labels": [], "entities": []}, {"text": "This paper shows how multi-scale cohesion can be captured in a Bayesian generative model and exploited for unsupervised hierarchical topic segmentation.", "labels": [], "entities": [{"text": "hierarchical topic segmentation", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6733143130938212}]}, {"text": "Latent topic models () provide a powerful statistical apparatus with which to study discourse structure.", "labels": [], "entities": []}, {"text": "A consistent theme is the treatment of individual words as draws from multinomial language models indexed by a hidden \"topic\" associated with the word.", "labels": [], "entities": []}, {"text": "In latent Dirichlet allocation (LDA) and related models, the hidden topic for each word is unconstrained and unrelated to the hidden topic of neighboring words (given the parameters).", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7471400201320648}]}, {"text": "In this paper, the latent topics are constrained to produce a hierarchical segmentation structure, as shown in Figure 1: Each word wt is drawn from a mixture of the language models located above tin the pyramid.", "labels": [], "entities": []}, {"text": "These structural requirements simplify inference, allowing the language models to be analytically marginalized.", "labels": [], "entities": []}, {"text": "The remaining hidden variables are the scale-level assignments for each word token.", "labels": [], "entities": []}, {"text": "Given marginal distributions over these variables, it is possible to search the entire space of hierarchical segmentations in polynomial time, using a novel dynamic program.", "labels": [], "entities": []}, {"text": "Collapsed variational Bayesian inference is then used to update the marginals.", "labels": [], "entities": []}, {"text": "This approach achieves high quality segmentation on multiple levels of the topic hierarchy.", "labels": [], "entities": []}, {"text": "Source code is available at http://people.", "labels": [], "entities": []}, {"text": "csail.mit.edu/jacobe/naacl09.html.", "labels": [], "entities": [{"text": "csail.mit.edu", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9468500018119812}]}], "datasetContent": [{"text": "Corpora The dataset for evaluation is drawn from a medical textbook ().", "labels": [], "entities": []}, {"text": "The text contains 17083 sentences, segmented hierarchically into twelve high-level parts, 150 chapters, and 520 sub-chapter sections.", "labels": [], "entities": []}, {"text": "Evaluation is performed separately on each of the twelve parts, with the task of correctly identifying the chapter and section boundaries.", "labels": [], "entities": []}, {"text": "use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries.", "labels": [], "entities": [{"text": "linear topic segmentation", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6150311927000681}]}, {"text": "Practical applications of topic segmentation typically relate to more informal documents such as blogs or speech transcripts (), as formal texts such as books already contain segmentation markings provided by the author.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7217921316623688}]}, {"text": "The premise of this evaluation is that textbook corpora provide a reasonable proxy for performance on less structured data.", "labels": [], "entities": []}, {"text": "However, further clarification of this point is an important direction for future research.", "labels": [], "entities": []}, {"text": "Metrics All experiments are evaluated in terms of the commonly-used P k and WindowDiff metrics ().", "labels": [], "entities": []}, {"text": "Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other.", "labels": [], "entities": []}, {"text": "WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while P k only asks whether the two sentences are in the same segment or not.", "labels": [], "entities": [{"text": "WindowDiff", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8556729555130005}]}, {"text": "This eval-uation uses source code provided by.", "labels": [], "entities": []}, {"text": "The joint hierarchical Bayesian model described in this paper is called HIERBAYES.", "labels": [], "entities": [{"text": "HIERBAYES", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.7189196944236755}]}, {"text": "It performs a three-level hierarchical segmentation, in which the lowest level is for subchapter sections, the middle level is for chapters, and the top level spans the entire part.", "labels": [], "entities": [{"text": "hierarchical segmentation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6978953182697296}]}, {"text": "This top-level has the effect of limiting the influence of words that are common throughout the document.", "labels": [], "entities": []}, {"text": "Baseline systems As noted in Section 2, there is little related work on unsupervised hierarchical segmentation.", "labels": [], "entities": []}, {"text": "However, a straightforward baseline is a greedy approach: first segment at the top level, and then recursively feed each top-level segment to the segmenter again.", "labels": [], "entities": []}, {"text": "Any linear segmenter can be plugged into this baseline as a \"black box.\"", "labels": [], "entities": []}, {"text": "To isolate the contribution of joint inference, the greedy framework can be combined with a one-level version of the Bayesian segmentation algorithm described here.", "labels": [], "entities": [{"text": "Bayesian segmentation", "start_pos": 117, "end_pos": 138, "type": "TASK", "confidence": 0.6396974176168442}]}, {"text": "This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset.", "labels": [], "entities": [{"text": "BAYESSEG", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8126183748245239}]}, {"text": "The hierarchical segmenter built by placing BAYESSEG in a greedy algorithm is called GREEDY-BAYES.", "labels": [], "entities": [{"text": "GREEDY-BAYES", "start_pos": 85, "end_pos": 97, "type": "METRIC", "confidence": 0.9698910713195801}]}, {"text": "To identify the contribution of the Bayesian segmentation framework, we can plugin alternative linear segmenters.", "labels": [], "entities": [{"text": "Bayesian segmentation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.665624812245369}]}, {"text": "Two frequently-cited systems are LCSEG ( and TEXTSEG ().", "labels": [], "entities": [{"text": "LCSEG", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.5153725743293762}, {"text": "TEXTSEG", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9574211239814758}]}, {"text": "LC-SEG optimizes a metric of lexical cohesion based on lexical chains.", "labels": [], "entities": []}, {"text": "TEXTSEG employs a probabilistic segmentation objective that is similar to ours, but uses maximum a posteriori estimates of the language models, rather than marginalizing them out.", "labels": [], "entities": [{"text": "TEXTSEG", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5379421710968018}]}, {"text": "Other key differences are that they set \u03b1 = 1, and use a minimum description length criterion to determine segmentation granularity.", "labels": [], "entities": []}, {"text": "Both of these baselines were run using their default parametrization.", "labels": [], "entities": []}, {"text": "Finally, as a minimal baseline, UNIFORM produces a hierarchical segmentation with the ground truth number of segments per level and uniform duration per segment at each level.", "labels": [], "entities": []}, {"text": "Preprocessing The Porter (1980) stemming algorithm is applied to group equivalent lexical items.", "labels": [], "entities": []}, {"text": "A set of stop-words is also removed, using the same list originally employed by several competitive systems ().", "labels": [], "entities": []}, {"text": "presents performance results for the joint hierarchical segmenter and the three greedy baselines.", "labels": [], "entities": []}, {"text": "As shown in the table, the hierarchical system achieves the top overall performance on the harsher WindowDiff metric.", "labels": [], "entities": []}, {"text": "In general, the greedy segmenters each perform well atone of the two levels and poorly at the other level.", "labels": [], "entities": []}, {"text": "The joint hierarchical inference of HIERBAYES enables it to achieve balanced performance at the two levels.", "labels": [], "entities": [{"text": "HIERBAYES", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.8059714436531067}]}, {"text": "The GREEDY-BAYES system achieves a slightly better average P k than HIERBAYES, but has a very large gap between its P k and WindowDiff scores.", "labels": [], "entities": [{"text": "GREEDY-BAYES", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.7308590412139893}]}, {"text": "The P k metric requires only that the system correctly classify whether two points are in the same or different segments, while the WindowDiff metric insists that the exact number of interposing segments be identified correctly.", "labels": [], "entities": []}, {"text": "Thus, the generation of spurious short segments may explain the gap between the metrics.", "labels": [], "entities": []}, {"text": "LCSEG and TEXTSEG use heuristics to determine segmentation granularity; even though these methods did not score well in terms of segmentation accuracy, they were generally closer to the correct granularity.", "labels": [], "entities": [{"text": "LCSEG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8086002469062805}, {"text": "TEXTSEG", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9075960516929626}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9573166966438293}]}, {"text": "In the Bayesian methods, granularity is enforced by the Markov prior described in Section 3.2.", "labels": [], "entities": []}, {"text": "This prior was particularly ineffective for GREEDY-BAYES, which gave nearly the same number of segments at both levels, despite the different settings of the expected duration parameter d.", "labels": [], "entities": [{"text": "GREEDY-BAYES", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.6345074772834778}]}], "tableCaptions": [{"text": " Table 1: Segmentation accuracy and granularity. Both the P k and WindowDiff (WD) metrics are penalties, so lower  scores are better. The # segs columns indicate the average number of segments at each level; the gold standard  segmentation granularity is given in the UNIFORM row, which obtains this granularity by construction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9376453757286072}]}]}