{"title": [{"text": "Using Integer Linear Programming for Detecting Speech Disfluencies", "labels": [], "entities": [{"text": "Detecting Speech Disfluencies", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.8428378502527872}]}], "abstractContent": [{"text": "We present a novel two-stage technique for detecting speech disfluencies based on Integer Linear Programming (ILP).", "labels": [], "entities": [{"text": "detecting speech disfluencies", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.824233611424764}]}, {"text": "In the first stage we use state-of-the-art models for speech dis-fluency detection, in particular, hidden-event language models, maximum entropy models and conditional random fields.", "labels": [], "entities": [{"text": "speech dis-fluency detection", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.7318870921929678}]}, {"text": "During testing each model proposes possible disfluency labels which are then assessed in the presence of local and global constraints using ILP.", "labels": [], "entities": []}, {"text": "Our experimental results show that by using ILP we can improve the performance of our models with negligible cost in processing time.", "labels": [], "entities": []}, {"text": "The less training data is available the larger the improvement due to ILP.", "labels": [], "entities": [{"text": "ILP", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8560253977775574}]}], "introductionContent": [{"text": "Speech disfluencies (also known as speech repairs) occur frequently in spontaneous speech and can pose difficulties to natural language processing (NLP) since most NLP tools (e.g. parsers, part-of-speech taggers, information extraction modules) are traditionally trained on written language.", "labels": [], "entities": [{"text": "Speech disfluencies (also known as speech repairs)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6196692023012373}, {"text": "natural language processing (NLP)", "start_pos": 119, "end_pos": 152, "type": "TASK", "confidence": 0.7220029433568319}, {"text": "part-of-speech taggers", "start_pos": 189, "end_pos": 211, "type": "TASK", "confidence": 0.6948781311511993}, {"text": "information extraction modules", "start_pos": 213, "end_pos": 243, "type": "TASK", "confidence": 0.7492039302984873}]}, {"text": "Speech disfluencies can be divided into three intervals, the reparandum, the editing term and the correction).", "labels": [], "entities": [{"text": "Speech disfluencies", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7002775818109512}, {"text": "correction", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9799124598503113}]}, {"text": "(it was) * (you know) it was set In the above example, \"it was\" is the reparandum, \"you know\" is the editing term and the remaining sentence is the correction.", "labels": [], "entities": [{"text": "correction", "start_pos": 148, "end_pos": 158, "type": "METRIC", "confidence": 0.9503986239433289}]}, {"text": "The asterisk marks the interruption point at which the speaker halts the original utterance in order to start the repair.", "labels": [], "entities": []}, {"text": "The editing term is optional and consists of one or more filled pauses (e.g. uh, uh-huh) or discourse markers (e.g. you know, so).", "labels": [], "entities": []}, {"text": "Some researchers include editing terms in the definition of disfluencies.", "labels": [], "entities": []}, {"text": "Here we focus only on detecting repetitions (the speaker repeats some part of the utterance), revisions (the speaker modifies the original utterance) or restarts (the speaker abandons an utterance and starts over).", "labels": [], "entities": []}, {"text": "We also deal with complex disfluencies, i.e. a series of disfluencies in succession (\"I think I think uh I believe that...\").", "labels": [], "entities": []}, {"text": "In previous work many different approaches to detecting speech disfluencies have been proposed.", "labels": [], "entities": [{"text": "detecting speech disfluencies", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.8625624378522238}]}, {"text": "Different types of features have been used, e.g. lexical features only, acoustic and prosodic features only or a combination of both ().", "labels": [], "entities": []}, {"text": "Furthermore, a number of studies have been conducted on human transcriptions while other efforts have focused on detecting disfluencies from the speech recognition output.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.7445407509803772}]}, {"text": "In this paper we propose a novel framework for speech disfluency detection based on Integer Linear Programming (ILP).", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.8304167985916138}]}, {"text": "With Linear Programming (LP) problems the goal is to optimize a linear objective function subject to linear equality and linear inequality constraints.", "labels": [], "entities": []}, {"text": "When some or all the variables of the objective function and the constraints are non-negative integers, LP becomes ILP.", "labels": [], "entities": [{"text": "LP", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9517276287078857}]}, {"text": "ILP has recently attracted much attention in NLP.", "labels": [], "entities": [{"text": "ILP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9752249717712402}]}, {"text": "It has been applied to several problems including sentence compression and relation extraction ().", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.8139319121837616}, {"text": "relation extraction", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8613850474357605}]}, {"text": "Some of these methods (e.g. () follow the two-stage approach of first hypothesizing a list of possible answers using a classifier and then selecting the best answer by applying ILP.", "labels": [], "entities": []}, {"text": "We have adopted this twostage approach and applied it to speech disfluency detection.", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.8640154401461283}]}, {"text": "In the first stage we use state-of-the-art tech-niques for speech disfluency detection, in particular, Hidden-Event Language Models (HELMs), Maximum Entropy (ME) models and Conditional Random Fields (CRFs) ().", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.795959879954656}]}, {"text": "Nevertheless, any other classification method could be used instead.", "labels": [], "entities": []}, {"text": "During testing each classifier proposes possible labels which are then assessed in the presence of local and global constraints using ILP.", "labels": [], "entities": []}, {"text": "ILP makes the final decision taking into account both the constraints and the output of the classifier.", "labels": [], "entities": [{"text": "ILP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8829792737960815}]}, {"text": "In the following we use the Switchboard corpus and only lexical features for training our 3 classifiers.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9199330508708954}]}, {"text": "Then we apply ILP to the output of each classifier.", "labels": [], "entities": []}, {"text": "Our goal is not to investigate the best set of features or achieve the best possible results.", "labels": [], "entities": []}, {"text": "In that case we could also use prosodic features as they have been shown to improve performance.", "labels": [], "entities": []}, {"text": "Our target is to show that by using ILP we can improve with negligible cost in processing time the performance of state-of-the-art techniques, especially when not much training data is available.", "labels": [], "entities": []}, {"text": "The novelty of our work lies in the two following areas: First, we propose a novel approach for detecting disfluencies with improvements over stateof-the-art models (HELMs, ME models and CRFs) that use similar lexical features.", "labels": [], "entities": [{"text": "detecting disfluencies", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.8768603205680847}]}, {"text": "Although the twostage approach is not unique, as discussed above, the formulation of the ILP objective function and constraints for disfluency detection is entirely novel.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 132, "end_pos": 152, "type": "TASK", "confidence": 0.7853891849517822}]}, {"text": "Second, we compare our models using the tasks of both detecting the interruption point and finding the beginning of the reparandum.", "labels": [], "entities": []}, {"text": "In previous work () Hidden Markov Models (combination of decision trees and HELMs) and ME models were trained to detect the interruption points and then heuristic rules were applied to find the correct onset of the reparandum in contrast to CRFs that were trained to detect both points at the same time.", "labels": [], "entities": [{"text": "ME", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9293023943901062}]}, {"text": "The structure of the paper is as follows: In section 2 we describe our data set.", "labels": [], "entities": []}, {"text": "In section 3 we describe our approach in detail.", "labels": [], "entities": []}, {"text": "Then in section 4 we present our experiments and provide results.", "labels": [], "entities": []}, {"text": "Finally in section 5 we present our conclusion and propose future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For HELMs we use the SRI Statistical Language Modeling Toolkit.", "labels": [], "entities": [{"text": "SRI Statistical Language Modeling", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.7603870034217834}]}, {"text": "Each utterance is a sequence of word and Part-of-Speech (POS) pairs fed into the toolkit: i/prp BE was/vbd IE one/cd IP i/prp was/vbd right/jj.", "labels": [], "entities": [{"text": "BE", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9960864782333374}]}, {"text": "We report results with 4-grams.", "labels": [], "entities": []}, {"text": "For ME we use the OpenNLP MaxEnt toolkit and for CRFs the toolkit CRF++ (both available from sourceforge).", "labels": [], "entities": [{"text": "OpenNLP MaxEnt toolkit", "start_pos": 18, "end_pos": 40, "type": "DATASET", "confidence": 0.8978579243024191}]}, {"text": "We experimented with different sets of features and we achieved the best results with the following setup (i is the location of the word or POS in the sentence): Our word features are Our POS features have the same structure as the word features.", "labels": [], "entities": []}, {"text": "For ILP we use the lp solve software also available from sourceforge.", "labels": [], "entities": []}, {"text": "For evaluating the performance of our models we use standard metrics proposed in the literature, i.e. F-score and NIST Error Rate.", "labels": [], "entities": [{"text": "F-score", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9975244402885437}, {"text": "NIST Error Rate", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.7692707180976868}]}, {"text": "We report results for BE and IP.", "labels": [], "entities": [{"text": "BE", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.7517659068107605}, {"text": "IP", "start_pos": 29, "end_pos": 31, "type": "DATASET", "confidence": 0.7378898859024048}]}, {"text": "F-score is the harmonic mean of precision and recall (we equally weight precision and recall).", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9821531176567078}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9995560050010681}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9964956641197205}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9988901019096375}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9969545602798462}]}, {"text": "Precision is computed as the ratio of the correctly identified tags X to all the tags X detected by the model (where X is BE or IP).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9898746013641357}, {"text": "BE", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9852591753005981}]}, {"text": "Recall is the ratio of the correctly identified tags X to all the tags X that appear in the reference utterance.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9900071620941162}]}, {"text": "The NIST Error Rate measures the average number of incorrectly identified tags per reference tag, i.e. the sum of insertions, deletions and substitutions divided by the total number of reference tags ().", "labels": [], "entities": [{"text": "NIST Error Rate", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.7624167402585348}]}, {"text": "To calculate the level of statistical significance we always use the Wilcoxon signed-rank test.", "labels": [], "entities": []}, {"text": "HELMs and ME models benefit more from the ILP model than the ILP-model (ME only for the BE tag) whereas ILP-appears to perform better than ILP for CRFs.", "labels": [], "entities": [{"text": "BE", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.941839873790741}]}, {"text": "shows the effect of the training set size on the error rates only for BE due to space restrictions.", "labels": [], "entities": [{"text": "BE", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9867419600486755}]}, {"text": "The trend is similar for IP.", "labels": [], "entities": []}, {"text": "The test set is always the same.", "labels": [], "entities": []}, {"text": "Both ILP and ILP-perform better than the plain models.", "labels": [], "entities": []}, {"text": "This is true even when the ILP and ILP-models are trained with less data (HELMs and ME models only).", "labels": [], "entities": []}, {"text": "Note that HELM (or ME) with ILP or ILP-trained on 25% of the data performs better than plain HELM (or ME) trained on 100% of the data (p<10 \u22128 ).", "labels": [], "entities": []}, {"text": "This is very important because collecting and annotating data is expensive and timeconsuming.", "labels": [], "entities": []}, {"text": "Furthermore, for CRFs in particular the training process takes long especially for large data sets.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.9431531429290771}]}, {"text": "In our experiments CRFs took about 400 iterations to converge (approx. 136 min for the whole training set) whereas ME models took approx. 48 min for the same number of iterations and training set size.", "labels": [], "entities": []}, {"text": "Also, ME models trained with 100 iterations (approx. 11 min) performed better than ME models trained with 400 iterations.", "labels": [], "entities": []}, {"text": "The cost of applying ILP is negligible since the process is fast and applied during testing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparative results between our models.", "labels": [], "entities": []}, {"text": " Table 3: Error rate variation for BE depending on the  training set size.", "labels": [], "entities": [{"text": "Error rate variation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9713818430900574}, {"text": "BE", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.6167778372764587}]}]}