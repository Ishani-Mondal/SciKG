{"title": [{"text": "Semi-Supervised Lexicon Mining from Parenthetical Expressions in Monolingual Web Pages", "labels": [], "entities": [{"text": "Lexicon Mining from Parenthetical Expressions in Monolingual Web Pages", "start_pos": 16, "end_pos": 86, "type": "TASK", "confidence": 0.7558329734537337}]}], "abstractContent": [{"text": "This paper presents a semi-supervised learning framework for mining Chinese-English lexicons from large amount of Chinese Web pages.", "labels": [], "entities": []}, {"text": "The issue is motivated by the observation that many Chinese neologisms are accompanied by their English translations in the form of parenthesis.", "labels": [], "entities": []}, {"text": "We classify par-enthetical translations into bilingual abbreviations , transliterations, and translations.", "labels": [], "entities": []}, {"text": "A frequency-based term recognition approach is applied for extracting bilingual abbreviations.", "labels": [], "entities": [{"text": "term recognition", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7080046087503433}, {"text": "extracting bilingual abbreviations", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.8605469663937887}]}, {"text": "A self-training algorithm is proposed for mining transliteration and translation lexicons.", "labels": [], "entities": []}, {"text": "In which, we employ available lexicons in terms of morpheme levels, i.e., phoneme correspondences in transliteration and grapheme (e.g., suffix, stem, and prefix) correspondences in translation.", "labels": [], "entities": []}, {"text": "The experimental results verified the effectiveness of our approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual lexicons, as lexical or phrasal parallel corpora, are widely used in applications of multilingual language processing, such as statistical machine translation (SMT) and cross-lingual information retrieval.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 137, "end_pos": 174, "type": "TASK", "confidence": 0.8109792073567709}, {"text": "cross-lingual information retrieval", "start_pos": 179, "end_pos": 214, "type": "TASK", "confidence": 0.6807233889897665}]}, {"text": "However, it is a time-consuming task for constructing large-scale bilingual lexicons by hand.", "labels": [], "entities": []}, {"text": "There are many facts cumber the manual development of bilingual lexicons, such as the continuous emergence of neologisms (e.g., new technical terms, personal names, abbreviations, etc.), the difficulty of keeping up with the neologisms for lexicographers, etc.", "labels": [], "entities": []}, {"text": "In order to turn the facts to a better way, one of the simplest strategies is to automatically mine large-scale lexicons from corpora such as the daily updated Web.", "labels": [], "entities": []}, {"text": "Generally, there are two kinds of corpora used for automatic lexicon mining.", "labels": [], "entities": [{"text": "automatic lexicon mining", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6741186877091726}]}, {"text": "One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to) algorithms and cognate clues play a central role ().", "labels": [], "entities": []}, {"text": "presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account.", "labels": [], "entities": []}, {"text": "The other is multilingual parallel and comparable corpora (e.g., Wikipedia 1 ), wherein features such as cooccurrence frequency and context are popularly employed (.", "labels": [], "entities": []}, {"text": "In this paper, we focus on a special type of comparable corpus, parenthetical translations.", "labels": [], "entities": []}, {"text": "The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses.", "labels": [], "entities": []}, {"text": "This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets.", "labels": [], "entities": []}, {"text": "Formally, a parenthetical translation can be expressed by the following pattern, f 1 f 2 ...", "labels": [], "entities": []}, {"text": "f J (e 1 e 2 ... e I ).", "labels": [], "entities": []}, {"text": "(1) Here, f 1 f 2 ...", "labels": [], "entities": []}, {"text": "f J (f J 1 ), the pre-parenthesis text, denotes the word sequence of some language other than English; and e 1 e 2 ...", "labels": [], "entities": []}, {"text": "e I (e I 1 ), the in-parenthesis text, denotes the word sequence of English.", "labels": [], "entities": []}, {"text": "We separate parenthetical translations into three categories:  bilingual abbreviation, transliteration, and translation.", "labels": [], "entities": []}, {"text": "illustrates examples of these categories.", "labels": [], "entities": []}, {"text": "We address several characteristics of parenthetical translations that differ from traditional comparable corpora.", "labels": [], "entities": []}, {"text": "The first is that they only appear in monolingual Web pages or documents, and the context information of e I 1 is unknown.", "labels": [], "entities": []}, {"text": "Second, frequency and word number of e I 1 are frequently small.", "labels": [], "entities": [{"text": "frequency", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9943530559539795}, {"text": "word number of e I 1", "start_pos": 22, "end_pos": 42, "type": "METRIC", "confidence": 0.892487237850825}]}, {"text": "This is because parenthetical translations are only used when the authors thought that f J 1 contained some neologism(s) which deserved further explanation in another popular language (e.g., English).", "labels": [], "entities": []}, {"text": "Thus, traditional context based approaches are not applicable and frequency based approaches may yield low recall while with high precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9994255304336548}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9923783540725708}]}, {"text": "Furthermore, cognate clues such as orthographic features are not applicable between language pairs such as English and Chinese.", "labels": [], "entities": []}, {"text": "Parenthetical translation mining faces the following issues.", "labels": [], "entities": [{"text": "Parenthetical translation mining", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9023435711860657}]}, {"text": "First, we need to distinguish parenthetical translations from parenthetical expressions, since parenthesis has many functions (e.g., defining abbreviations, elaborations, ellipsis, citations, annotations, etc.) other than translation.", "labels": [], "entities": []}, {"text": "Second, the left boundary (denoted as in) of the preparenthesis text need to be determined to get rid of the unrelated words.", "labels": [], "entities": []}, {"text": "Third, we need further distinguish different translation types, such as bilingual abbreviation, the mixture of translation and transliteration, as shown in.", "labels": [], "entities": []}, {"text": "In order to deal with these problems, supervised) and unsupervised () methods have been proposed.", "labels": [], "entities": []}, {"text": "However, supervised approaches are restricted by the quality and quantity of manually constructed training data, and unsupervised approaches are totally frequency-based without using any semantic clues.", "labels": [], "entities": []}, {"text": "In contrast, we propose a semi-supervised framework for mining parenthetical translations.", "labels": [], "entities": [{"text": "mining parenthetical translations", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.7171980341275533}]}, {"text": "We apply a monolingual abbreviation extraction approach to bilingual abbreviation extraction.", "labels": [], "entities": [{"text": "abbreviation extraction", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6694145798683167}, {"text": "bilingual abbreviation extraction", "start_pos": 59, "end_pos": 92, "type": "TASK", "confidence": 0.7196636199951172}]}, {"text": "We construct an English-syllable to Chinese-pinyin transliteration model which is selftrained using phonemic similarity measurements.", "labels": [], "entities": []}, {"text": "We further employ our cascaded translation model () which is self-trained based on morpheme-level translation similarity.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "We briefly review the related work in the next section.", "labels": [], "entities": []}, {"text": "Our system framework and self-training algorithm is described in Section 3.", "labels": [], "entities": []}, {"text": "Bilingual abbreviation extraction, self-trained transliteration models and cascaded translation models are described in Section 4, 5, and 6, respectively.", "labels": [], "entities": [{"text": "Bilingual abbreviation extraction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7585096756617228}, {"text": "cascaded translation", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.599731907248497}]}, {"text": "In Section 7, we evaluate our mined lexicons by Wikipedia.", "labels": [], "entities": []}, {"text": "We conclude in Section 8 finally.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Wanfang Chinese-English technical term dictionary 12 , which contains 525,259 entries in total, was used for training and testing.", "labels": [], "entities": [{"text": "Wanfang Chinese-English technical term dictionary 12", "start_pos": 4, "end_pos": 56, "type": "DATASET", "confidence": 0.8442429800828298}]}, {"text": "10,000 entries were randomly selected as the test set and the remaining as the training set.", "labels": [], "entities": []}, {"text": "Again, we investigated the scalability of the self-trained cascaded translation model by respectively using 20, 40, 60, 80, and 100 percent of initial training data.", "labels": [], "entities": []}, {"text": "An aggressive similar- . Table 4: The BLEU score of self-trained cascaded translation model under five initial training sets.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9707404673099518}, {"text": "cascaded translation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.5972891449928284}]}, {"text": "ity measurement was used for selecting new training samples: first char(c) = first char(C ) \u2227 min{ed(c, C )}.", "labels": [], "entities": []}, {"text": "(6) Here, we judge if the first characters of c and C are similar or not.", "labels": [], "entities": []}, {"text": "c was gained by deleting zero or more characters from the left side off J 1 . When more than one c satisfied this condition, the c that had the smallest edit distance with C was selected.", "labels": [], "entities": []}, {"text": "When applying Algorithm 1 for translation lexicon mining, we took e I 1 as one input for decoding instead of decoding each word respectively.", "labels": [], "entities": [{"text": "translation lexicon mining", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.9000511566797892}]}, {"text": "Only the top-1 output (C ) was used for comparing.", "labels": [], "entities": []}, {"text": "The algorithm stopped in five iterations when we set the terminal threshold to be 2000.", "labels": [], "entities": []}, {"text": "For simplicity, only illustrates the BLEU score of the cascaded translation model under five initial training sets.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9704337120056152}]}, {"text": "For the reason that there are finite phonemes in English and Chinese while the semantic correspondences between the two languages tend to be infinite, is harder to be analyzed than.", "labels": [], "entities": []}, {"text": "When initially using 40%, 60%, and 100% training data for self-training, the results tend to be better at some iterations.", "labels": [], "entities": []}, {"text": "We gain 35.6%, 5.2%, and 9.4% relative improvements, respectively.", "labels": [], "entities": []}, {"text": "However, the results tend to be worse when 20% and 80% training data were used initially, with 11.6% and 3.0% minimal relative loss.", "labels": [], "entities": []}, {"text": "The best BLEU scores tend to be better when more initial training data are available.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9981862902641296}]}, {"text": "We mined 1,038,617, 1,025,606, 1,048,761, 1,056,311, and 1,060,936 distinct entries under the five types of initial training data.", "labels": [], "entities": []}, {"text": "The 1,060,936 entries are taken as the final translation lexicon for further comparison.", "labels": [], "entities": []}, {"text": "Similar to (, the transliteration models were trained and tested on the LDC ChineseEnglish Named Entity Lists Version 1.0 8 . The original list contains 572,213 English people names with Chinese transliterations.", "labels": [], "entities": [{"text": "LDC ChineseEnglish Named Entity Lists Version 1.0 8", "start_pos": 72, "end_pos": 123, "type": "DATASET", "confidence": 0.9362043961882591}]}, {"text": "We extracted 74,725 entries in which the English names also appeared in the CMU pronunciation dictionary.", "labels": [], "entities": [{"text": "CMU pronunciation dictionary", "start_pos": 76, "end_pos": 104, "type": "DATASET", "confidence": 0.9236253499984741}]}, {"text": "We randomly selected 3,736 entries as an open testing set and the remaining entries as a training set 9 . The results were evaluated using the character/pinyin-based 4-gram BLEU score (), word error rate (WER), position independent word error rate (PER), and exact match (EMatch).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 173, "end_pos": 183, "type": "METRIC", "confidence": 0.9744747877120972}, {"text": "word error rate (WER)", "start_pos": 188, "end_pos": 209, "type": "METRIC", "confidence": 0.8955616255601248}, {"text": "position independent word error rate (PER)", "start_pos": 211, "end_pos": 253, "type": "METRIC", "confidence": 0.8166690617799759}, {"text": "exact match (EMatch)", "start_pos": 259, "end_pos": 279, "type": "METRIC", "confidence": 0.9271159648895264}]}, {"text": "reports the performances of the three models and the comparison based on EMatch.", "labels": [], "entities": [{"text": "EMatch", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9115188121795654}]}, {"text": "From the results, we can easily draw the conclusion that the hybrid model performs the best under the maximal phrase length (mpl, the maximal phrase length allowed in Moses) from 1 to 8.", "labels": [], "entities": [{"text": "maximal phrase length", "start_pos": 102, "end_pos": 123, "type": "METRIC", "confidence": 0.7552302976449331}]}, {"text": "The performances of the models converge at or right after mpl = 4.", "labels": [], "entities": []}, {"text": "The pinyin-based WER of the hybrid model is 39.13%, comparable to the pinyin error rate 39.6%, reported in (: The BLEU score of self-trained h4 transliteration models under four selection strategies.", "labels": [], "entities": [{"text": "WER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.8860769867897034}, {"text": "pinyin error rate", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.9659680922826132}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9977564215660095}]}, {"text": "nt (n=1..5) stands for the n-th iteration.", "labels": [], "entities": []}, {"text": "self-training experiments are pursued on the hybrid model taking mpl to be 4 (short for h4, hereafter).", "labels": [], "entities": []}, {"text": "As former mentioned, we investigate the scalability of the self-trained h4 model by respectively using 5, 10, 20, 40, 60, 80, and 100 percent of initial training data, and the performances of using exact matching (em) or approximate matching (am, line 8 in Algorithm 1) on the top-1 and top-5 outputs (line 7 in Algorithm 1) for selecting new training samples.", "labels": [], "entities": []}, {"text": "We used edit distance (ed) to measure the em and am similarities: When applying Algorithm 1 for transliteration lexicon mining, we decode each word in e I 1 respectively.", "labels": [], "entities": [{"text": "edit distance (ed)", "start_pos": 8, "end_pos": 26, "type": "METRIC", "confidence": 0.8883263826370239}, {"text": "transliteration lexicon mining", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.6539503037929535}]}, {"text": "The algorithm terminated in five iterations when we set the terminal threshold (Line 13 in Algorithm 1) to be 100.", "labels": [], "entities": []}, {"text": "For simplicity, only illustrates the BLEU score of h4 models under four selection strategies.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9753478467464447}]}, {"text": "From this table, we can draw the following conclusions.", "labels": [], "entities": []}, {"text": "First, with fewer initial training data, the improvement is better.", "labels": [], "entities": []}, {"text": "The best relative improvements additional Web resources as are 8.74%, 8.46%, 4.41%, 0.67%, 0.68%, 0.32%, and 1.39%, respectively.", "labels": [], "entities": []}, {"text": "Second, using top-5 and em for new training data selection performs the best among the four strategies.", "labels": [], "entities": []}, {"text": "Compared under each iteration, using top-5 is better than using top-1; em is better than am; and top-5 with am is a little better than top-1 with em.", "labels": [], "entities": []}, {"text": "We mined distinct entries under the six types of initial data with top-5 plus em strategy.", "labels": [], "entities": []}, {"text": "The 50,313 entries are taken as the final transliteration lexicon for further comparison.", "labels": [], "entities": []}, {"text": "We have mined three kinds of lexicons till now, an abbreviation lexicon containing 107,856 dis-  , we compare our final mined lexicon with a dictionary extracted from Wikipedia, the biggest multilingual free-content encyclopedia on the Web.", "labels": [], "entities": []}, {"text": "We extracted the titles of Chinese and English Wikipedia articles 13 that are linked to each other.", "labels": [], "entities": []}, {"text": "Since most titles contain less than five words, we take a linked title pair as a translation entry without considering the word alignment relation between the words inside the titles.", "labels": [], "entities": []}, {"text": "The result lexicon contains 105,320 translation pairs between 103,823 Chinese titles and 103,227 English titles.", "labels": [], "entities": []}, {"text": "Obviously, only a small percentage of titles have more than one translation.", "labels": [], "entities": []}, {"text": "Whenever there is more than one translation, we take the candidate entry as correct if and only if it matches one of the translations.", "labels": [], "entities": []}, {"text": "Moreover, we compare our semi-supervised approach with an unsupervised approach (.", "labels": [], "entities": []}, {"text": "took \u03d5 2 (f j , e i ) score 14 ( with threshold 0.001 as the word alignment probability in a word alignment algorithm, Competitive Link.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.6974860429763794}, {"text": "word alignment", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.6949279457330704}]}, {"text": "Competitive Link tries to align an unlinked e i with an unlinked f j by the condition that \u03d5 2 (f j , e i ) is the biggest.", "labels": [], "entities": []}, {"text": "relaxed the unlinked constraints to allow consecutive sequence of words on one side to be linked to the same word on the other side  , where a is the number off J 1 (e I 1 ) containing both ei and fj; (a + b) is the number off J 1 (e I 1 ) containing ei; (a + c) is the number off J 1 (e I 1 ) containing fj; and dis the number off J 1 (e I 1 ) containing neither ei nor fj.", "labels": [], "entities": []}, {"text": "15 Instead of requiring both ei and fj to have no previous linkboundary inside f J 1 is determined when each e i in e I 1 is aligned.", "labels": [], "entities": []}, {"text": "After applying the modified Competitive Link on the partially parallel corpus which includes 12,444,264 entries (Section 4.2), we obtained 2,628,366 distinct pairs.", "labels": [], "entities": []}, {"text": "shows the results of the two lexicons evaluated under Wikipedia title dictionary.", "labels": [], "entities": [{"text": "Wikipedia title dictionary", "start_pos": 54, "end_pos": 80, "type": "DATASET", "confidence": 0.8760794599850973}]}, {"text": "The coverage is measured by the percentage of titles which appears in the mined lexicon.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9729692339897156}]}, {"text": "We then check whether the translation in the mined lexicon is an exact match of one of the translations in the Wikipedia lexicon.", "labels": [], "entities": [{"text": "Wikipedia lexicon", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.9344533085823059}]}, {"text": "Through comparing the results, our mined lexicon is comparable with the lexicon mined in an unsupervised way.", "labels": [], "entities": []}, {"text": "Since the selection is based on phonemic and semantic clues instead of frequency, a parenthetical translation candidate will not be selected if the in-parenthetical English text is failed to be transliterated or translated.", "labels": [], "entities": []}, {"text": "This is one reason that explains why we earned a little lower coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9921126365661621}]}, {"text": "Another reason comes from the low coverage rate of seed lexicons used for self-training, only 8.65% English words in the partially parallel corpus are covered by the Wanfang dictionary.", "labels": [], "entities": [{"text": "Wanfang dictionary", "start_pos": 166, "end_pos": 184, "type": "DATASET", "confidence": 0.9206349849700928}]}], "tableCaptions": [{"text": " Table 5: The results of our lexicon and an unsupervised- mined lexicon (Lin et al., 2008) evaluated under  Wikipedia title dictionary. Cov is short for coverage.", "labels": [], "entities": [{"text": "Wikipedia title dictionary. Cov", "start_pos": 108, "end_pos": 139, "type": "DATASET", "confidence": 0.7703687906265259}]}]}