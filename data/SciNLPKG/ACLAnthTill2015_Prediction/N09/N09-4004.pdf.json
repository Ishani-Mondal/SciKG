{"title": [{"text": "Extracting World and Linguistic Knowledge from Wikipedia", "labels": [], "entities": [{"text": "Extracting World and Linguistic Knowledge", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.894574499130249}]}], "abstractContent": [{"text": "Overview Many research efforts have been devoted to develop robust statistical modeling techniques for many NLP tasks.", "labels": [], "entities": []}, {"text": "Our field is now moving towards more complex tasks (e.g. RTE, QA), which require to complement these methods with a semantically rich representation based on world and linguistic knowledge (i.e. annotated linguistic data).", "labels": [], "entities": []}, {"text": "In this tutorial we show several approaches to extract this knowledge from Wikipedia.", "labels": [], "entities": []}, {"text": "This resource has attracted the attention of much work in the AI community, mainly because it provides semi-structured information and a large amount of manual annotations.", "labels": [], "entities": []}, {"text": "The purpose of this tutorial is to introduce Wikipedia as a resource to the NLP community and to provide an introduction for NLP researchers both from a scientific and a practical (i.e. data acquisition and processing issues) perspective.", "labels": [], "entities": [{"text": "data acquisition and processing issues)", "start_pos": 186, "end_pos": 225, "type": "TASK", "confidence": 0.8556401431560516}]}, {"text": "Outline The tutorial is divided into three main parts: 1.", "labels": [], "entities": [{"text": "Outline", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9269049763679504}]}, {"text": "Extracting world knowledge from Wikipedia.", "labels": [], "entities": [{"text": "Extracting world knowledge from Wikipedia", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8681553363800049}]}, {"text": "We review methods aiming at extracting fully struc-tured world knowledge from the content of the online encyclopedia.", "labels": [], "entities": []}, {"text": "We show how to take categories, hyperlinks and infoboxes as building blocks fora semantic network with unlabeled relations between the concepts.", "labels": [], "entities": []}, {"text": "The task of taxonomy induction then boils down to labeling the relations between these concepts, e.g. with isa, part-of, instance-of, located-in, etc. relations.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8849467039108276}]}, {"text": "2. Leveraging linguistic knowledge from Wikipedia.", "labels": [], "entities": [{"text": "Leveraging linguistic knowledge from Wikipedia", "start_pos": 3, "end_pos": 49, "type": "TASK", "confidence": 0.8442978382110595}]}, {"text": "Wikipedia provides shallow markup annotations which can be interpreted as manual annotations of linguistic phenomena.", "labels": [], "entities": []}, {"text": "These 'annotations' include word boundaries, word senses, named entities, translations of concepts in many languages.", "labels": [], "entities": []}, {"text": "Furthermore , Wikipedia can be used as a multilingual comparable corpus.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}