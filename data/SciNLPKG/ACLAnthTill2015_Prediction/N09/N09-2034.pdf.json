{"title": [{"text": "A Speech Understanding Framework that Uses Multiple Language Models and Multiple Understanding Models \u2020", "labels": [], "entities": [{"text": "Speech Understanding", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.7405285537242889}]}], "abstractContent": [{"text": "The optimal combination of language model (LM) and language understanding model (LUM) varies depending on available training data and utterances to be handled.", "labels": [], "entities": [{"text": "language understanding model (LUM)", "start_pos": 51, "end_pos": 85, "type": "METRIC", "confidence": 0.6745675653219223}]}, {"text": "Usually, a lot of effort and time are needed to find the optimal combination.", "labels": [], "entities": []}, {"text": "Instead, we have designed and developed anew framework that uses multiple LMs and LUMs to improve speech understanding accuracy under various situations.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7543038427829742}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.8385401368141174}]}, {"text": "As one implementation of the framework , we have developed a method for selecting the most appropriate speech understanding result from several candidates.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.7325158417224884}]}, {"text": "We use two LMs and three LUMs, and thus obtain six combinations of them.", "labels": [], "entities": []}, {"text": "We empirically show that our method improves speech understanding accuracy.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7951047420501709}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9362122416496277}]}, {"text": "The performance of the oracle selection suggests further potential improvements in our system.", "labels": [], "entities": []}], "introductionContent": [{"text": "The speech understanding component in a spoken dialogue system consists of an automatic speech recognition (ASR) component and a language understanding (LU) component.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7215011268854141}, {"text": "automatic speech recognition (ASR)", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.7711264888445536}]}, {"text": "To develop a speech understanding component, we need to prepare an ASR language model (LM) and a language understanding model (LUM) for the dialogue domain of the system.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7066956460475922}]}, {"text": "There are many types of LMs such as finite-state grammars and N-grams, and many types of LUMs such as finite-state transducers (FST), weighted finite-state transducers (WFST), and keyphrase-extractors (extractor).", "labels": [], "entities": []}, {"text": "Selecting a suitable combination of LM and LUM is necessary for robust speech understanding against various user utterances.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7253989279270172}]}, {"text": "Conventional studies of speech understanding have investigated which LM and LUM give the best performance by using fixed training and test data such as the Air Travel Information System (ATIS) corpus.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7733901143074036}, {"text": "Air Travel Information System (ATIS) corpus", "start_pos": 156, "end_pos": 199, "type": "DATASET", "confidence": 0.7186403013765812}]}, {"text": "However, in real system development, resources such as training data for statistical models and efforts to write finite-state grammars vary according to the available human resources or budgets.", "labels": [], "entities": []}, {"text": "Domain-dependent training data are particularly difficult to obtain.", "labels": [], "entities": []}, {"text": "Therefore, in conventional system development, system developers determine the types of LM and LUM by trial and error.", "labels": [], "entities": []}, {"text": "Every LM and LUM has some advantages and disadvantages, so it is difficult fora single combination of LM and LUM to gain high accuracy except in a situation involving a lot of training data and effort.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9987053871154785}]}, {"text": "Therefore, using multiple speech understanding methods is a more effective approach.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7648625075817108}]}, {"text": "In this paper, we propose a speech understanding framework called \"Multiple Language models and Multiple Understanding models (MLMU)\", in which multiple LMs and LUMs are used, to achieve better performance under the various development situations.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7022810876369476}]}, {"text": "It selects the best speech understanding result from the multiple results generated by arbitrary combinations of LMs and LUMs.", "labels": [], "entities": []}, {"text": "So far there have been several attempts to improve ASR and speech understanding using multiple speech recognizers and speech understanding modules.", "labels": [], "entities": [{"text": "ASR", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9978741407394409}, {"text": "speech understanding", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7195783108472824}]}, {"text": "ROVER: Flow of speech understanding in MLMU els.", "labels": [], "entities": [{"text": "ROVER", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9182165861129761}, {"text": "speech understanding", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.688679575920105}, {"text": "MLMU els", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8177016973495483}]}, {"text": "The work is different from our study in the following two points: it does not deal with speech understanding, and it assumes that each ASR is welldeveloped and achieves high accuracy fora variety of speech inputs.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.725327655673027}, {"text": "ASR", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.92574143409729}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9950653910636902}]}, {"text": "used multiple LMs to deal with both in-grammar utterances and out-of-grammar utterances, but did not mention language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7086482793092728}]}, {"text": "used multiple LUMs, but just a single language model.", "labels": [], "entities": []}, {"text": "System developers list available LMs and LUMs for each system's domain, and the system understands utterances by using these models.", "labels": [], "entities": []}, {"text": "The framework selects one understanding result from multiple results or calculates a confidence score of the result by using the generated multiple understanding results.", "labels": [], "entities": []}, {"text": "MLMU can improve speech understanding for the following reason.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.7454530298709869}]}, {"text": "The performance of each speech understanding (a combination of LM and LUM) might not be very high when either training data for the statistical model or available expertise and effort for writing grammar are insufficient.", "labels": [], "entities": []}, {"text": "In such cases, some utterances might not be covered by the system's finite-state grammar LM, and probability estimation in the statistical models may not be very good.", "labels": [], "entities": []}, {"text": "Using multiple speech understanding models is expected to solve this problem because each model has different specialities.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.742903470993042}]}, {"text": "For example, finitestate grammar LMs and FST-based LUMs achieve high accuracy in recognizing and understanding ingrammar utterances, whereas out-of-grammar utterances are covered by N-gram models and LUMs based on WFST and keyphrase-extractors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9976608753204346}, {"text": "WFST", "start_pos": 214, "end_pos": 218, "type": "DATASET", "confidence": 0.8553072214126587}]}, {"text": "Therefore it is more possible that the understanding results of MLMU will include the correct result than a case when a single understanding model is used.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a preliminary experiment to show the potential of the framework by using the two LMs and three LUMs noted in Section 3.1.", "labels": [], "entities": []}, {"text": "We used 3,055 utterances in the rent-a-car reservation domain (.", "labels": [], "entities": []}, {"text": "We used Julius (ver. 4.0.2) as the speech recognizer and a 3000-state phonetic tied-mixture (PTM) triphone model as the acoustic model 1 . ASR accuracy in mora accuracy when using the FSG and the N-gram model were 71.9% and 75.5% respectively.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.6937746107578278}, {"text": "ASR", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.6328372955322266}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.8867307901382446}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.961366593837738}, {"text": "FSG", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.9290843605995178}]}, {"text": "We used concept error rates (CERs) to represent the speech understanding accuracy, which is calculated as fol- We manually annotated whether an understanding result of each utterance was corrector not, and used them as training data to fit the coefficients a i1 , . .", "labels": [], "entities": [{"text": "concept error rates (CERs)", "start_pos": 8, "end_pos": 34, "type": "METRIC", "confidence": 0.8603940606117249}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.8337315320968628}]}, {"text": ", aim , bi .  We fitted the coefficients of regression functions and selected understanding results with a 10-fold cross validation.", "labels": [], "entities": [{"text": "aim", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9843037724494934}]}, {"text": "lists the CERs based on combinations of single LM and LUM and by our method.", "labels": [], "entities": []}, {"text": "Of all combinations of single LM and LUM, the best accuracy was obtained with (5) (N-gram + WFST).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9994550347328186}, {"text": "WFST", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.956652045249939}]}, {"text": "Our method improved by 2.6 points over (5).", "labels": [], "entities": []}, {"text": "Although we achieved a lower CER, we used a lot of data to estimate logistic regression coefficients.", "labels": [], "entities": [{"text": "CER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9977595806121826}]}, {"text": "Such a large amount of data may not be available in areal situation.", "labels": [], "entities": []}, {"text": "We will conduct more experiments by changing the amount of training data.", "labels": [], "entities": []}, {"text": "also shows the accuracy of the oracle selection, which selected the best speech understanding result manually.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9997581839561462}]}, {"text": "The CER of the oracle selection was 13.5%, a significant improvement compared to all combinations of a LM and LUM.", "labels": [], "entities": [{"text": "CER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9989956021308899}]}, {"text": "There is no combination of a LM and LUM whose understanding results were not selected at all in the oracle selection and our method's selection.", "labels": [], "entities": []}, {"text": "These results show that using multiple LMs and multiple LUMs can potentially improve speech understanding accuracy.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.8128040432929993}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9022732973098755}]}], "tableCaptions": [{"text": " Table 2: CERs [%] for each speech understanding  method  speech understanding method  (LM + LUM)  CER  (1) FSG + FST  26.9  (2) FSG + WFST  29.9  (3) FSG + extractor  27.1  (4) N-gram + FST  35.2  (5) N-gram + WFST  25.3  (6) N-gram + extractor  26.0  selection from (1) through (6) (our method) 22.7  oracle selection  13.5", "labels": [], "entities": [{"text": "CERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8829061388969421}, {"text": "speech understanding  method  speech understanding", "start_pos": 28, "end_pos": 78, "type": "TASK", "confidence": 0.6724792003631592}, {"text": "CER", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.868763267993927}, {"text": "FSG", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.7923921346664429}, {"text": "FSG", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.8588314652442932}, {"text": "WFST", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.7889134883880615}]}]}