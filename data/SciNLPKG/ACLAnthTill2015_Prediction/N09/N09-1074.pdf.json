{"title": [{"text": "Improved Syntactic Models for Parsing Speech with Repairs *", "labels": [], "entities": [{"text": "Parsing Speech with Repairs", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.8909667730331421}]}], "abstractContent": [{"text": "This paper introduces three new syntactic models for representing speech with repairs.", "labels": [], "entities": []}, {"text": "These models are developed to test the intuition that the erroneous parts of speech repairs (reparanda) are not generated or recognized as such while occurring, but only after they have been corrected.", "labels": [], "entities": [{"text": "speech repairs (reparanda)", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7850327014923095}]}, {"text": "Thus, they are designed to minimize the differences in grammar rule applications between fluent and disfluent speech containing similar structure.", "labels": [], "entities": []}, {"text": "The three models considered in this paper are also designed to isolate the mechanism of impact, by systematically exploring different variables.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work in recognition of speech with repairs has shown that syntactic cues to speech repair can improve both overall parsing accuracy and detection of repaired sections).", "labels": [], "entities": [{"text": "speech repair", "start_pos": 83, "end_pos": 96, "type": "TASK", "confidence": 0.7180947512388229}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9376237988471985}]}, {"text": "These techniques work by explictly modeling the structure of speech repair, specifically the tendency of repairs to follow unfinished constituents of the same category.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7172148078680038}]}, {"text": "This is the essence of what was termed the well-formedness rule by Willem in his psycholinguistic studies of repair.", "labels": [], "entities": []}, {"text": "The work presented here uses the same motivations as those cited above (to be described in more detail below), in that it attempts to model the syntactic structure relating unfinished erroneous con- * This research was supported by NSF CAREER award 0447685.", "labels": [], "entities": [{"text": "NSF CAREER award 0447685", "start_pos": 232, "end_pos": 256, "type": "DATASET", "confidence": 0.632649190723896}]}, {"text": "The views expressed are not necessarily endorsed by the sponsors.", "labels": [], "entities": []}, {"text": "stituents to the repair of those constituents.", "labels": [], "entities": []}, {"text": "However, this work attempts to improve on those models by focusing on the generative process used by a speaker in creating the repair.", "labels": [], "entities": []}, {"text": "This is done first by eschewing any labels representing the presence of an erroneous constituent while processing the text.", "labels": [], "entities": []}, {"text": "This modeling representation reflects the intuition that speakers do not intend to generate erroneous speech -they intend their speech to be fluent, or a correction to an error, and can stop very quickly when an error is noticed.", "labels": [], "entities": []}, {"text": "This corresponds to Levelt's Main Interruption Rule, which states that a speaker will \"Stop the flow of speech immediately upon detecting the occasion of repair.\"", "labels": [], "entities": [{"text": "Main Interruption Rule", "start_pos": 29, "end_pos": 51, "type": "METRIC", "confidence": 0.670333206653595}]}, {"text": "Rather than attempting to recognize a special syntactic category called EDITED during the processing phase, this work introduces the REPAIRED category to signal the ending of a repaired section only.", "labels": [], "entities": [{"text": "REPAIRED", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9145470857620239}]}, {"text": "The second part of the modeling framework is the use of a right-corner transform on training data, which converts phrase-structure trees into heavily left-branching structures.", "labels": [], "entities": []}, {"text": "This transformation has been shown to represent the structure of unfinished constituents like those seen in speech repair in a natural way, leading to improved detection of speech repair.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.7031929343938828}, {"text": "speech repair", "start_pos": 173, "end_pos": 186, "type": "TASK", "confidence": 0.7363977283239365}]}, {"text": "Combining these two modeling techniques in a bottom-up parsing framework results in a parsing architecture that is a reasonable approximation to the sequential processing that must be done by the human speech processor when recognizing spoken language with repairs.", "labels": [], "entities": []}, {"text": "This parser also recognizes sentences containing speech repair with better accuracy than the previous models on which it is based.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.7169012129306793}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.998071014881134}]}, {"text": "Therefore, these syntactic models hold promise for integration into systems for processing of streaming speech.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of this model was performed using a probabilistic CYK parser 2 . This parser operates in a bottom-up fashion, building up constituent structure from the words it is given as input.", "labels": [], "entities": []}, {"text": "This parsing architecture is a good match for the structure generated by the right-corner transform because it does not need to consider any categories related to speech repair until the repaired section has been completed.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 163, "end_pos": 176, "type": "TASK", "confidence": 0.7484568655490875}]}, {"text": "Moreover, the structure of the trees means that the parser is also building up structure from left to right.", "labels": [], "entities": []}, {"text": "That mode of operation is useful for any model which purports to be potentially extensible to speech recognition or to model the human speech processor.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.782966285943985}]}, {"text": "In contrast, top-down parsers require exhaustive searches, meaning that they need to explore interpretations containing disfluency, even in the absence of syntactic cues for its existence.", "labels": [], "entities": []}, {"text": "These experiments used the Switchboard corpus (), a syntactically-annotated corpus of spontaneous dialogues between human interlocutors.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.7977087795734406}]}, {"text": "This corpus is annotated for phrase structure in much the same way as the Penn Treebank Wall Street Journal corpus, with the addition of several speech-specific categories as described in Section 2.1.", "labels": [], "entities": [{"text": "phrase structure", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.8328647911548615}, {"text": "Penn Treebank Wall Street Journal corpus", "start_pos": 74, "end_pos": 114, "type": "DATASET", "confidence": 0.9803351759910583}]}, {"text": "For training, trees in sections 2 and 3 of this corpus were transformed as described in Section 2, and rule probabilities were estimated in the usual way.", "labels": [], "entities": []}, {"text": "For testing, trees in section 4, subsections 0 and 1, were used.", "labels": [], "entities": []}, {"text": "Data from the tail end of section 4 (subsections 3 and 4) was used during development of this work.", "labels": [], "entities": []}, {"text": "Before doing any training or testing, all trees in the data set were stripped of punctuation, empty categories, typos, all categories representing repair structure, and partial words -anything that would be difficult or impossible to obtain reliably with a speech recognizer.", "labels": [], "entities": []}, {"text": "A baseline parser was then trained and tested using the split described above, achieving standard results as seen in the table below.", "labels": [], "entities": []}, {"text": "For a fair comparison to the evaluation in, the parser was given part-of-speech tags along with each word as input.", "labels": [], "entities": []}, {"text": "The structure obtained by the parser was then in the right-corner format.", "labels": [], "entities": []}, {"text": "For standardized scoring, the right-corner transform, binarization, and augmented repair annotation were undone, so that comparison was done against the nearly pristine test corpus.", "labels": [], "entities": [{"text": "augmented repair annotation", "start_pos": 72, "end_pos": 99, "type": "METRIC", "confidence": 0.7220879197120667}]}, {"text": "Several test configurations were then evaluated, and compared to three baseline approaches.", "labels": [], "entities": []}, {"text": "The two metrics used here are the standard Parseval F-measure, and Edit-finding F.", "labels": [], "entities": []}, {"text": "The first takes the F-score of labeled precision and recall of the nonterminals in a hypothesized tree relative to the gold standard tree.", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9986452460289001}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9279015064239502}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9993439316749573}]}, {"text": "The second measure marks words in the gold standard as edited if they are dominated by anode labeled EDITED, and measures the F-score of the hypothesized edited words relative to the gold standard (recall in this case is percentage of actual edited words that were hypothesized as edited, and precision is percentage of hypothesized edited words that were actually edited).", "labels": [], "entities": [{"text": "F-score", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9989873766899109}, {"text": "recall", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.9989564418792725}, {"text": "precision", "start_pos": 293, "end_pos": 302, "type": "METRIC", "confidence": 0.999448835849762}]}, {"text": "The first three lines in the table refer to baseline approaches to compare against.", "labels": [], "entities": []}, {"text": "\"Plain\" refers to a configuration with no modifications other than the removal of repair cues.", "labels": [], "entities": []}, {"text": "The next result shown is a reproducton of the results from The following three lines contain the three experimental configurations.", "labels": [], "entities": []}, {"text": "First, the configuration denoted EDITED-BIN refers to the simple binarized speech repair described in Section 2.3 (Equation 6).", "labels": [], "entities": [{"text": "EDITED-BIN", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.8310818672180176}, {"text": "binarized speech repair", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.683401087919871}]}, {"text": "REPAIRED-BIN refers to the binarized speech repair in which the labels are basically reversed from EDITED-BIN (Equation 5).", "labels": [], "entities": [{"text": "REPAIRED-BIN", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9323410391807556}, {"text": "binarized speech repair", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.6193015476067861}]}, {"text": "Finally, REPAIRED-INT refers to the speech repair type where the REPAIRED category maybe a child of a non-identity category, representing an interruption of the outermost desired constituent (Equation 4).", "labels": [], "entities": [{"text": "REPAIRED-INT", "start_pos": 9, "end_pos": 21, "type": "METRIC", "confidence": 0.948576807975769}, {"text": "speech repair", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.7268965989351273}]}], "tableCaptions": [{"text": " Table 1: Table of parsing results. Star (  *  ) indicates sig- nificance relative to the 'Standard Right Corner' baseline  (p < 0.05), dagger (  \u2020 ) indicates significance relative to  the 'Baseline' labeled result (p < 0.05). Double star and  dagger indicate highly significant results (p < 0.001).", "labels": [], "entities": [{"text": "dagger", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9817181825637817}]}]}