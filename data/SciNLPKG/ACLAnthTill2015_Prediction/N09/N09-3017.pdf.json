{"title": [{"text": "Disambiguation of Preposition Sense Using Linguistically Motivated Features", "labels": [], "entities": [{"text": "Disambiguation of Preposition Sense", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8425314128398895}]}], "abstractContent": [{"text": "In this paper, we present a supervised classification approach for disambiguation of preposition senses.", "labels": [], "entities": [{"text": "disambiguation of preposition senses", "start_pos": 67, "end_pos": 103, "type": "TASK", "confidence": 0.8104688078165054}]}, {"text": "We use the SemEval 2007 Preposition Sense Disambiguation datasets to evaluate our system and compare its results to those of the systems participating in the workshop.", "labels": [], "entities": [{"text": "SemEval 2007 Preposition Sense Disambiguation datasets", "start_pos": 11, "end_pos": 65, "type": "DATASET", "confidence": 0.698944220940272}]}, {"text": "We derived linguistically motivated features from both sides of the preposition.", "labels": [], "entities": []}, {"text": "Instead of restricting these to a fixed window size, we utilized the phrase structure.", "labels": [], "entities": []}, {"text": "Testing with five different classifiers, we can report an increased accuracy that outperforms the best system in the SemEval task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9992555975914001}, {"text": "SemEval task", "start_pos": 117, "end_pos": 129, "type": "TASK", "confidence": 0.905404269695282}]}], "introductionContent": [{"text": "Classifying instances of polysemous words into their proper sense classes (aka sense disambiguation) is potentially useful to any NLP application that needs to extract information from text or build a semantic representation of the textual information.", "labels": [], "entities": [{"text": "Classifying instances of polysemous words", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.847726845741272}, {"text": "sense disambiguation)", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8083362976710001}]}, {"text": "However, to date, disambiguation between preposition senses has not been an object of great study.", "labels": [], "entities": []}, {"text": "Instead, most word sense disambiguation work has focused upon classifying noun and verb instances into their appropriate WordNet senses.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.7232743004957835}]}, {"text": "Prepositions have mostly been studied in the context of verb complements (.", "labels": [], "entities": []}, {"text": "Like instances of other word classes, many prepositions are ambiguous, carrying different semantic meanings (including notions of instrumental, accompaniment, location, etc.) as in \"He ran with determination\", \"He ran with a broken leg\", or \"He ran with Jane\".", "labels": [], "entities": []}, {"text": "As NLP systems take more and more semantic content into account, disambiguating between preposition senses becomes increasingly important for text processing tasks.", "labels": [], "entities": [{"text": "text processing", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.7436084449291229}]}, {"text": "In order to disambiguate different senses, most systems to date use a fixed window size to derive classification features.", "labels": [], "entities": []}, {"text": "These mayor may not be syntactically related to the preposition in question, resulting-in the worst case-in an arbitrary bag of words.", "labels": [], "entities": []}, {"text": "In our approach, we make use of the phrase structure to extract words that have a certain syntactic relation with the preposition.", "labels": [], "entities": []}, {"text": "From the words collected that way, we derive higher level features.", "labels": [], "entities": []}, {"text": "In 2007, the SemEval workshop presented participants with a formal preposition sense disambiguation task to encourage the development of systems for the disambiguation of preposition senses (.", "labels": [], "entities": [{"text": "preposition sense disambiguation task", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.7130672186613083}]}, {"text": "The training and test data sets used for SemEval have been released to the general public, and we used these data to train and test our system.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9520254731178284}]}, {"text": "The SemEval workshop data consists of instances of 34 prepositions in natural text that have been tagged with the appropriate sense from the list of the common English preposition senses compiled by The Preposition Project, cf..", "labels": [], "entities": [{"text": "SemEval workshop data", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7273165384928385}]}, {"text": "The SemEval data provides a natural method for comparing the performance of preposition sense disambiguation systems.", "labels": [], "entities": [{"text": "preposition sense disambiguation", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.618012011051178}]}, {"text": "In our paper, we follow the task requirements and can thus directly compare our results to the ones from the study.", "labels": [], "entities": []}, {"text": "For evaluation, we compared our results to those of the three systems that participated in the task (MELB:; KU: Yuret (2007); IRST:).", "labels": [], "entities": [{"text": "MELB", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.8298652172088623}, {"text": "IRST", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.32983389496803284}]}, {"text": "We also used the \"first sense\" and the \"most frequent sense\" baselines (see section 3 and table 1).", "labels": [], "entities": []}, {"text": "These baselines are determined by the TPP listing and the frequency in the training data, respectively.", "labels": [], "entities": [{"text": "TPP listing", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.6853729486465454}]}, {"text": "Our system beat the baselines and outperformed the three participating systems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. It is notable that our system  produced good results with all classifiers: For three  of the classifiers, the accuracy is higher than MELB,  the winning system of the task. As expected, the  highest accuracy was achieved using the maximum  entropy classifier. Overall, our system outperformed", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9994088411331177}, {"text": "MELB", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.957280158996582}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9969649910926819}]}, {"text": " Table 1: Accuracy results on SemEval data (with 4000  features)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986276626586914}, {"text": "SemEval", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9429810047149658}]}, {"text": " Table 1. It is notable that our  system produced good results with all classifiers:  For three of the classifiers, the accuracy is higher  than MELB, the winning system of the task. As", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9995173215866089}, {"text": "MELB", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9674680829048157}]}, {"text": " Table 1. Accuracy results on SemEval-2007 data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985337257385254}, {"text": "SemEval-2007 data", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.7711288332939148}]}]}