{"title": [{"text": "Interactive Annotation Learning with Indirect Feature Voting", "labels": [], "entities": []}], "abstractContent": [{"text": "We demonstrate that a supervised annotation learning approach using structured features derived from tokens and prior annotations performs better than a bag of words approach.", "labels": [], "entities": []}, {"text": "We present a general graph representation for automatically deriving these features from labeled data.", "labels": [], "entities": []}, {"text": "Automatic feature selection based on class association scores requires a large amount of labeled data and direct voting can be difficult and error-prone for structured features , even for language specialists.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6791946142911911}]}, {"text": "We show that highlighted rationales from the user can be used for indirect feature voting and same performance can be achieved with less labeled data.We present our results on two annotation learning tasks for opinion mining from product and movie reviews.", "labels": [], "entities": [{"text": "opinion mining from product and movie reviews", "start_pos": 210, "end_pos": 255, "type": "TASK", "confidence": 0.8469433656760624}]}], "introductionContent": [{"text": "Interactive Annotation Learning is a supervised approach to learning annotations with the goal of minimizing the total annotation cost.", "labels": [], "entities": []}, {"text": "In this work, we demonstrate that with additional supervision per example, such as distinguishing discriminant features, same performance can be achieved with less annotated data.", "labels": [], "entities": []}, {"text": "Supervision for simple features has been explored in the literature ().", "labels": [], "entities": []}, {"text": "In this work, we propose an approach that seeks supervision from the user on structured features.", "labels": [], "entities": []}, {"text": "Features that capture the linguistic structure in text such as n-grams and syntactic patterns, referred to as structured features in this work, have been found to be useful for supervised learning of annotations.", "labels": [], "entities": []}, {"text": "For example, show that using features like syntactic path from constituent to predicate improves performance of a semantic parser.", "labels": [], "entities": []}, {"text": "However, often such features are \"handcrafted\" by domain experts and do not generalize to other tasks and domains.", "labels": [], "entities": []}, {"text": "In this work, we propose a general graph representation for automatically extracting structured features from tokens and prior annotations such as part of speech, dependency triples, etc.", "labels": [], "entities": []}, {"text": "shows that an approach using a large set of structured features and a feature selection procedure performs better than an approach that uses a few \"handcrafted\" features.", "labels": [], "entities": []}, {"text": "Our hypothesis is that structured features are important for supervised annotation learning and can be automatically derived from tokens and prior annotations.", "labels": [], "entities": []}, {"text": "We test our hypothesis and present our results for opinion mining from product reviews.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7380860894918442}]}, {"text": "Deriving features from the annotation graph gives us a large number of very sparse features.", "labels": [], "entities": []}, {"text": "Feature selection based on class association scores such as mutual information and chi-square have often been used to identify the most discriminant features ().", "labels": [], "entities": []}, {"text": "However, these scores are calculated from labeled data and they are not very meaningful when the dataset is small.", "labels": [], "entities": []}, {"text": "Supervised feature selection, i.e. asking the user to vote for the most discriminant features, has been used as an alternative when the training dataset is small. and seek feedback on unigram features from the user for document classification tasks.", "labels": [], "entities": [{"text": "document classification", "start_pos": 219, "end_pos": 242, "type": "TASK", "confidence": 0.7562704086303711}]}, {"text": "ask the user to suggest a few prototypes (examples) for each class and use those as features.", "labels": [], "entities": []}, {"text": "These approaches ask the annotators to identify globally rel-evant features, but certain features are difficult to vote on without the context and may take on very different meanings in different contexts.", "labels": [], "entities": []}, {"text": "Also, all these approaches have been demonstrated for unigram features and it is not clear how they can be extended straightforwardly to structured features.", "labels": [], "entities": []}, {"text": "We propose an indirect approach to interactive feature selection that makes use of highlighted rationales from the user.", "labels": [], "entities": [{"text": "interactive feature selection", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6695316831270853}]}, {"text": "A rationale () is the span of text a user highlights in support of his/her annotation.", "labels": [], "entities": []}, {"text": "Rationales also allow us to seek feedback on features in context.", "labels": [], "entities": []}, {"text": "Our hypothesis is that with rationales, we can achieve same performance with lower annotation cost and we demonstrate this for opinion mining from movie reviews.", "labels": [], "entities": [{"text": "opinion mining from movie reviews", "start_pos": 127, "end_pos": 160, "type": "TASK", "confidence": 0.8271990060806275}]}, {"text": "In Section 2, we describe the annotation graph representation and motivate the use of structured features with results on learning opinions from product reviews.", "labels": [], "entities": []}, {"text": "In Section 3, we show how rationales can be used for identifying the most discriminant features for opinion classification with less training data.", "labels": [], "entities": [{"text": "opinion classification", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.8398425579071045}]}, {"text": "We then list the conclusions we can draw from this work, followed by suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset we used is a collection of 244 Amazon's customer reviews (2962 comments) for five products (.", "labels": [], "entities": []}, {"text": "A review comment is annotated as an opinion if it expresses an opinion about an aspect of the product and the aspect is explicitly mentioned in the sentence.", "labels": [], "entities": []}, {"text": "We performed 10-fold cross validation (CV) using the Support Vector Machine (SVM) classifier in MinorThird) with the default linear kernel and chi-square feature selection to select the top 5000 features.", "labels": [], "entities": [{"text": "MinorThird", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9546778202056885}]}, {"text": "As can be seen in, an approach using degree \u2212 0 features, i.e. unigrams, part of speech and dependency triples together, outperforms using any of those features alone and this difference is significant.", "labels": [], "entities": []}, {"text": "Using degree \u2212 1 features with two nodes and an edge improves performance further.", "labels": [], "entities": []}, {"text": "However, using degree\u22120 features in addition to degree\u22121 features does not improve performance.", "labels": [], "entities": []}, {"text": "This suggests that when using higher degree features, we may leave out the features with lower degree that they subsume.", "labels": [], "entities": []}, {"text": "The data set by consists of 2000 movie reviews (1000-pos, 1000-neg) from the IMDb review archive.", "labels": [], "entities": [{"text": "IMDb review archive", "start_pos": 77, "end_pos": 96, "type": "DATASET", "confidence": 0.9578743378321329}]}, {"text": "provide rationales for 1800 reviews (900-pos, 900-neg).", "labels": [], "entities": []}, {"text": "The annotation guidelines for marking rationales are described in ().", "labels": [], "entities": []}, {"text": "An example of a rationale is: \"the movie is so badly put together that even the most casual viewer may notice the miserable pacing and stray plot threads\".", "labels": [], "entities": []}, {"text": "For a test dataset of 200 reviews, randomly selected from 1800 reviews, we varied the training data size from 50 to 500 reviews, adding 50 reviews at a time.", "labels": [], "entities": []}, {"text": "Training examples were randomly selected from the remaining 1600 reviews.", "labels": [], "entities": []}, {"text": "During testing, information about rationales is not used.", "labels": [], "entities": []}, {"text": "We used tokens 1 , part of speech and dependency triples as features.", "labels": [], "entities": []}, {"text": "We used the KStem stemmer to stem the token features.", "labels": [], "entities": [{"text": "KStem stemmer", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.8153713643550873}]}, {"text": "In order to compare the approaches at their best performing feature configuration, we varied the total number of features used, choosing from the set: {1000, 2000, 5000, 10000, 50000}.", "labels": [], "entities": []}, {"text": "We used chi-square feature selection () and the SVM learner with default settings from the Minorthird package) for these experiments.", "labels": [], "entities": [{"text": "Minorthird package", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.9529149532318115}]}, {"text": "We compare the following approaches: Base Training Dataset (BT D): We train a model from the labeled data with no feature voting.", "labels": [], "entities": [{"text": "Base Training Dataset (BT D)", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.7231514624186924}]}, {"text": "We experimented with two different settings for indirect feature voting: 1) only using features that overlap with rationales (RT D(1, 0)); 2) features from rationales weighted twice as much as features from other parts of the text).", "labels": [], "entities": [{"text": "indirect feature voting", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6661863227685293}, {"text": "RT D(1", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9089164882898331}]}, {"text": "In general, R(i, j) describes an experimental condition where features from rationales are weighted i times and other features are weighted j times.", "labels": [], "entities": [{"text": "R", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9195108413696289}]}, {"text": "In Minorthird, weighing a feature two times more than other features is equivalent to that feature occurring twice as much.", "labels": [], "entities": [{"text": "Minorthird", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.8996834754943848}]}, {"text": "Oracle voted Training Data (OTD): In order to compare indirect feature voting to direct voting on features, we simulate the user's vote on the features with class association scores from a large dataset (all 1600 documents used for selecting training documents).", "labels": [], "entities": [{"text": "Oracle voted Training Data (OTD)", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.6892064554350716}]}, {"text": "This is based on the assumption that the class association scores, such as chi-square, from a large dataset can be used as a reliable discriminator of the most relevant features.", "labels": [], "entities": []}, {"text": "This approach of simulating the oracle with large amount of labeled data has been used previously in feature voting).", "labels": [], "entities": [{"text": "feature voting", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.8132076263427734}]}], "tableCaptions": [{"text": " Table 2: Accuracy performance for four approaches, five feature con- figurations and increasing training dataset size. Accuracy reported is  averaged over five random selection of training documents for three ran- domly selected test datasets. The numbers in bold in a row represents  the best performing feature configuration for a given approach and train- ing dataset size. The approach in bold represents the best performing  approach among BT D, RT D(1, 0) and RT D(2, 1) for a given train- ing dataset size. '*' indicates significant improvement in performance  over BT D (paired t-test with p < 0.05 considered significant).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9948504567146301}, {"text": "Accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9992585778236389}]}, {"text": " Table 3: The Table reports accuracy for four approaches in a setting  similar to (", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995241165161133}]}, {"text": " Table 4: Weighted F-measure performance comparison of ranked list  of features from RT D(1, 0) & OT D(RO) and", "labels": [], "entities": [{"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9125041961669922}, {"text": "OT D(RO)", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.763842636346817}]}]}