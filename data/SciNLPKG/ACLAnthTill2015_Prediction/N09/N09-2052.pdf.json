{"title": [{"text": "Using N-gram based Features for Machine Translation System Combination", "labels": [], "entities": [{"text": "Machine Translation System Combination", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.8210956007242203}]}], "abstractContent": [{"text": "Conventional confusion network based system combination for machine translation (MT) heavily relies on features that are based on the measure of agreement of words in different translation hypotheses.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.8520784616470337}]}, {"text": "This paper presents two new features that consider agreement of n-grams in different hypotheses to improve the performance of system combination.", "labels": [], "entities": []}, {"text": "The first one is based on a sentence specific online n-gram language model, and the second one is based on n-gram voting.", "labels": [], "entities": []}, {"text": "Experiments on a large scale Chinese-to-English MT task show that both features yield significant improvements on the translation performance, and a combination of them produces even better translation results.", "labels": [], "entities": [{"text": "MT task", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.8591446280479431}, {"text": "translation", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.9569722414016724}]}], "introductionContent": [{"text": "In past years, the confusion network based system combination approach has been shown with substantial improvements in various machine translation (MT) tasks.", "labels": [], "entities": [{"text": "machine translation (MT) tasks", "start_pos": 127, "end_pos": 157, "type": "TASK", "confidence": 0.8467165182034174}]}, {"text": "Given hypotheses of multiple systems, a confusion network is built by aligning all these hypotheses.", "labels": [], "entities": []}, {"text": "The resulting network comprises a sequence of correspondence sets, each of which contains the alternative words that are aligned with each other.", "labels": [], "entities": []}, {"text": "To derive a consensus hypothesis from the confusion network, decoding is performed by selecting a path with the maximum overall confidence score among all paths that pass the confusion network).", "labels": [], "entities": []}, {"text": "The work was performed when Yong Zhao was an intern at Microsoft Research The confidence score of a hypothesis could be assigned in various ways.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 78, "end_pos": 94, "type": "METRIC", "confidence": 0.9643002450466156}]}, {"text": "used voting by frequency of word occurrences.", "labels": [], "entities": []}, {"text": "computed a word posterior probability based on voting of that word in different hypotheses.", "labels": [], "entities": [{"text": "word posterior probability", "start_pos": 11, "end_pos": 37, "type": "METRIC", "confidence": 0.6991750597953796}]}, {"text": "Moreover, the overall confidence score is usually formulated as a loglinear model including extra features including language model (LM) score, word count, etc.", "labels": [], "entities": []}, {"text": "Features based on word agreement measure are extensively studied in past work.", "labels": [], "entities": []}, {"text": "However, utilization of n-gram agreement information among the hypotheses has not been fully explored yet.", "labels": [], "entities": []}, {"text": "Moreover, it was argued that the confusion network decoding may introduce undesirable spur words that break coherent phrases.", "labels": [], "entities": []}, {"text": "Therefore, we would prefer the consensus translation that has better ngram agreement among outputs of single systems.", "labels": [], "entities": []}, {"text": "In the literature, proposed an n-gram posterior probability based LM for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9710032939910889}]}, {"text": "For each source sentence, a LM is trained on the n-best list produced by a single MT system and is used to re-rank that n-best list itself.", "labels": [], "entities": []}, {"text": "On the other hand, proposed an \"adapted\" LM for system combination, where this \"adapted\" LM is trained on translation hypotheses of the whole test corpus from all single MT systems involved in system combination.", "labels": [], "entities": [{"text": "system combination", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7767408490180969}]}, {"text": "Inspired by these ideas, we propose two new features based on n-gram agreement measure to improve the performance of system combination.", "labels": [], "entities": []}, {"text": "The first one is a sentence specific LM built on translation hypotheses of multiple systems; the second one is n-gram-voting-based confidence.", "labels": [], "entities": []}, {"text": "Experimental results are presented in the context of a large-scale Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.7175279557704926}]}], "datasetContent": [{"text": "We evaluate the proposed n-gram based features on the Chinese-to-English (C2E) test in the past NIST Open MT Evaluations.", "labels": [], "entities": [{"text": "NIST Open MT Evaluations", "start_pos": 96, "end_pos": 120, "type": "DATASET", "confidence": 0.8098292946815491}]}, {"text": "The experimental results are reported in case sensitive BLEU score).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9626551866531372}]}, {"text": "The dev set, which is used for system combination parameter training, is the newswire and newsgroup parts of NIST MT06, which contains a total of 1099 sentences.", "labels": [], "entities": [{"text": "system combination parameter training", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7093466520309448}, {"text": "NIST MT06", "start_pos": 109, "end_pos": 118, "type": "DATASET", "confidence": 0.8799552619457245}]}, {"text": "The test set is the \"current\" test set of NIST MT08, which contains 1357 sentences of newswire and webblog data.", "labels": [], "entities": [{"text": "NIST MT08", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.8457815051078796}]}, {"text": "Both dev and test sets have four reference translations per sentence.", "labels": [], "entities": []}, {"text": "Outputs from a total of eight single MT systems were combined for consensus translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9604066014289856}, {"text": "consensus translations", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.840976744890213}]}, {"text": "These selected systems are based on various translation paradigms, such as phrasal, hierarchical, and syntax-based systems.", "labels": [], "entities": []}, {"text": "Each system produces 10-best hypotheses per translation.", "labels": [], "entities": []}, {"text": "The BLEU score range for the eight individual systems are from 26.11% to 31.09% on the dev set and from 20.42% to 26.24% on the test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9762845635414124}]}, {"text": "In our experiments, a state-of-the-art system combination method proposed by  is implemented as the baseline.", "labels": [], "entities": []}, {"text": "The true-casing model proposed by is used.", "labels": [], "entities": []}, {"text": "shows results of adding the online LM feature.", "labels": [], "entities": []}, {"text": "Different LM orders up to four are tested.", "labels": [], "entities": []}, {"text": "Results show that using a 2-gram online LM yields a half BLEU point gain over the baseline.", "labels": [], "entities": [{"text": "BLEU point gain", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.9747645060221354}]}, {"text": "However, the gain is saturated after a LM order of three, and fluctuates after that.", "labels": [], "entities": []}, {"text": "shows the performance of using ngram-voting-based confidence features.", "labels": [], "entities": []}, {"text": "The best result of 31.01% is achieved when up to 4-gram confidence features are used.", "labels": [], "entities": []}, {"text": "The BLEU score keeps improving when longer n-gram confidence features are added.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9746848940849304}]}, {"text": "This indicates that the n-gram voting based confidence feature is robust to high order n-grams.", "labels": [], "entities": []}, {"text": "We further experimented with incorporating both features in the log-linear model and reported the results in.", "labels": [], "entities": []}, {"text": "Given the observation that the n-gram voting based confidence feature is more robust to high order n-grams, we further tested using different ngram orders for them.", "labels": [], "entities": []}, {"text": "As shown in, using 3-gram online LM plus 2~4-gram voting based confidence scores yields the best BLEU scores on both dev and test sets, which are 37.98% and 31.35%, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.999337375164032}]}, {"text": "This is a 0.84 BLEU point gain over the baseline on the MT08 test set.", "labels": [], "entities": [{"text": "BLEU point gain", "start_pos": 15, "end_pos": 30, "type": "METRIC", "confidence": 0.971166749795278}, {"text": "MT08 test set", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9690374533335367}]}], "tableCaptions": [{"text": " Table 1: Results of adding the n-gram online LM.  BLEU %  Dev  Test  Baseline  37.34  30.51  1-gram online LM  37.34  30.51  2-gram online LM  37.86  31.02  3-gram online LM  37.87  31.08  4-gram online LM  37.86  31.01", "labels": [], "entities": [{"text": "BLEU %  Dev  Test  Baseline  37.34  30.51  1-gram online LM  37.34  30.51  2-gram online LM  37.86  31.02  3-gram online LM  37.87  31.08  4-gram online LM  37.86  31.01", "start_pos": 51, "end_pos": 220, "type": "DATASET", "confidence": 0.664069715473387}]}, {"text": " Table 2: Results of adding n-gram voting based  confidence features.  BLEU %  Dev  Test  Baseline  37.34  30.51  + 2-gram voting  37.58  30.88  + 2~3-gram voting  37.66  30.96  + 2~4-gram voting  37.77  31.01", "labels": [], "entities": [{"text": "BLEU %  Dev  Test  Baseline", "start_pos": 71, "end_pos": 98, "type": "METRIC", "confidence": 0.9148232340812683}]}, {"text": " Table 3: Results of using both n-gram online LM  and n-gram voting based confidence features  BLEU %  Dev  Test  Baseline  37.34 30.51  2-gram LM + 2-gram voting  37.78 30.98  3-gram LM + 2~3-gram voting 37.89 31.21  4-gram LM + 2~4-gram voting 37.93 31.08  3-gram LM + 2~4-gram voting 37.98 31.35", "labels": [], "entities": [{"text": "BLEU %  Dev  Test  Baseline", "start_pos": 95, "end_pos": 122, "type": "METRIC", "confidence": 0.8749799609184266}]}]}