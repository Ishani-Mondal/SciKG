{"title": [{"text": "Learning Bilingual Linguistic Reordering Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Bilingual Linguistic Reordering", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.6159804264704386}, {"text": "Statistical Machine Translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.8417927622795105}]}], "abstractContent": [{"text": "In this paper, we propose a method for learning reordering model for BTG-based statistical machine translation (SMT).", "labels": [], "entities": [{"text": "BTG-based statistical machine translation (SMT)", "start_pos": 69, "end_pos": 116, "type": "TASK", "confidence": 0.770807317325047}]}, {"text": "The model focuses on linguistic features from bilingual phrases.", "labels": [], "entities": []}, {"text": "Our method involves extracting reordering examples as well as features such as part-of-speech and word class from aligned parallel sentences.", "labels": [], "entities": []}, {"text": "The features are classified with special considerations of phrase lengths.", "labels": [], "entities": [{"text": "phrase lengths", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.6887029111385345}]}, {"text": "We then use these features to train the maximum entropy (ME) reordering model.", "labels": [], "entities": []}, {"text": "With the model, we performed Chinese-to-English translation tasks.", "labels": [], "entities": [{"text": "Chinese-to-English translation tasks", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.7157731354236603}]}, {"text": "Experimental results show that our bilingual linguistic model outper-forms the state-of-the-art phrase-based and BTG-based SMT systems by improvements of 2.41 and 1.31 BLEU points respectively.", "labels": [], "entities": [{"text": "BTG-based SMT", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.5596252530813217}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9958311915397644}]}], "introductionContent": [{"text": "Bracketing Transduction Grammar (BTG) is a special case of Synchronous Context Free Grammar (SCFG), with binary branching rules that are either straight or inverted.", "labels": [], "entities": [{"text": "Bracketing Transduction Grammar (BTG)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.844987948735555}]}, {"text": "BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness.", "labels": [], "entities": [{"text": "BTG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7748121619224548}, {"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9941225647926331}]}, {"text": "In BTG, the ratio of legal alignments and all possible alignment in a translation pair drops drastically especially for long sentences, yet it still covers most of the syntactic diversities between two languages.", "labels": [], "entities": []}, {"text": "It is common to utilize phrase translation in BTG systems.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7989068329334259}]}, {"text": "For example in (), source sentences are segmented into phrases.", "labels": [], "entities": []}, {"text": "Each sequences of consecutive phrases, mapping to cells in a CKY matrix, are then translated through a bilingual phrase table and scored as implemented in ().", "labels": [], "entities": []}, {"text": "In other words, their system shares the same phrase table with standard phrase-based SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.8750165104866028}]}], "datasetContent": [{"text": "We perform Chinese-to-English translation task on NIST MT-06 test set, and use Moses and MEBTG as our competitors.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.6845579594373703}, {"text": "NIST MT-06 test set", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.9408088624477386}]}, {"text": "The bilingual training data containing 2.2M sentences pairs from Hong Kong Parallel Text (LDC2004T08) and Xinhua News Agency (LDC2007T09), with length shorter than 60, is used to train the translation and reordering model.", "labels": [], "entities": [{"text": "Hong Kong Parallel Text (LDC2004T08) and Xinhua News Agency (LDC2007T09)", "start_pos": 65, "end_pos": 137, "type": "DATASET", "confidence": 0.8629804658038276}, {"text": "translation", "start_pos": 189, "end_pos": 200, "type": "TASK", "confidence": 0.9809849262237549}]}, {"text": "The source sentences are tagged and segmented with CKIP Chinese word segmentation system.", "labels": [], "entities": [{"text": "CKIP Chinese word segmentation", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.8568376898765564}]}, {"text": "About 35M reordering examples are extracted from top 1.1M sentence pairs with higher alignment scores.", "labels": [], "entities": []}, {"text": "We generate 171K features for lexicalized model used in MEBTG system, and 1.41K features for our proposed reordering model.", "labels": [], "entities": [{"text": "MEBTG", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.8516451120376587}]}, {"text": "For our language model, we use Xinhua news from English Gigaword Third Edition (LDC2007T07) to build a trigram model with SRILM toolkit.", "labels": [], "entities": [{"text": "Xinhua news from English Gigaword Third Edition (LDC2007T07)", "start_pos": 31, "end_pos": 91, "type": "DATASET", "confidence": 0.9022003769874573}]}, {"text": "Our development set for running minimum error rate training is NIST MT-08 test set, with sentence lengths no more than 20.", "labels": [], "entities": [{"text": "NIST MT-08 test set", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.9065544158220291}]}, {"text": "We report the experimental results on NIST MT-06 test set.", "labels": [], "entities": [{"text": "NIST MT-06 test set", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.9265600442886353}]}, {"text": "Our evaluation metric is BLEU () with caseinsensitive matching from unigram to four-gram.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9989939332008362}]}, {"text": "The overall result of our experiment is shown in.", "labels": [], "entities": []}, {"text": "The lexicalized MEBTG system proposed by uses first words on adjacent blocks as lexical features, and outperforms phrasebased Moses with default distortion model and enhanced lexicalized model, by 1.1 and 0.23 BLEU points respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 210, "end_pos": 214, "type": "METRIC", "confidence": 0.9991174340248108}]}, {"text": "This suggests lexicalized Moses and MEBTG with context information outperforms distance-based distortion model.", "labels": [], "entities": []}, {"text": "Besides, MEBTG with structure constraints has better global reordering estimation than unstructured Moses, while incorporating their local reordering ability by using phrase tables.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: List of feature sets in the form of  {\"T1L=14\", \"T1R=X\", \"T2L=14\", \"T2R=50\"}.", "labels": [], "entities": []}, {"text": " Table 2: Performances of various systems.", "labels": [], "entities": []}, {"text": " Table 4: Performances of BTG systems with dif- ferent representativeness.", "labels": [], "entities": []}, {"text": " Table 5: Different representativeness with word  classification.", "labels": [], "entities": [{"text": "word  classification", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7651293873786926}]}]}