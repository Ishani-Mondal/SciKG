{"title": [{"text": "A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches", "labels": [], "entities": [{"text": "Similarity and Relatedness", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.855802039305369}]}], "abstractContent": [{"text": "This paper presents and compares WordNet-based and distributional similarity approaches.", "labels": [], "entities": []}, {"text": "The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented.", "labels": [], "entities": []}, {"text": "Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.", "labels": [], "entities": [{"text": "RG", "start_pos": 81, "end_pos": 83, "type": "DATASET", "confidence": 0.9271939396858215}, {"text": "WordSim353 datasets", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.8840841352939606}]}, {"text": "Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted fora cross-lingual task with minor losses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring semantic similarity and relatedness between terms is an important problem in lexical semantics.", "labels": [], "entities": []}, {"text": "It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.750586062669754}, {"text": "Word Sense Disambiguation", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.589993804693222}, {"text": "Information Extraction", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.7539705336093903}, {"text": "Information Retrieval", "start_pos": 173, "end_pos": 194, "type": "TASK", "confidence": 0.826229989528656}]}, {"text": "The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) ( and those inducing distributional properties of words from corpora (.", "labels": [], "entities": []}, {"text": "In this paper, we explore both families.", "labels": [], "entities": []}, {"text": "For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9595091938972473}]}, {"text": "Previous work suggests that distributional similarities suffer from certain limitations, which make them less useful than knowledge resources for semantic similarity.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7926384806632996}]}, {"text": "For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and highlight other problems that stem from the imbalance and sparseness of the corpora.", "labels": [], "entities": []}, {"text": "Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets.", "labels": [], "entities": []}, {"text": "An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information Retrieval or CL sponsored search.", "labels": [], "entities": [{"text": "cross-lingual (CL) similarity identification", "start_pos": 18, "end_pos": 62, "type": "TASK", "confidence": 0.6448009312152863}, {"text": "CL Information Retrieval", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.6466047465801239}, {"text": "CL sponsored search", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.5825810333093008}]}, {"text": "A discussion on the differences between learning similarity and relatedness scores is provided.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first present the WordNet-based method, followed by the distributional methods.", "labels": [], "entities": []}, {"text": "Section 4 is devoted to the evaluation and results on the monolingual and crosslingual tasks.", "labels": [], "entities": []}, {"text": "Section 5 presents some analysis, including learning curves for distributional methods, the use of distributional similarity to improve WordNet similarity, the contrast between similarity and relatedness, and the combination of methods.", "labels": [], "entities": []}, {"text": "Section 6 presents related work, and finally, Section 7 draws the conclusions and mentions future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have used two standard datasets.", "labels": [], "entities": []}, {"text": "The first one, RG, consists of 65 pairs of words collected by, who had them judged by 51 human subjects in a scale from 0.0 to 4.0 according to their similarity, but ignoring any other possible semantic relationships that might appear between the terms.", "labels": [], "entities": []}, {"text": "The second dataset, WordSim353   latedness are annotated without any distinction.", "labels": [], "entities": [{"text": "WordSim353   latedness", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.7940777242183685}]}, {"text": "Several studies indicate that the human scores consistently have very high correlations with each other, thus validating the use of these datasets for evaluating semantic similarity.", "labels": [], "entities": []}, {"text": "For the cross-lingual evaluation, the two datasets were modified by translating the second word in each pair into Spanish.", "labels": [], "entities": []}, {"text": "Two humans translated simultaneously both datasets, with an inter-tagger agreement of 72% for RG and 84% for WordSim353.", "labels": [], "entities": [{"text": "RG", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.5311924815177917}, {"text": "WordSim353", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.9807302951812744}]}, {"text": "shows the Spearman correlation obtained on the RG and WordSim353 datasets, including the interval at 0.95 of confidence .", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.7341051399707794}, {"text": "RG", "start_pos": 47, "end_pos": 49, "type": "DATASET", "confidence": 0.9390981793403625}, {"text": "WordSim353 datasets", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.9605453014373779}]}], "tableCaptions": [{"text": " Table 1: Spearman correlation results for the various WordNet-based  models and distributional models. CW=Context Windows, BoW=bag  of words, Syn=syntactic vectors. For Syn, the window size is actually  the tree-depth for the governors and descendants. For examples, G1", "labels": [], "entities": [{"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.714221715927124}]}, {"text": " Table 3: Results obtained by the different methods on the Span- ish/English cross-lingual datasets. The \u2206 column shows the perfor- mance difference with respect to the results on the original dataset.", "labels": [], "entities": [{"text": "Span- ish/English cross-lingual datasets", "start_pos": 59, "end_pos": 99, "type": "DATASET", "confidence": 0.6852197476795742}]}, {"text": " Table 4: Results obtained replacing unknown words with their most  similar three words (WordSim353 dataset).", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.9870535135269165}]}, {"text": " Table 5: Results obtained on the WordSim353 dataset and on the two  similarity and relatedness subsets.", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.9888370633125305}]}, {"text": " Table 8: Our best results for the MC dataset.", "labels": [], "entities": [{"text": "MC dataset", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.83088219165802}]}]}