{"title": [{"text": "Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction", "labels": [], "entities": [{"text": "Soft Parameter Tying", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.6410454014937083}]}], "abstractContent": [{"text": "We present a family of priors over probabilis-tic grammar weights, called the shared logistic normal distribution.", "labels": [], "entities": []}, {"text": "This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing anew way to encode prior knowledge about an unknown grammar.", "labels": [], "entities": []}, {"text": "We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.", "labels": [], "entities": []}, {"text": "We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6565867960453033}]}], "introductionContent": [{"text": "Probabilistic grammars have become an important tool in natural language processing.", "labels": [], "entities": [{"text": "Probabilistic grammars", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7642968595027924}, {"text": "natural language processing", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.654289036989212}]}, {"text": "They are most commonly used for parsing and linguistic analysis, but are now commonly seen in applications like machine translation () and question answering ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9789643883705139}, {"text": "linguistic analysis", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.6515215784311295}, {"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8153875768184662}, {"text": "question answering", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9281864762306213}]}, {"text": "An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning-both from labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors.", "labels": [], "entities": []}, {"text": "There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities ) to putting non-parametric priors over derivations) to learning the set of states in a grammar (.", "labels": [], "entities": []}, {"text": "Bayesian methods offer an elegant framework for combining prior knowledge with data.", "labels": [], "entities": []}, {"text": "The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable.", "labels": [], "entities": [{"text": "Bayesian grammar learning", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.7762260834376017}, {"text": "approximating probabilistic inference", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.7505965232849121}]}, {"text": "Most commonly variational) or sampling techniques are applied).", "labels": [], "entities": []}, {"text": "Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family.", "labels": [], "entities": []}, {"text": "Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability.", "labels": [], "entities": []}, {"text": "Following work by for topic models, proposed an alternative to Dirichlet priors for probabilistic grammars, based on the logistic normal (LN) distribution over the probability simplex.", "labels": [], "entities": []}, {"text": "used this prior to softly tie grammar weights through the covariance parameters of the LN.", "labels": [], "entities": []}, {"text": "The prior encodes information about which grammar rules' weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.", "labels": [], "entities": []}, {"text": "The contribution of this paper is two-fold.", "labels": [], "entities": []}, {"text": "First, from the modeling perspective, we present a generalization of the LN prior of, showing how to extend the use of the LN prior to tie between any grammar weights in a probabilistic grammar (instead of only allowing weights within the same multinomial distribution to covary).", "labels": [], "entities": []}, {"text": "Second, from the experimental perspective, we show how such flexibility in parameter tying can help in unsupervised grammar learning in the well-known monolingual setting and in anew bilingual setting where grammars for two languages are learned at once (without parallel corpora).", "labels": [], "entities": []}, {"text": "Our method is based on a distribution which we call the shared logistic normal distribution, which is a distribution over a collection of multinomials from different probability simplexes.", "labels": [], "entities": []}, {"text": "We provide a variational EM algorithm for inference.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In \u00a72, we give a brief explanation of probabilistic grammars and introduce some notation for the specific type of dependency grammar used in this paper, due to.", "labels": [], "entities": []}, {"text": "In \u00a73, we present our model and a variational inference algorithm for it.", "labels": [], "entities": []}, {"text": "In \u00a74, we report on experiments for both monolingual settings and a bilingual setting and discuss them.", "labels": [], "entities": []}, {"text": "We discuss future work ( \u00a75) and conclude in \u00a76.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments involve data from two treebanks: the Wall Street Journal Penn treebank) and the Chinese treebank ().", "labels": [], "entities": [{"text": "Wall Street Journal Penn treebank)", "start_pos": 53, "end_pos": 87, "type": "DATASET", "confidence": 0.9695729712645212}, {"text": "Chinese treebank", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.9429841041564941}]}, {"text": "In both cases, following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure.", "labels": [], "entities": []}, {"text": "For English, we train on \u00a72-21, tune on \u00a722 (without using annotated data), and report final results on \u00a723.", "labels": [], "entities": []}, {"text": "For Chinese, we train on \u00a71-270, use \u00a7301-1151 for development and report testing results on \u00a7271-300.", "labels": [], "entities": []}, {"text": "To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus.", "labels": [], "entities": [{"text": "gold standard corpus", "start_pos": 92, "end_pos": 112, "type": "DATASET", "confidence": 0.73606805006663}]}, {"text": "This performance measure is also known as attachment accuracy.", "labels": [], "entities": [{"text": "attachment", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.8588671684265137}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.7766194343566895}]}, {"text": "We considered two parsing methods after extracting a point estimate for the grammar: the most probable \"Viterbi\" parse (argmax y p(y | x, \u03b8)) and the minimum Bayes risk (MBR) parse (argmin y E p(y |x,\u03b8) [(y; x, y )]) with dependency attachment error as the loss function.", "labels": [], "entities": [{"text": "argmax y p", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9478035171826681}, {"text": "Bayes risk (MBR)", "start_pos": 158, "end_pos": 174, "type": "METRIC", "confidence": 0.8198535203933716}]}, {"text": "Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing.", "labels": [], "entities": [{"text": "MBR parsing", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.7008559107780457}, {"text": "MBR parsing", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.7398328185081482}]}, {"text": "We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above.", "labels": [], "entities": []}, {"text": "The attachment accuracy for this set of experiments is described in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9489582777023315}]}, {"text": "The baselines include right attachment (where each word is attached to the word to its right), MLE via EM (), and empirical Bayes with Dirichlet and LN priors).", "labels": [], "entities": [{"text": "MLE", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.559510350227356}]}, {"text": "We also include a \"ceiling\" (DMV trained using supervised MLE from the training sentences' trees).", "labels": [], "entities": []}, {"text": "For English, we see that tying nouns, verbs or adjectives improves performance compared to the LN baseline.", "labels": [], "entities": []}, {"text": "Tying both nouns and verbs improves performance a bit more.", "labels": [], "entities": []}, {"text": "Leveraging information from one language for the task of disambiguating another language has received considerable attention.", "labels": [], "entities": []}, {"text": "Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.", "labels": [], "entities": []}, {"text": "Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly.", "labels": [], "entities": []}, {"text": "Shar-5 presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora.", "labels": [], "entities": []}, {"text": "ing information between those two models is done by softly tying grammar weights in the two hidden grammars.", "labels": [], "entities": []}, {"text": "We first merge the models for English and Chinese by taking a union of the multinomial families of each and the corresponding prior parameters.", "labels": [], "entities": []}, {"text": "We then add a normal expert that ties between the parts of speech in the respective partition structures for both grammars together.", "labels": [], "entities": []}, {"text": "Parts of speech are matched through the single coarse tagset (footnote 4).", "labels": [], "entities": []}, {"text": "For example, with TIEV, let V = V Eng \u222a V Chi be the set of part-of-speech tags which belong to the verb category for either treebank.", "labels": [], "entities": []}, {"text": "Then, we tie parameters for all part-of-speech tags in V . We tested this joint model for each of TIEV, TIEN, TIEV&N, and TIEA.", "labels": [], "entities": [{"text": "TIEV", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.6989882588386536}, {"text": "TIEN", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.5808731913566589}, {"text": "TIEA", "start_pos": 122, "end_pos": 126, "type": "DATASET", "confidence": 0.7846335768699646}]}, {"text": "After running the inference algorithm which learns the two models jointly, we use unseen data to test each learned model separately.", "labels": [], "entities": []}, {"text": "includes the results for these experiments.", "labels": [], "entities": []}, {"text": "The performance on English improved significantly in the bilingual setting, achieving highest performance with TIEV&N.", "labels": [], "entities": [{"text": "TIEV&N", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.7926060160001119}]}, {"text": "Performance with Chinese is also the highest in the bilingual setting, with TIEA and TIEV&N.", "labels": [], "entities": [{"text": "TIEA", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7048928737640381}, {"text": "TIEV&N", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.8728183309237162}]}], "tableCaptions": [{"text": " Table 1: Attachment accuracy of different models, on test  data from the Penn Treebank and the Chinese Treebank  of varying levels of difficulty imposed through a length  filter. Attach-Right attaches each word to the word on  its right and the last word to $. Bold marks best overall  accuracy per length bound, and  \u2020 marks figures that are  not significantly worse (binomial sign test, p < 0.05).", "labels": [], "entities": [{"text": "Attachment", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8395307660102844}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.7226162552833557}, {"text": "Penn Treebank", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9963330328464508}, {"text": "Chinese Treebank", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.9862945079803467}, {"text": "accuracy", "start_pos": 287, "end_pos": 295, "type": "METRIC", "confidence": 0.9912191033363342}, {"text": "binomial sign test", "start_pos": 370, "end_pos": 388, "type": "METRIC", "confidence": 0.7949758569399515}]}]}