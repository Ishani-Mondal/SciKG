{"title": [{"text": "WordNet::SenseRelate::AllWords - A Broad Coverage Word Sense Tagger that Maximizes Semantic Relatedness", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9535239338874817}, {"text": "Broad Coverage Word Sense Tagger", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.5569556891918183}, {"text": "Maximizes Semantic Relatedness", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.8151155312856039}]}], "abstractContent": [{"text": "WordNet::SenseRelate::AllWords is a freely available open source Perl package that assigns a sense to every content word (known to WordNet) in a text.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.961158275604248}]}, {"text": "It finds the sense of each word that is most related to the senses of surrounding words, based on measures found in WordNet::Similarity.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9561145901679993}]}, {"text": "This method is shown to be competitive with results from recent evaluations including SENSEVAL-2 and SENSEVAL-3.", "labels": [], "entities": [{"text": "SENSEVAL-2", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.6773419976234436}, {"text": "SENSEVAL-3", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.8174182176589966}]}], "introductionContent": [{"text": "Word sense disambiguation is the task of assigning a sense to a word based on the context in which it occurs.", "labels": [], "entities": [{"text": "Word sense disambiguation is the task of assigning a sense to a word based on the context in which it occurs", "start_pos": 0, "end_pos": 108, "type": "Description", "confidence": 0.7129744433221363}]}, {"text": "This is one of the central problems in Natural Language Processing, and has along history of research.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.6479110320409139}]}, {"text": "A great deal of progress has been made in using supervised learning to build models of disambiguation that assign a sense to a single target word in context.", "labels": [], "entities": []}, {"text": "This is sometimes referred to as the lexical sample or target word formulation of the task.", "labels": [], "entities": []}, {"text": "However, to be effective, supervised learning requires many manually disambiguated examples of a single target word in different contexts to serve as training data to learn a classifier for that word.", "labels": [], "entities": []}, {"text": "While the resulting models are often quite accurate, manually creating training data in sufficient volume to cover even a few words is very time consuming and error prone.", "labels": [], "entities": []}, {"text": "Worse yet, creating sufficient training data to coverall the different words in a text is essentially impossible, and has never even been attempted.", "labels": [], "entities": []}, {"text": "Despite these difficulties, word sense disambiguation is often a necessary step in NLP and can't simply be ignored.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7224262952804565}]}, {"text": "The question arises as to how to develop broad coverage sense disambiguation modules that can be deployed in a practical setting without investing huge sums in manual annotation efforts.", "labels": [], "entities": []}, {"text": "Our answer is WordNet::SenseRelate::AllWords (SR-AW), a method that uses knowledge already available in the lexical database WordNet to assign senses to every content word in text, and as such offers broad coverage and requires no manual annotation of training data.", "labels": [], "entities": []}, {"text": "SR-AW finds the sense of each word that is most related or most similar to those of its neighbors in the sentence, according to any of the ten measures available in WordNet::Similarity ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.924257755279541}]}, {"text": "It extends WordNet::SenseRelate::TargetWord, a lexical sample word sense disambiguation algorithm that finds the maximum semantic relatedness between a target word and its neighbors.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.6775513490041097}]}, {"text": "SR-AW was originally developed by) (through version 0.06) and is now being significantly enhanced.", "labels": [], "entities": [{"text": "SR-AW", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9559276103973389}]}], "datasetContent": [{"text": "We have evaluated SR-AW using three corpora that have been manually annotated with senses from WordNet.", "labels": [], "entities": [{"text": "SR-AW", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.5228637456893921}, {"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9829695224761963}]}, {"text": "These include the SemCor corpus, and the SENSEVAL-2 and SENSEVAL-3 corpora.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.7414872348308563}]}, {"text": "SemCor is made up of more than 200,000 words of running text from news articles found in the Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.9859859645366669}]}, {"text": "The SENSEVAL data sets are each approximately 4,000 words of running text from Wall Street Journal news articles from the Penn Treebank.", "labels": [], "entities": [{"text": "SENSEVAL data sets", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.831638753414154}, {"text": "Wall Street Journal news articles from the Penn Treebank", "start_pos": 79, "end_pos": 135, "type": "DATASET", "confidence": 0.9264476166831123}]}, {"text": "Note that only the words known to WordNet in these corpora have been sense tagged.", "labels": [], "entities": [{"text": "WordNet in these corpora", "start_pos": 34, "end_pos": 58, "type": "DATASET", "confidence": 0.8864961117506027}]}, {"text": "As a result, there are 185,273 sense tagged words in SemCor, 2,260 in SENSEVAL-2, and 1,937 in SENSEVAL-3.", "labels": [], "entities": []}, {"text": "We have used versions of these corpora where the WordNet senses have been mapped to WordNet 3.0 2 . In we report results using Precision (P), Recall (R), and F-Measure (F).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 127, "end_pos": 140, "type": "METRIC", "confidence": 0.9463954120874405}, {"text": "Recall (R)", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.9366034716367722}, {"text": "F-Measure (F)", "start_pos": 158, "end_pos": 171, "type": "METRIC", "confidence": 0.9518315196037292}]}, {"text": "We use three window sizes in these experiments, three WordNet::Similarity measures (lch, jcn, and lesk),and three different corpora : SemCor (SC), SENSEVAL-2 (S2), SENSEVAL-3 (S3).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.9283930659294128}]}, {"text": "These experiments were carried outwith version 0.17 of SR-AW.", "labels": [], "entities": [{"text": "SR-AW", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9768459796905518}]}, {"text": "For all corpora we observe the same patterns.", "labels": [], "entities": []}, {"text": "The lesk measure tends to result in much higher recall with smaller window sizes, since it is able to measure similarity between words with any parts of speech, whereas lch and jcn are limited to making noun-noun and verb-verb measurements.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9988873600959778}]}, {"text": "But, as the window size increases so does recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9962378740310669}]}, {"text": "Precision continues to increase for lesk as the window size increases.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9962122440338135}]}, {"text": "Our best results come from using the lesk measure with a window size of 15.", "labels": [], "entities": []}, {"text": "For SemCor this results in an F-measure of 61%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9995700716972351}]}, {"text": "For SENSEVAL-2 it 2 http://www.cse.unt.edu/\u02dcrada/downloads.html results in an F-measure of 59%, and for SENSEVAL-3 it results in an F-measure of 54%.", "labels": [], "entities": [{"text": "SENSEVAL-2", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.8534567952156067}, {"text": "F-measure", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9994081258773804}, {"text": "SENSEVAL-3", "start_pos": 104, "end_pos": 114, "type": "TASK", "confidence": 0.7913973927497864}, {"text": "F-measure", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9990905523300171}]}, {"text": "These results would have ranked 4th of 22 teams and 15th of 26 in the respective SENSEVAL events.", "labels": [], "entities": [{"text": "SENSEVAL", "start_pos": 81, "end_pos": 89, "type": "TASK", "confidence": 0.5602627992630005}]}, {"text": "A well known baseline for all words disambiguation is to assign the first WordNet sense to each ambiguous word.", "labels": [], "entities": [{"text": "words disambiguation", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7279713451862335}]}, {"text": "This results in an F-measure of 76% for SemCor, 69% for SENSEVAL-2, and 68% for SENSEVAL-3.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9996446371078491}, {"text": "SemCor", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.7933961749076843}]}, {"text": "A lower bound can be established by randomly assigning senses to words.", "labels": [], "entities": []}, {"text": "This results in an F-Measure of 41% for SemCor, 41% for SENSEVAL-2, and 37% for SENSEVAL-3.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9996205568313599}]}, {"text": "This is relatively high due to the large number of words that have just one possible sense (so randomly selecting will result in a correct assignment).", "labels": [], "entities": []}, {"text": "For example, in SemCor approximately 20% of the ambiguous words have just one sense.", "labels": [], "entities": []}, {"text": "From these results we can see that SR-AW lags behind the sense one baseline (which is common among all words systems), but significantly outperforms the random baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SR-AW Results (%)", "labels": [], "entities": [{"text": "SR-AW", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.7807801365852356}]}]}