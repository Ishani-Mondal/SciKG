{"title": [], "abstractContent": [{"text": "Language models are used in a wide variety of natural language applications, including machine translation, speech recognition, spelling correction, optical character recognition, etc.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8083818554878235}, {"text": "speech recognition", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7394101172685623}, {"text": "spelling correction", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.9409974217414856}, {"text": "optical character recognition", "start_pos": 149, "end_pos": 178, "type": "TASK", "confidence": 0.7098967134952545}]}, {"text": "Recent studies have shown that more data is better data, and bigger language models are better language models: the authors found nearly constant machine translation improvements with each doubling of the training data size even at 2 trillion tokens (resulting in 400 billion n-grams).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.6733358502388}]}, {"text": "Training and using such large models is a challenge.", "labels": [], "entities": []}, {"text": "This tutorial shows efficient methods for distributed training of large language models based on the MapReduce computing model.", "labels": [], "entities": []}, {"text": "We also show efficient ways of using distributed models in which requesting individual n-grams is expensive because they require communication between different machines.", "labels": [], "entities": []}, {"text": "Tutorial Outline 1) Training Distributed Models * N-gram collection Use of the MapReduce model; compressing intermediate data; minimizing communication overhead with good sharding functions.", "labels": [], "entities": []}, {"text": "* Smoothing Challenges of Katz Backoff and Kneser-Ney Smoothing in a distributed system; Smoothing techniques that are easy to compute in a distributed system: Stupid Backoff, Linear Interpolation; minimizing communication by sharding and aggregation.", "labels": [], "entities": [{"text": "minimizing communication", "start_pos": 198, "end_pos": 222, "type": "TASK", "confidence": 0.8789771795272827}]}, {"text": "2) Model Size Reduction * Pruning Reducing the size of the model by removing n-grams that don't have much impact.", "labels": [], "entities": [{"text": "Model Size Reduction", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7045723795890808}]}, {"text": "Entropy pruning is simple to compute for Stupid Backoff, requires some effort for Katz and Kneser-Ney in a distributed system.", "labels": [], "entities": []}, {"text": "* Quantization Reducing the memory size of the model by storing approximations of the values.", "labels": [], "entities": []}, {"text": "We discuss several quantizers; typically 4 to 8 bits are sufficient to store a floating point value.", "labels": [], "entities": []}, {"text": "* Randomized Data Structures Reducing the memory size of the model by changing the set of n-grams that is stored.", "labels": [], "entities": []}, {"text": "This typically lets us store models in 3 bytes per n-gram, independent of the n-gram 3", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}