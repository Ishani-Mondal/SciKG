{"title": [], "abstractContent": [{"text": "The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence.", "labels": [], "entities": []}, {"text": "In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM.", "labels": [], "entities": []}, {"text": "We support these findings on four unsuper-vised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7798854410648346}, {"text": "document classification", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.807778924703598}, {"text": "word segmentation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7834653258323669}, {"text": "word alignment", "start_pos": 127, "end_pos": 141, "type": "TASK", "confidence": 0.7949206829071045}]}], "introductionContent": [{"text": "In unsupervised NLP tasks such as tagging, parsing, and alignment, one wishes to induce latent linguistic structures from raw text.", "labels": [], "entities": [{"text": "tagging, parsing", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6033790409564972}]}, {"text": "Probabilistic modeling has emerged as a dominant paradigm for these problems, and the EM algorithm has been a driving force for learning models in a simple and intuitive manner.", "labels": [], "entities": []}, {"text": "However, on some tasks, EM can converge slowly.", "labels": [], "entities": [{"text": "EM", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.7977763414382935}]}, {"text": "For instance, on unsupervised part-ofspeech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.6494489312171936}, {"text": "Wall-Street Journal", "start_pos": 122, "end_pos": 141, "type": "DATASET", "confidence": 0.8224602341651917}]}, {"text": "The slowness of EM is mainly due to its batch nature: Parameters are updated only once after each pass through the data.", "labels": [], "entities": [{"text": "EM", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.8419076800346375}]}, {"text": "When parameter estimates are still rough or if there is high redundancy in the data, computing statistics on the entire dataset just to make one update can be wasteful.", "labels": [], "entities": []}, {"text": "In this paper, we investigate two flavors of online EM-incremental EM ( and stepwise EM, both of which involve updating parameters after each example or after a mini-batch (subset) of examples.", "labels": [], "entities": [{"text": "EM-incremental EM", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7258843779563904}]}, {"text": "Online algorithms have the potential to speedup learning by making updates more frequently.", "labels": [], "entities": []}, {"text": "However, these updates can be seen as noisy approximations to the full batch update, and this noise can in fact impede learning.", "labels": [], "entities": []}, {"text": "This tradeoff between speed and stability is familiar to online algorithms for convex supervised learning problems-e.g., Perceptron, MIRA, stochastic gradient, etc.", "labels": [], "entities": [{"text": "speed", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9938451051712036}, {"text": "stability", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9283161163330078}]}, {"text": "Unsupervised learning raises two additional issues: (1) Since the EM objective is nonconvex, we often get convergence to different local optima of varying quality; and (2) we evaluate on accuracy metrics which are at best loosely correlated with the EM likelihood objective ( ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9955989122390747}, {"text": "EM likelihood objective", "start_pos": 250, "end_pos": 273, "type": "METRIC", "confidence": 0.6559920410315195}]}, {"text": "We will see that these issues can lead to surprising results.", "labels": [], "entities": []}, {"text": "In Section 4, we present a thorough investigation of online EM, mostly focusing on stepwise EM since it dominates incremental EM.", "labels": [], "entities": []}, {"text": "For stepwise EM, we find that choosing a good stepsize and mini-batch size is important but can fortunately be done adequately without supervision.", "labels": [], "entities": [{"text": "EM", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.873801589012146}]}, {"text": "With a proper choice, stepwise EM reaches the same performance as batch EM, but much more quickly.", "labels": [], "entities": [{"text": "EM", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9139215350151062}]}, {"text": "Moreover, it can even surpass the performance of batch EM.", "labels": [], "entities": []}, {"text": "Our results are particularly striking on part-of-speech tagging: Batch EM crawls to an accuracy of 57.3% after 100 iterations, whereas stepwise EM shoots up to 65.4% after just two iterations.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6824481785297394}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.999019980430603}]}, {"text": "a sentence) and hidden output z (e.g., a parse tree) with parameters \u03b8 (e.g., rule probabilities).", "labels": [], "entities": []}, {"text": "Given a set of unlabeled examples x (1) , . .", "labels": [], "entities": []}, {"text": ", x (n) , the standard training objective is to maximize the marginal log-likelihood of these examples: A trained mode\u00ee \u03b8 is then evaluated on the accuracy of its predictions: argmax z p(z | x (i) ; \u02c6 \u03b8) against the true output z (i) ; the exact evaluation metric depends on the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9978277087211609}]}, {"text": "What makes unsupervised induction hard at best and ill-defined at worst is that the training objective (1) does not depend on the true outputs at all.", "labels": [], "entities": []}, {"text": "We ran experiments on four tasks described below.", "labels": [], "entities": []}, {"text": "Two of these tasks-part-of-speech tagging and document classification-are \"clustering\" tasks.", "labels": [], "entities": [{"text": "document classification-are \"clustering\"", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.8329009771347046}]}, {"text": "For these, the output z consists of labels; for evaluation, we map each predicted label to the true label that maximizes accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9972955584526062}]}, {"text": "The other two taskssegmentation and alignment-only involve unlabeled combinatorial structures, which can be evaluated directly.", "labels": [], "entities": [{"text": "alignment-only", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.9481835961341858}]}, {"text": "Part-of-speech tagging For each sentence x = (x 1 , . .", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7641395926475525}]}, {"text": ", x ), represented as a sequence of words, we wish to predict the corresponding sequence of partof-speech (POS) tags z = (z 1 , . .", "labels": [], "entities": []}, {"text": "We used a simple bigram HMM trained on the Wall Street Journal (WSJ) portion of the Penn Treebank (49208 sentences, 45 tags).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the Penn Treebank", "start_pos": 43, "end_pos": 97, "type": "DATASET", "confidence": 0.947250404141166}]}, {"text": "No tagging dictionary was used.", "labels": [], "entities": []}, {"text": "We evaluated using per-position accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9117130637168884}]}, {"text": "Document classification For each document x = (x 1 , . .", "labels": [], "entities": [{"text": "Document classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6929602026939392}]}, {"text": ", x ) consisting of words, 1 we wish to predict the document class z \u2208 {1, . .", "labels": [], "entities": []}, {"text": "Each document x is modeled as a bag of words drawn independently given the class z.", "labels": [], "entities": []}, {"text": "We used the 20 Newsgroups dataset (18828 documents, 20 classes).", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 12, "end_pos": 33, "type": "DATASET", "confidence": 0.7380900184313456}]}, {"text": "We evaluated on class accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9677014350891113}]}, {"text": "Word segmentation For each sentence x = (x 1 , . .", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6760933101177216}]}, {"text": ", x ), represented as a sequence of English phonemes or Chinese characters without spaces separating the words, we would like to predict a segmentation of the sequence into words z = (z 1 , . .", "labels": [], "entities": []}, {"text": ", z |z| ), where each segment (word) z i is a contiguous subsequence of 1, . .", "labels": [], "entities": []}, {"text": ", . Since the na\u00a8\u0131vena\u00a8\u0131ve unigram model has a degenerate maximum likelihood solution that makes each sentence a separate word, we incorporate a penalty for longer segments: where \u03b2 > 1 determines the strength of the penalty.", "labels": [], "entities": []}, {"text": "For English, we used \u03b2 = 1.6; Chinese, \u03b2 = 2.5.", "labels": [], "entities": []}, {"text": "To speedup inference, we restricted the maximum segment length to 10 for English and 5 for Chinese.", "labels": [], "entities": []}, {"text": "We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences).", "labels": [], "entities": [{"text": "CHILDES database", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.8980283737182617}, {"text": "Academia Sinica (AS) corpus", "start_pos": 112, "end_pos": 139, "type": "DATASET", "confidence": 0.5465444624423981}, {"text": "SIGHAN Chinese word segmentation bakeoff", "start_pos": 155, "end_pos": 195, "type": "TASK", "confidence": 0.6625538825988769}]}, {"text": "We evaluated using F 1 on word tokens.", "labels": [], "entities": [{"text": "F 1", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9811163544654846}]}, {"text": "To the best of our knowledge, our penalized unigram model is new and actually beats the more complicated model of 83.5% to 78%, which had been the best published result on this task.", "labels": [], "entities": []}, {"text": "Word alignment For each pair of translated sentences x = (e 1 , . .", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7144207060337067}]}, {"text": ", e ne , f 1 , . .", "labels": [], "entities": []}, {"text": ", f n f ), we wish to predict the word alignments z \u2208 {0, 1} nen f . We trained two IBM model 1s using agreement-based learning ( . We used the first 30K sentence pairs of the English-French Hansards data from the NAACL 2003 Shared Task, 447+37 of which were hand-aligned.", "labels": [], "entities": [{"text": "Hansards data from the NAACL 2003 Shared Task", "start_pos": 191, "end_pos": 236, "type": "DATASET", "confidence": 0.8448201045393944}]}, {"text": "We evaluated using the standard alignment error rate (AER).", "labels": [], "entities": [{"text": "standard alignment error rate (AER)", "start_pos": 23, "end_pos": 58, "type": "METRIC", "confidence": 0.8580540759222848}]}], "datasetContent": [{"text": "We now present our empirical results for batch EM and online EM (iEM and sEM) on the four tasks described in Section 2: part-of-speech tagging, document classification, word segmentation (English and Chinese), and word alignment.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.7591250836849213}, {"text": "document classification", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.7722441554069519}, {"text": "word segmentation", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.7487574219703674}, {"text": "word alignment", "start_pos": 214, "end_pos": 228, "type": "TASK", "confidence": 0.8111458718776703}]}, {"text": "We used the following protocol for all experiments: We initialized the parameters to a neutral setting plus noise to break symmetries.", "labels": [], "entities": []}, {"text": "Training was performed for 20 iterations.", "labels": [], "entities": []}, {"text": "No parameter smoothing was used.", "labels": [], "entities": [{"text": "parameter smoothing", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.6707100421190262}]}, {"text": "All runs used a fixed random seed for initializing the parameters and permuting the examples at the beginning of each iteration.", "labels": [], "entities": []}, {"text": "We report two performance metrics: log-likelihood normalized by the number of examples and the task-specific accuracy metric (see Section 2).", "labels": [], "entities": [{"text": "accuracy metric", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.9570253789424896}]}, {"text": "All numbers are taken from the final iteration.", "labels": [], "entities": []}, {"text": "Stepwise EM (sEM) requires setting two optimization parameters: the stepsize reduction power \u03b1 and the mini-batch size m (see Section 3.2).", "labels": [], "entities": []}, {"text": "As Section 4.3 will show, these two parameters can have a large impact on performance.", "labels": [], "entities": []}, {"text": "As a default rule of thumb, we chose (\u03b1, m) \u2208 {0.5, 0.6, 0.7, 0.8, 0.9, 1.0} \u00d7 {1, 3, 10, 30, 100, 300, 1K, 3K, 10K} to maximize log-likelihood; let sEM denote stepwise EM with this setting.", "labels": [], "entities": []}, {"text": "Note that this setting requires no labeled data.", "labels": [], "entities": []}, {"text": "We will also consider fixing (\u03b1, m) = (1, 1) (sEM i ) and choosing (\u03b1, m) to maximize accuracy (sEM a ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9977242350578308}]}, {"text": "In the results to follow, we first demonstrate that online EM is faster (Section 4.1) and sometimes leads to higher accuracies (Section 4.2).", "labels": [], "entities": [{"text": "EM", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.862983763217926}, {"text": "accuracies", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9874715209007263}]}, {"text": "Next, we explore the effect of the optimization parameters (\u03b1, m) (Section 4.3), briefly revisiting the connection between incremental and stepwise EM.", "labels": [], "entities": []}, {"text": "Finally, we show the stability of our results under different random seeds (Section 4.4).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of batch EM and stepwise EM, where  the optimization parameters (\u03b1, m) are tuned to either  maximize log-likelihood (sEM ) or accuracy (sEM a ).  With an appropriate setting of (\u03b1, m), stepwise EM out- performs batch EM significantly on POS tagging and  document classification.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9737496972084045}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9990429282188416}, {"text": "POS tagging", "start_pos": 256, "end_pos": 267, "type": "TASK", "confidence": 0.796305924654007}, {"text": "document classification", "start_pos": 273, "end_pos": 296, "type": "TASK", "confidence": 0.7473388612270355}]}, {"text": " Table 2: Mean and standard deviation over different ran- dom seeds. For EM and sEM, the first number after \u00b1  is the standard deviation due to different initializations  of the parameters. For sEM, the second number is the  standard deviation due to different permutations of the  examples. Standard deviation for log-likelihoods are all  < 0.01 and therefore left out due to lack of space.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9926708936691284}, {"text": "Standard", "start_pos": 292, "end_pos": 300, "type": "METRIC", "confidence": 0.9870187044143677}]}]}