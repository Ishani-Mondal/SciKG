{"title": [{"text": "Spherical Discriminant Analysis in Semi-supervised Speaker Clustering *", "labels": [], "entities": [{"text": "Spherical Discriminant Analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8964864412943522}]}], "abstractContent": [{"text": "Semi-supervised speaker clustering refers to the use of our prior knowledge of speakers in general to assist the unsupervised speaker clustering process.", "labels": [], "entities": [{"text": "Semi-supervised speaker clustering", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6147361099720001}]}, {"text": "In the form of an independent training set, the prior knowledge helps us learn a speaker-discriminative feature transformation, a universal speaker prior model, and a discriminative speaker subspace, or equivalently a speaker-discriminative distance metric.", "labels": [], "entities": []}, {"text": "The directional scattering patterns of Gaussian mixture model mean su-pervectors motivate us to perform discrimi-nant analysis on the unit hypersphere rather than in the Euclidean space, which leads to a novel dimensionality reduction technique called spherical discriminant analysis (SDA).", "labels": [], "entities": [{"text": "spherical discriminant analysis (SDA)", "start_pos": 252, "end_pos": 289, "type": "TASK", "confidence": 0.8097529311974844}]}, {"text": "Our experiment results show that in the SDA subspace, speaker clustering yields superior performance than that in other reduced-dimensional subspaces (e.g., PCA and LDA).", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8444440960884094}]}], "introductionContent": [{"text": "Speaker clustering is a critical part of speaker diarization (a.k.a. speaker segmentation and clustering) (.", "labels": [], "entities": [{"text": "Speaker clustering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.800556868314743}, {"text": "speaker segmentation", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7286759614944458}]}, {"text": "Unlike speaker recognition, where we have the training data of a set of known speakers and thus recognition can be done supervised, speaker clustering is usually performed in a completely unsupervised manner.", "labels": [], "entities": [{"text": "speaker recognition", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8898268640041351}, {"text": "speaker clustering", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7433082163333893}]}, {"text": "The output of speaker clustering is the internal labels relative to a dataset rather than real * This work was funded in part by DARPA contract HR0011-06-2-0001.", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8394547700881958}, {"text": "DARPA contract HR0011-06-2-0001", "start_pos": 129, "end_pos": 160, "type": "DATASET", "confidence": 0.6359335978825887}]}, {"text": "An interesting question is: Can we do semi-supervised speaker clustering?", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.6852578073740005}]}, {"text": "That is, can we make use of any available information that can be helpful to speaker clustering?", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.817033052444458}]}, {"text": "Our answer to this question is positive.", "labels": [], "entities": []}, {"text": "Here, semi-supervision refers to the use of our prior knowledge of speakers in general to assist the unsupervised speaker clustering process.", "labels": [], "entities": []}, {"text": "In the form of an independent training set, the prior knowledge helps us learn a speaker-discriminative feature transformation, a universal speaker prior model, and a discriminative speaker subspace, or equivalently a speaker-discriminative distance metric.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our speaker clustering experiments are based on a test set of 630 speakers and 19024 utterances selected from the GALE database (, which contains about 1900 hours of broadcasting news speech data collected from various TV programs.", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7229949533939362}, {"text": "GALE database", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.9058928489685059}]}, {"text": "An independent training set of 498 speakers and 18327 utterances is also selected from the GALE database.", "labels": [], "entities": [{"text": "GALE database", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9520276188850403}]}, {"text": "In either data set, there are an average of 30-40 utterances per speaker and the average duration of the utterances is about 3-4 seconds.", "labels": [], "entities": []}, {"text": "Note that there are no overlapping speakers in the two data sets -speakers in the test set are not present in the independent training set.", "labels": [], "entities": []}, {"text": "The acoustic features are 13 basic PLP features with cepstrum mean subtraction.", "labels": [], "entities": []}, {"text": "In computing the LDA feature transformation using the independent training set, K Land K R are both set to 4, and the dimensionality of the low-dimensional feature space is set to 40.", "labels": [], "entities": [{"text": "LDA feature transformation", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.7295345664024353}]}, {"text": "The entire independent training set is used to train a UBM via the EM algorithm, and a GMM mean supervector is obtained for every utterance in the test set via MAP adaptation.", "labels": [], "entities": [{"text": "MAP adaptation", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.6679206043481827}]}, {"text": "The trained UBM has 64 mixture components.", "labels": [], "entities": []}, {"text": "Thus, the dimension of the GMM mean supervectors is 2560.", "labels": [], "entities": [{"text": "GMM", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7661992907524109}]}, {"text": "We employ the hierarchical agglomerative clustering technique with the \"ward\" linkage method.", "labels": [], "entities": []}, {"text": "Our experiments are carried out as follows.", "labels": [], "entities": []}, {"text": "In each experiment, we perform 4 cases, each of which is associated with a specific number of test speakers, i.e., 5, 10, 20, and 50, respectively.", "labels": [], "entities": []}, {"text": "In each case, the corresponding number of speakers are drawn randomly from the test set, and all the utterances from the selected speakers are used for clustering.", "labels": [], "entities": []}, {"text": "For each case, 100 trials are run, each of which involves a random draw of the test speakers, and the average of the clustering accuracies across the 100 trials is recorded.", "labels": [], "entities": []}, {"text": "First, we perform speaker clustering in the original GMM mean supervector space using the Euclidean distance metric and the cosine distance metric, respectively.", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7703116834163666}, {"text": "GMM mean supervector space", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.8837945610284805}]}, {"text": "The results indicate that the cosine distance metric consistently outperforms the Euclidean distance metric.", "labels": [], "entities": []}, {"text": "Next, we perform speaker clustering in the reduced-dimensional subspaces using the eigenvoice (PCA) and fishervoice (LDA) approaches, respectively.", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8189709186553955}]}, {"text": "The results show that the fishervoice approach significantly outperforms the eigenvoice approach in all cases.", "labels": [], "entities": []}, {"text": "Finally, we perform speaker clustering in the SDA subspace.", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.8143055737018585}]}, {"text": "The results demonstrate that in the SDA subspace, speaker clustering yields superior performance than that in other reduced-dimensional subspaces (e.g., PCA and LDA).", "labels": [], "entities": [{"text": "speaker clustering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8407742083072662}]}], "tableCaptions": [{"text": " Table 1: Average speaker clustering accuracies (unit:%).", "labels": [], "entities": []}]}