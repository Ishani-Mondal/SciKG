{"title": [{"text": "A Fully Unsupervised Word Sense Disambiguation Method Using Dependency Knowledge", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6833361784617106}]}], "abstractContent": [{"text": "Word sense disambiguation is the process of determining which sense of a word is used in a given context.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6898069481054941}]}, {"text": "Due to its importance in understanding semantics of natural languages, word sense disambiguation has been extensively studied in Computational Linguistics.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.7449160814285278}]}, {"text": "However, existing methods either are brittle and narrowly focus on specific topics or words, or provide only mediocre performance in real-world settings.", "labels": [], "entities": []}, {"text": "Broad coverage and disambiguation quality are critical fora word sense disambiguation system.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6529314617315928}]}, {"text": "In this paper we present a fully unsupervised word sense dis-ambiguation method that requires only a dictionary and unannotated text as input.", "labels": [], "entities": []}, {"text": "Such an automatic approach overcomes the problem of brittleness suffered in many existing methods and makes broad-coverage word sense dis-ambiguation feasible in practice.", "labels": [], "entities": []}, {"text": "We evaluated our approach using SemEval 2007 Task 7 (Coarse-grained English All-words Task), and our system significantly outperformed the best unsupervised system participating in Se-mEval 2007 and achieved the performance approaching top-performing supervised systems.", "labels": [], "entities": []}, {"text": "Although our method was only tested with coarse-grained sense disambiguation, it can be directly applied to fine-grained sense disam-biguation.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many natural languages, a word can represent multiple meanings/senses, and such a word is called a homograph.", "labels": [], "entities": []}, {"text": "Word sense disambiguation(WSD) is the process of determining which sense of a homograph is used in a given context.", "labels": [], "entities": [{"text": "Word sense disambiguation(WSD)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8320411692063013}]}, {"text": "WSD is a long-standing problem in Computational Linguistics, and has significant impact in many real-world applications including machine translation, information extraction, and information retrieval.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7644033432006836}, {"text": "Computational Linguistics", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.7999795973300934}, {"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8145420551300049}, {"text": "information extraction", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.8490549921989441}, {"text": "information retrieval", "start_pos": 179, "end_pos": 200, "type": "TASK", "confidence": 0.8459276258945465}]}, {"text": "Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet,, Open Mind Word Expert (), eXtended WordNet (), Wikipedia, parallel corpora (Ng,.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9705998301506042}, {"text": "WordNet", "start_pos": 193, "end_pos": 200, "type": "DATASET", "confidence": 0.9462641477584839}, {"text": "Wikipedia", "start_pos": 250, "end_pos": 259, "type": "DATASET", "confidence": 0.9412383437156677}]}, {"text": "In () many different WSD approaches were described.", "labels": [], "entities": [{"text": "WSD", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9853718876838684}]}, {"text": "Usually, WSD techniques can be divided into four categories), \u2022 Dictionary and knowledge based methods.", "labels": [], "entities": [{"text": "WSD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9773573279380798}]}, {"text": "These methods use lexical knowledge bases such as dictionaries and thesauri, and hypothesize that context knowledge can be extracted from definitions of words.", "labels": [], "entities": []}, {"text": "For example, Lesk disambiguated two words by finding the pair of senses with the greatest word overlap in their dictionary definitions).", "labels": [], "entities": []}, {"text": "Supervised methods mainly adopt context to disambiguate words.", "labels": [], "entities": []}, {"text": "A supervised method includes a training phase and a testing phase.", "labels": [], "entities": []}, {"text": "In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to create a classifier using machine learning techniques, such as Support Vector Machine ().", "labels": [], "entities": []}, {"text": "In the following testing phase, a word is classified into senses).", "labels": [], "entities": []}, {"text": "Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference).", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9995922446250916}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.99912029504776}]}, {"text": "Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is hard to annotate and acquire sufficient contextual information for every sense of a large number of words existing in natural languages.", "labels": [], "entities": []}, {"text": "To overcome the knowledge acquisition bottleneck problem suffered by supervised methods, these methods make use of a small annotated corpus as seed data in a bootstrapping process.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.8558246493339539}]}, {"text": "A word-aligned bilingual corpus can also serve as seed data).", "labels": [], "entities": []}, {"text": "These methods acquire contextual information directly from unannotated raw text, and senses can be induced from text using some similarity measure.", "labels": [], "entities": []}, {"text": "However, automatically acquired information is often noisy or even erroneous.", "labels": [], "entities": []}, {"text": "In the most recent, the best unsupervised systems only achieved about 70% precision and 50% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9996731281280518}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9993107318878174}]}, {"text": "Disambiguation of a limited number of words is not hard, and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in.", "labels": [], "entities": [{"text": "Disambiguation of", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9013883471488953}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9826154708862305}]}, {"text": "However, such approaches suffer a significant performance drop in practice when domain or vocabulary is not limited.", "labels": [], "entities": []}, {"text": "Such a \"cliff-style\" performance collapse is called brittleness, which is due to insufficient knowledge and shared by many techniques in Artificial Intelligence.", "labels": [], "entities": []}, {"text": "The main challenge of a WSD system is how to overcome the knowledge acquisition bottleneck and efficiently collect the huge amount of context knowledge.", "labels": [], "entities": [{"text": "WSD", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9736451506614685}]}, {"text": "More precisely, a practical WSD need figure out how to create and maintain a comprehensive, dynamic, and up-todate context knowledge base in a highly automatic manner.", "labels": [], "entities": [{"text": "WSD", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9833548665046692}]}, {"text": "The context knowledge required in WSD has the following properties: 1.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9491878747940063}]}, {"text": "The context knowledge need cover a large number of words and their usage.", "labels": [], "entities": []}, {"text": "Such a requirement of broad coverage is not trivial because a natural language usually contains thousands of words, and some popular words can have dozens of senses.", "labels": [], "entities": []}, {"text": "For example, the Oxford English Dictionary has approximately 301,100 main entries, and the average polysemy of the WordNet inventory is 6.18.", "labels": [], "entities": [{"text": "Oxford English Dictionary", "start_pos": 17, "end_pos": 42, "type": "DATASET", "confidence": 0.9419529835383097}, {"text": "polysemy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9879854321479797}, {"text": "WordNet inventory", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.9476935565471649}]}, {"text": "Clearly acquisition of such a huge amount of knowledge can only be achieved with automatic techniques.", "labels": [], "entities": []}, {"text": "2. Natural language is not a static phenomenon.", "labels": [], "entities": []}, {"text": "New usage of existing words emerges, which creates new senses.", "labels": [], "entities": []}, {"text": "New words are created, and some words may \"die\" overtime.", "labels": [], "entities": []}, {"text": "It is estimated that every year around 2,500 new words appear in English.", "labels": [], "entities": []}, {"text": "Such dynamics requires a timely maintenance and updating of context knowledge base, which makes manual collection even more impractical.", "labels": [], "entities": [{"text": "manual collection", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.5771956741809845}]}, {"text": "Taking into consideration the large amount and dynamic nature of context knowledge, we only have limited options when choosing knowledge sources for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9645928740501404}]}, {"text": "WSD is often an unconscious process to human beings.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9867095351219177}]}, {"text": "With a dictionary and sample sentences/phrases an average educated person can correctly disambiguate most polysemous words.", "labels": [], "entities": []}, {"text": "Inspired by human WSD process, we choose an electronic dictionary and unannotated text samples of word instances as context knowledge sources for our WSD system.", "labels": [], "entities": []}, {"text": "Both sources can be automatically accessed, provide an excellent coverage of word meanings and usage, and are actively updated to reflect the current state of languages.", "labels": [], "entities": []}, {"text": "In this paper we present a fully unsupervised WSD system, which only requires WordNet sense inventory and unannotated text.", "labels": [], "entities": [{"text": "WSD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9427667260169983}]}, {"text": "In the rest of this paper, section 2 describes how to acquire and represent the context knowledge for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9643970131874084}]}, {"text": "We present our WSD algorithm in section 3.", "labels": [], "entities": [{"text": "WSD", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8629653453826904}]}, {"text": "Our WSD system is evaluated with SemEval-2007 Task 7 (Coarse-grained English All-words Task) data set, and the experiment results are discussed in section 4.", "labels": [], "entities": [{"text": "SemEval-2007 Task 7 (Coarse-grained English All-words Task) data set", "start_pos": 33, "end_pos": 101, "type": "DATASET", "confidence": 0.558944967660037}]}, {"text": "We conclude in section 5.", "labels": [], "entities": []}, {"text": "shows an overview of our context knowledge acquisition process, and collected knowledge is saved in a local knowledge base.", "labels": [], "entities": [{"text": "context knowledge acquisition", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.7054923176765442}]}, {"text": "Here are some details about each step.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set ().", "labels": [], "entities": [{"text": "SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set", "start_pos": 35, "end_pos": 104, "type": "DATASET", "confidence": 0.5762094476006248}]}, {"text": "The task organizers provide a coarse-grained sense inventory created with SSI algorithm), training data, and test data.", "labels": [], "entities": []}, {"text": "Since our method does not need any training or special tuning, neither coarse-grained sense inventory nor training data was used.", "labels": [], "entities": []}, {"text": "The test data includes: a news article about \"homeless\" (including totally 951 words, 368 words are annotated and need to be disambiguated), a review of the book \"Feeding Frenzy\" (including totally 987 words, 379 words are annotated and need to be disambiguated), an article about some traveling experience in France (including totally 1311 words, 500 words are annotated and need to be disambiguated), computer programming(including totally 1326 words, 677 words are annotated and need to be disambiguated), and a biography of the painter Masaccio (including totally 802 words, 345 words are annotated and need to be disambiguated).", "labels": [], "entities": []}, {"text": "Two authors of () independently and manually annotated part of the test set (710 word instances), and the pairwise agreement was 93.80%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.827109694480896}]}, {"text": "This inter-annotator agreement is usually considered an upper-bound for WSD systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.8507800698280334}]}, {"text": "We followed the WSD process described in Section 2 and 3 using the WordNet 2.1 sense repository that is adopted by SemEval-2007 Task 07.", "labels": [], "entities": [{"text": "WSD", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7251232862472534}, {"text": "WordNet 2.1 sense repository", "start_pos": 67, "end_pos": 95, "type": "DATASET", "confidence": 0.9024234563112259}, {"text": "SemEval-2007 Task 07", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.6346862514813741}]}, {"text": "All experiments were performed on a Pentium 2.33GHz dual core PC with 3GB memory.", "labels": [], "entities": []}, {"text": "Among the 2269 tobe-disambiguated words in the five test documents, 1112 words are unique and submitted to Google API as queries.", "labels": [], "entities": []}, {"text": "The retrieved Web pages were cleaned, and 1945189 relevant sentences were extracted.", "labels": [], "entities": [{"text": "1945189", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.6182301640510559}]}, {"text": "On average 1749 sentences were obtained for each word.", "labels": [], "entities": []}, {"text": "The Web page retrieval step took 3 days, and the cleaning step took 2 days.", "labels": [], "entities": [{"text": "Web page retrieval", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6679159502188364}]}, {"text": "Parsing was very time-consuming and took 11 days.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9671643376350403}]}, {"text": "The merging step took 3 days.", "labels": [], "entities": [{"text": "merging", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9722107648849487}]}, {"text": "Disambiguation of 2269 words in the 5 test articles took 4 hours.", "labels": [], "entities": []}, {"text": "All these steps can be parallelized and run on multiple computers, and the whole process will be shortened accordingly.", "labels": [], "entities": []}, {"text": "The overall disambiguation results are shown in.", "labels": [], "entities": []}, {"text": "For comparison we also listed the results of the top three systems and three unsupervised systems participating in SemEval-2007 Task 07.", "labels": [], "entities": [{"text": "SemEval-2007 Task 07", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.8259324232737223}]}, {"text": "All of the top three systems (UoR-SSI, NUS-PT, NUS-ML) are supervised systems, which used annotated resources (e.g., SemCor, Defense Science Organization Corpus) during the training phase.", "labels": [], "entities": [{"text": "UoR-SSI", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.8441140651702881}, {"text": "NUS-PT", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.8453891277313232}, {"text": "Defense Science Organization Corpus", "start_pos": 125, "end_pos": 160, "type": "DATASET", "confidence": 0.7768029123544693}]}, {"text": "Our fully unsupervised WSD system significantly outperforms the three unsupervised systems (SUSSZ-FR, SUSSX-C-WD, SUSSX-CR) and achieves performance approaching the top-performing supervised WSD systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall disambiguation scores (Our system \"TreeMatch\" is marked in bold)", "labels": [], "entities": []}, {"text": " Table 3: Disambiguation scores by article with a smaller  knowledge base", "labels": [], "entities": [{"text": "Disambiguation", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9506418108940125}]}]}