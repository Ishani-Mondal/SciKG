{"title": [{"text": "Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages", "labels": [], "entities": [{"text": "Improve SMT", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.7111657857894897}]}], "abstractContent": [{"text": "We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems.", "labels": [], "entities": [{"text": "precedence reordering", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7415217161178589}, {"text": "statistical machine translation", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.6204806466897329}]}, {"text": "Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.", "labels": [], "entities": [{"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9834415316581726}]}, {"text": "For a set of five subject-object-verb (SOV) order languages , we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9992497563362122}, {"text": "SMT", "start_pos": 215, "end_pos": 218, "type": "TASK", "confidence": 0.8908172845840454}]}], "introductionContent": [{"text": "Over the past ten years, statistical machine translation has seen many exciting developments.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.8382638295491537}]}, {"text": "Phrasebased systems) advanced the machine translation field by allowing translations of word sequences (a.k.a., phrases) instead of single words.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7731198072433472}, {"text": "translations of word sequences (a.k.a., phrases)", "start_pos": 72, "end_pos": 120, "type": "TASK", "confidence": 0.7763597766558329}]}, {"text": "This approach has since been the state-of-the-art because of its robustness in modeling local word reordering and the existence of an efficient dynamic programming decoding algorithm.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.6918953359127045}]}, {"text": "However, when phrase-based systems are used between languages with very different word orders, such as between subject-verb-object (SVO) and subject-object-verb (SOV) languages, long distance reordering becomes one of the key weaknesses.", "labels": [], "entities": []}, {"text": "Many reordering methods have been proposed in recent years to address this problem in different aspects.", "labels": [], "entities": []}, {"text": "The first class of approaches tries to explicitly model phrase reordering distances.", "labels": [], "entities": [{"text": "phrase reordering distances", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7706337173779806}]}, {"text": "Distance based distortion model) is a simple way of modeling phrase level reordering.", "labels": [], "entities": [{"text": "phrase level reordering", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7014363805452982}]}, {"text": "It penalizes non-monotonicity by applying a weight to the number of words between two source phrases corresponding to two consecutive target phrases.", "labels": [], "entities": []}, {"text": "Later on, this model was extended to lexicalized phrase reordering) by applying different weights to different phrases.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6840081959962845}]}, {"text": "Most recently, a hierarchical phrase reordering model () was proposed to dynamically determine phrase boundaries using efficient shift-reduce parsing.", "labels": [], "entities": []}, {"text": "Along this line of research, discriminative reordering models based on a maximum entropy classifier () also showed improvements over the distance based distortion model.", "labels": [], "entities": []}, {"text": "None of these reordering models changes the word alignment step in SMT systems, therefore, they cannot recover from the word alignment errors.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7382366359233856}, {"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9889265298843384}, {"text": "word alignment", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.6905349940061569}]}, {"text": "These models are also limited by a maximum allowed reordering distance often used in decoding.", "labels": [], "entities": []}, {"text": "The second class of approaches puts syntactic analysis of the target language into both modeling and decoding.", "labels": [], "entities": []}, {"text": "It has been shown that direct modeling of target language constituents movement in either constituency trees; or dependency trees) can result in significant improvements in translation quality for translating languages like Chinese and Arabic into English.", "labels": [], "entities": []}, {"text": "A simpler alternative, the hierarchical phrase-based approach) also showed promising results for translating Chinese to English.", "labels": [], "entities": [{"text": "translating Chinese to English", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.8753117173910141}]}, {"text": "Similar to the distance based reordering models, the syntactical or hierarchical approaches also rely on other models to get word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 125, "end_pos": 140, "type": "TASK", "confidence": 0.7027804255485535}]}, {"text": "These models typically combine machine translation decoding with chart parsing, therefore significantly increase the decoding complexity.", "labels": [], "entities": [{"text": "chart parsing", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7075598537921906}]}, {"text": "Even though some recent work has shown great improvements in decoding efficiency for syntactical and hierarchical approaches, they are still not as efficient as phrase-based systems, especially when higher order language models are used.", "labels": [], "entities": []}, {"text": "Finally, researchers have also tried to put source language syntax into reordering in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7390129268169403}]}, {"text": "Syntactical analysis of source language can be used to deterministically reorder input sentences (, or to provide multiple orderings as weighted options.", "labels": [], "entities": []}, {"text": "In these approaches, input source sentences are reordered based on syntactic analysis and some reordering rules at preprocessing step.", "labels": [], "entities": []}, {"text": "The reordering rules can be either manually written or automatically extracted from data.", "labels": [], "entities": []}, {"text": "Deterministic reordering based on syntactic analysis for the input sentences provides a good way of resolving long distance reordering, without introducing complexity to the decoding process.", "labels": [], "entities": [{"text": "Deterministic reordering", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.827092319726944}, {"text": "resolving long distance reordering", "start_pos": 100, "end_pos": 134, "type": "TASK", "confidence": 0.6300895810127258}]}, {"text": "Therefore, it can be efficiently incorporated into phrase-based systems.", "labels": [], "entities": []}, {"text": "Furthermore, when the same preprocessing reordering is performed for the training data, we can still apply other reordering approaches, such as distance based reordering and hierarchical phrase reordering, to capture additional local reordering phenomena that are not captured by the preprocessing reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.6955433487892151}]}, {"text": "The work presented in this paper is largely motivated by the preprocessing reordering approaches.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we first introduce our dependency parser based reordering approach based on the analysis of the key issues when translating SVO languages to SOV languages.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.707022100687027}]}, {"text": "Then, we show experimental results of applying this approach to phrasebased SMT systems for translating from English to five SOV languages (Korean, Japanese, Hindi, Urdu and Turkish).", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.8175857663154602}]}, {"text": "After showing that this approach can also be beneficial for hierarchical phrase-based sys-", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out all our experiments based on a stateof-the-art phrase-based statistical machine translation system.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 62, "end_pos": 106, "type": "TASK", "confidence": 0.6022539511322975}]}, {"text": "When training a system for English to any of the 5 SOV languages, the word alignment step includes 3 iterations of IBM Model-1 training and 2 iterations of HMM training.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.7563636600971222}]}, {"text": "We do not use Model-4 because it is slow and it does not add much value to our systems in a pilot study.", "labels": [], "entities": []}, {"text": "We use the standard phrase extraction algorithm to get all phrases up to length 5.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7740066945552826}]}, {"text": "In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model () as a feature used in decoding.", "labels": [], "entities": []}, {"text": "In this model, we use 4 reordering classes (+1, > 1, \u22121, < \u22121) and words from both source and target as features.", "labels": [], "entities": []}, {"text": "For source words, we use the current aligned word, the word before the current aligned word and the next aligned word; for target words, we use the previous two words in the immediate history.", "labels": [], "entities": []}, {"text": "Using this type of features makes it possible to directly use the maximum entropy model in the decoding process).", "labels": [], "entities": []}, {"text": "The maximum entropy models are trained on all events extracted from training data word alignments using the LBFGS algorithm  to 30 features, whose weights are optimized using MERT, with an implementation based on the lattice MERT).", "labels": [], "entities": []}, {"text": "For parallel training data, we use an in-house collection of parallel documents.", "labels": [], "entities": []}, {"text": "They come from various sources with a substantial portion coming from the web after using simple heuristics to identify potential document pairs.", "labels": [], "entities": []}, {"text": "Therefore, for some documents in the training data, we do not necessarily have the exact clean translations.", "labels": [], "entities": []}, {"text": "shows the actual statistics about the training data for all five languages we study.", "labels": [], "entities": []}, {"text": "For all 5 SOV languages, we use the target side of the parallel data and some more monolingual text from crawling the web to build 4-gram language models.", "labels": [], "entities": []}, {"text": "We also collected about 10K English sentences from the web randomly.", "labels": [], "entities": []}, {"text": "Among them, 9.5K are used as evaluation data.", "labels": [], "entities": []}, {"text": "Those sentences were translated by humans to all 5 SOV languages studied in this paper.", "labels": [], "entities": [{"text": "SOV languages", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8837032318115234}]}, {"text": "Each sentence has only one reference translation.", "labels": [], "entities": []}, {"text": "We split them into 3 subsets: dev contains 3,500 sentences, test contains 1,000 sentences and the rest of 5,000 sentences are used in a blindtest set.", "labels": [], "entities": []}, {"text": "The dev set is used to perform MERT training, while the test set is used to select trained weights due to some nondeterminism of MERT training.", "labels": [], "entities": [{"text": "MERT training", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.893631786108017}]}, {"text": "We use IBM BLEU () to evaluate our translations and use character level BLEU for Korean and Japanese.", "labels": [], "entities": [{"text": "IBM", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.5633761882781982}, {"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.831057071685791}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.8788208961486816}]}], "tableCaptions": [{"text": " Table 3: BLEU Scores on Dev, Test and Blindtest for En- glish to 5 SOV Languages with Various Reordering Op- tions (BL means baseline, LR means maximum entropy  based lexialized phrase reordering model, PR means  precedence rules based preprocessing reordering.)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.99801105260849}, {"text": "Blindtest", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9874853491783142}]}, {"text": " Table 4: BLEU Scores on Dev, Test and Blindtest for En- glish to 5 SOV Languages in Hierarchical Phrase-based  Systems (PR is precedence rules based preprocessing re- ordering, same as in", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9683961272239685}, {"text": "Blindtest", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.990625262260437}]}]}