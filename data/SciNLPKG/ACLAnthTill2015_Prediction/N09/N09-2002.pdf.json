{"title": [{"text": "Revisiting Optimal Decoding for Machine Translation IBM Model 4", "labels": [], "entities": [{"text": "Revisiting Optimal Decoding", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8829639355341593}, {"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7319471538066864}]}], "abstractContent": [{"text": "This paper revisits optimal decoding for statistical machine translation using IBM Model 4.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.7176718910535177}]}, {"text": "We show that exact/optimal inference using Integer Linear Programming is more practical than previously suggested when used in conjunction with the Cutting-Plane Algorithm.", "labels": [], "entities": []}, {"text": "In our experiments we see that exact inference can provide again of up to one BLEU point for sentences of length up to 30 tokens.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.999058187007904}]}], "introductionContent": [{"text": "Statistical machine translation (MT) systems typically contain three essential components: (1) a model, specifying how the process of translation occurs; (2) learning regime, dictating the estimation of model's parameters; (3) decoding algorithm which provides the most likely translation of an input sentence given a model and its parameters.", "labels": [], "entities": [{"text": "Statistical machine translation (MT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7942717870076498}]}, {"text": "The search space in statistical machine translation is vast which can make it computationally prohibitively to perform exact/optimal decoding (also known as search and MAP inference) especially since dynamic programming methods (such as the Viterbi algorithm) are typically not applicable.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.6530871291955312}]}, {"text": "Thus greedy or heuristic beam-based methods have been prominent () due to their efficiency.", "labels": [], "entities": []}, {"text": "However, the efficiency of such methods have two drawbacks: (1) they are approximate and give no bounds as to how far their solution is away from the true optimum; (2) it can be difficult to incorporate additional generic global constraints into the search.", "labels": [], "entities": []}, {"text": "The first point maybe especially problematic from a research perspective as without bounds on the solutions it is difficult to determine whether the model or the search algorithm requires improvement for better translations.", "labels": [], "entities": []}, {"text": "Similar problems exist more widely throughout natural language processing where greedy based methods and heuristic beam search have been used in lieu of exact methods.", "labels": [], "entities": []}, {"text": "However, recently there has been an increasing interest in using Integer Linear Programming (ILP) as a means to find MAP solutions.", "labels": [], "entities": [{"text": "MAP", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.9556607007980347}]}, {"text": "ILP overcomes the two drawbacks mentioned above as it is guaranteed to be exact, and has the ability to easily enforce global constraints through additional linear constraints.", "labels": [], "entities": []}, {"text": "However, efficiency is usually sacrificed for these benefits.", "labels": [], "entities": []}, {"text": "Integer Linear Programming has previously been used to perform exact decoding for MT using IBM Model 4 and a bigram language model.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9539524912834167}]}, {"text": "view the translation process akin to the travelling salesman problem; however, from their reported results it is clear that using ILP naively for decoding does not scale up beyond short sentences (of eight tokens).", "labels": [], "entities": [{"text": "translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9667860269546509}]}, {"text": "This is due to the exponential number of constraints required to represent the decoding problem as an ILP program.", "labels": [], "entities": []}, {"text": "However, work in dependency parsing () has demonstrated that it is possible to use ILP to perform efficient inference for very large programs when used in an incremental manner.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8370214998722076}]}, {"text": "This raises the question as to whether incremental (or Cutting-Plane) ILP can also be used to decode IBM Model 4 on real world sentences.", "labels": [], "entities": []}, {"text": "In this work we show that it is possible.", "labels": [], "entities": []}, {"text": "Decoding IBM Model 4 (in combination with a bigram language model) using Cutting-Plane ILP scales to much longer sentences.", "labels": [], "entities": []}, {"text": "This affords us the opportunity to finally analyse the performance of IBM Model 4 and the performance of its state-of-the-art ReWrite decoder.", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9038063287734985}]}, {"text": "We show that using exact inference provides an increase of up to one BLEU point on two language pairs (French-English and German-English) in comparison to decoding using the ReWrite decoder.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9990078806877136}, {"text": "ReWrite decoder", "start_pos": 174, "end_pos": 189, "type": "DATASET", "confidence": 0.942357987165451}]}, {"text": "Thus the ReWrite decoder performs respectably but can be improved slightly, albeit at the cost of efficiency.", "labels": [], "entities": [{"text": "ReWrite decoder", "start_pos": 9, "end_pos": 24, "type": "DATASET", "confidence": 0.8461746573448181}]}, {"text": "Although the community has generally moved away from word-based models, we believe that displaying optimal decoding in IBM Model 4 lays the foundations of future work.", "labels": [], "entities": []}, {"text": "It is the first step in providing a method for researchers to gain greater insight into their translation models by mapping the decoding problem of other models into an ILP representation.", "labels": [], "entities": []}, {"text": "ILP decoding will also allow the incorporation of global linguistic constraints in a manner similar to work in other areas of natural language processing.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows: Sections 2 and 3 briefly recap IBM Model 4 and its ILP formulation.", "labels": [], "entities": [{"text": "ILP formulation", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.7494473159313202}]}, {"text": "Section 4 reviews the Cutting-Plane Algorithm.", "labels": [], "entities": []}, {"text": "Section 5 outlines our experiments and we end the paper with conclusions and a discussion of open questions for the community.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experimental setup and results.", "labels": [], "entities": []}, {"text": "Our experimental setup is designed to answer several questions: (1) Is exact inference in IBM Model 4 possible for sentences of moderate length?", "labels": [], "entities": []}, {"text": "(2) How fast is exact inference using Cutting-Plane ILP?", "labels": [], "entities": []}, {"text": "(3) How well does the ReWrite Decoder 4 perform in terms of finding the optimal solution?", "labels": [], "entities": [{"text": "ReWrite Decoder 4", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.656237006187439}]}, {"text": "(4) Does optimal decoding produce better translations?", "labels": [], "entities": []}, {"text": "In order to answer these questions we obtain a trained IBM Model 4 for French-English and German-English on Europarl v3 using GIZA++.", "labels": [], "entities": [{"text": "Europarl v3", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.95787313580513}]}, {"text": "A bigram language model with Witten-Bell smoothing was estimated from the corpus using the CMUCambridge Language Modeling Toolkit.", "labels": [], "entities": [{"text": "CMUCambridge Language Modeling Toolkit", "start_pos": 91, "end_pos": 129, "type": "DATASET", "confidence": 0.8265937715768814}]}, {"text": "For exact decoding we use the two models to generate ILP programs for sentences of length up to (and including) 30 tokens for French and 25 tokens for German.", "labels": [], "entities": []}, {"text": "We filter translation candidates following by using only the top ten translations for each word and a list of zero fertility words.", "labels": [], "entities": []}, {"text": "This resulted in 1101 French and 1062 German sentences for testing purposes.", "labels": [], "entities": []}, {"text": "The ILP programs were then solved using the method described in Section 3.", "labels": [], "entities": []}, {"text": "This was repeated using the ReWrite Decoder using the same models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the two corpora. Len: range of sentence lengths; #: number of sentences in this range; %Eq: percentage of", "labels": [], "entities": [{"text": "Len", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9839015603065491}]}]}