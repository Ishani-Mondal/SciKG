{"title": [{"text": "Performance Prediction for Exponential Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the task of performance prediction for language models belonging to the exponential family.", "labels": [], "entities": [{"text": "performance prediction", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7215047478675842}]}, {"text": "First, we attempt to empirically discover a formula for predicting test set cross-entropy for n-gram language models.", "labels": [], "entities": []}, {"text": "We build models over varying domains, data set sizes, and n-gram orders, and perform linear regression to see whether we can model test set performance as a simple function of training set performance and various model statistics.", "labels": [], "entities": []}, {"text": "Remarkably, we find a simple relationship that predicts test set performance with a correlation of 0.9997.", "labels": [], "entities": []}, {"text": "We analyze why this relationship holds and show that it holds for other exponential language models as well, including class-based models and minimum discrimination information models.", "labels": [], "entities": []}, {"text": "Finally, we discuss how this relationship can be applied to improve language model performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we investigate the following question for language models belonging to the exponential family: given some training data and test data drawn from the same distribution, can we accurately predict the test set performance of a model estimated from the training data?", "labels": [], "entities": []}, {"text": "This problem is known as performance prediction and is relevant for model selection, the task of selecting the best model from a set of candidate models given data.", "labels": [], "entities": [{"text": "performance prediction", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7366928160190582}, {"text": "model selection", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7228028178215027}]}, {"text": "Let us first define some notation.", "labels": [], "entities": []}, {"text": "Events have the form (x, y), where we attempt to predict the current wordy given previous words x.", "labels": [], "entities": []}, {"text": "We denote the training data as D = (x 1 , y 1 ), . .", "labels": [], "entities": [{"text": "D", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.9454553127288818}]}, {"text": ", (x D , y D ) and define\u02dcpfine\u02dc fine\u02dcp(x, y) = count D (x, y)/D to be the empirical distribution of the training data.", "labels": [], "entities": []}, {"text": "Similarly, we have 1 A long version of this paper can be found at a test set D * and an associated empirical distribution p * (x, y).", "labels": [], "entities": []}, {"text": "We take the performance of a conditional language model p(y|x) to be the cross-entropy H(p * , p) between the empirical test distribution p * and the model p(y|x): This is equivalent to the negative mean loglikelihood per event, as well as to log perplexity.", "labels": [], "entities": []}, {"text": "We only consider models in the exponential family.", "labels": [], "entities": []}, {"text": "An exponential model p \u039b (y|x) is a model with a set of features {f 1 (x, y), . .", "labels": [], "entities": []}, {"text": ", f F (x, y)} and equal number of parameters \u039b = {\u03bb 1 , . .", "labels": [], "entities": []}, {"text": ", \u03bb F } where and where Z \u039b (x) is a normalization factor.", "labels": [], "entities": []}, {"text": "One of the seminal methods for performance prediction is the Akaike Information Criterion (AIC)).", "labels": [], "entities": [{"text": "performance prediction", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7241681814193726}]}, {"text": "For a model, let\u02c6\u039blet\u02c6 let\u02c6\u039b be the maximum likelihood estimate of \u039b on some training data.", "labels": [], "entities": [{"text": "maximum likelihood estimate", "start_pos": 36, "end_pos": 63, "type": "METRIC", "confidence": 0.6971447269121805}]}, {"text": "Akaike derived the following estimate for the expected value of the test set cross-entropy H(p * , p \u02c6 \u039b ): H(\u02dc p, p \u02c6 \u039b ) is the cross-entropy of the training set, F is the number of parameters in the model, and Dis the number of events in the training data.", "labels": [], "entities": []}, {"text": "However, maximum likelihood estimates for language models typically yield infinite cross-entropy on test data, and thus AIC behaves poorly for these domains.", "labels": [], "entities": []}, {"text": "In this work, instead of deriving a performance prediction relationship theoretically, we attempt to empirically discover a formula for predicting test performance.", "labels": [], "entities": []}, {"text": "Initially, we consider only n-gram language models, and build models over varying domains, data set sizes, and n-gram orders.", "labels": [], "entities": []}, {"text": "We perform linear regression to discover whether we can model test set cross-entropy as a simple function of training set cross-entropy and other model statistics.", "labels": [], "entities": []}, {"text": "For the 200+ n-gram models we evaluate, we find that the empirical relationship holds with a correlation of 0.9997 where \u03b3 is a constant and where\u02dc\u039bwhere\u02dc where\u02dc\u039b = { \u02dc \u03bb i } are regularized parameter estimates; i.e., rather than estimating performance for maximum likelihood models as in AIC, we do this for regularized models.", "labels": [], "entities": []}, {"text": "In other words, test set cross-entropy can be approximated by the sum of the training set cross-entropy and the scaled sum of the magnitudes of the model parameters.", "labels": [], "entities": []}, {"text": "To maximize the correlation achieved by eq.", "labels": [], "entities": [{"text": "correlation", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9845116138458252}]}, {"text": "(4), we find that it is necessary to use the same regularization method and regularization hyperparameters across models and that the optimal value of \u03b3 depends on the values of the hyperparameters.", "labels": [], "entities": []}, {"text": "Consequently, we first evaluate several types of regularization and find which of these (and which hyperparameter values) work best across all domains, and use these values in all subsequent experiments.", "labels": [], "entities": []}, {"text": "While 2 2 regularization gives the best performance reported in the literature for n-gram models, we find here that 1 + 2 2 regularization works even better.", "labels": [], "entities": []}, {"text": "The organization of this paper is as follows: In Section 2, we evaluate various regularization techniques for n-gram models and select the method and hyperparameter values that give the best overall performance.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss experiments to find a formula for predicting n-gram model performance, and provide an explanation for why eq. works so well.", "labels": [], "entities": []}, {"text": "In Section 4, we evaluate how well eq.", "labels": [], "entities": []}, {"text": "(4) holds for several class-based language models and minimum discrimination information models.", "labels": [], "entities": []}, {"text": "Finally, in Sections 5 and 6 we discuss related work and conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of data sets. RH = Random House  dictionary; WSJ = Wall Street Journal; BN = Broadcast  News; SWB = Switchboard.", "labels": [], "entities": [{"text": "RH", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.8809311389923096}, {"text": "Random House  dictionary", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.9218656619389852}, {"text": "Wall Street Journal", "start_pos": 72, "end_pos": 91, "type": "DATASET", "confidence": 0.8413417140642802}, {"text": "BN", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9620199203491211}]}]}