{"title": [{"text": "An Iterative Reinforcement Approach for Fine-Grained Opinion Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "With the in-depth study of sentiment analysis research, finer-grained opinion mining, which aims to detect opinions on different review features as opposed to the whole review level, has been receiving more and more attention in the sentiment analysis research community recently.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9210988283157349}, {"text": "opinion mining", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.7846423089504242}, {"text": "sentiment analysis research", "start_pos": 233, "end_pos": 260, "type": "TASK", "confidence": 0.8573521773020426}]}, {"text": "Most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms, which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews, which satisfies (1) the review features are not appear explicit in the review sentences; (2) it can be deduced by the opinion words in its context.", "labels": [], "entities": []}, {"text": "From an information theoretic point of view, this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem.", "labels": [], "entities": []}, {"text": "More specifically, the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that our approach outperforms the template extraction based approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the Web2.0 era, the Internet turns from a static information media into a platform for dynamic information exchanging, on which people can express their views and show their selfhood.", "labels": [], "entities": []}, {"text": "More and more people are willing to record their feelings (blog), give voice to public affairs (news review), express their likes and dislikes on products (product review), and soon.", "labels": [], "entities": []}, {"text": "In the face of the volume of sentimental information available on the Internet continues to increase, there is growing interest in helping people better find, filter, and manage these resources.", "labels": [], "entities": []}, {"text": "Automatic opinion mining ( can play an important role in a wide variety of more flexible and dynamic information management tasks.", "labels": [], "entities": [{"text": "Automatic opinion mining", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6315491298834482}, {"text": "information management", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.7677218914031982}]}, {"text": "For example, with the help of sentiment analysis system, in the field of public administration, the administrators can receive the feedbacks on one policy in a timelier manner; in the field of business, manufacturers can perform more targeted updates on products to improve the consumer experience.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9357270300388336}]}, {"text": "The research of opinion mining began in 1997, the early research results mainly focused on the polarity of opinion words () and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text ().", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.8502429723739624}]}, {"text": "With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (), domain transfer problem of the sentiment analysis () and finegrained opinion mining ( are the main branches of the research of opinion mining.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.8232753872871399}, {"text": "sentiment summarization", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.8135136663913727}, {"text": "domain transfer problem of the sentiment analysis", "start_pos": 150, "end_pos": 199, "type": "TASK", "confidence": 0.6815857120922634}, {"text": "finegrained opinion mining", "start_pos": 207, "end_pos": 233, "type": "TASK", "confidence": 0.6313715676466624}, {"text": "opinion mining", "start_pos": 277, "end_pos": 291, "type": "TASK", "confidence": 0.8029053211212158}]}, {"text": "In this paper, we focus on the fine-grained (feature-level) opinion mining.", "labels": [], "entities": [{"text": "feature-level) opinion mining", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6274468079209328}]}, {"text": "For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging the sentiment orientation of a review unit is not sufficient.", "labels": [], "entities": [{"text": "public affairs review analysis", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.7605900466442108}]}, {"text": "Researchers () began to work on finer-grained opinion mining which predicts the sentiment orientation related to different review features.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7489704787731171}]}, {"text": "The task is known as feature-level opinion mining.", "labels": [], "entities": [{"text": "feature-level opinion mining", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6566420992215475}]}, {"text": "In feature-level opinion mining, most of the existing researches associate product features and opinion words by their explicit co-occurrence.", "labels": [], "entities": [{"text": "feature-level opinion mining", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.5999248226483663}]}, {"text": "Template extraction based method () and association rule mining based method () are the representative ones.", "labels": [], "entities": [{"text": "Template extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.770368367433548}, {"text": "association rule mining", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7037661472956339}]}, {"text": "These approaches did good jobs for identifying the review features that appear explicitly in reviews, however, real reviews from customers are usually complicated.", "labels": [], "entities": []}, {"text": "In some cases, the review features are implicit in the review sentences, but can be deduced by the opinion words in its context.", "labels": [], "entities": []}, {"text": "The detection of such hidden sentiment association is a big challenge in feature-level opinion mining on Chinese reviews due to the nature of Chinese language (.", "labels": [], "entities": [{"text": "feature-level opinion mining", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6329359412193298}]}, {"text": "Obviously, neither the template extraction based method nor the association rule mining based method is effective for such cases.", "labels": [], "entities": [{"text": "template extraction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7340861558914185}]}, {"text": "Moreover, in some cases, even if the review features appear explicitly in the review sentences, the co-occurrence information between review features and opinion words is too quantitatively sparse to be utilized.", "labels": [], "entities": []}, {"text": "So we consider whether it is a more sensible way to constructor cluster review feature groups and opinion words groups to mine the implicit or hidden sentiment association in the reviews.", "labels": [], "entities": []}, {"text": "The general approach will cluster the two types of objects separately, which neglects the highly interrelationship.", "labels": [], "entities": []}, {"text": "To address this problem, in this paper, we propose an iterative reinforcement framework, under which we cluster product features and opinion words simultaneously and iteratively by fusing both their semantic information and sentiment link information.", "labels": [], "entities": []}, {"text": "We take improved information bottleneck algorithm  as the kernel of the proposed framework.", "labels": [], "entities": []}, {"text": "The information bottleneck approach was presented by . The basic idea of the approach is that it treats the clustering problems from the information compressing point of view, and takes this problem as a case of much more fundamental problem: what are the features of the variable X that are relevant for the prediction of another, relevance, variable Y?", "labels": [], "entities": []}, {"text": "Based on the information theory, the problem can be formulated as: find a compressed representation of the variable X, denoted C, such that the mutual information between C and Y is as high as possible, under a constraint on the mutual information between X and C.", "labels": [], "entities": []}, {"text": "For our case, take the hotel reviews as example, X is one type of objects of review features (e.g. facilities, service, surrounding environment, etc) or opinion words (e.g. perfect, circumspect, quiet, etc), and Y is another one.", "labels": [], "entities": []}, {"text": "Given some review features (or opinion words) gained from review corpus, we want to assemble them into categories, conserving the information about opinion words (or review features) as high as possible.", "labels": [], "entities": []}, {"text": "The information bottleneck algorithm has some benefits, mainly including (1) it treats the trade-off of precision versus complexity of clustering model through the rate distortion theory, which is a subfield of information theory; (2) it defines the \"distance\" or \"similarity\" in a well-defined way based on the mutual information.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9974985718727112}]}, {"text": "The efficiency of information bottleneck algorithm) motivates us to take it as the kernel of our framework.", "labels": [], "entities": []}, {"text": "As far as we know, this approach has not been employed in opinion mining yet.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.8317199647426605}]}, {"text": "In traditional information bottleneck approach, the distance between two data objects is measured by the Jensen-Shannon divergence, which aims to measure the divergence between two probability distributions.", "labels": [], "entities": []}, {"text": "We alter this measure to integrate more semantic information, which will be illustrated in detail in the following sections, and the experimental result shows the effectiveness of the alteration.", "labels": [], "entities": []}, {"text": "It would be worthwhile to highlight several aspects of our work here: \ud97b\udf59 We propose an iterative reinforcement framework, and under this framework, review feature words and opinion words are organized into categories in a simultaneous and iterative manner.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 In the process of clustering, the semantic information and the co-occurrence information are integrated.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 The experimental results on real Chinese web reviews demonstrate that proposed method outperforms the template extraction based algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experiments and the data used in these experiments.", "labels": [], "entities": []}, {"text": "We evaluate our approach from two perspectives: 1) Effectiveness of product feature category construction by mutual reinforcement based clustering; 2) Precision of sentiment association between product feature categories and opinion word groups;   To calculate agreement between the review feature category construction results and the correct labels, we make use of the Rand index.", "labels": [], "entities": [{"text": "product feature category construction", "start_pos": 68, "end_pos": 105, "type": "TASK", "confidence": 0.6287463158369064}, {"text": "Precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.8818380832672119}]}, {"text": "This allows fora measure of agreement between two partitions, P 1 and P 2 , of the same data set D.", "labels": [], "entities": []}, {"text": "Each partition is viewed as a collection of n*(n-1)/2 pairwise decisions, where n is the size of D.", "labels": [], "entities": []}, {"text": "For each pair of points d i and d j in D, P i either assigns them to the same cluster or to different clusters.", "labels": [], "entities": []}, {"text": "Leta be the number of decisions where d i is in the same cluster as d j in P 1 and in P 2 . Let b be the number of decisions where the two instances are placed in different clusters in both partitions.", "labels": [], "entities": []}, {"text": "Total agreement can then be calculated using In our case, the parts of product feature words in the pre-constructed evaluation set are used to represent the data set D; a and b represent the partition agreements between the pairs of any two words in the parts and in the clustering results respectively.", "labels": [], "entities": []}, {"text": "In equation 10, the parameter \u03b1 reflects the respective contribution of semantic information and co-occurrence information to the final distance.", "labels": [], "entities": []}, {"text": "When 0 \u03b1 = or 1 \u03b1 = , the co-occurrence information or the semantic information will be utilized alone.", "labels": [], "entities": []}, {"text": "In order to get the optimal combination of the two type of distance, we adjust the parameter \u03b1 from 0 to 1(stepped by 0.2), and the accuracy of feature category construction with different \u03b1 are shown in: From this figure, we can find that the semantic information (\u03b1 =1) contributes much more to the accuracy of review feature category construction than the co-occurrence information ( \u03b1 =0), and when \u03b1 =0, the approach is equivalent to the traditional information bottleneck approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9987879395484924}, {"text": "feature category construction", "start_pos": 144, "end_pos": 173, "type": "TASK", "confidence": 0.66295059521993}, {"text": "accuracy", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.998354434967041}, {"text": "review feature category construction", "start_pos": 313, "end_pos": 349, "type": "TASK", "confidence": 0.6471864506602287}]}, {"text": "We consider this is due partly to the sparseness of the cor-pus, by enlarging the scale of the corpus or using the search engine (e.g. google etc), we can get more accurate results.", "labels": [], "entities": []}, {"text": "Additionally, by a sensible adjust on the parameter \u03b1 (in this experiment, we set \u03b1 as 0.6), we can get higher accuracy than the two baselines ( \u03b1 =0 and \u03b1 =1), which indicates the necessity and effectiveness of the integration of semantic information and co-occurrence information in the proposed approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9990853071212769}]}, {"text": "We use precision to evaluate the performance of sentiment association.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9993987083435059}, {"text": "sentiment association", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.9040375649929047}]}, {"text": "An evaluation set is constructed manually first, in which there are not only the categories that every review feature word belong to, but also the relationship between each category and opinion word.", "labels": [], "entities": []}, {"text": "Then we define precision as: number of correctly associated pairs Precision number of detected pairs = (12) A comparative result is got by the means of template-extraction based approach on the same test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9992375373840332}]}, {"text": "By the usage of regular expression, the nouns (phrase) and gerund (phrase) are extracted as the review features, and the nearest adjectives are extracted as the related opinion words.", "labels": [], "entities": []}, {"text": "Because the modifiers of adjectives in reviews also contain rich sentiment information and express the view of customs, we extract adjectives and their modifiers simultaneously, and take them as the opinion words.", "labels": [], "entities": []}, {"text": "78.90% shows the advantage of our approach over the extraction by explicit adjacency.", "labels": [], "entities": []}, {"text": "Using the same product feature categorization, our sentiment association approach get a more accurate pair set than the direct extraction based on explicit adjacency.", "labels": [], "entities": []}, {"text": "The precision we obtained by the iterative reinforcement approach is 78.90%, almost 13 points higher than the adjacency approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990062117576599}]}, {"text": "This indicates that there area large number of hidden sentiment associations in the real custom reviews, which underlines the importance and value of our work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The detail information of corpus", "labels": [], "entities": [{"text": "detail", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9921252131462097}]}, {"text": " Table 2: The number of candidate review features  and opinion words in our corpus", "labels": [], "entities": []}, {"text": " Table 3: Performance comparison in sentiment asso- ciation", "labels": [], "entities": [{"text": "sentiment asso- ciation", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.6300013065338135}]}]}