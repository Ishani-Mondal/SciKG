{"title": [{"text": "Quadratic Features and Deep Architectures for Chunking", "labels": [], "entities": [{"text": "Chunking", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.8943408727645874}]}], "abstractContent": [{"text": "We experiment with several chunking models.", "labels": [], "entities": []}, {"text": "Deeper architectures achieve better generalization.", "labels": [], "entities": []}, {"text": "Quadratic filters, a simplification of a theoretical model of V1 complex cells, reliably increase accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9965575933456421}]}, {"text": "In fact, logistic regression with quadratic filters outperforms a standard single hidden layer neural network.", "labels": [], "entities": []}, {"text": "Adding quadratic filters to logistic regression is almost as effective as feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.9150102734565735}]}, {"text": "Despite predicting each output label independently , our model is competitive with ones that use previous decisions.", "labels": [], "entities": []}], "introductionContent": [{"text": "There are three general approaches to improving chunking performance: engineer better features, improve inference, and improve the model.", "labels": [], "entities": []}, {"text": "Manual feature engineering is a common direction.", "labels": [], "entities": []}, {"text": "One technique is to take primitive features and manually compound them.", "labels": [], "entities": []}, {"text": "This technique is common, and most NLP systems use n-gram based features, for example).", "labels": [], "entities": []}, {"text": "Another approach is linguistically motivated feature engineering, e.g..", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7132608294487}]}, {"text": "Other works have looked in the direction of improving inference.", "labels": [], "entities": []}, {"text": "Rather than predicting each decision independently, previous decisions can be included in the inference process.", "labels": [], "entities": []}, {"text": "In this work, we use the simplest approach of modeling each decision independently.", "labels": [], "entities": []}, {"text": "The third direction is by using a better model.", "labels": [], "entities": []}, {"text": "If modeling capacity can be added without introducing too many extra degrees of freedom, generalization could be improved.", "labels": [], "entities": [{"text": "generalization", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.9602652192115784}]}, {"text": "One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities, for example SVMs with a non-linear kernel (), inducing compound features in a CRF, neural networks, and boosting decision trees).", "labels": [], "entities": []}, {"text": "Recently, showed that capacity can be increased by adding quadratic filters, leading to improved generalization on vision tasks.", "labels": [], "entities": []}, {"text": "This work examines how well quadratic filters work for an NLP task.", "labels": [], "entities": []}, {"text": "Compared to manual feature engineering, improved models are appealing because they are less task-specific.", "labels": [], "entities": []}, {"text": "We experiment on the task of chunking), a syntactic sequence labeling task.", "labels": [], "entities": [{"text": "syntactic sequence labeling task", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.7146074026823044}]}], "datasetContent": [{"text": "We follow the conditions in the CoNLL-2000 shared task).", "labels": [], "entities": [{"text": "CoNLL-2000 shared task", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.8271112640698751}]}, {"text": "Of the 8936 training sentences, we used 1000 randomly sampled sentences (23615 words) for validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on validation of varying the feature set,  for the architecture in Figure 1 with 4 quadratic filters.", "labels": [], "entities": []}, {"text": " Table 3: Test set results for Ando and Zhang (2005), Kudo  and Matsumoto (2001), our I-T-W-W-O model, Carreras  and M` arquez (2003), Sha and Pereira (2003), McCallum  (2003), Zhang et al. (2002), and our best I-O model.  AZ05-is Ando and Zhang (2005) using purely supervised  training, not semi-supervised training. Scores are noun  phrase F1, and overall chunk precision, recall, and F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 342, "end_pos": 344, "type": "METRIC", "confidence": 0.9424448013305664}, {"text": "precision", "start_pos": 364, "end_pos": 373, "type": "METRIC", "confidence": 0.9090961217880249}, {"text": "recall", "start_pos": 375, "end_pos": 381, "type": "METRIC", "confidence": 0.9994490742683411}, {"text": "F1", "start_pos": 387, "end_pos": 389, "type": "METRIC", "confidence": 0.9997914433479309}]}]}