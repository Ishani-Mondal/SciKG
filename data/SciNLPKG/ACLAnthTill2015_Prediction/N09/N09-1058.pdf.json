{"title": [{"text": "Streaming for large scale NLP: Language Modeling", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.6962085366249084}]}], "abstractContent": [{"text": "In this paper, we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems.", "labels": [], "entities": []}, {"text": "We present an efficient low-memory method for constructing high-order approximate n-gram frequency counts.", "labels": [], "entities": []}, {"text": "The method is based on a determinis-tic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint.", "labels": [], "entities": []}, {"text": "We show that this method easily scales to billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine.", "labels": [], "entities": []}, {"text": "Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.764940619468689}]}], "introductionContent": [{"text": "In many NLP problems, we are faced with the challenge of dealing with large amounts of data.", "labels": [], "entities": []}, {"text": "Many problems boil down to computing relative frequencies of certain items on this data.", "labels": [], "entities": []}, {"text": "Items can be words, patterns, associations, n-grams, and others.", "labels": [], "entities": []}, {"text": "Language modeling, noun-clustering (), constructing syntactic rules for SMT (), and finding analogies are examples of some of the problems where we need to compute relative frequencies.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6993343979120255}, {"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9861077070236206}]}, {"text": "We use language modeling as a canonical example of a large-scale task that requires relative frequency estimation.", "labels": [], "entities": [{"text": "relative frequency estimation", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.5871214171250662}]}, {"text": "Computing relative frequencies seems like an easy problem.", "labels": [], "entities": []}, {"text": "However, as corpus sizes grow, it becomes a highly computational expensive task.", "labels": [], "entities": []}, {"text": "used 1500 machines fora day to compute the relative frequencies of n-grams (summed overall orders from 1 to 5) from 1.8TB of web data.", "labels": [], "entities": []}, {"text": "Their resulting model contained 300 million unique n-grams.", "labels": [], "entities": []}, {"text": "It is not realistic using conventional computing resources to use all the 300 million n-grams for applications like speech recognition, spelling correction, information extraction, and statistical machine translation (SMT).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7694013714790344}, {"text": "spelling correction", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.9277678430080414}, {"text": "information extraction", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.8098235428333282}, {"text": "statistical machine translation (SMT)", "start_pos": 185, "end_pos": 222, "type": "TASK", "confidence": 0.7853945344686508}]}, {"text": "Hence, one of the easiest way to reduce the size of this model is to use count-based pruning which discards all n-grams whose count is less than a pre-defined threshold.", "labels": [], "entities": []}, {"text": "Although countbased pruning is quite simple, yet it is effective for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7395484149456024}]}, {"text": "As we do not have a copy of the web, we will use a portion of gigaword i.e. EAN (see Section 4.1) to show the effect of count-based pruning on performance of SMT (see Section 5.1).", "labels": [], "entities": [{"text": "SMT", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.983996570110321}]}, {"text": "shows that using a cutoff of 100 produces a model of size 1.1 million n-grams with a Bleu score of 28.03.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9788612127304077}]}, {"text": "If we compare this with an exact model of size 367.6 million n-grams, we see an increase of 0.8 points in Bleu (95% statistical significance level: Effect of entropy-based pruning on SMT performance using EAN corpus. Results are as in is \u2248 0.53 Bleu).", "labels": [], "entities": [{"text": "Bleu", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9953985810279846}, {"text": "SMT", "start_pos": 183, "end_pos": 186, "type": "TASK", "confidence": 0.9916270971298218}, {"text": "EAN corpus.", "start_pos": 205, "end_pos": 216, "type": "DATASET", "confidence": 0.6900992095470428}]}, {"text": "However, we need 300 times bigger model to get such an increase.", "labels": [], "entities": []}, {"text": "Unfortunately, it is not possible to integrate such a big model inside a decoder using normal computation resources.", "labels": [], "entities": []}, {"text": "A better way of reducing the size of n-grams is to use entropy pruning.", "labels": [], "entities": []}, {"text": "shows the results with entropy pruning with different settings of \u01eb.", "labels": [], "entities": []}, {"text": "We see that for three settings of \u01eb equal to 1e-10, 5e-10 and 1e-9, we get Bleu scores comparable to the exact model.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9874477982521057}]}, {"text": "However, the size of all these models is not at all small.", "labels": [], "entities": []}, {"text": "The size of smallest model is 25% of the exact model.", "labels": [], "entities": []}, {"text": "Even with this size it is still not feasible to integrate such a big model inside a decoder.", "labels": [], "entities": []}, {"text": "If we take a model of size comparable to count cutoff of 100, i.e., with \u01eb = 5e-7, we see both count-based pruning as well as entropy pruning performs the same.", "labels": [], "entities": [{"text": "\u01eb", "start_pos": 73, "end_pos": 74, "type": "METRIC", "confidence": 0.9860963225364685}]}, {"text": "There also have been prior work on maintaining approximate counts for higher-order language models (LMs)) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately.", "labels": [], "entities": []}, {"text": "There are two difficulties with scaling all the above approaches as the order of the LM increases.", "labels": [], "entities": []}, {"text": "Firstly, the computation time to build the database of counts increases rapidly.", "labels": [], "entities": []}, {"text": "Secondly, the initial disk storage required to maintain these counts, prior to building the compressed representation is enormous.", "labels": [], "entities": []}, {"text": "The method we propose solves both of these problems.", "labels": [], "entities": []}, {"text": "We do this by making use of the streaming algorithm paradigm (.", "labels": [], "entities": []}, {"text": "Working under the assumption that multiple-GB models are infeasible, our goal is to instead of estimating a large model and then compressing it, we directly estimate a small model.", "labels": [], "entities": []}, {"text": "We use a deterministic streaming algorithm () that computes approximate frequency counts of frequently occurring n-grams.", "labels": [], "entities": []}, {"text": "This scheme is considerably more accurate in getting the actual counts as compared to other schemes) that find the set of frequent items without caring about the accuracy of counts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9987853169441223}]}, {"text": "We use these counts directly as features in an SMT system, and propose a direct way to integrate these features into an SMT decoder.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.982132077217102}, {"text": "SMT decoder", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.8931097388267517}]}, {"text": "Experiments show that directly storing approximate counts of frequent 5-grams compared to using count or entropybased pruning counts gives equivalent SMT performance, while dramatically reducing the memory usage and getting rid of pre-computing a large model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 150, "end_pos": 153, "type": "TASK", "confidence": 0.9948931932449341}]}], "datasetContent": [{"text": "All the experiments conducted here make use of publicly available resources.", "labels": [], "entities": []}, {"text": "Europarl (EP) corpus French-English section is used as parallel data.", "labels": [], "entities": [{"text": "Europarl (EP) corpus French-English section", "start_pos": 0, "end_pos": 43, "type": "DATASET", "confidence": 0.9444816964013236}]}, {"text": "The publicly available Moses 4 decoder is used for training and decoding.", "labels": [], "entities": [{"text": "Moses 4 decoder", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.8621127009391785}]}, {"text": "The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences 5 is used as the development set.", "labels": [], "entities": [{"text": "ACL SMT workshop", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.5833041568597158}]}, {"text": "Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT)", "start_pos": 0, "end_pos": 34, "type": "METRIC", "confidence": 0.8336854108742305}]}, {"text": "The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (), NIST) and METEOR () scores.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9950821399688721}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9988009929656982}, {"text": "NIST", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.5514342188835144}, {"text": "METEOR", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9490025043487549}]}, {"text": "The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.", "labels": [], "entities": [{"text": "2007 news test data from ACL SMT workshop 2007", "start_pos": 55, "end_pos": 101, "type": "DATASET", "confidence": 0.7807070877816942}]}, {"text": "In our first experiment, we use accuracy, \u03c1 and MSE metrics for evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9996963739395142}, {"text": "\u03c1", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.9476637840270996}, {"text": "MSE", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9927466511726379}]}, {"text": "Here, we compute 5-gram stream counts with different settings of \u01eb on the EAN corpus.", "labels": [], "entities": [{"text": "EAN corpus", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.8639000356197357}]}, {"text": "\u01eb controls the number of stream counts produced by the algorithm.", "labels": [], "entities": []}, {"text": "The results in support the theory that decreasing the value of \u01eb improves the quality of stream counts.", "labels": [], "entities": []}, {"text": "Also, as expected, the algorithm produces more stream counts with smaller values of \u01eb.", "labels": [], "entities": []}, {"text": "The evaluation of stream counts obtained with \u01eb = 50e-8 and 20e-8 reveal that the stream counts learned with this large value are more susceptible to errors.", "labels": [], "entities": [{"text": "\u01eb", "start_pos": 46, "end_pos": 47, "type": "METRIC", "confidence": 0.993190586566925}]}, {"text": "If we look closely at the counts for \u01eb = 50e-8, we see that we get at least 30% of the stream counts from 245k true counts.", "labels": [], "entities": []}, {"text": "This number is not significantly worse than the 36% of stream counts obtained from 4, 018k true counts for the smallest value of \u01eb = 5e-8.", "labels": [], "entities": [{"text": "\u01eb", "start_pos": 129, "end_pos": 130, "type": "METRIC", "confidence": 0.9710896015167236}]}, {"text": "However, if we look at the other two metrics, the ranking correlation \u03c1 of stream counts compared with true counts on \u01eb = 50e-8 and 20e-8 is low compared to other \u01eb values.", "labels": [], "entities": [{"text": "ranking correlation \u03c1", "start_pos": 50, "end_pos": 71, "type": "METRIC", "confidence": 0.7751475969950358}]}, {"text": "For the MSE, the error with stream counts on these \u01ebm values is again high compared to other values.", "labels": [], "entities": []}, {"text": "As we decrease the value of \u01eb we continually get better results: decreasing \u01eb pushes the stream counts towards the true counts.", "labels": [], "entities": []}, {"text": "However, using a smaller \u01eb increases the memory usage.", "labels": [], "entities": [{"text": "memory", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.938977062702179}]}, {"text": "Looking at the evaluation, it is therefore advisable to use 5-gram stream counts produced with at most \u01eb \u2264 10e-7 for the EAN corpus.", "labels": [], "entities": [{"text": "EAN corpus", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.8374451398849487}]}, {"text": "Since it is not possible to compute true 7-grams counts on EAN with available computing resources,   we carryout a similar experiment for 7-grams on EP to verify the results for higher order n-grams 2 . The results in tell a story similar to our results for 7-grams.", "labels": [], "entities": []}, {"text": "The size of EP corpus is much smaller than EAN and so we see even better results on each of the metrics with decreasing the value of \u01eb.", "labels": [], "entities": [{"text": "EP corpus", "start_pos": 12, "end_pos": 21, "type": "DATASET", "confidence": 0.8759589791297913}]}, {"text": "The overall trend remains the same; here too, setting \u01eb \u2264 10e-8 is the most effective strategy.", "labels": [], "entities": [{"text": "\u01eb", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9934700727462769}]}, {"text": "The fact that these results are consistent across two datasets of different sizes and different n-gram sizes suggests that they will carryover to other tasks.", "labels": [], "entities": []}, {"text": "In the second experiment, we evaluate the quality of the top K (sorted by frequency) 5-gram stream counts.", "labels": [], "entities": []}, {"text": "Here again, we use accuracy, \u03c1 and MSE for evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9998164772987366}, {"text": "\u03c1", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.976129949092865}, {"text": "MSE", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9977760910987854}]}, {"text": "We fix the value of \u01eb to 5e-8 and compute 5-gram stream counts on the EAN corpus.", "labels": [], "entities": [{"text": "EAN corpus", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.8721843659877777}]}, {"text": "We vary the value of K between 100k and 4, 018k (i.e all the n-gram counts produced by the stream algorithm).", "labels": [], "entities": []}, {"text": "The experimental results in support the theory that stream count algorithm computes the exact count of most of the high frequency n-grams.", "labels": [], "entities": []}, {"text": "Looking closer, we see that if we evaluate the algorithm on just the top 100k 5-grams (roughly 5% of all 5-grams produced), we see almost perfect results.", "labels": [], "entities": []}, {"text": "Further, if we take the top 1, 000k 5-grams (approximately 25% of all 5-grams) we again see excellent  performance on all metrics.", "labels": [], "entities": []}, {"text": "The accuracy of the results decrease slightly, but the \u03c1 and M SE metrics are not decreased that much in comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997020363807678}, {"text": "M SE metrics", "start_pos": 61, "end_pos": 73, "type": "METRIC", "confidence": 0.826063315073649}]}, {"text": "Performance starts to degrade as we get to 2, 000k (over 50% of all 5-grams), a result that is not too surprising.", "labels": [], "entities": []}, {"text": "However, even here we note that the MSE is low, suggesting that the frequencies of stream counts (found in top K true counts) are very close to the true counts.", "labels": [], "entities": [{"text": "MSE", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8881044387817383}]}, {"text": "Thus, we conclude that the quality of the 5-gram stream counts produced for this value of \u01eb is quite high (in relation to the true counts).", "labels": [], "entities": []}, {"text": "As before, we corroborate our results with higher order n-grams.", "labels": [], "entities": []}, {"text": "We evaluate the quality of top K 7-gram stream counts on EP.", "labels": [], "entities": []}, {"text": "Since EP is a smaller corpus, we evaluate the stream counts produced by setting \u01eb to 10e-8.", "labels": [], "entities": []}, {"text": "Here we vary the value of K between 10k and 246k (the total number produced by the stream algorithm).", "labels": [], "entities": []}, {"text": "As we saw earlier with 5-grams, the top 10k (i.e. approximately 5% of all 7-grams) are of very high quality.", "labels": [], "entities": []}, {"text": "Results, and this remains true even when we increase K to 100k.", "labels": [], "entities": []}, {"text": "There is a drop in the accuracy and a slight drop in \u03c1, while the MSE remains the same.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9998718500137329}, {"text": "\u03c1", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9980266690254211}, {"text": "MSE", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.8887566924095154}]}, {"text": "Taking all counts again shows a significant decrease in both accuracy and \u03c1 scores, but this does not affect MSE scores significantly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9996470212936401}, {"text": "\u03c1 scores", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9365086555480957}, {"text": "MSE", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.4738817811012268}]}, {"text": "Hence, the 7-gram stream counts i.e. 246k counts produced by \u01eb = 10e-8 are quite accurate when compared to the top 246k true counts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect of count-based pruning on SMT per- formance using EAN corpus. Results are according to  BLEU, NIST and METEOR (MET) metrics. Bold #s are  not statistically significant worse than exact model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9906566143035889}, {"text": "EAN corpus", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.7695537507534027}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9978808164596558}, {"text": "NIST", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.9199057817459106}, {"text": "METEOR (MET)", "start_pos": 120, "end_pos": 132, "type": "METRIC", "confidence": 0.9019059985876083}]}, {"text": " Table 2: Effect of entropy-based pruning on SMT perfor- mance using EAN corpus. Results are as in", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9798105955123901}, {"text": "EAN corpus", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.7829045057296753}]}, {"text": " Table 3: Corpus Statistics and perplexity of LMs made  with each of these corpuses on development set", "labels": [], "entities": []}, {"text": " Table 4: Evaluating quality of 5-gram stream counts for  different settings of \u01eb on EAN corpus", "labels": [], "entities": [{"text": "EAN corpus", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.8790880441665649}]}, {"text": " Table 5: Evaluating quality of 7-gram stream counts for  different settings of \u01eb on EP corpus", "labels": [], "entities": []}, {"text": " Table 6: Evaluating top K sorted 5-gram stream counts  for \u01eb=5e-8 on EAN corpus", "labels": [], "entities": [{"text": "\u01eb", "start_pos": 60, "end_pos": 61, "type": "METRIC", "confidence": 0.96684730052948}, {"text": "EAN", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9159379601478577}]}, {"text": " Table 7: Evaluating top K sorted 7-gram stream counts  for \u01eb=10e-8 on EP corpus", "labels": [], "entities": [{"text": "\u01eb", "start_pos": 60, "end_pos": 61, "type": "METRIC", "confidence": 0.9676951766014099}]}, {"text": " Table 8: Gzipped space required to store n-gram counts on disk and their coverage on a test set with different \u01ebm", "labels": [], "entities": []}, {"text": " Table 9: Evaluating SMT with different LMs on EAN.  Results are according to BLEU, NIST and MET metrics.  Bold #s are not statistically significant worse than exact.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9914342164993286}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9984650611877441}, {"text": "NIST", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8893000483512878}, {"text": "MET", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.7272023558616638}]}]}