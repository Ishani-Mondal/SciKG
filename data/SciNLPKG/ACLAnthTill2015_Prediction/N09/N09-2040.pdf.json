{"title": [{"text": "Answer Credibility: A Language Modeling Approach to Answer Validation", "labels": [], "entities": [{"text": "Answer Credibility", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9588854610919952}, {"text": "Answer Validation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.9653922021389008}]}], "abstractContent": [{"text": "Answer Validation is a topic of significant interest within the Question Answering community.", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9362519085407257}, {"text": "Question Answering", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8377196788787842}]}, {"text": "In this paper, we propose the use of language modeling methodologies for Answer Validation, using corpus-based methods that do not require the use of external sources.", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.9645351469516754}]}, {"text": "Specifically , we propose a model for Answer Credibility which quantifies the reliability of a source document that contains a candidate answer and the Question's Context Model.", "labels": [], "entities": [{"text": "Answer Credibility", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9289372861385345}]}], "introductionContent": [{"text": "In recent years, Answer Validation has become a topic of significant interest within the Question Answering community.", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9473971724510193}, {"text": "Question Answering", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8326822519302368}]}, {"text": "In the general case, one can describe Answer Validation as the process that decides whether a Question is correctly answered by an Answer according to a given segment of supporting Text.) presents an approach to Answer Validation that uses redundant information sources on the Web; they propose that the number of Web documents in which the question and the answer co-occurred can serve as an indicator of answer validity.", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.8859105706214905}, {"text": "Answer Validation", "start_pos": 212, "end_pos": 229, "type": "TASK", "confidence": 0.8787263929843903}]}, {"text": "Other recent approaches to Answer Validation Exercise in the Cross-Language Evaluation Forum (CLEF)) make use of textual entailment methodologies for the purposes of Answer Validation.", "labels": [], "entities": [{"text": "Answer Validation Exercise", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.9575238029162089}, {"text": "Answer Validation", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.9221680462360382}]}, {"text": "In this paper, we propose the use of language modeling methodologies for Answer Validation, using corpus-based methods that do not require the use of external sources.", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.9645351469516754}]}, {"text": "Specifically, we propose the development of an Answer Credibility score which quantifies reliability of a source document that contains a candidate answer with respect to the Question's Context Model.", "labels": [], "entities": []}, {"text": "Unlike many textual entailment methods, our methodology has the advantage of being applicable to question types for which hypothesis generation is not easily accomplished.", "labels": [], "entities": [{"text": "hypothesis generation", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.7785844504833221}]}, {"text": "The remainder of this paper describes our work in progress, including our model for Answer Credibility, our experiments and results to date, and future work.", "labels": [], "entities": [{"text": "Answer Credibility", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9626034796237946}]}], "datasetContent": [{"text": "The experimental methodology we used is shown as a block diagram in.", "labels": [], "entities": []}, {"text": "To validate our approach, we used the set of all factoid questions from the Text Retrieval Conference (TREC) 2006 Question Answering Track).", "labels": [], "entities": [{"text": "Text Retrieval Conference (TREC) 2006 Question Answering Track", "start_pos": 76, "end_pos": 138, "type": "TASK", "confidence": 0.8104896783828736}]}, {"text": "The OpenEphyra Question Answering testbed) was then used as the framework for our Answer Credibility implementation.", "labels": [], "entities": [{"text": "OpenEphyra Question Answering testbed", "start_pos": 4, "end_pos": 41, "type": "DATASET", "confidence": 0.8163387775421143}, {"text": "Answer Credibility implementation", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.926107664903005}]}, {"text": "OpenEphyra uses a baseline Answer Validation mechanism which uses documents retrieved using Yahoo!", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7816754281520844}]}, {"text": "search to support candidate answers found in retrieved passages.", "labels": [], "entities": []}, {"text": "In our experiments, we constructed the Question Context according to the methodology described in.", "labels": [], "entities": []}, {"text": "Our experiments used the Lemur Language Modeling toolkit () and the Indri search engine) to construct the Question Context and document language models.", "labels": [], "entities": []}, {"text": "We then inserted an Answer Credibility filter into the OpenEphyra processing pipeline which modulates the OpenEphyra answer score according to the following formula: Here score is the original OpenEphyra answer score and score' is the modulated answer score.", "labels": [], "entities": []}, {"text": "In this model, \u03bb is an interpolation constant which we set using the average of the P(z|R) values for those aspects that are included in the Question Context.", "labels": [], "entities": []}, {"text": "For the purposes of evaluating the effectiveness of our theoretical model, we use the accuracy and Mean Reciprocal Rank (MRR) metrics).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9997581839561462}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 99, "end_pos": 125, "type": "METRIC", "confidence": 0.9752382338047028}]}], "tableCaptions": [{"text": " Table 1: Average MRR of Baseline vs. Baseline Including  Answer Credibility", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9890775680541992}, {"text": "MRR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.7439923882484436}]}, {"text": " Table 2: Average Accuracy of Baseline vs. Baseline In- cluding Answer Credibility", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.962407648563385}, {"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.71019446849823}]}]}