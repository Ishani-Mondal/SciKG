{"title": [], "abstractContent": [{"text": "Both coarse-to-fine and A * parsing use simple grammars to guide search in complex ones.", "labels": [], "entities": []}, {"text": "We compare the two approaches in a common , agenda-based framework, demonstrating the tradeoffs and relative strengths of each method.", "labels": [], "entities": []}, {"text": "Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A * is superior.", "labels": [], "entities": [{"text": "coarse-to-fine", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.9791380763053894}, {"text": "A", "start_pos": 107, "end_pos": 108, "type": "METRIC", "confidence": 0.9874931573867798}]}, {"text": "In addition , we present the first experiments on hierarchical A * parsing, in which computation of heuristics is itself guided by meta-heuristics.", "labels": [], "entities": [{"text": "hierarchical A * parsing", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6270222887396812}]}, {"text": "Multi-level hierarchies are helpful in both approaches , but are more effective in the coarse-to-fine case because of accumulated slack in A * heuristics.", "labels": [], "entities": []}], "introductionContent": [{"text": "The grammars used by modern parsers are extremely large, rendering exhaustive parsing impractical.", "labels": [], "entities": []}, {"text": "For example, the lexicalized grammars of and and the statesplit grammars of are all too large to construct unpruned charts in memory.", "labels": [], "entities": []}, {"text": "One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar ().", "labels": [], "entities": []}, {"text": "Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used.", "labels": [], "entities": []}, {"text": "In particular, generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn.", "labels": [], "entities": []}, {"text": "Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees on optimality.", "labels": [], "entities": []}, {"text": "Another line of work has explored A * search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space ().", "labels": [], "entities": []}, {"text": "In particular, Klein and Manning (2003a) investigated A * for lexicalized parsing in a factored model.", "labels": [], "entities": [{"text": "A", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9820252656936646}]}, {"text": "In that case, A * vastly improved the search in the lexicalized grammar, with provable optimality.", "labels": [], "entities": []}, {"text": "However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A * heuristic itself.", "labels": [], "entities": []}, {"text": "It is not obvious, however, how A * can be stacked in a hierarchical or multi-pass way to speedup the computation of such complex heuristics.", "labels": [], "entities": []}, {"text": "In this paper, we address three open questions regarding efficient hierarchical search.", "labels": [], "entities": []}, {"text": "First, can a hierarchy of A * bounds be used, analogously to hierarchical coarse-to-fine pruning?", "labels": [], "entities": []}, {"text": "We show that recent work in hierarchical A * can naturally be applied to both the hierarchically refined grammars of as well as the lexicalized grammars of.", "labels": [], "entities": []}, {"text": "Second, what are the tradeoffs between coarse-to-fine pruning and A * methods?", "labels": [], "entities": []}, {"text": "We show that coarse-to-fine is generally much faster, but at the cost of search errors.", "labels": [], "entities": []}, {"text": "1 Below a certain search error rate, A * is faster and, of course, optimal.", "labels": [], "entities": [{"text": "A", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9967670440673828}]}, {"text": "Finally, when and how, qualitatively, do these methods fail?", "labels": [], "entities": []}, {"text": "A * search's work grows quickly as the slack increases between the heuristic bounds and the true costs.", "labels": [], "entities": []}, {"text": "On the other hand, coarse-to-fine prunes unreliably when the approximating grammar Name Rule Priority IN r : wr I(Bt, i, k) :: Deduction rule for A * parsing.", "labels": [], "entities": [{"text": "A * parsing", "start_pos": 146, "end_pos": 157, "type": "TASK", "confidence": 0.5167896548906962}]}, {"text": "The items on the left of the \u21d2 indicate what edges must be present on the chart and what rule can be used to combine them, and the item on the right is the edge that maybe added to the agenda.", "labels": [], "entities": []}, {"text": "The weight of each edge appears after the colon.", "labels": [], "entities": []}, {"text": "The ruler is A \u2192 BC. is very different from the target grammar.", "labels": [], "entities": [{"text": "BC.", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.7235235571861267}]}, {"text": "We empirically demonstrate both failure modes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our focus is parsing speed.", "labels": [], "entities": [{"text": "parsing speed", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7809312343597412}]}, {"text": "Thus, we would ideally evaluate our algorithms in terms of CPU time.", "labels": [], "entities": []}, {"text": "However, this measure is problematic: CPU time is influenced by a variety of factors, including the architecture of the hardware, low-level implementation details, and other running processes, all of which are hard to normalize.", "labels": [], "entities": []}, {"text": "It is common to evaluate best-first parsers in terms of edges popped off the agenda.", "labels": [], "entities": []}, {"text": "This measure is used by and.", "labels": [], "entities": []}, {"text": "However, when edges from grammars of varying size are processed on the same agenda, the number of successor edges per edge popped changes depending on what grammar the edge was constructed from.", "labels": [], "entities": []}, {"text": "In particular, edges in more refined grammars are more expensive than edges in coarser grammars.", "labels": [], "entities": []}, {"text": "Thus, our basic unit of measurement will be edges pushed onto the agenda.", "labels": [], "entities": []}, {"text": "We found in our experiments that this was well correlated with CPU time.", "labels": [], "entities": []}, {"text": "Figure 4: Efficiency of several hierarchical parsing algorithms, across the test set.", "labels": [], "entities": []}, {"text": "UCS and all A * variants are optimal and thus make no search errors.", "labels": [], "entities": [{"text": "UCS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9307337999343872}]}, {"text": "The CTF variants all make search errors on about 2% of sentences.", "labels": [], "entities": []}, {"text": "We also experimented with the lexicalized parsing model described in.", "labels": [], "entities": []}, {"text": "This lexicalized parsing model is constructed as the product of a dependency model and the unlexical-: Performance of CTF as a function of search errors for state split grammars.", "labels": [], "entities": []}, {"text": "The dashed lines represent the time taken by UCS and HA * which make no search errors.", "labels": [], "entities": [{"text": "UCS", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9267386794090271}]}, {"text": "As search accuracy increases, the time taken by CTF increases until it eventually becomes slower than HA * . The y-axis is a log scale.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9915621876716614}, {"text": "HA", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9897980690002441}]}, {"text": "We constructed these grammars using the Stanford Parser.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.910836786031723}]}, {"text": "The PCFG has 19054 symbols 36078 rules.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9725545644760132}]}, {"text": "The combined (sentence-specific) grammar has n times as many symbols and 2n 2 times as many rules, where n is the length of an input sentence.", "labels": [], "entities": []}, {"text": "This model was trained on sections 2-20 of the Penn Treebank and tested on section 21.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.992505669593811}]}, {"text": "For these lexicalized grammars, we did not perform experiments with UCS or more than one level of HA * . We used only the single PCFG projection used in.", "labels": [], "entities": [{"text": "UCS", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.8164932131767273}]}, {"text": "This grammar differs from the state split grammars in that it factors into two separate projections, a dependency projection and a PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.8508369326591492}]}, {"text": "show that one can use the sum of outside scores computed in these two projections as a heuristic in the combined lexicalized grammar.", "labels": [], "entities": []}, {"text": "The generalization of HA * to the factored case is straightforward but not effective.", "labels": [], "entities": []}, {"text": "We therefore treated the dependency projection as a black box and used only the PCFG projection inside the HA * framework.", "labels": [], "entities": [{"text": "PCFG projection", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9242342412471771}, {"text": "HA * framework", "start_pos": 107, "end_pos": 121, "type": "DATASET", "confidence": 0.8871695001920065}]}, {"text": "When computing A * outside estimates in the combined space, we use the sum of the two projections' outside scores as our completion costs.", "labels": [], "entities": [{"text": "completion", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9581441879272461}]}, {"text": "This is the same procedure as.", "labels": [], "entities": []}, {"text": "For CTF, we carryout a uniform cost search in the combined space where we have pruned items based on their max-marginals in the PCFG model only.", "labels": [], "entities": [{"text": "CTF", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7540955543518066}, {"text": "PCFG", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9269139170646667}]}, {"text": "In, we examine the speed/accuracy trade off for the lexicalized grammar.", "labels": [], "entities": [{"text": "speed", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9941243529319763}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8232371807098389}]}, {"text": "The trend here is the reverse of the result for the state split grammars: HA * is always faster than posterior pruning, even for thresholds which produce many search errors.", "labels": [], "entities": [{"text": "HA *", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9693835377693176}]}, {"text": "This is because the heuristic used in this model is actually an extraordinarily tight bound -on average, the slack even for spans of length 1 was less than 1% of the overall model cost.", "labels": [], "entities": []}], "tableCaptions": []}