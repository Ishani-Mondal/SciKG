{"title": [{"text": "Exploring Topic Continuation Follow-up Questions using Machine Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "Some of the Follow-Up Questions (FU Q) that an Interactive Question Answering (IQA) system receives are not topic shifts, but rather continuations of the previous topic.", "labels": [], "entities": [{"text": "Interactive Question Answering (IQA)", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.7560467720031738}]}, {"text": "In this paper , we propose an empirical framework to explore such questions, with two related goals in mind: (1) modeling the different relations that hold between the FU Q's answer and either the FU Q or the preceding dialogue, and (2) showing how this model can be used to identify the correct answer among several answer candidates.", "labels": [], "entities": [{"text": "FU Q", "start_pos": 197, "end_pos": 201, "type": "DATASET", "confidence": 0.8239706456661224}]}, {"text": "For both cases, we use Logistic Regression Models that we learn from real IQA data collected through a live system.", "labels": [], "entities": []}, {"text": "We show that by adding dialogue context features and features based on sequences of domain-specific actions that represent the questions and answers , we obtain important additional predic-tors for the model, and improve the accuracy with which our system finds correct answers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9992150068283081}]}], "introductionContent": [{"text": "Interactive Question Answering (IQA) can be described as a fusion of the QA paradigm with dialogue system capabilities.", "labels": [], "entities": [{"text": "Interactive Question Answering (IQA)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7625286976496378}]}, {"text": "While classical QA is concerned with questions posed in isolation, its interactive variant is intended to support the user in finding the correct answer via natural-language dialogue.", "labels": [], "entities": []}, {"text": "In an IQA setting, both the system and the user can pose Follow-Up Questions (FU Q).", "labels": [], "entities": [{"text": "Follow-Up Questions (FU Q)", "start_pos": 57, "end_pos": 83, "type": "METRIC", "confidence": 0.7570911347866058}]}, {"text": "In the second case, whenever an IQA system receives an additional user question (note that this is what we calla Follow-Up Question throughout this work), it can either interpret it as being thematically related to a previous dialogue segment (topic continuation), or as a shift to some new, unrelated topic (topic shift).", "labels": [], "entities": []}, {"text": "A definition of thematic relatedness of FU Qs might rely on the elements of the attentional state, i.e., on the objects, properties and relations that are salient before and after processing the user question.", "labels": [], "entities": [{"text": "FU Qs", "start_pos": 40, "end_pos": 45, "type": "TASK", "confidence": 0.5018327832221985}]}, {"text": "Topic continuation FU Qs should be interpreted within the context, whereas topic shift FU Qs have to be treated as first questions and can thus be processed with standard QA technologies.", "labels": [], "entities": [{"text": "Topic continuation FU Qs", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7691823616623878}]}, {"text": "Therefore, a first task in IQA is to detect whether a FU Q is a topic shift or a topic continuation ().", "labels": [], "entities": [{"text": "IQA", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9582303166389465}, {"text": "FU Q", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9057801067829132}]}, {"text": "To help answering topic continuation FU Qs, an IQA system would need to fuse the FU Q with certain information from the dialogue context).", "labels": [], "entities": [{"text": "answering topic continuation FU Qs", "start_pos": 8, "end_pos": 42, "type": "TASK", "confidence": 0.8707576513290405}]}, {"text": "Thus, a second task in IQA is to understand which turns in the dialogue context are possible locations of such information, and exactly what kind of information should be considered.", "labels": [], "entities": [{"text": "IQA", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9833508133888245}]}, {"text": "Knowing that a FU Q concerns the same topic as the previous question or answer, we thus want to study in more detail the way the informational content of questions and answers evolves before/after the FU Q is asked.", "labels": [], "entities": []}, {"text": "A model of these so-called informational transitions would provide insights into what a user is likely to ask about next in natural coherent humanmachine dialogue.", "labels": [], "entities": []}, {"text": "In order to tackle any of the two IQA tasks mentioned above we need IQA dialogues.", "labels": [], "entities": []}, {"text": "Most current work on IQA uses the TREC QA data; the TREC QA tracks in 2001 and 2004 included series of context questions, where FU Qs always depended on the context set by an earlier question from the same series.", "labels": [], "entities": [{"text": "TREC QA data", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8604394396146139}, {"text": "TREC QA tracks", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.8203626672426859}]}, {"text": "However, these data were constructed artificially and are not representative of actual dialogues from an IQA system (for instance, system answers are not considered at all).", "labels": [], "entities": []}, {"text": "Real IQA data yield chal-lenges for an automatic processing approach).", "labels": [], "entities": []}, {"text": "Our work is based on collecting and analyzing IQA dialogues from users of a deployed system.", "labels": [], "entities": []}, {"text": "In this paper, we address the second task introduced above, namely the study of common relations between the answer to a topic continuation FU Q and other turns in the dialogue context.", "labels": [], "entities": []}, {"text": "Our collected dialogue data are from the \"library help desk\" domain.", "labels": [], "entities": []}, {"text": "In many of the dialogues, library users request information about a specific library-related action; we are thus dealing with task-oriented dialogues.", "labels": [], "entities": []}, {"text": "This work is based on two hypotheses regarding relations holding between the FU Q's answer and the dialogue context.", "labels": [], "entities": [{"text": "FU Q's answer", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9278046488761902}]}, {"text": "For studying such relations, we want to explore the usefulness of a representation of the library-related action underlying questions and answers, and (2) a representation of the dialogue context of the FU Q.", "labels": [], "entities": [{"text": "FU Q", "start_pos": 203, "end_pos": 207, "type": "DATASET", "confidence": 0.8908970355987549}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Mean ranks of correct A 0 out of 529 answer candidates, across models and training/test splits", "labels": [], "entities": [{"text": "Mean ranks of correct A 0", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8112634519735972}]}]}