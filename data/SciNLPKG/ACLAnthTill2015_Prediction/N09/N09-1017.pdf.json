{"title": [{"text": "The Role of Implicit Argumentation in Nominal SRL", "labels": [], "entities": [{"text": "Nominal SRL", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.7800953388214111}]}], "abstractContent": [{"text": "Nominals frequently surface without overtly expressed arguments.", "labels": [], "entities": []}, {"text": "In order to measure the potential benefit of nominal SRL for downstream processes, such nominals must be accounted for.", "labels": [], "entities": [{"text": "SRL", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8405121564865112}]}, {"text": "In this paper, we show that a state-of-the-art nominal SRL system with an overall argument F 1 of 0.76 suffers a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation.", "labels": [], "entities": [{"text": "F 1", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.8755415380001068}]}, {"text": "We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%.", "labels": [], "entities": []}, {"text": "Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argu-mentation an important step for nominal SRL.", "labels": [], "entities": [{"text": "nominal SRL", "start_pos": 170, "end_pos": 181, "type": "TASK", "confidence": 0.587252289056778}]}], "introductionContent": [{"text": "In the past few years, a number of studies have focused on verbal semantic role labeling (SRL).", "labels": [], "entities": [{"text": "verbal semantic role labeling (SRL)", "start_pos": 59, "end_pos": 94, "type": "TASK", "confidence": 0.7736443749495915}]}, {"text": "Driven by annotation resources such as FrameNet () and), many systems developed in these studies have achieved argument F 1 scores near 80% in large-scale evaluations such as the one reported by.", "labels": [], "entities": [{"text": "argument F 1 scores", "start_pos": 111, "end_pos": 130, "type": "METRIC", "confidence": 0.7439304366707802}]}, {"text": "More recently, the automatic identification of nominal argument structure has received increased attention due to the release of the NomBank corpus This instance demonstrates the annotation of split arguments (Arg1) and modifying adjuncts (Location), which are also annotated in PropBank.", "labels": [], "entities": [{"text": "automatic identification of nominal argument structure", "start_pos": 19, "end_pos": 73, "type": "TASK", "confidence": 0.721491331855456}, {"text": "NomBank corpus", "start_pos": 133, "end_pos": 147, "type": "DATASET", "confidence": 0.9185775816440582}, {"text": "Arg1", "start_pos": 210, "end_pos": 214, "type": "METRIC", "confidence": 0.9293532371520996}, {"text": "PropBank", "start_pos": 279, "end_pos": 287, "type": "DATASET", "confidence": 0.9713907241821289}]}, {"text": "In cases where a nominal has a verbal counterpart, the interpretation of argument positions Arg0-Arg5 is consistent between the two corpora.", "labels": [], "entities": [{"text": "Arg0-Arg5", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9803323745727539}]}, {"text": "In addition to deverbal (i.e., event-based) nominalizations, NomBank annotates a wide variety of nouns that are not derived from verbs and do not denote events.", "labels": [], "entities": []}, {"text": "An example is given below of the partitive noun percent: In this case, the noun phrase headed by the predicate % (i.e., \"about 11% of Integra\") denotes a fractional part of the argument in position Arg1.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.8994308114051819}]}, {"text": "Since NomBank's release, a number of studies have applied verbal SRL techniques to the task of nominal SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8541936874389648}, {"text": "SRL", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.7429555654525757}]}, {"text": "For example,  As in (2), distribution in (4) has a noun phrase and multiple prepositional phrases in its environment, but not one of these constituents is an argument to distribution in (4); rather, any arguments are implicitly supplied by the surrounding discourse.", "labels": [], "entities": []}, {"text": "As described by, instances such as (2) are called \"markable\" because they contain overt arguments, and instances such as (4) are called \"unmarkable\" because they do not.", "labels": [], "entities": []}, {"text": "In the NomBank corpus, only markable instances have been annotated.", "labels": [], "entities": [{"text": "NomBank corpus", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.9230875968933105}]}, {"text": "Previous evaluations (e.g., those by and) have been based on markable instances, which constitute 57% of all instances of nominals from the NomBank lexicon.", "labels": [], "entities": [{"text": "NomBank lexicon", "start_pos": 140, "end_pos": 155, "type": "DATASET", "confidence": 0.9026959538459778}]}, {"text": "In order to use nominal SRL systems for downstream processing, it is important to develop and evaluate techniques that can handle markable as well as unmarkable nominal instances.", "labels": [], "entities": []}, {"text": "To address this issue, we investigate the role of implicit argumentation for nominal SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.7538825273513794}]}, {"text": "This is, in part, inspired by the recent CoNLL Shared Task (, which was the first evaluation of syntactic and semantic dependency parsing to include unmarkable nominals.", "labels": [], "entities": [{"text": "syntactic and semantic dependency parsing", "start_pos": 96, "end_pos": 137, "type": "TASK", "confidence": 0.6550401628017426}]}, {"text": "In this paper, we extend this task to constituent parsing with techniques and evaluations that focus specifically on implicit argumentation in nominals.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7707880735397339}]}, {"text": "We first present our NomBank SRL system, which improves the best reported argument F 1 score in the markable-only evaluation from 0.7283 to 0.7630 using a single-stage classification approach.", "labels": [], "entities": [{"text": "NomBank SRL", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.6497779488563538}, {"text": "reported argument F 1 score", "start_pos": 65, "end_pos": 92, "type": "METRIC", "confidence": 0.7237364888191223}]}, {"text": "We show that this system, when applied to all nominal instances, achieves an argument F 1 score of only 0.6895, a loss of more than 9%.", "labels": [], "entities": [{"text": "argument F 1 score", "start_pos": 77, "end_pos": 95, "type": "METRIC", "confidence": 0.7329968586564064}]}, {"text": "We then present a model of implicit argumentation that reduces this loss by 46%, resulting in an F 1 score of 0.7235 on the more complete evaluation task.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9926524758338928}]}, {"text": "In our analyses, we find that SRL performance varies widely among specific classes of nominals, suggesting interesting directions for future work.", "labels": [], "entities": [{"text": "SRL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.977764368057251}]}], "datasetContent": [{"text": "Our evaluation methodology reflects a practical scenario in which the nominal SRL system must process each token in a sentence.", "labels": [], "entities": [{"text": "SRL", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9278911352157593}]}, {"text": "The system cannot safely assume that each token bears overt arguments; rather, this decision must be made automatically.", "labels": [], "entities": []}, {"text": "In section 5.1, we present results for the automatic identification of nominals with overt arguments.", "labels": [], "entities": []}, {"text": "Then, in section 5.2, we present results for the combined task in which nominal classification is followed by argument identification.", "labels": [], "entities": [{"text": "nominal classification", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7317070960998535}, {"text": "argument identification", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7375738769769669}]}], "tableCaptions": [{"text": " Table 1: Markable-only NomBank SRL results for ar- gument prediction using automatically generated parse  trees. The f-measure statistics were calculated by ag- gregating predictions across all classes. \"-\" indicates  that the result was not reported.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of the markable-only and all- token evaluations of the baseline argument model.", "labels": [], "entities": []}]}