{"title": [{"text": "Improving the Arabic Pronunciation Dictionary for Phone and Word Recognition with Linguistically-Based Pronunciation Rules", "labels": [], "entities": [{"text": "Phone and Word Recognition", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.716415598988533}]}], "abstractContent": [{"text": "In this paper, we show that linguistically motivated pronunciation rules can improve phone and word recognition results for Modern Standard Arabic (MSA).", "labels": [], "entities": [{"text": "phone and word recognition", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.5859900563955307}, {"text": "Modern Standard Arabic (MSA)", "start_pos": 124, "end_pos": 152, "type": "DATASET", "confidence": 0.8341199358304342}]}, {"text": "Using these rules and the MADA morphological analysis and dis-ambiguation tool, multiple pronunciations per word are automatically generated to build two pronunciation dictionaries; one for training and another for decoding.", "labels": [], "entities": [{"text": "MADA morphological analysis", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6521060367425283}]}, {"text": "We demonstrate that the use of these rules can significantly improve both MSA phone recognition and MSA word recognition accuracies over a base-line system using pronunciation rules typically employed in previous work on MSA Automatic Speech Recognition (ASR).", "labels": [], "entities": [{"text": "MSA phone recognition", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.8913562695185343}, {"text": "MSA word recognition", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.8010063568751017}, {"text": "MSA Automatic Speech Recognition (ASR)", "start_pos": 221, "end_pos": 259, "type": "TASK", "confidence": 0.8169089215142387}]}, {"text": "We obtain a significant improvement in absolute accuracy in phone recognition of 3.77%-7.29% and a significant improvement of 4.1% in absolute accuracy in ASR.", "labels": [], "entities": [{"text": "absolute", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9296150207519531}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.8302215337753296}, {"text": "phone recognition", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8084330856800079}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.8397278189659119}, {"text": "ASR", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.9552564024925232}]}], "introductionContent": [{"text": "The correspondence between orthography and pronunciation in Modern Standard Arabic (MSA) falls somewhere between that of languages such as Spanish and Finnish, which have an almost one-to-one mapping between letters and sounds, and languages such as English and French, which exhibit a more complex letter-to-sound mapping).", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.4870704859495163}]}, {"text": "The more complex this mapping is, the more difficult the language is for Automatic Speech Recognition (ASR).", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 73, "end_pos": 107, "type": "TASK", "confidence": 0.7711221079031626}]}, {"text": "An essential component of an ASR system is its pronunciation dictionary (lexicon), which maps the orthographic representation of words to their phonetic or phonemic pronunciation variants.", "labels": [], "entities": [{"text": "ASR", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.990218460559845}]}, {"text": "For languages with complex letter-to-sound mappings, such dictionaries are typically written by hand.", "labels": [], "entities": []}, {"text": "However, for morphologically rich languages, such as MSA, 1 pronunciation dictionaries are difficult to create by hand, because of the large number of word forms, each of which has a large number of possible pronunciations.", "labels": [], "entities": [{"text": "MSA, 1 pronunciation dictionaries", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.5784984409809113}]}, {"text": "Fortunately, the relationship between orthography and pronunciation is relatively regular and well understood for MSA.", "labels": [], "entities": [{"text": "MSA", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.9008153676986694}]}, {"text": "Moreover, recent automatic techniques for morphological analysis and disambiguation (MADA) can also be useful in automating part of the dictionary creation process) Nonetheless, most documented Arabic ASR systems appear to handle only a subset of Arabic phonetic phenomena; very few use morphological disambiguation tools.", "labels": [], "entities": [{"text": "morphological analysis and disambiguation", "start_pos": 42, "end_pos": 83, "type": "TASK", "confidence": 0.7540848851203918}, {"text": "dictionary creation process", "start_pos": 136, "end_pos": 163, "type": "TASK", "confidence": 0.7656340996424357}]}, {"text": "In Section 2, we briefly describe related work, including the baseline system we use.", "labels": [], "entities": []}, {"text": "In Section 3, we outline the linguistic phenomena we believe are critical to improving MSA pronunciation dictionaries.", "labels": [], "entities": [{"text": "MSA pronunciation dictionaries", "start_pos": 87, "end_pos": 117, "type": "TASK", "confidence": 0.9353964924812317}]}, {"text": "In Section 4, we describe the pronunciation rules we have developed based upon these linguistic phenomena.", "labels": [], "entities": []}, {"text": "In Section 5, we describe how these rules are used, together with MADA, to build our pronunciation dictionaries for training and decoding automatically.", "labels": [], "entities": [{"text": "MADA", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.6946018934249878}]}, {"text": "In Section 6, we present results of our evaluations of our phone-and word-recognition systems (XPR and XWR) on MSA comparing these systems to two baseline systems, BASEPR and BASEWR.", "labels": [], "entities": [{"text": "MSA", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9414739012718201}, {"text": "BASEPR", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.833406388759613}, {"text": "BASEWR", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.48314231634140015}]}, {"text": "We conclude in Section 7 and identify directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To determine whether our pronunciation rules are useful in speech processing applications, we evaluated their impact on two tasks, automatic phone recognition and ASR.", "labels": [], "entities": [{"text": "automatic phone recognition", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.6009756922721863}, {"text": "ASR", "start_pos": 163, "end_pos": 166, "type": "TASK", "confidence": 0.9643521308898926}]}, {"text": "For our experiments, we used the broadcast news TDT4 corpus (Arabic Set 1), divided into 47.61 hours of speech (89 news shows) for training and 5.18 hours (11 shows); test and training shows were selected at random.", "labels": [], "entities": [{"text": "broadcast news TDT4 corpus (Arabic Set 1)", "start_pos": 33, "end_pos": 74, "type": "DATASET", "confidence": 0.7497638397746615}]}, {"text": "Both training and test data were segmented based on silence and non-speech segments and down-sampled to 8Khz.", "labels": [], "entities": []}, {"text": "This segmentation produced 20,707 speech segments for our training data and 2,255 segments for testing.", "labels": [], "entities": []}, {"text": "We hypothesize that improved pronunciation rules will have a profound impact on phone recognition accuracy.", "labels": [], "entities": [{"text": "phone recognition", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9026232659816742}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.8717670440673828}]}, {"text": "To compare our phone recognition (XPR) system with the baseline (BASEPR), we train two phone recognizers using HTK.", "labels": [], "entities": [{"text": "phone recognition (XPR)", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8055480897426606}, {"text": "BASEPR", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9910705089569092}, {"text": "HTK", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9243847131729126}]}, {"text": "The BASEPR recognizer uses the training-pronunciation dictionary generated using the baseline rules; the XPR system uses a pronunciation dictionary generated using these rules plus our modified and new rules (cf. Section 5).", "labels": [], "entities": [{"text": "BASEPR recognizer", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7504843175411224}]}, {"text": "The two systems are identical except for their pronunciation dictionaries.", "labels": [], "entities": []}, {"text": "We evaluate the two systems under two conditions: (1) phone recognition with a bigram phone language model (LM) 9 and (2) phone recognition with an open-loop phone recognizer, such that any phoneme can follow any other phoneme with a uniform distribution.", "labels": [], "entities": [{"text": "phone recognition", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.8265993893146515}, {"text": "phone recognition", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7943104803562164}]}, {"text": "Results of this evaluation are presented in.", "labels": [], "entities": []}, {"text": "Ideally, we would like to compare the performance of these systems against a common MSA phonetically-transcribed gold standard.", "labels": [], "entities": []}, {"text": "Unfortunately, to our knowledge, such a data set does not exist.", "labels": [], "entities": []}, {"text": "So we approximate such a gold standard on a blind test set through forced alignment, using the trained acoustic models and pronunciation Since our focus is a comparison of different approaches to pronunciation modeling on Arabic recognition tasks, we have not experimented with different features, parameters, and different machine learning approaches (such as discriminative training and/or the combination of both).", "labels": [], "entities": [{"text": "pronunciation modeling on Arabic recognition tasks", "start_pos": 196, "end_pos": 246, "type": "TASK", "confidence": 0.6957231412331263}]}, {"text": "The bigram phoneme LM of each phone recognizer is trained on the phonemes obtained from forced aligning the training transcript to the speech data using that recognizer's training pronunciation dictionary and acoustic models. dictionaries.", "labels": [], "entities": []}, {"text": "Since our choice of acoustic model (of BASEPR or XPR) and pronunciation dictionary (again of BASEPR or XPR) can bias our results, we consider four gold variants (GV) with different combinations of acoustic model and pronunciation dictionary, to set expected lower and upper bounds.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9103094339370728}, {"text": "BASEPR", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.8669261336326599}]}, {"text": "These combinations are represented in as GV1-4, where the source of acoustic models is BASEPR or XPR and source of pronunciation rules are BASEPR, XPR or XPR and BASEPR combined.", "labels": [], "entities": [{"text": "GV1-4", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9290050268173218}, {"text": "BASEPR", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9971606731414795}, {"text": "BASEPR", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9882686138153076}, {"text": "BASEPR", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9860290884971619}]}, {"text": "These GV are described in more detail below, as we describe our results.", "labels": [], "entities": []}, {"text": "Since BASEPR system uses a pronunciation dictionary with a one-to-one mapping of orthography to phones, the GV1 phone sequence for any test utterance's orthographical transcript according to BASEPR can be obtained directly from the orthographic transcript.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 191, "end_pos": 197, "type": "DATASET", "confidence": 0.7230867147445679}]}, {"text": "Note that if, in fact, GV1 does represent the true gold standard (i.e., the correct phone sequence for the test utterances) then if XPR obtains a lower phone error rate using this gold standard than BASEPR does, we can conclude that in fact XPR's acoustic models are better estimated.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 199, "end_pos": 205, "type": "METRIC", "confidence": 0.8795285224914551}]}, {"text": "This is in fact the case.", "labels": [], "entities": []}, {"text": "In, first line, we see that XPR under both conditions (open-loop and bigram LM) significantly (p-value < 2.2e \u2212 16) outperforms the corresponding BASEPR phone recognizer using GV1.", "labels": [], "entities": [{"text": "BASEPR phone recognizer", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.6583499908447266}, {"text": "GV1", "start_pos": 176, "end_pos": 179, "type": "DATASET", "confidence": 0.93171226978302}]}, {"text": "If GV1 does not accurately represent the phone sequences of the test data, then there must be some phones in the GV1 sequences that should be deleted, inserted, or substituted.", "labels": [], "entities": [{"text": "GV1", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.842799961566925}, {"text": "GV1 sequences", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9012318849563599}]}, {"text": "On the hypothesis that our training-pronunciation dictionary might improve the BASEPR assignments, we enrich the baseline pronunciation dictionary with XPR's dictionary.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.8014320731163025}]}, {"text": "Now, we force-align the orthographic transcript using this extended pronunciation dictionary, still using BASEPR's acoustic models, with the acoustic signal.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.5570359230041504}]}, {"text": "We denote the output phone sequences as GV2.", "labels": [], "entities": []}, {"text": "If a pronunciation generated using the BASEPR dictionary was already correct (in GV1) according to the acoustic signal, this forced alignment process still has the option of choosing it.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.8552525639533997}]}, {"text": "We hypothesize that the result, GV2, is a more accurate representation of the true phone sequences in the test data, since it should be able to model the acoustic signal more accurately.", "labels": [], "entities": []}, {"text": "On GV2, as on GV1, we see that XPR, under both conditions, significantly (p-: Comparing the effect of BASEPR and XPR pronunciation rules, alone and in combination, using 4 Gold Variants under two conditions (Open-loop and LM) value < 2.2e \u2212 16) outperforms the corresponding BASEPR phone recognizers (see, second line).", "labels": [], "entities": [{"text": "GV2", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.9353519082069397}, {"text": "BASEPR", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.7531664967536926}, {"text": "BASEPR phone recognizers", "start_pos": 275, "end_pos": 299, "type": "TASK", "confidence": 0.6425575812657675}]}, {"text": "We also compared the performance of the two systems using upper bound variants.", "labels": [], "entities": []}, {"text": "For GV3 we used the forced alignment of the orthographic transcription using only XPR's pronuncation dictionary with XPR's acoustic models.", "labels": [], "entities": [{"text": "GV3", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8488845825195312}]}, {"text": "In GV4 we combine the pronunciation dictionary of XPR with BASEPR dictionary and use XPR's acoustic models.", "labels": [], "entities": [{"text": "GV4", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.9386172294616699}, {"text": "BASEPR", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.990146815776825}]}, {"text": "Unsurprisingly, we find that the XPR recognizer significantly (p-value <2.2e \u2212 16) outperforms BASEPR when using these two variants under both conditions (see, third and fourth lines).", "labels": [], "entities": [{"text": "XPR recognizer", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7676557004451752}, {"text": "BASEPR", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.8919371366500854}]}, {"text": "The results presented in compare the robustness of the acoustic models as well as the pronunciation components of the two systems.", "labels": [], "entities": []}, {"text": "We also want to evaluate the accuracy of our pronunciation predictions in representing the actual acoustic signal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9993302822113037}]}, {"text": "One way to do this is to see how often the forced alignment process choose phone sequences using the BASEPR pronunciation dictionary as opposed to XPR's.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.8274587392807007}]}, {"text": "We forced aligned the test transcriptusing the XPR acoustic models and only the XPR pronunciation dictionary -with the acoustic signal.", "labels": [], "entities": []}, {"text": "We then compare the output sequences to the output of the forced alignment process where the combined pronunciations from BASEPR+XPR and the XPR acoustic models were used.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9237762689590454}]}, {"text": "We find that the difference between the two is only 1.03% (with 246,376 phones, 557 deletions, 1696 substitutions, and 277 insertions).", "labels": [], "entities": []}, {"text": "Thus, adding the BASEPR rules to XPR does not appear to contribute a great deal to the representation chosen by forced alignment.", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9169464707374573}]}, {"text": "Ina similar experiment, we use the BASEPR acoustic models instead of the XPR models and compare the results of using BASEPR-pronunciation dictionary with the combination of XPR+BASEPR's dictionaries for forced alignment.", "labels": [], "entities": []}, {"text": "Interestingly, in this experiment we do find a significantly larger difference between the two outputs 17.04% (with 233,787 phones, 1404 deletions, 14013 substitutions, and 27040 insertions).", "labels": [], "entities": []}, {"text": "We can hypothesize from these experiments that the baseline pronunciation dictionary alone is not sufficient to represent the acoustic signal accurately, since large numbers of phonemes are edited when adding the XPR pronunciations.", "labels": [], "entities": []}, {"text": "In contrast, adding the BASEPR's pronunciation dictionary to XPR's shows a relatively small percentage of edits, which suggests that the XPR pronunciation dictionary extends and covers more accurately the pronunciations already contained in the BASEPR dictionary.", "labels": [], "entities": [{"text": "BASEPR's pronunciation dictionary", "start_pos": 24, "end_pos": 57, "type": "DATASET", "confidence": 0.8009805977344513}, {"text": "BASEPR dictionary", "start_pos": 245, "end_pos": 262, "type": "DATASET", "confidence": 0.9173860251903534}]}, {"text": "We have also conducted an ASR experiment to evaluate the usefulness of our pronunciation rules for this application.", "labels": [], "entities": [{"text": "ASR", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9560556411743164}]}, {"text": "We employ the baseline pronunciation rules to generate the baseline training and decoding pronunciation dictionaries.", "labels": [], "entities": []}, {"text": "Using these dictionaries, we build the baseline ASR system (BASEWR).", "labels": [], "entities": [{"text": "ASR", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.7850996255874634}, {"text": "BASEWR", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9707726836204529}]}, {"text": "Using our extended pronunciation rules, we generate our dictionaries and train our ASR system (XWR).", "labels": [], "entities": []}, {"text": "Both systems have the same model settings, as described in Section 6.1.", "labels": [], "entities": []}, {"text": "They also share the same language model (LM), a trigram LM trained on the undiacritized transcripts of the training data and a subset of Arabic gigawords (approximately 281 million words, in total), using the SRILM toolkit.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 209, "end_pos": 222, "type": "DATASET", "confidence": 0.8777989447116852}]}, {"text": "presents the comparison of BASEWR with the XWR system.", "labels": [], "entities": [{"text": "BASEWR", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.8518595695495605}]}, {"text": "In Section 5.1, we noted that the top two choices from MADA maybe included in the XWR pronunciation dictionary when the difference in MADA confidence scores for these two is less than a given threshold.", "labels": [], "entities": [{"text": "XWR pronunciation dictionary", "start_pos": 82, "end_pos": 110, "type": "DATASET", "confidence": 0.7510958512624105}, {"text": "MADA confidence scores", "start_pos": 134, "end_pos": 156, "type": "METRIC", "confidence": 0.8034680485725403}]}, {"text": "So we analyze the impact of including this second MADA option in both the training and decoding dictionaries on ASR results.", "labels": [], "entities": [{"text": "ASR", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9856857061386108}]}, {"text": "In all cases, whether the second MADA choice is included or not, XWR significantly (p-values < 8.1e-15) outperforms BASEWR.", "labels": [], "entities": [{"text": "XWR", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.7997280955314636}, {"text": "BASEWR", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9593347311019897}]}, {"text": "Our best results are obtained when we include the top first and second MADA option in the decoding pronunciation dictionary but only the top MADA choice in the training pronunciation dictionary.", "labels": [], "entities": []}, {"text": "The difference between this version of XWR and an XWR version which includes the top second MADA choice in the training dictionary is significant (p-value = 0.017).", "labels": [], "entities": []}, {"text": "To evaluate the impact of the set of rules that generate additional pronunciation variants (described in Section 4 -IV) on word recognition, we built a system, denoted as XWR_I-III, that uses only the first three sets of rules (I-III) and compared its performance to that of both BASEWR and the corresponding XWR system.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.8341801464557648}, {"text": "BASEWR", "start_pos": 280, "end_pos": 286, "type": "METRIC", "confidence": 0.6810882091522217}]}, {"text": "As shown in, we observe that XWR_I-III significantly outperforms BASEWR in 2.27 (p-value < 2.2e-16).", "labels": [], "entities": [{"text": "BASEWR", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.988663375377655}]}, {"text": "Also, the corresponding XWR that uses all the rules (including IV set) significantly outperforms XWR_I-III in 1.24 (p-value < 2.2e-16).", "labels": [], "entities": []}, {"text": "The undiacritized vocabulary size used in our experiment was 34,511.", "labels": [], "entities": []}, {"text": "We observe that 6.38% of the words in the test data were out of vocabulary (OOV), which may partly explain our low absolute recognition accuracy.", "labels": [], "entities": [{"text": "vocabulary (OOV)", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.6733129993081093}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.7325490117073059}]}, {"text": "The dictionary size statistics (for entries generated from the training data only) used in these experiments are shown in.", "labels": [], "entities": []}, {"text": "We have done some error analysis to understand the reason behind high absolute error rate for both systems.", "labels": [], "entities": [{"text": "absolute error rate", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.7969080408414205}]}, {"text": "We observe that many of the test utterances are very noisy.", "labels": [], "entities": []}, {"text": "We wanted to see whether XWR still outperforms BASEWR if we remove these utterances.", "labels": [], "entities": [{"text": "XWR", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6111817359924316}, {"text": "BASEWR", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9934472441673279}]}, {"text": "Removing all utterances for which BASEWR obtains an accuracy of less than 25%, we are left with 1720/2255 utterances.", "labels": [], "entities": [{"text": "BASEWR", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.6877452731132507}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9992419481277466}, {"text": "1720/2255 utterances", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.9045932739973068}]}, {"text": "On these remaining utterances, the BASEWR accuracy is 64.4% and XWR's accuracy is 67.23% -a significant difference despite the bias in favor of BASEWR.", "labels": [], "entities": [{"text": "BASEWR", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9988910555839539}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8815439343452454}, {"text": "XWR's", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.8793996572494507}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.6732782125473022}, {"text": "BASEWR", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.901188850402832}]}], "tableCaptions": [{"text": " Table 1: Comparing the effect of BASEPR and XPR pronunciation rules, alone and in combination, using 4 Gold  Variants under two conditions (Open-loop and LM)", "labels": [], "entities": [{"text": "BASEPR", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9711109399795532}]}, {"text": " Table 2: Comparing the performance of BASEWR to  XWR, where the top 1 or 2 MADA options are included  in the training dictionary (TD) and decoding dictionary  (DD). XWR I-III uses only the first three sets of pro- nunciation rules in Section 4. Accuracy = (100 -WER);  Correct is Accuracy without counting insertions (%). To- tal number of words is 36,538.", "labels": [], "entities": [{"text": "BASEWR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.6483823657035828}, {"text": "Accuracy", "start_pos": 246, "end_pos": 254, "type": "METRIC", "confidence": 0.9988566637039185}, {"text": "Accuracy", "start_pos": 281, "end_pos": 289, "type": "METRIC", "confidence": 0.9846937656402588}]}, {"text": " Table 3: Dictionary sizes generated fom the training data  only (PPW: pronunciations per word, TD: Training pro- nunciation dictionary, DD: Decoding pronunciation dic- tionary).", "labels": [], "entities": []}]}