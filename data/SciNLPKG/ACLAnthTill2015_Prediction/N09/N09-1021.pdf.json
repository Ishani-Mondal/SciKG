{"title": [{"text": "Phrase-Based Query Degradation Modeling for Vocabulary-Independent Ranked Utterance Retrieval", "labels": [], "entities": [{"text": "Phrase-Based Query Degradation Modeling", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8068452775478363}, {"text": "Vocabulary-Independent Ranked Utterance Retrieval", "start_pos": 44, "end_pos": 93, "type": "TASK", "confidence": 0.5881345570087433}]}], "abstractContent": [{"text": "This paper introduces anew approach to ranking speech utterances by a system's confidence that they contain a spoken word.", "labels": [], "entities": [{"text": "ranking speech utterances", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.7965493003527323}]}, {"text": "Multiple alternate pronunciations, or degradations, of a query word's phoneme sequence are hypothesized and incorporated into the ranking function.", "labels": [], "entities": []}, {"text": "We consider two methods for hypothesizing these degradations, the best of which is constructed using factored phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "factored phrase-based statistical machine translation", "start_pos": 101, "end_pos": 154, "type": "TASK", "confidence": 0.6375302672386169}]}, {"text": "We show that this approach is able to significantly improve upon a state-of-the-art baseline technique in an evaluation on held-out speech.", "labels": [], "entities": []}, {"text": "We evaluate our systems using three different methods for indexing the speech utterances (using phoneme, phoneme multigram, and word recognition), and find that degradation modeling shows particular promise for locating out-of-vocabulary words when the underlying indexing system is constructed with standard word-based speech recognition.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.7193804830312729}]}], "introductionContent": [{"text": "Our goal is to find short speech utterances which contain a query word.", "labels": [], "entities": []}, {"text": "We accomplish this goal by ranking the set of utterances by our confidence that they contain the query word, a task known as Ranked Utterance Retrieval (RUR).", "labels": [], "entities": [{"text": "Ranked Utterance Retrieval (RUR)", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.6111628462870916}]}, {"text": "In particular, we are interested in the case when the user's query word cannot be anticipated by a Large Vocabulary Continuous Speech Recognizer's (LVCSR) decoding dictionary, so that the word is said to be Out-OfVocabulary (OOV).", "labels": [], "entities": []}, {"text": "Rare words tend to be the most informative, but are also most likely to be OOV.", "labels": [], "entities": [{"text": "OOV", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.7747676968574524}]}, {"text": "When words are OOV, we must use vocabulary-independent techniques to locate them.", "labels": [], "entities": []}, {"text": "One popular approach is to search for the words in output from a phoneme recognizer), although this suffers from the low accuracy typical of phoneme recognition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9970216155052185}, {"text": "phoneme recognition", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7474624216556549}]}, {"text": "We consider two methods for handling this inaccuracy.", "labels": [], "entities": []}, {"text": "First, we compare an RUR indexing system using phonemes with two systems using longer recognition units: words or phoneme multigrams.", "labels": [], "entities": [{"text": "RUR indexing", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.8491488099098206}]}, {"text": "Second, we consider several methods for handling the recognition inaccuracy in the utterance ranking function itself.", "labels": [], "entities": []}, {"text": "Our baseline generative model handles errorful recognition by estimating term frequencies from smoothed language models trained on phoneme lattices.", "labels": [], "entities": [{"text": "errorful recognition", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.6454216837882996}]}, {"text": "Our new approach, which we call query degradation, hypothesizes many alternate \"pronunciations\" for the query word and incorporates them into the ranking function.", "labels": [], "entities": [{"text": "query degradation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7877819836139679}]}, {"text": "These degradations are translations of the lexical phoneme sequence into the errorful recognition language, which we hypothesize using a factored phrase-based statistical machine translation system.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 146, "end_pos": 190, "type": "TASK", "confidence": 0.6692043244838715}]}, {"text": "Our speech collection is a set of oral history interviews from the MALACH collection (), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts () and for vocabulary-independent RUR).", "labels": [], "entities": [{"text": "MALACH collection", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.8231358826160431}, {"text": "ad hoc speech retrieval evaluations", "start_pos": 124, "end_pos": 159, "type": "TASK", "confidence": 0.7586520910263062}, {"text": "RUR", "start_pos": 232, "end_pos": 235, "type": "TASK", "confidence": 0.46042224764823914}]}, {"text": "The interviews were conducted with survivors and witnesses of the Holocaust, who discuss their experiences before, during, and after the Second World War.", "labels": [], "entities": []}, {"text": "Their speech is predominately spontaneous and conversational.", "labels": [], "entities": []}, {"text": "It is often also emotional and heavily accented.", "labels": [], "entities": []}, {"text": "Because the speech contains many words unlikely to occur within a general purpose speech recognition lexicon, it repre-sents an excellent collection for RUR evaluation.", "labels": [], "entities": [{"text": "general purpose speech recognition lexicon", "start_pos": 66, "end_pos": 108, "type": "TASK", "confidence": 0.6653954446315765}, {"text": "RUR evaluation", "start_pos": 153, "end_pos": 167, "type": "TASK", "confidence": 0.8825850188732147}]}, {"text": "We were graciously permitted to use BBN Technology's speech recognition system Byblos () for our speech recognition experiments.", "labels": [], "entities": [{"text": "BBN Technology's speech recognition system Byblos", "start_pos": 36, "end_pos": 85, "type": "DATASET", "confidence": 0.7380264231136867}, {"text": "speech recognition", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7921336591243744}]}, {"text": "We train on approximately 200 hours of transcribed audio excerpted from about 800 unique speakers in the MALACH collection.", "labels": [], "entities": [{"text": "MALACH collection", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.943101704120636}]}, {"text": "To provide a realistic set of OOV query words, we use an LVCSR dictionary previously constructed fora different topic domain (broadcast news and conversational telephone speech) and discard all utterances in our acoustic training data which are not covered by this dictionary.", "labels": [], "entities": [{"text": "LVCSR dictionary", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.86001056432724}]}, {"text": "New acoustic and language models are trained for each of the phoneme, multigram and word recognition systems.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7400643229484558}]}, {"text": "The output of LVCSR is a lattice of recognition hypotheses for each test speech utterance.", "labels": [], "entities": []}, {"text": "A lattice is a directed acyclic graph that is used to compactly represent the search space fora speech recognition system.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7367222011089325}]}, {"text": "Each node represents a point in time and arcs between nodes indicates a word occurs between the connected nodes' times.", "labels": [], "entities": []}, {"text": "Arcs are weighted by the probability of the word occurring, so that the so-called \"one-best\" path through the lattice (what a system might return as a transcription) is the path through the lattice having highest probability under the acoustic and language models.", "labels": [], "entities": []}, {"text": "Each RUR model we consider is constructed using the expected counts of a query word's phoneme sequences in these recognition lattices.", "labels": [], "entities": []}, {"text": "We consider three approaches to producing these phoneme lattices, using standard word-based LVCSR, phoneme recognition, and LVCSR using phoneme multigrams.", "labels": [], "entities": [{"text": "phoneme recognition", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7780413329601288}]}, {"text": "Our word system's dictionary contains about 50,000 entries, while the phoneme system contains 39 phonemes from the ARPABET set.", "labels": [], "entities": [{"text": "ARPABET set", "start_pos": 115, "end_pos": 126, "type": "DATASET", "confidence": 0.9125553965568542}]}, {"text": "Originally proposed by to model variable length regularities in streams of symbols (e.g., words, graphemes, or phonemes), phoneme multigrams are short sequences of one or more phonemes.", "labels": [], "entities": []}, {"text": "We produce a set of \"phoneme transcripts\" by replacing transcript words with their lexical pronunciation.", "labels": [], "entities": []}, {"text": "The set of multigrams is learned by then choosing a maximumlikelihood segmentation of these training phoneme transcripts, where the segmentation is viewed as hidden data in an Expectation-Maximization algorithm.", "labels": [], "entities": []}, {"text": "The set of all continuous phonemes occurring between segment boundaries is then chosen as our multigram dictionary.", "labels": [], "entities": []}, {"text": "This multigram recognition dictionary contains 16,409 entries.", "labels": [], "entities": [{"text": "multigram recognition", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.8332348763942719}]}, {"text": "After we have obtained each recognition lattice, our indexing approach follows that of.", "labels": [], "entities": []}, {"text": "Namely, for the word and multigram systems, we first expand lattice arcs containing multiple phones to produce a lattice having only single phonemes on its arcs.", "labels": [], "entities": []}, {"text": "Then, we compute the expected count of all phoneme n-grams n \u2264 5 in the lattice.", "labels": [], "entities": []}, {"text": "These n-grams and their counts are inserted in our inverted index for retrieval.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce our baseline RUR methods.", "labels": [], "entities": [{"text": "RUR", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.8110343217849731}]}, {"text": "In Section 3 we introduce our query degradation approach.", "labels": [], "entities": [{"text": "query degradation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7836592197418213}]}, {"text": "We introduce our experimental validation in Section 4 and our results in Section 5.", "labels": [], "entities": []}, {"text": "We find that using phrase-based query degradations can significantly improve upon a strong RUR baseline.", "labels": [], "entities": [{"text": "RUR", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.6717813014984131}]}, {"text": "Finally, in Section 6 we conclude and outline several directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "An appropriate and commonly used measure for RUR is Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "RUR", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.7009639143943787}, {"text": "Mean Average Precision (MAP)", "start_pos": 52, "end_pos": 80, "type": "METRIC", "confidence": 0.9603993892669678}]}, {"text": "Given a ranked list of utterances being searched through, we define the precision at position i in the list as the proportion of the topi utterances which actually contain the corresponding query word.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9967273473739624}]}, {"text": "Average Precision (AP) is the average of the precision values computed for each position containing a relevant utterance.", "labels": [], "entities": [{"text": "Average Precision (AP)", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.9352704763412476}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9981980919837952}]}, {"text": "To assess the effectiveness of a system across multiple queries, Mean Average Precision is defined as the arithmetic mean of per-query average precision, MAP = 1 n n AP n . Throughout this paper, when we report statistically significant improvements in MAP, we are comparing AP for paired queries using a Wilcoxon signed rank test at \u03b1 = 0.05.", "labels": [], "entities": [{"text": "Mean Average Precision", "start_pos": 65, "end_pos": 87, "type": "METRIC", "confidence": 0.9691673318545023}, {"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.7587465047836304}, {"text": "MAP", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9628846645355225}, {"text": "AP", "start_pos": 275, "end_pos": 277, "type": "METRIC", "confidence": 0.9796985387802124}]}, {"text": "Note, RUR is different than spoken term detection in two ways, and thus warrants an evaluation measure (e.g., MAP) different than standard spoken term detection measures (such as NIST's actual term weighted value).", "labels": [], "entities": [{"text": "RUR", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9605748653411865}, {"text": "spoken term detection", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.6172724862893423}, {"text": "MAP", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9507237076759338}, {"text": "spoken term detection", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.6675015290578207}]}, {"text": "First, STD measures require locating a term with granularity finer than that of an utterance.", "labels": [], "entities": [{"text": "STD", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9859352111816406}]}, {"text": "Second, STD measures are computed using a fixed detection threshold.", "labels": [], "entities": [{"text": "STD", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9404419660568237}]}, {"text": "This latter requirement will be unnecessary in many applications (e.g., where a user might prefer to decide themselves when to stop reading down the ranked list of retrieved utterances) and unlikely to be helpful for downstream evidence combination (where we may prefer to keep all putative hits and weight them by some measure of confidence).", "labels": [], "entities": []}, {"text": "For our evaluation, we consider retrieving short utterances from seventeen fully transcribed MALACH interviews.", "labels": [], "entities": []}, {"text": "Our query set contains all single words occurring in these interviews that are OOV with respect to the word dictionary.", "labels": [], "entities": [{"text": "OOV", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9781298637390137}]}, {"text": "This gives us a total of 261 query terms for evaluation.", "labels": [], "entities": []}, {"text": "Note, query words are also not present in the multigram training transcripts, in any language model training data, or in any transcripts used for degradation modeling.", "labels": [], "entities": []}, {"text": "Some example query words include BUCHENWALD, KINDERTRANSPORT, and SONDERKOMMANDO.", "labels": [], "entities": [{"text": "BUCHENWALD", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9765601754188538}, {"text": "KINDERTRANSPORT", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.8719622492790222}]}, {"text": "To train our degradation models, we used a held outset of 22,810 manually transcribed utterances.", "labels": [], "entities": []}, {"text": "We run each recognition system (phoneme, multigram, and word) on these utterances and, for each, train separate degradation models using the aligned reference and hypothesis transcripts.", "labels": [], "entities": []}, {"text": "For CMQD, we computed 100 random traversals on each lattice, giving us a total of 2,281,000 hypothesis and reference pairs to align for our confusion matrices.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: PER and MAP results for baseline and degradation models. The best result for each indexing approach is  shown in bold.", "labels": [], "entities": [{"text": "PER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9841606020927429}, {"text": "MAP", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9333366751670837}]}]}