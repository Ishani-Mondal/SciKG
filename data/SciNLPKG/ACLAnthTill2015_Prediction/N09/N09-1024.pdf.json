{"title": [{"text": "Unsupervised Morphological Segmentation with Log-Linear Models", "labels": [], "entities": [{"text": "Morphological Segmentation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.8300431668758392}]}], "abstractContent": [{"text": "Morphological segmentation breaks words into morphemes (the basic semantic units).", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8966325223445892}]}, {"text": "It is a key component for natural language processing systems.", "labels": [], "entities": []}, {"text": "Unsupervised morphological segmentation is attractive, because in every language there are virtually unlimited supplies of text, but very few labeled resources.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.7689246237277985}]}, {"text": "However, most existing model-based systems for unsupervised morphological segmentation use directed generative models, making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning.", "labels": [], "entities": []}, {"text": "In this paper, we present the first log-linear model for unsupervised morphological seg-mentation.", "labels": [], "entities": []}, {"text": "Our model uses overlapping features such as morphemes and their contexts, and incorporates exponential priors inspired by the minimum description length (MDL) principle.", "labels": [], "entities": []}, {"text": "We present efficient algorithms for learning and inference by combining con-trastive estimation with sampling.", "labels": [], "entities": []}, {"text": "Our system , based on monolingual features only, out-performs a state-of-the-art system by a large margin, even when the latter uses bilingual information such as phrasal alignment and pho-netic correspondence.", "labels": [], "entities": []}, {"text": "On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor.", "labels": [], "entities": [{"text": "Arabic Penn Treebank", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9351982871691386}, {"text": "F1 error", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.8606953918933868}, {"text": "Morfessor", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.9376384615898132}]}], "introductionContent": [{"text": "The goal of morphological segmentation is to segment words into morphemes, the basic syntactic/semantic units.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7298367023468018}]}, {"text": "This is a key subtask in many * This research was conducted during the author's internship at Microsoft Research.", "labels": [], "entities": []}, {"text": "NLP applications, including machine translation, speech recognition and question answering.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8176516890525818}, {"text": "speech recognition", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7836746275424957}, {"text": "question answering", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8878463208675385}]}, {"text": "Past approaches include rule-based morphological analyzers) and supervised learning).", "labels": [], "entities": []}, {"text": "While successful, these require deep language expertise and along and laborious process in system building or labeling.", "labels": [], "entities": []}, {"text": "Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied fora number of languages (.", "labels": [], "entities": []}, {"text": "The lack of supervised labels makes it even more important to leverage rich features and global dependencies.", "labels": [], "entities": []}, {"text": "However, existing systems use directed generative models), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation.", "labels": [], "entities": []}, {"text": "In this paper, we present the first log-linear model for unsupervised morphological segmentation.", "labels": [], "entities": [{"text": "unsupervised morphological segmentation", "start_pos": 57, "end_pos": 96, "type": "TASK", "confidence": 0.633596787850062}]}, {"text": "Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their contexts (e.g., in Arabic, the string Al is likely a morpheme, as is any string between Al and a word boundary).", "labels": [], "entities": []}, {"text": "We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation () and sampling.", "labels": [], "entities": []}, {"text": "We focus on inflectional morphology and test our approach on datasets in Arabic and Hebrew.", "labels": [], "entities": []}, {"text": "Our system, using monolingual features only, outperforms by a large margin, even when their system uses bilingual information such as phrasal alignment and phonetic correspondence.", "labels": [], "entities": []}, {"text": "On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (.", "labels": [], "entities": [{"text": "Arabic Penn Treebank", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9326555132865906}, {"text": "F1 error", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9072860777378082}]}, {"text": "Our system can be readily applied to supervised and semi-supervised learning.", "labels": [], "entities": []}, {"text": "Using a fraction of the labeled data, it already outperforms Snyder & Barzilay's supervised results (2008a), which further demonstrates the benefit of using a log-linear model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our system on two datasets.", "labels": [], "entities": []}, {"text": "Our main evaluation is on a multi-lingual dataset constructed by.", "labels": [], "entities": []}, {"text": "It consists of 6192 short parallel phrases in Hebrew, Arabic, Aramaic (a dialect of Arabic), and English.", "labels": [], "entities": []}, {"text": "The parallel phrases were extracted from the Hebrew Bible and its translations via word alignment and postprocessing.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.7051254212856293}]}, {"text": "For Arabic, the gold segmentation was obtained using a highly accurate Arabic morphological analyzer); for Hebrew, from a Bible edition distributed by Westminster Hebrew Institute ().", "labels": [], "entities": [{"text": "Westminster Hebrew Institute", "start_pos": 151, "end_pos": 179, "type": "DATASET", "confidence": 0.9365837574005127}]}, {"text": "There is no gold segmentation for English and Aramaic.", "labels": [], "entities": []}, {"text": "Like Snyder & Barzilay, we evaluate on the Arabic and Hebrew portions only; unlike their approach, our system does not use any bilingual information.", "labels": [], "entities": []}, {"text": "We refer to this dataset as S&B . We also report our results on the Arabic Penn Treebank (ATB), which provides gold segmentations for an Arabic corpus with about 120,000 Arabic words.", "labels": [], "entities": [{"text": "Arabic Penn Treebank (ATB)", "start_pos": 68, "end_pos": 94, "type": "DATASET", "confidence": 0.8552138606707255}]}, {"text": "As in previous work, we report recall, precision, and F1 over segmentation points.", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9995119571685791}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9995368719100952}, {"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9997187256813049}]}, {"text": "We used 500 phrases from the S&B dataset for feature development, and also tuned our model hyperparameters there.", "labels": [], "entities": [{"text": "S&B dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.7240573987364769}, {"text": "feature development", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7920787930488586}]}, {"text": "The weights for the lexicon and corpus priors were set to \u03b1 = \u22121, \u03b2 = \u221220.", "labels": [], "entities": []}, {"text": "The feature weights were initialized to zero and were penalized by a Gaussian prior with \u03c3 2 = 100.", "labels": [], "entities": []}, {"text": "The learning rate was set to 0.02 for all experiments, except the full Arabic Penn Treebank, for which it was set to 0.005.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9559309184551239}, {"text": "Arabic Penn Treebank", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9541696310043335}]}, {"text": "We used 30 iterations for learning.", "labels": [], "entities": []}, {"text": "In each iteration, 200 samples were collected to compute each of the two expected counts.", "labels": [], "entities": []}, {"text": "The sampler was initialized by running annealing for 2000 samples, with the temperature dropping from 10 to 0.1 at 0.1 decrements.", "labels": [], "entities": []}, {"text": "The most probable segmentation was obtained by running annealing for 10000 samples, using the same temperature schedule.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9797724485397339}]}, {"text": "We restricted the segmentation candidates to those with no greater than five segments in all experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of segmentation results on the S&B  dataset.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8447489738464355}, {"text": "S&B  dataset", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.7033918499946594}]}, {"text": " Table 2: Ablation test results on the S&B dataset.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9934458136558533}, {"text": "S&B dataset", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.7053481712937355}]}, {"text": " Table 3: Comparison of segmentation results with super- vised and semi-supervised learning on the S&B dataset.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.9361834526062012}, {"text": "S&B dataset", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.644041009247303}]}, {"text": " Table 4: Comparison of segmentation results on the Ara- bic Penn Treebank.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.886625349521637}, {"text": "Ara- bic Penn Treebank", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.91681067943573}]}]}