{"title": [{"text": "Tied-Mixture Language Modeling in Continuous Space", "labels": [], "entities": [{"text": "Tied-Mixture Language Modeling", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7724632620811462}]}], "abstractContent": [{"text": "This paper presents anew perspective to the language modeling problem by moving the word representations and modeling into the continuous space.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7232394069433212}]}, {"text": "Ina previous work we introduced Gaussian-Mixture Language Model (GMLM) and presented some initial experiments.", "labels": [], "entities": []}, {"text": "Here, we propose Tied-Mixture Language Model (TMLM), which does not have the model parameter estimation problems that GMLM has.", "labels": [], "entities": []}, {"text": "TMLM provides a great deal of parameter tying across words, hence achieves robust parameter estimation.", "labels": [], "entities": [{"text": "TMLM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8052457571029663}]}, {"text": "As such, TMLM can estimate the probability of any word that has as few as two occurrences in the training data.", "labels": [], "entities": []}, {"text": "The speech recognition experiments with the TMLM show improvement over the word trigram model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7478096187114716}]}], "introductionContent": [{"text": "Despite numerous studies demonstrating the serious short-comings of the n-gram language models, it has been surprisingly difficult to outperform n-gram language models consistently across different domains, tasks and languages.", "labels": [], "entities": []}, {"text": "It is well-known that ngram language models are not effective in modeling long range lexical, syntactic and semantic dependencies.", "labels": [], "entities": []}, {"text": "Nevertheless, n-gram models have been very appealing due to their simplicity; they require only a plain corpus of data to train the model.", "labels": [], "entities": []}, {"text": "The improvements obtained by some more elaborate language models) come from the explicit use of syntactic and semantic knowledge put into the annotated corpus.", "labels": [], "entities": []}, {"text": "In addition to the mentioned problems above, traditional n-gram language models do not lend themselves easily to rapid and effective adaptation and discriminative training.", "labels": [], "entities": []}, {"text": "A typical n-gram model contains millions of parameters and has no structure capturing dependencies and relationships between the words beyond a limited local context.", "labels": [], "entities": []}, {"text": "These parameters are estimated from the empirical distributions, and suffer from data sparseness.", "labels": [], "entities": []}, {"text": "n-gram language model adaptation (to new domain, speaker, genre and language) is difficult, simply because of the large number of parameters, for which large amount of adaptation data is required.", "labels": [], "entities": [{"text": "n-gram language model adaptation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5624871850013733}]}, {"text": "Instead of updating model parameters with an adaptation method, the typical practice is to collect some data in the target domain and build a domain specific language model.", "labels": [], "entities": []}, {"text": "The domain specific language model is interpolated with a generic language model trained on a larger domain independent data to achieve robustness.", "labels": [], "entities": []}, {"text": "On the other hand, rapid adaptation for acoustic modeling, using such methods as Maximum Likelihood Linear Regression (MLLR), is possible using very small amount of acoustic data, thanks to the inherent structure of acoustic models that allow large degrees of parameter tying across different words (several thousand context dependent states are shared by all the words in the dictionary).", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7740608751773834}, {"text": "Maximum Likelihood Linear Regression (MLLR)", "start_pos": 81, "end_pos": 124, "type": "METRIC", "confidence": 0.7662074736186436}]}, {"text": "Likewise, even though discriminatively trained acoustic models have been widely used, discriminatively trained languages models) have not widely accepted as a standard practice yet.", "labels": [], "entities": []}, {"text": "In this study, we present anew perspective to the language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7226649522781372}]}, {"text": "In this perspective, words are not treated as discrete entities but rather vectors of real numbers.", "labels": [], "entities": []}, {"text": "As a result, long-term semantic relationships between the words could be quantified and can be integrated into a model.", "labels": [], "entities": []}, {"text": "The proposed formulation casts the language modeling problem as an acoustic modeling problem in speech recognition.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7194015681743622}, {"text": "speech recognition", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7204145193099976}]}, {"text": "This approach opens up new possibilities from rapid and effective adaptation of language models to using discriminative acoustic modeling tools and methods, such as Minimum Phone Error (MPE)) training to train discriminative language models.", "labels": [], "entities": []}, {"text": "We introduced the idea of language modeling in continuous space from the acoustic modeling perspective and proposed Gaussian Mixture Language Model (GMLM) (.", "labels": [], "entities": []}, {"text": "However, GMLM has model parameter estimation problems.", "labels": [], "entities": [{"text": "GMLM", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.7903615832328796}]}, {"text": "In GMLM each word is represented by a specific set of Gaussian mixtures.", "labels": [], "entities": []}, {"text": "Robust parameter estimation of the Gaussian mixtures requires hundreds or even thousands of examples.", "labels": [], "entities": []}, {"text": "As a result, we were able to estimate the GMLM probabilities only for words that have at least 50 or more examples.", "labels": [], "entities": []}, {"text": "Essentially, this was meant to estimate the GMLM probabilities for only about top 10% of the words in the vocabulary.", "labels": [], "entities": []}, {"text": "Not surprisingly, we have not observed improvements in speech recognition accuracy.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7965438067913055}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9115227460861206}]}, {"text": "Tied-Mixture Language Model (TMLM) does not have these requirements in model estimation.", "labels": [], "entities": []}, {"text": "In fact, language model probabilities can be estimated for words having as few as two occurrences in the training data.", "labels": [], "entities": []}, {"text": "The concept of language modeling in continuous space was previously proposed) using Neural Networks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7269236445426941}]}, {"text": "However, our method offers several potential advantages over including adaptation, and modeling of semantic dependencies because of the way we represent the words in the continuous space.", "labels": [], "entities": []}, {"text": "Moreover, our method also allows efficient model training using large amounts of training data, thanks to the acoustic modeling tools and methods which are optimized to handle large amounts of data efficiently.", "labels": [], "entities": []}, {"text": "It is important to note that we have to realize the full potential of the proposed model, before investigating the potential benefits such as adaptation and discriminative training.", "labels": [], "entities": []}, {"text": "To this end, we propose TMLM, which does not have the problems GMLM has and, unlike GMLM we report improvements in speech recognition over the corresponding n-gram models.", "labels": [], "entities": [{"text": "TMLM", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.5263910889625549}, {"text": "GMLM", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8726603388786316}, {"text": "GMLM", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8983258605003357}, {"text": "speech recognition", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.6922903209924698}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the concept of language modeling in continuous space.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7280917167663574}]}, {"text": "Section 3 describes the tiedmixture modeling.", "labels": [], "entities": [{"text": "tiedmixture modeling", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.9067330360412598}]}, {"text": "Speech recognition architecture is summarized in Section 4, followed by the experimental results in Section 5.", "labels": [], "entities": [{"text": "Speech recognition architecture", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8370701471964518}]}, {"text": "Section 6 discusses various issues with the proposed method and finally, Section 7 summarizes our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the following TMLM parameters to build the model.", "labels": [], "entities": []}, {"text": "The SVD projection size is set to 200 (i.e. R = 200) for each bigram history.", "labels": [], "entities": []}, {"text": "This results into a trigram history vector of size 400.", "labels": [], "entities": []}, {"text": "This vector is further projected down to a 50 dimensional feature space using LDA transform.", "labels": [], "entities": []}, {"text": "The total number of Gaussian densities used for the TM-HMM is set to 1024.", "labels": [], "entities": []}, {"text": "In order to find the overall relationship between trigram and TMLM probabilities we show the scatter plot of the trigram and TMMT probabilities in.", "labels": [], "entities": []}, {"text": "While calculating the TMLM score the TMLM likelihood generated by the model is divided by 40 to balance its dynamic range with that of the n-gram model.", "labels": [], "entities": [{"text": "TMLM likelihood", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.8546155095100403}]}, {"text": "Most of the probabilities lie along the diagonal line.", "labels": [], "entities": []}, {"text": "However, some trigram probabilities are modulated making TMLM probabilities quite different than the corresponding trigram probabilities.", "labels": [], "entities": []}, {"text": "Analysis of TMLM probabilities with respect to the trigram probabilities would bean interesting future research.", "labels": [], "entities": []}, {"text": "We conducted the speech recognition language modeling experiments on 3 testsets: TestA, TestB and TestC.", "labels": [], "entities": [{"text": "speech recognition language modeling", "start_pos": 17, "end_pos": 53, "type": "TASK", "confidence": 0.8488687425851822}]}, {"text": "All three test sets are from July'07 official evaluations of the IBM's speech-to-speech translation system by DARPA.", "labels": [], "entities": [{"text": "IBM's speech-to-speech translation", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.5321837291121483}]}, {"text": "TestA consists of sentences spoken out in the field to the IBM's S2S system during live evaluation.", "labels": [], "entities": []}, {"text": "TestB contains sentences spoken in an office environment to the live S2S system.", "labels": [], "entities": []}, {"text": "Using on-the-spot speakers for TestA and TestB meant to have shorter and clean sentences.", "labels": [], "entities": [{"text": "TestA", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9166449308395386}, {"text": "TestB", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9324129819869995}]}, {"text": "Finally TestC contains pre-recorded sentences with much more hesitations and more casual conversations compared to the other two testsets.", "labels": [], "entities": []}, {"text": "TestA, TestB and TestC have 309, 320 and 561 sentences, respectively.", "labels": [], "entities": [{"text": "TestA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8099434971809387}]}, {"text": "In order to evaluate the performance of the TMLM, a lattice with a low oracle error rate was generated by a Viterbi decoder using the word trigram model (Word-3gr) model.", "labels": [], "entities": []}, {"text": "From the lattice at most 30 (N=30) sentences are extracted for each utterance to form an N-best list.", "labels": [], "entities": []}, {"text": "The N-best error rate for the combined test set (All) is 22.7%.", "labels": [], "entities": [{"text": "N-best error rate", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9079199433326721}]}, {"text": "The Nbest size is limited (it is not in the hundreds), simply because of faster experiment turn-around.", "labels": [], "entities": [{"text": "Nbest size", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.7774076759815216}]}, {"text": "These utterances are rescored using TMLM.", "labels": [], "entities": [{"text": "TMLM", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.6973156929016113}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "The first two rows in the table show the baseline numbers for the word trigram (Word-3gr) model.", "labels": [], "entities": []}, {"text": "TestA has a WER of 18.7% similar to that of TestB (18.6%).", "labels": [], "entities": [{"text": "TestA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9192559123039246}, {"text": "WER", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9996333122253418}, {"text": "TestB", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.9338310360908508}]}, {"text": "The WER for TestC is relatively high (38.9%), because, as explained above, TestC contains causal conversation with hesitations and repairs, and speakers do not necessarily stick to the domain.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9966024160385132}]}, {"text": "Moreover, when users are speaking to a device, as in the case of TestA and TestB, they use clear and shorter sentences, which are easier to recognize.", "labels": [], "entities": [{"text": "TestB", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9093843698501587}]}, {"text": "The TMLM does not provide improvements for TestA and TestB but it improves the WER by 0.7% for TestC.", "labels": [], "entities": [{"text": "TMLM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7169567346572876}, {"text": "TestB", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9370364546775818}, {"text": "WER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9937466382980347}, {"text": "TestC", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.9380304217338562}]}, {"text": "The combined overall result is a 0.4% improvement over baseline.", "labels": [], "entities": []}, {"text": "This improvement is not statistically significant.", "labels": [], "entities": []}, {"text": "However, interpolating TMLM with Word-3gr improves the WER to 31.9%, which is 1.0% better than that of the Word-3gr.", "labels": [], "entities": [{"text": "Word-3gr", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9254684448242188}, {"text": "WER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9966099858283997}, {"text": "Word-3gr", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.9648244976997375}]}, {"text": "Standard p-test (Matched Pairs Sentence-Segment Word Error test available in standard SCLITEs statistical system comparison program from NIST) shows that this improvement is significant at p < 0.05 level.", "labels": [], "entities": [{"text": "NIST", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.9000434875488281}]}, {"text": "The interpolation weights are set equally to 0.5 for each LM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Speech Recognition Language Model Rescoring  Results.", "labels": [], "entities": [{"text": "Speech Recognition Language Model Rescoring", "start_pos": 10, "end_pos": 53, "type": "TASK", "confidence": 0.8730360627174377}]}]}