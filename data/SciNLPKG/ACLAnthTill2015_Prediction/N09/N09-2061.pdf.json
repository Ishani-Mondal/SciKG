{"title": [{"text": "Sentence Boundary Detection and the Problem with the U.S", "labels": [], "entities": [{"text": "Sentence Boundary Detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9647348523139954}]}], "abstractContent": [{"text": "Sentence Boundary Detection is widely used but often with outdated tools.", "labels": [], "entities": [{"text": "Sentence Boundary Detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9522010087966919}]}, {"text": "We discuss what makes it difficult, which features are relevant, and present a fully statistical system, now publicly available, that gives the best known error rate on a standard news corpus: Of some 27,000 examples, our system makes 67 errors, 23 involving the word \"U.S.\"", "labels": [], "entities": [{"text": "error rate", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9200969934463501}]}], "introductionContent": [{"text": "Many natural language processing tasks begin by identifying sentences, but due to the semantic ambiguity of the period, the sentence boundary detection (SBD) problem is non-trivial.", "labels": [], "entities": [{"text": "sentence boundary detection (SBD)", "start_pos": 124, "end_pos": 157, "type": "TASK", "confidence": 0.7752001533905665}]}, {"text": "While reported error rates are low, significant improvement is possible and potentially valuable.", "labels": [], "entities": [{"text": "error", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9199935793876648}]}, {"text": "For example, since a single error can ruin an automatically generated summary, reducing the error rate from 1% to 0.25% reduces the rate of damaged 10-sentence summaries from 1 in 10 to 1 in 40.", "labels": [], "entities": [{"text": "error rate", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9830273389816284}]}, {"text": "Better SBD may improve language models and sentence alignment as well.", "labels": [], "entities": [{"text": "SBD", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9663510918617249}, {"text": "sentence alignment", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7431498467922211}]}, {"text": "SBD has been addressed only a few times in the literature, and each result points to the importance of developing lists of common abbreviations and sentence starters.", "labels": [], "entities": [{"text": "SBD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9353771209716797}]}, {"text": "Further, most practical implementations are not readily available (with one notable exception).", "labels": [], "entities": []}, {"text": "Here, we present a fully statistical system that we argue benefits from avoiding manually constructed or tuned lists.", "labels": [], "entities": []}, {"text": "We provide a detailed analysis of features, training variations, and errors, all of which are under-explicated in the literature, and discuss the possibility of a more structured classification approach.", "labels": [], "entities": []}, {"text": "Our implementation gives the best performance, to our knowledge, reported on a standard Wall Street Journal task; it is open-source and available to the public.", "labels": [], "entities": [{"text": "Wall Street Journal task", "start_pos": 88, "end_pos": 112, "type": "DATASET", "confidence": 0.9664091318845749}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: SVM and Naive Bayes classification error rates  on different corpora using a model trained from a disjoint  WSJ data set.", "labels": [], "entities": [{"text": "Naive Bayes classification", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.703280488650004}, {"text": "WSJ data set", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.961697002251943}]}, {"text": " Table 4: SVM error rates on the test corpora, using mod- els built from different numbers of training sentences.", "labels": [], "entities": []}]}