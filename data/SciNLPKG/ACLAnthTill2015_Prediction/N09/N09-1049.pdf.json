{"title": [{"text": "Hierarchical Phrase-Based Translation with Weighted Finite State Transducers", "labels": [], "entities": [{"text": "Hierarchical Phrase-Based Translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6653748353322347}]}], "abstractContent": [{"text": "This paper describes a lattice-based decoder for hierarchical phrase-based translation.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.5883926252524058}]}, {"text": "The decoder is implemented with standard WFST operations as an alternative to the well-known cube pruning procedure.", "labels": [], "entities": []}, {"text": "We find that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, direct generation of translation lattices in the target language, better parameter optimization, and improved translation performance when rescoring with long-span language models and MBR decoding.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8402204513549805}, {"text": "translation search", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.9267188608646393}, {"text": "translation", "start_pos": 244, "end_pos": 255, "type": "TASK", "confidence": 0.9576422572135925}]}, {"text": "We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFST-based hierarchical decoder with hierarchical translation under cube pruning.", "labels": [], "entities": [{"text": "NIST translation", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7411537170410156}]}], "introductionContent": [{"text": "Hierarchical phrase-based translation generates translation hypotheses via the application of hierarchical rules in CYK parsing.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7068518996238708}, {"text": "CYK parsing", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.6928471624851227}]}, {"text": "Cube pruning is used to apply language models at each cell of the CYK grid as part of the search fora k-best list of translation candidates.", "labels": [], "entities": [{"text": "CYK grid", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9107099771499634}]}, {"text": "While this approach is very effective and has been shown to produce very good quality translation, the reliance on k-best lists is a limitation.", "labels": [], "entities": []}, {"text": "We take an alternative approach and describe a lattice-based hierarchical decoder implemented with Weighted Finite State Transducers (WFSTs).", "labels": [], "entities": []}, {"text": "In every CYK cell we build a single, minimal word lattice containing all possible translations of the source sentence span covered by that cell.", "labels": [], "entities": []}, {"text": "When derivations contain non-terminals, we use pointers to lowerlevel lattices for memory efficiency.", "labels": [], "entities": []}, {"text": "The pointers are only expanded to the actual translations if pruning is required during search; expansion is otherwise only carried out at the upper-most cell, after the full CYK grid has been traversed.", "labels": [], "entities": [{"text": "CYK grid", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.8942812085151672}]}, {"text": "We describe how this decoder can be easily implemented with WFSTs.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.898564875125885}]}, {"text": "For this we employ the OpenFST libraries.", "labels": [], "entities": []}, {"text": "Using standard FST operations such as composition, epsilon removal, determinization, minimization and shortest-path, we find this search procedure to be simpler to implement than cube pruning.", "labels": [], "entities": [{"text": "FST", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8456538319587708}]}, {"text": "The main modeling advantages area significant reduction in search errors, a simpler implementation, direct generation of target language word lattices, and better integration with other statistical MT procedures.", "labels": [], "entities": [{"text": "MT", "start_pos": 198, "end_pos": 200, "type": "TASK", "confidence": 0.8676313757896423}]}, {"text": "We report translation results in Arabic-to-English and Chinese-to-English translation and contrast the performance of lattice-based and cube pruning hierarchical decoding.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report experiments on the NIST MT08 Arabicto-English and Chinese-to-English translation tasks.", "labels": [], "entities": [{"text": "NIST MT08 Arabicto-English", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.8829149405161539}, {"text": "Chinese-to-English translation tasks", "start_pos": 60, "end_pos": 96, "type": "TASK", "confidence": 0.6939701835314432}]}, {"text": "We contrast two hierarchical phrase-based decoders.", "labels": [], "entities": []}, {"text": "The first decoder, Hiero Cube Pruning (HCP), is a kbest decoder using cube pruning implemented as described by.", "labels": [], "entities": []}, {"text": "In our implementation, kbest lists contain unique hypotheses.", "labels": [], "entities": []}, {"text": "The second decoder, Hiero FST (HiFST), is a lattice-based decoder implemented with Weighted Finite State Transducers as described in the previous section.", "labels": [], "entities": [{"text": "Hiero FST (HiFST)", "start_pos": 20, "end_pos": 37, "type": "DATASET", "confidence": 0.643869799375534}]}, {"text": "Hypotheses are generated after determinization under the tropical semiring so that scores assigned to hypotheses arise from single minimum cost / maximum likelihood derivations.", "labels": [], "entities": []}, {"text": "We also use a variant of the k-best decoder which works in alignment mode: given an input k-best list, it outputs the feature scores of each hypothesis in the list without applying any pruning.", "labels": [], "entities": []}, {"text": "This is used for Minimum Error Training (MET) with the HiFST system.", "labels": [], "entities": [{"text": "Minimum Error Training (MET)", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.6102471848328909}, {"text": "HiFST", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9362757205963135}]}, {"text": "These two language pairs pose very different translation challenges.", "labels": [], "entities": []}, {"text": "For example, Chineseto-English translation requires much greater word movement than Arabic-to-English.", "labels": [], "entities": [{"text": "Chineseto-English translation", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6669027507305145}]}, {"text": "In the framework of hierarchical translation systems, we have found that shallow decoding (see section 3.2) is as good as full hierarchical decoding in Arabicto-English (.", "labels": [], "entities": []}, {"text": "In Chinese-toEnglish, we have not found this to be the case.", "labels": [], "entities": []}, {"text": "Therefore, we contrast the performance of HiFST and HCP under shallow hierarchical decoding for Arabic-to-English, while for Chinese-to-English we perform full hierarchical decoding.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.8386284708976746}]}, {"text": "Both hierarchical translation systems share a common architecture.", "labels": [], "entities": []}, {"text": "For both language pairs, alignments are generated over the parallel data.", "labels": [], "entities": []}, {"text": "The following features are extracted and used in translation: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7844394147396088}]}, {"text": "The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition.", "labels": [], "entities": [{"text": "English Gigaword Third Edition", "start_pos": 139, "end_pos": 169, "type": "DATASET", "confidence": 0.846787229180336}]}, {"text": "Details of the parallel corpus and development sets used for each language pair are given in their respective section.", "labels": [], "entities": []}, {"text": "Standard MET iterative parameter estimation under IBM BLEU () is performed on the corresponding development set.", "labels": [], "entities": [{"text": "MET iterative parameter estimation", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.6969199180603027}, {"text": "IBM", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7221742868423462}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9568701386451721}]}, {"text": "For the HCP system, MET is done following.", "labels": [], "entities": [{"text": "MET", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9204840064048767}]}, {"text": "For the HiFST system, we obtain a k-best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.8157873749732971}]}, {"text": "After translation with optimized feature weights, we carryout the two following rescoring steps.", "labels": [], "entities": []}, {"text": "We build sentencespecific zero-cutoff stupid-backoff () 5-gram language models, estimated using \u223c4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 231, "end_pos": 236, "type": "DATASET", "confidence": 0.9680017232894897}]}, {"text": "Lattices provide avast search space relative to k-best lists, with translation lattice sizes of 10 81 hypotheses reported in the literature ().", "labels": [], "entities": []}, {"text": "\u2022 Minimum Bayes Risk (MBR).", "labels": [], "entities": [{"text": "Minimum Bayes Risk (MBR)", "start_pos": 2, "end_pos": 26, "type": "METRIC", "confidence": 0.8813426991303762}]}, {"text": "We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function ().", "labels": [], "entities": [{"text": "MBR", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8076558709144592}, {"text": "BLEU score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9371101260185242}]}], "tableCaptions": [{"text": " Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU | TER) after MET and subsequent  rescoring steps. Decoding time reported for mt02-05-tune.", "labels": [], "entities": [{"text": "Constrative Arabic-to-English translation", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.49808381001154584}, {"text": "IBM BLEU | TER)", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.7559379935264587}, {"text": "MET", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.6809802055358887}]}, {"text": " Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU|TER) after MET and subsequent  rescoring steps. The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.", "labels": [], "entities": [{"text": "Contrastive Chinese-to-English translation", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.6909826397895813}, {"text": "BLEU|TER)", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.7595766484737396}, {"text": "MET optimization", "start_pos": 212, "end_pos": 228, "type": "TASK", "confidence": 0.975770890712738}]}]}