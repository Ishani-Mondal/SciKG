{"title": [{"text": "Active Zipfian Sampling for Statistical Parser Training *", "labels": [], "entities": [{"text": "Statistical Parser Training", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.8715826670328776}]}], "abstractContent": [{"text": "Active learning has proven to be a successful strategy in quick development of corpora to be used in training of statistical natural language parsers.", "labels": [], "entities": [{"text": "statistical natural language parsers", "start_pos": 113, "end_pos": 149, "type": "TASK", "confidence": 0.6301425248384476}]}, {"text": "A vast majority of studies in this field has focused on estimating informative-ness of samples; however, representativeness of samples is another important criterion to be considered in active learning.", "labels": [], "entities": []}, {"text": "We present a novel metric for estimating representativeness of sentences, based on a modification of Zipf's Principle of Least Effort.", "labels": [], "entities": [{"text": "estimating representativeness of sentences", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.808986634016037}]}, {"text": "Experiments on WSJ corpus with a wide-coverage parser show that our method performs always at least as good as and generally significantly better than alternative representativeness-based methods.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.9388805031776428}]}], "introductionContent": [{"text": "Wide coverage statistical parsers) have proven to require large amounts of manually annotated data for training to achieve substantial performance.", "labels": [], "entities": []}, {"text": "However, building such large annotated corpora is very expensive in terms of human effort, time and cost.", "labels": [], "entities": []}, {"text": "Several alternatives of the standard supervised learning setting have been proposed to reduce the annotation costs, one of which is active learning.", "labels": [], "entities": []}, {"text": "Active learning setting allows the learner to select its own samples to be labeled and added to the training data iteratively.", "labels": [], "entities": []}, {"text": "The motive behind active learning * Vast majority of this work was done while the author was a graduate student in Middle East Technical University, under the funding from T \u00a8 UB \u02d9 ITAK-B \u02d9 IDEB through 2210 National Scholarship Programme for is that if the learner may select highly informative samples, it can eliminate the redundancy generally found in random data; however, informative samples can be very untypical ().", "labels": [], "entities": [{"text": "T \u00a8 UB", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.7794455091158549}, {"text": "ITAK-B \u02d9 IDEB through 2210 National Scholarship Programme", "start_pos": 181, "end_pos": 238, "type": "DATASET", "confidence": 0.6867195218801498}]}, {"text": "Unlike random sampling, active learning has no guarantee of selecting representative samples and untypical training samples are expected to degrade test performance of a classifier.", "labels": [], "entities": []}, {"text": "To get around this problem, several methods of estimating representativeness of a sample have been introduced.", "labels": [], "entities": []}, {"text": "In this study, we propose a novel representativeness estimator fora sentence, which is based on a modification of Zipf's Principle of Least Effort, theoretically sound and empirically validated on Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 197, "end_pos": 209, "type": "DATASET", "confidence": 0.9622474014759064}]}, {"text": "Experiments conducted with a wide coverage CCG parser ( show that using our estimator as a representativeness metric never performs worse than and generally outperforms length balanced sampling), which is another representativeness based active learning method, and pure informativeness based active learning.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on CCGbank corpus) with the wide coverage CCG parser of Clark and . C&C parser was fast enough to enable us to use the whole available training data pool for sample selection in experiments, but not for training (since training C&C parser is not that fast).", "labels": [], "entities": [{"text": "CCGbank corpus", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9323891997337341}, {"text": "Clark", "start_pos": 81, "end_pos": 86, "type": "DATASET", "confidence": 0.9610252976417542}]}, {"text": "Among the models implemented in the parser, the normal-form model is used.", "labels": [], "entities": []}, {"text": "We used the default settings of the C&C parser distribution for fair evaluation.", "labels": [], "entities": []}, {"text": "WSJ Sections 02-21 (39604 sentences) are used for training and WSJ Section 23 (2407 sentences) is used for testing.", "labels": [], "entities": [{"text": "WSJ Sections 02-21", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9031165242195129}, {"text": "WSJ Section 23", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.844573974609375}]}, {"text": "Following, we evaluated the parser performance using the labeled f-score of the predicate-argument dependencies produced by the parser.", "labels": [], "entities": []}, {"text": "For each active learning scheme and random sampling, the size of the seed training set is 500 sentences, the batch size is 100 sentences and iteration stops after reaching 2000 sentences.", "labels": [], "entities": []}, {"text": "For statistical significance, each experiment is replicated 5 times.", "labels": [], "entities": []}, {"text": "We evaluate the active learning performance in terms of Percentage Reduction in Utilized Data, i.e. how many percents less data is used by AL compared to random sampling, in order to reach a certain performance score.", "labels": [], "entities": []}, {"text": "Amount of used data is measured with the number of brackets in the data.", "labels": [], "entities": [{"text": "Amount", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9861599802970886}]}, {"text": "In CCGbank, a bracket always corresponds to a parse decision, so it is a reasonable approximation of the amount of annotator effort.", "labels": [], "entities": []}, {"text": "Our measure is compared to length balanced sampling and using no representativeness measures.", "labels": [], "entities": []}, {"text": "Since there is not a trivial distance metric between CCG parses and we do not know a proposed one, we could not test it against sample density method.", "labels": [], "entities": []}, {"text": "We limited the informativeness measures to be tested to the four single-learner measures we mentioned in Section 2.", "labels": [], "entities": []}, {"text": "Multi-learner and ensemble methods are excluded, since the success of such methods re-lies heavily on the diversity of the available models ().", "labels": [], "entities": []}, {"text": "The models in C&C parser are not diverse enough and we left crafting such diverse models to future work.", "labels": [], "entities": [{"text": "C&C parser", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.5626069009304047}]}, {"text": "We combined f zipf \u2212eng with the informativeness measures as follow: With tree entropy, sentences with the highest is the tree entropy of the sentence sunder the current model G, normalized by the binary logarithm of the number of parses, following).", "labels": [], "entities": []}, {"text": "With lowest best probability, sentences with the highest f zipf \u2212eng (s) \u00d7 (1 \u2212 f bp (s, G)) values are selected, where f bp is the best probability function (see Section 2).", "labels": [], "entities": []}, {"text": "With unparsed/entropy, we primarily chose the unparsable sentences having highest f zipf \u2212eng (s) values and filled the rest of the batch according to f zipf \u2212entropy . With two-stage active learning, we primarily chose sentences that can be parsed by the full parser but not the bagged parser and have the highest f zipf \u2212eng (s) values, we secondarily chose sentences that cannot be parsed by both parsers and have the highest f zipf \u2212eng (s) values, the third priority is given to sentences having highest f zipf \u2212entropy values.", "labels": [], "entities": []}, {"text": "Combining length balanced sampling with all of these informativeness measures is straightforward.", "labels": [], "entities": []}, {"text": "For statistical significance, a different random sample is used for length histogram in each replication of experiment.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8088425397872925}, {"text": "length", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9835249185562134}]}], "tableCaptions": [{"text": " Table 1: PRUD values of different AL schemes. The row includes the informativeness measure and the column  includes the representativeness measure used. The column with the label random always includes the results for  random sampling. The numbers in parentheses are the labeled f-score values reached by the schemes.", "labels": [], "entities": []}]}