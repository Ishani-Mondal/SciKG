{"title": [{"text": "Domain Adaptation with Latent Semantic Association for Named Entity Recognition", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7230192869901657}, {"text": "Named Entity Recognition", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6446782549222311}]}], "abstractContent": [{"text": "Domain adaptation is an important problem in named entity recognition (NER).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7701063752174377}, {"text": "named entity recognition (NER)", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.791647712389628}]}, {"text": "NER classi-fiers usually lose accuracy in the domain transfer due to the different data distribution between the source and the target domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9988011121749878}]}, {"text": "The major reason for performance degrading is that each entity type often has lots of domain-specific term representations in the different domains.", "labels": [], "entities": []}, {"text": "The existing approaches usually need an amount of labeled target domain data for tuning the original model.", "labels": [], "entities": []}, {"text": "However, it is a labor-intensive and time-consuming task to build annotated training data set for every target domain.", "labels": [], "entities": []}, {"text": "We present a domain adaptation method with latent semantic association (LaSA).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7109870314598083}]}, {"text": "This method effectively overcomes the data distribution difference without lever-aging any labeled target domain data.", "labels": [], "entities": []}, {"text": "LaSA model is constructed to capture latent semantic association among words from the unla-beled corpus.", "labels": [], "entities": []}, {"text": "It groups words into a set of concepts according to the related context snippets.", "labels": [], "entities": []}, {"text": "In the domain transfer, the original term spaces of both domains are projected to a concept space using LaSA model at first, then the original NER model is tuned based on the semantic association features.", "labels": [], "entities": [{"text": "domain transfer", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.726337194442749}]}, {"text": "Experimental results on English and Chinese corpus show that LaSA-based domain adaptation significantly enhances the performance of NER.", "labels": [], "entities": [{"text": "LaSA-based domain adaptation", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6741620302200317}, {"text": "NER", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9660401940345764}]}], "introductionContent": [{"text": "Named entities (NE) are phrases that contain names of persons, organizations, locations, etc.", "labels": [], "entities": [{"text": "Named entities (NE) are phrases that contain names of persons, organizations, locations", "start_pos": 0, "end_pos": 87, "type": "TASK", "confidence": 0.640290342271328}]}, {"text": "NER is an important task in information extraction and natural language processing (NLP) applications.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8975756168365479}, {"text": "information extraction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.789554238319397}]}, {"text": "Supervised learning methods can effectively solve NER problem by learning a model from manually labeled data.", "labels": [], "entities": [{"text": "NER problem", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9337268769741058}]}, {"text": "However, empirical study shows that NE types have different distribution across domains ().", "labels": [], "entities": []}, {"text": "Trained NER classifiers in the source domain usually lose accuracy in anew target domain when the data distribution is different between both domains.", "labels": [], "entities": [{"text": "NER classifiers", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.9191537201404572}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9986485838890076}]}, {"text": "Domain adaptation is a challenge for NER and other NLP applications.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8084828555583954}]}, {"text": "In the domain transfer, the reason for accuracy loss is that each NE type often has various specific term representations and context clues in the different domains.", "labels": [], "entities": [{"text": "domain transfer", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7251080274581909}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.998400866985321}]}, {"text": "For example, {\"economist\", \"singer\", \"dancer\", \"athlete\", \"player\", \"philosopher\", ...} are used as context clues for NER.", "labels": [], "entities": []}, {"text": "However, the distribution of these representations are varied with domains.", "labels": [], "entities": []}, {"text": "We expect to do better domain adaptation for NER by exploiting latent semantic association among words from different domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7313701659440994}, {"text": "NER", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9474238157272339}]}, {"text": "Some approaches have been proposed to group words into \"topics\" to capture important relationships between words, such as Latent Semantic Indexing (LSI)), probabilistic Latent Semantic Indexing (pLSI)), Latent Dirichlet Allocation (LDA) ().", "labels": [], "entities": []}, {"text": "These models have been successfully employed in topic modeling, dimensionality reduction for text categorization (, ad hoc IR (), and soon.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.918784886598587}, {"text": "dimensionality reduction", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.6611501276493073}, {"text": "text categorization", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.735036700963974}]}, {"text": "In this paper, we present a domain adaptation method with latent semantic association.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7467946708202362}]}, {"text": "We focus on capturing the hidden semantic association among words in the domain adaptation.", "labels": [], "entities": []}, {"text": "We introduce the LaSA model to overcome the distribution difference between the source domain and the target domain.", "labels": [], "entities": []}, {"text": "LaSA model is constructed from the unlabeled corpus at first.", "labels": [], "entities": [{"text": "LaSA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8798779845237732}]}, {"text": "It learns latent semantic association among words from their related context snippets.", "labels": [], "entities": []}, {"text": "In the domain transfer, words in the corpus are associated with a low-dimension concept space using LaSA model, then the original NER model is tuned using these generated semantic association features.", "labels": [], "entities": []}, {"text": "The intuition behind our method is that words in one concept set will have similar semantic features or latent semantic association, and share syntactic and semantic context in the corpus.", "labels": [], "entities": []}, {"text": "They can be considered as behaving in the same way for discriminative learning in the source and target domains.", "labels": [], "entities": []}, {"text": "The proposed method associates words from different domains on a semantic level rather than by lexical occurrence.", "labels": [], "entities": []}, {"text": "It can better bridge the domain distribution gap without any labeled target domain samples.", "labels": [], "entities": []}, {"text": "Experimental results on English and Chinese corpus show that LaSA-based adaptation significantly enhances NER performance across domains.", "labels": [], "entities": [{"text": "LaSA-based adaptation", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.9144338369369507}, {"text": "NER", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9876081347465515}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes the related works.", "labels": [], "entities": []}, {"text": "Section 3 presents a domain adaptation method based on latent semantic association.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7691980004310608}]}, {"text": "Section 4 illustrates how to learn LaSA model from the unlabeled corpus.", "labels": [], "entities": []}, {"text": "Section 5 shows experimental results on large-scale English and Chinese corpus across domains, respectively.", "labels": [], "entities": []}, {"text": "The conclusion is given in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate LaSA-based domain adaptation method on both English and Chinese corpus in this section.", "labels": [], "entities": [{"text": "LaSA-based domain adaptation", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6369164089361826}]}, {"text": "In the experiments, we focus on recognizing person (PER), location (LOC) and organization (ORG) in the given four domains, including economics (Eco), entertainment (Ent), politics (Pol) and sports (Spo).", "labels": [], "entities": [{"text": "ORG", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.6971735954284668}]}, {"text": "In the NER domain adaptation, nouns and adjectives make a significant impact on the performance.", "labels": [], "entities": [{"text": "NER domain adaptation", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.9305122097333273}]}, {"text": "Thus, we focus on capturing latent semantic association for high-frequency nouns and adjectives (i.e. occurrence count \u2265 50 ) in the unlabeled corpus.", "labels": [], "entities": []}, {"text": "LaSA models for nouns and adjectives are learned from the unlabeled corpus using Algorithm 1 (see section 4.2), respectively.", "labels": [], "entities": []}, {"text": "Our empirical study shows that better adaptation is obtained with a 50-topic LaSA model.", "labels": [], "entities": []}, {"text": "Therefore, we set the number of topics N as 50, and define the context view window size as {-3,3} (i.e. previous 3 words and next 3 words) in the LaSA model learning.", "labels": [], "entities": []}, {"text": "LaSA features for other irrespective words (e.g. token unit \"the\") are assigned with a default topic value N +1.", "labels": [], "entities": []}, {"text": "All the basic NER models are trained on the domain-specific training data using RRM classifier ().", "labels": [], "entities": [{"text": "NER", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9758868217468262}]}, {"text": "RRM is a generalization Winnow learning algorithm ().", "labels": [], "entities": []}, {"text": "We set the context view window size as {-2,2} in NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9088911414146423}]}, {"text": "Given a word instance x, we employ local linguistic features (e.g. word unit, part of speech) of x and its context units ( i.e. previous 2 words and next 2 words ) in NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.6752228140830994}]}, {"text": "All Chinese texts in the experiments are automatically segmented into words using HMM.", "labels": [], "entities": []}, {"text": "In LaSA-based domain adaptation, the semantic association features of each unit in the observation window {-2,2} are generated by LaSA model at first, then the basic source domain NER model is tuned on the original source domain training data set by incorporating the semantic association features.", "labels": [], "entities": [{"text": "LaSA-based domain adaptation", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7335305611292521}]}, {"text": "For example, given the sentence \"This popular new singer attended the new year party\", illustrates various features and views at the current word w i = \"singer\" in LaSA-based adaptation.", "labels": [], "entities": [{"text": "LaSA-based adaptation", "start_pos": 164, "end_pos": 185, "type": "TASK", "confidence": 0.5958550274372101}]}, {"text": "In the viewing window at the word \"singer\" (see), each word unit around \"singer\" is codified with a set of primitive features (e.g. P OS, SA, T ag), together with its relative position to \"singer\".", "labels": [], "entities": []}, {"text": "Here, \"SA\" denotes semantic association feature set which is generated by LaSA model.", "labels": [], "entities": []}, {"text": "\"T ag\" denotes NE tags labeled in the data set.", "labels": [], "entities": []}, {"text": "Given the input vector constructed with the above features, RRM method is then applied to train linear weight vectors, one for each possible class-label.", "labels": [], "entities": []}, {"text": "In the decoding stage, the class with the maximum confidence is then selected for each token unit.", "labels": [], "entities": []}, {"text": "In our evaluation, only NEs with correct boundaries and correct class labels are considered as the correct recognition.", "labels": [], "entities": []}, {"text": "We use the standard Precision (P), Recall (R), and F-measure (F = 2P RP +R ) to measure the performance of NER models.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.9515309184789658}, {"text": "Recall (R)", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9550133794546127}, {"text": "F-measure", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9915456175804138}, {"text": "F = 2P RP +R )", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.7978538019316537}]}, {"text": "All the experiments are conducted on the above large-scale English and Chinese corpus.", "labels": [], "entities": [{"text": "English and Chinese corpus", "start_pos": 59, "end_pos": 85, "type": "DATASET", "confidence": 0.6609441861510277}]}, {"text": "The overall performance enhancement of NER by LaSA-based  domain adaptation is evaluated at first.", "labels": [], "entities": [{"text": "NER", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9772439002990723}, {"text": "LaSA-based  domain adaptation", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.7006436983744303}]}, {"text": "Since the distribution of each NE type is different across domains, we also analyze the performance enhancement on each entity type by LaSA-based adaptation. and 7 show the experimental results for all pairs of domain adaptation on both English and Chinese corpus, respectively.", "labels": [], "entities": []}, {"text": "In the experiment, the basic source domain NER model Ms is learned from the specific domain training data set D dom (see   Experimental results on English and Chinese corpus indicate that the performance of Ms significantly degrades in each basic domain transfer without using LaSA model (see and 7).", "labels": [], "entities": []}, {"text": "For example, in the \"Eco\u2192Ent\" transfer on Chinese corpus (see  Experimental results on English corpus show that LaSA-based adaptation effectively enhances the performance in each domain transfer (see).", "labels": [], "entities": [{"text": "Chinese corpus", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.7849235236644745}]}, {"text": "For example, in the \"Pol\u2192Eco\" transfer, F Base is 63.62% while F LaSA achieves 68.10%.", "labels": [], "entities": [{"text": "F Base", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.8721030652523041}, {"text": "F LaSA", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.8076871037483215}]}, {"text": "Compared with F Base , LaSA-based method significantly enhances F-measure by 7.04%.", "labels": [], "entities": [{"text": "F Base", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.6888933181762695}, {"text": "F-measure", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9795106053352356}]}, {"text": "We perform t-tests on F-measure of all the comparison experiments on English corpus.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.991858184337616}, {"text": "English corpus", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.8667552471160889}]}, {"text": "The p-value is 2.44E-06, which shows that the improvement is statistically significant.", "labels": [], "entities": []}, {"text": "Experimental results on Chinese corpus also show that LaSA-based adaptation effectively increases the accuracy in all the tests (see).", "labels": [], "entities": [{"text": "Chinese corpus", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9268383085727692}, {"text": "LaSA-based adaptation", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.9058108627796173}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9995452761650085}]}, {"text": "For example, in the \"Eco\u2192Ent\" transfer, compared with F Base , LaSA-based adaptation significantly increases Fmeasure by 9.88%.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9978058934211731}]}, {"text": "We also perform t-tests on F-measure of 12 comparison experiments on Chinese corpus.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9754461050033569}, {"text": "Chinese corpus", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9111156761646271}]}, {"text": "The p-value is 1.99E-06, which shows that the enhancement is statistically significant.", "labels": [], "entities": []}, {"text": "Moreover, the relative reduction in error is above 10% with LaSA-based method in each test.", "labels": [], "entities": [{"text": "error", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.7473470568656921}, {"text": "LaSA-based", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.5934984683990479}]}, {"text": "LaSA model decreases the accuracy loss by 16.43% in average.", "labels": [], "entities": [{"text": "LaSA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9008533954620361}, {"text": "accuracy loss", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.987655371427536}]}, {"text": "Especially for the \"Eco\u2192Ent\" transfer (see), \u03b4(loss) achieves 26.29% with LaSA-based method.", "labels": [], "entities": [{"text": "\u03b4(loss)", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.915226474404335}]}], "tableCaptions": [{"text": " Table 1: Top 10 nouns from 5 randomly selected topics  computed on the economics and entertainment domains", "labels": [], "entities": []}, {"text": " Table 2: English training and test data sets", "labels": [], "entities": [{"text": "English training and test data sets", "start_pos": 10, "end_pos": 45, "type": "DATASET", "confidence": 0.5960464229186376}]}, {"text": " Table 3: Domain distribution in the unlabeled English  data set", "labels": [], "entities": [{"text": "unlabeled English  data set", "start_pos": 37, "end_pos": 64, "type": "DATASET", "confidence": 0.7948817014694214}]}, {"text": " Table 4: Chinese training and test data sets", "labels": [], "entities": [{"text": "Chinese training and test data sets", "start_pos": 10, "end_pos": 45, "type": "DATASET", "confidence": 0.6127268821001053}]}, {"text": " Table 5: Domain distribution in the unlabeled Chinese  data set", "labels": [], "entities": [{"text": "Chinese  data set", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8040322959423065}]}, {"text": " Table 6: Experimental results on English corpus", "labels": [], "entities": []}]}