{"title": [{"text": "Comparison of Extended Lexicon Models in Search and Rescoring for SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9709569811820984}]}], "abstractContent": [{"text": "We show how the integration of an extended lexicon model into the decoder can improve translation performance.", "labels": [], "entities": []}, {"text": "The model is based on lexical triggers that capture long-distance dependencies on the sentence level.", "labels": [], "entities": []}, {"text": "The results are compared to variants of the model that are applied in reranking of n-best lists.", "labels": [], "entities": []}, {"text": "We present how a combined application of these models in search and rescoring gives promising results.", "labels": [], "entities": []}, {"text": "Experiments are reported on the GALE Chinese-English task with improvements of up to +0.9% BLEU and-1.5% TER absolute on a competitive baseline.", "labels": [], "entities": [{"text": "GALE Chinese-English task", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.49577592809995014}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9996465444564819}, {"text": "TER absolute", "start_pos": 105, "end_pos": 117, "type": "METRIC", "confidence": 0.966749906539917}]}], "introductionContent": [{"text": "Phrase-based statistical machine translation has improved significantly over the last decade.", "labels": [], "entities": [{"text": "Phrase-based statistical machine translation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7074898928403854}]}, {"text": "The availability of large amounts of parallel data and access to open-source software allow for easy setup of translation systems with acceptable performance.", "labels": [], "entities": []}, {"text": "Public evaluations such as the NIST MT Eval or the WMT Shared Task help to measure overall progress within the community.", "labels": [], "entities": [{"text": "NIST MT Eval", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.853476365407308}, {"text": "WMT Shared Task", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.6902902126312256}]}, {"text": "Most of the groups use a phrase-based decoder (e.g. Pharaoh or the more recent Moses) based on a log-linear fusion of models that enable the avid researcher to quickly incorporate additional features and investigate the effect of additional knowledge sources to guide the search for better translation hypotheses.", "labels": [], "entities": []}, {"text": "In this paper, we deal with an extended lexicon model and its incorporation into a state-of-the-art decoder.", "labels": [], "entities": []}, {"text": "We compare the results of the integration to a similar setup used within a rescoring framework and show the benefits of integrating additional models directly into the search process.", "labels": [], "entities": []}, {"text": "As will be shown, although a rescoring framework is suitable for obtaining quick trends of incorporating additional models into a system, an alternative that includes the model in search should be preferred.", "labels": [], "entities": []}, {"text": "The integration does not only yield better performance, we will also show the benefit of combining both approaches in order to boost translation quality even more.", "labels": [], "entities": []}, {"text": "The extended lexicon model which we apply is motivated by a trigger-based approach.", "labels": [], "entities": []}, {"text": "A standard lexicon modeling dependencies of target and source words, i.e. p(e|f ), is extended with a second trigger f on the source side, resulting in p(e|f, f ).", "labels": [], "entities": []}, {"text": "This model allows fora more fine-grained lexical choice of the target word depending on the additional source word f . Since the second trigger can move over the whole sentence, we capture global (sentence-level) context that is not modeled in local n-grams of the language model or in bilingual phrase pairs that cover only a limited amount of consecutive words.", "labels": [], "entities": []}, {"text": "Related work A similar approach has been tried in the word-sense disambiguation (WSD) domain where local but also across-sentence unigram collocations of words are used to refine phrase pair selection dynamically by incorporating scores from the WSD classifier (.", "labels": [], "entities": [{"text": "word-sense disambiguation (WSD) domain", "start_pos": 54, "end_pos": 92, "type": "TASK", "confidence": 0.8347066193819046}, {"text": "phrase pair selection", "start_pos": 179, "end_pos": 200, "type": "TASK", "confidence": 0.6776178081830343}]}, {"text": "A maximumentropy based approach with different features of surrounding words that are locally bound to a context of three positions to the left and right is reported in).", "labels": [], "entities": []}, {"text": "A logistic regression-based word translation model is investigated by but has not been evaluated on a machine translation task.", "labels": [], "entities": [{"text": "word translation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.714860126376152}, {"text": "machine translation task", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.7835494875907898}]}, {"text": "Another WSD approach incorporating context-dependent phrasal translation lexicons is presented by and has been evaluated on several translation tasks.", "labels": [], "entities": [{"text": "WSD", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9597165584564209}, {"text": "context-dependent phrasal translation lexicons", "start_pos": 35, "end_pos": 81, "type": "TASK", "confidence": 0.7174622714519501}]}, {"text": "The triplet lexicon model presented in this work can also be interpreted as an extension of the standard IBM model 1 () with an additional trigger.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are carried outwith a GALE system using the official development and test sets of the GALE 2008 evaluation.", "labels": [], "entities": [{"text": "GALE", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8188834190368652}, {"text": "GALE 2008 evaluation", "start_pos": 102, "end_pos": 122, "type": "DATASET", "confidence": 0.9351588487625122}]}, {"text": "The corpus statistics are shown in.", "labels": [], "entities": []}, {"text": "The triplet lexicon model was trained on a subset of the overall data.", "labels": [], "entities": []}, {"text": "We used 1.4M sentence pairs with 32.3M running words on the English side.", "labels": [], "entities": []}, {"text": "The vocabulary sizes were 76.5K for the source and 241.7K for the target language.", "labels": [], "entities": []}, {"text": "The final lexicon contains roughly 62 million triplets.", "labels": [], "entities": []}, {"text": "The baseline system incorporates the standard model setup used in phrase-based SMT which combines phrase translation and word lexicon models in both directions, a 5-gram language model, word and phrase penalties, and two models for reordering (a standard distortion model and a discriminative phrase orientation model).", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.6526668071746826}, {"text": "phrase translation", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7543125152587891}]}, {"text": "For a fair comparison, we also added the related IBM model 1 p(e|f ) to the baseline since it can be computed on the sentencelevel for this direction, target given source.", "labels": [], "entities": []}, {"text": "This step achieves +0.5% BLEU on the development set for newswire but has no effect on test.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9996976852416992}]}, {"text": "As will be presented in the next section, the extension to another trigger results in improvements over this baseline, indicating that the extended triplet model is superior to the standard IBM model 1.", "labels": [], "entities": []}, {"text": "The feature weights were optimized on separate development sets for both newswire and web text.", "labels": [], "entities": []}, {"text": "We perform the following pipeline of experiments: A first run generates word graphs using the baseline models.", "labels": [], "entities": []}, {"text": "From this word graph, we extract 10k-best lists and compare the performance to a reranked version including the additional models.", "labels": [], "entities": []}, {"text": "Ina second step, we add one of the trigger lexi-: Results obtained for the two test sets.", "labels": [], "entities": []}, {"text": "For the triplet models, \"fe\" means p(f |e, e ) and \"ef\" denotes p(e|f, f ).", "labels": [], "entities": []}, {"text": "BLEU/TER scores are shown in percent.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.991719126701355}, {"text": "TER", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.7544546127319336}]}, {"text": "con models to the search process, regenerate word graphs, extract updated n-best lists and add the remaining models again in a reranking step.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: GALE Chinese-English corpus statistics.", "labels": [], "entities": [{"text": "GALE Chinese-English corpus statistics", "start_pos": 10, "end_pos": 48, "type": "DATASET", "confidence": 0.8358851075172424}]}, {"text": " Table 2: Results obtained for the two test sets. For the  triplet models, \"fe\" means p(f |e, e ) and \"ef\" denotes  p(e|f, f ). BLEU/TER scores are shown in percent.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9977282881736755}, {"text": "TER", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.7955969572067261}]}]}