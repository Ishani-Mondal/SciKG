{"title": [{"text": "Inducing Compact but Accurate Tree-Substitution Grammars", "labels": [], "entities": [{"text": "Inducing Compact but Accurate Tree-Substitution Grammars", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.6711980551481247}]}], "abstractContent": [{"text": "Tree substitution grammars (TSGs) area compelling alternative to context-free grammars for modelling syntax.", "labels": [], "entities": [{"text": "Tree substitution grammars (TSGs", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7499744713306427}]}, {"text": "However, many popular techniques for estimating weighted TSGs (under the moniker of Data Oriented Parsing) suffer from the problems of inconsistency and over-fitting.", "labels": [], "entities": [{"text": "estimating weighted TSGs", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8699554602305094}]}, {"text": "We present a theoretically princi-pled model which solves these problems using a Bayesian non-parametric formulation.", "labels": [], "entities": []}, {"text": "Our model learns compact and simple grammars , uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 163, "end_pos": 167, "type": "DATASET", "confidence": 0.8726174831390381}]}], "introductionContent": [{"text": "Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g.,).", "labels": [], "entities": []}, {"text": "However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption.", "labels": [], "entities": []}, {"text": "Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.7434042096138}]}, {"text": "This effort must be repeated when moving to different languages, grammar formalisms or treebanks.", "labels": [], "entities": []}, {"text": "We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar.", "labels": [], "entities": []}, {"text": "We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary trees), not just immediate children.", "labels": [], "entities": [{"text": "Probabilistic Tree Substitution Grammar (PTSG)", "start_pos": 48, "end_pos": 94, "type": "TASK", "confidence": 0.6808306234223502}]}, {"text": "These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation.", "labels": [], "entities": []}, {"text": "Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank.", "labels": [], "entities": [{"text": "TSG derivations", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8010132312774658}]}, {"text": "This is the same problem addressed by Data Oriented Parsing (DOP,), a method which uses as productions all subtrees of the training corpus.", "labels": [], "entities": [{"text": "Data Oriented Parsing (DOP", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7667217254638672}]}, {"text": "However, many of the DOP estimation methods have serious shortcomings), namely inconsistency for DOP1 and overfitting of the maximum likelihood estimate.", "labels": [], "entities": [{"text": "DOP estimation", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.8718652129173279}]}, {"text": "In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees.", "labels": [], "entities": []}, {"text": "This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9919191598892212}]}, {"text": "The framework allows us to perform inference over an infinite space of grammar productions in an elegant and efficient manner.", "labels": [], "entities": []}, {"text": "The net result is a grammar which only uses the increased context afforded by the TSG when necessary to model the data, and otherwise uses context-free rules.", "labels": [], "entities": []}, {"text": "That is, our model learns to use larger rules when the CFG's independence assumptions do not hold.", "labels": [], "entities": [{"text": "CFG", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9410257935523987}]}, {"text": "This contrasts with DOP, which seeks to use all elementary trees from the training set.", "labels": [], "entities": []}, {"text": "While our model is able, in theory, to use all such trees, in practice the data does not justify such a large grammar.", "labels": [], "entities": []}, {"text": "Grammars that are only about twice the size of a treebank PCFG provide large gains inaccuracy.", "labels": [], "entities": []}, {"text": "We obtain additional improvements with grammars that are somewhat larger, but still much smaller than the DOP all-subtrees grammar.", "labels": [], "entities": []}, {"text": "The rules in these grammars are intuitive, potentially offering insights into grammatical structure which could be used in, e.g., the development of syntactic ontologies and guidelines for future treebanking projects.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Development results for models trained on  section 2 of the Penn tree-bank, showing labelled  constituent F1 and exact match accuracy. Grammar  sizes are the number of rules with count \u2265 1.", "labels": [], "entities": [{"text": "Penn tree-bank", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.9916170239448547}, {"text": "exact match", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.8196747601032257}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.6605223417282104}]}]}