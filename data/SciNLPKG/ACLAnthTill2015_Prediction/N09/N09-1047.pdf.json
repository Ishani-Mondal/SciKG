{"title": [{"text": "Active Learning for Statistical Phrase-based Machine Translation *", "labels": [], "entities": [{"text": "Statistical Phrase-based Machine Translation", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.7918089777231216}]}], "abstractContent": [{"text": "Statistical machine translation (SMT) models need large bilingual corpora for training , which are unavailable for some language pairs.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8071209142605463}]}, {"text": "This paper provides the first serious experimental study of active learning for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9954319000244141}]}, {"text": "We use active learning to improve the quality of a phrase-based SMT system, and show significant improvements in translation compared to a random sentence selection baseline, when test and training data are taken from the same or different domains.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.7858723998069763}]}, {"text": "Experimental results are shown in a simulated setting using three language pairs, and in a realistic situation for Bangla-English, a language pair with limited translation resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems have made great strides in translation quality.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8139337201913198}, {"text": "translation quality", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8956649005413055}]}, {"text": "However, high quality translation output is dependent on the availability of massive amounts of parallel text in the source and target language.", "labels": [], "entities": []}, {"text": "However, there area large number of languages that are considered \"lowdensity\", either because the population speaking the language is not very large, or even if millions of people speak the language, insufficient amounts of parallel text are available in that language.", "labels": [], "entities": []}, {"text": "A statistical translation system can be improved or adapted by incorporating new training data in the form of parallel text.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.7186796963214874}]}, {"text": "In this paper, we propose several novel active learning (AL) strategies for statistical machine translation in order to attack this problem.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.7457406520843506}]}, {"text": "Conventional techniques for AL of classifiers are problematic in the SMT setting.", "labels": [], "entities": [{"text": "AL of classifiers", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8943659861882528}, {"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9947574138641357}]}, {"text": "Selective sampling of sentences for AL may lead to a parallel corpus where each sentence does not share any phrase * We would like to thank Chris Callison-Burch for fruitful discussions.", "labels": [], "entities": []}, {"text": "This research was partially supported by NSERC, and by an IBM Faculty Award to the third author.", "labels": [], "entities": [{"text": "NSERC", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.8883910775184631}]}, {"text": "Thus, new sentences cannot be translated since we lack evidence for how phrase pairs combine to form novel translations.", "labels": [], "entities": []}, {"text": "In this paper, we take the approach of exploration vs. exploitation: wherein some cases we pick sentences that are not entirely novel to improve translation statistics, while also injecting novel translation pairs to improve coverage.", "labels": [], "entities": []}, {"text": "There maybe evidence to show that AL is useful even when we have massive amounts of parallel training data.", "labels": [], "entities": [{"text": "AL", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.6353043913841248}]}, {"text": "() presents a comprehensive learning curve analysis of a phrase-based SMT system, and one of the conclusions they draw is, \"The first obvious approach is an effort to identify or produce data sets on demand (active learning, where the learning system can request translations of specific sentences, to satisfy its information needs).\"", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8656770586967468}]}, {"text": "Despite the promise of active learning for SMT there has been very little experimental work published on this issue (see.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9966593980789185}]}, {"text": "In this paper, we make several novel contributions to the area of active learning for SMT: \u2022 We use a novel framework for AL, which to our knowledge has not been used in AL experiments before.", "labels": [], "entities": [{"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9953410625457764}]}, {"text": "We assume a small amount of parallel text and a large amount of monolingual source language text.", "labels": [], "entities": []}, {"text": "Using these resources, we create a large noisy parallel text which we then iteratively improve using small injections of human translations.", "labels": [], "entities": []}, {"text": "\u2022 We provide many useful and novel features useful for AL in SMT.", "labels": [], "entities": [{"text": "AL", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.930316686630249}, {"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9537380933761597}]}, {"text": "In translation, we can leverage a whole new set of features that were out of reach for classification systems: we devise features that look at the source language, but also devise features that make an estimate of the potential utility of translations from the source, e.g. phrase pairs that could be extracted.", "labels": [], "entities": [{"text": "translation", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9748080968856812}]}, {"text": "\u2022 We show that AL can be useful in domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7931056022644043}]}, {"text": "We provide the first experimental evidence in SMT that active learning can be used to inject care-fully selected translations in order to improve SMT output in anew domain.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.994408130645752}, {"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9959801435470581}]}, {"text": "\u2022 We compare our proposed features to a random selection baseline in a simulated setting for three language pairs.", "labels": [], "entities": []}, {"text": "We also use a realistic setting: using human expert annotations in our AL system we create an improved SMT system to translate from Bangla to English, a language pair with very few resources.", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9891331791877747}, {"text": "translate from Bangla", "start_pos": 117, "end_pos": 138, "type": "TASK", "confidence": 0.8400631546974182}]}], "datasetContent": [{"text": "The SMT system we applied in our experiments is PORTAGE ( model which assigns a penalty based on the number of source words which are skipped when generating anew target phrase, and (d) a word penalty.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9895482063293457}, {"text": "PORTAGE", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9876460433006287}]}, {"text": "These different models are combined log-linearly.", "labels": [], "entities": []}, {"text": "Their weights are optimized w.r.t.", "labels": [], "entities": []}, {"text": "BLEU score using the algorithm described in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.983325719833374}]}, {"text": "This is done on a development corpus which we will call dev1 in this paper.", "labels": [], "entities": []}, {"text": "The weight vectors in n-gram and similarity methods are set to) to emphasize longer n-grams.", "labels": [], "entities": []}, {"text": "We set \u03b1 = \u03b2 = .35 for HAS, and use the 100-best list of translations when identifying candidate phrases while setting the maximum phrase length to 10.", "labels": [], "entities": [{"text": "HAS", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.6874184012413025}]}, {"text": "We set \u01eb = .5 to smooth probabilities when computing scores based on translation units.", "labels": [], "entities": [{"text": "\u01eb", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.987721860408783}]}], "tableCaptions": [{"text": " Table 1: Specification of different data sets we will use in  experiments. The target language is English in the bilin- gual sets, and the source languages are either French (Fr),  German (Ge), Spanish (Sp), or Bangla.", "labels": [], "entities": []}, {"text": " Table 2: Phrase-based utility selection is compared  with random sentence selection baseline with respect to  BLEU, wer (word error rate), and per (position indepen- dent word error rate) across three language pairs.", "labels": [], "entities": [{"text": "Phrase-based utility selection", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.8754432400067648}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9990798234939575}, {"text": "wer (word error rate)", "start_pos": 117, "end_pos": 138, "type": "METRIC", "confidence": 0.8874898056189219}, {"text": "per (position indepen- dent word error rate)", "start_pos": 144, "end_pos": 188, "type": "METRIC", "confidence": 0.6691153913736343}]}, {"text": " Table 3: Comparison of methods in domain adaptation  scenario. The bold numbers show statistically significant  improvement with respect to the baseline.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7723666429519653}]}, {"text": " Table 4: Average number of english phrases per source  language phrase, average length of the source language  phrases, number of source language phrases, and number  of phrase pairs which has been seen once in the phrase ta- bles across three language pairs (French text taken from  Hansard is abbreviated by", "labels": [], "entities": [{"text": "Hansard", "start_pos": 285, "end_pos": 292, "type": "DATASET", "confidence": 0.7377477288246155}]}]}