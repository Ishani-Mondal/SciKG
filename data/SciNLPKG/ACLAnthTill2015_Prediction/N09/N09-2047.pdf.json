{"title": [{"text": "MICA: A Probabilistic Dependency Parser Based on Tree Insertion Grammars Application Note", "labels": [], "entities": []}], "abstractContent": [{"text": "MICA is a dependency parser which returns deep dependency representations, is fast, has state-of-the-art performance, and is freely available.", "labels": [], "entities": [{"text": "MICA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6719409823417664}]}, {"text": "1 Overview This application note presents a freely available parser, MICA (Marseille-INRIA-Columbia-AT&T).", "labels": [], "entities": [{"text": "MICA", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.8633691072463989}, {"text": "Marseille-INRIA-Columbia-AT&T)", "start_pos": 75, "end_pos": 105, "type": "DATASET", "confidence": 0.8211394846439362}]}, {"text": "1 MICA has several key characteristics that make it appealing to researchers in NLP who need an off-the-shelf parser.", "labels": [], "entities": [{"text": "MICA", "start_pos": 2, "end_pos": 6, "type": "TASK", "confidence": 0.518578827381134}]}, {"text": "\u2022 MICA returns a deep dependency parse, in which dependency is defined in terms of lexical predicate-argument structure, not in terms of surface-syntactic features such as subject-verb agreement.", "labels": [], "entities": [{"text": "MICA", "start_pos": 2, "end_pos": 6, "type": "TASK", "confidence": 0.85541170835495}, {"text": "dependency parse", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7230943441390991}]}, {"text": "Function words such as auxiliaries and determiners depend on their lexical head, and strongly governed prepositions (such as to for give) are treated as co-heads rather than as syntactic heads in their own right.", "labels": [], "entities": []}, {"text": "For example, John is giving books to Mary gets the following analysis (the arc label is on the terminal).", "labels": [], "entities": []}, {"text": "giving John arc=0 is arc=adj books arc=1 to arc=co-head Mary arc=2 The arc labels for the three arguments John, books, and Mary do not change when the sentence is passivized or Mary undergoes dative shift.", "labels": [], "entities": []}, {"text": "1 We would like to thank Ryan Roth for contributing the MALT data.", "labels": [], "entities": [{"text": "MALT data", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.6974980384111404}]}, {"text": "\u2022 MICA is based on an explicit phrase-structure tree grammar extracted from the Penn Treebank.", "labels": [], "entities": [{"text": "MICA", "start_pos": 2, "end_pos": 6, "type": "TASK", "confidence": 0.7769774794578552}, {"text": "Penn Treebank", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9962968528270721}]}, {"text": "Therefore, MICA can associate dependency parses with rich linguistic information such as voice, the presence of empty subjects (PRO), wh-movement, and whether a verb heads a relative clause.", "labels": [], "entities": [{"text": "MICA", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.959077000617981}, {"text": "dependency parses", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7488013803958893}]}, {"text": "\u2022 MICA is fast (450 words per second plus 6 seconds initialization on a standard high-end machine on sentences with fewer than 200 words) and has state-of-the-art performance (87.6% unlabeled dependency accuracy, see Section 5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9170942306518555}]}, {"text": "\u2022 MICA consists of two processes: the supertag-ger, which associates tags representing rich syntactic information with the input word sequence, and the actual parser, which derives the syntactic structure from the n-best chosen supertags.", "labels": [], "entities": [{"text": "MICA", "start_pos": 2, "end_pos": 6, "type": "TASK", "confidence": 0.8927934765815735}]}, {"text": "Only the su-pertagger uses lexical information, the parser only sees the supertag hypotheses.", "labels": [], "entities": []}, {"text": "\u2022 MICA returns n-best parses for arbitrary n; parse trees are associated with probabilities.", "labels": [], "entities": []}, {"text": "A packed forest can also be returned.", "labels": [], "entities": []}, {"text": "\u2022 MICA is freely available 2 , easy to install under Linux, and easy to use.", "labels": [], "entities": [{"text": "MICA", "start_pos": 2, "end_pos": 6, "type": "DATASET", "confidence": 0.6158102750778198}]}, {"text": "(Input is one sentence per line with no special tokenization required.)", "labels": [], "entities": [{"text": "Input", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9549081921577454}]}, {"text": "There is an enormous amount of related work, and we can mention only the most salient, given space constraints.", "labels": [], "entities": []}, {"text": "Our parser is very similar to the work of (Shen and Joshi, 2005).", "labels": [], "entities": []}, {"text": "They do not employ a supertagging step, and we do not restrict our trees to spinal projections.", "labels": [], "entities": []}, {"text": "Other parsers using su-pertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We compare MICA to the MALT parser.", "labels": [], "entities": []}, {"text": "Both parsers are trained on sections 02-21 of our dependency version of the WSJ PennTreebank, and tested on Section 00, not counting true punctuation.", "labels": [], "entities": [{"text": "WSJ PennTreebank", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.7225737869739532}]}, {"text": "\"Predicted\" refers to tags (PTB-tagset POS and supertags) predicted by our taggers; \"Gold\" refers to the gold POS and supertags.", "labels": [], "entities": []}, {"text": "We tested MALT using only POS tags (MALT-POS), and POS tags as well as 1-best supertags (MALT-all).", "labels": [], "entities": [{"text": "MALT", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.6934403777122498}]}, {"text": "We provide unlabeled (\"Un\") and labeled (\"Lb\") dependency accuracy (%).", "labels": [], "entities": [{"text": "labeled (\"Lb\") dependency accuracy", "start_pos": 32, "end_pos": 66, "type": "METRIC", "confidence": 0.658386617898941}]}, {"text": "As we can see, the predicted supertags do not help MALT.", "labels": [], "entities": [{"text": "MALT", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.6271623373031616}]}, {"text": "MALT is significantly slower than MICA, running at about 30 words a second (MICA: 450 words a second", "labels": [], "entities": [{"text": "MALT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6579005122184753}, {"text": "MICA", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6489430665969849}]}], "tableCaptions": []}