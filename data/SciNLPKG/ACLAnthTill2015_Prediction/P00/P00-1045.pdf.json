{"title": [{"text": "Memory-Efficient and Thread-Safe Quasi-Destructive Graph Unification", "labels": [], "entities": []}], "abstractContent": [{"text": "In terms of both speed and memory consumption, graph unification remains the most expensive component of unification-based grammar parsing.", "labels": [], "entities": [{"text": "graph unification", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7528562843799591}, {"text": "unification-based grammar parsing", "start_pos": 105, "end_pos": 138, "type": "TASK", "confidence": 0.7935073574384054}]}, {"text": "We present a technique to reduce the memory usage of unification algorithms considerably , without increasing execution times.", "labels": [], "entities": []}, {"text": "Also, the proposed algorithm is thread-safe, providing an efficient algorithm for parallel processing as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Both in terms of speed and memory consumption, graph unification remains the most expensive component in unification-based grammar parsing.", "labels": [], "entities": [{"text": "graph unification", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.749817430973053}, {"text": "unification-based grammar parsing", "start_pos": 105, "end_pos": 138, "type": "TASK", "confidence": 0.787408192952474}]}, {"text": "Unification is a well known algorithm.", "labels": [], "entities": []}, {"text": "Prolog, for example, makes extensive use of term unification.", "labels": [], "entities": [{"text": "Prolog", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9284467101097107}, {"text": "term unification", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6919248402118683}]}, {"text": "Graph unification is slightly different.", "labels": [], "entities": [{"text": "Graph unification", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7745065689086914}]}, {"text": "Two different graph notations and an example unification are shown in and 2, respectively.", "labels": [], "entities": []}, {"text": "In typical unification-based grammar parsers, roughly 90% of the unifications fail.", "labels": [], "entities": [{"text": "unification-based grammar parsers", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.8848511377970377}]}, {"text": "Any processing to create, or copy, the result graph before the point of failure is redundant.", "labels": [], "entities": []}, {"text": "As copying is the most expensive part of unification, a great deal of research has gone in eliminating superfluous copying.", "labels": [], "entities": [{"text": "copying", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9817580580711365}, {"text": "unification", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.9895370006561279}]}, {"text": "Examples of these approaches are given in and.", "labels": [], "entities": []}, {"text": "In order to avoid superfluous copying, these algorithms incorporate control data in the graphs.", "labels": [], "entities": []}, {"text": "This has several drawbacks, as we will discuss next.", "labels": [], "entities": []}, {"text": "Memory Consumption To achieve the goal of eliminating superfluous copying, the aforementioned algorithms include administrative fields-which we will call scratch fields-in the node structure.", "labels": [], "entities": [{"text": "Memory Consumption", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.703904777765274}]}, {"text": "These fields do not attribute to the definition of the graph, but are used to efficiently guide the unification and copying process.", "labels": [], "entities": []}, {"text": "Before a graph is used in unification, or after a result graph has been copied, these fields just take up space.", "labels": [], "entities": [{"text": "unification", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9616969227790833}]}, {"text": "This is undesirable, because memory usage is of great concern in many unification-based grammar parsers.", "labels": [], "entities": []}, {"text": "This problem is especially of concern in Tomabechi's algorithm, as it increases the node size by at least 60% for typical implementations.", "labels": [], "entities": []}, {"text": "In the ideal case, scratch fields would be stored in a separate buffer allowing them to be reused for each unification.", "labels": [], "entities": []}, {"text": "The size of such a buffer would be proportional to the maximum number of nodes that are involved in a single unification.", "labels": [], "entities": []}, {"text": "Although this technique reduces memory usage considerably, it does not reduce the amount of data involved in a single unification.", "labels": [], "entities": []}, {"text": "Nevertheless, storing and loading nodes without scratch fields will be faster, because they are smaller.", "labels": [], "entities": []}, {"text": "Because scratch fields are reused, there is a high probability that they will remain in cache.", "labels": [], "entities": []}, {"text": "As the difference in speed between processor and memory continues to grow, caching is an important consideration ().", "labels": [], "entities": []}, {"text": "1 A straightforward approach to separate the scratch fields from the nodes would be to use a hash table to associate scratch structures with the addresses of nodes.", "labels": [], "entities": []}, {"text": "The overhead of a hash table, however, maybe significant.", "labels": [], "entities": [{"text": "overhead", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9973157048225403}]}, {"text": "In general, any binding mechanism is bound to require some extra work.", "labels": [], "entities": []}, {"text": "Nevertheless, considering the difference in speed between processors and memory, reducing the memory footprint may compensate for the loss of performance to some extent.", "labels": [], "entities": []}, {"text": "Symmetric Multi Processing Smallscale desktop multiprocessor systems (e.g. dual or even quad Pentium machines) are becoming more commonplace and affordable.", "labels": [], "entities": []}, {"text": "If we focus on graph unification, there are two ways to exploit their capabilities.", "labels": [], "entities": [{"text": "graph unification", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7808788120746613}]}, {"text": "First, it is possible to parallelize a single graph unification, as proposed by e.g..", "labels": [], "entities": []}, {"text": "Suppose we are unifying graph a with graph b, then we could allow multiple processors to work on the unification of a and b simultaneously.", "labels": [], "entities": []}, {"text": "We will call this parallel unification.", "labels": [], "entities": []}, {"text": "Another approach is to allow multiple graph unifications to run concurrently.", "labels": [], "entities": []}, {"text": "Suppose we are unifying graph a and bin addition to unifying graph a and c.", "labels": [], "entities": []}, {"text": "By assigning a different processor to each operation we obtain what we will call concurrent unification.", "labels": [], "entities": [{"text": "concurrent unification", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.768100768327713}]}, {"text": "Parallel unification exploits parallelism inherent of graph unification itself, whereas concurrent unification exploits parallelism at the context-free grammar backbone.", "labels": [], "entities": [{"text": "Parallel unification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7682344019412994}, {"text": "concurrent unification", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7762898802757263}]}, {"text": "As long as the number of unification operations in one parse is large, we believe it is preferable to choose concurrent unification.", "labels": [], "entities": []}, {"text": "Especially when a large number of unifications terminates quickly (e.g. due to failure), the overhead of more finely grained parallelism can be considerable.", "labels": [], "entities": []}, {"text": "In the example of concurrent unification, graph a was used in both unifications.", "labels": [], "entities": [{"text": "concurrent unification", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7117382884025574}]}, {"text": "This suggests that in order for concurrent unification to work, the input graphs need to be read only.", "labels": [], "entities": [{"text": "concurrent unification", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7085835337638855}]}, {"text": "With destructive unification algorithms this does not pose a problem, as the source graphs are copied before unification.", "labels": [], "entities": []}, {"text": "However, including scratch fields in the node structure (as Tomabechi's and Wroblewski's algorithms do) thwarts the implementation of concurrent unification, as different processors will need to write different values in these fields.", "labels": [], "entities": [{"text": "concurrent unification", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.708469420671463}]}, {"text": "One way to solve this problem is to disallow a single graph to be used in multiple unification operations simultaneously.) it is shown, however, that this will greatly impair the ability to achieve speedup.", "labels": [], "entities": []}, {"text": "Another solution is to duplicate the scratch fields in the nodes for each processor.", "labels": [], "entities": []}, {"text": "This, however, will enlarge the node size even further.", "labels": [], "entities": []}, {"text": "In other words, Tomabechi's and Wroblewski's algorithms are not suited for concurrent unification.", "labels": [], "entities": [{"text": "concurrent unification", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7812803387641907}]}], "datasetContent": [{"text": "We have tested our algorithm with a mediumsized grammar for Dutch.", "labels": [], "entities": []}, {"text": "The system was implemented in Objective-C using a fixed arity graph representation.", "labels": [], "entities": []}, {"text": "We used a test set of 22 sentences of varying length.", "labels": [], "entities": []}, {"text": "Usually, approximately 90% of the unifications fails.", "labels": [], "entities": [{"text": "unifications", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9720360636711121}]}, {"text": "On average, graphs consist of 60 nodes.", "labels": [], "entities": []}, {"text": "The experiments were run on a Pentium III 600EB (256 KB L2 cache) box, with 128 MB memory, running Linux.", "labels": [], "entities": []}, {"text": "We tested both memory usage and execution time for various configurations.", "labels": [], "entities": []}, {"text": "The results are shown in and 8.", "labels": [], "entities": []}, {"text": "It includes aversion of Tomabechi's algorithm.", "labels": [], "entities": []}, {"text": "The node size for this implementation is 20 bytes.", "labels": [], "entities": []}, {"text": "For the proposed algorithm we have included several versions: a basic implementation, a packed version, aversion with deferred copying, and aversion with structure sharing.", "labels": [], "entities": []}, {"text": "The basic implementation has anode size of 8 bytes, the others have a variable node size.", "labels": [], "entities": []}, {"text": "Whenever applicable, we applied the same optimizations to all algorithms.", "labels": [], "entities": []}, {"text": "We also tested the speedup on a dual Pentium II 266 Mhz.", "labels": [], "entities": [{"text": "speedup", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9692574143409729}]}, {"text": "Each processor was assigned its own scratch tables.", "labels": [], "entities": []}, {"text": "Apart from that, no changes to the algorithm were required.", "labels": [], "entities": []}, {"text": "For more details on the multi-processor implementation, see).", "labels": [], "entities": []}, {"text": "The memory utilization results show significant improvements for our approach.", "labels": [], "entities": []}, {"text": "6 Packing decreased memory utilization by almost 40%.", "labels": [], "entities": []}, {"text": "Structure sharing roughly halved this once more.", "labels": [], "entities": [{"text": "Structure sharing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.696391150355339}]}, {"text": "The third condition prohibited sharing in less than 2% of the cases where it would be possible in Tomabechi's approach.", "labels": [], "entities": []}, {"text": "shows that our algorithm does not increase execution times.", "labels": [], "entities": []}, {"text": "Our algorithm even scrapes off roughly 7% of the total parsing time.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9718190431594849}]}, {"text": "This speedup can be attributed to improved cache utilization.", "labels": [], "entities": []}, {"text": "We verified this by running the same tests with cache disabled.", "labels": [], "entities": []}, {"text": "This made our algorithm actually run slower than Tomabechi's algorithm.", "labels": [], "entities": []}, {"text": "Deferred copying did not improve performance.", "labels": [], "entities": [{"text": "Deferred copying", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5337194502353668}]}, {"text": "The additional overhead of dereferencing during subsumption was not compensated by the savings on copying.", "labels": [], "entities": []}, {"text": "Structure sharing did not significantly alter the performance as well.", "labels": [], "entities": []}, {"text": "Although, this version uses less memory, it has to perform additional work.", "labels": [], "entities": []}, {"text": "Running the same tests on machines with less memory showed a clear performance advantage for the algorithms using less memory, because paging could be avoided.", "labels": [], "entities": []}], "tableCaptions": []}