{"title": [{"text": "Distribution-Based Pruning of Backoff Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a distribution-based pruning of n-gram backoff language models.", "labels": [], "entities": []}, {"text": "Instead of the conventional approach of pruning n-grams that are infrequent in training data, we prune n-grams that are likely to be infrequent in anew document.", "labels": [], "entities": []}, {"text": "Our method is based on the n-gram distribution i.e. the probability that an n-gram occurs in anew document.", "labels": [], "entities": []}, {"text": "Experimental results show that our method performed 7-9% (word perplexity reduction) better than conventional cutoff methods.", "labels": [], "entities": [{"text": "word perplexity reduction", "start_pos": 58, "end_pos": 83, "type": "METRIC", "confidence": 0.8705671628316244}]}], "introductionContent": [{"text": "Statistical language modelling (SLM) has been successfully applied to many domains such as speech recognition, information retrieval (), and spoken language understanding.", "labels": [], "entities": [{"text": "Statistical language modelling (SLM)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8858733077843984}, {"text": "speech recognition", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7913280725479126}, {"text": "information retrieval", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.7861925065517426}, {"text": "spoken language understanding", "start_pos": 141, "end_pos": 170, "type": "TASK", "confidence": 0.6492751240730286}]}, {"text": "In particular, n-gram language model has been demonstrated to be highly effective for these domains.", "labels": [], "entities": []}, {"text": "N-gram LM estimates the probability of a word given previous words, P(w n |w 1 ,\u2026,w n-1 ).", "labels": [], "entities": []}, {"text": "In applying an SLM, it is usually the case that more training data will improve a language model.", "labels": [], "entities": [{"text": "SLM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9588389992713928}]}, {"text": "However, as training data size increases, LM size increases, which can lead to models that are too large for practical use.", "labels": [], "entities": []}, {"text": "To deal with the problem, count cutoff is widely used to prune language models.", "labels": [], "entities": []}, {"text": "The cutoff method deletes from the LM those n-grams that occur infrequently in the training data.", "labels": [], "entities": []}, {"text": "The cutoff method assumes that if an n-gram is infrequent in training data, it is also infrequent in testing data.", "labels": [], "entities": []}, {"text": "But in the real world, training data rarely matches testing data perfectly.", "labels": [], "entities": []}, {"text": "Therefore, the count cutoff method is not perfect.", "labels": [], "entities": []}, {"text": "In this paper, we propose a distribution-based cutoff method.", "labels": [], "entities": []}, {"text": "This approach estimates if an n-gram is \"likely to be infrequent in testing data\".", "labels": [], "entities": []}, {"text": "To determine this likelihood, we divide the training data into partitions, and use a cross-validation-like approach.", "labels": [], "entities": []}, {"text": "Experiments show that this method performed 7-9% (word perplexity reduction) better than conventional cutoff methods.", "labels": [], "entities": [{"text": "word perplexity reduction", "start_pos": 50, "end_pos": 75, "type": "METRIC", "confidence": 0.8256880044937134}]}, {"text": "In section 2, we discuss prior SLM research, including backoff bigram LM, perplexity, and related works on LM pruning methods.", "labels": [], "entities": [{"text": "SLM", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.958180844783783}]}, {"text": "In section 3, we propose anew criterion for LM pruning based on n-gram distribution, and discuss in detail how to estimate the distribution.", "labels": [], "entities": []}, {"text": "In section 4, we compare our method with count cutoff, and present experimental results in perplexity.", "labels": [], "entities": []}, {"text": "Finally, we present our conclusions in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report the experimental results on bigram pruning based on distribution versus count cutoff pruning method.", "labels": [], "entities": []}, {"text": "In conventional approaches, a document is defined as the subunit of training data for term distribution estimating.", "labels": [], "entities": [{"text": "term distribution estimating", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6445629596710205}]}, {"text": "But fora very large training corpus that consists of millions of documents, the estimation for the bigram distribution is very time-consuming.", "labels": [], "entities": []}, {"text": "To cope with this problem, we use a cluster of documents as the subunit.", "labels": [], "entities": []}, {"text": "As the number of clusters can be controlled, we can define an efficient computation method, and optimise the clustering algorithm.", "labels": [], "entities": []}, {"text": "In what follows, we will report the experimental results with document and cluster being defined as the subunit, respectively.", "labels": [], "entities": []}, {"text": "In our experiments, documents are clustered in three ways: by similar domain, style, or time.", "labels": [], "entities": []}, {"text": "In all experiments described below, we use an open testing data consisting of 15 million characters that have been proofread and balanced among domain, style and time.", "labels": [], "entities": []}, {"text": "Training data are obtained from newspaper (People's Daily) and novels.", "labels": [], "entities": [{"text": "newspaper (People's Daily)", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.771533211072286}]}, {"text": "shows the results when we define a document as the subunit.", "labels": [], "entities": []}, {"text": "We used approximately 450 million characters of People's Daily training data, which consists of 39708 documents.", "labels": [], "entities": [{"text": "People's Daily training data", "start_pos": 48, "end_pos": 76, "type": "DATASET", "confidence": 0.9588506340980529}]}, {"text": "shows the results when we define a domain cluster as the subunit.", "labels": [], "entities": []}, {"text": "We also used approximately 450 million characters of People's Daily training data.", "labels": [], "entities": [{"text": "People's Daily training data", "start_pos": 53, "end_pos": 81, "type": "DATASET", "confidence": 0.9664807200431824}]}, {"text": "To cluster the documents, we used an SVM classifier developed by Platt to cluster documents of similar domains together automatically, and obtain a domain hierarchy incrementally.", "labels": [], "entities": []}, {"text": "We also added a constraint to balance the size of each cluster, and finally we obtained 105 clusters.", "labels": [], "entities": []}, {"text": "It turns out that using domain clusters as subunits performs almost as well as the case of documents as subunits.", "labels": [], "entities": []}, {"text": "Furthermore, we found that by using the pruning criterion based on bigram distribution, a lot of domain-specific bigrams are pruned.", "labels": [], "entities": []}, {"text": "It then results in a relatively domain-independent language model.", "labels": [], "entities": []}, {"text": "Therefore, we call this pruning method domain subtraction based pruning.", "labels": [], "entities": []}, {"text": "shows the results when we define a style cluster as the subunit.", "labels": [], "entities": []}, {"text": "For this experiment, we used 220 novels written by different writers, each approximately 500 kilonbytes in size, and defined each novel as a style cluster.", "labels": [], "entities": []}, {"text": "Just like in domain clustering, we found that by using the pruning criterion based on bigram distribution, a lot of style-specific bigrams are pruned.", "labels": [], "entities": [{"text": "domain clustering", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.726449728012085}]}, {"text": "It then results in a relatively style-independent language model.", "labels": [], "entities": []}, {"text": "Therefore, we call this pruning method style subtraction based pruning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word perplexity comparison of  different bigram distribution models.", "labels": [], "entities": [{"text": "Word perplexity comparison", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.743626336256663}]}]}