{"title": [{"text": "Statistical parsing with an automatically-extracted tree adjoining grammar", "labels": [], "entities": [{"text": "Statistical parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8665678203105927}]}], "abstractContent": [{"text": "We discuss the advantages of lexical-ized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7532685995101929}, {"text": "Penn Treebank", "start_pos": 192, "end_pos": 205, "type": "DATASET", "confidence": 0.9943397641181946}]}, {"text": "We \ud97b\udf59nd that this induction method is an improvement over the EM-based method of Hwa, 1998, and that the induced model yields results comparable to lexicalized PCFG.", "labels": [], "entities": []}], "introductionContent": [{"text": "Why use tree-adjoining grammar for statistical parsing?", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8368048965930939}]}, {"text": "Given that statistical natural language processing is concerned with the probable rather than the possible, it is not because TAG can describe constructions like arbitrarily large Dutch verb clusters.", "labels": [], "entities": []}, {"text": "Rather, what makes TAG useful for statistical parsing are the structural descriptions it assigns to breadand-butter sentences.", "labels": [], "entities": [{"text": "TAG", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9536855816841125}, {"text": "statistical parsing", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7401486933231354}]}, {"text": "The approach of Chelba and Jelinek 1998 to language modeling is illustrative: even though the probability estimate of w appearing as the kth word can be conditioned on the entire history w 1 ; : : : ; w k,1 , the quantity of available training data limits the usable context to about two words|but which two?", "labels": [], "entities": [{"text": "language modeling", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7892504036426544}]}, {"text": "A trigram model chooses w k,1 and w k,2 and works quite well; a model which chose w k,7 and w k,11 would probably work less well.", "labels": [], "entities": []}, {"text": "But chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model.", "labels": [], "entities": []}, {"text": "Thus the virtual grammar serves to structure the history so that the two most useful words can be chosen, even though the structure of the problem itself is entirely linear.", "labels": [], "entities": []}, {"text": "Similarly, nothing about the parsing problem requires that we construct any structure other than phrase structure.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9651482105255127}]}, {"text": "But beginning with Magerman, 1995 statistical parsers have used bilexical dependencies with great success.", "labels": [], "entities": []}, {"text": "Since these dependencies are not encoded in plain phrase-structure trees, the standard approach has been to let the lexical heads percolate up the tree, so that when one lexical head is immediately dominated by another, it is understood to be dependent on it.", "labels": [], "entities": []}, {"text": "EEectively, a dependency structure is made parasitic on the phrase structure so that they can be generated together by a context-free model.", "labels": [], "entities": [{"text": "EEectively", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7786597609519958}]}, {"text": "However, this solution is not ideal.", "labels": [], "entities": []}, {"text": "Aside from cases where context-free derivations are incapable of encoding both constituency and dependency which are somewhat isolated and not of great interest for statistical parsing there are common cases where percolation of single heads is not suucient to encode dependencies correctly|for example, relative clause attachment or raisinggauxiliary verbs see Section 3.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.7594550251960754}, {"text": "relative clause attachment", "start_pos": 304, "end_pos": 330, "type": "TASK", "confidence": 0.7530556519826254}]}, {"text": "More complicated grammar transformations are necessary.", "labels": [], "entities": []}, {"text": "A more suitable approach is to employ a grammar formalism which produces structural descriptions that can encode both constituency and dependency.", "labels": [], "entities": []}, {"text": "Lexicalized TAG is such a formalism, because it assigns to each sentence not only a parse tree, which is built out of elementary trees and is interpreted as encoding constituency, but a derivation tree, which records how the various elementary trees were combined together and is commonly intepreted as encoding dependency.", "labels": [], "entities": [{"text": "Lexicalized TAG", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5258774608373642}]}, {"text": "The ability of probabilistic LTAG to model bilexical dependencies was noted early on by.", "labels": [], "entities": []}, {"text": "It turns out that there are other pieces of contextual information that need to be explicitly accounted for in a CFG by grammar transformations but come for free in a TAG.", "labels": [], "entities": []}, {"text": "We discuss a few such cases in Section 3.", "labels": [], "entities": [{"text": "Section 3", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.8122093379497528}]}, {"text": "In Sections 4 and 5 we describe an experiment to test the parsing accuracy of a probabilistic TAG extracted automatically from the Penn Treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9548399448394775}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.8935214281082153}, {"text": "Penn Treebank", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.9947651922702789}]}, {"text": "We \ud97b\udf59nd that the automatically-extracted grammar gives an improvement over the EM-based induction method of, and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for improvement.", "labels": [], "entities": []}, {"text": "We emphasize that TAG is attractive not because it can do things that CFG cannot, but because it does everything that CFG can, only more cleanly.", "labels": [], "entities": [{"text": "TAG", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.934203565120697}]}, {"text": "This is where the analogy with Chelba and Jelinek, 1998 breaks down.", "labels": [], "entities": []}, {"text": "Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PT AG framework; we conclude by ooering two such possibilities.", "labels": [], "entities": [{"text": "PT AG framework", "start_pos": 138, "end_pos": 153, "type": "DATASET", "confidence": 0.8715548714001974}]}], "datasetContent": [], "tableCaptions": []}