{"title": [{"text": "Headline Generation Based on Statistical Translation", "labels": [], "entities": [{"text": "Headline Generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6892471760511398}, {"text": "Statistical Translation", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7539534568786621}]}], "abstractContent": [{"text": "Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8807380199432373}]}, {"text": "An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding.", "labels": [], "entities": []}, {"text": "A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9745868444442749}, {"text": "statistical machine translation", "start_pos": 130, "end_pos": 161, "type": "TASK", "confidence": 0.6364666720231374}]}, {"text": "The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language.", "labels": [], "entities": []}, {"text": "This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus.", "labels": [], "entities": [{"text": "term ordering", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.6924194693565369}]}], "introductionContent": [{"text": "Generating effective summaries requires the ability to select, evaluate, order and aggregate items of information according to their relevance to a particular subject or fora particular purpose.", "labels": [], "entities": [{"text": "Generating effective summaries", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7012354532877604}]}, {"text": "Most previous work on summarization has focused on extractive summarization: selecting text spans -either complete sentences or paragraphs -from the original document.", "labels": [], "entities": [{"text": "summarization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9901565313339233}, {"text": "extractive summarization", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.5304148495197296}]}, {"text": "These extracts are Vibhu Mittal is now at Xerox PARC, 3333 Coyote Hill Road, Palo Alto, CA 94304, USA.", "labels": [], "entities": [{"text": "Xerox PARC, 3333 Coyote Hill Road", "start_pos": 42, "end_pos": 75, "type": "DATASET", "confidence": 0.8119053840637207}]}, {"text": "e-mail: vmittal@parc.xerox.com; Michael Witbrock's initial work on this system was performed whilst at Just Research.", "labels": [], "entities": [{"text": "Just Research", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.8714732527732849}]}, {"text": "then arranged in a linear order (usually the same order as in the original document) to form a summary document.", "labels": [], "entities": []}, {"text": "There are several possible drawbacks to this approach, one of which is the focus of this paper: the inability to generate coherent summaries shorter than the smallest textspans being considered -usually a sentence, and sometimes a paragraph.", "labels": [], "entities": []}, {"text": "This can be a problem, because in many situations, a short headline style indicative summary is desired.", "labels": [], "entities": []}, {"text": "Since, in many cases, the most important information in the document is scattered across multiple sentences, this is a problem for extractive summarization; worse, sentences ranked best for summary selection often tend to be even longer than the average sentence in the document.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.8427083790302277}, {"text": "summary selection", "start_pos": 190, "end_pos": 207, "type": "TASK", "confidence": 0.7671775221824646}]}, {"text": "This paper describes an alternative approach to summarization capable of generating summaries shorter than a sentence, some examples of which are given in.", "labels": [], "entities": [{"text": "summarization", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9925093650817871}]}, {"text": "It does so by building statistical models for content selection and surface realization.", "labels": [], "entities": [{"text": "content selection", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.74518883228302}, {"text": "surface realization", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7838082313537598}]}, {"text": "This paper reviews the framework, discusses some of the pros and cons of this approach using examples from our corpus of news wire stories, and presents an initial evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Zero level-Model: The system was trained on approximately 25,000 news articles from Reuters dated between 1/Jan/1997 and 1/Jun/1997.", "labels": [], "entities": [{"text": "Reuters dated between 1/Jan/1997 and 1/Jun/1997", "start_pos": 84, "end_pos": 131, "type": "DATASET", "confidence": 0.8638091087341309}]}, {"text": "After punctuation had been stripped, these contained about 44,000 unique tokens in the articles and slightly more than 15,000 tokens in the headlines.", "labels": [], "entities": []}, {"text": "Representing all the pairwise conditional probabilities for all combinations of article and headline words 3 added significant complexity, so we simplified our model further and investigated the effectiveness of training on a more limited vocabulary: the set of all the words that appeared in any of the headlines.", "labels": [], "entities": []}, {"text": "Conditional probabilities for words in the headlines that also appeared in the articles were computed.", "labels": [], "entities": []}, {"text": "As discussed earlier, in our zero-level model, the system was also trained on bigram transition probabilities as an approximation to the headline syntax.", "labels": [], "entities": []}, {"text": "Sample output from the system using this simplified model is shown in.", "labels": [], "entities": []}, {"text": "The zero-level model, that we have discussed so far, works surprisingly well, given its strong independence assumptions and very limited vocabulary.", "labels": [], "entities": []}, {"text": "There are problems, some of which are most likely due to lack of sufficient training data.", "labels": [], "entities": []}, {"text": "Ideally, we should want to evaluate the system's performance in terms both of content selection success and realization quality.", "labels": [], "entities": []}, {"text": "However, it is hard to computationally evaluate coherence and phrasing effectiveness, so we have, to date, restricted ourselves to the content aspect, which is more amenable to a quantitative analysis.", "labels": [], "entities": []}, {"text": "(We have experience doing much more laborious human eval- This requires a matrix with 660 million entries, or about 2.6GB of memory.", "labels": [], "entities": []}, {"text": "This requirement can be significantly reduced by using a threshold to prune values and using a sparse matrix representation for the remaining pairs.", "labels": [], "entities": []}, {"text": "However, inertia and the easy availability of the CMU-Cambridge Statistical Modeling Toolkit -which generates the full matrix -have so far conspired to prevent us from exercising that option.", "labels": [], "entities": [{"text": "CMU-Cambridge Statistical Modeling Toolkit", "start_pos": 50, "end_pos": 92, "type": "DATASET", "confidence": 0.8845198154449463}]}, {"text": "An alternative approach to limiting the size of the mappings that need to be estimated would be to use only the top \u0082 words, where \u0082 could have a small value in the hundreds, rather than the thousands, together with the words appearing in the headlines.", "labels": [], "entities": []}, {"text": "This would limit the size of the model while still allowing more flexible content selection.", "labels": [], "entities": []}, {"text": "We estimate that approximately 100MB of training data would give us reasonable estimates for the models that we would like to evaluate; we had access to much less.", "labels": [], "entities": []}, {"text": "Figure 3: Sample article (with original headline) and system generated output using the simplest, zero-level, lexical model.", "labels": [], "entities": []}, {"text": "Numbers to the right are log probabilities of the string, and search beam size, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluating the use of the simplest lexi- cal model for content selection on 1000 Reuters  news articles. The headline length given is that a  which the overlap between the terms in the target  headline and the generated summary was maxi- mized. The percentage of complete matches in- dicates how many of the summaries of a given  length had all their terms included in the target  headline.", "labels": [], "entities": []}, {"text": " Table 2: Overlap between terms in the generated headlines and in the original headlines and extracted  summary sentences, respectively, of the article. Using Part of Speech (POS) and information about a  token's location in the source document, in addition to the lexical information, helps improve perfor- mance on the Reuters' test set.", "labels": [], "entities": [{"text": "perfor- mance", "start_pos": 300, "end_pos": 313, "type": "METRIC", "confidence": 0.9518207112948099}, {"text": "Reuters' test set", "start_pos": 321, "end_pos": 338, "type": "DATASET", "confidence": 0.9679958621660868}]}]}