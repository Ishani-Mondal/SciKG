{"title": [{"text": "Using Confidence Bands for Parallel Texts Alignment", "labels": [], "entities": [{"text": "Parallel Texts Alignment", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7077270746231079}]}], "abstractContent": [{"text": "This paper describes a language independent method for alignment of parallel texts that makes use of homograph tokens for each pair of languages.", "labels": [], "entities": [{"text": "alignment of parallel texts", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.884041041135788}]}, {"text": "In order to filter out tokens that may cause misalignment, we use confidence bands of linear regression lines instead of heuristics which are not theoretically supported.", "labels": [], "entities": []}, {"text": "This method was originally inspired on work done by Pascale Fung and Kathleen McKeown, and Melamed, providing the statistical support those authors could not claim.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human compiled bilingual dictionaries do not cover every term translation, especially when it comes to technical domains.", "labels": [], "entities": []}, {"text": "Moreover, we can no longer afford to waste human time and effort building manually these ever changing and incomplete databases or design language specific applications to solve this problem.", "labels": [], "entities": []}, {"text": "The need for an automatic language independent task for equivalents extraction becomes clear in multilingual regions like Hong Kong, Macao, Quebec, the European Union, where texts must be translated daily into eleven languages, or even in the U.S.A. where Spanish and English speaking communities are intermingled.", "labels": [], "entities": [{"text": "equivalents extraction", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.944000244140625}]}, {"text": "Parallel texts (texts that are mutual translations) are valuable sources of information for bilingual lexicography.", "labels": [], "entities": []}, {"text": "However, they are not of much use unless a computational system may find which piece of text in one language corresponds to which piece of text in the other language.", "labels": [], "entities": []}, {"text": "In order to achieve this, they must be aligned first, i.e. the various pieces of text must be put into correspondence.", "labels": [], "entities": []}, {"text": "This makes the translations extraction task easier and more reliable.", "labels": [], "entities": [{"text": "translations extraction", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.9835070669651031}]}, {"text": "Alignment is usually done by finding correspondence points -sequences of characters with the same form in both texts (homographs, e.g. numbers, proper names, punctuation marks), similar forms (cognates, like Region and Regi\u00e3o in English and Portuguese, respectively) or even previously known translations.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9740679860115051}]}, {"text": "Pascale present an alignment algorithm that uses term translations as correspondence points between English and Chinese.", "labels": [], "entities": []}, {"text": "Melamed (1999) aligns texts using correspondence points taken either from orthographic cognates or from a seed translation lexicon.", "labels": [], "entities": []}, {"text": "However, although the heuristics both approaches use to filter noisy points maybe intuitively quite acceptable, they are not theoretically supported by Statistics.", "labels": [], "entities": []}, {"text": "The former approach considers a candidate correspondence point reliable as long as, among some other constraints, \" it is not too faraway from the diagonal\" (Pascale) of a rectangle whose sides sizes are proportional to the lengths of the texts in each language (henceforth, 'the golden translation diagonal').", "labels": [], "entities": []}, {"text": "The latter approach uses other filtering parameters: maximum point ambiguity level, point dispersion and angle deviation.", "labels": [], "entities": []}, {"text": "Ant\u00f3nio propose a method to filter candidate correspondence points generated from homograph words which occur only once in parallel texts (hapaxes) using linear regressions and statistically supported noise filtering methodologies.", "labels": [], "entities": []}, {"text": "The method avoids heuristic filters and they claim high precision alignments.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9620923399925232}]}, {"text": "In this paper, we will extend this work by defining a linear regression line with all points generated from homographs with equal frequencies in parallel texts.", "labels": [], "entities": []}, {"text": "We will filter out those points which lie outside statistically defined confidence bands.", "labels": [], "entities": []}, {"text": "Our method will repeatedly use a standard linear regression line adjustment technique to filter unreliable points until there is no misalignment.", "labels": [], "entities": []}, {"text": "Points resulting from this filtration are chosen as correspondence points.", "labels": [], "entities": []}, {"text": "The following section will discuss related work.", "labels": [], "entities": []}, {"text": "The method is described in section 2 and we will evaluate and compare the results in section 3.", "labels": [], "entities": []}, {"text": "Finally, we present conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran our alignment algorithm on the parallel texts of 10 language pairs as described in section 2.: Average number of correspondence points in the first non-misalignment (average ratio of filtered and initial candidate correspondence points inside brackets).", "labels": [], "entities": []}, {"text": "On average, we end up with about 2% of the initial correspondence points which means that we are able to break a text in about 90 segments (ranging from 70 words to 12 pages per segment A for the Debates).", "labels": [], "entities": []}, {"text": "An average of just three filtrations are needed: the Histogram filter plus two filtrations with the Confidence Bands.", "labels": [], "entities": [{"text": "Histogram filter", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.6511555016040802}]}, {"text": "The figure below shows an example of a misaligning correspondence point.", "labels": [], "entities": []}, {"text": "Had we restricted ourselves to using homographs which occur only once (hapaxes), we would get about one third of the final points).", "labels": [], "entities": []}, {"text": "Hapaxes turnout to be good candidate correspondence points because they work like cognates that are not mistaken for others within the full text scope).", "labels": [], "entities": []}, {"text": "When they are in similar positions, they turnout to be reliable correspondence points.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: A sample of the distances between  expected and real positions of noisy points in", "labels": [], "entities": []}, {"text": " Table 5: Comparison with the Jacal alignment  (Michel Simard and Pierre Plamondon", "labels": [], "entities": [{"text": "Jacal alignment", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.7304485142230988}]}]}