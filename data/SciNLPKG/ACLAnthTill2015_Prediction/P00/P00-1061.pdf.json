{"title": [{"text": "Lexicalized Stochastic Modeling of Constraint-Based Grammars using Log-Linear Measures and EM Training", "labels": [], "entities": [{"text": "Lexicalized Stochastic Modeling of Constraint-Based Grammars", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7528525938590368}]}], "abstractContent": [{"text": "We present anew approach to stochastic modeling of constraint-based grammars that is based on log-linear models and uses EM for estimation from unannotated data.", "labels": [], "entities": [{"text": "stochastic modeling of constraint-based grammars", "start_pos": 28, "end_pos": 76, "type": "TASK", "confidence": 0.7159207463264465}]}, {"text": "The techniques are applied to an LFG grammar for German.", "labels": [], "entities": []}, {"text": "Evaluation on an exact match task yields 86% precision for an ambiguity rate of 5.4, and 90% precision on a subcat frame match for an ambiguity rate of 25.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9995386600494385}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9990760087966919}]}, {"text": "Experimental comparison to training from a parsebank shows a 10% gain from EM training.", "labels": [], "entities": []}, {"text": "Also, anew class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Stochastic parsing models capturing contextual constraints beyond the dependencies of probabilistic context-free grammars (PCFGs) are currently the subject of intensive research.", "labels": [], "entities": []}, {"text": "An interesting feature common to most such models is the incorporation of contextual dependencies on individual head words into rulebased probability models.", "labels": [], "entities": []}, {"text": "Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g.,,, or.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.775177389383316}]}, {"text": "However, it is still an open question which kind of lexicalization, e.g., statistics on individual words or statistics based upon word classes, is the best choice.", "labels": [], "entities": []}, {"text": "Secondly, these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences.", "labels": [], "entities": []}, {"text": "In all of the cited approaches, the Penn Wall Street Journal) is used, the availability of which ob viates the standard eeort required for treebank trainingghandannotating large corpora of speciic domains of speciic languages with speciic parse types.", "labels": [], "entities": [{"text": "Penn Wall Street Journal)", "start_pos": 36, "end_pos": 61, "type": "DATASET", "confidence": 0.9717585444450378}]}, {"text": "Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm yields poor results unless at least partial annotation is applied.", "labels": [], "entities": []}, {"text": "Experimental results connrming this wisdom have beenpresented, e.g., by and for EM training of Hidden Markov Models and PCFGs.", "labels": [], "entities": []}, {"text": "In this paper, we present anew lexicalized stochastic model for constraint-based grammars that employs a combination of headword frequencies and EM-based clustering for grammar lexicalization.", "labels": [], "entities": []}, {"text": "Furthermore, we make crucial use of EM for estimating the parameters of the stochastic grammar from unannotated data.", "labels": [], "entities": [{"text": "EM", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.6115538477897644}]}, {"text": "Our usage of EM was initiated by the current lack of large uniicationbased treebanks for German.", "labels": [], "entities": []}, {"text": "However, our experimental results also show an exception to the common wisdom of the insuuciency of EM for highly accurate statistical modeling.", "labels": [], "entities": []}, {"text": "Our approach to lexicalized stochastic modeling is based on the parametric family of loglinear probability models, which i s u s ed to d e\ud97b\udf59ne a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German.", "labels": [], "entities": []}, {"text": "In previous work on log-linear models for LFG by, pseudolikelihood estimation from annotated corpora has been introduced and experimented with on a small scale.", "labels": [], "entities": []}, {"text": "However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available.", "labels": [], "entities": []}, {"text": "Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data ).", "labels": [], "entities": []}, {"text": "We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text.", "labels": [], "entities": []}, {"text": "In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG.", "labels": [], "entities": [{"text": "parsing 36,000 newspaper sentences", "start_pos": 73, "end_pos": 107, "type": "TASK", "confidence": 0.8417479395866394}, {"text": "German LFG", "start_pos": 117, "end_pos": 127, "type": "DATASET", "confidence": 0.7959128022193909}]}, {"text": "Experimental evaluation of our models on an exact-match task (i.e. percentage of exact match of most probable parse with correct parse) on 550 manually examined examples with on average 5.4 analyses gave 86% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.9987447261810303}]}, {"text": "Another evaluation on a verb frame recognition task (i.e. percentage of agreement between subcategorization frames of main verb of most probable parse and correct parse) gave 90% precision on 375 manually disambiguated examples with an average ambiguity of 25.", "labels": [], "entities": [{"text": "verb frame recognition task", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.7031812965869904}, {"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.999377429485321}]}, {"text": "Clearly, a direct comparison of these results to stateof-the-art statistical parsers cannot bemade because of diierent training and test data and other evaluation measures.", "labels": [], "entities": []}, {"text": "However, we would like to draw the following conclusions from our experiments: \ud97b\udf59 The problem of chaotic convergence behaviour of EM estimation can besolved for log-linear models.", "labels": [], "entities": [{"text": "EM estimation", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.9324085712432861}]}, {"text": "\ud97b\udf59 EM does help constraint-based grammars, e.g. using about 10 times more sentences and about 100 times more parses for EM training than for training from an automatically constructed parsebank can improve precision by about 10%.", "labels": [], "entities": [{"text": "precision", "start_pos": 205, "end_pos": 214, "type": "METRIC", "confidence": 0.9985635876655579}]}, {"text": "\ud97b\udf59 Class-based lexicalization can yield again in precision of about 10%.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9992044568061829}]}, {"text": "In the rest of this paper we introduce incomplete-data estimation for log-linear models, and present the actual design of our models (Sec. 3) and report our experimental results (Sec. 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our models, we constructed two diierent test corpora.", "labels": [], "entities": []}, {"text": "We \ud97b\udf59rst parsed with the LFG grammar 550 sentences which are used for illustrative purposes in the foreign language learner's grammar of.", "labels": [], "entities": [{"text": "LFG grammar", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.953174889087677}]}, {"text": "Ina next step, the correct parse was indicated by a human disambiguator, according to the reading intended in.", "labels": [], "entities": []}, {"text": "Thus a precise indication of correct c/f-structure pairs was possible.", "labels": [], "entities": []}, {"text": "However, the average ambiguity of this corpus is only 5.4 parses per sentence, for sentences with on average 7.5 words.", "labels": [], "entities": []}, {"text": "In order to evaluate on sentences with higher ambiguity rate, we manually disambiguated further 375 sentences of LFG-parsed newspaper text.", "labels": [], "entities": [{"text": "LFG-parsed newspaper text", "start_pos": 113, "end_pos": 138, "type": "DATASET", "confidence": 0.9094986319541931}]}, {"text": "The sentences of this corpus have on average 25 parses and 11.2 words.", "labels": [], "entities": []}, {"text": "We tested our models on two evaluation tasks.", "labels": [], "entities": []}, {"text": "The statistical disambiguator was tested on an \ud97b\udf59exact matchh task, where exact correspondence of the full c/f-structure pair of the hand-annotated correct parse and the most probable parse is checked.", "labels": [], "entities": []}, {"text": "Another evaluation was done on a frame matchh task, where exact correspondence only of the subcategorization frame of the main verb of the most probable parse and the correct parse is checked.", "labels": [], "entities": []}, {"text": "Clearly, the latter task involves a smaller eeective ambiguity rate, and is thus to be interpreted as an evaluation of the combined system of highly-constrained symbolic parsing and statistical disambiguation.", "labels": [], "entities": [{"text": "symbolic parsing", "start_pos": 161, "end_pos": 177, "type": "TASK", "confidence": 0.6686964631080627}]}, {"text": "Performance on these two evaluation tasks was assessed according to the following evaluation measures: Precision = #correct #correct+#incorrect , EEectiveness = #correct #correct+#incorrect+#don't know . \ud97b\udf59Correctt and \ud97b\udf59incorrectt speciies a success/failure on the respective e valuation taskss don't knoww cases are cases where the system is unable to make a decision, i.e. cases with more than one most probable parse.", "labels": [], "entities": [{"text": "Precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.990997314453125}, {"text": "EEectiveness", "start_pos": 146, "end_pos": 158, "type": "METRIC", "confidence": 0.9952051639556885}, {"text": "\ud97b\udf59Correctt and \ud97b\udf59incorrectt speciies", "start_pos": 204, "end_pos": 238, "type": "METRIC", "confidence": 0.7107103864351908}]}, {"text": "For each task and each test corpus, we calculated a random baseline by averaging over several models with randomly chosen parameter values.", "labels": [], "entities": []}, {"text": "This baseline measures the disambiguation power of the pure symbolic parser.", "labels": [], "entities": []}, {"text": "The results of an exact-match evaluation on the Helbig-Buscha corpus is shown in.", "labels": [], "entities": [{"text": "Helbig-Buscha corpus", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.8480688035488129}]}, {"text": "The random baseline was around 33% for this case.", "labels": [], "entities": [{"text": "random baseline", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9522266983985901}]}, {"text": "The columns list diierent models according to their property-vectors.", "labels": [], "entities": []}, {"text": "\ud97b\udf59Basicc models consist of 190 conngurational properties as described in Sec.", "labels": [], "entities": []}, {"text": "\ud97b\udf59Lexical-izedd models are extended by 45 lexical predisambiguation properties as described in Sec.", "labels": [], "entities": []}, {"text": "\ud97b\udf59Selected + lexicalizedd models result from a simple property selection procedure where a cutoo on the numberof parses with non-negative value of the property-functions was set.", "labels": [], "entities": []}, {"text": "Estimation of basic models from complete data gave 68% precision (P), whereas training lexicalized and selected models from incomplete data gave 86.1% precision, which is an improvement of 18%.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.875087320804596}, {"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9988062381744385}]}, {"text": "Comparing lexicalized models in the estimation method shows that incomplete-data estimation gives an improvement of 12% precision over training from the parsebank.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9993352293968201}]}, {"text": "A comparison of models trained from incomplete data shows that lexicalization yields again of 13% in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9989155530929565}]}, {"text": "Note also the gain in eeectiveness (E) due to the pre-disambigution routine included in the lexicalized properties.", "labels": [], "entities": [{"text": "eeectiveness (E)", "start_pos": 22, "end_pos": 38, "type": "METRIC", "confidence": 0.8331881165504456}]}, {"text": "The gain due to property selection both in precision and eeectiveness is minimal.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9994750618934631}]}, {"text": "A similar pattern of performance arises in an exact match evaluation on the newspaper corpus with an ambiguity rate of 25.", "labels": [], "entities": [{"text": "newspaper corpus", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.9629378616809845}]}, {"text": "The lexicalized and selected model trained from incomplete data achieved here 60.1% precision and 57.9% eeectiveness, fora random baseline of around 17%.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9996559619903564}, {"text": "eeectiveness", "start_pos": 104, "end_pos": 116, "type": "METRIC", "confidence": 0.9705618619918823}]}, {"text": "As shown in, the improvement in performance due to both lexicalization and EM training is smaller for the easier task of frame evaluation.", "labels": [], "entities": [{"text": "frame evaluation", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.7526094615459442}]}, {"text": "Here the random baseline is 70% for frame evaluation on the newspaper corpus with an ambiguity rate of 25.", "labels": [], "entities": [{"text": "newspaper corpus", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.9640953838825226}]}, {"text": "An overall gain of roughly 10% can be achieved by going from unlexicalized parsebank models (80.6% precision) to lexicalized EM-trained models (90% precision).", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9968348145484924}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9961795806884766}]}, {"text": "Again, the contribution to this improvement is about the same for lexicalization and incomplete-data training.", "labels": [], "entities": []}, {"text": "Applying the same evaluation to the Helbig-Buscha corpus shows 97.6% precision and 96.7% eectiveness for the lexicalized and selected incompletedata model, compared to around 80% for the random baseline.", "labels": [], "entities": [{"text": "Helbig-Buscha corpus", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.7544828057289124}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9994658827781677}, {"text": "eectiveness", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.9587351083755493}]}, {"text": "Optimal iteration numbers were decided by repeated evaluation of the models at every \ud97b\udf59fth iteration.", "labels": [], "entities": []}, {"text": "shows the precision of lexicalized and selected models on the exact In terms of maximization of likelihood, this corresponds to the fact that uniform starting values immediately push the likelihood up to nearly its nal value, whereas random starting values yield an initial likelihood which has to be increased by factors of 2 to 20 to an often lower nal value.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.998472273349762}]}], "tableCaptions": []}