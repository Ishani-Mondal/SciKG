{"title": [{"text": "An Information-Theory-Based Feature Type Analysis for the Modelling of Statistical Parsing", "labels": [], "entities": [{"text": "Information-Theory-Based Feature Type Analysis", "start_pos": 3, "end_pos": 49, "type": "TASK", "confidence": 0.6916021928191185}, {"text": "Statistical Parsing", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7474737167358398}]}], "abstractContent": [{"text": "The paper proposes an information-theory-based method for feature types analysis in probabilistic evaluation modelling for statistical parsing.", "labels": [], "entities": [{"text": "feature types analysis", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.707044800122579}, {"text": "statistical parsing", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7626667022705078}]}, {"text": "The basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction.", "labels": [], "entities": [{"text": "syntactic structure prediction", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.723412275314331}]}, {"text": "Our experiment quantitatively analyzes several feature types' power for syntactic structure prediction and draws a series of interesting conclusions.", "labels": [], "entities": [{"text": "syntactic structure prediction", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.8009509642918905}]}], "introductionContent": [{"text": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [ [Charniak, 1997] [     [Magerman, 1995].", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8234424293041229}]}, {"text": "How to evaluate the different feature types' effects for syntactic parsing?", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7548764646053314}]}, {"text": "The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types' or feature types combination's predictive power for syntactic structure.", "labels": [], "entities": [{"text": "information-theory-based feature types analysis", "start_pos": 22, "end_pos": 69, "type": "TASK", "confidence": 0.6170705854892731}, {"text": "predictive information redundancy", "start_pos": 166, "end_pos": 199, "type": "TASK", "confidence": 0.7357625166575114}, {"text": "predictive information summation", "start_pos": 204, "end_pos": 236, "type": "TASK", "confidence": 0.7054348389307658}, {"text": "syntactic structure", "start_pos": 355, "end_pos": 374, "type": "TASK", "confidence": 0.7900286614894867}]}, {"text": "In the following, Section 2 describes the probabilistic evaluation model for syntactic trees; Section 3 proposes an information-theory-based feature type analysis model; Section 4 introduces several experimental issues; Section 5 quantitatively analyses the different contextual feature types or feature types combination in the view of information theory and draws a series of conclusion on their predictive powers for syntactic structures.", "labels": [], "entities": [{"text": "information-theory-based feature type analysis", "start_pos": 116, "end_pos": 162, "type": "TASK", "confidence": 0.6321993693709373}]}], "datasetContent": [{"text": "Given a sentence, the task of statistical syntactic parsing is to assign a probability to each candidate parsing tree that conforms to the grammar and select the one with highest probability as the final analysis result.", "labels": [], "entities": [{"text": "statistical syntactic parsing", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.6801929771900177}]}, {"text": "That is: (1) where S denotes the given sentence, T denotes the set of all the candidate parsing trees that conform to the grammar, P(T|S) denotes the probability of parsing tree T for the given sentence S.", "labels": [], "entities": []}, {"text": "The task of probabilistic evaluation model in syntactic parsing is the estimation of P(T|S).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.746929943561554}]}, {"text": "In the syntactic parsing model which uses rulebased grammar, the probability of a parsing tree can be defined as the probability of the derivation which generates the current parsing tree for the given sentence.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7143861949443817}]}, {"text": "That is, According to the equation of and, we have the following equation: In this way, we can get a unite expression of probabilistic evaluation model for statistical syntactic parsing.", "labels": [], "entities": [{"text": "statistical syntactic parsing", "start_pos": 156, "end_pos": 185, "type": "TASK", "confidence": 0.7129695018132528}]}, {"text": "The difference among the different parsing models lies mainly in that they use different feature types or feature type combination to divide the contextual condition into equivalent classes.", "labels": [], "entities": []}, {"text": "Our ultimate aim is to determine which combination of feature types is optimal for the probabilistic evaluation model of statistical syntactic parsing.", "labels": [], "entities": [{"text": "statistical syntactic parsing", "start_pos": 121, "end_pos": 150, "type": "TASK", "confidence": 0.6801737149556478}]}, {"text": "Unfortunately, the state of knowledge in this regard is very limited.", "labels": [], "entities": []}, {"text": "Many probabilistic evaluation models have been published inspired by one or more of these feature types [ [, but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively.", "labels": [], "entities": []}, {"text": "In the paper, we propose an information-theory-based feature type analysis model by which we can quantitatively analyse the predictive power of different feature types or feature type combinations for syntactic structure in a systematic way.", "labels": [], "entities": [{"text": "information-theory-based feature type analysis", "start_pos": 28, "end_pos": 74, "type": "TASK", "confidence": 0.6901019960641861}]}, {"text": "The conclusion is expected to provide reliable reference for feature type selection in the probabilistic evaluation modelling for statistical syntactic parsing.", "labels": [], "entities": [{"text": "feature type selection", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.6067395110925039}, {"text": "statistical syntactic parsing", "start_pos": 130, "end_pos": 159, "type": "TASK", "confidence": 0.6671375532944998}]}, {"text": "The experimental corpus is derived from Penn TreeBank.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9963666796684265}]}, {"text": "We semiautomatically assign a headword and a POS tag to each non-terminal node.", "labels": [], "entities": []}, {"text": "80% of the corpus (979,767 words) is taken as the training set, used for estimating the various co-occurrence probabilities, 10% of the corpus (133,814 words) is taken as the testing set, used to calculate predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation.", "labels": [], "entities": [{"text": "predictive information redundancy", "start_pos": 268, "end_pos": 301, "type": "TASK", "confidence": 0.7956346472104391}, {"text": "predictive information summation", "start_pos": 306, "end_pos": 338, "type": "TASK", "confidence": 0.8114544947942098}]}, {"text": "The other 10% of the corpus (133,814 words) is taken as the held-out set.", "labels": [], "entities": []}, {"text": "The grammar rule set is composed of 8,126 CFG rules extracted from Penn TreeBank.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9740562736988068}]}, {"text": "In the information-theory-based feature type analysis model, we need to estimate joint probability.", "labels": [], "entities": [{"text": "information-theory-based feature type analysis", "start_pos": 7, "end_pos": 53, "type": "TASK", "confidence": 0.7657480761408806}]}, {"text": "where ) (r c is the total number of time that r has been seen in the corpus.", "labels": [], "entities": []}, {"text": "According to the escape mechanism in, we define the weights wk where e k denotes the escape probability of context ) , , , ( , that is, the probability in which (f 1 , f 2 , ... , f k , r) is unseen in the corpus.", "labels": [], "entities": []}, {"text": "In such case, the blending model has to escape to the lower contexts to approximate where", "labels": [], "entities": []}], "tableCaptions": []}