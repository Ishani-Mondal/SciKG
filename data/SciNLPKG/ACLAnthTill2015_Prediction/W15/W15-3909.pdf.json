{"title": [{"text": "Neural Network Transduction Models in Transliteration Generation", "labels": [], "entities": [{"text": "Neural Network Transduction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7442136208216349}]}], "abstractContent": [{"text": "In this paper we examine the effectiveness of neural network sequence-to-sequence transduction in the task of transliteration generation.", "labels": [], "entities": [{"text": "transliteration generation", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.9474732577800751}]}, {"text": "In this year's shared evaluation we submitted two systems into all tasks.", "labels": [], "entities": []}, {"text": "The primary system was based on the system used for the NEWS 2012 workshop , but was augmented with an additional feature which was the generation probability from a neural network.", "labels": [], "entities": [{"text": "NEWS 2012 workshop", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.8528266350428263}]}, {"text": "The secondary system was the neural network model used on its own together with a simple beam search algorithm.", "labels": [], "entities": []}, {"text": "Our results show that adding the neural network score as a feature into the phrase-based statistical machine transliteration system was able to increase the performance of the system.", "labels": [], "entities": []}, {"text": "In addition, although the neural network alone was notable to match the performance of our primary system (which exploits it), it was able to deliver a respectable performance for most language pairs which is very promising considering the recency of this technique.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our primary system for the NEWS shared evaluation on transliteration generation is based on the system entered into the 2012 evaluation () which in turn was a development of the 2011 system).", "labels": [], "entities": [{"text": "NEWS", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.7157655954360962}, {"text": "transliteration generation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7552788853645325}]}, {"text": "The system is based around the application of phrase-based statistical machine translation (PB-SMT) techniques to the task of transliteration, as in (.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PB-SMT)", "start_pos": 46, "end_pos": 99, "type": "TASK", "confidence": 0.6534326076507568}]}, {"text": "The system differs from atypical phrase-based machine translation system in a number of important respects: \u2022 Characters rather than words are used as the atomic elements used in the transductive process \u2022 The generative process is constrained to be monotonic.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6969614028930664}]}, {"text": "No re-ordering model is used.", "labels": [], "entities": []}, {"text": "\u2022 The alignment process is constrained to be monotonic.", "labels": [], "entities": [{"text": "alignment", "start_pos": 6, "end_pos": 15, "type": "TASK", "confidence": 0.9699179530143738}]}, {"text": "-A non-parametric Bayesian aligner is used instead of GIZA++ and extraction heuristics, to provide a joint alignment/phrase pair induction process.", "labels": [], "entities": [{"text": "joint alignment/phrase pair induction", "start_pos": 101, "end_pos": 138, "type": "TASK", "confidence": 0.7117764502763748}]}, {"text": "\u2022 The log-linear weights are tuned towards the F-score evaluation metric used in the NEWS evaluation, rather than a machine translation oriented score such as BLEU ().", "labels": [], "entities": [{"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9782485961914062}, {"text": "NEWS evaluation", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.7114740908145905}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9975159168243408}]}, {"text": "\u2022 A bilingual language model () is used as a feature during decoding.", "labels": [], "entities": []}, {"text": "An n-best list of hypotheses from the PBSMT system outlined above was then re-scored using the following set of models: \u2022 A maximum entropy model (described in).", "labels": [], "entities": []}, {"text": "\u2022 A recurrent neural network RNN target language model ().", "labels": [], "entities": []}, {"text": "\u2022 An RNN bilingual language model (as in (). \u2022 A neural network transliteration model).", "labels": [], "entities": []}, {"text": "The re-scoring was done by extending the loglinear model of the PBSMT system with these 4 additional features.", "labels": [], "entities": []}, {"text": "The weights for these features were tuned to maximize F-score in a second tuning step.", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9941146969795227}]}, {"text": "The novel aspect of our system in this year's evaluation is the use of a neural network that is capable of performing the entire transductive process.", "labels": [], "entities": []}, {"text": "Neural networks capable of sequence-tosequence transduction where the sequences are of different lengths () area very recent development in the field of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7961429357528687}]}, {"text": "We believe this type of approach ought to be well suited to the task of transliteration, which is a task strongly related to that of machine translation but with typically much smaller vocabulary sizes and no problems related to reordering and inmost cases no issues relating to out of vocabulary words (characters in our case).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7534855008125305}]}, {"text": "On the other hand, it is generally believed (for example) that neural networks can require large amounts of data in order to train effective models, and the data set sizes available in this shared evaluation are quite small, and this lack of data may have caused problems for the neural networks employed.", "labels": [], "entities": []}, {"text": "In all our experiments we have taken a strictly language independent approach.", "labels": [], "entities": []}, {"text": "Each of the language pairs were processed automatically from the character sequence representation supplied for the shared tasks, with no language specific treatment for any of the language pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "The official scores for our system are given in Table 1.", "labels": [], "entities": []}, {"text": "It is interesting to compare the results of the 2012 system with the results from this year's primary submission on the 2012 test set, since these results show the effect of adding the neural network transliteration scores into the re-scorer.", "labels": [], "entities": [{"text": "2012 test set", "start_pos": 120, "end_pos": 133, "type": "DATASET", "confidence": 0.7455111344655355}]}, {"text": "In 11 out of 14 of the runs, the system's performance was improved, and for some language pairs, notably En-He, En-Hi, En-Ka, En-Pe, En-Ta, En-Th, Th-En and Jn-Jk the improvement was substantial.", "labels": [], "entities": []}, {"text": "The using the neural network model scores was ineffective for Ar-En, Ch-En and En-Ch.", "labels": [], "entities": [{"text": "Ar-En", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.980556845664978}]}, {"text": "Ar-En was surprising as the training corpus size for this task was considerably larger than for any other task, and we expected this to benefit the neural network approach.", "labels": [], "entities": [{"text": "Ar-En", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9370627403259277}]}, {"text": "Overall however, it is clear from the results that the neural network re-scoring was very effective and the effect was considerably greater than that from the RNN re-scoring models introduced in the 2012 system.", "labels": [], "entities": []}, {"text": "The results on the Jn-Jk task were surprising.", "labels": [], "entities": [{"text": "Jn-Jk task", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.5034905672073364}]}, {"text": "The neural network transliteration system alone produced very low accuracy scores, but when used in combination with the PBSMT system gave a 9.7% increase in top-1 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9987043142318726}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9598864316940308}]}, {"text": "One particular characteristic of this data set is the disparity in length between the sequences; kanji sequences were very short whereas the romanized form was much longer.", "labels": [], "entities": []}, {"text": "Visual inspection of the output from: The evaluation results on the 2015 shared task for our systems in terms of the top-1 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.97783362865448}]}, {"text": "the direct neural network transliteration showed that the output sequences derived from the roman character sequences, but were too long.", "labels": [], "entities": []}, {"text": "When integrated with the PBSMT system, output sequences of this form were not a problem as they were rarely generated as candidates for re-scoring.", "labels": [], "entities": []}, {"text": "We conducted two experiments in the reverse direction from Jk to Jn.", "labels": [], "entities": [{"text": "Jk", "start_pos": 59, "end_pos": 61, "type": "DATASET", "confidence": 0.9525923132896423}]}, {"text": "The first was based on a neural network transliteration system from character to character in the same manner as the secondary submission.", "labels": [], "entities": []}, {"text": "The second system was a neural network that transduced from character to character sequence.", "labels": [], "entities": []}, {"text": "We used a 1-to-many sequence alignment induced by the Bayesian aligner to train this model.", "labels": [], "entities": []}, {"text": "The character-to-character system had a top-1 accuracy of 0.245, the characterto-character sequence system had a top-1 accuracy of 0.305.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9872570633888245}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9082303643226624}]}, {"text": "These results indicate that the neural network is capable of generating long sequences from short sequences with reasonably high accuracy, and that there maybe something to be gained by using phrasal units in the neural network transduction process, as was the case when moving from word-based models to phrase-based models in machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9959509372711182}, {"text": "machine translation", "start_pos": 327, "end_pos": 346, "type": "TASK", "confidence": 0.7026735544204712}]}], "tableCaptions": [{"text": " Table 1: The evaluation results on the 2015 shared task for our systems in terms of the top-1 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9814789295196533}]}]}