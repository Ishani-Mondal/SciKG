{"title": [], "abstractContent": [{"text": "We report the results of our experiments in the context of the NEWS 2015 Shared Task on Transliteration.", "labels": [], "entities": [{"text": "NEWS 2015 Shared Task on Transliteration", "start_pos": 63, "end_pos": 103, "type": "TASK", "confidence": 0.7345859507719675}]}, {"text": "We focus on methods of combining multiple base systems , and leveraging transliterations from multiple languages.", "labels": [], "entities": []}, {"text": "We show error reductions over the best base system of up to 10% when using supplemental translitera-tions, and up to 20% when using system combination.", "labels": [], "entities": [{"text": "error", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9792873859405518}]}, {"text": "We also discuss the quality of the shared task datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The 2015 NEWS Shared Task on Machine Transliteration continues the series of shared tasks that were held yearly between 2009 and 2012.", "labels": [], "entities": [{"text": "NEWS Shared Task on Machine Transliteration", "start_pos": 9, "end_pos": 52, "type": "TASK", "confidence": 0.6395060916741689}]}, {"text": "With the exception of the 2010 edition that included transliteration mining, the task has been limited to learning transliteration models from the training sets of word pairs.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.8692108392715454}]}, {"text": "Participants are allowed to use target lexicons or monolingual corpora, but since those are \"non-standard\", the results are not comparable across different teams.", "labels": [], "entities": []}, {"text": "Another drawback of the current framework is the lack of context that is required to account for morphological alterations.", "labels": [], "entities": []}, {"text": "Our University of Alberta team has participated in each of the five editions of this shared task.", "labels": [], "entities": []}, {"text": "Although this year's task is virtually identical to the 2012 task, there has been progress in transliteration research since then.", "labels": [], "entities": []}, {"text": "In particular, transliteration projects at the University of Alberta have led to the design of novel techniques for leveraging supplemental information such as phonetic transcriptions and transliterations from other languages.", "labels": [], "entities": []}, {"text": "During those projects, we also observed that combinations of diverse systems often outperform their component systems.", "labels": [], "entities": []}, {"text": "We decided to test this hypothesis in the current rerun of the NEWS shared task.", "labels": [], "entities": [{"text": "NEWS shared task", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.6707034905751547}]}, {"text": "In this paper, we describe experiments that involve three well-known transliteration approaches.", "labels": [], "entities": []}, {"text": "DIRECTL+, SEQUITUR, and statistical machine translation toolkits (SMT).", "labels": [], "entities": [{"text": "statistical machine translation toolkits (SMT", "start_pos": 24, "end_pos": 69, "type": "TASK", "confidence": 0.7683030515909195}]}, {"text": "In an effort to harness the strengths of each system, we explore various techniques of combining their outputs.", "labels": [], "entities": []}, {"text": "Furthermore, we experiment with leveraging transliterations from other languages, in order to test whether this can improve the overall results.", "labels": [], "entities": []}, {"text": "We obtain state-of-the-art results on most language pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our development experiments, we randomly split the provided training sets into ten equal folds, of which eight were used for base system training, and one for base system tuning, with the final fold held out for system combination training.", "labels": [], "entities": [{"text": "base system tuning", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.5886624753475189}]}, {"text": "The base models were trained without languagespecific preprocessing.", "labels": [], "entities": []}, {"text": "shows the results on the provided development set.", "labels": [], "entities": []}, {"text": "DIRECTL+ is the best performing base system on eight datasets, with SEQUITUR winning on the remaining three.", "labels": [], "entities": [{"text": "SEQUITUR", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.7495046854019165}]}, {"text": "Although SMT is never the best, it comes second on three tasks.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9856670498847961}]}, {"text": "The absolute differences between the three system are within 10%.", "labels": [], "entities": []}, {"text": "Because of its simplicity, we expected LIN-COMB to serve as the baseline combination method.", "labels": [], "entities": [{"text": "LIN-COMB", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.749370276927948}]}, {"text": "However, as shown in, it performs surprisingly well, providing an improvement over the best base system on eight out of eleven datasets.", "labels": [], "entities": []}, {"text": "An additional advantage of LIN-COMB is that it requires no training or parameter tuning.", "labels": [], "entities": []}, {"text": "Since the other two combination methods are more complicated and less reliable, we chose LINCOMB as our default method.", "labels": [], "entities": [{"text": "LINCOMB", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9223320484161377}]}, {"text": "74  Some configurations of RERANK did achieve improvements over the best base system on most sets, but the results were generally below LIN-COMB.", "labels": [], "entities": [{"text": "RERANK", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9379702806472778}, {"text": "LIN-COMB", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9916796088218689}]}, {"text": "This confirms the observation of) that LINCOMB is a strong combination baseline because it utilizes entire n-best lists from all systems.", "labels": [], "entities": [{"text": "LINCOMB", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9635279178619385}]}, {"text": "The JOINT approach was unable to improve over base DIRECTL+ when trained on relatively small held-out sets.", "labels": [], "entities": [{"text": "JOINT", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.4591473340988159}]}, {"text": "We also tried to leverage the entire training set for this purpose using 10-cross validation.", "labels": [], "entities": []}, {"text": "However, that method requires a substantial amount of time and computing resources, and after disappointing initial results on selected datasets, we decided to forgo further experimentation.", "labels": [], "entities": []}, {"text": "It remains an open question whether the joint generation approach can be made to work as a system combination.", "labels": [], "entities": []}, {"text": "The JOINT approach performs much better in its original setup, in which additional transliterations from other languages are provided as input.", "labels": [], "entities": []}, {"text": "However, its effectiveness depends on the amount of supplemental information that is available per source word.", "labels": [], "entities": []}, {"text": "The improvement of JOINT over base DIRECTL+ seems to be correlated with the percentage of words with at least two supplemental transliterations in the corresponding test set.", "labels": [], "entities": [{"text": "JOINT", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.6341057419776917}]}, {"text": "The language pairs with over 50% of such words in the development set include EnHi, EnKa, and EnTa.", "labels": [], "entities": []}, {"text": "on EnHe, EnPe, and EnTa, where DIRECTL+ was chosen instead (see the results in).", "labels": [], "entities": [{"text": "EnHe", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.9796426892280579}, {"text": "EnPe", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9438562393188477}, {"text": "EnTa", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.9719594717025757}, {"text": "DIRECTL", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9442523121833801}]}, {"text": "Overall, our standard runs achieved top results on 14 out of 22 datasets.", "labels": [], "entities": []}, {"text": "includes our remaining test results.", "labels": [], "entities": []}, {"text": "We submitted the JOINT runs on languages that had promising improvements in the development results.", "labels": [], "entities": [{"text": "JOINT runs", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.6135095357894897}]}, {"text": "These runs were designated as non-standard even though the supplemental transliterations are from the provided NEWS datasets.", "labels": [], "entities": [{"text": "NEWS datasets", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.975335031747818}]}, {"text": "For these languages, we also submitted standard DIRECTL+ runs, in order to gauge the improvement obtained by JOINT.", "labels": [], "entities": [{"text": "JOINT", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.8274164199829102}]}, {"text": "The JOINT outperformed base DI-RECTL+ on six out of eight datasets.", "labels": [], "entities": [{"text": "JOINT", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.40532466769218445}]}], "tableCaptions": [{"text": " Table 1: Transliteration accuracy of DIRECTL+,  SEQUITUR, and SMT on the development sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9168814420700073}, {"text": "DIRECTL", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.833705484867096}, {"text": "SEQUITUR", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8914036154747009}, {"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.6588162183761597}]}, {"text": " Table 2: Official test results for standard linear  combination (LINCOMB).", "labels": [], "entities": [{"text": "LINCOMB", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.8793447613716125}]}, {"text": " Table 3: Official test results for standard DI- RECTL+, and for non-standard JOINT with sup- plemental transliterations.", "labels": [], "entities": [{"text": "DI- RECTL+", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.37030212581157684}]}]}