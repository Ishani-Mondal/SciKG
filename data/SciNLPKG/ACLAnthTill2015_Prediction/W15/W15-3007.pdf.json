{"title": [{"text": "CimS -The CIS and IMS Joint Submission to WMT 2015 addressing morphological and syntactic differences in English to German SMT", "labels": [], "entities": [{"text": "IMS Joint Submission to WMT", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.4539735496044159}, {"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.7344006299972534}]}], "abstractContent": [{"text": "We present the CimS submissions to the WMT 2015 Shared Task for the translation direction English to German.", "labels": [], "entities": [{"text": "WMT 2015 Shared Task", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.7190384268760681}, {"text": "translation direction English to German", "start_pos": 68, "end_pos": 107, "type": "TASK", "confidence": 0.9075772166252136}]}, {"text": "Similar to our previous submissions, all of our systems are aware of the complex nominal morphology of German.", "labels": [], "entities": []}, {"text": "In this paper , we combine source-side reordering and target-side compound processing with basic morphological processing in order to obtain improved translation results.", "labels": [], "entities": []}, {"text": "We also report on morphological processing for English to French.", "labels": [], "entities": [{"text": "morphological processing", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.7556294798851013}]}], "introductionContent": [{"text": "This paper presents our submissions to the WMT shared task 2015.", "labels": [], "entities": [{"text": "WMT shared task 2015", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.6582100316882133}]}, {"text": "We use customised solutions to address morphological challenges in the English to German translation direction.", "labels": [], "entities": []}, {"text": "Our goal is to make German and English as similar as possible in order to obtain better word alignments and hence an improved translation quality.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.6941882967948914}]}, {"text": "We base our work on three main components, which we have carefully investigated separately in the past.", "labels": [], "entities": []}, {"text": "(i) Nominal Inflection We use context-based prediction of German inflectional endings.", "labels": [], "entities": [{"text": "Nominal Inflection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.784114420413971}, {"text": "context-based prediction of German inflectional endings", "start_pos": 30, "end_pos": 85, "type": "TASK", "confidence": 0.731547032793363}]}, {"text": "This improves fluency and enables the creation of morphological forms which have not occurred in the training data.", "labels": [], "entities": []}, {"text": "(ii) Source-side Reordering We reorder the English source text in order to make it more similar to the German word order.", "labels": [], "entities": [{"text": "Source-side Reordering", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.459058552980423}]}, {"text": "This improves word alignment and thus translation quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.8052311539649963}, {"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9607588052749634}]}, {"text": "It also makes the reordering task in decoding easier.", "labels": [], "entities": []}, {"text": "(iii) Compound Processing We split German compounds into simple words for training.", "labels": [], "entities": [{"text": "Compound Processing", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.8030684888362885}]}, {"text": "In decoding, we translate only simple words, some of which are re-combined into compounds afterwards in post-processing.", "labels": [], "entities": []}, {"text": "This allows us to create compounds which have not occurred in the training data.", "labels": [], "entities": []}, {"text": "This year, our main focus is on combining nominal inflection prediction and source-side reordering.", "labels": [], "entities": [{"text": "nominal inflection prediction", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7325764298439026}]}, {"text": "We investigated both of these components separately in the past and expect an additive positive effect on translation quality when combined.", "labels": [], "entities": []}, {"text": "We then added compound processing, which we already have investigated in combination with nominal inflection before, but not together with sourceside reordering.", "labels": [], "entities": []}, {"text": "Here, we also expect the combination to outperform the single components in terms of translation quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the WMT shared task, we combined the three components which we have described in the previous section.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.8829744855562845}]}, {"text": "An overview of all systems we trained can be found in.", "labels": [], "entities": []}, {"text": "Data For all of our systems, we exclusively used data distributed for the WMT shared task 2015.", "labels": [], "entities": [{"text": "WMT shared task 2015", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.6159825772047043}]}, {"text": "We used all of the available monolingual data for German and all of the available parallel data for German and English.", "labels": [], "entities": []}, {"text": "UTF8 Cleaning Even though the submitted training data is provided in UTF-8 encoding, it contains a considerable number of characters that are not cleanly encoded into UTF8.", "labels": [], "entities": [{"text": "UTF8 Cleaning", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7533519566059113}]}, {"text": "We identified these characters and sequences thereof by reading all data bytewise and mapping it to the main UTF-8 encoding tables covering the Western European languages.", "labels": [], "entities": []}, {"text": "All lines that contained one or more characters which did not fit these tables -either because they have been broken or because they belong to non-latin scripts like, e.g., Chinese or Arabic, were removed from the corpora as we expected those lines to lead to erroneous analyses in the subsequent preprocessing steps of our pipeline.", "labels": [], "entities": []}, {"text": "Length Constraints To ensure good alignment quality, we removed sentence pairs where one language is considerably longer than the other (pairs exceeding the ratio 1:9 words), as well as sentences containing many special characters (e.g. several dashes in row) indicating that the line in question is part of e.g. a table.", "labels": [], "entities": []}, {"text": "Furthermore, we removed all sentences with a sentence length of more than 100 words.", "labels": [], "entities": []}, {"text": "gives an overview of the parallel data after cleaning and pre-processing.: Overview of the parallel data after cleaning and pre-processing.", "labels": [], "entities": []}, {"text": "English Variants The English source-side is mapped into British English in order to make the data as consistent as possible.", "labels": [], "entities": [{"text": "British English", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.9215692281723022}]}, {"text": "Linguistic Preprocessing The abstract representation for the nominal inflection requires the annotation of morphological features.", "labels": [], "entities": []}, {"text": "After tokenization, we thus parsed all target-side data with BitPar).", "labels": [], "entities": [{"text": "tokenization", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9647713899612427}]}, {"text": "To obtain the lemmas and suitable compound splittings, we applied SMOR ( ).", "labels": [], "entities": [{"text": "SMOR", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9768690466880798}]}, {"text": "Language Model We trained 5-gram Language Models for each of the available German monolingual corpora and the German sections of the parallel data.", "labels": [], "entities": []}, {"text": "For each corpus (the monolingual news corpora 07-14 and the parallel corpora europarl, commoncrawl and news), we built separate language models using the SRILM toolkit) with Kneser-Ney smoothing and then interpolated 1 them using weights optimized on development data (cf. tuning set 08-13).", "labels": [], "entities": []}, {"text": "We then used KenLM (Heafield, 2011) for faster processing.", "labels": [], "entities": [{"text": "KenLM (Heafield, 2011)", "start_pos": 13, "end_pos": 35, "type": "DATASET", "confidence": 0.7212618539730707}]}, {"text": "We performed this language model training for two different kinds of experiments: those without compound processing are trained on the underspecified (= lemmatised) representation, while those with compound processing are trained on a split underspecified representation.", "labels": [], "entities": []}, {"text": "Phrase-based Translation Model For word alignment, we use the multi-threaded GIZA++ toolkit ( Our translation models were trained using Moses (, following the instructions fora baseline shared task system, using default settigs.", "labels": [], "entities": [{"text": "Phrase-based Translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.738557368516922}, {"text": "word alignment", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.812577873468399}]}, {"text": "All our systems are trained identically -what differs is the degree to which the underlying training data has been modified.", "labels": [], "entities": []}, {"text": "Tuning We tuned feature weights using batchmira with 'safe-hope' (Cherry and Foster, 2012) until convergence (or up to 25 runs).", "labels": [], "entities": [{"text": "convergence", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9533653259277344}]}, {"text": "We used the tuning data of all previous shared tasks from 2008 to 2013, which gave us 16,071 sentences for tuning.", "labels": [], "entities": []}, {"text": "We tuned each experiment separately against an underspecified (i.e. lemmatised) version of the tuning reference optimising BLEU scores ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.955706775188446}]}, {"text": "Note also that we integrated the CRF-based compound prediction and merging procedure for each experiment with compound processing into each tuning iteration and thus scored the output against a non-split lemmatised reference.", "labels": [], "entities": [{"text": "CRF-based compound prediction and merging", "start_pos": 33, "end_pos": 74, "type": "TASK", "confidence": 0.6568044424057007}]}, {"text": "Testing After decoding, some post-processing is required in order to retransform the underspecified representation into fluent German text.", "labels": [], "entities": []}, {"text": "Our post-processing consists of the following steps: 1) translate into (split) underspecified German 2) merge compounds 3) predict nominal inflection 4) merge portmanteaus Finally, the output was recapitalised and detokenised using the shared task tools and all available German training data.", "labels": [], "entities": []}, {"text": "We calculated BLEU scores using the NIST script version 11b.: BLEU scores for all our systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9990077614784241}, {"text": "NIST script version 11b.", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.960226371884346}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9992305040359497}]}, {"text": "The upper part lists the submitted results (using a language model built on a subset of the available data), the lower part compares all our variants which have been computed after the deadline with a language model based on all available data for the constrained task.", "labels": [], "entities": []}, {"text": "In an additional set of experiments, we applied the nominal inflection system also to an EnglishFrench system.", "labels": [], "entities": [{"text": "EnglishFrench", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9555691480636597}]}, {"text": "Nominal Inflection for French The general pipeline is the same as for translation into German.", "labels": [], "entities": []}, {"text": "We used RFTagger for French ( for morphological tagging and a French version of SMOR to generate inflected forms.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.6784254610538483}]}, {"text": "The stem-markup on the French data corresponds to that of the German markup (number and gender on nouns).", "labels": [], "entities": [{"text": "French data", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.7856670916080475}]}, {"text": "In contrast to four morphological features for nominal inflection in German, only number and gender need to be modelled for French.", "labels": [], "entities": []}, {"text": "Data The EN-FR data set is much larger than that for EN-DE; after applying the same preprocessing steps, we obtained a parallel corpus of more than 36 million sentence pairs.", "labels": [], "entities": [{"text": "EN-FR data set", "start_pos": 9, "end_pos": 23, "type": "DATASET", "confidence": 0.8879767457644144}]}, {"text": "For the language model, we used an additional 45.9 million lines (news07-14 and newsdiscuss corpus).", "labels": [], "entities": [{"text": "newsdiscuss corpus", "start_pos": 80, "end_pos": 98, "type": "DATASET", "confidence": 0.8720383942127228}]}, {"text": "The language model was interpolated over separate language models built on the different corpora using the development set to obtain optimal weights.", "labels": [], "entities": []}, {"text": "Results The nominal inflection system is our primary system.", "labels": [], "entities": []}, {"text": "Due to the large amount of EN-FR parallel training data, we assume that here the BLEU score correctly shows that there is not much difference in performance between the two systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9992775321006775}]}], "tableCaptions": [{"text": " Table 2: Names and components of our SMT systems; the submitted system are named CIMS-primary  and CIMS.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9842733144760132}, {"text": "CIMS-primary", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.887768030166626}, {"text": "CIMS", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.9145949482917786}]}, {"text": " Table 3: Overview of the parallel data after cleaning and pre-processing.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores for all our systems. The upper part lists the submitted results (using a language  model built on a subset of the available data), the lower part compares all our variants which have been  computed after the deadline with a language model based on all available data for the constrained task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986660480499268}]}]}