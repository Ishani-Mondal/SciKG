{"title": [{"text": "From distributional semantics to feature norms: grounding semantic models inhuman perceptual data", "labels": [], "entities": []}], "abstractContent": [{"text": "Multimodal semantic models attempt to ground distributional semantics through the integration of visual or perceptual information.", "labels": [], "entities": []}, {"text": "Feature norms provide useful insight into human concept acquisition , but cannot be used to ground large-scale semantics because they are expensive to produce.", "labels": [], "entities": [{"text": "human concept acquisition", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.6887598832448324}]}, {"text": "We present an automatic method for predicting feature norms for new concepts by learning a mapping from a text-based distributional semantic space to a space built using feature norms.", "labels": [], "entities": [{"text": "predicting feature norms", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8462725679079691}]}, {"text": "Our experimental results show that we are able to generalise feature-based concept representations, which opens up the possibility of developing large-scale semantic models grounded in a proxy for human perceptual data.", "labels": [], "entities": [{"text": "generalise feature-based concept representations", "start_pos": 50, "end_pos": 98, "type": "TASK", "confidence": 0.6507912203669548}]}], "introductionContent": [{"text": "Distributional semantic models) represent the meanings of words by relying on their statistical distribution in text.", "labels": [], "entities": []}, {"text": "Despite performing well in a wide range of semantic tasks, a common criticism is that by only representing meaning through linguistic input these models are not grounded in perception, since the words only exist in relation to each other and are not in relation to the physical world.", "labels": [], "entities": []}, {"text": "This concern is motivated by the increasing evidence in the cognitive science literature that the semantics of words is derived not only from our exposure to the language, but also through our interactions with the world.", "labels": [], "entities": []}, {"text": "One way to overcome this issue would be to include perceptual information in the semantic models (.", "labels": [], "entities": []}, {"text": "It has already been shown, for example, that models that learn from both visual and linguistic input improve performance on a variety of tasks such as word association or semantic similarity ().", "labels": [], "entities": [{"text": "word association", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.7514383792877197}]}, {"text": "However, the visual modality alone cannot capture all perceptual information that humans possess.", "labels": [], "entities": []}, {"text": "A more cognitively sound representation of human intuitions in relation to particular concepts is given by semantic property norms, also known as semantic feature norms.", "labels": [], "entities": []}, {"text": "A number of property norming studies () have focused on collecting feature norms for various concepts in order to allow for empirical testing of psychological semantic theories.", "labels": [], "entities": []}, {"text": "In these studies humans are asked to identify the most important attributes of a concept; e.g. given AIRPLANE, its most important features could be to_fly, has_wings and is_used_for_transport.", "labels": [], "entities": []}, {"text": "These datasets provide a valuable insight into human concept representation and have been successfully used for tasks such as text simplification for limited vocabulary groups, personality modelling and metaphor processing, as well as a proxy for modelling perceptual information ().", "labels": [], "entities": [{"text": "human concept representation", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.7618080178896586}, {"text": "text simplification", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7348898649215698}, {"text": "personality modelling", "start_pos": 177, "end_pos": 198, "type": "TASK", "confidence": 0.7695752680301666}, {"text": "metaphor processing", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.9388884007930756}]}, {"text": "Feature norms provide an interesting source of semantic information because they capture higher level conceptual knowledge in comparison to the low level perceptual information represented in images, for example.", "labels": [], "entities": []}, {"text": "Despite their advantages, semantic feature norms are not widely used in computational linguistics, mainly because they are expensive to produce and have only been collected for small sets of words; moreover there is no finite list of features that can be produced fora given concept.", "labels": [], "entities": []}, {"text": "In is long, 15 is formal, 10 tastes good, 9 eaten as pickles, 12 is long, 10 has a shell, 8 has skin, 9 different styles, 9 lives in oceans, 8 grows in gardens, 7 made of material, 9: Examples of features and production frequencies for concepts from the McRae norms, the authors construct a three-way multimodal model, integrating textual, feature and visual modalities.", "labels": [], "entities": [{"text": "McRae norms", "start_pos": 254, "end_pos": 265, "type": "DATASET", "confidence": 0.925758957862854}]}, {"text": "However, this method is restricted to the same disadvantages of feature norm datasets.", "labels": [], "entities": []}, {"text": "There have been some attempts at automatically generating feature norms using large text corpora ( but the generated features are often a production of carefully crafted rules and statistical distribution of words in text rather than a proxy for human conceptual knowledge.", "labels": [], "entities": []}, {"text": "Our work focuses on predicting features for new concepts, by learning a mapping from a distributional semantic space based solely on linguistic input to a more cognitively-sound semantic space where feature norms are seen as a proxy for perceptual information.", "labels": [], "entities": [{"text": "predicting features for new concepts", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.8458152770996094}]}, {"text": "A precedent for this work has been set in, but whilst they predict feature representations through global lexical similarity, we infer them through learning a cross-modal mapping.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the largest and most widely used feature-norm datasets is from.", "labels": [], "entities": []}, {"text": "Participants were asked to produce a list of features fora given concept, whilst being encouraged to write down different kinds of properties, e.g. how the concept feels, smells or for what it is used (.", "labels": [], "entities": []}, {"text": "The dataset contains a total of 2526 features for 541 concrete concepts, with a mean of 13.7 features per concept.", "labels": [], "entities": []}, {"text": "More recently, collected semantic properties for 638 concrete concepts in a similar fashion.", "labels": [], "entities": []}, {"text": "There are also other property norms datasets which contain verbs and nouns referring to events.", "labels": [], "entities": []}, {"text": "Since the semantic property norms in the McRae dataset have been used extensively in the literature as a proxy for perceptual information, we will report our experimental results on this dataset.", "labels": [], "entities": [{"text": "McRae dataset", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9772436916828156}]}, {"text": "We performed all experiments using a training set of 400 randomly selected McRae concepts and a test set of the remaining 138.", "labels": [], "entities": []}, {"text": "We use the featural representations of the concepts in the training set in order to learn a mapping between the two spaces, and the featural representations of the concepts in the test set as gold-standard vectors in order to analyse the quality of the learned transformation.", "labels": [], "entities": []}, {"text": "For each item in the test set, we computed the concept's predicted vector, f ( x), by applying the learned mapping, f , to the concept's representation in DS, x.", "labels": [], "entities": []}, {"text": "We then retrieved the top neighbours of the predicted vector in FS using cosine similarity.", "labels": [], "entities": [{"text": "FS", "start_pos": 64, "end_pos": 66, "type": "DATASET", "confidence": 0.8562217950820923}]}, {"text": "We were interested in observing, fora given concept, whether the gold-standard featural vector was retrieved in the topN neighbours of the predicted featural vector.", "labels": [], "entities": []}, {"text": "Results averaged over the entire test set are summarised in.", "labels": [], "entities": []}, {"text": "We also report the performance of a random baseline (RAND), where a concept's nearest neighbours are randomly ranked, and we note that our model outperforms chance by a large margin.", "labels": [], "entities": []}, {"text": "For the experiments in which the feature space dimensions are interpretable, i.e. not reduced (FS1), we also report the MAP (Mean Average Precision).", "labels": [], "entities": [{"text": "FS1", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9797800183296204}, {"text": "MAP", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9970471262931824}, {"text": "Mean Average Precision)", "start_pos": 125, "end_pos": 148, "type": "METRIC", "confidence": 0.9495799988508224}]}, {"text": "This allows us to measure the learnt mapping's ability to assign higher values to the gold features of a McRae concept (those properties that have a nonzero production frequency fora particular concept in the McRae dataset) than to the non-gold features.", "labels": [], "entities": [{"text": "McRae concept", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.929797500371933}, {"text": "McRae dataset", "start_pos": 209, "end_pos": 222, "type": "DATASET", "confidence": 0.9753171801567078}]}, {"text": "We compute the MAP score as follows: for each concept in the test set, we rank the features from the predicted feature vector in terms of their values, and measure the quality of this ranking with IR-based average precision, using the gold-standard feature set as the \"relevant\" feature set.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.7469756305217743}, {"text": "IR-based average precision", "start_pos": 197, "end_pos": 223, "type": "METRIC", "confidence": 0.7973217566808065}]}, {"text": "The MAP score is then obtained by taking the mean average precision over the entire test set.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9486739635467529}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.5839717388153076}]}, {"text": "Overall, the model seems to rank gold features highly, but the MAP score is certainly affected by the features which have not been seen in training (these account for 18.8% of the total number of features), because these will have a zero weight assigned to them, and so will be found at the end of the ranked feature list for that concept.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9564263820648193}]}, {"text": "A qualitative evaluation of the top neighbours for predicted featural vectors can be found in.", "labels": [], "entities": []}, {"text": "Overall, the mapping results look promising, even for items that do not list the gold feature vector as one of the top neighbours.", "labels": [], "entities": []}, {"text": "However, overall the mapping looks too coarse.", "labels": [], "entities": []}, {"text": "One reason could be the fact that the feature-based space is relatively sparse (the maximum number of features fora concept is 26, whereas there are over 2500 dimensions in the space).", "labels": [], "entities": []}, {"text": "The reason why, for example, the predicted vector for JAR does not contain its gold standard in the top 20 neighbours might simply be that there are not enough discriminating features for the model to learn that ajar usually has a lid and a bucket does not; or that jeans are worn on the lower body, as opposed to shawls which are worn on the shoulders.", "labels": [], "entities": [{"text": "JAR", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.7089744806289673}]}, {"text": "It is important to note that a production frequency of zero fora concept-feature pair in the McRae dataset does not necessarily mean that the feature is not a plausible property of the concept, but only that it is not one of the most salient features, since it was not produced by any of the human participants (e.g. the feature has_teeth has not been listed as a property of CAT in the McRae dataset, but it is clearly a plausible property of the CAT concept).", "labels": [], "entities": [{"text": "McRae dataset", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9734062552452087}, {"text": "McRae dataset", "start_pos": 387, "end_pos": 400, "type": "DATASET", "confidence": 0.9791648983955383}]}, {"text": "Many of the top-predicted features for the concepts in the test set are plausible, even if they are not listed in the gold data (e.g lives_in_water for SEAWEED).", "labels": [], "entities": [{"text": "SEAWEED", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.6242215633392334}]}, {"text": "This is yet another indication that the concept-feature pairs listed in the McRae dataset are not complete, meaning that there are salient features that apply to some concepts which have not been spelled out by the participants.", "labels": [], "entities": [{"text": "McRae dataset", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9710405766963959}]}], "tableCaptions": [{"text": " Table 2: Example representation of CAT in the feature-based and distributional spaces", "labels": [], "entities": [{"text": "CAT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9661216139793396}]}, {"text": " Table 3: Percentage (%) of test items that retrieve their gold-standard vector in the topN neighbours of  their predicted vector.", "labels": [], "entities": []}]}