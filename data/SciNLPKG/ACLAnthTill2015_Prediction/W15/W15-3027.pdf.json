{"title": [{"text": "Why Predicting Post-Edition is so Hard? Failure Analysis of LIMSI Submission to the APE Shared Task", "labels": [], "entities": [{"text": "Failure Analysis", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.6353920996189117}, {"text": "APE Shared Task", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.5513460437456766}]}], "abstractContent": [{"text": "This paper describes the two systems submitted by LIMSI to the WMT'15 Shared Task on Automatic Post-Editing.", "labels": [], "entities": [{"text": "WMT'15 Shared Task", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.6718162894248962}]}, {"text": "The first one relies on a reformulation of the APE task as a Machine Translation task; the second implements a simple rule-based approach.", "labels": [], "entities": [{"text": "APE task", "start_pos": 47, "end_pos": 55, "type": "TASK", "confidence": 0.7311763167381287}, {"text": "Machine Translation task", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.783855140209198}]}, {"text": "Neither of these two systems manage to improve the automatic translation.", "labels": [], "entities": []}, {"text": "We show, by carefully analyzing the failure of our systems that this counter-performance mainly results from the inconsistency in the annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes LIMSI submission to the WMT'15 Shared Task on Automatic Post-Editing (APE).", "labels": [], "entities": [{"text": "WMT'15 Shared Task on Automatic Post-Editing (APE)", "start_pos": 45, "end_pos": 95, "type": "TASK", "confidence": 0.5860439704524146}]}, {"text": "This task aims at automatically correcting errors produced by an unknown Machine Translation (MT) system by learning from human posteditions.", "labels": [], "entities": [{"text": "correcting errors produced by an unknown Machine Translation (MT)", "start_pos": 32, "end_pos": 97, "type": "TASK", "confidence": 0.7979937846010382}]}, {"text": "For the first edition of this Shared Task we have submitted two APE systems.", "labels": [], "entities": [{"text": "APE", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.7656602263450623}]}, {"text": "The first one, described in Section 3, is based on the approach of and considers the APE task as the automatic translation between a translation hypothesis and its post-edition.", "labels": [], "entities": [{"text": "APE task", "start_pos": 85, "end_pos": 93, "type": "TASK", "confidence": 0.6724285185337067}, {"text": "translation between a translation hypothesis", "start_pos": 111, "end_pos": 155, "type": "TASK", "confidence": 0.612138295173645}]}, {"text": "This straightforward approach does not succeed in improving translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9767023324966431}]}, {"text": "To understand the reasons of this failure, we present, in Section 4 a detailed analysis of the training data that highlights some of the difficulties of training an APE system.", "labels": [], "entities": [{"text": "APE", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.8513582944869995}]}, {"text": "The second submitted system implements a series of sieves, applying, each, a simple postediting rule.", "labels": [], "entities": []}, {"text": "The definition of these rules is based on our analysis of the most frequent error corrections.", "labels": [], "entities": []}, {"text": "Experiments with this approach show that this system also hurts translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9709004163742065}]}, {"text": "However, analyzing its failures allows us to show that the main difficulties in correcting MT errors result from the inconsistency between the different post-editions.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.6435437798500061}]}], "datasetContent": [{"text": "sieve approach than there are errors that are corrected.", "labels": [], "entities": []}, {"text": "The analysis of our errors shows that the observed drop in performance can be explained by the inconsistencies in the post-editions.", "labels": [], "entities": []}, {"text": "For instance, in the case of interrogative sentences, there are 558 translation hypotheses in the training set that end with an interrogative mark, 203 of which do not contain an inverted mark.", "labels": [], "entities": []}, {"text": "Applying Algorithm 1, will correct all of them.", "labels": [], "entities": [{"text": "correct", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9551708102226257}]}, {"text": "However, it also appears that, in 108 of these 203 sentences (53%) no inverted interrogative marks were added by the post-editors -resulting in 'un-grammatical' sentences.", "labels": [], "entities": []}, {"text": "At the end, even the correct introduction of inverted question marks would make translation hypotheses less similar to the human post-edition.", "labels": [], "entities": [{"text": "translation hypotheses", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.9047380983829498}]}, {"text": "A similar observation can be made for the exclamatory sentences.", "labels": [], "entities": []}, {"text": "Regarding the correction of case, the proposed post-edition rule achieves very good performance when its application is restricted to the word that have to be post-edited (i.e. when using the postedition as an oracle to identify which words must be corrected): it is able to correctly predict the case of the word in almost 85% of the case.", "labels": [], "entities": [{"text": "correction of case", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8614924152692159}]}, {"text": "The erroneous corrections mainly result from alignment errors.", "labels": [], "entities": [{"text": "alignment", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9321312308311462}]}, {"text": "However, when applied on the whole corpora it will also change the case of many words the post-editors have not modified.", "labels": [], "entities": []}, {"text": "When we looked at these words we did not see any reasons why they should not have been modified.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: hTER score achieved by MT system train  to predict the post-edition from the MT output.", "labels": [], "entities": [{"text": "hTER score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8976740837097168}, {"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.7985922694206238}, {"text": "MT output", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.7395948767662048}]}, {"text": " Table 3: Distribution of the edit types in the train- ing set.", "labels": [], "entities": []}, {"text": " Table 4: Most frequent post-edits on the training  set. Additions and deletion are denoted by '+' and  '-'; substitutions by '\u2192' .", "labels": [], "entities": []}, {"text": " Table 5: Number of edits involving only punctua- tion.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9562015533447266}]}, {"text": " Table 7: PoS of the words involved in a substitu- tion.", "labels": [], "entities": [{"text": "PoS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9494449496269226}]}, {"text": " Table 9: hTER score achieved by our multi-sieve  approach on the development data.", "labels": [], "entities": [{"text": "hTER score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8489542603492737}]}]}