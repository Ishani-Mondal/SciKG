{"title": [{"text": "Automated Scoring of Picture-based Story Narration", "labels": [], "entities": [{"text": "Automated Scoring of Picture-based Story Narration", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8274460236231486}]}], "abstractContent": [{"text": "This work investigates linguistically motivated features for automatically scoring a spoken picture-based narration task.", "labels": [], "entities": [{"text": "scoring a spoken picture-based narration task", "start_pos": 75, "end_pos": 120, "type": "TASK", "confidence": 0.6664640605449677}]}, {"text": "Specifically, we build scoring models with features for story development, language use and task relevance of the response.", "labels": [], "entities": [{"text": "story development", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7650673687458038}]}, {"text": "Results show that combinations of these features outperform a base-line system that uses state of the art speech-based features, and that best results are obtained by combining the linguistic and speech features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Story-telling has been used in evaluating the development of language skills.", "labels": [], "entities": []}, {"text": "It has also been incorporated into assessment of English language proficiency in tests such as ETS's TOEFL Junior Comprehensive Test , where English language skills of non-native middleschool students are tested on a task designed to elicit stories based on pictures.", "labels": [], "entities": [{"text": "ETS's TOEFL Junior Comprehensive Test", "start_pos": 95, "end_pos": 132, "type": "DATASET", "confidence": 0.8029095331827799}]}, {"text": "The Six-Picture Narration task presents a series of six pictures (similar to a comic strip) to the test taker, who must orally produce a story which incorporates the events depicted in the pictures.", "labels": [], "entities": [{"text": "Six-Picture Narration task", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7466991345087687}]}, {"text": "As the scoring guide 2 for this task indicates, in addition to fluidity of speech and few pronunciation errors, high scoring responses must also show good command of language conventions, including grammar and word usage, and must also be relevant to the task.", "labels": [], "entities": []}, {"text": "Previous work) explored automated assessment of the speech component of the spoken responses to the picture narration task, but the linguistic and narrative aspects of the response have not received much attention.", "labels": [], "entities": [{"text": "picture narration task", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7849464615186056}]}, {"text": "In this work, we investigate linguistic and constructrelevant aspects of the test such as (1) relevance and completeness of the content of the responses with respect to the prompt pictures, (2) proper word usage (3) use of narrative techniques such as detailing to enhance the story, and (4) sequencing strategies to build a coherent story.", "labels": [], "entities": [{"text": "relevance", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9729537963867188}]}, {"text": "The contribution of this work is three-fold.", "labels": [], "entities": []}, {"text": "First, we improve the construct coverage of the automated scoring models by incorporating evaluation of elements prescribed in the scoring rubric.", "labels": [], "entities": []}, {"text": "Second, our linguistically motivated features allow for clear interpretation and explanation of scores, which is especially important if the automated scoring is to be employed for educational purposes.", "labels": [], "entities": []}, {"text": "Finally, our results are promising -we show that the combination of linguistic and construct-relevant features which we explore in this work outperforms the state of the art baseline system, and that the best performance is obtained when the linguistic and construct-relevant features are combined with the speech features.", "labels": [], "entities": []}, {"text": "use features extracted mainly from speech for scoring the picture narration task.", "labels": [], "entities": [{"text": "picture narration task", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7747105757395426}]}, {"text": "They employ measures capturing fluency, prosody and pronunciation.", "labels": [], "entities": []}, {"text": "Our work explores the other (complementary) dimensions of the test such as language use, content relevance and story development.", "labels": [], "entities": [{"text": "story development", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.7477065622806549}]}], "datasetContent": [{"text": "For our experiments, we used a supervised learning framework, with the data described above, to build scoring models based on our feature sets.", "labels": [], "entities": []}, {"text": "We evaluated several different learning algorithms and found that a Random Forest Classifier consistently produced the best results in cross-validation experiments on the training data when we used our features as well as when we used the baseline set of features.", "labels": [], "entities": []}, {"text": "Hence, all of our results in this section are reported using this Random Forest learner.", "labels": [], "entities": [{"text": "Random Forest learner", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.8620092471440634}]}, {"text": "Performance was calculated using Quadratic Weighted Kappa (QWK), which is the standard evaluation metric used in automated scoring.", "labels": [], "entities": [{"text": "Quadratic Weighted Kappa (QWK)", "start_pos": 33, "end_pos": 63, "type": "METRIC", "confidence": 0.9043687780698141}]}, {"text": "QWK measures the agreement between the system score and the  human-annotated score, correcting for chance agreement and penalizing large disagreements more than small ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of responses and score distributions for  training and evaluation datasets.", "labels": [], "entities": []}, {"text": " Table 2: Performance of different feature sets.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the Baseline when each individ- ual feature set is added to it.", "labels": [], "entities": []}]}