{"title": [{"text": "An Analysis of Biomedical Tokenization: Problems and Strategies", "labels": [], "entities": []}], "abstractContent": [{"text": "Choosing the right tokenizer is a non-trivial task, especially in the biomedical domain, where it poses additional challenges, which if not resolved means the propagation of errors in successive Natural Language Processing analysis pipeline.", "labels": [], "entities": []}, {"text": "This paper aims to identify these problematic cases and analyze the output that, a representative and widely used set of tokenizers, shows on them.", "labels": [], "entities": []}, {"text": "This work will aid the decision making process of choosing the right strategy according to the downstream application.", "labels": [], "entities": [{"text": "decision making", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.8710258901119232}]}, {"text": "In addition, it will help developers to create accurate tokenization tools or improve the existing ones.", "labels": [], "entities": []}, {"text": "A total of 14 problematic cases were described, showing biomedical samples for each of them.", "labels": [], "entities": []}, {"text": "The outputs of 12 tokenizers were provided and discussed in relation to the level of agreement among tools.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tokenization is considered the first step in Natural Language Processing (henceforth, NLP) and it is broadly defined as the segmentation of text into primary building blocks for subsequent analysis.", "labels": [], "entities": []}, {"text": "Tokenization may seem simple if we assume that all it involves is the recognition of a space as a word separator).", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9140129685401917}]}, {"text": "However, a closer examination will make it clear that a blank space alone is not enough even for general English.", "labels": [], "entities": []}, {"text": "Furthermore, choosing the right tokenization strategy is a non-trivial task, especially in the biomedical domain where it poses additional challenges () which if not resolved means the propagation of errors in successive NLP analysis pipeline.", "labels": [], "entities": [{"text": "NLP analysis pipeline", "start_pos": 221, "end_pos": 242, "type": "TASK", "confidence": 0.83269731203715}]}, {"text": "As a consequence, text mining modules, such as Named Entity Recognition, will inevitably suffer in terms of effectiveness).", "labels": [], "entities": [{"text": "text mining", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.840351939201355}, {"text": "Named Entity Recognition", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.6581814686457316}]}, {"text": "Tokenization in biomedical literature is particularly difficult due to the fact that general English differ from biomedical text in vocabulary and grammar.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9759198427200317}]}, {"text": "In addition, scientific information has a particular structure).", "labels": [], "entities": []}, {"text": "For example, carried out three experiments to evaluate the syntactic dissimilarities between medical discharge summaries and everyday English, showing significant differences in syntactic content and complexity.", "labels": [], "entities": [{"text": "syntactic dissimilarities between medical discharge summaries", "start_pos": 59, "end_pos": 120, "type": "TASK", "confidence": 0.6813035806020101}]}, {"text": "Another feature of the biomedical literature is related to terminology, which is inconsistently spelt and may vary from typographical errors to lowercase and capitalized medication names ().", "labels": [], "entities": []}, {"text": "Furthermore, biomedical texts could be ungrammatical (especially, clinical documents) as well as often include abbreviations and acronyms.", "labels": [], "entities": []}, {"text": "Biomedical terms contain digits, capitalized letters within words, Latin and Greek letters, Roman digits, measurement units, list and enumerations, tabular data, hyphens and other special symbols.", "labels": [], "entities": []}, {"text": "In addition, another complexity is the ambiguity, i.e., words and abbreviations that have different meanings (homonymy) and concepts described in more than one way (synonymy).", "labels": [], "entities": []}, {"text": "For these reasons, the identification of terminology in the biomedical literature is one of the most challenging research topics in the last few years in NLP and biomedical communities and tokenization plays an important role in handling them.", "labels": [], "entities": [{"text": "identification of terminology", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.8506038188934326}]}, {"text": "There is no widely accepted tokenization method for English text, including biomedical documents since tokenization strategies can vary depending on language, task goals and other criteria.", "labels": [], "entities": []}, {"text": "Previous approaches to biomedical tokenization lack guidance on how to modify existing tokenizers to new domains and how even to select them.", "labels": [], "entities": []}, {"text": "Their idiosyncratic nature, detailed above, complicates this selection, modification and implementation.", "labels": [], "entities": []}, {"text": "Some authors also highlight the clear need for tokenization evaluation through the alignment and com-parison of the results of different tokenizers).", "labels": [], "entities": [{"text": "tokenization evaluation", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.9167874157428741}]}, {"text": "To address this challenge, this paper identifies and describes all the problematic cases that can be found when tokenizing a biomedical text.", "labels": [], "entities": [{"text": "tokenizing a biomedical text", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.906117707490921}]}, {"text": "In addition, it includes a list of useful tokenizers and a comparison of their outputs on biomedical text samples.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Firstly, the most relevant related research is outlined.", "labels": [], "entities": []}, {"text": "Secondly, the tokenizers are listed and their outputs are shown.", "labels": [], "entities": []}, {"text": "The paper finishes with conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 14: Tokenizers output for sentence (13)  Tokenizer  Output  1, 5, 6, 8, 9,  10, 11, 12", "labels": [], "entities": []}, {"text": " Table 15: Tokenizers output for sentence (14)", "labels": [], "entities": []}, {"text": " Table 16: Tokenizers output for sentence (15)", "labels": [], "entities": []}, {"text": " Table 17: Tokenizers output for sentence (16)", "labels": [], "entities": []}, {"text": " Table 18: Tokenizers output for sentence (17)", "labels": [], "entities": []}, {"text": " Table 19: Tokenizers output for sentence (18)", "labels": [], "entities": []}, {"text": " Table 20: Tokenizers output for sentence (19)", "labels": [], "entities": []}, {"text": " Table 21: Tokenizers output for sentence (20)", "labels": [], "entities": []}, {"text": " Table 22: Tokenizers output for sentence (21)", "labels": [], "entities": []}, {"text": " Table 23: Tokenizers output for sentence (22)", "labels": [], "entities": []}, {"text": " Table 24: Tokenizers output for sentence (23)", "labels": [], "entities": []}]}