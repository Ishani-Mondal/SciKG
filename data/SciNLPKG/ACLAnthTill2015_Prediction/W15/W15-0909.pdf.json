{"title": [{"text": "The Impact of Multiword Expression Compositionality on Machine Translation Evaluation", "labels": [], "entities": [{"text": "Multiword Expression Compositionality", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.7593878308931986}, {"text": "Machine Translation Evaluation", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.8758437434832255}]}], "abstractContent": [{"text": "In this paper, we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation, in integrating compositionality scores for English noun compounds into the TESLA machine translation evaluation metric.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.7807310024897257}, {"text": "TESLA machine translation evaluation", "start_pos": 238, "end_pos": 274, "type": "TASK", "confidence": 0.7187030166387558}]}, {"text": "The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact.", "labels": [], "entities": []}], "introductionContent": [{"text": "While the explicit identification of multiword expressions (\"MWEs\":,) has been shown to be useful in various NLP applications, recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\"IR\": Acosta et al.) and machine translation (\"MT\":, and).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 261, "end_pos": 282, "type": "TASK", "confidence": 0.7850954830646515}, {"text": "machine translation", "start_pos": 309, "end_pos": 328, "type": "TASK", "confidence": 0.8117170631885529}]}, {"text": "For instance, showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and showed that by adding compositionality scores to the Moses SMT system (, they could improve translation quality.", "labels": [], "entities": [{"text": "Moses SMT system", "start_pos": 203, "end_pos": 219, "type": "DATASET", "confidence": 0.7612460056940714}]}, {"text": "This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs.", "labels": [], "entities": [{"text": "MWE compositionality", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.928472250699997}, {"text": "MT system outputs", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.8775461912155151}]}, {"text": "The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the reference translations, based on compositionality.", "labels": [], "entities": []}, {"text": "For example, an MT output of white tower should not be rewarded for partial overlap with ivory tower in the reference translation, as tower here is most naturally interpreted compositionally in the MT output, but non-compositionally in the reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9361678957939148}]}, {"text": "On the other hand, a partial mismatch between traffic signal and traffic light should be rewarded, as the usage of traffic is highly compositional in both cases.", "labels": [], "entities": []}, {"text": "That is, we ask the question: can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the TESLA machine translation metric ().", "labels": [], "entities": [{"text": "TESLA machine translation", "start_pos": 203, "end_pos": 228, "type": "TASK", "confidence": 0.5530005395412445}]}], "datasetContent": [{"text": "Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9272727072238922}, {"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9662644267082214}]}, {"text": "This scoring can be based on: (1) simple string similarity); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms; or (3) deeper linguistic information such as semantic roles (.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.7754471898078918}]}, {"text": "In this research, we focus on the TESLA MT eval-uation metric (, which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs.", "labels": [], "entities": [{"text": "TESLA MT eval-uation metric", "start_pos": 34, "end_pos": 61, "type": "DATASET", "confidence": 0.49698951095342636}]}, {"text": "We evaluate our method over the data from WMT 2013, which is made up of a total of 3000 translations for five to-English language pairs (.", "labels": [], "entities": [{"text": "WMT 2013", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9165663421154022}]}, {"text": "As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgements for each MT output, as collected by.", "labels": [], "entities": [{"text": "WMT 2013", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.8989490270614624}, {"text": "MT output", "start_pos": 193, "end_pos": 202, "type": "TASK", "confidence": 0.7768340408802032}]}, {"text": "We used the Stanford CoreNLP parser to identify English noun compounds in the translations.", "labels": [], "entities": [{"text": "Stanford CoreNLP parser", "start_pos": 12, "end_pos": 35, "type": "DATASET", "confidence": 0.8958575328191122}]}, {"text": "Among the 3000 sentences, 579 sentences contain at least one noun compound.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Kendall's (\u03c4 ) correlation over WMT 2013 (all- en), for the full dataset and also the subset of the data  containing a noun compound in both the reference and  the MT output", "labels": [], "entities": [{"text": "Kendall's (\u03c4 ) correlation", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.5775494873523712}, {"text": "WMT 2013", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.8628541827201843}]}, {"text": " Table 2: Pearson's (r) correlation results over the WMT  all-en dataset, and the subset of the dataset that contains  noun compounds", "labels": [], "entities": [{"text": "Pearson's (r) correlation", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7571190794308981}, {"text": "WMT  all-en dataset", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.8345754345258077}]}]}