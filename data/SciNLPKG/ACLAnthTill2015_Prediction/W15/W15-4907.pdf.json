{"title": [{"text": "The role of artificially generated negative data for quality estimation of machine translation", "labels": [], "entities": []}], "abstractContent": [{"text": "The modelling of natural language tasks using data-driven methods is often hindered by the problem of insufficient naturally occurring examples of certain linguistic constructs.", "labels": [], "entities": []}, {"text": "The task we address in this paper-quality estimation (QE) of machine translation-suffers from lack of negative examples at training time, i.e., examples of low quality translation.", "labels": [], "entities": [{"text": "machine translation-suffers", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.8143592476844788}]}, {"text": "We propose various ways to artificially generate examples of translations containing errors and evaluate the influence of these examples on the performance of QE models both at sentence and word levels.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of classifying texts as \"correct\" or \"incorrect\" often faces the problem of unbalanced training sets: examples of the \"incorrect\" class can be very limited or even absent.", "labels": [], "entities": []}, {"text": "In many cases, naturally occurring instances of these examples are rare (e.g. incoherent sentences, errors inhuman texts).", "labels": [], "entities": []}, {"text": "In others, the labelling of data is a non-trivial task which requires expert knowledge.", "labels": [], "entities": []}, {"text": "Consider the task of quality estimation (QE) of machine translation (MT) systems output.", "labels": [], "entities": [{"text": "quality estimation (QE) of machine translation (MT)", "start_pos": 21, "end_pos": 72, "type": "TASK", "confidence": 0.7257944264195182}]}, {"text": "When performing binary classification of automatically translated sentences one should provide examples of both bad and good quality sentences.", "labels": [], "entities": [{"text": "binary classification of automatically translated sentences", "start_pos": 16, "end_pos": 75, "type": "TASK", "confidence": 0.7578723331292471}]}, {"text": "Good quality sentences can betaken from any parallel corpus of human translations, whereas there are very few corpora of sentences annotated as having low quality.", "labels": [], "entities": []}, {"text": "These corpora need to be created by human translators, who post-edit automatic translations, mark errors in translations, or rate translations for quality.", "labels": [], "entities": []}, {"text": "This process is slow and expensive.", "labels": [], "entities": []}, {"text": "It is therefore desirable to devise automatic procedures to generate negative training data for QE model learning.", "labels": [], "entities": [{"text": "QE model learning", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8156339923540751}]}, {"text": "Previous work has followed the hypothesis that machine translations can be assumed to have low quality ().", "labels": [], "entities": [{"text": "machine translations", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.6898614764213562}]}, {"text": "However, this is not the case nowadays: many translations can be considered flawless.", "labels": [], "entities": []}, {"text": "Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9119411110877991}]}, {"text": "Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data.", "labels": [], "entities": [{"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9846863746643066}]}, {"text": "Metrics such as BLEU (), TER () and ME-TEOR (Banerjee and can be used to compare the automatic and reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9990344047546387}, {"text": "TER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9973037242889404}, {"text": "ME-TEOR", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9857431054115295}]}, {"text": "However, these scores can be very unreliable, especially for word-level QE, as every word that differs inform or position would be annotated as bad.", "labels": [], "entities": []}, {"text": "Previous efforts have been made for negative data generation, including random generation of sentences from word distributions and the use of translations in low-ranked positions in n-best lists produced by statistical MT (SMT) systems.", "labels": [], "entities": [{"text": "negative data generation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7620838483174642}, {"text": "MT (SMT)", "start_pos": 219, "end_pos": 227, "type": "TASK", "confidence": 0.8381105363368988}]}, {"text": "These methods are however unsuitable for QE at the word level, as they provide no information about the quality of individual words in a sentence.", "labels": [], "entities": [{"text": "QE", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9727972745895386}]}, {"text": "In this paper we adopt a different strategy: we insert errors in otherwise correct sentences.", "labels": [], "entities": []}, {"text": "This provides control over the proportion of errors in the negative data, as well as knowledge about the quality of individual words in the generated sentences.", "labels": [], "entities": []}, {"text": "The goals of the research presented here are to understand the influence of artificially generated data (by various methods and in various quan-tities) on the performance of QE models at both sentence and word levels, and ultimately improve upon baseline models by extending the training data with suitable artificially created examples.", "labels": [], "entities": []}, {"text": "In Section 2 we further review existing strategies for artificial data generation.", "labels": [], "entities": [{"text": "artificial data generation", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.8036948839823405}]}, {"text": "We explain our generation strategies in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our experiment and their results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a set of experiments to evaluate the performance of artificially generated data on different tasks of QE at the sentence and word levels.", "labels": [], "entities": []}, {"text": "The tools and resources required for our experiments are: a QE toolkit to build QE models, the training data for them, the data to extract statistics for the generation of additional examples.", "labels": [], "entities": []}, {"text": "The for sentence-level QE we used the QUEST toolkit (.", "labels": [], "entities": []}, {"text": "It trains QE models using sklearn 1 versions of Support Vector Machine (SVM) classifier (for ternary classification task, Section 4.4) and SVM regression (for HTER prediction, Section 4.5).", "labels": [], "entities": [{"text": "ternary classification task", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.7190241614977518}, {"text": "HTER prediction", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.8003071844577789}]}, {"text": "The wordlevel version of QUEST 2 was used for word-level feature extraction.", "labels": [], "entities": [{"text": "word-level feature extraction", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.7035234173138937}]}, {"text": "Word-level classifiers were trained with CRFSuite 3 . The CRF error models were trained with CRF++ 4 . POS tagging was performed with TreeTagger (.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.8071751296520233}]}, {"text": "Sentence-level QuEst uses 17 baseline features for all tasks.", "labels": [], "entities": []}, {"text": "Word-level QuEst reimplements the set of 30 baseline features described in ().", "labels": [], "entities": []}, {"text": "The QE models were built and tested based on the data provided for the WMT14 English-Spanish QE shared task (Section 4.3).", "labels": [], "entities": [{"text": "WMT14 English-Spanish QE shared task", "start_pos": 71, "end_pos": 107, "type": "DATASET", "confidence": 0.8761636972427368}]}, {"text": "The statistics on error distributions were computed using the English-Spanish part of training data for WMT13 shared task on QE . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus).", "labels": [], "entities": [{"text": "WMT13 shared task on QE", "start_pos": 104, "end_pos": 127, "type": "DATASET", "confidence": 0.7770047307014465}, {"text": "Europarl corpus", "start_pos": 237, "end_pos": 252, "type": "DATASET", "confidence": 0.9957838952541351}]}, {"text": "We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (.", "labels": [], "entities": [{"text": "phrase table building", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.6680756211280823}]}, {"text": "For all the methods, errors were injected into the News Commentary corpus 7 .  We evaluated the performance of the artificially generated data in three tasks: the ternary classification of sentences as \"good\", \"almost good\" or \"bad\", the prediction of HTER () score fora sentence, and the classification of words in a sentence as \"good\" or \"bad\" (tasks 1.1, 1.2 and 2 of WMT14 QE shared task 8 , respectively).", "labels": [], "entities": [{"text": "News Commentary corpus 7", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.8985267281532288}, {"text": "HTER () score", "start_pos": 252, "end_pos": 265, "type": "METRIC", "confidence": 0.8545657992362976}, {"text": "WMT14 QE", "start_pos": 371, "end_pos": 379, "type": "DATASET", "confidence": 0.879144936800003}]}, {"text": "The goal of the experiments was to check whether it is possible to improve upon the baseline results by adding artificially generated examples to the training sets.", "labels": [], "entities": []}, {"text": "The baseline models for all tasks were trained on the data provided for the corresponding shared tasks for the English-Spanish language pair.", "labels": [], "entities": []}, {"text": "All models were tested on the official test sets provided for the corresponding shared tasks.", "labels": [], "entities": []}, {"text": "Since we know how many errors were injected into the sentences, we know the TER scores for our artificial data.", "labels": [], "entities": [{"text": "TER scores", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9744508862495422}]}, {"text": "The discrete labels for the ternary classification task are defined as follows: \"bad\" sentences have four or more non-adjacent errors (two adjacent erroneous words are considered one error), \"almost good\" sentences contain one erroneous phrase (possibly of several words), and \"good\" sentences are error-free.", "labels": [], "entities": [{"text": "ternary classification task", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.9186400771141052}]}, {"text": "The new training examples were added to the baseline datasets.", "labels": [], "entities": []}, {"text": "We ran a number of experiments gradually increasing the number of artificially generated sentences used.", "labels": [], "entities": []}, {"text": "At every run, the new data was chosen randomly in order to reduce the influence of outliers.", "labels": [], "entities": []}, {"text": "In order to make the results more stable, we ran each experiment 10 times and averaged the evaluation scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexities of the artificial datasets", "labels": [], "entities": []}]}