{"title": [{"text": "Improving cross-domain dependency parsing with dependency-derived clusters", "labels": [], "entities": [{"text": "Improving cross-domain dependency parsing", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8464417308568954}]}], "abstractContent": [{"text": "This paper describes a semi-supervised approach to improving statistical dependency parsing using dependency-based word clusters.", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.7058027187983195}]}, {"text": "After applying a baseline parser to unlabeled text, clusters are induced using K-means with word features based on the dependency structures.", "labels": [], "entities": []}, {"text": "The parser is then retrained using information about the clusters, yielding improved parsing accuracy on a range of different data sets, including WSJ and the English Web Treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9690495133399963}, {"text": "WSJ", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.9670022130012512}, {"text": "English Web Treebank", "start_pos": 159, "end_pos": 179, "type": "DATASET", "confidence": 0.9687078595161438}]}, {"text": "We report improved results using both in-domain and out-of-domain data, and also include a comparison with using n-gram-based Brown clustering.", "labels": [], "entities": []}], "introductionContent": [{"text": "Several recent studies have attempted to improve dependency parsers by including information about word clusters into their statistical parsing models.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7874260544776917}]}, {"text": "This is typically motivated by at least two concerns, both of which relate to the shortage of labeled training data.", "labels": [], "entities": []}, {"text": "As argued by, the lexicalized statistics important to disambiguation in parsing are often sparse, and modeling relationships on a more general level than the words themselves may therefore be helpful.", "labels": [], "entities": []}, {"text": "The other motivation is domain adaptation, attempting to leverage a parsing model for use on data from anew domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7731009423732758}]}, {"text": "By including information about word clusters estimated from unlabeled in-domain data, one can hope to reduce the loss in performance expected from using a parser trained on an out-of-domain treebank.", "labels": [], "entities": []}, {"text": "While previous approaches have typically relied on the n-gram-based Brown clustering, this paper instead describes experiments using dependency-based word clusters formed using the generic clustering algorithm Kmeans.", "labels": [], "entities": []}, {"text": "After applying a baseline dependency parser to unlabeled text, K-means is applied to form word clusters with features based on the dependency structures produced by the parser.", "labels": [], "entities": []}, {"text": "The parser is then re-trained using features that record information about the dependency-derived clusters, thereby introducing an element of selftraining.", "labels": [], "entities": []}, {"text": "The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) () and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB).", "labels": [], "entities": [{"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9629260301589966}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9166630506515503}, {"text": "English Web Treebank (EWT)", "start_pos": 129, "end_pos": 155, "type": "DATASET", "confidence": 0.9405772487322489}, {"text": "Wall Street Journal (WSJ)", "start_pos": 167, "end_pos": 192, "type": "DATASET", "confidence": 0.8430061042308807}, {"text": "Penn Treebank (PTB)", "start_pos": 208, "end_pos": 227, "type": "DATASET", "confidence": 0.9302356123924256}]}, {"text": "We document improvements using both in-domain and outof-domain data, and also when compared to using Brown clusters.", "labels": [], "entities": []}, {"text": "All our parsing experiments use MaltParser (), a data-driven transition-based dependency parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9624360203742981}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of previous work.", "labels": [], "entities": []}, {"text": "Section 3 details the data sets we use, including comments on the pre-processing.", "labels": [], "entities": []}, {"text": "Section 4 then describes the experimental set-up, while the actual experiments and results are described in Section 5.", "labels": [], "entities": []}, {"text": "A summary with thoughts about future directions is provided in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the set-up of the experimental process.", "labels": [], "entities": []}, {"text": "We start by describing the set-up for the clustering before turning to how the parser is trained and applied.", "labels": [], "entities": []}, {"text": "The actual experiments and results are the provided in Section 5.", "labels": [], "entities": []}, {"text": "The development results reported below are obtained by; parsing unlabeled data using the baseline feature set trained on WSJ 02-21; computing lemma clusters from dependency features; retraining the parser on WSJ 02-21 with the augmented Form all feature set; and finally tuning the number of clusters (K) and the C parameter 5 of  the parser's SVM classifier to find the configuration with the highest LAS.", "labels": [], "entities": [{"text": "WSJ 02-21", "start_pos": 121, "end_pos": 130, "type": "DATASET", "confidence": 0.9688137173652649}, {"text": "WSJ 02-21", "start_pos": 208, "end_pos": 217, "type": "DATASET", "confidence": 0.9647665321826935}]}, {"text": "The best configuration found for the development data is then applied to the held-out data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of sentences and tokens in the unlabeled corpora used for clustering.", "labels": [], "entities": []}, {"text": " Table 2: The number of sentences and tokens in the labeled corpora used for parsing.", "labels": [], "entities": []}, {"text": " Table 4: The effect on LAS for training on gold  vs. predicted PoS tags, when testing on predicted  PoS tags.", "labels": [], "entities": []}, {"text": " Table 5: LAS results for the development data  -WSJ 22 and the two SANCL test domains - comparing the baseline parser to parsers re-trained  using word clusters from various sources of un- labeled data: the Reuters corpus, the unlabeled  SANCL data for the respective domains, or clus- tering all the unlabeled SANCL data from all five  domains together. (All data sets are PoS-tagged  automatically using SVMTool.)", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9345704317092896}, {"text": "WSJ 22", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9026888608932495}, {"text": "Reuters corpus", "start_pos": 208, "end_pos": 222, "type": "DATASET", "confidence": 0.9718109965324402}]}, {"text": " Table 6: Held-out LAS evaluation on the three  SANCL test domains using the baseline parser  compared to parsers re-trained with informa- tion about word clusters generated from vari- ous sources: the Reuters corpus, the unlabeled  SANCL data for the respective test domains, or  clustering all the unlabeled SANCL data from all  five domains together. (All data sets are PoS- tagged automatically using SVMTool.)", "labels": [], "entities": [{"text": "Reuters corpus", "start_pos": 202, "end_pos": 216, "type": "DATASET", "confidence": 0.9743175804615021}, {"text": "SVMTool", "start_pos": 405, "end_pos": 412, "type": "DATASET", "confidence": 0.9173069596290588}]}, {"text": " Table 7: Comparing LAS with \u00d8vrelid and  Skjaerholt (2012), using data sets with automatic  part-of-speech tags generated by SVMTool.", "labels": [], "entities": []}]}