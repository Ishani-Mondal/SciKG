{"title": [{"text": "Unsupervised training of maximum-entropy models for lexical selection in rule-based machine translation", "labels": [], "entities": [{"text": "rule-based machine translation", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.68942791223526}]}], "abstractContent": [{"text": "This article presents a method of training maximum-entropy models to perform lexical selection in a rule-based machine translation system.", "labels": [], "entities": []}, {"text": "The training method described is unsupervised; that is, it does not require any annotated corpus.", "labels": [], "entities": []}, {"text": "The method uses source-language monolingual corpora, the machine translation (MT) system in which the models are integrated, and a statistical target-language model.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8334864139556885}]}, {"text": "Using the MT system, the sentences in the source-language corpus are translated in all possible ways according to the different translation equivalents in the bilingual dictionary of the system.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.8376025557518005}]}, {"text": "These translations are then scored on the target-language model and the scores are normalised to provide fractional counts for training source-language maximum-entropy lexical-selection models.", "labels": [], "entities": []}, {"text": "We show that these models can perform equally well, or better, than using the target-language model directly for lexical selection, at a substantially reduced computational cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpus-based machine translation (MT) has been the primary research direction in the field of MT in recent years.", "labels": [], "entities": [{"text": "Corpus-based machine translation (MT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7736683140198389}, {"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9913474917411804}]}, {"text": "However, rule-based MT (RBMT) systems are still being developed, and there are many successful commercial and non-commercial systems.", "labels": [], "entities": [{"text": "MT (RBMT)", "start_pos": 20, "end_pos": 29, "type": "TASK", "confidence": 0.7785256654024124}]}, {"text": "One reason for the continued development of RBMT systems is that in order to be successful, corpus-based MT requires parallel corpora in the order of tens of millions of words.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.9544366002082825}, {"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9123463034629822}]}, {"text": "Although for some language pairs these exist, they only exist fora fraction of the world's languages.", "labels": [], "entities": []}, {"text": "An RBMT system typically consists of an analysis component, 1 a transfer component and a generation component.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.8842684030532837}]}, {"text": "As part of the transfer component it is necessary to make choices regarding words in the source language (SL) which may have more than one translation in the target language (TL).", "labels": [], "entities": []}, {"text": "Lexical selection is the task of choosing, fora given SL word, the most adequate translation in the TL among a known set of alternatives.", "labels": [], "entities": [{"text": "Lexical selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.88846355676651}]}, {"text": "The task is related to the task of word-sense disambiguation).", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7328530848026276}]}, {"text": "However, it is different to word-sense disambiguation in that lexical selection is a bilingual problem, not a monolingual problem: its aim is to find the most adequate translation, not the most adequate sense.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7462473511695862}]}, {"text": "Thus, it is not necessary to choose among a series of fine-grained senses if all these senses result in the same final translation; however, it may sometimes be necessary to choose a different translation for the same sense, for example in a collocation.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the training and evaluation settings used in the remainder of this paper.", "labels": [], "entities": []}, {"text": "The primary motivation behind the evaluation is that it should be automatic, meaningful, and be performed over a test set which is large enough to be representative.", "labels": [], "entities": []}, {"text": "It should evaluate both performance on the specific subtask of lexical selection, and on the whole translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.8970244526863098}]}, {"text": "Evaluating lexical-selection performance is an intrinsic module-based evaluation.", "labels": [], "entities": []}, {"text": "It measures how well the lexical selection module disambiguates the lexical-transfer output as compared to a gold-standard corpus.", "labels": [], "entities": []}, {"text": "The lexical transfer output is the result of looking up the translations of the SL lexical forms -lemmas and tags -in the bilingual dictionary.", "labels": [], "entities": []}, {"text": "The whole translation task evaluation is an extrinsic evaluation, which tests how the system improves as regards final translation quality in areal system.", "labels": [], "entities": [{"text": "translation task evaluation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8247905373573303}]}, {"text": "The lexical-selection module should be as language-independent as possible.", "labels": [], "entities": []}, {"text": "To that end, the language pairs tested show a wide variety of linguistic phenomena.", "labels": [], "entities": []}, {"text": "It is also important that the methodology be as applicable to lesser-resourced and marginalised languages as to major languages.", "labels": [], "entities": []}, {"text": "This section begins with a short description of the Apertium platform (Forcada et al., 2011).", "labels": [], "entities": [{"text": "Apertium platform", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9749544560909271}]}, {"text": "This is followed by an overview of each of the language pairs chosen for the evaluation.", "labels": [], "entities": []}, {"text": "The corpora to be used for training and evaluation will subsequently be described, along with the method used for annotating them.", "labels": [], "entities": []}, {"text": "This is followed by a description of the performance measures to be used in the evaluation, and the reference results using these metrics for each of the language pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics about the test corpora. The columns SL  and TL give the number of tokens in the source and target  languages respectively. The columns amb. words and % am- big gives the number of word with more than one translation  and the percentage of SL words which have more than one  translation respectively.", "labels": [], "entities": []}, {"text": " Table 1: Statistics about the source corpora. The column no. amb gives the number of unique tokens with more than one  possible translation. The column av. amb gives the average number of translations per ambiguous word. This is calculated by  looking up each word in the corpus in the bilingual dictionary of the MT system and dividing the total number of translation by  the number of words. Both av. amb and no. amb are calculated over the whole corpus.", "labels": [], "entities": []}, {"text": " Table 3: Features in each rule set and pruning frequency.", "labels": [], "entities": []}]}