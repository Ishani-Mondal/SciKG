{"title": [{"text": "Overview of the DSL Shared Task 2015", "labels": [], "entities": [{"text": "DSL Shared Task", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.5387942492961884}]}], "abstractContent": [{"text": "We present the results of the 2 nd edition of the Discriminating between Similar Languages (DSL) shared task, which was organized as part of the LT4VarDial'2015 workshop and focused on the identification of very similar languages and language varieties.", "labels": [], "entities": [{"text": "Discriminating between Similar Languages (DSL) shared task", "start_pos": 50, "end_pos": 108, "type": "TASK", "confidence": 0.702353212568495}, {"text": "identification of very similar languages and language varieties", "start_pos": 189, "end_pos": 252, "type": "TASK", "confidence": 0.7043721601366997}]}, {"text": "Unlike in the 2014 edition , in 2015 we had an Others category with languages that were not seen on training.", "labels": [], "entities": []}, {"text": "Moreover, we had two test datasets: one using the original texts (test set A), and one with named entities replaced by placeholders (test set B).", "labels": [], "entities": []}, {"text": "Ten teams participated in the task, and the best-performing system achieved 95.54% average accuracy on test set A, and 94.01% on test set B.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.977857232093811}]}], "introductionContent": [{"text": "Identifying the language of an input text is an important step for many natural language processing (NLP) applications, especially when processing speech or social media messages.", "labels": [], "entities": [{"text": "Identifying the language of an input text", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.880516699382237}]}, {"text": "State-of-the-art language identification systems perform very well when discriminating between unrelated languages on standard datasets.", "labels": [], "entities": [{"text": "language identification", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7821195721626282}]}, {"text": "For example, used TED talks and reported 97% accuracy for discriminating between 25 languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9992196559906006}]}, {"text": "Yet, this is not a solved problem, and there area number of scenarios in which language identification has proven to be a very challenging task, especially in the case of very closely-related languages.", "labels": [], "entities": [{"text": "language identification", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.7180346995592117}]}, {"text": "For example, despite their good overall results, had really hard time discriminating between Brazilian and European Portuguese, which has made them propose to \"remove the Brazilian Portuguese and/or merge it with the European Portuguese variant\" to increase system's performance.", "labels": [], "entities": []}, {"text": "So far, researchers in language identification have focused on the following challenges: \u2022 Increasing the coverage of language identification systems by extending the number of languages that are recognizable, e.g., trained a system to identify over 1,000 languages, whereas Brown (2014) developed a language identification tool able to discriminate between over 1,300 languages.", "labels": [], "entities": [{"text": "language identification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7170894742012024}, {"text": "language identification", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.7517276406288147}, {"text": "language identification", "start_pos": 300, "end_pos": 323, "type": "TASK", "confidence": 0.7385658025741577}]}, {"text": "\u2022 Improving the robustness of language identification systems, e.g., by training on multiple domains and various text types ().", "labels": [], "entities": [{"text": "language identification", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.71959488093853}]}, {"text": "\u2022 Handling non-standard texts, e.g., very short () or involving code-switching ().", "labels": [], "entities": []}, {"text": "\u2022 Discriminating between very similar languages (, language varieties ( ), and dialects.", "labels": [], "entities": []}, {"text": "It has been argued that the latter challenge is one of the main bottlenecks for state-of-the-art language identification systems ().", "labels": [], "entities": [{"text": "language identification", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.7035922706127167}]}, {"text": "Thus, this was the task that we focused on in our shared task on Discriminating between Similar Languages (DSL), which we organized as part of the LT4VarDial'2015 workshop at RANLP'2015.", "labels": [], "entities": [{"text": "Discriminating between Similar Languages (DSL)", "start_pos": 65, "end_pos": 111, "type": "TASK", "confidence": 0.6660647945744651}, {"text": "RANLP'2015", "start_pos": 175, "end_pos": 185, "type": "DATASET", "confidence": 0.5798059105873108}]}, {"text": "This is the second edition of the task.", "labels": [], "entities": []}, {"text": "The attention received from the research community and the feedback provided by the participants of the first edition motivated us to organize this second DSL shared task, where we made two important changes compared to the first edition.", "labels": [], "entities": []}, {"text": "1 First, in order to simulate a real-world language identification scenario, we included in the testing dataset some languages that were not present in the training dataset.", "labels": [], "entities": [{"text": "language identification", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.7620935440063477}]}, {"text": "Moreover, we included a second test set, where we substituted the named entities with placeholders to make the task more challenging and less dependent on the text topic and domain.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: DSL 2014 results: accuracy.", "labels": [], "entities": [{"text": "DSL 2014 results", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.9157875378926595}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996115565299988}]}, {"text": " Table 4: The participating teams in the DSL 2015 Shared Task.", "labels": [], "entities": [{"text": "DSL 2015 Shared Task", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.5889236778020859}]}, {"text": " Table 5: Closed submission results for test set A.", "labels": [], "entities": []}, {"text": " Table 6. We can see a drop  in accuracy, which is to be expected. Once again,  the MAC team performed best with 94.01% ac- curacy, followed by SUKI and NRC with 93.02%  and 93.01%, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9995524287223816}, {"text": "MAC team", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9016880691051483}, {"text": "ac- curacy", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9637406468391418}, {"text": "SUKI", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.7940064072608948}, {"text": "NRC", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.8762380480766296}]}, {"text": " Table 6: Closed submission results for test set B.", "labels": [], "entities": []}, {"text": " Table 7: Open submission results for test set A.", "labels": [], "entities": []}, {"text": " Table 8: we can see once again improved  performance for NLEL and NRC. 9", "labels": [], "entities": [{"text": "NLEL", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8666467666625977}, {"text": "NRC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8552550077438354}]}, {"text": " Table 8: Open submission results for test set B.", "labels": [], "entities": [{"text": "Open submission", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6574691236019135}]}]}