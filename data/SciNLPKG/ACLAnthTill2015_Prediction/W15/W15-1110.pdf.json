{"title": [{"text": "Modeling fMRI time courses with linguistic structure at various grain sizes", "labels": [], "entities": []}], "abstractContent": [{"text": "Neuroimaging while participants listen to au-diobooks provides a rich data source for theories of incremental parsing.", "labels": [], "entities": []}, {"text": "We compare nested regression models of these data.", "labels": [], "entities": []}, {"text": "These mixed-effects models incorporate linguistic predictors at various grain sizes ranging from part-of-speech bigrams, through sur-prisal on context-free treebank grammars, to incremental node counts in trees that are derived by Minimalist Grammars.", "labels": [], "entities": []}, {"text": "The fine-grained structures make an independent contribution over and above coarser predic-tors.", "labels": [], "entities": []}, {"text": "However, this result only obtains with time courses from anterior temporal lobe (aTL).", "labels": [], "entities": []}, {"text": "In analogous time courses from inferior frontal gyrus, only n-grams improve upon a non-syntactic baseline.", "labels": [], "entities": []}, {"text": "These results support the idea that aTL does combinatoric processing during naturalistic story comprehension , processing that bears a systematic relationship to linguistic structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "The cognitive science of language confronts two different notions of its own subject matter.", "labels": [], "entities": []}, {"text": "One notion is rooted in the psychology of an individual: what states of mind does this person go through as he or she uses language?", "labels": [], "entities": []}, {"text": "The other notion starts from languages themselves.", "labels": [], "entities": []}, {"text": "As a structural system, how does this language differ from another?", "labels": [], "entities": []}, {"text": "There is a tension between these two views.", "labels": [], "entities": []}, {"text": "Classically, this tension is resolved by adopting the Competence Hypothesis.", "labels": [], "entities": []}, {"text": "It suggests that the best description of the language system should also figure as a \"basic component\" in the best description of the language-user.", "labels": [], "entities": []}, {"text": "This Hypothesis is programmatic enough to have received several different interpretations over the years (.", "labels": [], "entities": []}, {"text": "Can a refined version of it be accepted or rejected in light of experimental data?", "labels": [], "entities": []}, {"text": "Recent work with eye-tracking has wrestled with just this question (.", "labels": [], "entities": []}, {"text": "The argument concerns the strength of the fitted coefficients for different types of grammatical predictors.", "labels": [], "entities": []}, {"text": "These \"language model\" predictors contribute to varying degrees in regression models of the eye-fixation record.", "labels": [], "entities": []}, {"text": "In certain cases, it appears that higher-order structure -for instance, phrase structure -is unhelpful.", "labels": [], "entities": [{"text": "phrase structure", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7826360464096069}]}, {"text": "On the other hand, other cases suggest that higher-order structure does shine through in the eye-movement record.", "labels": [], "entities": []}, {"text": "In this debate, fitted coefficients on the more linguisticallysophisticated predictors have been taken to quantify the veridicality of the Competence Hypothesis.", "labels": [], "entities": [{"text": "veridicality", "start_pos": 119, "end_pos": 131, "type": "METRIC", "confidence": 0.9826837182044983}]}, {"text": "The linguistic predictors that researchers examine qualify as \"basic components\" to the extent that they improve the regression model that they are part of.", "labels": [], "entities": []}, {"text": "Of course, psycholinguists have known fora longtime that low-level factors such as word frequency and bigram probability are useful in explaining eye-fixation times ().", "labels": [], "entities": [{"text": "bigram probability", "start_pos": 102, "end_pos": 120, "type": "METRIC", "confidence": 0.8904179334640503}]}, {"text": "These are not relevant to the Competence Hypothesis.", "labels": [], "entities": []}, {"text": "Rather, the action is with higher-order factors: predictors based on larger domains of locality, as defined by grammars that could plausibly play a role in the best descrip-tion of language as a structural system.", "labels": [], "entities": []}, {"text": "The research reported in this paper adopts the same model-comparison methodology as Frank, Fossum, van Schijndel and their co-authors.", "labels": [], "entities": []}, {"text": "But it applies this method to spatially localized neural time courses obtained using fMRI.", "labels": [], "entities": []}, {"text": "Using grammatical predictors at six different levels of \"richness\" we compare a family of nested regression models.", "labels": [], "entities": []}, {"text": "We find that phrase structure in the style of the Penn Treebank () improves a regression, over and above various n-gram baselines.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9947342872619629}]}, {"text": "X-bar structures generated by Minimalist Grammars) improve yet further over that.", "labels": [], "entities": []}, {"text": "This holds for time courses taken from anterior temporal lobe (aTL), an area that has been implicated in \"basic syntactic processing\".", "labels": [], "entities": []}, {"text": "But only the n-gram predictors are useful in modeling time courses from inferior frontal gyrus (IFG), a traditional syntax area ().", "labels": [], "entities": []}, {"text": "Section 6 discusses this pattern of results in light of other work on naturalistic language comprehension.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}