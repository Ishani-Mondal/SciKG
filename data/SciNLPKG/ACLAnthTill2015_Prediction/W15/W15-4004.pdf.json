{"title": [{"text": "Joint Semantic Relevance Learning with Text Data and Graph Knowledge", "labels": [], "entities": [{"text": "Semantic Relevance Learning", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.7882882754007975}]}], "abstractContent": [{"text": "Inferring semantic relevance among entities (e.g., entries of Wikipedia) is important and challenging.", "labels": [], "entities": [{"text": "semantic relevance among entities (e.g., entries of Wikipedia)", "start_pos": 10, "end_pos": 72, "type": "TASK", "confidence": 0.6801956268874082}]}, {"text": "According to the information resources, the inference can be categorized into learning with either raw text data, or labeled text data (e.g., wik-i page), or graph knowledge (e.g, Word-Net).", "labels": [], "entities": []}, {"text": "Although graph knowledge tends to be more reliable, text data is much less costly and offers a better coverage.", "labels": [], "entities": []}, {"text": "We show in this paper that different resources are complementary and can be combined to improve semantic learning.", "labels": [], "entities": [{"text": "semantic learning", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.9335200190544128}]}, {"text": "Particularly, we present a joint learning approach that learns vectors of entities by leveraging resources of both text data and graph knowledge.", "labels": [], "entities": []}, {"text": "The experiments conducted on the semantic relatedness task show that text-based learning works well on general domain tasks, however for tasks in specific domains, joint learning that involves both text data and graph knowledge offers significant improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the development of deep learning and the establishment of large knowledge bases, knowledge embedding has gained much interest in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.652604748805364}]}, {"text": "In general, knowledge can be represented by some entities that represent semantic concepts, plus the relations among them.", "labels": [], "entities": []}, {"text": "Knowledge embedding involves representing entities of knowledge bases in a low-dimensional continuous space so that the relations among them can be well represented.", "labels": [], "entities": [{"text": "Knowledge embedding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7708740830421448}]}, {"text": "The embedding can be conducted with different objectives with different tasks in concern.", "labels": [], "entities": []}, {"text": "This paper focuses on the semantic learning task which intends to optimize the semantic relevance among entities by knowledge embedding, e.g., inferring appropriate knowledge (entity) vectors.", "labels": [], "entities": []}, {"text": "According to the information resource that is used to learn with, knowledge embedding can be classified into three categories: raw text learning, labeled text learning and graph knowledge learning.", "labels": [], "entities": []}, {"text": "In the raw text learning, the entities are treated as words or phrases, and the local word context information in the raw text is used to drive the embedding.", "labels": [], "entities": []}, {"text": "Various approaches of word/phrase embedding belong to this category ().", "labels": [], "entities": [{"text": "word/phrase embedding", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.6030188202857971}]}, {"text": "In the labeled text learning, the embedding is based on the description text associated to each entity.", "labels": [], "entities": []}, {"text": "A simple approach belonging to this category derives the vector of an entity by averaging the word vectors of the description associated to the entity.", "labels": [], "entities": []}, {"text": "Essentially, the knowledge used in this learning is the cooccurrence statistics of the words in the descriptions.", "labels": [], "entities": []}, {"text": "Finally, in the graph knowledge learning, the relations among entities labeled by people are used to direct the embedding.", "labels": [], "entities": []}, {"text": "Representative approaches of this category include) and.", "labels": [], "entities": []}, {"text": "Different information resources possess their respective advantages and disadvantages.", "labels": [], "entities": []}, {"text": "Raw text is totally unstructured and unsupervised (no data annotated).", "labels": [], "entities": []}, {"text": "The training data is easy to be collected and inmost cases, it offers good entity coverage.", "labels": [], "entities": []}, {"text": "The shortcoming, however, is that the useful information is often buried in noise and therefore it is not trivial to extract the desired information.", "labels": [], "entities": []}, {"text": "Finally, the learning purely relies on word occurrence statistics, which often under-estimates entities that are infrequent in the training data.", "labels": [], "entities": []}, {"text": "labeled text offers a text description for each entity, so it is more supervised than raw text in the sense that some human-specified annotations are involved.", "labels": [], "entities": []}, {"text": "However, the supervision is rather weak, since the relations among entities are not explicitly annotated but implicitly encoded within word co-occurrences of entity descriptions.", "labels": [], "entities": []}, {"text": "A particular advantage of the labeled text learning is that the entities that are difficult to learn with raw text because of their limited occurrences can be learned by referring to the words in the descriptions, for instance by averaging the vectors of the words.", "labels": [], "entities": []}, {"text": "Finally, graph knowledge is the most structured and supervised information resource.", "labels": [], "entities": []}, {"text": "It is annotated by people and therefore is much more clean and reliable, and the relations among entities can be far beyond the ones that are represented byword local contexts as in raw text.", "labels": [], "entities": []}, {"text": "Additionally, the learning does not rely on word statistics and so is mostly suitable for new and infrequent entities, for instance those in a specific domain.", "labels": [], "entities": []}, {"text": "An obvious disadvantage of graph knowledge is the high cost in data annotation and the low coverage of the entities and relations.", "labels": [], "entities": []}, {"text": "The emergence of large-scale public knowledge bases such as Freebase and Yago partly solved the problem, however for many infrequent entities, the annotations are far from satisfactory and most of the relations are missing.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9521262049674988}, {"text": "Yago", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.8090956807136536}]}, {"text": "Due to the respective advantages and disadvantages of different information resources, it is natural to combine them to provide better knowledge embedding.", "labels": [], "entities": []}, {"text": "A number of researches have been conducted in this direction.", "labels": [], "entities": []}, {"text": "For example, proposed a method to employ graph knowledge to improve word embedding, and  used text data to assist new relation discovery for graph knowledge bases.", "labels": [], "entities": [{"text": "word embedding", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7347764670848846}, {"text": "relation discovery", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7419494986534119}]}, {"text": "Nevertheless, there is not a satisfactory framework to learn with multiple and heterogeneous information resources.", "labels": [], "entities": []}, {"text": "Particularly, there is limited investigation onto what extent heterogeneous information can be complementary and how they contribute in different situations.", "labels": [], "entities": []}, {"text": "This paper presents a joint learning approach that learns entity vectors by leveraging resources of both raw and labeled text as well as graph knowledge.", "labels": [], "entities": []}, {"text": "We first present a joint text learning approach which learns word and entity vectors together with both raw and labeled text.", "labels": [], "entities": []}, {"text": "This is similar to the paragraph vector (PV) model) though a different training approach is adopted in our study.", "labels": [], "entities": []}, {"text": "This joint text learning approach is then combined with the graph knowledge learning to form a joint text and graph learning, by integrating the cost functions of the two learning methods.", "labels": [], "entities": []}, {"text": "The experiments are conducted with three information resources: Wikipedia as the raw and labeled text, WordNet and Yago as the graph knowledge.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.9339579939842224}, {"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.966000497341156}]}, {"text": "The entity relatedness task is selected to evaluate the performance of the learning methods.", "labels": [], "entities": []}, {"text": "Two scenarios have been conducted, one is based on WordNet and the other is based on Yago.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9785664081573486}, {"text": "Yago", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9567441940307617}]}, {"text": "The test on WordNet is a general domain task while the test on Yago is a specific domain task.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9750442504882812}, {"text": "Yago", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9431790709495544}]}, {"text": "The experimental results show that the joint text learning offers consistent improvement compared to learning with raw text only.", "labels": [], "entities": []}, {"text": "When involving graph knowledge, the performance on the general domain task does not show apparent improvement, however on the specific domain task, a significant performance improvement has been observed.", "labels": [], "entities": []}, {"text": "These results confirm the importance of learning with heterogeneous information resources.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 briefly describes the related works, Section 3 presents the joint learning approach.", "labels": [], "entities": []}, {"text": "The experiments are presented in Section 4, and some discussions are in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports the experimental settings and results.", "labels": [], "entities": []}, {"text": "The semantic relatedness task was chosen in the study, which measures semantic relatedness among entities and compare the measurements with human-specified scores.", "labels": [], "entities": []}, {"text": "We start by presenting the databases, and then report the results on a general domain task and a specific domain task.", "labels": [], "entities": []}, {"text": "The data sets and codes are available online.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sets for knowledge embedding and relatedness test.", "labels": [], "entities": []}, {"text": " Table 2: Experimental results with raw text learning, labeled text learning, joint text learning and graph  knowledge learning. Bold numbers shows the highest performance in each column.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results with joint text and  graph learning, where \u03b2 has been optimized.", "labels": [], "entities": []}, {"text": " Table 4: Experimental results with various graph- based entity relateness inference methods.", "labels": [], "entities": []}]}