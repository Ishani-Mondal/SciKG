{"title": [], "abstractContent": [{"text": "In this paper, we introduce the notion of visually descriptive language (VDL)-intuitively a text segment whose truth can be confirmed by visual sense alone.", "labels": [], "entities": []}, {"text": "VDL can be exploited in many vision-based tasks, e.g. image interpretation and story illustration.", "labels": [], "entities": [{"text": "image interpretation", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7785618603229523}, {"text": "story illustration", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.778304785490036}]}, {"text": "In contrast to previous work requiring pre-aligned texts and images, we propose a broader definition of VDL that extends to a much larger range of texts without associated images.", "labels": [], "entities": []}, {"text": "We also discuss possible VDL annotation tasks and make recommendations for difficult cases.", "labels": [], "entities": [{"text": "VDL annotation", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.8434775471687317}]}, {"text": "Lastly, we demonstrate the viability of our definition via an annotation exercise across several text genres and analyse inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Results show reasonably high levels of agreement between annotators can be reached.", "labels": [], "entities": [{"text": "agreement", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9703782796859741}]}], "introductionContent": [{"text": "Recent years have seen rapid growth in research integrating visual and textual modalities, including associating named entities in captions with faces in images (), generating image descriptions), text/image retrieval, story illustration, and learning visual recognition of fine-grained object categories (.", "labels": [], "entities": [{"text": "text/image retrieval", "start_pos": 197, "end_pos": 217, "type": "TASK", "confidence": 0.5859009400010109}, {"text": "story illustration", "start_pos": 219, "end_pos": 237, "type": "TASK", "confidence": 0.7981472313404083}, {"text": "learning visual recognition of fine-grained object categories", "start_pos": 243, "end_pos": 304, "type": "TASK", "confidence": 0.825019223349435}]}, {"text": "This previous work concentrates on solving image-based tasks, and is heavily reliant upon datasets with pre-aligned images and texts, most of which have been manually collected and/or annotated.", "labels": [], "entities": []}, {"text": "Thus, such imagecentric texts are assumed to beat least partially, if not predominantly, 'visually descriptive' in nature.", "labels": [], "entities": []}, {"text": "This raises some interesting research questions: (i) how much text out there without associated images is 'visually descriptive' and thus potentially useful for such image-based tasks?", "labels": [], "entities": []}, {"text": "(ii) can these 'visually descriptive' text segments be identified automatically within documents which may consist of predominantly 'non-visual' text?", "labels": [], "entities": []}, {"text": "To be able to answer these questions, we first require a robust, inter-subjectively reliable definition of 'visually descriptive' text.", "labels": [], "entities": []}, {"text": "Although previous work exists that models the 'visualness' of terms or concepts from images, they are presented without an explicit definition apart from the intuitive notion that a visual term should exhibit some consistent visual characteristics across different objects.", "labels": [], "entities": []}, {"text": "To our knowledge, the only work that explicitly proposes a definition for visually descriptive text is that of, where noun phrases within an image caption are classified as to whether or not they are depicted in the corresponding image.", "labels": [], "entities": []}, {"text": "In this paper, we propose a broader definition of Visually Descriptive Language (VDL).", "labels": [], "entities": []}, {"text": "Our work differs from in that our definition revolves around identifying text segments that express propositions that can be 'visually confirmed' rather than identifying 'visually concrete' noun phrase segments whose denotation can be located in an associated image.", "labels": [], "entities": []}, {"text": "The consequences of this different definition are significant: (i) we are not restricted to mining VDL from texts with associated images, but can exploit any text, massively extending the volume of data that can be mined; (ii) we can gather larger, richer fragments of text than just noun phrases; (iii) we are not limited to the sort of language found in image captions or texts with embedded images (typically news), but can consider texts of any genre.", "labels": [], "entities": []}, {"text": "It is unlikely there is anyone 'correct' definition of VDL.", "labels": [], "entities": [{"text": "VDL", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.7826506495475769}]}, {"text": "Rather, any proposed definition maybe assessed in terms of how useful it is for some particular purpose and how easy it is to apply.", "labels": [], "entities": []}, {"text": "Our purpose in defining VDL is to allow us to identify, within abroad corpus of texts, segments that can be used to inform computational models useful in image interpretation and description.", "labels": [], "entities": [{"text": "image interpretation and description", "start_pos": 154, "end_pos": 190, "type": "TASK", "confidence": 0.7466226071119308}]}, {"text": "For example, co-occurrence in VDL of certain attribute values and object types, or of pairs of objects types, or of object types in particular semantic roles in relation to an activity or event type provide prior information that can be used in Bayesian models to help interpret or describe anew image.", "labels": [], "entities": []}, {"text": "Corpora of VDL can also be used to learn language models for generating image descriptions, e.g. for the visually impaired.", "labels": [], "entities": []}, {"text": "Other potential applications include identifying candidate text segments within a novel to be illustrated, automatic collection of joint visual-text training data, and automatic extraction of discriminative object descriptions for visual recognition (e.g. butterfly descriptions in).", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 231, "end_pos": 249, "type": "TASK", "confidence": 0.727624773979187}]}], "datasetContent": [{"text": "A small pilot annotation exercise was carried out to test the viability of our definition and annotation guidelines on a variety of text genres.", "labels": [], "entities": []}, {"text": "As data we used two random chapters from The Wonderful Wizard of Oz and six samples from the Brown Corpus, selected randomly among five hand-picked categories (two news articles, one biography and three novels).", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.9912689328193665}]}, {"text": "As a pilot study, all texts were annotated by the authors at segmentlevel, the Oz texts by three annotators and the Brown texts by two, using the brat rapid annotation tool 4 . Sentence-level annotations are inferred from these segment-level annotations.", "labels": [], "entities": []}, {"text": "We chose to annotate at segment level rather than sentence level as identifying VDL segments must be done mentally at sentence level anyway.", "labels": [], "entities": []}, {"text": "Marking the segments directly with just a little additional effort will result in a more informative resource.", "labels": [], "entities": []}, {"text": "shows the selected texts and an analysis of the resulting annotations.", "labels": [], "entities": []}, {"text": "All texts are of similar length (mean 10,834, standard deviation 1,558 characters).", "labels": [], "entities": []}, {"text": "Column |S| shows the number of sentences in each corpus.", "labels": [], "entities": []}, {"text": "Columns S=1 and S=2 shows the average proportion of sentences labelled for each VDL type (VDL or partially VDL), and columns VDL and IVDL the number of segments marked as pure and impure VDL on average (rounded to the nearest integer).", "labels": [], "entities": []}, {"text": "Percentage agreement (% Agree) and Kappa are computed at the sentence level.", "labels": [], "entities": [{"text": "Percentage agreement", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.8963740468025208}, {"text": "Agree", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.7462039589881897}, {"text": "Kappa", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9504758715629578}]}, {"text": "We also report an analysis of the annotation at the segment level: column IoU (Intersection-over-Union) shows the overlap of the annotations at word level; i.e. the ratio of words labelled by two annotators as visually descriptive to total number of labelled words by any annotator; at this point we did not distinguish between pure and impure VDL.", "labels": [], "entities": [{"text": "IoU", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9388329982757568}]}, {"text": "Figures for the Oz data are averaged pairwise scores over the three annotators.", "labels": [], "entities": [{"text": "Oz data", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.8741964995861053}]}], "tableCaptions": [{"text": " Table 1: Selected texts and results of the annotation experiment. Column |S| shows the number of  sentences, columns S=1 and S=2 the average proportion of sentences labelled for each VDL type, and  columns VDL and IVDL the number of segments marked as pure and impure VDL on average. Columns  % Agree and Kappa show the inter-annotator agreement at sentence level, and IoU the agreement at  segment level. Please refer to main text for more details.", "labels": [], "entities": [{"text": "IoU", "start_pos": 370, "end_pos": 373, "type": "METRIC", "confidence": 0.9740020036697388}]}]}