{"title": [{"text": "Quality-adaptive Spoken Dialogue Initiative Selection And Implications On Reward Modelling", "labels": [], "entities": [{"text": "Reward Modelling", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.868770569562912}]}], "abstractContent": [{"text": "Adapting Spoken Dialogue Systems to the user is supposed to result in more efficient and successful dialogues.", "labels": [], "entities": [{"text": "Adapting Spoken Dialogue", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.825173020362854}]}, {"text": "In this work, we present an evaluation of a quality-adaptive strategy with a user simulator adapting the dialogue initiative dynamically during the ongoing interaction and show that it outperforms conventional non-adaptive strategies and a random strategy.", "labels": [], "entities": []}, {"text": "Furthermore , we indicate a correlation between Interaction Quality and dialogue completion rate, task success rate, and average dialogue length.", "labels": [], "entities": []}, {"text": "Finally, we analyze the correlation between task success and interaction quality in more detail identifying the usefulness of interaction quality for modelling the reward of reinforcement learning strategy optimization.", "labels": [], "entities": [{"text": "reinforcement learning strategy optimization", "start_pos": 174, "end_pos": 218, "type": "TASK", "confidence": 0.7106581255793571}]}], "introductionContent": [{"text": "Maximizing task success in task-oriented dialogue systems has always been a central claim of Spoken Dialogue (SDS) research.", "labels": [], "entities": [{"text": "Maximizing task success", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8259567419687907}, {"text": "Spoken Dialogue (SDS)", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.85747789144516}]}, {"text": "Today, commercial systems are still inflexible and do not adapt to users or the dialogue flow.", "labels": [], "entities": []}, {"text": "This usually results in bad performance and in frequently unsuccessful dialogues.", "labels": [], "entities": []}, {"text": "In recent years, adaptation strategies have been investigated for rendering SDS more flexible and robust.", "labels": [], "entities": [{"text": "SDS", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9654756784439087}]}, {"text": "The aim of those strategies is to adapt the dialogue flow based on observations that are made during an ongoing dialogue.", "labels": [], "entities": []}, {"text": "One approach to observe and score the interaction between the system and the user is the Interaction Quality (IQ) () originally presented by . Their Interaction Quality paradigm is one of the first metrics which can be used for this purpose.", "labels": [], "entities": [{"text": "Interaction Quality (IQ)", "start_pos": 89, "end_pos": 113, "type": "METRIC", "confidence": 0.685661667585373}]}, {"text": "A pilot user study on adapting the dialogue to the Interaction Quality by in a limited domain has already shown encouraging results.", "labels": [], "entities": []}, {"text": "There, similar dialogue performance was achieved for both the strategy adapting the grounding mechanism to Interaction Quality and the strategy of always applying implicit confirmation prompts previously known to achieve best user feedback.", "labels": [], "entities": []}, {"text": "While the previous experiment showed encouraging results for adapting the grounding strategy, it is unclear if other aspects of a dialogue strategy may also be positively affected.", "labels": [], "entities": []}, {"text": "Hence, in this contribution, we investigate if applying rules for adapting the dialogue initiative to IQ may also result in an increase in IQ and if other metrics like task success rate or dialogue completion rate may correlate . To investigate this, we have designed a basic experiment having an IQ-adaptive dialogue strategy adapting the dialogue initiative.", "labels": [], "entities": [{"text": "IQ", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9418027400970459}]}, {"text": "Depending on the IQ score, the system chooses between user-initiative, system-initiative and mixedinitiative.", "labels": [], "entities": []}, {"text": "Moreover, the performance of four additional strategies is analyzed regarding a correlation between IQ and other performance measures.", "labels": [], "entities": [{"text": "IQ", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.5273778438568115}]}, {"text": "Besides the interest in the general performance of the quality-adaptive strategy, we are specifically interested whether implications maybe drawn from the experiments about the usage of IQ in a reinforcement learning setting for modelling the reward function.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows: in Section 2, we present significant related work on adaptive dialogue and quality metrics including the Interaction Quality (IQ) paradigm, a more abstract form of user satisfaction.", "labels": [], "entities": []}, {"text": "All five dialogue strategies are described in detail in Section 3.", "labels": [], "entities": []}, {"text": "The experimental setup including the test system in the the \"Let's Go\" domain is presented in Section 4 followed by a thorough presentation of the experimental results based on dialogues with the user simulator.", "labels": [], "entities": []}, {"text": "Based on the experiments' results, inferences are drawn on using IQ for reward modelling.", "labels": [], "entities": []}, {"text": "Finally, we conclude and outline future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of the dialogue strategies presented in Section 3 is performed using an adaptive dialogue system interacting with a user simulator.", "labels": [], "entities": []}, {"text": "A user simulator offers an easy and cost-effective way forgetting a basic impression about the performance of the designed dialogue strategies.", "labels": [], "entities": []}, {"text": "Furthermore, we describe the setup of the experiments followed by a discussion of the results.", "labels": [], "entities": []}, {"text": "In order to evaluate the dialogue strategies, we use the adaptive dialogue manager OwlSpeak (, originally created by, extended for including qualityadaptivity ().", "labels": [], "entities": []}, {"text": "OwlSpeak is based on the Model-View-Presenter paradigm separating the dialogue description and dialogue state in the model from the dialogue control logic in the presenter.", "labels": [], "entities": [{"text": "OwlSpeak", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7750418186187744}]}, {"text": "Originally, the interface to a voice browser using VoiceXML) is embedded in the view.", "labels": [], "entities": []}, {"text": "For this work, the view has been replaced in order to provide an interface to LGUS which is instantiated as a server application communicating to other modules using JSON).", "labels": [], "entities": []}, {"text": "Furthermore, the system has been extended to handle multi-slot user input.", "labels": [], "entities": []}, {"text": "For rendering the system adaptive, included an interaction estimation module into the system.", "labels": [], "entities": []}, {"text": "It is based on the Support Vector Machine (SVM) implementation LibSVM (Chang and Lin, 2011) using a linear kernel.", "labels": [], "entities": []}, {"text": "Interaction with real users requires a more complex system than an interaction with a simulated user.", "labels": [], "entities": []}, {"text": "Thus, some SDS modules are missing and not all parameters of the IQ paradigm are available.", "labels": [], "entities": []}, {"text": "This results in a feature set of only 16 parameters 3 . The trained model achieves an unweighted average recall 4 of 0.56 5 on the training data using 10-fold cross-validation which is a considerably good performance.", "labels": [], "entities": [{"text": "recall 4", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9137000441551208}]}, {"text": "All exchanges of the LEGO corpus ( ) have been used for training.", "labels": [], "entities": [{"text": "LEGO corpus", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.84900763630867}]}, {"text": "Evaluation of the dialogue strategies is performed by creating 5,000 simulated dialogues for each strategy.", "labels": [], "entities": []}, {"text": "Like, short dialogues (\u2264 5 exchanges 6 ) which are considered \"not be genuine attempts at using the system\" are excluded from all statistics in this paper.", "labels": [], "entities": []}, {"text": "Three objective metrics are used to evaluate the dialogue performance: the average dialogue length (ADL), the dialogue completion rate (DCR) and task success rate (TSR).", "labels": [], "entities": [{"text": "average dialogue length (ADL)", "start_pos": 75, "end_pos": 104, "type": "METRIC", "confidence": 0.7883418748776118}, {"text": "dialogue completion rate (DCR)", "start_pos": 110, "end_pos": 140, "type": "METRIC", "confidence": 0.7732071777184805}, {"text": "task success rate (TSR)", "start_pos": 145, "end_pos": 168, "type": "METRIC", "confidence": 0.8011092593272527}]}, {"text": "The ADL is modeled by the average number of exchanges per completed dialogue.", "labels": [], "entities": []}, {"text": "A dialogue is regarded as being completed if the system provides a resultwhether corrector not-to the user.", "labels": [], "entities": []}, {"text": "Hence, DCR represents the ratio of dialogues for which the system was able to provide a result, i.e., provide schedule information: TSR is the ratio of completed dialogues where the user goal matches the information the system acquired during the interaction: Here, only destination place, arrival place, and travel time are considered as the bus number is not a mandatory slot and hence not necessary for providing information to the user.", "labels": [], "entities": [{"text": "TSR", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9981727600097656}]}, {"text": "As a correlation between objective measures and IQ is investigated, the average IQ value (AIQ) is calculated for each strategy based on the IQ The parameters applied are ASRRecognitionStatus, ASRConfidence, RePrompt?, #Exchanges, ActivityType, Confirmation?, MeanASRConfidence, #ASR-Success, %ASRSuccess, #ASRRejections, %ASR-Rejections, {Mean}ASRConfidence, {#}ASRSuccess, {#}ASRRejections, {#}RePrompts, {#}SystemQuestions.", "labels": [], "entities": [{"text": "IQ value (AIQ)", "start_pos": 80, "end_pos": 94, "type": "METRIC", "confidence": 0.6044133365154266}, {"text": "MeanASRConfidence", "start_pos": 259, "end_pos": 276, "type": "METRIC", "confidence": 0.9786540865898132}]}, {"text": "The arithemtic average overall class-wise recalls.", "labels": [], "entities": [{"text": "recalls", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9327265620231628}]}, {"text": "Comparable to the best-know approaches.", "labels": [], "entities": []}, {"text": "The minimum number of exchanges to successfully complete the dialogue is 5.", "labels": [], "entities": []}, {"text": "value of the last exchange of each dialogue.", "labels": [], "entities": []}, {"text": "Furthermore, this measure is also used to investigate if adapting the course of the dialogue to IQ also results in higher IQ values.", "labels": [], "entities": [{"text": "IQ", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.8065202832221985}, {"text": "IQ", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9647766947746277}]}, {"text": "shows the ratio of complete, incomplete, and omitted dialogues for each strategy with respect to the total 5,000 dialogues.", "labels": [], "entities": []}, {"text": "As can be seen, about the same ratio of dialogues is omitted due to being too short.", "labels": [], "entities": []}, {"text": "The DCR clearly varies more strongly for the five strategies.", "labels": [], "entities": [{"text": "DCR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.6729309558868408}]}, {"text": "The results for DCR, TSR, ADL, and AIQ are presented in and.", "labels": [], "entities": [{"text": "TSR", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.8430328965187073}, {"text": "AIQ", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.44629722833633423}]}, {"text": "TSR is almost the same for all strategies, meaning that, if a dialogue completes, the system almost always found the correct user goal.", "labels": [], "entities": [{"text": "TSR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6448389887809753}]}, {"text": "DCR, ADL and AIQ on the other hand vary strongly.", "labels": [], "entities": [{"text": "DCR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6020381450653076}, {"text": "ADL", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.8740290403366089}, {"text": "AIQ", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.8639668822288513}]}, {"text": "They strongly correlate with a Pearson's correlation of \u03c1 = \u22120.953 (\u03b1 < 0.05) for DCR and ADL, \u03c1 = 0.960 (\u03b1 < 0.01) for DCR and AIQ, and \u03c1 = \u2212.997 (\u03b1 < 0.01) for ADL and AIQ.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 31, "end_pos": 52, "type": "METRIC", "confidence": 0.9290889302889506}]}, {"text": "This shows that by improving IQ, being a subjective measure, an increase in objective measures maybe expected.", "labels": [], "entities": [{"text": "IQ", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.64017653465271}]}, {"text": "Comparing the performance of the adaptive strategy to the three non-adaptive strategy clearly shows that the adaptive strategy performs significantly best for all metrics.", "labels": [], "entities": []}, {"text": "With a DCR of of 54.27%, the performance is comparable to the rate achieved on the training data of).", "labels": [], "entities": [{"text": "DCR", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9945422410964966}]}, {"text": "The non-adaptive strategies achieve a much lower DCR having the system initiative strategy as second best with only 29.48%.", "labels": [], "entities": [{"text": "DCR", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9972479939460754}]}, {"text": "This performance goes together with shorter dialogues shown by the ADL.", "labels": [], "entities": [{"text": "ADL", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8914241790771484}]}, {"text": "Furthermore, the results for DCR clearly show that the user initiative strategy is unusable.", "labels": [], "entities": [{"text": "DCR", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.4970569312572479}]}, {"text": "Thus, this strategy will not be analyzed any further.", "labels": [], "entities": []}, {"text": "Furthermore, it is of interest if better objective performance also results in better IQ values for the complete dialogue.", "labels": [], "entities": [{"text": "IQ", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9468228220939636}]}, {"text": "This is especially important since it is imperative for the relevance of the Interaction Quality.", "labels": [], "entities": []}, {"text": "Adapting to IQ to improve the dialogue must also result in an increase of the IQ value.", "labels": [], "entities": [{"text": "IQ", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.7453775405883789}, {"text": "IQ", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9502649307250977}]}, {"text": "This effect has been validated by these experiments.", "labels": [], "entities": []}, {"text": "The adaptive strategy has a significant higher average IQ (AIQ) value calculated from the IQ value for the whole dialogues, i.e., the IQ value of the last system-user-exchange, than all other non-adaptive strategies.", "labels": [], "entities": [{"text": "IQ (AIQ)", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9337829798460007}, {"text": "IQ", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.913530707359314}]}, {"text": "The question remains if adapting to IQ is the actual reason for the improvement.", "labels": [], "entities": [{"text": "IQ", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.6062216758728027}]}, {"text": "Maybe, the user simulated with LGUS only \"likes\" diversified initiative prompts better which is represented by the random strategy.", "labels": [], "entities": []}, {"text": "While this statement is true to some extent (see ADL), reasonably adapting to IQ further improves the system performance significantly as shown by DCR and AIQ.", "labels": [], "entities": [{"text": "DCR", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.849941074848175}, {"text": "AIQ", "start_pos": 155, "end_pos": 158, "type": "DATASET", "confidence": 0.8519381284713745}]}], "tableCaptions": [{"text": " Table 2: Example of the task success rate with re- spect to IQ and the dialogue length (DL). Disre- garding rows with less then 15 dialogues, there is  clearly a trend for higher task success rates if the  IQ value increases as well.", "labels": [], "entities": [{"text": "dialogue length (DL)", "start_pos": 72, "end_pos": 92, "type": "METRIC", "confidence": 0.8740739107131958}]}, {"text": " Table 3: The precision of success and failure di- alogues (along with the unweighted average pre- cision (UAP)) when setting all dialogue with final  IQ greater or equal a given IQ value to be success- ful and the remainder to be a failure.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9996132254600525}, {"text": "unweighted average pre- cision (UAP))", "start_pos": 75, "end_pos": 112, "type": "METRIC", "confidence": 0.862350583076477}]}]}