{"title": [{"text": "Bilingual Word Representations with Monolingual Quality in Mind", "labels": [], "entities": [{"text": "Bilingual Word Representations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7180399696032206}]}], "abstractContent": [{"text": "Recent work in learning bilingual representations tend to tailor towards achieving good performance on bilingual tasks, most often the crosslingual document classification (CLDC) evaluation, but to the detriment of preserving clustering structures of word representations monolin-gually.", "labels": [], "entities": [{"text": "crosslingual document classification (CLDC)", "start_pos": 135, "end_pos": 178, "type": "TASK", "confidence": 0.7618942409753799}]}, {"text": "In this work, we propose a joint model to learn word representations from scratch that utilizes both the context coocurrence information through the monolingual component and the meaning equivalent signals from the bilingual constraint.", "labels": [], "entities": []}, {"text": "Specifically, we extend the recently popular skipgram model to learn high quality bilingual representations efficiently.", "labels": [], "entities": []}, {"text": "Our learned embeddings achieve anew state-of-the-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classification direction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9995525479316711}, {"text": "German to English CLDC task", "start_pos": 78, "end_pos": 105, "type": "DATASET", "confidence": 0.5202720582485199}]}, {"text": "At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the mono-lingual word similarity evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed word representations have been key to the recent success of many neural network models in tackling various NLP tasks such as tagging, chunking), sentiment analysis (), and parsing (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.9349218606948853}, {"text": "parsing", "start_pos": 184, "end_pos": 191, "type": "TASK", "confidence": 0.9745451807975769}]}, {"text": "So far, most of the focus has been spent on monolingual problems despite the existence of a wide variety of multilingual NLP tasks, which include not only machine translation), but also noun bracketing), entity clustering (, and bilingual NER (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7144623696804047}, {"text": "noun bracketing", "start_pos": 186, "end_pos": 201, "type": "TASK", "confidence": 0.7664587795734406}, {"text": "entity clustering", "start_pos": 204, "end_pos": 221, "type": "TASK", "confidence": 0.791327178478241}]}, {"text": "These multilingual applications have motivated recent work in training bilingual representations where similar-meaning words in two languages are embedded close together in the same highdimensional space.", "labels": [], "entities": []}, {"text": "However, most bilingual representation work tend to focus on learning embeddings that are tailored towards achieving good performance on a bilingual task, often the crosslingual document classification (CLDC) task, but to the detriment of preserving clustering structures of word representations monolingually.", "labels": [], "entities": [{"text": "crosslingual document classification (CLDC) task", "start_pos": 165, "end_pos": 213, "type": "TASK", "confidence": 0.804443997996194}]}, {"text": "In this work, we demonstrate that such a goal of learning representations of high quality both bilingually and monolingually is achievable through a joint learning approach.", "labels": [], "entities": []}, {"text": "Specifically, our joint model utilizes both the context concurrence information present in the monolingual data and the meaning equivalent signals exhibited in the parallel data.", "labels": [], "entities": []}, {"text": "The key for our approach to work is in designing a bilingual constraint consistent with monolingual components in our joint objective.", "labels": [], "entities": []}, {"text": "To that end, we propose a novel bilingual skipgram model that extends the recently proposed skipgram approach () to the bilingual context.", "labels": [], "entities": []}, {"text": "Our model is efficient to train and achieves state-of-the-art performance in the CLDC task for the direction from German to English.", "labels": [], "entities": []}, {"text": "At the same time, we demonstrate that our model well preserves the monolingual clustering structures in each language both quantitatively through the word similarity task and qualitatively through our detailed analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on two aspects: (a) monolingually with a word similarity task and (b) bilingually through a cross-lingual document classification setup.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 115, "end_pos": 152, "type": "TASK", "confidence": 0.686598519484202}]}], "tableCaptions": []}