{"title": [{"text": "Short Text Clustering via Convolutional Neural Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "Short text clustering has become an increasing important task with the popularity of social media, and it is a challenging problem due to its sparseness of text representation.", "labels": [], "entities": [{"text": "Short text clustering", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.707990805308024}]}, {"text": "In this paper, we propose a Short Text Clustering via Convolutional neural networks (abbr.", "labels": [], "entities": []}, {"text": "to STCC), which is more beneficial for clustering by considering one constraint on learned features through a self-taught learning framework without using any external tags/labels.", "labels": [], "entities": [{"text": "STCC", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.8223795890808105}]}, {"text": "First, we embed the original keyword features into compact binary codes with a locality-preserving constraint.", "labels": [], "entities": []}, {"text": "Then, word embed-dings are explored and fed into convolution-al neural networks to learn deep feature representations , with the output units fitting the pre-trained binary code in the training process.", "labels": [], "entities": []}, {"text": "After obtaining the learned representations , we use K-means to cluster them.", "labels": [], "entities": []}, {"text": "Our extensive experimental study on two public short text datasets shows that the deep feature representation learned by our approach can achieve a significantly better performance than some other existing features, such as term frequency-inverse document frequency, Laplacian eigenvectors and average embedding , for clustering.", "labels": [], "entities": []}], "introductionContent": [{"text": "Different from the normal text clustering, short text clustering has the problem of sparsity).", "labels": [], "entities": [{"text": "short text clustering", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.6177721520264944}]}, {"text": "Most words only occur once in each short text, as a result, the term frequencyinverse document frequency (TF-IDF) measure cannot work well in the short text setting.", "labels": [], "entities": [{"text": "frequencyinverse document frequency (TF-IDF) measure", "start_pos": 69, "end_pos": 121, "type": "METRIC", "confidence": 0.8695239680153983}]}, {"text": "In order to address this problem, some researchers work on expanding and enriching the context of data from Wikipedia ( or an ontology).", "labels": [], "entities": []}, {"text": "However, these methods involve solid natural language processing knowledge and still use high-dimensional representation which may result in a waste of both memory and computation time.", "labels": [], "entities": []}, {"text": "Another way to overcome these issues is to explore some sophisticated models to cluster short texts.", "labels": [], "entities": []}, {"text": "For example, proposed a Dirichlet multinomial mixture model-based approach for short text clustering and clustered texts using Locality Preserving Indexing (LPI) algorithm.", "labels": [], "entities": [{"text": "short text clustering", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.6446396509806315}]}, {"text": "Yet how to design an effective model is an open question, and most of these methods directly trained based on bagof-words (BoW) are shallow structures which cannot preserve the accurate semantic similarities.", "labels": [], "entities": []}, {"text": "With the recent revival of interest in Deep Neural Network (DNN), many researchers have concentrated on using Deep Learning to learn features.", "labels": [], "entities": []}, {"text": "use deep auto encoder (DAE) to learn text representation from raw text representation.", "labels": [], "entities": []}, {"text": "Recently, with the help of word embedding, neural networks demonstrate their great performance in terms of constructing text representation, such as Recursive Neural Network (RecN-N) and Recurrent Neural Network (RNN)).", "labels": [], "entities": []}, {"text": "However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model.", "labels": [], "entities": []}, {"text": "More recently, Convolution Neural Network (CNN), applying convolutional filters to capture local features, has achieved a better performance in many NLP applications, such as sentence modeling), relation classification (, and other traditional NLP tasks).", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 175, "end_pos": 192, "type": "TASK", "confidence": 0.7470801472663879}, {"text": "relation classification", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.9435697793960571}]}, {"text": "Most of the previous works focus CNN on solving supervised NLP tasks, while in this paper we aim to explore the power of CNN on one unsupervised NLP task, short text clustering.", "labels": [], "entities": [{"text": "short text clustering", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.6347028613090515}]}, {"text": "To address the above challenges, we systematically introduce a short text clustering method via con- volutional neural networks.", "labels": [], "entities": [{"text": "short text clustering", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.6540752450625101}]}, {"text": "An overall architecture of the proposed method is illustrated in.", "labels": [], "entities": []}, {"text": "Given a short text collection X, the goal of this work is to cluster these texts into clusters C based on the deep feature representation h learned from CNN models.", "labels": [], "entities": []}, {"text": "In order to train the CNN models, we, inspired by (, utilize a self-taught learning framework in our work.", "labels": [], "entities": []}, {"text": "In particular, we first embed the original features into compact binary code B with a locality-preserving constraint.", "labels": [], "entities": []}, {"text": "Then word vectors S projected from word embeddings are fed into a CNN model to learn the feature representation hand the output units are used to fit the pretrained binary code B.", "labels": [], "entities": []}, {"text": "After obtaining the learned features, traditional K-means algorithm is employed to cluster texts into clusters C.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are summarized as follows: 1).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first attempt to explore the feasibility and effectiveness of combining CNN and traditional semantic constraint, with the help of word embedding to solve one unsupervised learning task, short text clustering.", "labels": [], "entities": [{"text": "short text clustering", "start_pos": 228, "end_pos": 249, "type": "TASK", "confidence": 0.6253177424271902}]}, {"text": "We learn deep feature representations with locality-preserving constraint through a self-taught learning framework, and our approach do not use any external tags/labels or complicated NLP preprocessing.", "labels": [], "entities": []}, {"text": "We conduct experiments on two short text datasets.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that the proposed method achieves excellent perfor- The remainder of this paper is organized as follows: In Section 2, we first describe the proposed approach STCC and implementation details.", "labels": [], "entities": []}, {"text": "Experimental results and analyses are presented in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we briefly survey several related works.", "labels": [], "entities": []}, {"text": "Finally, conclusions are given in the last Section.", "labels": [], "entities": []}], "datasetContent": [{"text": "The clustering performance is evaluated by comparing the clustering results of texts with the tags/labels provided by the text corpus.", "labels": [], "entities": []}, {"text": "Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance).", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9699772447347641}, {"text": "mutual information metric (NMI)", "start_pos": 51, "end_pos": 82, "type": "METRIC", "confidence": 0.771182581782341}]}, {"text": "Given a text xi , let c i and y i be the obtained cluster label and the label provided by the corpus, respectively.", "labels": [], "entities": []}, {"text": "Accuracy is defined as: where, n is the total number of texts, \u03b4(x, y) is the indicator function that equals one if x = y and equals zero otherwise, and map(c i ) is the permutation mapping function that maps each cluster label c i to the equivalent label from the text data by Hungarian algorithm (.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.992003321647644}]}, {"text": "Normalized mutual information) between tag/label set Y and cluster set C is a popular metric used for evaluating clustering tasks.", "labels": [], "entities": []}, {"text": "It is defined as follows: where, M I(Y, C) is the mutual information between Y and C, H(\u00b7) is entropy and the denominator \u221a H(Y)H(C) is used for normalizing the mutual information to be in the range of.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the text datasets. C: the number of  classes; Num: the dataset size; L(mean/max): the mean  and max length of texts and |V |: the vocabulary size.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of ACC and NMI of clustering methods on two short text datasets. For Spectral Clustering, the  dimension of subspace are set to the number of clusters, and Spectral Clustering (best) get the best performance by  iterating the dimensions ranging from 10:10:200. More details about the baseline setting are described in Section 3.3", "labels": [], "entities": []}]}