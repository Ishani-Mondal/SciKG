{"title": [{"text": "NRC: Infused Phrase Vectors for Named Entity Recognition in Twitter", "labels": [], "entities": [{"text": "NRC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9281551837921143}, {"text": "Named Entity Recognition", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6265594561894735}]}], "abstractContent": [{"text": "Our submission to the W-NUT Named Entity Recognition in Twitter task closely follows the approach detailed by Cherry and Guo (2015), who use a discrimi-native, semi-Markov tagger, augmented with multiple word representations.", "labels": [], "entities": [{"text": "W-NUT Named Entity Recognition in Twitter task", "start_pos": 22, "end_pos": 68, "type": "TASK", "confidence": 0.7378906863076347}]}, {"text": "We enhance this approach with updated gazetteers, and with infused phrase em-beddings that have been adapted to better predict the gazetteer membership of each phrase.", "labels": [], "entities": []}, {"text": "Our system achieves a typed F1 of 44.7, resulting in a third-place finish, despite training only on the official training set.", "labels": [], "entities": [{"text": "typed", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9649055600166321}, {"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.7826392650604248}]}, {"text": "A post-competition analysis indicates that also training on the provided development data improves our performance to 54.2 F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9983978867530823}]}], "introductionContent": [{"text": "Named entity recognition (NER) is the task of finding rigid designators as they appear in free text and assigning them to coarse types such as person or geo-location (.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7922905286153158}]}, {"text": "NER is the first step in many information extraction tasks, but in social media, this task is extremely challenging.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9461279511451721}, {"text": "information extraction tasks", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8708209991455078}]}, {"text": "The text to be analyzed is unedited and noisy, and covers a much more diverse set of topics than one might expect in newswire.", "labels": [], "entities": []}, {"text": "As such, we are quite interested in the W-NUT Named Entity Recognition in Twitter task ( as a platform to benchmark and drive forward work on NER in social media.", "labels": [], "entities": [{"text": "W-NUT Named Entity Recognition in Twitter task", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.7057405625070844}]}, {"text": "Our submission to this competition closely follows, who advocate the use of a semi-Markov tagger trained online with standard discriminative tagging features, gazetteer matches, Brown clusters, and word embeddings.", "labels": [], "entities": []}, {"text": "We augment this approach with updated gazetteers, phrase embeddings, and infused embeddings that have been adapted to better predict gazetteer membership.", "labels": [], "entities": []}, {"text": "Our novel infusion technique allows us to adapt existing vectors to NER regardless of their source, by training a typelevel auto-encoder whose hidden layer must predict the corresponding phrase's gazetteer memberships while also recovering the original vector.", "labels": [], "entities": []}, {"text": "Our submitted system achieved a typed F1 of 44.7, placing third in the competition, while training only on the provided training data.", "labels": [], "entities": [{"text": "typed", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9348961114883423}, {"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.6362872123718262}]}, {"text": "The competition organizers provided two development sets, one (dev) that is close to the training data, with both train and dev being drawn from the year 2010, and another (dev 2015) that is close to the test data, with both dev 2015 and test being drawn from the winter of.", "labels": [], "entities": []}, {"text": "We present a postcompetition system that achieves an F1 of 54.2 using the same features and hyper-parameters as our submitted system, except that our tagger is also trained on all provided development data.", "labels": [], "entities": [{"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9992647767066956}]}, {"text": "We close with an analysis of dev 2015's relation to the test set, and argue that these results may overestimate the impact that a small, in-domain training set can have on NER performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 172, "end_pos": 175, "type": "TASK", "confidence": 0.9687761068344116}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Experimental results for variants of our system, reporting Precision, Recall and balanced F- measure. The Avg F column lists the average F-measure across dev and dev 2015, which was our model  selection criterion. The Rnk column lists the retro-active rank of each system in the competition.", "labels": [], "entities": [{"text": "Precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9971524477005005}, {"text": "Recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9883098602294922}, {"text": "F- measure", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9567224780718485}, {"text": "Avg F column", "start_pos": 116, "end_pos": 128, "type": "METRIC", "confidence": 0.9350646535555521}, {"text": "F-measure", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9341036677360535}]}]}