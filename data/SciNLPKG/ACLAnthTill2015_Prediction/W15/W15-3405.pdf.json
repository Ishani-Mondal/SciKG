{"title": [{"text": "Attempting to Bypass Alignment from Comparable Corpora via Pivot Language", "labels": [], "entities": [{"text": "Bypass Alignment", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.874592512845993}]}], "abstractContent": [{"text": "Alignment from comparable corpora usually involves two languages, one source and one target language.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9650798439979553}]}, {"text": "Previous works on bilingual lexicon extraction from parallel corpora demonstrated that more than two languages can be useful to improve the alignments.", "labels": [], "entities": [{"text": "bilingual lexicon extraction from parallel corpora", "start_pos": 18, "end_pos": 68, "type": "TASK", "confidence": 0.7841295202573141}]}, {"text": "Our works have investigated to which extent a third language could be interesting to bypass the original alignment.", "labels": [], "entities": []}, {"text": "We have defined two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular.", "labels": [], "entities": []}, {"text": "The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main goal of this work is to investigate to which extent bilingual lexicon extraction using comparable corpora can be improved using a third language when dealing with poor resource language pairs.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6607839167118073}]}, {"text": "Indeed, the quality of the result of the extracted bilingual lexicon strongly depends on the quality of the resources, that is to say the corpora and a general language bilingual dictionary.", "labels": [], "entities": []}, {"text": "In this study, we stress the key role of the potential high quality resources of the pivot language ().", "labels": [], "entities": []}, {"text": "The idea of involving a third language is to benefit from the lexical information conveyed by the additional language.", "labels": [], "entities": []}, {"text": "We also assume that in the case of not so usual language pairs the two comparable corpora are of medium quality, and the bilingual dictionary seems weak, due to the nonexistence of such a dictionary.", "labels": [], "entities": []}, {"text": "We expect as a consequence a bad quality of the extracted lexicon.", "labels": [], "entities": []}, {"text": "Nevertheless, we are highly confident that a language for which we have of a lot of resources can thwart the effect of the poor original resources.", "labels": [], "entities": []}, {"text": "English is probably the first language in term of work and resources in Natural Language Processing, hence it can appear as a good candidate as pivot language.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: we give a short overview of bilingual lexicon extraction standard method in Section 2.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.636801024278005}]}, {"text": "Our proposed approaches are described in Section 3.", "labels": [], "entities": []}, {"text": "The resources we have used are presented in Section 4 and experimental results in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we expose further works and improvements in Sections 6 and 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Pre-processing French, English, Spanish and German documents were pre-processed using TTC TermSuite (Rocheteau and Daille, 2011) . The operations done during pre-processing were the following: tokenization, part-of-speech tagging and lemmatization.", "labels": [], "entities": [{"text": "TTC TermSuite", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.8124505877494812}, {"text": "tokenization", "start_pos": 193, "end_pos": 205, "type": "TASK", "confidence": 0.9839196801185608}, {"text": "part-of-speech tagging", "start_pos": 207, "end_pos": 229, "type": "TASK", "confidence": 0.7494373917579651}]}, {"text": "Moreover, function words and hapaxes had been removed.", "labels": [], "entities": []}, {"text": "Context vectors In order to compute and normalize context vectors, the value a(ct) associated to each co-occurrence ct of a given word win the corpus was computed.", "labels": [], "entities": []}, {"text": "Such value could be computed thanks to Log Likelihood or Mutual Information for instance.", "labels": [], "entities": []}, {"text": "Among them we chose Log Likelihood as its representativity is the most accurate.", "labels": [], "entities": []}, {"text": "Context vectors were computed by TermSuite, as one of its components performed this operation.", "labels": [], "entities": [{"text": "TermSuite", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.7474421262741089}]}, {"text": "Similarity measures The so-called similarity could be computed according to Cosine similarity ( or Weighted Jaccard Distance.", "labels": [], "entities": [{"text": "similarity", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9815288782119751}, {"text": "Cosine similarity", "start_pos": 76, "end_pos": 93, "type": "METRIC", "confidence": 0.9461585581302643}, {"text": "Weighted Jaccard Distance", "start_pos": 99, "end_pos": 124, "type": "METRIC", "confidence": 0.7633775671323141}]}, {"text": "We decided to only present the results achieved using Cosine similarity.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 54, "end_pos": 71, "type": "METRIC", "confidence": 0.8283767700195312}]}, {"text": "The differences between them in term of Mean Reciprocal Rank (MRR) were insignificant.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 40, "end_pos": 66, "type": "METRIC", "confidence": 0.9707543849945068}]}, {"text": "Evaluation metrics In order to evaluate our approaches, we used Mean Reciprocal Rank).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank", "start_pos": 64, "end_pos": 84, "type": "METRIC", "confidence": 0.9289010365804037}]}, {"text": "The strength of this metric is that it takes into account the rank of the candidate translations.", "labels": [], "entities": []}, {"text": "Hereinafter, the MRR defined as follows (t stands for the terms to evaluate and rt for the rank achieved by the system for the good translation of t): Results The MRR achieved for both approaches is shown in for Wind Energy and Mobile Technologies corpora respectively.", "labels": [], "entities": [{"text": "MRR", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.5078993439674377}, {"text": "MRR", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.6284435987472534}]}, {"text": "We present, for the sake of comparison, the results achieved by the standard method (Std.), method transferring context vectors successively (P 1 ) and the method transposing context vectors to pivot language (P 2 ).", "labels": [], "entities": []}, {"text": "We also give additional information, such as the best achievable result according to the reference lists and the words belonging to the filtered corpus (R M AX ) and corpora comparability C (Li and Gaussier, 2010).", "labels": [], "entities": []}, {"text": "The corpus comparability metric consists in the expectation of finding the translation in target language for each source word in the corpus.", "labels": [], "entities": []}, {"text": "Therewith, it is a good way of measuring the distributional symmetry between two corpora and given a dictionary.", "labels": [], "entities": []}, {"text": "We can also notice that the Maximun Recall R M AX is quite low for some pairs of languages: this is due to the high number of hapaxes belonging to the reference lists that were filtered out during pre-processing.", "labels": [], "entities": [{"text": "Maximun Recall R M AX", "start_pos": 28, "end_pos": 49, "type": "METRIC", "confidence": 0.6044896245002747}]}, {"text": "According to the results, we can see that there is a strong correlation between the improvements achieved by pivot based approaches and corpus comparability.", "labels": [], "entities": []}, {"text": "We have improved the quality of the extracted bilingual lexicon only in the case of poorly comparable corpora, respectively \u2264 65.76% and \u2264 66.52% for Wind Energy and Mobile Technologies corpora.", "labels": [], "entities": []}, {"text": "For instance, we have increased the MRR from 0.268 to 0.390 and 0.374 in the case of translation from English to Spanish for the Wind Energy corpus, and from 0.126 to 0.355 and 0.347 for German to Spanish via French for the Mobile Technologies corpus.", "labels": [], "entities": [{"text": "MRR", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8027843832969666}, {"text": "Mobile Technologies corpus", "start_pos": 224, "end_pos": 250, "type": "DATASET", "confidence": 0.6217558880647024}]}], "tableCaptions": [{"text": " Table 2: Number of SWT in reference lists.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9679852724075317}, {"text": "SWT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8915694952011108}]}, {"text": " Table 3: MRR achieved for pivot dictionary based approaches.", "labels": [], "entities": [{"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7958743572235107}]}]}