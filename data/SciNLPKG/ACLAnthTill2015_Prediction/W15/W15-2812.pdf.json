{"title": [{"text": "Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval", "labels": [], "entities": [{"text": "Generating Semantically Precise Scene Graphs from Textual Descriptions", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.7280631810426712}, {"text": "Improved Image Retrieval", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.6069589157899221}]}], "abstractContent": [{"text": "Semantically complex queries which include attributes of objects and relations between objects still pose a major challenge to image retrieval systems.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.6983573138713837}]}, {"text": "Recent work in computer vision has shown that a graph-based semantic representation called a scene graph is an effective representation for very detailed image descriptions and for complex queries for retrieval.", "labels": [], "entities": []}, {"text": "In this paper, we show that scene graphs can be effectively created automatically from a natural language scene description.", "labels": [], "entities": []}, {"text": "We present a rule-based and a classifier-based scene graph parser whose output can be used for image retrieval.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7221371680498123}]}, {"text": "We show that including relations and attributes in the query graph outperforms a model that only considers objects and that using the output of our parsers is almost as effective as using human-constructed scene graphs (Re-call@10 of 27.1% vs. 33.4%).", "labels": [], "entities": []}, {"text": "Additionally , we demonstrate the general usefulness of parsing to scene graphs by showing that the output can also be used to generate 3D scenes.", "labels": [], "entities": [{"text": "parsing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.9687286615371704}]}], "introductionContent": [{"text": "One of the big remaining challenges in image retrieval is to be able to search for very specific images.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8276995718479156}]}, {"text": "The continuously growing number of images that are available on the web gives users access to almost any picture they can imagine, but in order to find these images users have to be able to express what they are looking for in a detailed and efficient way.", "labels": [], "entities": []}, {"text": "For example, if a user wants to find an image of a boy wearing a t-shirt with a plane on it, an image retrieval system has to understand that the image should contain a boy who is wearing a shirt and that on that shirt is a picture of a plane.", "labels": [], "entities": []}, {"text": "Keyword-based image retrieval systems are clearly unable to deal with the rich semantics of such a query (.", "labels": [], "entities": [{"text": "Keyword-based image retrieval", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5962119400501251}]}, {"text": "They might be able to retrieve images that contain a boy, a t-shirt and a plane but they are unable to interpret the relationships and attributes of these objects which is crucial for retrieving the correct images.", "labels": [], "entities": []}, {"text": "As shown in, a possible but incorrect combination of these objects is that a boy is wearing a t-shirt and playing with a toy plane.", "labels": [], "entities": []}, {"text": "One proposed solution to these issues is the mapping of image descriptions to multi-modal embeddings of sentences and images and using these embeddings to retrieve images (.", "labels": [], "entities": []}, {"text": "However, one problem of these models is that they are trained on single-sentence captions which are typically unable to capture the rich content of visual scenes in their entirety.", "labels": [], "entities": []}, {"text": "Further, the coverage of the description highly depends on the subjectivity of human perception ().", "labels": [], "entities": [{"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.951653778553009}]}, {"text": "Certain details such as whether there is a plane on the boy's shirt or not might seem irrelevant to the per-son who writes the caption, but for another user this difference might determine whether a result is useful or not.", "labels": [], "entities": []}, {"text": "try to solve these problems by annotating images with a graph-based semantic representation called a scene graph which explicitly captures the objects in an image, their attributes and the relations between objects.", "labels": [], "entities": []}, {"text": "They plausibly argue that paragraph-long image descriptions written in natural language are currently too complex to be mapped automatically to images and instead they show that very detailed image descriptions in the form of scene graphs can be obtained via crowdsourcing.", "labels": [], "entities": []}, {"text": "They also show that they can perform semantic image retrieval on unannotated images using partial scene graphs.", "labels": [], "entities": [{"text": "semantic image retrieval", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6748079756895701}]}, {"text": "However, one big shortcoming of their model is that it requires the user to enter a query in the form of a scene graph instead of an image description in natural language which is unlikely to find widespread adoption among potential users.", "labels": [], "entities": []}, {"text": "To address this problem, we propose anew task of parsing image descriptions to scene graphs which can then be used as a query for image retrieval.", "labels": [], "entities": [{"text": "parsing image descriptions", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.8613011837005615}, {"text": "image retrieval", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.7150345891714096}]}, {"text": "While our main goal is to show the effectiveness of parsing image descriptions for image retrieval, we believe that scene graphs can be a useful intermediate representation for many applications that involve text and images.", "labels": [], "entities": [{"text": "parsing image descriptions", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.8706191380818685}, {"text": "image retrieval", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7289810478687286}]}, {"text": "One great advantage of such an intermediate representation is the resulting modularity which allows independent development, improvement and reuse of NLP, vision and graphics subsystems.", "labels": [], "entities": []}, {"text": "For example, we can reuse a scene graph parser for systems that generate 2D-scenes () or 3D-scenes () which require input in the form of similar graph-based representations to which a scene graph can be easily converted.", "labels": [], "entities": [{"text": "scene graph parser", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6999761660893759}]}, {"text": "In this paper, we introduce the task of parsing image descriptions to scene graphs.", "labels": [], "entities": [{"text": "parsing image descriptions", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.8969852725664774}]}, {"text": "We build and evaluate a rule-based and a classifier-based scene graph parser which map from dependency syntax representations to scene graphs.", "labels": [], "entities": []}, {"text": "We use these parsers in a pipeline which first parses an image description to a scene graph and then uses this scene graph as input to a retrieval system.", "labels": [], "entities": []}, {"text": "We show that such a pipeline outperforms a system which only considers objects in the description and we show that the output of both of our parsers is almost as effective as human-constructed scene graphs in retrieving images.", "labels": [], "entities": []}, {"text": "Lastly, we demonstrate the more general applicability of our parsers by generating 3D scenes from their output.", "labels": [], "entities": []}, {"text": "We make our parsers and models available at http://nlp.stanford.edu/software/scenegraphparser.shtml.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we split the data into training, development and held-out test sets of size 3,614, 454, and 456 images, respectively.", "labels": [], "entities": []}, {"text": "shows the aggregated statistics of our training and test sets.", "labels": [], "entities": []}, {"text": "We compare our two parsers against the following two baselines.", "labels": [], "entities": []}, {"text": "Nearest neighbor Our first baseline computes a term-frequency vector for an input sentence and returns the scene graph of the nearest neighbor in the training data.", "labels": [], "entities": []}, {"text": "Object only Our second baseline is a parser that only outputs objects but no attributes or relationships.", "labels": [], "entities": []}, {"text": "It uses the first two components of the classifier-based parser, namely the semantic graph processor and the object extractor, and then simply outputs all candidate objects.", "labels": [], "entities": []}, {"text": "We use the downstream performance on the image retrieval task as our main evaluation metric.", "labels": [], "entities": [{"text": "image retrieval task", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8127149343490601}]}, {"text": "We train our reimplementation of the model by on our training set with human-constructed scene graphs.", "labels": [], "entities": []}, {"text": "For each sentence we use the parser's output as a query and rank all images in the test set.", "labels": [], "entities": []}, {"text": "For evaluation, we consider the human-constructed scene graph G h of the sentence and construct a set of images I = i 1 , ..., in such that G h is a subgraph of the image's complete scene graph.", "labels": [], "entities": []}, {"text": "We compute the rank of each image in I and compute recall at 5 and 10 based on these ranks 3 . We also compute the median rank of the first correct result.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9990185499191284}]}, {"text": "We compare these numbers against an oracle system which uses the human-constructed scene graphs as queries instead of the scene graphs generated by the parser.", "labels": [], "entities": []}, {"text": "One drawback of evaluating on a downstream task is that evaluation is typically slower compared to using an intrinsic metric.", "labels": [], "entities": []}, {"text": "We therefore also compare the parsed scene graphs to the humanconstructed scene graphs.", "labels": [], "entities": []}, {"text": "As scene graphs consist of object instances, attributes, and relations and are therefore similar to Abstract Meaning Representation (AMR) () graphs, we use Smatch F1  as an additional intrinsic metric.: Intrinsic (Smatch F1) and extrinsic (recall at 5 and 10, and median rank) performance of our two baselines, our rule-based and our classifier-based parser. and our implementation.", "labels": [], "entities": [{"text": "recall", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.9801764488220215}]}, {"text": "Both systems were trained and tested on the data sets of the original authors.", "labels": [], "entities": []}, {"text": "shows the performance of our baselines and our two final parsers on the development and held-out test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Aggregate statistics of the raw, canoni- calized (processed) and filtered datasets.", "labels": [], "entities": []}, {"text": " Table 2: Aggregate statistics of the training, de- velopment (dev) and test sets.", "labels": [], "entities": [{"text": "de- velopment (dev)", "start_pos": 48, "end_pos": 67, "type": "METRIC", "confidence": 0.8219226598739624}]}, {"text": " Table 3: Intrinsic (Smatch F1) and extrinsic (recall at 5 and 10, and median rank) performance of our  two baselines, our rule-based and our classifier-based parser.", "labels": [], "entities": [{"text": "Intrinsic (Smatch F1)", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8751560688018799}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9876367449760437}]}, {"text": " Table 4: Comparison of the results of the original  implementation by Johnson et al.", "labels": [], "entities": []}]}