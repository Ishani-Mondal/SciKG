{"title": [{"text": "A Maximum Entropy Classifier for Cross-Lingual Pronoun Prediction", "labels": [], "entities": [{"text": "Cross-Lingual Pronoun Prediction", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6390129923820496}]}], "abstractContent": [{"text": "We present a maximum entropy classifier for cross-lingual pronoun prediction.", "labels": [], "entities": [{"text": "cross-lingual pronoun prediction", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.7292308608690897}]}, {"text": "The features are based on local source-and target-side contexts and antecedent information obtained by a co-reference resolution system.", "labels": [], "entities": []}, {"text": "With only a small set of feature types our best performing system achieves an accuracy of 72.31%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9996417760848999}]}, {"text": "According to the shared task's official macro-averaged F1-score at 57.07%, we are among the top systems, at position three out of 14.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9680734276771545}]}, {"text": "Feature ablation results show the important role of target-side information in general and of the resolved target-side antecedent in particular for predicting the correct classes.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we focus on pronouns which pose a problem for machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.8292509317398071}]}, {"text": "Pronoun translation is challenging due to the fact that pronouns often refer to entities mentioned in a nonlocal context such as previous clauses or sentences.", "labels": [], "entities": [{"text": "Pronoun translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9323294758796692}]}, {"text": "Furthermore, languages differ with respect to usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage.", "labels": [], "entities": []}, {"text": "Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated.", "labels": [], "entities": []}, {"text": "This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora) and MT systems focusing on translation of pronouns have been proposed).", "labels": [], "entities": [{"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9655256867408752}]}, {"text": "The DiscoMT 2015 shared task on pronoun translation) calls for contributions to tackle this problem.We focus on the cross-lingual pronoun prediction subtask, which is setup as follows: the two English (source language) third-person subject pronouns it and they can be translated in a variety of ways into French.", "labels": [], "entities": [{"text": "DiscoMT 2015 shared task", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9260888993740082}, {"text": "pronoun translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7988876104354858}, {"text": "cross-lingual pronoun prediction", "start_pos": 116, "end_pos": 148, "type": "TASK", "confidence": 0.6744897067546844}]}, {"text": "A common set of nine classes (ce, cela, elle, elles, il, ils, on, c \u00b8a) is defined as possible translations including an extra class OTHER which groups together any less frequent translations, including null, noun translations, alignment errors.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9410868287086487}]}, {"text": "The source and target corpora both consist of humancreated documents and therefore abstract away from additional difficulties that arise with noisy automatic translations.", "labels": [], "entities": []}, {"text": "propose a neuralnetwork-based approach fora similar crosslingual pronoun prediction task.", "labels": [], "entities": [{"text": "crosslingual pronoun prediction task", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.7219676226377487}]}, {"text": "Their model jointly models anaphora resolution and pronoun prediction.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7311718463897705}, {"text": "pronoun prediction", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7155502736568451}]}, {"text": "Our approach builds on a maximum entropy (MaxEnt) classifier that incorporates various features based on the source pronoun and local source-and target-side contexts.", "labels": [], "entities": []}, {"text": "Moreover, the target-side noun referent (i.e. the antecedent) of a pronoun is used and obtained with an automatic co-reference resolution system.", "labels": [], "entities": []}, {"text": "Our system achieves high accuracy and performs third-best according to the official evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995928406715393}]}, {"text": "In Section 2 we present our MaxEnt classifier including a description of the features used.", "labels": [], "entities": []}, {"text": "This is followed by Section 3 with experiments and evaluation.", "labels": [], "entities": []}, {"text": "Furthermore, in Section 4 we discuss the results and in Section 5 we give concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The official evaluation metric for the shared task is the macro-averaged F-score overall prediction classes (Mac-F1).", "labels": [], "entities": [{"text": "F-score overall prediction classes", "start_pos": 73, "end_pos": 107, "type": "METRIC", "confidence": 0.922616258263588}]}, {"text": "Since this metric favours systems that perform equally well on all classes, the task puts emphasis on handling low-frequency classes well instead of only getting the frequent classes right.", "labels": [], "entities": []}, {"text": "In addition to scores with the official metric we also report overall accuracy (Acc), i.e. the ratio between the correctly predicted classes and all test instances.", "labels": [], "entities": [{"text": "accuracy (Acc)", "start_pos": 70, "end_pos": 84, "type": "METRIC", "confidence": 0.8143052160739899}]}, {"text": "The evaluation script of the shared task provides results for the official fine-grained class separation with nine classes.", "labels": [], "entities": []}, {"text": "It also provides a coarsegrained separation where some of the class labels are merged.", "labels": [], "entities": [{"text": "coarsegrained", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9450594782829285}]}, {"text": "Results reflect the fine-grained distinction except where stated.", "labels": [], "entities": [{"text": "fine-grained distinction", "start_pos": 20, "end_pos": 44, "type": "METRIC", "confidence": 0.9374905824661255}]}, {"text": "shows the official results on the test set together with the respective ranks out of 14 submitted systems.", "labels": [], "entities": []}, {"text": "provide the per-class precision, recall and F1, overall accuracy, and overall macro-averaged F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9827593564987183}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9998077750205994}, {"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9949736595153809}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9919459223747253}, {"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9289252161979675}]}, {"text": "shows results of our feature ablation experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of ALLINONE classifier on  the test set.", "labels": [], "entities": [{"text": "ALLINONE classifier", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6865417659282684}]}, {"text": " Table 3: Performance of POSTCOMBINED classi- fier on the test set.", "labels": [], "entities": []}, {"text": " Table 4: Feature ablation for both types of classifiers on the test set.", "labels": [], "entities": []}, {"text": " Table 5: Confusion matrix for the ALLINONE classifier on the test set. Row labels are gold labels and  column labels are labels as they were classified.", "labels": [], "entities": [{"text": "ALLINONE classifier", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7088125944137573}]}, {"text": " Table 6: Confusion matrix for the POSTCOMBINED classifier on the test set. Row labels are gold labels  and column labels are labels as they were classified.", "labels": [], "entities": [{"text": "POSTCOMBINED classifier", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.5077809989452362}]}]}