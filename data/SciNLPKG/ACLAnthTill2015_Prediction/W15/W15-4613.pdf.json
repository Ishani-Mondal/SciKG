{"title": [{"text": "Which Synthetic Voice Should I Choose for an Evocative Task?", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore different evaluation methods for 4 different synthetic voices and 1 human voice.", "labels": [], "entities": []}, {"text": "We investigate whether in-telligibility, naturalness, or likability of a voice is correlated to the voice's evocative function potential, a measure of the voice's ability to evoke an intended reaction from the listener.", "labels": [], "entities": []}, {"text": "We also investigate the extent to which naturalness and lika-bility ratings vary depending on whether or not exposure to a voice is extended and continuous vs. short-term and sporadic (interleaved with other voices).", "labels": [], "entities": []}, {"text": "Finally , we show that an automatic test can replace the standard intelligibility tests for text-to-speech (TTS) systems, which eliminates the need to hire humans to perform transcription tasks saving both time and money.", "labels": [], "entities": []}], "introductionContent": [{"text": "Currently there area wealth of choices for which output voice to use fora spoken dialogue system.", "labels": [], "entities": []}, {"text": "If the set of prompts is fixed and small, one can use a human voice actor.", "labels": [], "entities": []}, {"text": "If a wider variety and/or dynamic utterances are needed, then text-to-speech synthesis (TTS) is a better solution.", "labels": [], "entities": [{"text": "text-to-speech synthesis", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.7004227340221405}]}, {"text": "There are high quality commercial solutions as well as toolkits for building voices.", "labels": [], "entities": []}, {"text": "While many of these are getting better, none are completely natural, especially when it comes to emotional and conversational speech.", "labels": [], "entities": []}, {"text": "It can be difficult to decide which voice to choose fora specific system, given multiple criteria, and also since TTS evaluation is a labor-intensive process, without good automated understudies.", "labels": [], "entities": [{"text": "TTS evaluation", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.9323534965515137}]}, {"text": "In this paper, we perform a comparative evaluation of several natural and synthetic voices using several different criteria, including subjective ratings and objective task measures.", "labels": [], "entities": []}, {"text": "In particular, we compare the relationship of a voice's evocative function potential, a measure of the voice's ability to evoke an intended reaction from the listener, to the voice's intelligibility and to the listener's perception of the voice's naturalness and likability.", "labels": [], "entities": []}, {"text": "Our first hypothesis is that voice quality is a multi-dimensional construct, and that the best voice for some purposes may not be the best for all purposes.", "labels": [], "entities": []}, {"text": "There maybe different aspects that govern subjective perceptions of a voice and objective task performance, and different aspects may facilitate different tasks.", "labels": [], "entities": []}, {"text": "For example, a neutral highly intelligible voice maybe perfect fora system that provides information but very unpleasant fora story-telling system that is trying to express strong emotion.", "labels": [], "entities": []}, {"text": "Our second hypothesis is that naturalness and likability perceptions of a voice may depend on whether or not the user's exposure to a voice is extended and continuous vs. short-term and sporadic (interleaved with other voices).", "labels": [], "entities": []}, {"text": "The current practice in speech synthesis evaluation is to ask human raters to rate isolated audio clips, usually in terms of naturalness and intelligibility, without extended exposure to a voice.", "labels": [], "entities": [{"text": "speech synthesis evaluation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8222023447354635}]}, {"text": "This approach can certainly inform us about the general quality of a synthetic voice; but it cannot necessarily provide any insight about the appropriateness of this voice fora task that requires that the listener be exposed to that voice fora considerable amount of time.", "labels": [], "entities": []}, {"text": "Taking this idea one step further, we explore whether or not standard TTS evaluation tests such as transcription tasks (designed to assess the intelligibility of a voice) can be fully automated by using automatic speech recognition (ASR) output rather than manual transcriptions.", "labels": [], "entities": [{"text": "TTS evaluation", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.891622006893158}, {"text": "automatic speech recognition (ASR)", "start_pos": 203, "end_pos": 237, "type": "TASK", "confidence": 0.7679735918839773}]}, {"text": "To test our hypotheses we perform 5 experiments using 4 synthetic voices (covering a range of speech synthesis techniques) and 1 human voice.", "labels": [], "entities": []}, {"text": "Each experiment is defined by a unique set of stimuli, subjects, and measures.", "labels": [], "entities": []}, {"text": "In the first two experiments, we perform standard speech synthesis evaluation, i.e., human raters rate isolated audio clips with regard to naturalness in one experiment and likability in the other experiment (each rater has short-term sporadic exposure to the voices).", "labels": [], "entities": [{"text": "speech synthesis evaluation", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.77523606022199}]}, {"text": "Experiments 3 and 4 are intelligibility experiments; in one, participants transcribe the utterances that they hear; in the other, we send audio files through an ASR engine.", "labels": [], "entities": []}, {"text": "The fifth experiment is conducted in the context of a guessing game with extended continuous naturalness and likability ratings collected from participants.", "labels": [], "entities": []}, {"text": "The evocative intention of an utterance is the behavior of the addressee that a speaker intends to evoke.", "labels": [], "entities": []}, {"text": "In the case of the guessing game, a clue is given to evoke the expression of a target word.", "labels": [], "entities": []}, {"text": "We ascertain a voice's evocative function potential (EVP) by calculating the ratio of targets that a clue evokes from listeners.", "labels": [], "entities": [{"text": "evocative function potential (EVP)", "start_pos": 23, "end_pos": 57, "type": "METRIC", "confidence": 0.7074134995539983}]}, {"text": "Each participant listens to many consecutive clues uttered with the same voice (extended continuous exposure).", "labels": [], "entities": []}, {"text": "Our participants are recruited using the Amazon Mechanical Turk (AMT) service 2 in the same fashion as in (.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT) service 2", "start_pos": 41, "end_pos": 79, "type": "DATASET", "confidence": 0.8576803430914879}]}, {"text": "To the best of our knowledge, our work is the first to systematically attempt to validate or disprove the hypotheses mentioned above, and compare the results of human transcriptions to ASR results in order to determine whether or not the latter can be used as an automatic intelligibility test for TTS system evaluations.", "labels": [], "entities": [{"text": "ASR", "start_pos": 185, "end_pos": 188, "type": "TASK", "confidence": 0.8283584713935852}, {"text": "TTS system evaluations", "start_pos": 298, "end_pos": 320, "type": "TASK", "confidence": 0.8837250073750814}]}, {"text": "This is also a first important step towards speech synthesis evaluation in a full dialogue context.", "labels": [], "entities": [{"text": "speech synthesis evaluation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.854498008886973}]}, {"text": "Finally, this is the first time that a systematic evaluation is conducted on a voice's EVP.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we discuss previous work in Section 2 on TTS system evaluations.", "labels": [], "entities": [{"text": "TTS system evaluations", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.8743868668874105}]}, {"text": "In Section 3 we present the voices that we use as well as meta-data about the clues that the voices spoke.", "labels": [], "entities": []}, {"text": "In Section 4 we delineate the experiment methodology, and in Section 5 we report the results of our experiments and some inferences we can draw from them.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: S/S & E/C Naturalness Means", "labels": [], "entities": []}, {"text": " Table 5: S/S & E/C Likability Means", "labels": [], "entities": []}, {"text": " Table 6: Objective Measure Means", "labels": [], "entities": [{"text": "Objective Measure Means", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.5969469447930654}]}, {"text": " Table 7: First vs. Last Naturalness Scores", "labels": [], "entities": []}, {"text": " Table 8: Guessability Correlations  Categories  r s", "labels": [], "entities": [{"text": "Guessability Correlations", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8720320463180542}]}]}