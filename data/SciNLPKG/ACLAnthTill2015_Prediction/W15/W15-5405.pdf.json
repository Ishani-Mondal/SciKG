{"title": [{"text": "Use of Transformation-Based Learning in Annotation Pipeline of Igbo, an African Language", "labels": [], "entities": []}], "abstractContent": [{"text": "The accuracy of an annotated corpus can be increased through evaluation and revision of the annotation scheme, and through adjudication of the disagreements found.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9989933371543884}]}, {"text": "In this paper, we describe a novel process that has been applied to improve a part-of-speech (POS) tagged corpus for the African language Igbo.", "labels": [], "entities": []}, {"text": "An inter-annotation agreement (IAA) exercise was undertaken to iteratively revise the tagset used in the creation of the initial tagged corpus, with the aim of refining the tagset and maximizing annotator performance.", "labels": [], "entities": []}, {"text": "The tagset revisions and other corrections were efficiently propagated to the overall corpus in a semi-automated manner using transformation-based learning (TBL) to identify candidates for correction and to propose possible tag corrections.", "labels": [], "entities": []}, {"text": "The affected word-tag pairs in the corpus were inspected to ensure a high quality end-product with an accuracy that would not be achieved through a purely automated process.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9988652467727661}]}, {"text": "The results show that the tagging accuracy increases from 88% to 94%.", "labels": [], "entities": [{"text": "tagging", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9529310464859009}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9888211488723755}]}, {"text": "The tagged corpus is potentially re-usable for other dialects of the language.", "labels": [], "entities": []}], "introductionContent": [{"text": "When texts and human judgements are stored in computer-readable form, the result is called annotation.", "labels": [], "entities": []}, {"text": "Annotation is developed mostly through hand-coded means, so it is important to measure the reliability of the tagset that produced it.", "labels": [], "entities": [{"text": "reliability", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9873292446136475}]}, {"text": "The fundamental assumption of this exercise, as discussed in, is that the output of manual annotation is considered reliable if it can be computed that annotators are consistent, and the consistency is measured using metrics from the study of,, and.", "labels": [], "entities": [{"text": "consistency", "start_pos": 187, "end_pos": 198, "type": "METRIC", "confidence": 0.9830438494682312}]}, {"text": "If different annotators produce consistently similar results then we can infer that they have internalized a similar understanding of the tagging scheme, and can expect them to perform consistently under this understanding.", "labels": [], "entities": [{"text": "tagging scheme", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.8873350024223328}]}, {"text": "The outcome of this exercise is high consistency tagged sub-corpora containing POS-tags described in the tagset.", "labels": [], "entities": []}, {"text": "This paper describes how we leveraged the byproducts of the inter-annotation agreement (IAA) exercise to improve the quality of the initial tagged Igbo corpus (ITC0), instead of ignoring them and tagging new text, which saves effort, time and money.", "labels": [], "entities": [{"text": "Igbo corpus (ITC0)", "start_pos": 147, "end_pos": 165, "type": "DATASET", "confidence": 0.8368995249271393}]}, {"text": "A quality tagged corpus can help to maximize the performance of automatic POS-taggers used for tagging similar texts.", "labels": [], "entities": []}, {"text": "We employ both manual and automatic processes in a semi-automatic method for this work.", "labels": [], "entities": []}, {"text": "Our semi-automatic annotation method uses Transformation-based Learning (TBL) and a human expert, who is involved in several stages of the process.", "labels": [], "entities": []}, {"text": "First, an initial Igbo tagged corpus (ITC0) was developed in a distributed manner using the tagset reported in.", "labels": [], "entities": [{"text": "Igbo tagged corpus (ITC0)", "start_pos": 18, "end_pos": 43, "type": "DATASET", "confidence": 0.7589962730805079}]}, {"text": "Through an inter-annotation agreement (IAA) exercise, this tagset (TS0) was evaluated and revised to ensure a more reliable and reproducible result.", "labels": [], "entities": []}, {"text": "Then we use TBL to find and propagate changes from the IAA to this initial tagged corpus in an automated manner; an expert human annotator verifies locations TBL has marked for changes instead going through the entire text.", "labels": [], "entities": [{"text": "IAA", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.8601685762405396}]}, {"text": "Through this semiautomated process, the quality of the tagged corpus is increased with minimum expense.", "labels": [], "entities": []}, {"text": "TBL is suitable for this because its inductive method performs very well using annotated corpora whose sizes are smaller than that of n-gram models, and it is an error-driven learner.", "labels": [], "entities": [{"text": "TBL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8065449595451355}]}, {"text": "TBL is a machine learning (ML) algorithm originally developed by.", "labels": [], "entities": [{"text": "machine learning (ML) algorithm", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.7326338291168213}]}, {"text": "It starts with an initial state and requires a correctly tagged text, called truth, for training.", "labels": [], "entities": []}, {"text": "The training process iteratively acquires an ordered list of rules that correct the errors found in the initial state until this initial state resembles the truth to some acceptable degree.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present evaluation results for all the outputs of the above process: ITC0, ITC1, ITC2 and ITC3 to show improvement rates.", "labels": [], "entities": [{"text": "ITC0", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.8834553956985474}, {"text": "ITC3", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.8921517729759216}]}, {"text": "For the evaluation performance, we split the corpora into 10 folds.", "labels": [], "entities": []}, {"text": "10-fold subsets were created by slicing the the corpora into 822 sentences, each is 25,981 words on the average.", "labels": [], "entities": []}, {"text": "Slicing on the sentences is making sure that each piece contained full sentences (rather than cutting off the text in the middle of a sentence).", "labels": [], "entities": []}, {"text": "For 10-fold steps and on closed vocabulary, we trained TBL classifier on 9-fold and tested on the held-out.", "labels": [], "entities": []}, {"text": "The results are summarised in: Simple accuracy on 10-fold evaluation", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9740801453590393}]}], "tableCaptions": [{"text": " Table 1: Bible Book Selections by Group", "labels": [], "entities": [{"text": "Bible Book Selections", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.9139549732208252}]}, {"text": " Table 2: Different error forms and corrections", "labels": [], "entities": []}, {"text": " Table 3: IAA texts statistics", "labels": [], "entities": [{"text": "IAA texts", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.42998139560222626}]}, {"text": " Table 4: Some POS tags precision, recall and f - measure of first, second and third phases of anno- tations.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9902147054672241}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9994995594024658}, {"text": "f - measure", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9891847372055054}]}, {"text": " Table 5: Some WORST POS tags precision, recall,  and f -measure and solution proffered.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9950522780418396}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9993869066238403}, {"text": "f -measure", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9810068806012472}, {"text": "solution", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9511022567749023}]}, {"text": " Table 6: IAA scores based on Kappa statistics and  simple accuracy formula for the first, second and  third annotations.", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8685216903686523}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9995471835136414}]}, {"text": " Table 7: Result statistics after inspection", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.6358175277709961}]}, {"text": " Table 9: Simple accuracy on 10-fold evaluation", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9948603510856628}]}, {"text": " Table 10: Frequency of words found in main text  and TBL flagged samples", "labels": [], "entities": [{"text": "Frequency", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.8780528903007507}]}]}