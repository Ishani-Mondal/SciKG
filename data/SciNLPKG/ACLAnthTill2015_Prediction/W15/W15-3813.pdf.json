{"title": [{"text": "Lexical Characteristics Analysis of Chinese Clinical Documents", "labels": [], "entities": [{"text": "Lexical Characteristics Analysis of Chinese Clinical Documents", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.8331056407519749}]}], "abstractContent": [{"text": "Understanding lexical characteristics of clinical documents is the foundation of sublanguage based Medical Language Processing (MLP) approach.", "labels": [], "entities": []}, {"text": "However, there are limited studies focused on the lexical characters of Chinese clinical documents.", "labels": [], "entities": []}, {"text": "In this study, a lexical characteristics analysis on both syntactic and semantic levels was conducted in a clinical corpus which contains 3,500 clinical documents generated during daily practices.", "labels": [], "entities": []}, {"text": "The analysis was based on the automatic tagging results of a lexicon-based part-of-speech (POS) and semantic tagging method.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.7304934561252594}]}, {"text": "The medical lexicon contains 237,291 entries annotated with both semantic and syntactic classes.", "labels": [], "entities": []}, {"text": "The normalized frequency of different terms, syntactic and semantic classes was calculated and visualized.", "labels": [], "entities": []}, {"text": "Major contribution of this paper is providing a wide-coverage Chinese medical semantic lexicon and presenting the lexical characteristics of Chinese clinical documents.", "labels": [], "entities": [{"text": "Chinese medical semantic lexicon", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.6357901617884636}]}, {"text": "Both of these will lay a good foundation for sublanguage based MLP studies in China.", "labels": [], "entities": [{"text": "MLP", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.8030432462692261}]}], "introductionContent": [{"text": "Clinical documents which contain a tremendous amount of patient information to facilitate interprovider communication, also the most important part of clinical data for secondary use such as clinical research and administration.", "labels": [], "entities": [{"text": "interprovider communication", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.7006295770406723}]}, {"text": "Recent advance in MLP technologies (;; Irina P.; Irina P.), such as de-identification (), text classification (, information retrieval, etc., affords an opportunity to study and analyze clinical documents at an unprecedented scale.", "labels": [], "entities": [{"text": "MLP", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9357489943504333}, {"text": "text classification", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7888560593128204}, {"text": "information retrieval", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.770894467830658}]}, {"text": "In recent years, Chinese MLP topics have drawn increasing public attention as there are more and more electronic clinical data that major exist in free text format such as clinical documents and reports were accumulated in many hospitals.", "labels": [], "entities": [{"text": "MLP topics", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.8531579375267029}]}, {"text": "Some Chinese MLP studies have been reported such as information extraction (), NER(Named Entity Recognition) ( . However, systematic studies of lexical characters of Chinese clinical documents, that is the foundation of sublanguage based MLP approach and have been widely studied in other language), are seldom reported.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7834425866603851}, {"text": "NER(Named Entity Recognition)", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.5836764375368754}]}, {"text": "Lack of accessibility of clinical documents corpus and comprehensive lexical resources for the research community is the major obstacle.", "labels": [], "entities": []}, {"text": "Both syntactic and semantic lexical features are important to understand the medical language structure and grammar.", "labels": [], "entities": []}, {"text": "However, studying lexical features in both syntactic and semantic levels in large scale corpus requires a comprehensive medical lexicon to support the automatic lexical tagging process (.", "labels": [], "entities": []}, {"text": "Unfortunately, such lexical resources in Chinese are not available.", "labels": [], "entities": []}, {"text": "In this study, we constructed a 237,291 entries Chinese medical lexicon using computer aided methods at first.", "labels": [], "entities": []}, {"text": "Then a lexical analysis which aims to present syntactic and semantic features of Chinese clinical documents was conducted in a corpus contains 3,500 clinical documents.", "labels": [], "entities": []}, {"text": "The lexical features of clinical documents from different departments were reported.", "labels": [], "entities": []}, {"text": "The annotated corpus was ready for further utilization such as collection of the cooccurrence patterns () and sublanguage grammar.", "labels": [], "entities": []}], "datasetContent": [{"text": "The quality of the lexical characters generated from statistical analysis depends on the coverage and completeness of the lexicon constructed.", "labels": [], "entities": []}, {"text": "Comparing with the typical comprehensive medical lexical resources such as UMLS which contains millions of terms, our lexicon scale is relatively small.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.9557778835296631}]}, {"text": "So we calculate the coverage and completeness of the lexicon during the tokenizing and annotation.", "labels": [], "entities": [{"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9936972260475159}, {"text": "tokenizing", "start_pos": 72, "end_pos": 82, "type": "TASK", "confidence": 0.9803428053855896}]}, {"text": "Total 13,660 lexemes were unrecognized among all 2,375,909 lexemes in the corpus.", "labels": [], "entities": []}, {"text": "The coverage of our lexicon in the corpus was 99.43% calculated by Formula 3.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9532098770141602}]}, {"text": "Similarly, the distinct lexemes among the unrecognized lexemes and lexemes in the corpus were 577 and 19,847 respectively.", "labels": [], "entities": []}, {"text": "Thus, the completeness of the lexicon was 91.11% calculated by Formula 4.", "labels": [], "entities": [{"text": "Formula 4", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.9378125667572021}]}, {"text": "Word segmentation and annotation regarding POS and semantics were conducted on the test set with the ICTCLAS.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6592606157064438}, {"text": "ICTCLAS", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9660550951957703}]}, {"text": "As a result, 4,006 lexemes were obtained excluding punctuations and Arabics by the automatic tagging process.", "labels": [], "entities": []}, {"text": "Manually checking by one of the authors, the number of error segments caused by ICTCLAS was counted.", "labels": [], "entities": [{"text": "ICTCLAS", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.5820959806442261}]}, {"text": "Meanwhile, the number of lexemes with error POS tag or semantic tag was picked out.", "labels": [], "entities": []}, {"text": "The accuracy of word segmentation, POS and semantics was calculated separately by Formula 5 and demonstrated in: The evaluation result of the lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994198083877563}, {"text": "word segmentation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7237949967384338}, {"text": "POS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8552327156066895}]}], "tableCaptions": [{"text": " Table 2: The evaluation result of the lexicon.", "labels": [], "entities": []}]}