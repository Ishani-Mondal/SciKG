{"title": [{"text": "ParsPer: A Dependency Parser for Persian", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a dependency parser for Per-sian, called ParsPer, developed using the graph-based parser in the Mate Tools.", "labels": [], "entities": []}, {"text": "The parser is trained on the entire Uppsala Persian Dependency Treebank with a specific configuration that was selected by MaltParser as the best performing parsing representation.", "labels": [], "entities": [{"text": "Uppsala Persian Dependency Treebank", "start_pos": 36, "end_pos": 71, "type": "DATASET", "confidence": 0.903123140335083}, {"text": "MaltParser", "start_pos": 123, "end_pos": 133, "type": "DATASET", "confidence": 0.9824173450469971}]}, {"text": "The treebank's syntactic annotation scheme is based on Stanford Typed Dependencies with extensions for Persian.", "labels": [], "entities": []}, {"text": "The results of the ParsPer evaluation revealed a best labeled accuracy over 82% with an unlabeled accuracy close to 87%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.8220430612564087}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9411800503730774}]}, {"text": "The parser is freely available and released as an open source tool for parsing Persian.", "labels": [], "entities": [{"text": "parsing Persian", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.9132192432880402}]}], "introductionContent": [{"text": "Data-driven dependency parsing is a modern approach that has been successfully applied to develop dependency parsers for different languages (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7045855522155762}]}, {"text": "The approach relies solely on a syntactically annotated dataset (treebank).", "labels": [], "entities": []}, {"text": "However, achieving the best results by this method relies partly on parsing algorithms and selecting the best feature settings.", "labels": [], "entities": []}, {"text": "As data-driven dependency parsers induce the syntactic structure backbone in a treebank, they are further, to a great extent, dependent on the representation setup for part-ofspeech and dependency labels.", "labels": [], "entities": []}, {"text": "These representations are always built upon an already tokenized text.", "labels": [], "entities": []}, {"text": "In other words, different tokenizations require different part-of-speech and dependency annotations, which in turn impact the quality of parsing analysis.", "labels": [], "entities": [{"text": "parsing analysis", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.9531596601009369}]}, {"text": "Processing and analysis of a language like Persian pose a variety of challenges on various levels, from orthography to syntactic structure.", "labels": [], "entities": [{"text": "syntactic structure", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7218922674655914}]}, {"text": "Persian orthography does not follow a consistent standardization.", "labels": [], "entities": []}, {"text": "The most challenging cases concern the handling of fixed expressions and various types of clitics.", "labels": [], "entities": []}, {"text": "Different variations of writing such cases as attached or detached forms (either delimited with whitespace or zerowidth non-joiner) pose challenges for tokenization which in turn impacts the quality of morphological and syntactic analysis.", "labels": [], "entities": []}, {"text": "Furthermore, the prevalence of multi-word compound verbs, functioning as a single verb in the form of so called complex predicates or light verb constructions (LVC), is another remarkable feature in the Persian syntactic structure.", "labels": [], "entities": []}, {"text": "The situation for automatic analysis of Persian is further complicated by its high degree of free word order.", "labels": [], "entities": [{"text": "automatic analysis of Persian", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.8148069381713867}]}, {"text": "Therefore, in preparing the treebank data for Persian many difficult decisions had to be made concerning handling fixed expressions and different types of clitics such as pronominal and copula clitics.", "labels": [], "entities": [{"text": "Persian", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9415816068649292}]}, {"text": "Fixed expressions in Persian are sometimes written as one single token and sometimes as several tokens.", "labels": [], "entities": []}, {"text": "The same happens for different types of clitics.", "labels": [], "entities": []}, {"text": "They are sometimes segmented and sometimes unsegmented from the head words.", "labels": [], "entities": []}, {"text": "Since the treebank data is taken from the large and open source Uppsala Persian Corpus (UPC), 2 it was impossible to manually separate fixed expressions and clitics from head words in a consistent way in a large corpus like the UPC, containing 2,703,265 tokens.", "labels": [], "entities": [{"text": "Uppsala Persian Corpus (UPC),", "start_pos": 64, "end_pos": 93, "type": "DATASET", "confidence": 0.9719724995749337}, {"text": "UPC", "start_pos": 228, "end_pos": 231, "type": "DATASET", "confidence": 0.9645551443099976}]}, {"text": "On the other hand, to automatically handle such cases was also impossi-ble since the process could result in many incorrect conversions by impacting orthographically similar words or endings with different part-of-speech categories.", "labels": [], "entities": []}, {"text": "Hence, to avoid introducing errors in the corpus, fixed expressions are handled as distinct tokens, as long as they were not written as attached forms, and clitics are not separated from the head words but analyzed with special labels at the syntactic level instead.", "labels": [], "entities": []}, {"text": "Therefore, in the annotation scheme of the Uppsala Persian Dependency Treebank, apart from 48 dependency labels for basic relations there are 48 complex dependency labels to cover syntactic relations for words containing unsegmented clitics.", "labels": [], "entities": [{"text": "Uppsala Persian Dependency Treebank", "start_pos": 43, "end_pos": 78, "type": "DATASET", "confidence": 0.9779200851917267}]}, {"text": "Fine-grained annotated data in treebanks normally provides a more complete grammatical analysis which in turn enhances the quality of parsing results.", "labels": [], "entities": []}, {"text": "However, complex annotation may not always be beneficial and can impair automatic analysis.", "labels": [], "entities": []}, {"text": "In this paper, we present different empirical studies where we systematically simplify the annotation schemes for part-of-speech tags and dependency relations within the treebank.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 briefly presents the Uppsala Persian Dependency Treebank.", "labels": [], "entities": [{"text": "Uppsala Persian Dependency Treebank", "start_pos": 67, "end_pos": 102, "type": "DATASET", "confidence": 0.9011487364768982}]}, {"text": "Section 3 introduces the experimental design.", "labels": [], "entities": []}, {"text": "In Section 4, ParsPer is presented and evaluated.", "labels": [], "entities": [{"text": "ParsPer", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.6773022413253784}]}, {"text": "Finally, Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carryout two types of experiments, experiments with different parsing representations (we define these as basic experiments henceforth) and experiments with different dependency parsers.", "labels": [], "entities": []}, {"text": "For the experiments, the treebank is sequentially split into 10 parts, of which segments 1-8 are used for training (80%), 9 for development (10%), and 10 for test (10%).", "labels": [], "entities": []}, {"text": "In the basic experiments, we train MaltParser on the training set and test on the development set.", "labels": [], "entities": []}, {"text": "In the latter experiments, we train different parsers on the joint training and development sets (90%) and test on the test set.", "labels": [], "entities": []}, {"text": "We perform the basic experiments under four different conditions.", "labels": [], "entities": []}, {"text": "We first experiment with all features and labels that already exist in the treebank.", "labels": [], "entities": []}, {"text": "The results achieved by this experiment will be used as the baseline results.", "labels": [], "entities": []}, {"text": "We then experiment with different relation sets by removing or merging various feature distinctions in the part-ofspeech tagset and the syntactic annotation scheme.", "labels": [], "entities": []}, {"text": "The experiments are designed as indicators to see if the conversions help or do not help the parser.", "labels": [], "entities": []}, {"text": "In order to get a realistic picture of the parser performance, all these experiments will be performed using automatically generated part-of-speech tags.", "labels": [], "entities": []}, {"text": "All the above experiments will be carried out using MaltParser ().", "labels": [], "entities": []}, {"text": "After discovering the best label set for both part-of-speech tags and dependency relations, we will experiment with other parsers such as MSTParser), MateParsers, and TurboParser () to find a state-of-the-art parser for Persian.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 138, "end_pos": 147, "type": "DATASET", "confidence": 0.8941619396209717}]}, {"text": "We evaluate the parsers by experimenting with various feature set-tings when optional parameter settings for optimization are available and given by the parsers.", "labels": [], "entities": []}, {"text": "However, only results for final settings are presented.", "labels": [], "entities": []}, {"text": "The selected state-of-the-art parser for Persian will be called ParsPer.", "labels": [], "entities": []}, {"text": "For evaluation of ParsPer we first perform a parsing experiment on the treebank data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9677801728248596}]}, {"text": "We then make an independent parsing evaluation by applying the parser on out-ofdomain text and present the final results.", "labels": [], "entities": []}, {"text": "To evaluate the overall performance of the parser, we tune parameters to acquire the highest possible results.", "labels": [], "entities": []}, {"text": "Thus, we experiment with different algorithms and feature settings to optimize MaltParser.", "labels": [], "entities": []}, {"text": "To accomplish the optimization process, we apply MaltOptimizer (.", "labels": [], "entities": []}, {"text": "Parser accuracy is evaluated on automatically generated part-of-speech tags.", "labels": [], "entities": [{"text": "Parser", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5385684370994568}, {"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.8634949326515198}]}, {"text": "In order to generate automatic part-of-speech tags, we used the Persian part-of-speech tagger, TagPer (Seraji, 2015).", "labels": [], "entities": [{"text": "TagPer (Seraji, 2015)", "start_pos": 95, "end_pos": 116, "type": "DATASET", "confidence": 0.8586231271425883}]}, {"text": "However, for the treebank experiments we retrained the tagger to exclude the treebank data to avoid data overlap.", "labels": [], "entities": []}, {"text": "The tagging evaluation performed by the new TagPer revealed an overall accuracy of 97.17%, when trained on 90% of the UPC and evaluated on the remaining 10%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9996077418327332}, {"text": "UPC", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.9417413473129272}]}, {"text": "The four different experiments include (1) an overall parsing evaluation on full treebank annotation (baseline), (2) an experiment without morphological features in the partof-speech tagset, (3) an experiment without finegrained LVC labels, and (4) an experiment without complex labels.", "labels": [], "entities": []}, {"text": "This part is designed for estimating the performance of different parsers on the best performing data representations selected by MaltParser in the baseline experiments.", "labels": [], "entities": []}, {"text": "Hence, we setup the data with the best achieved parameters which are using the automatically generated coarse-grained partof-speech tags with a single LVC label and the fine-grained dependency relations consisting of 96 basic and complex labels.", "labels": [], "entities": []}, {"text": "The treebank is further organized with a different split than in the basic experiments.", "labels": [], "entities": []}, {"text": "In other words, we train the parser on the joint training and development sets (90%) and test on the test set (10%).", "labels": [], "entities": []}, {"text": "We will experiment with MaltParser For evaluating MaltParser, we used Nivre's algorithms as the algorithms were the best parsing algorithms offered by MaltOptimizer during the previous experiments.", "labels": [], "entities": []}, {"text": "The parser resulted in scores of 79.40% and 83.47% for labeled and unlabeled attachment, respectively.", "labels": [], "entities": []}, {"text": "For evaluating MSTParser, we used the secondorder model with projective parsing as this setting had presented the highest results in the earlier parameter tuning experiments.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 15, "end_pos": 24, "type": "DATASET", "confidence": 0.6705120801925659}, {"text": "projective parsing", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7231281697750092}]}, {"text": "The parser presented the results of 77.79% for labeled and 83.45% for unlabeled attachment scores.", "labels": [], "entities": []}, {"text": "For experimenting with MateParsers, we trained the graph-based and transition-based parsers on the UPDT with the best parameters selected.", "labels": [], "entities": [{"text": "UPDT", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.9379567503929138}]}, {"text": "The results of Mate experiments showed that the graph-based parser outperformed the transitionbased parser, resulting in 82.58% for labeled and 86.69% for unlabeled attachment scores.", "labels": [], "entities": []}, {"text": "For experimenting with TurboParser, we trained the second-order non-projective parser with features for arcs, consecutive siblings and grandparents, using the AD 3 algorithm as a decoder.", "labels": [], "entities": []}, {"text": "We adapted the full setting as the setting had performed best with our earlier parameter-tuning experiments.", "labels": [], "entities": []}, {"text": "The full setting enables arc-factored, consecutive sibling, grandparent, arbitrary sibling, head bigram, grand-sibling (third-order), and trisibling (third-order) parts.", "labels": [], "entities": []}, {"text": "The parser showed the results of 80.57% for labeled and 85.32% for un- the graph-based parser in the Mate tools achieves the highest results for Persian.", "labels": [], "entities": []}, {"text": "The developed parser will be treated as the state-of-the-art parser for the language and will be called ParsPer.", "labels": [], "entities": []}, {"text": "The parser will undergo further evaluations which will be presented more in detail in the next section.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the ParsPer we made an independent parsing evaluation by running the parser on out-of-domain text.", "labels": [], "entities": []}, {"text": "For this, we used texts from the web-based journal www.hamshahri.com.", "labels": [], "entities": []}, {"text": "We downloaded multiple texts based on different genres and then randomly picked 100 sentences containing 2778 tokens with an average sentence length of 28 tokens to develop a test set.", "labels": [], "entities": []}, {"text": "As our experiment involved some manual work we opted fora small-sized sample to make the evaluation task more feasible.", "labels": [], "entities": []}, {"text": "We first created a gold file by manually normalizing the internal word boundaries and character sets and 8 http://stp.lingfil.uu.se/\u223cmojgan/parsper-mate.html then segmenting the text into sentence and token levels.", "labels": [], "entities": []}, {"text": "We then manually annotated the file with part-of-speech and dependency information using the same part-of-speech and dependency scheme that ParsPer was built onto be served as gold.", "labels": [], "entities": []}, {"text": "In this task we performed three different parsing evaluations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9625108242034912}]}, {"text": "First we applied the parser on the automatically normalized, tokenized and tagged text.", "labels": [], "entities": []}, {"text": "This is the main experiment in the ParsPer evaluation that also indicates the performance of automatic processing of Persian texts at various levels.", "labels": [], "entities": []}, {"text": "Next, we performed two more experiments with the 100 randomly selected sentences in order to analyze the results in a more nuanced way, by experimenting on the sentences when they are manually normalized and tokenized but automatically tagged and then, when they are manually normalized, tokenized, and tagged.", "labels": [], "entities": []}, {"text": "To create our test set for our first experiment, we automatically normalized the 100 sentences using the Persian normalizer PrePer, 9 segmented it with SeTPer, 10 and tagged with TagPer.", "labels": [], "entities": [{"text": "PrePer", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.45588168501853943}]}, {"text": "11 A comprehensive description of the tools PrePer, SeTPer, and TagPer are given in Seraji (2015, Chapter 4).", "labels": [], "entities": []}, {"text": "Then we parsed the automatically tokenized and tagged text with ParsPer.", "labels": [], "entities": []}, {"text": "Since the sentences were automatically tokenized, contained 10 tokens fewer than the gold file (the number of tokens in the gold file were 2788).", "labels": [], "entities": []}, {"text": "12 Therefore we could not directly present labeled and unlabeled attachment scores.", "labels": [], "entities": []}, {"text": "However, instead, we present labeled recall and precision as well as unlabeled recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9863240122795105}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9360431432723999}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9835087656974792}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.936914324760437}]}, {"text": "The parsing evaluation revealed a labeled recall and precision of 73.52% and 73.79%, and an unlabeled recall and precision of 81.99% and 82.28%, respectively.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9633918404579163}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9403197765350342}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9734546542167664}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9857627153396606}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9555613994598389}]}, {"text": "As could be expected, the results for labeled recall and precision are low.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9884958267211914}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9993818998336792}]}, {"text": "This is due to the fact that apart from incorrect tokens in the automatically tokenized file there are incorrect part-of-speech tags made by the tagger TagPer that have had a negative impact on the results.", "labels": [], "entities": []}, {"text": "Subsequently, we automatically parsed the manually normalized, tokenized, but automatically tagged text and compared the parsing results with the manually parsed gold text.", "labels": [], "entities": []}, {"text": "By this ex-   periment, we wanted to isolate the impact of tagging errors.", "labels": [], "entities": []}, {"text": "The evaluation resulted in labeled and unlabeled attachment scores (recall and precision) of 78.50% and 86.27% on the test set with 100 sentences and 2788 tokens.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9979064464569092}, {"text": "precision)", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9597984254360199}]}, {"text": "As the results indicate, the unlabeled attachment score is close to the unlabeled attachment score obtained by the parser when evaluated on in-domain text.", "labels": [], "entities": [{"text": "unlabeled attachment score", "start_pos": 29, "end_pos": 55, "type": "METRIC", "confidence": 0.6306338508923849}]}, {"text": "Furthermore, the unlabeled attachment score is 7.77% higher than the labeled attachment score.", "labels": [], "entities": [{"text": "unlabeled attachment score", "start_pos": 17, "end_pos": 43, "type": "METRIC", "confidence": 0.673619786898295}]}, {"text": "This may partly be due to fact that the structural variation for the head nodes is lower than the variation for labels.", "labels": [], "entities": []}, {"text": "Moreover, we have a firm structure for the head nodes in the syntactic annotation when invariably choosing content words as head position.", "labels": [], "entities": []}, {"text": "This solid structure in turn makes it easier for the parser to learn that after repeatedly seeing it.", "labels": [], "entities": []}, {"text": "Hence, the parser assigns the head nodes more accurately than the combinations of head and label.", "labels": [], "entities": []}, {"text": "This does not mean that it does not exist a consistent structure for the dependency relations.", "labels": [], "entities": []}, {"text": "What we mean is that the number of occurrence of certain cases for dependency relations may not be as many as the number of repeated cases for head structures.", "labels": [], "entities": []}, {"text": "This might be perceived as a sparseness by the parser which can directly affect the labeled attachment score.", "labels": [], "entities": []}, {"text": "Moreover, the syntactic (non)complexity in the data can have a direct impact on the parser performance.", "labels": [], "entities": []}, {"text": "Finally, we automatically parsed the manually normalized, tokenized, and tagged text and compared the parsing with the manually parsed gold file.", "labels": [], "entities": []}, {"text": "The evaluation resulted in a straightforward labeled and unlabeled attachment scores of 78.76% and 86.12% on the test set with 100 sentences and 2788 tokens.", "labels": [], "entities": []}, {"text": "The same kind of pattern as in the previous experiment was further found here.", "labels": [], "entities": []}, {"text": "In other words, we see nearly similar gap of 7.36% between the labeled and unlabeled attachment scores.", "labels": [], "entities": []}, {"text": "shows results from different evaluations of the ParsPer.", "labels": [], "entities": []}, {"text": "The comparison of shows that tokenization is a greater problem than tagging for syntactic parsing.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.9712688326835632}, {"text": "syntactic parsing", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7062765657901764}]}, {"text": "Whereas a perfectly tokenized text with tagging errors degrades parsing results by less than 1%, errors in tokenization may decrease parsing accuracy as much as 5%.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.964686930179596}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.8968327641487122}]}, {"text": "To some extent, this is probably due to additional tagging errors caused by tokenization errors.", "labels": [], "entities": []}, {"text": "It is nevertheless clear that tokenization errors disrupt the syntactic structure more than tagging errors do.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.9730316400527954}]}, {"text": "Adding variations of writing styles (as mentioned earlier) on top of this triggers variations in the tokenization process, which in turn leads to the parser being unable to realize similar sentences with different tokenizations.", "labels": [], "entities": []}, {"text": "However, this normally happens when the parser is not familiar with the tokens (or the order of how tokens are represented) in the sentence, which is due to the fact that the structure is not prevalent enough in the training data.", "labels": [], "entities": []}, {"text": "It might be possible to improve the parsing performance by adding to or modifying the part-ofspeech tag set as well as eliminating or modifying some structures in the syntactic annotation scheme that are not properly favor the parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9711053371429443}]}, {"text": "Moreover, one can use joint segmentation and tagging similar to that made for Chinese (.", "labels": [], "entities": [{"text": "joint segmentation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6461552828550339}]}, {"text": "However, this matter will remain for our future research.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Labeled recall and precision on the devel- opment set for the 20 most frequent dependency  types in the UPDT, when MaltParser is trained on  the full treebank annotation. DepRel = Depen- dency Relations, Freq. = Frequency, R = Recall,  P = Precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9725367426872253}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9995291233062744}, {"text": "UPDT", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.9005774855613708}, {"text": "Freq. = Frequency", "start_pos": 214, "end_pos": 231, "type": "METRIC", "confidence": 0.8542780727148056}, {"text": "R = Recall", "start_pos": 233, "end_pos": 243, "type": "METRIC", "confidence": 0.7230117718378702}, {"text": "Precision", "start_pos": 250, "end_pos": 259, "type": "METRIC", "confidence": 0.9355977773666382}]}, {"text": " Table 2: Labeled recall and precision on the devel- opment set for the 20 most frequent dependency  types in the UPDT, when MaltParser is trained on  the UPDT with coarse-grained auto part-of-speech  tags. DepRel = Dependency Relations, Freq. =  Frequency, R = Recall, P = Precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9780367016792297}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9995156526565552}, {"text": "UPDT", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.8950670957565308}, {"text": "UPDT", "start_pos": 155, "end_pos": 159, "type": "DATASET", "confidence": 0.9245237708091736}, {"text": "Freq. =  Frequency", "start_pos": 238, "end_pos": 256, "type": "METRIC", "confidence": 0.8555738031864166}, {"text": "Recall", "start_pos": 262, "end_pos": 268, "type": "METRIC", "confidence": 0.5294921398162842}, {"text": "Precision", "start_pos": 274, "end_pos": 283, "type": "METRIC", "confidence": 0.9330697655677795}]}, {"text": " Table 3: Labeled recall and precision on the devel- opment set for the 20 most frequent dependency  types in the UPDT, when MaltParser is trained on  the treebank with fine-grained auto part-of-speech  tags only one light verb construction. DepRel =  Dependency Relations, Freq. = Frequency, R =  Recall, P = Precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9817445874214172}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996646642684937}, {"text": "UPDT", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.8615696430206299}, {"text": "R =  Recall", "start_pos": 293, "end_pos": 304, "type": "METRIC", "confidence": 0.7113979856173197}, {"text": "Precision", "start_pos": 310, "end_pos": 319, "type": "METRIC", "confidence": 0.8412296772003174}]}, {"text": " Table 4: Recall and precision for LVC relations  with fine-grained predicted part-of-speech tags in  Experiments 1 and 3. DepRel = Dependency Re- lations, Freq. = Frequency, R = Recall, P = Preci- sion.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9768532514572144}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9996193647384644}, {"text": "DepRel", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9500958323478699}]}, {"text": " Table 5: Labeled recall and precision on the devel- opment set for the 20 most frequent dependency  types in the UPDT, when MaltParser is trained on  the treebank with fine-grained auto part-of-speech  tags and only basic dependency relations. DepRel  = Dependency Relations, Freq. = Frequency, R =  Recall, P = Precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9801689982414246}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.999595582485199}, {"text": "UPDT", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.8806801438331604}, {"text": "Recall", "start_pos": 301, "end_pos": 307, "type": "METRIC", "confidence": 0.488734632730484}, {"text": "Precision", "start_pos": 313, "end_pos": 322, "type": "METRIC", "confidence": 0.8910545110702515}]}, {"text": " Table 7: Best results given by different parsers  when trained on UPDT with auto part-of-speech  tags, 1LVC, CompRel in the model assessment.  MateGraph. = Mate graph-based, MateTrans. =  Mate transition-based", "labels": [], "entities": [{"text": "UPDT", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8556880950927734}]}]}