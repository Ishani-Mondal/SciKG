{"title": [{"text": "Your Sentiment Precedes You: Using an author's historical tweets to predict sarcasm", "labels": [], "entities": [{"text": "Sentiment Precedes You", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.8579487403233846}]}], "abstractContent": [{"text": "Sarcasm understanding may require information beyond the text itself, as in the case of 'I absolutely love this restaurant!'", "labels": [], "entities": [{"text": "Sarcasm understanding", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9426932036876678}]}, {"text": "which maybe sarcastic, depending on the contextual situation.", "labels": [], "entities": []}, {"text": "We present the first quantitative evidence to show that historical tweets by an author can provide additional context for sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.951194554567337}]}, {"text": "Our sarcasm detection approach uses two components: a contrast-based predictor (that identifies if there is a sentiment contrast within a target tweet), and a historical tweet-based predictor (that identifies if the sentiment expressed towards an entity in the target tweet agrees with sentiment expressed by the author towards that entity in the past).", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8322602808475494}]}], "introductionContent": [{"text": "Sarcasm 1 is defined as 'the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way' 2 . An example of sarcasm is 'Being stranded in traffic is the best way to start my week'.", "labels": [], "entities": []}, {"text": "There exists a sentiment contrast between the phrases 'being stranded' and 'best way' which enables an automatic sarcasm detection approach to identify the sarcasm in this sentence.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7027238458395004}]}, {"text": "Existing approaches rely on viewing sarcasm as a contrast in sentiment ().", "labels": [], "entities": []}, {"text": "However, consider the sentences 'Nicki Minaj, don't I hate her!' or 'I love spending four hours cooking on a weekend!'.", "labels": [], "entities": []}, {"text": "The sarcasm is ambiguous because of a likely hyperbole in the first sentence, and because sentiment associated with 'four hours cooking' depends on how much the author/speaker likes cooking.", "labels": [], "entities": []}, {"text": "Such sarcasm is difficult to judge for humans as well as an automatic sarcasm detection approach.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8318378925323486}]}, {"text": "Essentially, we need more context related to the author of these sentences to identify sarcasm within them.", "labels": [], "entities": []}, {"text": "The question we aim to answer in this paper is: 'What sentiment did the author express in the past about the entities in the tweet that is to be classified?", "labels": [], "entities": []}, {"text": "Can this information help us understand if the author is being sarcastic?'", "labels": [], "entities": []}, {"text": "We present the first quantitative evidence to show that historical text generated by an author maybe useful to detect sarcasm in text written by the author.", "labels": [], "entities": []}, {"text": "In this paper, we exploit the timeline structure of twitter for sarcasm detection of tweets.", "labels": [], "entities": [{"text": "sarcasm detection of tweets", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.8451244831085205}]}, {"text": "To gain additional context, we explore beyond the tweet to be classified (called 'target tweet'), and lookup the twitter timeline of the author of the target tweet (we refer to these tweets as the 'historical tweets').", "labels": [], "entities": []}, {"text": "Our method directly applies to discussion forums and review websites, where other posts or reviews by this author maybe looked at.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 contains the related work.", "labels": [], "entities": []}, {"text": "We present a motivating example in Section 3, and describe the architecture of our approach in Section 4.", "labels": [], "entities": []}, {"text": "The experimental setup and results are in Sections 5 and 6.", "labels": [], "entities": []}, {"text": "We present a discussion of challenges observed with the proposed historical tweet-based approach in Section 7, and conclude the paper in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the contrast-based predictor, we obtain the implicit sentiment phrases as follows: (1) We download a set of 8000 tweets marked with #sarcasm, and assume that they are sarcastic tweets.", "labels": [], "entities": []}, {"text": "These are not the same as the test tweets, (2) We extract 3-grams to 10-grams (1-gram represents a word) in these tweets, (3) We select phrases that occur at least thrice.", "labels": [], "entities": []}, {"text": "This results in a set of 445 phrases.", "labels": [], "entities": []}, {"text": "These phrases are used as implicit sentiment phrases for the contrast-based predictor.", "labels": [], "entities": []}, {"text": "For the historical tweet-based predictor, we first POS tag the sentence using.", "labels": [], "entities": []}, {"text": "We then select NNP sequences 6 in the target tweet as the target phrase.", "labels": [], "entities": []}, {"text": "Then, we download the complete timeline of the author using Twitter API , and select tweets containing the target phrase.", "labels": [], "entities": []}, {"text": "The historical tweet-based predictor then gives its prediction as described in the previous section.", "labels": [], "entities": []}, {"text": "Both the predictors rely on sentiment lexicons: The contrast-based predictor needs sentimentbearing words and phrases to detect contrast, while the historical tweet-based predictor needs sentiment-bearing words to identify sentiment of a tweet.", "labels": [], "entities": []}, {"text": "We experiment with two lexicons: Based on the two lexicons, we run two sets of experiments: 1.", "labels": [], "entities": []}, {"text": "Sarcasm detection with L1 (SD1): In this set, we use L1 as the lexicon for the two predictors.", "labels": [], "entities": [{"text": "Sarcasm detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9290473759174347}]}, {"text": "We show results for all four integrator versions (Only historical tweet-based, AND, OR, Relaxed-AND).", "labels": [], "entities": [{"text": "AND", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9734459519386292}, {"text": "OR", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9201338291168213}, {"text": "Relaxed-AND", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.9889602661132812}]}, {"text": "2. Sarcasm detection with L2 (SD2): In this set, we use L2 as the lexicon for the two predictors.", "labels": [], "entities": [{"text": "Sarcasm detection", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.9621069133281708}]}, {"text": "We show results for all four integrator versions (Only historical tweet-based, AND, OR, Relaxed-AND).", "labels": [], "entities": [{"text": "AND", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9734459519386292}, {"text": "OR", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9201338291168213}, {"text": "Relaxed-AND", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.9889602661132812}]}, {"text": "For all experiments, we use the test corpus given by.", "labels": [], "entities": []}, {"text": "This is a manually annotated corpus consisting of 2278 tweets 8 , out of which 506 are sarcastic.", "labels": [], "entities": []}, {"text": "show Precision (P), Recall (R) and F-score (F) for SD1 and SD2 respectively.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 5, "end_pos": 18, "type": "METRIC", "confidence": 0.960843488574028}, {"text": "Recall (R)", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9605009406805038}, {"text": "F-score (F)", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9676836580038071}]}, {"text": "We compare our values with the best reported values in.", "labels": [], "entities": []}, {"text": "This comparison is required because the test corpus that we used was obtained from them.", "labels": [], "entities": []}, {"text": "We also experimented with NN and JJ NN sequences.", "labels": [], "entities": []}, {"text": "However, the output turned out to be generic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Averaged Precision, Recall and F-score  of the SD1 approach for four configurations of the  integrator", "labels": [], "entities": [{"text": "Precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9628852605819702}, {"text": "Recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9976602792739868}, {"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9992853999137878}]}, {"text": " Table 2: Averaged Precision, Recall and F-score  of the SD2 approach for four configurations of the  integrator", "labels": [], "entities": [{"text": "Precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9621385335922241}, {"text": "Recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9977267384529114}, {"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9993301630020142}]}, {"text": " Table 3: Positive Precision (PP) and Recall (PR)  for SD1 and SD2; OHTB: Only Historical tweet- based", "labels": [], "entities": [{"text": "Precision (PP)", "start_pos": 19, "end_pos": 33, "type": "METRIC", "confidence": 0.8862364739179611}, {"text": "Recall (PR)", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9581984579563141}, {"text": "OHTB", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.6476249098777771}]}]}