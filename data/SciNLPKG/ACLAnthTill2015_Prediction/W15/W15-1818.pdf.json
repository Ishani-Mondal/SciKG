{"title": [], "abstractContent": [{"text": "This paper investigates sentence compression for automatic subtitle generation using supervised machine learning.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7757459282875061}, {"text": "automatic subtitle generation", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.6914200584093729}]}, {"text": "We present a method for sentence compression as well as discuss generation of training data from compressed Finnish sentences , and different approaches to the problem.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7937138080596924}]}, {"text": "The method we present outper-forms state-of-the-art baseline in both automatic and human evaluation.", "labels": [], "entities": []}, {"text": "On real data, 44.9% of the sentences produced by the compression algorithm have been judged to be useable as-is or after minor edits.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated broadcast programme subtitling is an important task, especially with the recent introduction of legislation which mandates all programmes of the Finnish national broadcasting corporation to be subtitled, even in cases where the programme is in Finnish, and not in a foreign language.", "labels": [], "entities": [{"text": "Automated broadcast programme subtitling", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7049492299556732}]}, {"text": "Providing such a subtitling is a resource-intensive task, requiring human editors to manually transcribe the programme and produce the subtitles.", "labels": [], "entities": []}, {"text": "There is an obvious motivation to automate this process to increase the subtitling throughput and drive the costs down.", "labels": [], "entities": []}, {"text": "While ultimately aiming at a fully automated pipeline from speech recognition to screen-ready subtitles, in this paper we focus specifically on the task of text compression for subtitling.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7304232120513916}, {"text": "text compression", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.7216996401548386}]}, {"text": "The need for this task arises from the fact that the whole spoken content of the programme cannot be displayed in the area of the screen devoted to subtitles while respecting the standards setting the maximum number of characters per line and the minimum amount of time the subtitles must be shown.", "labels": [], "entities": []}, {"text": "The subtitling naturally also needs to remain in time synchronisation with the spoken programme.", "labels": [], "entities": []}, {"text": "In practice, the subtitling editors thus need to compress the text of the subtitles, removing or abridging parts which are less critical for the understandability of the programme.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we compare the performance of our proposed methods and the baseline in terms of Fscore on the test set.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9991176724433899}]}, {"text": "Precision is defined as the proportion of predicted removed tokens which were also removed in gold standard, and recall is conversely defined as the proportion of removed tokens in the gold standard, whose removal is also predicted by the system.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9900879263877869}, {"text": "gold standard", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.863106906414032}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.999215841293335}]}, {"text": "The main results in show that with essentially equal compression ratios, the feature-rich CRF (referred to as crf ) results in the highest F-score in both rates of: F1-Scores for the test set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9981011748313904}, {"text": "F1-Scores", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9982975125312805}]}, {"text": "CR(tkn) is token level rate of compression and CR(chr) is character level compression rate of the system output.", "labels": [], "entities": [{"text": "CR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.953292727470398}, {"text": "CR(chr)", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9505272656679153}]}, {"text": "removal, followed by the SVM classifier (which makes independent predictions, unlike the CRF sequence classifier).", "labels": [], "entities": []}, {"text": "The baseline performs substantially worse.", "labels": [], "entities": []}, {"text": "As a further insight into the methods, we present in the ten most often removed dependency types for the best scoring crf model, the baseline, and in the test set.", "labels": [], "entities": []}, {"text": "As expected, with few exceptions we see dependency types associated with modifiers and functional words rather than core semantic arguments.", "labels": [], "entities": []}, {"text": "Most of these types will also tend to have a single, leaf dependent.", "labels": [], "entities": []}, {"text": "We can also observe a rather wide overlap (7/10) of the commonly removed dependency types between the two methods, showing that the systems target similar points of compression.", "labels": [], "entities": []}, {"text": "Further, we perform a small-scale feature ablation study with a CRF classifier.", "labels": [], "entities": []}, {"text": "The most important feature groups are those related to the token itself, such as its POS-tag and lemma.", "labels": [], "entities": []}, {"text": "The features gathered from the syntactic trees, and related location group of features both contribute positively to the classification, even though the contribution is not major.", "labels": [], "entities": []}, {"text": "The F-score measure can be evaluated on as many runs as necessary, for instance in parameter selection, but it does not necessarily reflect the ability of the system to produce fluent and meaning-preserving compressions.", "labels": [], "entities": [{"text": "F-score measure", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9822852313518524}, {"text": "parameter selection", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.6983139216899872}]}, {"text": "The underly-: Dependency types pruned in the test set ing problem is that any given sentence can have a number of valid compressions, but only one of them will be counted as a true positive, all others will be counted as false positives.", "labels": [], "entities": []}, {"text": "To address these issues, we perform also a manual evaluation of the result, discussed in the following section.", "labels": [], "entities": []}, {"text": "In this evaluation, we focus on the ability of the systems to produce fluent output and preserve important, content-bearing parts of the sentence (i.e. its \"main message\").", "labels": [], "entities": []}, {"text": "These two qualities are to some degree independent (although clearly not entirely so) and we thus evaluate them separately.", "labels": [], "entities": []}, {"text": "We selected 399 random sentences which had been compressed by the systems from the test section of the corpus, and for each sentence we produced four compressed versions: one using the baseline method, one using the crf model which got the highest F-score on the test set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 248, "end_pos": 255, "type": "METRIC", "confidence": 0.9972297549247742}]}, {"text": "In addition we test crf pos and crf ilp.", "labels": [], "entities": []}, {"text": "The crf pos set is selected because of its relatively high F-score on the test set, even though it does not employ the syntax of the sentence.", "labels": [], "entities": [{"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9976221919059753}]}, {"text": "The crf ilp is selected to test both the integer linear programming method and to provide the baseline with a comparison using the same approach to sentence reduction.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.7475073337554932}]}, {"text": "For the test, compression rates were aligned.", "labels": [], "entities": [{"text": "compression", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9824746251106262}]}, {"text": "In the end all systems had a rate of token removal of 75% for the sentences being tested.", "labels": [], "entities": []}, {"text": "The compressed versions of the sentences were subsequently evaluated by a native speaker in: Sum of the scores overall test sentences given by the evaluator.", "labels": [], "entities": [{"text": "Sum", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9480269551277161}]}, {"text": "The percentages are given in terms of maximum possible value, which for all quantities is 399 sentences \u00d7 maximum score of 3, i.e. 1197. terms of fluency and in terms of content preservation using the scales shown in.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 170, "end_pos": 190, "type": "TASK", "confidence": 0.704433262348175}]}, {"text": "The order in which the compressed versions were presented for evaluation was randomly changed for every sentence separately, i.e. it was not possible to distinguish the methods by the evaluator.", "labels": [], "entities": []}, {"text": "Further, the evaluator was not involved in the development of the methods in any manner and was thus not primed to recognize features typical of compressions produced by any of these methods.", "labels": [], "entities": []}, {"text": "To gain an initial insight into the evaluation results, we show in the sum of scores given across all sentences.", "labels": [], "entities": []}, {"text": "We can see that the crf gains on top of the baseline in terms of both measures: 6.5pp (percent points) in terms of fluency and 18.8pp in terms of meaning.", "labels": [], "entities": []}, {"text": "These correspond to 16.3% and 27.6% of relative error decrease over the baseline.", "labels": [], "entities": [{"text": "relative error decrease", "start_pos": 39, "end_pos": 62, "type": "METRIC", "confidence": 0.8510860204696655}]}, {"text": "Both differences are statistically significant with p < 0.02 (two-tailed paired t-test).", "labels": [], "entities": []}, {"text": "For practical deployment, the proportion of sentences which need no, or only minor corrections is a crucial measure.", "labels": [], "entities": []}, {"text": "For the best performing CRF method, 75.4% and 44.9% (fluency and meaning) were assigned score of 2 or 3, i.e. usable as-is or with only minor corrections.", "labels": [], "entities": [{"text": "CRF", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9150928854942322}, {"text": "meaning", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9548541903495789}]}, {"text": "For the baseline method, the corresponding proportions are 68.2% and 15.3%, reflecting a notable gain of the proposed method over the baseline.", "labels": [], "entities": []}, {"text": "When both fluency and meaning had to be assigned score of 2 or 3, 44.9% of the sentences produced by the crf method required only minor modifications for fluency or were readily usable.", "labels": [], "entities": []}, {"text": "For the baseline method only 15.0% of the sentences were rated as readily usable or requiring only minor modifications for fluency.", "labels": [], "entities": []}, {"text": "The 29.9pp gain of the proposed method corresponds to a 35.1% relative decrease in error, and ultimately manual effort saved by the proposed method.", "labels": [], "entities": [{"text": "error", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.7804510593414307}]}, {"text": "74.2% of the crf produced sentences are usable as-is or require minor fixing in terms of fluency and/or meaning, when using a more relaxed criteria and requiring fluency to be scored 2 or 3 and meaning to be 1 (Minor revisions needed for maintaining meaning) or greater, while baseline produces such sentences 63.9% of the time.", "labels": [], "entities": []}, {"text": "This difference of 10.3pp corresponds to a relative decrease in error rate of 28.5%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9878250658512115}]}, {"text": "The difference of performance between crf and crf pos when it comes to fluency or meaning is not statistically significant, although crf is rated higher on both measures.", "labels": [], "entities": []}, {"text": "Human evaluation would suggest the crf pos performs slightly worse in maintaining the meaning of the sentence (0.9pp) than crf, while the difference in fluency is very small (0.2pp).", "labels": [], "entities": []}, {"text": "This would suggest the syntax of the sentence might help the system deciding which tokens are important for the meaning of the sentence.", "labels": [], "entities": []}, {"text": "The difference in fluency between crf and crf ilp is not statistically significant, but difference between meaning is statistically significant (p < 0.02 on two-tailed paired t-test).", "labels": [], "entities": []}, {"text": "Because this system is identical in all respects but the scores used to prune the tree, to the baseline of which difference of fluency is statistically significant to the crf, this shows the CRF based scores do help with the fluency of the output.", "labels": [], "entities": []}, {"text": "The comparison between crf ilp and the baseline is interesting, because they are essentially the same system except one is based on supervised learning and the other is based on statistics.", "labels": [], "entities": [{"text": "crf ilp", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.8009855449199677}]}, {"text": "The crf ilp outperforms the baseline on both metrics and the results are statistically significant.", "labels": [], "entities": []}, {"text": "This speaks in favour of the supervised approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1-Scores for the test set. CR(tkn) is to- ken level rate of compression and CR(chr) is char- acter level compression rate of the system output.", "labels": [], "entities": [{"text": "F1-Scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9974852800369263}, {"text": "CR(tkn) is to- ken level rate of compression", "start_pos": 38, "end_pos": 82, "type": "METRIC", "confidence": 0.8222173949082693}, {"text": "CR(chr)", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9410410970449448}, {"text": "char- acter level compression rate", "start_pos": 98, "end_pos": 132, "type": "METRIC", "confidence": 0.6934972008069357}]}, {"text": " Table 2: CRF feature ablation table on develop- ment set with 85% compression rate.", "labels": [], "entities": []}]}