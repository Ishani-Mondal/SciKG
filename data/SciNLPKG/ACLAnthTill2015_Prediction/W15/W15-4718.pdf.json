{"title": [{"text": "Towards Flexible, Small-Domain Surface Generation: Combining Data-Driven and Grammatical Approaches", "labels": [], "entities": [{"text": "Small-Domain Surface Generation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.6559291084607443}]}], "abstractContent": [{"text": "As dialog systems are getting more and more ubiquitous, there is an increasing number of application domains for natural language generation , and generation objectives are getting more diverse (e.g., generating information-ally dense vs. less complex utterances, as a function of target user and usage situation).", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.7263756593068441}]}, {"text": "Flexible generation is difficult and labour-intensive with traditional template-based generation systems, while fully data-driven approaches may lead to less grammatical output, particularly if the measures used for generation objectives are correlated with measures of grammaticality.", "labels": [], "entities": []}, {"text": "We here explore the combination of a data-driven approach with two very simple automatic grammar induction methods, basing its implementation on OpenCCG.", "labels": [], "entities": [{"text": "OpenCCG", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.9343260526657104}]}], "introductionContent": [], "datasetContent": [{"text": "We build four grammars from data: two argument-only (A1, A2) and two modifier-only grammars (M1, M2).", "labels": [], "entities": []}, {"text": "In A1 and M1, verbs are exempt from merging, in A2 and M2, verbs are merged with surrounding padding words as described in 3.2.", "labels": [], "entities": []}, {"text": "We train a simple Kneser-Ney smoothed trigram on our training data, which we use in order to pre-select candidates for further evaluation.", "labels": [], "entities": []}, {"text": "After training and timeout selection, we automatically generated 200 semantic requests, each consisting of 2 to 8 semantic stacks, and generated realisations for each semantic request by each of our grammars.", "labels": [], "entities": []}, {"text": "We do this six times in total, varying the number of padding semantics P between 0 and 5.", "labels": [], "entities": []}, {"text": "We then select one short and one long sentence per semantic request from each grammar's output.", "labels": [], "entities": []}, {"text": "We pick the sentence with the lowest language model perplexity from the 25% longest and 25% shortest sentences, respectively, selecting 1540 sentences.", "labels": [], "entities": []}, {"text": "below shows the number of test semantics that each grammar is able to produce results for, grouped by the padding they contain (cf. 4.4).", "labels": [], "entities": []}, {"text": "Every other row indicates cumulative coverages, i.e., the number of covered semantics when using up to that many padding words, giving an impression of the coverage increments when using more padding words.: Test set coverage depending on request padding amount P.", "labels": [], "entities": []}, {"text": "A1: arg/POS/verbmerge, A2: arg/POS/fullmerge, M1, M2: mod grammars.", "labels": [], "entities": []}, {"text": "Coverages listed with padding = P and padding \u2264 P ( ).", "labels": [], "entities": []}, {"text": "In: Average values rounded to two decimal points.", "labels": [], "entities": []}, {"text": "\"S\": avg.", "labels": [], "entities": []}, {"text": "\"PCFG\": mean PCFG parse score.", "labels": [], "entities": [{"text": "mean PCFG parse score", "start_pos": 8, "end_pos": 29, "type": "METRIC", "confidence": 0.5895331278443336}]}, {"text": "\"HP\": fraction parseable with HPSG.", "labels": [], "entities": [{"text": "HP\"", "start_pos": 1, "end_pos": 4, "type": "DATASET", "confidence": 0.8068385124206543}, {"text": "HPSG", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9109485149383545}]}, {"text": "as well as the fact that correct use of long-range dependencies leads to local increases in perplexity when the trigram horizon fails to adequately capture the dependency.", "labels": [], "entities": []}, {"text": "G has consistently high output quality as evidenced by its small standard deviation of human ratings.", "labels": [], "entities": []}, {"text": "The modifier-only grammars consistently perform worst.", "labels": [], "entities": []}, {"text": "Both their fraction of HPSG-parseable sentences and human-perceived grammaticality are very low.", "labels": [], "entities": []}, {"text": "The argument-only grammars perform fairly well, but do not quite reach up to the manually-written grammar.", "labels": [], "entities": []}, {"text": "Their high standard deviation points towards a mix of high-quality and low-quality outputs.", "labels": [], "entities": []}, {"text": "Notet that higher HPSG parseability does not necessarily imply higher human ratings.", "labels": [], "entities": [{"text": "HPSG parseability", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.3965665251016617}]}, {"text": "We believe this is due to correct, but confusing or unnatural stacking of attributions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average values rounded to two decimal points.  \"S\": avg. sentence surprisal. \"PCFG\": mean PCFG  parse score. \"HP\": fraction parseable with HPSG.", "labels": [], "entities": [{"text": "mean PCFG  parse score", "start_pos": 95, "end_pos": 117, "type": "METRIC", "confidence": 0.6301806643605232}, {"text": "HPSG", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.8770700097084045}]}]}