{"title": [{"text": "Evaluating Models of Computation and Storage in Human Sentence Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "We examine the ability of several models of computation and storage to explain reading time data.", "labels": [], "entities": []}, {"text": "Specifically, we demonstrate on both the Dundee and the MIT reading time corpora, that fragment grammars , a model that optimizes the trade-off between computation and storage, is able to better explain people's reaction times than two baseline models which exclusively favor either storage or computation.", "labels": [], "entities": [{"text": "Dundee", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.9665180444717407}, {"text": "MIT reading time corpora", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.811519593000412}]}, {"text": "Additionally, we make a contribution by extending an existing incremental parser to handle more general grammars and scale well to larger rule and data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "A basic question for theories of language representation, processing, and acquisition is how the linguistic system balances storage and reuse of lexical units with productive computation.", "labels": [], "entities": [{"text": "language representation, processing, and acquisition", "start_pos": 33, "end_pos": 85, "type": "TASK", "confidence": 0.7339948543480465}]}, {"text": "At first glance, the question appears simple: words are stored; phrases and sentences are computed.", "labels": [], "entities": []}, {"text": "However, a closer look quickly invalidates this picture.", "labels": [], "entities": []}, {"text": "Some canonically computed structures, such as phrases, must be stored, as witnesses by verbal idioms like leave no stone unturned 2 (.", "labels": [], "entities": []}, {"text": "There is also compositionality at the sub-word level: affixes likeness in pine-scentedness, are almost always composed productively, whereas other affixes, e.g., thin warmth, are nearly always stored together with stems.", "labels": [], "entities": []}, {"text": "Facts such as these have led to a consensus in the field that storage and computation are properties that cut across different kinds of linguistic units and levels of linguistic structure-giving rise to hetergeneous lexicon 3 theories, in the terminology of.", "labels": [], "entities": []}, {"text": "Naturally, the question of what is computed and what is stored has been the focus of intense empirical and theoretical research across the language sciences.", "labels": [], "entities": []}, {"text": "On the empirical side, it has been the subject of many detailed linguistic analyses (e.g., Jackendoff (2002a)) and specific phenomena such as composition versus retrieval in word or idiom processing have been examined in many studies in experimental psycholinguistics.", "labels": [], "entities": []}, {"text": "On the theoretical side, there have been many proposals in linguistics regarding the structure and content of the heterogeneous lexicon (e.g.,,).", "labels": [], "entities": []}, {"text": "More recently, there have been a number of proposal from computational linguistics and natural language processing for how a learner might infer the correct pattern of computation and storage in their language.", "labels": [], "entities": []}, {"text": "However, there remains a gap between detailed, phenomenon-specific studies and broad architectural proposals and learning models.", "labels": [], "entities": []}, {"text": "Recently, however, a number of methodologies have emerged which promise to bridge this gap.", "labels": [], "entities": []}, {"text": "These methods make use of broad coverage probabilistic models which can encode representational and inferential assumptions, but which can also be applied to make detailed predictions on large psycholinguistic datasets encompassing a wide vari-ety of linguistic phenomena.", "labels": [], "entities": []}, {"text": "In the realm of syntax, one recent approach has been to use probabilistic models of sentence structures, paired with incremental parsing algorithms, to produce precise quantitative predictions for variables such as reading times ) or eye fixation times.", "labels": [], "entities": []}, {"text": "To date, no models of storage and computation in syntax have been applied to predict measures of human reading difficulty.", "labels": [], "entities": []}, {"text": "In this work, we employ several of the models of computation and storage studied by, to examine human sentence processing.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7011564671993256}]}, {"text": "We demonstrate that the fragment grammars model)-a model that treats the question of what to store and what to compute productively as a probabilistic inference-better explains human reading difficulty than two \"limiting-case\" baselines, MAP adaptor grammars (maximal storage) and Dirichlet-multinomial PCFG (maximal computation), in two datasets: the Dundee eye-tracking corpus () and the MIT reading time dataset ).", "labels": [], "entities": [{"text": "Dundee eye-tracking corpus", "start_pos": 352, "end_pos": 378, "type": "DATASET", "confidence": 0.9026147325833639}, {"text": "MIT reading time dataset", "start_pos": 390, "end_pos": 414, "type": "DATASET", "confidence": 0.7576206475496292}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. The last column indicates the num- ber of data points (i.e., word-specific fixation or  reading times) used in our analyses below. This  dataset was constructed by excluding data points  with zero reading times and removing rare words  (with frequencies less than 5 in the WSJ training  data). We also exclude long sentences (of greater  than 40 words) for parsing efficiency reasons.", "labels": [], "entities": [{"text": "WSJ training  data", "start_pos": 283, "end_pos": 301, "type": "DATASET", "confidence": 0.9301130374272665}]}, {"text": " Table 1: Summary statistics of reading time cor- pora -shown are the number of sentences, words,  subjects, data points before (orig) and after filter- ing (filtered).", "labels": [], "entities": []}, {"text": " Table 3: Psychological accuracy, additive tests - \u03c7 2 (1) and p values achieved by performing nested  model analysis between the models base+X and  the base model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9892629981040955}]}, {"text": " Table 4: Psychological accuracy, subtractive  test -\u03c7 2 (1) and p values achieved by performing  nested model analysis between the models full-X  and the full model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9848061203956604}, {"text": "subtractive  test -\u03c7 2", "start_pos": 34, "end_pos": 56, "type": "METRIC", "confidence": 0.8377469420433045}]}, {"text": " Table 5: Mixed-effects coefficients -the Indep.  columns refer to the coefficients learned by the  mixed-effects models base+X (one surprisal mea- sure per model), whereas the Joint columns refer  to coefficients of all surprisal measures within the  full model.", "labels": [], "entities": [{"text": "Indep", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.8301388025283813}, {"text": "surprisal mea- sure", "start_pos": 133, "end_pos": 152, "type": "METRIC", "confidence": 0.6692191809415817}]}]}