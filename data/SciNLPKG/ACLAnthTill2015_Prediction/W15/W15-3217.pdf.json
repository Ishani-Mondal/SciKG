{"title": [{"text": "QCMUQ@QALB-2015 Shared Task: Combining Character level MT and Error-tolerant Finite-State Recognition for Arabic Spelling Correction", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.772428572177887}]}], "abstractContent": [{"text": "We describe the CMU-Q and QCRI's joint efforts in building a spelling correction system for Arabic in the QALB 2015 Shared Task.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8449094891548157}, {"text": "QALB 2015 Shared Task", "start_pos": 106, "end_pos": 127, "type": "DATASET", "confidence": 0.7856736928224564}]}, {"text": "Our system is based on a hybrid pipeline that combines rule-based linguistic techniques with statistical methods using language modeling and machine translation, as well as an error-tolerant finite-state automata method.", "labels": [], "entities": []}, {"text": "We trained and tested our spelling corrector using the dataset provided by the shared task organizers.", "labels": [], "entities": [{"text": "spelling corrector", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8695508539676666}]}, {"text": "Our system outperforms the base-line system and yeilds better correction quality with an F-score of 68.12 on L1-test-2015 testset and 38.90 on the L2-test-2015.", "labels": [], "entities": [{"text": "correction quality", "start_pos": 62, "end_pos": 80, "type": "METRIC", "confidence": 0.8836139440536499}, {"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.999355137348175}]}, {"text": "This ranks us 2nd in the L2 subtask and 5th in the L1 subtask.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages.", "labels": [], "entities": []}, {"text": "Hence, the topic of text correction has seen a lot of interest in the past several years.", "labels": [], "entities": [{"text": "text correction", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8694425523281097}]}, {"text": "Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources.", "labels": [], "entities": [{"text": "correct spelling errors in texts", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8266480922698974}]}, {"text": "The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9737239480018616}]}, {"text": "The reason for this is the complexity of Arabic language and unavailability of language resources.", "labels": [], "entities": []}, {"text": "For example, the Arabic spellchecker in Microsoft Word gives incorrect suggests for even simple errors.", "labels": [], "entities": []}, {"text": "First shared task on automatic Arabic text correction ( ) has been established recently.", "labels": [], "entities": [{"text": "automatic Arabic text correction", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.5112500488758087}]}, {"text": "Its goal is to develop and evaluate spelling correction systems for Arabic trained either on naturally occurring errors in text written by humans or machines.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8807600736618042}]}, {"text": "Similar to the first version, in this task participants are asked to implement a system that takes as input MSA (Modern Standard Arabic) text with various spelling errors and automatically correct it.", "labels": [], "entities": []}, {"text": "In this year's edition, participants are asked to test their systems on two text genres: (i) news corpus (mainly newswire extracted from Aljazeera); (ii) a corpus of sentences written by learners of Arabic as a Second Language (ASL).", "labels": [], "entities": []}, {"text": "Texts produced by learners of ASL generally contain a number of spelling errors.", "labels": [], "entities": [{"text": "ASL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9586557745933533}]}, {"text": "The main problem faced by them is using Arabic with vocabulary and grammar rules that are different from their native language.", "labels": [], "entities": []}, {"text": "In this paper, we describe our Arabic spelling correction system.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7081775218248367}]}, {"text": "Our system is based on a hybrid pipeline which combines rule-based techniques with statistical methods using language modeling and machine translation, as well as an error-tolerant finite-state automata method.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7448484301567078}]}, {"text": "We trained and tested our spelling corrector using the dataset provided by the shared task organizers Arabic (.", "labels": [], "entities": [{"text": "spelling corrector", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8623802065849304}]}, {"text": "Our systems outperform the baseline and achieve better correction quality with an F-score of 68.42% on the 2014 testset and 44.02 % on the L2 Dev.", "labels": [], "entities": [{"text": "correction quality", "start_pos": 55, "end_pos": 73, "type": "METRIC", "confidence": 0.9138245284557343}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9996664524078369}]}], "datasetContent": [{"text": "We experimented with different configurations to reach an optimal setting when combining different modules.", "labels": [], "entities": []}, {"text": "We evaluated our system for precision, recall, and F measure (F1) against the devset reference and the test 2014 set.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9996048808097839}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9983636736869812}, {"text": "F measure (F1)", "start_pos": 51, "end_pos": 65, "type": "METRIC", "confidence": 0.9060471892356873}]}, {"text": "Results for vari-ous system configurations on the L2 dev and test 2014 sets are given in We achieved our best F-measure value with the following configuration: using CBMT system after applying the clitic re-attachment rules.", "labels": [], "entities": [{"text": "L2 dev and test 2014 sets", "start_pos": 50, "end_pos": 75, "type": "DATASET", "confidence": 0.6970748653014501}, {"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9874241948127747}]}, {"text": "These were then passed through the EFST.", "labels": [], "entities": [{"text": "EFST", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9755724668502808}]}, {"text": "Using this combination we are able to correct 66.79% of the errors on the 2014 test set with a precision of 70.14%.", "labels": [], "entities": [{"text": "correct", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9689364433288574}, {"text": "errors", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.8807955384254456}, {"text": "2014 test set", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.8980923891067505}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.998955488204956}]}, {"text": "Our system outperforms the baseline for the L2 data as well with an F-measure of 44.02% compared to (F1=20.28% when we use the Morph module).", "labels": [], "entities": [{"text": "L2 data", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.6957069337368011}, {"text": "F-measure", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9994528889656067}, {"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9978467226028442}]}], "tableCaptions": [{"text": " Table 1: Statistics on Error Types in the QALB 2014 and 2015 datasets", "labels": [], "entities": [{"text": "QALB 2014 and 2015 datasets", "start_pos": 43, "end_pos": 70, "type": "DATASET", "confidence": 0.9281437754631042}]}, {"text": " Table 3: System results on the QALB 2014 test set (left) and L2 dev set (right).", "labels": [], "entities": [{"text": "QALB 2014 test set", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.966118648648262}]}, {"text": " Table  4. These results rank us 2nd in the L2 subtask and  5th in the L1 subtask.", "labels": [], "entities": []}, {"text": " Table 4: The QCMUQ Official results on the 2015  test set.", "labels": [], "entities": [{"text": "QCMUQ", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.6241252422332764}, {"text": "2015  test set", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.8564019997914633}]}]}