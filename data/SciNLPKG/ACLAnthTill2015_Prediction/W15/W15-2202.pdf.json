{"title": [{"text": "Combining Active Learning and Partial Annotation for Domain Adaptation of a Japanese Dependency Parser", "labels": [], "entities": [{"text": "Domain Adaptation of a Japanese Dependency Parser", "start_pos": 53, "end_pos": 102, "type": "TASK", "confidence": 0.5780121513775417}]}], "abstractContent": [{"text": "The machine learning-based approaches that dominate natural language processing research require massive amounts of labeled training data.", "labels": [], "entities": []}, {"text": "Active learning has the potential to substantially reduce the human effort needed to prepare this data by allowing annotators to focus on only the most informative training examples.", "labels": [], "entities": []}, {"text": "This paper shows that active learning can be used for domain adaptation of dependency parsers, not just in single-domain settings.", "labels": [], "entities": [{"text": "domain adaptation of dependency parsers", "start_pos": 54, "end_pos": 93, "type": "TASK", "confidence": 0.7938181102275849}]}, {"text": "We also show that entropy-based query selection strategies can be combined with partial annotation to annotate informative examples in the new domain without annotating full sentences.", "labels": [], "entities": []}, {"text": "Simulations are common in work on active learning, but we measured the actual time needed for manual annotation of data to better frame the results obtained in our simulations.", "labels": [], "entities": []}, {"text": "We evaluate query strategies based on both full and partial annotation in several domains, and find that they reduce the amount of in-domain training data needed for domain adaptation by up to 75% compared to random selection.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.7184247374534607}]}, {"text": "We found that partial annotation delivers better in-domain performance for the same amount of human effort than full annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Active learning is a promising approach for domain adaptation because it offers away to reduce the amount of data needed to train classifiers, minimizing the amount of difficult in-domain annotation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7365697473287582}]}, {"text": "This type of annotation requires annotators to have both domain knowledge plus familiarity with annotation standards.", "labels": [], "entities": []}, {"text": "There has been much recent work on active learning fora variety of natural language processing tasks, but most of it is concerned only with the single-domain case.", "labels": [], "entities": []}, {"text": "Additionally, work on active learning commonly reports results for simulations only because of the high cost of annotation work.", "labels": [], "entities": []}, {"text": "We use active learning to perform domain adaptation fora Japanese dependency parsing task, and measure the actual time required for manual annotation of training data to better frame the results of our experiments.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7069321721792221}, {"text": "Japanese dependency parsing task", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.6379284486174583}]}, {"text": "This kind of evaluation is crucial for assessing the effectiveness of active learning in practice.", "labels": [], "entities": []}, {"text": "Previous work on active learning for structured prediction tasks like parsing) often assumes that the training data must be fully annotated.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.8649606704711914}]}, {"text": "But recent work on dependency parsing has shown that models trained from partially annotated data (where only part of the tree structure is annotated) can achieve competitive performance.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8493586778640747}]}, {"text": "However, deciding which portion of the tree structure to annotate remains a difficult problem.", "labels": [], "entities": []}], "datasetContent": [{"text": "To  reach a certain level of in-domain accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.99540114402771}]}, {"text": "For the 2-stage strategy, we also measured how many dependencies areal annotator could annotate in a given time using partial and full annotation.", "labels": [], "entities": []}, {"text": "Measuring the actual annotation time is important because our goal of active learning is to reduce the amount of human effort needed to prepare labeled training data for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 170, "end_pos": 187, "type": "TASK", "confidence": 0.7169062048196793}]}, {"text": "We used a corpus of example Japanese sentences from a dictionary as source domain training data).", "labels": [], "entities": []}, {"text": "This data was used as to train the initial model in each experiment.", "labels": [], "entities": []}, {"text": "We also collected Japanese text from three target domains: newspapers 3 , journal article abstracts, and patents).", "labels": [], "entities": []}, {"text": "For each domain, there is a large annotation pool of potential training examples and a smaller test set.", "labels": [], "entities": []}, {"text": "Domain adaptation is needed in each case, because sentence length and vocabulary differs for each.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7614493668079376}]}, {"text": "Words in each sentence were manually segmented and assigned POS automatically with the tagger KyTea.", "labels": [], "entities": [{"text": "POS", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.97728031873703}]}, {"text": "This step can be done automatically because KyTea's F-measure score for word segmentation and POS tagging is about 98% ).", "labels": [], "entities": [{"text": "F-measure score", "start_pos": 52, "end_pos": 67, "type": "METRIC", "confidence": 0.9708950817584991}, {"text": "word segmentation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7515573501586914}, {"text": "POS tagging", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.7682144343852997}]}, {"text": "Words were then manually annotated with their heads.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An example of full annotation (d i full) and partial annotation (d i part.) for a sentence. Features  for the dependency between the case marker (subj.) and the verb (welcomes) are also shown.", "labels": [], "entities": []}, {"text": " Table 2: Sizes of corpora.", "labels": [], "entities": []}, {"text": " Table 3: Annotation times for 2-stage methods.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8926473259925842}]}, {"text": " Table 4: Reduction in in-domain data.", "labels": [], "entities": []}]}