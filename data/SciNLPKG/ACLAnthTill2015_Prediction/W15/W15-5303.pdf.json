{"title": [{"text": "Resolving Entity Coreference in Croatian with a Constrained Mention-Pair Model", "labels": [], "entities": [{"text": "Resolving Entity Coreference", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8588634133338928}]}], "abstractContent": [{"text": "Being able to identify that different mentions refer to the same entity is beneficial for applications such as question answering and text summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.9361671209335327}, {"text": "text summarization", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.7855226993560791}]}, {"text": "In this paper, we propose the first model for entity corefer-ence resolution for Croatian.", "labels": [], "entities": [{"text": "entity corefer-ence resolution", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6555443306763967}]}, {"text": "We enforce transitivity constraints with integer linear programming on top of pairwise decisions produced by the supervised mention-pair model.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed model significantly outperforms two different rule-based baselines, reaching performance of 74.4% MUC score and 77.6% B 3 score.", "labels": [], "entities": [{"text": "MUC score", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9409520030021667}, {"text": "B 3 score", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9890486200650533}]}], "introductionContent": [{"text": "Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades ().", "labels": [], "entities": [{"text": "Entity coreference resolution", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8473724524180094}, {"text": "natural language processing (NLP)", "start_pos": 150, "end_pos": 183, "type": "TASK", "confidence": 0.784344325462977}]}, {"text": "Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction), question answering, and text summarization (.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.947265088558197}, {"text": "relation extraction", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.8665434718132019}, {"text": "question answering", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.9325181245803833}, {"text": "text summarization", "start_pos": 217, "end_pos": 235, "type": "TASK", "confidence": 0.7482029795646667}]}, {"text": "Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving \"U.S. President\" and \"Barack Obama\", one needs to know that Obama is the president of the USA) (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9771260321140289}]}, {"text": "Although machine learning-based approaches to anaphora and coreference resolution for English appeared almost two decades ago (), for many languages, including the majority of Slavic languages, no coreference resolution systems exist, mainly due to the lack of annotated corpora required for developing such systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.8812346756458282}, {"text": "coreference resolution", "start_pos": 197, "end_pos": 219, "type": "TASK", "confidence": 0.8022676706314087}]}, {"text": "In this paper, we present a coreference resolution model for Croatian.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.9275214374065399}]}, {"text": "Our model enforces transitivity of coreference relations via integer linear programming (ILP) optimization over a set of binary coreference decisions made by the supervised mention-pair model.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work on coreference resolution for Croatian, and one of the first efforts in coreference resolution for Slavic languages in general.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.9651789367198944}, {"text": "coreference resolution", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.9466758370399475}]}], "datasetContent": [{"text": "Supervised coreference models require a manually annotated dataset.", "labels": [], "entities": []}, {"text": "We next describe how we compiled a coreference resolution dataset for Croatian.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.9320251643657684}]}, {"text": "We split the manually annotated dataset consisting of 270 documents into a train set containing 220 documents and a test set with 50 documents.", "labels": [], "entities": []}, {"text": "We optimized the hyperparameters of our SVM mention-pair model (C and \u03b3) by means of 10-folded cross validation.", "labels": [], "entities": []}, {"text": "We then trained the model with the optimal hyperparameters on the entire train set and evaluated that model on the test set.", "labels": [], "entities": []}, {"text": "The test set is available from http://takelab.fer.hr/crocoref  Baselines.", "labels": [], "entities": []}, {"text": "We compare the performance of our transitively coherent mention-pair model against two different baseline models.", "labels": [], "entities": []}, {"text": "The OVERLAP baseline classifies two mentions as coreferent if they share at least one content word.", "labels": [], "entities": [{"text": "OVERLAP baseline classifies", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.8046825031439463}]}, {"text": "The GENDNUM baseline links each mention to the closest preceding mention with which it matches in gender and number.", "labels": [], "entities": [{"text": "GENDNUM baseline", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7634256184101105}]}, {"text": "Standard closest-first clustering () is applied for both baselines.", "labels": [], "entities": []}, {"text": "We show the performance of our mention-pair model, both without (MP) and with (MP+ILP) enforcing transitivity, along with the performance of both baselines in.", "labels": [], "entities": []}, {"text": "We evaluate all models in terms of two standard evaluation measures for coreference resolution -MUC score and B 3 score.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.9671635031700134}, {"text": "MUC score", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.8985370099544525}, {"text": "B 3 score", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9766214887301127}]}, {"text": "In order to evaluate the contribution of morphological features, we additionally evaluate the mention-pair model but excluding all features relying on morphological preprocessing (MP-MORPH).", "labels": [], "entities": []}, {"text": "Results show that the supervised mention-pair model significantly outperforms both reasonable rule-based baselines.", "labels": [], "entities": []}, {"text": "When morphological features are not used, the model exhibits a slightly lower performance, although the difference is not substantial.", "labels": [], "entities": []}, {"text": "Enforcing transitivity in an ILP setting marginally improves the overall MUC score, but yields notable 2-point improvement in B 3 score.", "labels": [], "entities": [{"text": "MUC score", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.7823904454708099}, {"text": "B 3 score", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9854121605555216}]}, {"text": "Precision is consistently higher than recall for all models and both evaluation metrics, which is consistent with the coreference resolution results for other languages.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9950481057167053}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9992337226867676}]}, {"text": "Overall, our results are over 10 points higher than the state-of-the-art performance for English () and comparable (higher MUC and lower B 3 score) to the best results obtained for Polish (, suggesting that coreference resolution maybe easier task for morphologically complex languages.", "labels": [], "entities": [{"text": "MUC", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9781966805458069}, {"text": "B 3 score", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9854796330134074}, {"text": "coreference resolution", "start_pos": 207, "end_pos": 229, "type": "TASK", "confidence": 0.9687938988208771}]}, {"text": "In an attempt to identify the most common types of errors, we manually analyzed the errors made by the supervised mentionpair model.", "labels": [], "entities": []}, {"text": "The vast majority of false negatives originate from mention pairs where external knowledge is necessary for inferring coreference, e.g., \u02c7 zeljezni kancelar (iron chancellor) and Bismarck).", "labels": [], "entities": []}, {"text": "Other common causes of false negatives include abbreviations, e.g., DS and Demokratski savez (Democratic Alliance), and distant pronominal anaphora (i.e., when an anaphoric pronoun is faraway from its preceding coreferent mention).", "labels": [], "entities": []}, {"text": "Most false positives stem from non-coreferent mentions with substantial lexical overlap, e.g., Dru\u0161tvo hrvatskih knji\u017eevnika (Croatian Writers' Association) and sve\u010danosti u Dru\u0161tvu hrvatskih knji\u017eevnika (ceremonies at the Croatian Writers' Association).", "labels": [], "entities": []}, {"text": "A significant number of false positives are due to a pronominal mention being close to some non-coreferent noun-phrase mention.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Coreference resolution performance.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9216417968273163}]}]}