{"title": [{"text": "Overview of the 2nd Workshop on Asian Translation", "labels": [], "entities": [{"text": "2nd Workshop on Asian Translation", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.5456902801990509}]}], "abstractContent": [{"text": "This paper presents the results of the shared tasks from the 2nd workshop on Asian translation (WAT2015) including J\u2194E, J\u2194C scientific paper translation subtasks and C\u2192J, K\u2192J patent translation subtasks.", "labels": [], "entities": [{"text": "Asian translation (WAT2015)", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.6941206276416778}, {"text": "J\u2194C scientific paper translation subtasks", "start_pos": 120, "end_pos": 161, "type": "TASK", "confidence": 0.7136591076850891}]}, {"text": "For the WAT2015, 12 institutions participated in the shared tasks.", "labels": [], "entities": [{"text": "WAT2015", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.5574740171432495}]}, {"text": "About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Workshop on Asian Translation (WAT) is anew open evaluation campaign focusing on Asian languages.", "labels": [], "entities": [{"text": "Asian Translation (WAT)", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8391997456550598}]}, {"text": "Following the success of the previous workshop WAT2014(), WAT2015 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7737557590007782}, {"text": "machine translation", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7420582175254822}]}, {"text": "We are working toward the practical use of machine translation among all Asian countries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7798082828521729}]}, {"text": "For the 2nd WAT, we adopt new translation subtasks \"Chinese-to-Japanese and Koreanto-Japanese patent translation\" in addition to the subtasks that were conducted in WAT2014.", "labels": [], "entities": [{"text": "2nd WAT", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.6628024876117706}, {"text": "Koreanto-Japanese patent translation", "start_pos": 76, "end_pos": 112, "type": "TASK", "confidence": 0.6527456839879354}, {"text": "WAT2014", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.8804150819778442}]}, {"text": "WAT is unique for the following reasons: \u2022 Open innovation platform The test data is fixed and open, so evaluations can be repeated on the same data set to confirm changes in translation accuracy overtime.", "labels": [], "entities": [{"text": "WAT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.661195695400238}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9039868116378784}]}, {"text": "WAT has no deadline for automatic translation quality evaluation (continuous evaluation), so translation results can be submitted at anytime.", "labels": [], "entities": [{"text": "WAT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6332970261573792}, {"text": "translation quality evaluation", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.8098666667938232}]}, {"text": "\u2022 Domain and language pairs WAT is the world's first workshop that uses scientific papers as the domain, and Chinese\u2194Japanese and Korean\u2192Japanese as language pairs.", "labels": [], "entities": [{"text": "WAT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.7314252257347107}]}, {"text": "In the future, we will add more Asian languages, such as Vietnamese, Indonesian, Thai, Burmese and soon.", "labels": [], "entities": []}, {"text": "\u2022 Evaluation method Evaluation is done both automatically and manually.", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.8451676964759827}]}, {"text": "For human evaluation, WAT uses crowdsourcing, which is low cost and allows multiple evaluations, as the first-stage evaluation.", "labels": [], "entities": [{"text": "WAT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9808019995689392}]}, {"text": "Also, JPO adequacy evaluation is conducted for the selected submissions according to the crowdsourcing evaluation results.", "labels": [], "entities": []}], "datasetContent": [{"text": "WAT uses the Asian Scientific Paper Excerpt Corpus (ASPEC) and JPO Patent Corpus (JPC) 2 as the dataset.", "labels": [], "entities": [{"text": "WAT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5935371518135071}, {"text": "Asian Scientific Paper Excerpt Corpus (ASPEC)", "start_pos": 13, "end_pos": 58, "type": "DATASET", "confidence": 0.8194211795926094}, {"text": "JPO Patent Corpus (JPC) 2", "start_pos": 63, "end_pos": 88, "type": "DATASET", "confidence": 0.9229679788861956}]}, {"text": "We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU () and RIBES (.", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9583835005760193}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9990173578262329}, {"text": "RIBES", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.9934886693954468}]}, {"text": "BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (; RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9807097911834717}, {"text": "RIBES", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9303491711616516}]}, {"text": "Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7038722932338715}]}, {"text": "For Japanese segmentation, we used three different tools: Juman version 7.0 (, KyTea 0.4.6) with Full SVM model).", "labels": [], "entities": [{"text": "Japanese segmentation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6330086290836334}, {"text": "Juman version 7.0", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.8835446635882059}]}, {"text": "For Korean segmentation we used mecab-ko . For English segmentation we used tokenizer.perl 13 in the Moses toolkit.", "labels": [], "entities": [{"text": "Korean segmentation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6943686306476593}, {"text": "English segmentation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7091419547796249}]}, {"text": "Detailed procedures for the automatic evaluation are shown on the WAT2015 evaluation web page .  The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results.", "labels": [], "entities": [{"text": "WAT2015 evaluation web page", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.9201230257749557}, {"text": "WAT2015 web page", "start_pos": 192, "end_pos": 208, "type": "DATASET", "confidence": 0.964542547861735}]}, {"text": "shows the submission interface for participants.", "labels": [], "entities": []}, {"text": "The system requires participants to provide the following information when they upload translation results: \u2022 Subtask: \u2022 Method (SMT, RBMT, SMT and RBMT, EBMT, Other); \u2022 Use of other resources in addition to ASPEC or JPC; \u2022 Permission to publish the automatic evaluation scores on the WAT2015 web page.", "labels": [], "entities": [{"text": "WAT2015 web page", "start_pos": 285, "end_pos": 301, "type": "DATASET", "confidence": 0.8885090748469034}]}, {"text": "The server for the system stores all submitted information, including translation results and scores, although participants can confirm only the information that they uploaded.", "labels": [], "entities": []}, {"text": "Information about translation results that participants permit to be published is disclosed on the web page.", "labels": [], "entities": []}, {"text": "In addition to submitting translation results for automatic evaluation, participants submit the results for human evaluation using the same web interface.", "labels": [], "entities": []}, {"text": "This automatic evaluation system will remain available even after WAT2015.", "labels": [], "entities": [{"text": "WAT2015", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.5821799039840698}]}, {"text": "Anybody can register to use the system on the registration web page 15 .  In WAT2015, we conducted 2 kinds of human evaluations: pairwise crowdsourcing evaluation and JPO adequacy evaluation.", "labels": [], "entities": [{"text": "WAT2015", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.7152031660079956}, {"text": "JPO adequacy evaluation", "start_pos": 167, "end_pos": 190, "type": "TASK", "confidence": 0.5858639677365621}]}, {"text": "The pairwise crowdsourcing evaluation is the same as the last year.", "labels": [], "entities": []}, {"text": "We used Lancers as the crowdsourcing platform.", "labels": [], "entities": [{"text": "Lancers", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.9480137825012207}]}, {"text": "There are two reasons of choosing Lancers.", "labels": [], "entities": []}, {"text": "One is that we can set the category of the crowdsourcing task ('Translation' in this case).", "labels": [], "entities": []}, {"text": "We can reach the appropriate workers by choosing the appropriate categories.", "labels": [], "entities": []}, {"text": "The other reason is that we can assign the task to identityverified workers.", "labels": [], "entities": []}, {"text": "This function guaranteed the quality of the workers.", "labels": [], "entities": []}, {"text": "These two advantages ensure a high evaluation quality.", "labels": [], "entities": []}, {"text": "We used the same sentences as the last year for the pairwise crowdsourcing evaluation.", "labels": [], "entities": []}, {"text": "We randomly chose documents from the Test set from the ASPEC data, fora total of 400 sentence pairs for JE and JC.", "labels": [], "entities": [{"text": "ASPEC data", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.8433589339256287}]}, {"text": "We excluded documents containing sentences longer than 100 Japanese characters.", "labels": [], "entities": []}, {"text": "Each submission is compared with the baseline translation (Phrase-based SMT, described in Section 3) and given a Crowd score 16 .  We conducted pairwise evaluation of each of the 400 test sentences.", "labels": [], "entities": [{"text": "Phrase-based SMT", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.5814702659845352}]}, {"text": "The input sentence and two translations (the baseline and a submission) are shown to the workers, and the workers are asked to judge which of the translation is better, or if they are of the same quality.", "labels": [], "entities": []}, {"text": "The order of the two translations are at random.", "labels": [], "entities": []}, {"text": "The participants' systems, which achieved the top 3 highest scores among the pairwise crowdsourcing evaluation results of each subtask, were also evaluated with the JPO adequacy evaluation.", "labels": [], "entities": [{"text": "JPO", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.6653140187263489}]}, {"text": "The JPO adequacy evaluation was carried out by translation experts with a quality evaluation criterion for translated patent documents which the Japanese Patent Office (JPO) decided.", "labels": [], "entities": [{"text": "Japanese Patent Office (JPO)", "start_pos": 145, "end_pos": 173, "type": "DATASET", "confidence": 0.6486300726731619}]}, {"text": "In addition to the top 3 systems, the Sense 1 system of the JPC-KJ subtask, which was the lower score on the pairwise crowdsourcing evaluation despite the high score on the automatic evaluation, was evaluated exceptionally.", "labels": [], "entities": [{"text": "JPC-KJ subtask", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9527065753936768}]}, {"text": "For each system, two annotators evaluate the test sentences to guarantee the quality.", "labels": [], "entities": []}, {"text": "The number of test sentences for the JPO adequacy evaluation is 200.", "labels": [], "entities": [{"text": "JPO", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.5961923003196716}]}, {"text": "The 200 test sentences were randomly selected from the 400 test sentences of the pairwise evaluation.", "labels": [], "entities": []}, {"text": "The test sentence include the input sentence, the submitted system's translation and the reference translation.", "labels": [], "entities": []}, {"text": "shows the JPO adequacy criterion from 5 to 1.", "labels": [], "entities": [{"text": "JPO", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6522168517112732}]}, {"text": "The evaluation is performed subjectively.", "labels": [], "entities": []}, {"text": "\"Important information\" represents the technical factors and their relationships.", "labels": [], "entities": []}, {"text": "The degree of importance of each element is also considered to evaluate.", "labels": [], "entities": []}, {"text": "The percentages in each grade are rough indications for the transmission degree of the source sentence meanings.", "labels": [], "entities": []}, {"text": "The detailed criterion can be found on the JPO document (in Japanese) . 6 Participants List shows the list of participants for WAT2015.", "labels": [], "entities": [{"text": "JPO document", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9676705300807953}, {"text": "WAT2015", "start_pos": 127, "end_pos": 134, "type": "TASK", "confidence": 0.7194952368736267}]}, {"text": "This includes not only Japanese organizations, but also some organizations from outside Japan.", "labels": [], "entities": []}, {"text": "12 teams submitted one or more translation results to the both automatic evaluation server and human evaluation.", "labels": [], "entities": []}, {"text": "Waseda University \u2713 naver ( NAVER Corporation \u2713 \u2713 EHR Ehara NLP Research Laboratory \u2713 \u2713 \u2713 \u2713 ntt ( NTT Communication Science Laboratories \u2713: List of participants who submitted translation results to WAT2015 and their participation in each subtasks.", "labels": [], "entities": [{"text": "naver", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.926486611366272}, {"text": "EHR Ehara NLP Research Laboratory", "start_pos": 50, "end_pos": 83, "type": "DATASET", "confidence": 0.9276692867279053}, {"text": "NTT Communication Science Laboratories", "start_pos": 98, "end_pos": 136, "type": "DATASET", "confidence": 0.8816275000572205}, {"text": "WAT2015", "start_pos": 198, "end_pos": 205, "type": "TASK", "confidence": 0.5219929218292236}]}, {"text": "In this section, the evaluation results for WAT2015 are reported from several perspectives.", "labels": [], "entities": [{"text": "WAT2015", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.832176685333252}]}, {"text": "Some of the results for both automatic and human evaluations are also accessible at the WAT2015 website 19 .   Crowd Score shows the official crowdsourcing evaluation results.", "labels": [], "entities": [{"text": "WAT2015 website 19 .   Crowd Score", "start_pos": 88, "end_pos": 122, "type": "DATASET", "confidence": 0.9170051713784536}]}, {"text": "The error bars in the figures show the 95% confidence interval (see Section 5.1.4).", "labels": [], "entities": [{"text": "confidence interval", "start_pos": 43, "end_pos": 62, "type": "METRIC", "confidence": 0.8412470519542694}]}, {"text": "Note that overlapping error bars between two submissions do not necessarily mean that there is no significant difference.", "labels": [], "entities": []}, {"text": "If an error bar crosses the x-axis (Crowd score = 0), it means that there is no significant difference between the submission and the baseline (SMT Phrase).", "labels": [], "entities": []}, {"text": "Figure 6 and 7 show the correlations between the automatic evaluation measures (BLEU/RIBES) and the Crowd score.: Statistical significance testing of the ASPEC-EJ Crowd scores.", "labels": [], "entities": [{"text": "BLEU/RIBES)", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.7870967835187912}, {"text": "ASPEC-EJ Crowd scores", "start_pos": 154, "end_pos": 175, "type": "DATASET", "confidence": 0.8655170202255249}]}, {"text": "shows the chronological evaluation results of ASPEC.", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 46, "end_pos": 51, "type": "TASK", "confidence": 0.5581189393997192}]}, {"text": "The x-axis indicates the BLEU score and the y-axis indicates the Crowd score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9838771224021912}, {"text": "Crowd score", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.6213897168636322}]}, {"text": "Note that the first 3 annotations among 5 by the crowdsourcing were used, and the decision for each sentence is made by the same criteria in WAT2014 for calculating the Crowd score of WAT2015 submissions.", "labels": [], "entities": [{"text": "WAT2014", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.9509154558181763}, {"text": "WAT2015", "start_pos": 184, "end_pos": 191, "type": "DATASET", "confidence": 0.5711936354637146}]}, {"text": "shows the summary of automatic and human evaluations for the selected submissions.", "labels": [], "entities": []}, {"text": "We can see that all of Crowd, BLEU and RIBES scores partially correlate to the JPO adequacy score, but none of them perfectly correlates.", "labels": [], "entities": [{"text": "Crowd", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8712971210479736}, {"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9949742555618286}, {"text": "RIBES", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9574326872825623}, {"text": "JPO adequacy score", "start_pos": 79, "end_pos": 97, "type": "DATASET", "confidence": 0.5562337040901184}]}, {"text": "Especially, for JPC-KJ, both BLEU and RIBES failed to correctly evaluate the quality of Sense team.", "labels": [], "entities": [{"text": "JPC-KJ", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8996729254722595}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9982712268829346}, {"text": "RIBES", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9750785827636719}]}, {"text": "From the evaluation results, the following can be observed (see):   \u2022 Neural Network based re-ranking is effective (NAIST, Kyoto-U, naver).", "labels": [], "entities": [{"text": "NAIST", "start_pos": 116, "end_pos": 121, "type": "DATASET", "confidence": 0.9506657719612122}, {"text": "Kyoto-U", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.8368221521377563}, {"text": "naver", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.9480013251304626}]}, {"text": "\u2022 The top SMT outperformed RBMT for CJ and KJ patent translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9882178902626038}, {"text": "RBMT", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.516053318977356}, {"text": "KJ patent translation", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.5722374022006989}]}, {"text": "\u2022 K\u2192J patent translation achieved high scores for automatic and human evaluations.", "labels": [], "entities": [{"text": "J patent translation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.5023691654205322}]}, {"text": "\u2022 A new problem of automatic evaluation was found in the KJ evaluation.", "labels": [], "entities": [{"text": "KJ evaluation", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.7686820030212402}]}], "tableCaptions": [{"text": " Table 12: The Fleiss' kappa values for the crowdsourcing evaluation results.", "labels": [], "entities": []}, {"text": " Table 13: JPO adequacy evaluation results.", "labels": [], "entities": []}]}