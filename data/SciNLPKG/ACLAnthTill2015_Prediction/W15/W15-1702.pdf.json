{"title": [{"text": "Paraphrase Identification and Semantic Similarity in Twitter with Simple Features", "labels": [], "entities": [{"text": "Paraphrase Identification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8767490088939667}]}], "abstractContent": [{"text": "Paraphrase Identification and Semantic Similarity are two different yet well related tasks in NLP.", "labels": [], "entities": [{"text": "Paraphrase Identification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8724450469017029}, {"text": "Semantic Similarity", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8034675419330597}]}, {"text": "There are many studies on these two tasks extensively on structured texts in the past.", "labels": [], "entities": []}, {"text": "However, with the strong rise of social media data, studying these tasks on unstructured texts, particularly, social texts in Twitter is very interesting as it could be more complicated problems to deal with.", "labels": [], "entities": []}, {"text": "We investigate and find a set of simple features which enables us to achieve very competitive performance on both tasks in Twitter data.", "labels": [], "entities": []}, {"text": "Interestingly, we also confirm the significance of using word alignment techniques from evaluation metrics in machine translation in the overall performance of these tasks.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7800540924072266}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7202859967947006}]}], "introductionContent": [{"text": "Paraphrase Identification and Semantic Similarity are important tasks that can be used as features to improve many other Natural Language Processing (NLP) tasks, e.g. Information Retrieval, Machine Translation Evaluation, Text Summarization, Question and Answering, and others.", "labels": [], "entities": [{"text": "Paraphrase Identification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8403124511241913}, {"text": "Semantic Similarity", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7264249324798584}, {"text": "Information Retrieval", "start_pos": 167, "end_pos": 188, "type": "TASK", "confidence": 0.8277846276760101}, {"text": "Machine Translation Evaluation", "start_pos": 190, "end_pos": 220, "type": "TASK", "confidence": 0.8770860433578491}, {"text": "Text Summarization", "start_pos": 222, "end_pos": 240, "type": "TASK", "confidence": 0.8238721787929535}, {"text": "Question and Answering", "start_pos": 242, "end_pos": 264, "type": "TASK", "confidence": 0.7391891876856486}]}, {"text": "Besides this, analyzing social media data like tweets of the social network Twitter is afield of growing interest for different purposes.", "labels": [], "entities": []}, {"text": "The study of these typical NLP tasks on Twitter data can be very interesting as social media data carries lot of surprises and unpredictable information.", "labels": [], "entities": []}, {"text": "The Paraphrase Identification is a classic NLP task which is a classification problem.", "labels": [], "entities": [{"text": "Paraphrase Identification", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9013371765613556}]}, {"text": "Given a pair of sentences, the system is required to assess if the two sentences carry the same meaning, to classify them paraphrase, or not paraphrase otherwise.", "labels": [], "entities": []}, {"text": "Likewise, Semantic Similarity is another NLP task in which the system needs to examine the similarity degree (in a pre-defined semantic scale) of a given pair of texts, varying in different levels such as word, phrase, sentence, or paragraph.", "labels": [], "entities": [{"text": "Semantic Similarity", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8246216177940369}]}, {"text": "There are different approaches, both supervised and unsupervised, have been proposed for these two tasks, ranging from simple level like word/n-gram overlapping, string matching, to more complicated ones like semantic word similarity, word alignment, syntactic structure, etc.", "labels": [], "entities": [{"text": "word/n-gram overlapping", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.5954555347561836}, {"text": "string matching", "start_pos": 162, "end_pos": 177, "type": "TASK", "confidence": 0.7144897431135178}, {"text": "semantic word similarity", "start_pos": 209, "end_pos": 233, "type": "TASK", "confidence": 0.5735026101271311}, {"text": "word alignment", "start_pos": 235, "end_pos": 249, "type": "TASK", "confidence": 0.765457034111023}]}, {"text": "1,2 However, it is challenging or even inapplicable to deploy all these approaches to social media data, like Twitter data, due to many differences the social media data carries, such as misspelling, word out of vocabulary, slang, acronyms, style, structure, etc.", "labels": [], "entities": []}, {"text": "In this paper, we study and find a set of simple features specifically chosen and suitable for social media data which is relatively easy to obtain, but able to achieve very competitive performance on both tasks for Twitter data.", "labels": [], "entities": []}, {"text": "We also analyze the significance of each feature quantitatively and qualitatively in the overall performance.", "labels": [], "entities": []}, {"text": "As a result, we can prove our hypothesis that the combination of simple features like word/n-gram overlapping, word alignment, and semantic word similarity can result in very good performance for both tasks on social media data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.7433347702026367}]}, {"text": "The paper is organized as follows: Section 2 presents the Related Work, Section 3 describes the tasks and set of features, Section 4 shows the Experiments, Section 5 reports the Evaluations, Section 6 discusses the Error Analysis, and finally Section 7 is the Conclusions and Future Work.", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 215, "end_pos": 229, "type": "TASK", "confidence": 0.7947636246681213}]}], "datasetContent": [{"text": "We use the latest version of METEOR () that find alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.8848541975021362}]}, {"text": "We used the system as distributed on its website using only the \"norm\" option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.", "labels": [], "entities": []}, {"text": "We compute the word alignment scores on sentences and on sentences with partof-speech and named entity tags, as our idea is that if two sentences are similar, their tagged version also should be similar.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.6351160556077957}]}, {"text": "We use another metric for machine translation BLEU () that is one of the most commonly used and because of that has an high reliability.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7367653846740723}, {"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9611364006996155}]}, {"text": "It is computed as the amount of n-gram overlap, for different values of n=1,2,3, and 4, between the system output and the reference translation, in our case between sentence pairs.", "labels": [], "entities": []}, {"text": "The score is tempered by a penalty for translations that might be too short.", "labels": [], "entities": []}, {"text": "BLEU relies on exact matching and has no concept of synonymy or paraphrasing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8124205470085144}]}, {"text": "As the length of tweets is relatively short, it is only 140-character message, we do not expect to have large n-gram overlaps, except 1-gram and 2-gram.", "labels": [], "entities": []}, {"text": "Our analysis actually shows that 3-gram, 4-gram and the average score may cause more noise.", "labels": [], "entities": []}, {"text": "We use the edit distance between sentences as a feature.", "labels": [], "entities": []}, {"text": "For that we used the Excitement Open Platform (EOP) ().", "labels": [], "entities": []}, {"text": "To obtain the edit distance, we use EDITS Entailment Decision Algorithm (EDITS EDA) taking the edit distance instead of entailment or not entailment decision.", "labels": [], "entities": [{"text": "EDITS Entailment Decision Algorithm (EDITS EDA", "start_pos": 36, "end_pos": 82, "type": "METRIC", "confidence": 0.5607328031744275}]}, {"text": "We configure the system to use lemmas and synonyms as identical words to compute sentence 6 http://hltfbk.github.io/Excitement-Open-Platform/ distance, the system normalizes the score on the number of token of the shortest sentence.", "labels": [], "entities": []}, {"text": "We choose this configuration because it returns the best performance evaluated on training and development data.", "labels": [], "entities": []}, {"text": "We speculate to improve paraphrase detection by adding a feature based on polarity given by a sentiment analysis system.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.9797690510749817}, {"text": "sentiment analysis", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8009517788887024}]}, {"text": "We evaluate this feature on all three datasets (training, develpment, and testing).", "labels": [], "entities": []}, {"text": "We use the Sentiment Pipeline of Stanford CoreNLP () to obtain this feature.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.7930524051189423}]}, {"text": "We configure the pipeline for tokenizing, splitting sentence, POS tagging, lemmatization , parsing, named entity recognition (NER) and, of course, sentiment analysis.", "labels": [], "entities": [{"text": "tokenizing", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.9669542908668518}, {"text": "POS tagging", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.8591887950897217}, {"text": "parsing", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.9694218635559082}, {"text": "named entity recognition (NER)", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.7611403713623682}, {"text": "sentiment analysis", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.9590469896793365}]}, {"text": "Despite the deep analysis, most of sentences are classified as either \"positive\", \"negative\" or \"neutral\"; classes \"very positive\" and \"very negative\" are rare.", "labels": [], "entities": []}, {"text": "We decide to use this as a polaritymatching feature (i.e. when both sentences in the pair are classified the same class), so we analyze the distribution of paraphrase and polarity matching on the three datasets, which results are shown in,.", "labels": [], "entities": []}, {"text": "Contrary to our intuition, this feature seems not to be strongly correlated with paraphrasing, in particular, pairs with polarity matching have 2.08% more of probability to be paraphrase in the training dataset, a bit more (3.65%) in the development dataset, but even less (1.76%) in the test dataset.", "labels": [], "entities": []}, {"text": "We also compute the information gain of the feature in the training dataset using WEKA () InfoGainAttributeEval with the default ranker and we obtain a low result, only 0.00107, so we decide to exclude this approach.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.6666944026947021}]}, {"text": "We still think that sentiment analysis could bean useful feature for paraphrase detection, and there would be away to use it properly.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9583649933338165}, {"text": "paraphrase detection", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.9841213524341583}]}, {"text": "To prove that, we try another different approach, instead of using a binary feature, we use three possible values: 0 if the polarity is opposite (\"positive\" and \"negative\"), 0.5 if one or both sentences in the pair are classified as \"neutral\" and 1 if they have the same polarity (both \"positive\" or \"negative\").", "labels": [], "entities": []}, {"text": "We compute the information gain of the feature in the training dataset and obtain a more promising score of 0.01272; this seems to confirm our idea on the sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.9127633273601532}]}, {"text": "Probably a wider range of values (more than just a 3 sub-classes) would possibly obtain better results.", "labels": [], "entities": []}, {"text": "We aim to use a continuous value that describes polarity distance to improve our system performance.", "labels": [], "entities": []}, {"text": "In this section, we describe the dataset, the task baselines and experiments carried on these two tasks.", "labels": [], "entities": []}, {"text": "The dataset () consists of three parts, the training and development datasets (18,000 sentence pairs), the test dataset (972 sentence pairs) for evaluation.", "labels": [], "entities": []}, {"text": "presents the setup and distribution of all datasets used for the experiments.", "labels": [], "entities": []}, {"text": "Each row of data contains six tab-separated columns presenting the Trending_Topic_Name, Sent_1, Sent_2, Label, Sent_1_tag and Sent_2_tag.", "labels": [], "entities": []}, {"text": "The Sent_1 and Sent_2 are two sentences which may not be necessarily full tweets.", "labels": [], "entities": []}, {"text": "The Label column is in a format suchlike \"(1, 4)\", which means among 5 votes from Amazon Mechanical turkers only 1 is positive and 4 are negative.", "labels": [], "entities": []}, {"text": "The mapping suggestions to binary labels are as follows: -paraphrases: (3, 2) (4, 1) (5, 0) -non-paraphrases: (1, 4) (0, 5) -debatable: (2, 3) which maybe discarded.", "labels": [], "entities": []}, {"text": "The Sent1_tag and Sent2_tag are the two sentences with part-of-speech and named entity tags.", "labels": [], "entities": []}, {"text": "However, there is no labels of semantic similarity scores provided in development and training data, but only evaluation data.", "labels": [], "entities": []}, {"text": "In this section, we discuss about the evaluation on both tasks.", "labels": [], "entities": []}, {"text": "shows the performance of our best models constructed by best sets of features in comparison with all the three baselines and the top three best systems reported in the shared-task.", "labels": [], "entities": []}, {"text": "For Paraphrase Identification task, our system outperforms all three baselines and achieves a very competitive result to the best systems.", "labels": [], "entities": [{"text": "Paraphrase Identification task", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.9220832586288452}]}, {"text": "The difference between our system and the best three systems is a very small variance by a slim margin around 1%.", "labels": [], "entities": []}, {"text": "In Semantic Similarity, though we only build simple model which averages the values of word alignment METEOR, BLEU and Edit Distance scores, our system still obtains better results than all three baselines and close to the top 9 http://alt.qcri.org/semeval2015/task1/data/uploads/semevalpit-2015-results.pdf three results.", "labels": [], "entities": [{"text": "Semantic Similarity", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8099260628223419}, {"text": "METEOR", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.8078580498695374}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9983137845993042}, {"text": "Edit Distance scores", "start_pos": 119, "end_pos": 139, "type": "METRIC", "confidence": 0.9437527656555176}]}, {"text": "These results on both tasks may place us at the 4 th rank in comparison to the official ranking of the shared-task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Paraphrase Identification Accuracy (%) obtained using different classifiers with different features on Develop- ment data.", "labels": [], "entities": [{"text": "Paraphrase Identification Accuracy", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.524064858754476}, {"text": "Develop- ment data", "start_pos": 113, "end_pos": 131, "type": "DATASET", "confidence": 0.7704026997089386}]}, {"text": " Table 2: Distribution of the paraphrase in training dataset  without sentiment analysis and with polarity matching and  mismatching.", "labels": [], "entities": []}, {"text": " Table 3: Distribution of the paraphrase in development  dataset without sentiment analysis and with polarity  matching and mismatching.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7532369196414948}]}, {"text": " Table 4: Distribution of the paraphrase in test dataset  without sentiment analysis and with polarity matching and  mismatching.", "labels": [], "entities": []}, {"text": " Table 5: Paraphrase Identification F1-score obtained using  different classifiers on the best set of features (word/n- gram overlap + METEOR + BLEU + EditDistance).", "labels": [], "entities": [{"text": "Paraphrase Identification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7571803629398346}, {"text": "F1-score", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.5901885032653809}, {"text": "METEOR", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.958590030670166}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.8498979806900024}, {"text": "EditDistance", "start_pos": 151, "end_pos": 163, "type": "METRIC", "confidence": 0.6440938711166382}]}, {"text": " Table 7: Semantic Similarity Results with different features on Test data.", "labels": [], "entities": [{"text": "Semantic Similarity", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7133243978023529}, {"text": "Test data", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.8077869415283203}]}, {"text": " Table 6: Distribution of Datasets.", "labels": [], "entities": [{"text": "Distribution of", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9020463526248932}]}, {"text": " Table 8: Paraphrase Identification (PI) and Semantic Simi- larity (SS) Evaluation Results on Test data.", "labels": [], "entities": [{"text": "Semantic Simi- larity (SS) Evaluation", "start_pos": 45, "end_pos": 82, "type": "METRIC", "confidence": 0.7057673595845699}]}, {"text": " Table 9: Error Analysis on Paraphrase Identification.", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8121554255485535}, {"text": "Paraphrase Identification", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.8576555252075195}]}]}