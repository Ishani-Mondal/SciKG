{"title": [{"text": "Human-Machine Dialogue as a Stochastic Game", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, an original framework to model human-machine spoken dialogues is proposed to deal with co-adaptation between users and Spoken Dialogue Systems in non-cooperative tasks.", "labels": [], "entities": []}, {"text": "The conversation is modeled as a Stochastic Game: both the user and the system have their own preferences but have to come up with an agreement to solve a non-cooperative task.", "labels": [], "entities": []}, {"text": "They are jointly trained so the Dialogue Manager learns the optimal strategy against the best possible user.", "labels": [], "entities": []}, {"text": "Results obtained by simulation show that non-trivial strategies are learned and that this framework is suitable for dialogue modeling.", "labels": [], "entities": [{"text": "dialogue modeling", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.9314987659454346}]}], "introductionContent": [{"text": "Ina Spoken Dialogue System (SDS), the Dialogue Manager (DM) is designed in order to implement a decision-making process (called strategy or policy) aiming at choosing the system interaction moves.", "labels": [], "entities": []}, {"text": "The decision is taken according to the current interaction context which can rely on bad transcriptions and misunderstandings due to Automatic Speech Recognition (ASR) and Spoken Language Understanding (SLU) errors.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 133, "end_pos": 167, "type": "TASK", "confidence": 0.6846827367941538}]}, {"text": "Machine learning methods, such as Reinforcement Learning (RL), are now very popular to learn optimal dialogue policies under noisy conditions and inter-user variability.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6400495707988739}]}, {"text": "In this framework, the dialogue task is modeled as a (Partially Observable) Markov Decision Process ((PO)MDP), and the DM is an RL-agent learning an optimal policy.", "labels": [], "entities": []}, {"text": "Yet, despite some rare examples, RL-based DMs only consider task-oriented dialogues and stationary (non-adapting) users.", "labels": [], "entities": [{"text": "RL-based DMs", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9155944883823395}]}, {"text": "Unfortunately, (PO)MDP are restricted to model game-against-nature problems.", "labels": [], "entities": []}, {"text": "These are problems in which the learning agent evolves in an environment that doesn't change with time and acts in a totally disinterested manner.", "labels": [], "entities": []}, {"text": "(PO)MDP-based dialogue modeling thus applies only if 1) the user doesn't modify his/her behavior along time (the strategy is learned fora stationary environment) and 2) the dialogue is task-oriented and requires the user and the machine to positively collaborate to achieve the user's goal.", "labels": [], "entities": [{"text": "MDP-based dialogue modeling", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6177045305569967}]}, {"text": "The first assumption doesn't hold if the user adapts his/her behavior to the continuously improving performance of a learning DM.", "labels": [], "entities": []}, {"text": "Some recent studies have tried to model this co-adaptation effect between a learning machine and a human () but this approach still considers the user and the machine as independent learning agents.", "labels": [], "entities": []}, {"text": "Although there has already been some few attempts to model the \"coevolution\" of human machine interfaces), this work doesn't extend to RL-based interfaces (automatically learning) and is not related to SDS.", "labels": [], "entities": []}, {"text": "More challenging situations do also arise when the common-goal assumption doesn't hold either, which is the casein many interesting applications such as negotiation), serious games, e-learning, robotic co-workers etc.", "labels": [], "entities": [{"text": "negotiation", "start_pos": 153, "end_pos": 164, "type": "TASK", "confidence": 0.9710638523101807}]}, {"text": "Especially, adapting the MDP paradigm to the case of negotiation dialogues has been the topic of recent works.", "labels": [], "entities": [{"text": "negotiation dialogues", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8235087096691132}]}, {"text": "In (), the authors model the problem of negotiation as a Multi-Agent Reinforcement problem.", "labels": [], "entities": []}, {"text": "Yet, this approach relies on algorithms that are treat-ing the multi-player issue as a non-stationnarity problem (e.g. WoLF-PHC ().", "labels": [], "entities": []}, {"text": "Each agent is assumed to keep a stable interaction policy fora time sufficiently long so that the other agent can learn it's current policy.", "labels": [], "entities": []}, {"text": "Otherwise, there is no convergence guarantees.", "labels": [], "entities": [{"text": "convergence", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9473130702972412}]}, {"text": "Another major issue with these works is that noise in the ASR or NLU results is not taken into account although this is a major reason for using stochastic dialogue models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.7646445631980896}]}, {"text": "In (, the authors follow the same direction by considering both agents as acting in a stationary MDP.", "labels": [], "entities": []}, {"text": "In this paper, we propose a paradigm shift from the now state-of-the-art (PO)MDP model to Stochastic) to model dialogue.", "labels": [], "entities": []}, {"text": "This model extends the MDP paradigm to multi-player interactions and allows learning jointly the strategies of both agents (the user and the DM), which leads to the best system strategy in the face of the optimal user/adversary (in terms of his/her goal).", "labels": [], "entities": []}, {"text": "This paradigm models both co-adaptation and possible non-cooperativness.", "labels": [], "entities": []}, {"text": "Unlike models based on standard game theory, Stochastic Games allow to learn from data.", "labels": [], "entities": []}, {"text": "Especially, departing from recent results, we show that the optimal strategy can be learned from batch data as for MDPs ).", "labels": [], "entities": []}, {"text": "This means that optimal negotiation policies can be learnt from non-optimal logged interactions.", "labels": [], "entities": []}, {"text": "This new paradigm is also very different from MARL methods proposed in previous work) since optimization is jointly performed instead of alternatively optimizing each agent, considering the other can stay stationary fora while.", "labels": [], "entities": []}, {"text": "Although experiments are only concerned with purely adversarial tasks (Zero-Sum games), we show that it could be naturally extended to collaborative tasks (general sum games) (.", "labels": [], "entities": []}, {"text": "Experiments show that an efficient strategy can be learned even under noisy conditions which is suitable for modeling realistic human-machine spoken dialogues.", "labels": [], "entities": []}], "datasetContent": [{"text": "Effects of the multi-agent setting are studied here through one special feature of the human-machine dialogue: the uncertainty management due to the dysfunctions of the ASR and the NLU.", "labels": [], "entities": [{"text": "uncertainty management", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.6338911801576614}]}, {"text": "To promote simple algorithms, we ran our experiments on the zero-sum dialogue game presented above.", "labels": [], "entities": []}, {"text": "On this task, we compare three algorithms: QLearning, WoLF-PHC and AGPI-Q.", "labels": [], "entities": []}, {"text": "Among those algorithms, only AGPI-Q is proved to converge towards a Nash Equilibrium in a Multi-Agent setting.", "labels": [], "entities": [{"text": "AGPI-Q", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.4781583249568939}]}, {"text": "Q-Learning and WoLF-PHC have however been used as Multi-Agent learning algorithm in a dialogue setting).", "labels": [], "entities": []}, {"text": "Similarly to these papers, experiments will be done using simulation.", "labels": [], "entities": []}, {"text": "We will show that, contrarily to AGPI-Q, they do not converge towards the Nash Equilibrium and therefore do not fit to the dialogue problem.", "labels": [], "entities": [{"text": "AGPI-Q", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.7669646739959717}]}], "tableCaptions": []}