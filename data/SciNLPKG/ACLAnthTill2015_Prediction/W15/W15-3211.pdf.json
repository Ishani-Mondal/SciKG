{"title": [{"text": "Best Practices for Crowdsourcing Dialectal Arabic Speech Transcription", "labels": [], "entities": [{"text": "Crowdsourcing Dialectal Arabic Speech Transcription", "start_pos": 19, "end_pos": 70, "type": "TASK", "confidence": 0.7901025712490082}]}], "abstractContent": [{"text": "In this paper, we investigate different approaches in crowdsourcing transcriptions of Dialectal Arabic speech with automatic quality control to ensure good transcription at the source.", "labels": [], "entities": [{"text": "crowdsourcing transcriptions of Dialectal Arabic speech", "start_pos": 54, "end_pos": 109, "type": "TASK", "confidence": 0.717443197965622}]}, {"text": "Since Dialectal Arabic has no standard orthographic representation , it is very challenging to perform quality control.", "labels": [], "entities": []}, {"text": "We propose a complete recipe for speech transcription quality control that includes using output of an Automatic Speech Recognition system.", "labels": [], "entities": [{"text": "speech transcription quality control", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.8661679625511169}]}, {"text": "We evaluated the quality of the transcribed speech and through this recipe, we achieved a reduction in transcription error of 1.0% compared with 13.2% baseline with no quality control for Egyptian data, and down to 4% compared with 7.8% for the North African dialect.", "labels": [], "entities": [{"text": "transcription error", "start_pos": 103, "end_pos": 122, "type": "METRIC", "confidence": 0.8681530058383942}]}], "introductionContent": [{"text": "Crowdsourcing is the process of segmenting a complex task into smaller units of work and distributing them among a large number of nonexpert workers at a lower cost and for less time than professional companies.", "labels": [], "entities": []}, {"text": "The usage of popular crowdsource platforms such as Amazon Mechanical Turk (MTurk) and CrowdFlower (CF) for the acquisition, transcription, and annotation of speech data has been well demonstrated (), among others.", "labels": [], "entities": []}, {"text": "However, using crowdsourcing for the transcription of speech for languages with nonstandard orthographies is less explored, especially with regards the development of quality control protocols in the absence of established writing standards.", "labels": [], "entities": []}, {"text": "Although the writing system of Modern Standard Arabic (MSA) is standardized, the varieties of Dialectal Arabic (DA) are written without standard orthography, typically by utilizing the writing system of MSA.", "labels": [], "entities": []}, {"text": "In this paper, we present best practices for crowdsourcing transcriptions of report and conversational DA and present results of experiments varying automatic quality control parameters that led to the creation of these best practices.", "labels": [], "entities": [{"text": "crowdsourcing transcriptions of report and conversational DA", "start_pos": 45, "end_pos": 105, "type": "TASK", "confidence": 0.553960131747382}]}, {"text": "We show that comparing output from an MSA-based Automatic Speech Recognition (ASR) system trained on a minimal amount of DA to output from a human transcriber outperforms other methods of quality control and results in low rates of data attrition.", "labels": [], "entities": [{"text": "MSA-based Automatic Speech Recognition (ASR)", "start_pos": 38, "end_pos": 82, "type": "TASK", "confidence": 0.7154416484492165}]}, {"text": "We show that utilizing a forgiving edit distance algorithm to compare ASR and user transcripts retains natural variation in orthographic usage without sacrificing quality.", "labels": [], "entities": [{"text": "ASR", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9241406917572021}]}, {"text": "This paper is organized as follows: in Section 2 we discuss issues in crowdsourcing written DA, with particular reference to the usage of nonstandard orthography.", "labels": [], "entities": [{"text": "crowdsourcing written DA", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.5625923871994019}]}, {"text": "Section 3 outlines the utilization of professional transcription for DA and compares it in general terms to the usage of crowdsourcing for the same task.", "labels": [], "entities": [{"text": "DA", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9631584286689758}]}, {"text": "The DA audio data used in this study is described in detail in Section 4.", "labels": [], "entities": [{"text": "DA audio data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.6675261954466502}]}, {"text": "Crowdsourcing experiments are detailed in Section 5, and best practices based on the results of these experiments are presented in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 summarizes our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "To guide our ideas for the development of possible protocols for quality control to test, we first submitted approximately two hours of EGY audio to CF for transcription by users in Egypt over the course of a month and observed what kinds of errors existed in poor quality transcripts in the results.", "labels": [], "entities": []}, {"text": "Examples of poor quality transcripts are shown in.", "labels": [], "entities": []}, {"text": "After development of potential quality control methods (covered in detail in subsection 5.1), we ran experiments to test their efficiency.", "labels": [], "entities": []}, {"text": "To de-: Types of poor transcription termine the highest performing protocol for quality control, we sampled 100 new audio segments of the EGY data described in Section 4 and submitted them to CF for transcription by users in Egypt.", "labels": [], "entities": [{"text": "EGY data", "start_pos": 138, "end_pos": 146, "type": "DATASET", "confidence": 0.9020281136035919}, {"text": "CF", "start_pos": 192, "end_pos": 194, "type": "DATASET", "confidence": 0.9018946886062622}]}, {"text": "The 100 segments were submitted eight times: once for both High Quality and High Speed users for each of the four conditions described in the following section.", "labels": [], "entities": []}, {"text": "For each segment, five separate transcripts were collected from five different users.", "labels": [], "entities": []}, {"text": "Users were presented with an audio button which they could press to listen to the audio an unlimited number of times, and a text box for entering the transcript.", "labels": [], "entities": []}, {"text": "Users were directed to write as precisely as possible, to heed the item ID number, and to avoid using nonArabic characters.", "labels": [], "entities": []}, {"text": "Five items were presented per page, and completion of a page resulted in USD .05 compensation.", "labels": [], "entities": []}, {"text": "An example of a single item as viewed by a user is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Percent of low quality transcripts across automatic quality control conditions", "labels": [], "entities": []}, {"text": " Table 4: Percent of low quality transcripts across automatic quality control conditions", "labels": [], "entities": []}]}