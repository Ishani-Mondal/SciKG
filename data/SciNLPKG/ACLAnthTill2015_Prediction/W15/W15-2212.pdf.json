{"title": [{"text": "Maximising spanning subtree scores for parsing tree approximations of semantic dependency digraphs", "labels": [], "entities": [{"text": "Maximising spanning subtree", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8669074773788452}, {"text": "parsing tree approximations of semantic dependency digraphs", "start_pos": 39, "end_pos": 98, "type": "TASK", "confidence": 0.7740188666752407}]}], "abstractContent": [{"text": "We present a method for finding the best tree approximation parse of a dependency digraph fora given sentence, with respect to a dataset of semantic digraphs as a com-putationally efficient and accurate alternative to DAG parsing.", "labels": [], "entities": [{"text": "DAG parsing", "start_pos": 218, "end_pos": 229, "type": "TASK", "confidence": 0.7634148001670837}]}, {"text": "We present a training algorithm that learns the spanning subtree parses with the highest scores with respect to the data, and consider the output of this algorithm a description of the best tree approximations for digraphs of sentences from similar data.", "labels": [], "entities": []}, {"text": "With the results from this approach, we acquire some important insights on the limits of solely data-driven tree approximation approaches to semantic dependency DAG parsing, and their rule-based, pre-processed tree approximation counterparts.", "labels": [], "entities": [{"text": "semantic dependency DAG parsing", "start_pos": 141, "end_pos": 172, "type": "TASK", "confidence": 0.642854742705822}]}], "introductionContent": [{"text": "In semantic dependency parsing, the aim is to recover sentence-internal predicate argument relationships; structurally speaking, given a sentence, the objective is to recover the possibly disconnected digraph (which represents the semantic structure of the sentence).", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6512098709742228}]}, {"text": "The sparsity of digraph representations of some semantic dependency digraph datasets (i.e., the fact that the number of edges is linear in the number of vertices), as well as the well-performing first attempts at such tree approximations from,, and Agi\u00b4c suggest that tree approximations for digraph semantic dependency structures area relevant avenue to the development if not the sidestepping of some computationally harder models of directed acyclic graph (DAG) and digraph decoding.", "labels": [], "entities": []}, {"text": "In this paper, we present a simple adaptation of the passive-aggressive perceptron training algorithm used in) to the task of finding the parameters that describe the highest scoring tree approximations of semantic dependency digraphs, given a training corpus.", "labels": [], "entities": []}, {"text": "The key change in the algorithm is to iteratively minimise the error in precision between output spanning subtrees and corresponding training digraph instances, allowing therefore the algorithm to choose best spanning subtree approximations with respect to the dataset, rather than forming tree approximations as a pre-processing step to training as was done by,.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9911465048789978}]}, {"text": "Because we directly adapt the software used by, without increasing any computational complexity, the approach also benefits from a syntactic parsing algorithm optimised both practically and theoretically for efficiency, robustness, and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 236, "end_pos": 244, "type": "METRIC", "confidence": 0.9950680732727051}]}, {"text": "Supposing these natural tree patterns exist in the data, the approach intuitively is very attractive, since it lets natural patterns of the data dictate tree approximations, rather than depending on either the previous knowledge of parser behaviour or anecdotal linguistic knowledge.", "labels": [], "entities": []}, {"text": "Moreover, the approach promises some insight into the nature of the patterns inherent in the semantic digraph data, reflecting, namely, the question of the existence of unique most likely sub-tree structures in the data and why rule-based pre-processing tree approximations work so well for semantic digraph dependency parsing.", "labels": [], "entities": [{"text": "semantic digraph dependency parsing", "start_pos": 291, "end_pos": 326, "type": "TASK", "confidence": 0.6719624400138855}]}], "datasetContent": [{"text": "For our experiments, we used precisely the same data split as in SemEval 2014's task 8 and the original mate parser default parameters for the modified version, with the exception that we increased the number of iterations to 15.", "labels": [], "entities": [{"text": "SemEval 2014's task 8", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.5897037148475647}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "Compared with the preprocessing tree approximations from (), the subgraph score maximisation approach performs quite poorly.", "labels": [], "entities": []}, {"text": "The approach successfully closes the gap between precision and recall, when compared to the pruning approach in), but both precision and recall are relatively low.: Precision, recall and f-score over the three datasets (pack and prune results are from () . An analysis of the pre-processing () versus statistical (this paper) approaches may provide some insight as to why.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9989676475524902}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9906584024429321}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9995146989822388}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9974507689476013}, {"text": "Precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9962127208709717}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9985107779502869}, {"text": "f-score", "start_pos": 187, "end_pos": 194, "type": "METRIC", "confidence": 0.964131772518158}]}, {"text": "Especially Agi\u00b4c show that many non-tree-like structures of the SDP data are predictable; that is, we can turn them into trees by consistently pruning edges and \"understand\" that they are in fact more complex than trees, without encoding this information into the resulting pruned tree in anyway.", "labels": [], "entities": []}, {"text": "Ina post-processing step, these edges are simply reintroduced using some rules.", "labels": [], "entities": []}, {"text": "These are edges for which it would be difficult for the passive aggressive algorithm that we employ to choose between, since there are virtually no structures that require only a subset of them.", "labels": [], "entities": []}, {"text": "As a result, making a strict rule about what tree structures should be predicted as a pre-processing step results in better tree approximations than the purely data-driven approach presented here.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision, recall and f-score over the  three datasets (pack and prune results are from  (", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9992627501487732}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9992489218711853}, {"text": "f-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9855405688285828}]}]}