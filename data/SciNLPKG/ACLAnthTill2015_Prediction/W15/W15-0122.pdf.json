{"title": [{"text": "Automatic Noun Compound Interpretation using Deep Neural Networks and Word Embeddings", "labels": [], "entities": [{"text": "Automatic Noun Compound Interpretation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.643292136490345}]}], "abstractContent": [{"text": "The present paper reports on the results of automatic noun compound interpretation for English using a deep neural network classifier and a selection of publicly available word embeddings to represent the individual compound constituents.", "labels": [], "entities": [{"text": "noun compound interpretation", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.6867832740147909}]}, {"text": "The task at hand consists of identifying the semantic relation that holds between the constituents of a compound (e.g. WHOLE+PART_OR_MEMBER_OF in the case of 'robot arm', LOCATION in the case of 'hillside home').", "labels": [], "entities": [{"text": "WHOLE+PART_OR_MEMBER_OF", "start_pos": 119, "end_pos": 142, "type": "METRIC", "confidence": 0.8359489242235819}]}, {"text": "The experiments reported in the present paper use the noun compound dataset described in Tratz (2011), a revised version of the dataset used by Tratz and Hovy (2010) for training their Maximum Entropy classifier.", "labels": [], "entities": []}, {"text": "Our experiments yield results that are comparable to those reported in Tratz and Hovy (2010) in a cross-validation setting, but outperform their system on unseen compounds by a large margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent research in computational semantics has increasingly made use of vector space representations of words in combination with deep neural network classifiers.", "labels": [], "entities": []}, {"text": "This recent trend builds on the earlier successes of such representations and classifiers for morphological and syntactic NLP tasks , and now also includes semantic tasks such as word similarity, word analogy as well as sentiment analysis).", "labels": [], "entities": [{"text": "word analogy", "start_pos": 196, "end_pos": 208, "type": "TASK", "confidence": 0.7805213034152985}, {"text": "sentiment analysis", "start_pos": 220, "end_pos": 238, "type": "TASK", "confidence": 0.9266010522842407}]}, {"text": "The fact that the same type of vector representations can be initially trained for one or more NLP tasks and then be re-used and fine-tuned fora new, seemingly unrelated task suggests that such models can provide a unified architecture for NLP.", "labels": [], "entities": []}, {"text": "The fact that the performance of word embeddings, when combined with deep neural networks, improves in a multi-task learning scenario and can provide state of the art results for NLP further adds to the attractiveness of such methods.", "labels": [], "entities": []}, {"text": "One of the ways to further test the viability of such models and methods is to subject them to a wider range of well-studied NLP tasks and compare the results with previous studies on state-of-the-art NLP datasets.", "labels": [], "entities": []}, {"text": "One such task concerns the automatic interpretation of nominal compounds, a semantic phenomenon that has been widely studied in both theoretical and computational linguistics.", "labels": [], "entities": [{"text": "automatic interpretation of nominal compounds", "start_pos": 27, "end_pos": 72, "type": "TASK", "confidence": 0.7868942856788635}]}, {"text": "This task consists of identifying the semantic relation that holds between the constituents of a compound (e.g. WHOLE+PART_ OR_MEMBER_OF in the case of robot arm, LOCATION in the case of hillside home).", "labels": [], "entities": [{"text": "WHOLE+PART_ OR_MEMBER_OF", "start_pos": 112, "end_pos": 136, "type": "METRIC", "confidence": 0.8416768842273288}]}, {"text": "Given that noun compounding is a highly productive word formation process in many natural languages, the semantic interpretation of compounds constitutes an important task fora variety of NLP tasks including machine translation, information retrieval, question answering, etc.", "labels": [], "entities": [{"text": "noun compounding", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7750474214553833}, {"text": "semantic interpretation of compounds", "start_pos": 105, "end_pos": 141, "type": "TASK", "confidence": 0.808974027633667}, {"text": "machine translation", "start_pos": 208, "end_pos": 227, "type": "TASK", "confidence": 0.7954534292221069}, {"text": "information retrieval", "start_pos": 229, "end_pos": 250, "type": "TASK", "confidence": 0.7621560096740723}, {"text": "question answering", "start_pos": 252, "end_pos": 270, "type": "TASK", "confidence": 0.8673430979251862}]}, {"text": "Due to the productiveness of compounding, an adequate NLP system for the automatic interpretation of compounds will need to be able to generalize well to unseen data, i.e. to compounds that it has not been trained on.", "labels": [], "entities": []}, {"text": "Vector space models that are based on very large training corpora and thus have a good coverage of the lexicon of a language provide a good foundation for achieving such generalization behavior.", "labels": [], "entities": []}, {"text": "Novel compounds are typically formed of existing words in the language that are recombined to form anew complex word, whose meaning is usually more than the sum of the meaning of its constituents, but which are constrained by the combinatory potential of a word.", "labels": [], "entities": []}, {"text": "This combinatory potential, i.e. the tendencies to combine with other words, is exactly what is captured in a vector space model, since such models capture the sum of the contexts that a word typically appears in.", "labels": [], "entities": []}, {"text": "Hence, vector space models and deep neural network classifiers appear to be well suited for experimenting with this task.", "labels": [], "entities": []}, {"text": "Such experiments are facilitated by the availability of a large, annotated dataset of English compounds that is described in; and that was used in machine learning experiments.", "labels": [], "entities": []}, {"text": "The present paper reports on the results of experimenting with the Tratz (2011) dataset using four publicly available word embeddings for English and a deep neural network classifier implemented using the Torch7 scientific computing framework . These experiments yield results that are comparable to those reported by and by Tratz (2011), but outperform their system on unseen compounds by a large margin.", "labels": [], "entities": [{"text": "Tratz (2011) dataset", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.5716541528701782}]}, {"text": "The remainder of this paper is structured as follows: Section 2 presents previous work related to the automatic classification of compound relations.", "labels": [], "entities": [{"text": "automatic classification of compound relations", "start_pos": 102, "end_pos": 148, "type": "TASK", "confidence": 0.697687292098999}]}, {"text": "Sections 3 and 4 present the annotated noun compounds dataset and the four word embeddings that were used in the experiments.", "labels": [], "entities": []}, {"text": "These experiments are summarized in Section 5.", "labels": [], "entities": []}, {"text": "The paper concludes with a summary of the main results and an outlook towards future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments reported in this paper use the noun compound dataset described in . This dataset, subsequently referred to as the Tratz dataset, is a revised version of the data used by in their machine learning experiments, subsequently referred to as the Tratz and Hovy dataset.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 130, "end_pos": 143, "type": "DATASET", "confidence": 0.745321661233902}, {"text": "Hovy dataset", "start_pos": 267, "end_pos": 279, "type": "DATASET", "confidence": 0.8654215335845947}]}, {"text": "The Tratz dataset is the largest publicly-available annotated noun compound dataset, containing 19158 compounds annotated with 37 semantic relations., which is an abbreviated version of.5 in Tratz (2011), illustrates these relations by characteristic examples and indicates the relative frequency of each relation within the dataset as a whole.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8782125115394592}]}, {"text": "The inventory of relations consists of seman-tic categories that resemble but are not identical to the inventories previously proposed by and.", "labels": [], "entities": []}, {"text": "Tratz and Hovy (2010) motivate their new inventory by the necessity to achieve more reliable inter-annotator agreement than was obtained for these earlier inventories.", "labels": [], "entities": []}, {"text": "The original Tratz and Hovy dataset consisted of 17509 compounds annotated with 43 semantic relations.", "labels": [], "entities": [{"text": "Tratz and Hovy dataset", "start_pos": 13, "end_pos": 35, "type": "DATASET", "confidence": 0.5316374599933624}]}, {"text": "Tratz (2011)'s motivation for creating a revised noun compound relation inventory with only 37 semantic relations was to create a better mapping between prepositional paraphrases and noun compound relations.", "labels": [], "entities": []}, {"text": "The compound classification experiments described in were, however, not re-run on the revised dataset.", "labels": [], "entities": [{"text": "compound classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7675304412841797}]}, {"text": "Since only the Tratz dataset is publicly available as part of the semantically-enriched parser described in, this dataset was used in the experiments reported on in the present paper.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.8480816185474396}]}, {"text": "This section summarizes the experiments performed on the Tratz dataset using the four embeddings described in the previous section.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9128243029117584}]}, {"text": "Section 5.1 describes the pre-processing steps that had to be performed on the Tratz dataset in order to make it inter-operable with the embedding dictionaries.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9200277924537659}]}, {"text": "Section 5.2 describes the architecture of the classifier used in all the experiments.", "labels": [], "entities": []}, {"text": "Section 5.3 presents the experiments performed using each of the embeddings individually as well as the best performing system that resulted from the concatenation of three out of the four selected word embeddings.", "labels": [], "entities": []}, {"text": "In order to make the best use of the word embeddings described in the previous section, several preprocessing steps had to be performed.", "labels": [], "entities": []}, {"text": "The Tratz dataset contains about 1% training examples that are person names or titles starting with a capital letter, whereas such names appear in all lowercase in the embedding dictionaries . Therefore all compound constituents of the Tratz dataset were converted to lowercase.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9278213083744049}, {"text": "Tratz dataset", "start_pos": 236, "end_pos": 249, "type": "DATASET", "confidence": 0.9350471198558807}]}, {"text": "This resulted in a constituent dictionary C, |C| = 5242 unique constituents for the entire Tratz dataset of 19158 compound instances.", "labels": [], "entities": [{"text": "Tratz dataset of 19158 compound instances", "start_pos": 91, "end_pos": 132, "type": "DATASET", "confidence": 0.8980699876944224}]}, {"text": "The constituent dictionary C obtained from the Tratz dataset includes complex words such as 'healthcare' that are themselves compounds and which appear in the dataset as parts of larger compounds such as 'health-care legislation'.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9405772984027863}]}, {"text": "Moreover, such complex words are not uniform in their spelling, appearing sometimes as a single word (e.g. 'healthcare'), sometimes hyphenated (e.g. 'health-care') and sometimes as two separate words (e.g. 'health care').", "labels": [], "entities": []}, {"text": "Therefore such spelling variation had to be adapted to the spelling conventions used by each individual embedding.", "labels": [], "entities": []}, {"text": "The same type of adaptation had to be performed in the case of misspelled words in the Tratz dataset and singular/plural forms of the same lemma.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.8731063306331635}]}, {"text": "In cases where a constituent appears in the embedding dictionary as two separate words we use the average of the individual word embeddings as a representation for the constituent.", "labels": [], "entities": []}, {"text": "The Tratz dataset also contains some infrequent English words such as 'chintz' (in 'chintz skirt'), 'fastbreak' (in 'fastbreak layup') or 'secretin' (in 'sham secretin'), which are not part of the embeddings dictionaries.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.933818131685257}]}, {"text": "We used an unknown word embedding for representing such words.", "labels": [], "entities": []}, {"text": "This embedding is already part of the dictionary for some embeddings (e.g. the CW embedding), and was obtained by averaging over the embeddings corresponding to the least frequent 1000 words for the other embeddings.", "labels": [], "entities": []}, {"text": "The pre-processed Tratz dataset was then partitioned into train, dev and test splits containing 13409, 1920 and 3829 noun compounds, respectively.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.8458454608917236}]}, {"text": "The combined train and dev splits were also used to construct a 10-fold cross-validation set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Semantic relation inventory used by the Tratz dataset -abbreviated version of Table 4.5 from  Tratz (2011). Note that some relations have a slightly different name in the actual dataset than the  aforementioned table; this table lists the relation names as found in the dataset.", "labels": [], "entities": [{"text": "Tratz dataset", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.860592395067215}]}, {"text": " Table 2: Overview of embedding sizes, dictionary sizes, training data sizes and support corpora for the  four selected embeddings. The training data size is reported in billions of words.", "labels": [], "entities": []}, {"text": " Table 3. All the results in this table  represent micro-averaged F1 measures 7 .", "labels": [], "entities": [{"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9664040207862854}]}, {"text": " Table 3: Results for the task of automatic classification of noun compounds, using the same embeddings  with two different architectures: (i) with fine-tuning (F) and (ii) without fine-tuning (NO-F)", "labels": [], "entities": [{"text": "automatic classification of noun compounds", "start_pos": 34, "end_pos": 76, "type": "TASK", "confidence": 0.7769669651985168}]}]}