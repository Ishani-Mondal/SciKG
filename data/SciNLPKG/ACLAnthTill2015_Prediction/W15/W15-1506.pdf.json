{"title": [{"text": "Relation Extraction: Perspective from Convolutional Neural Networks", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9756743013858795}, {"text": "Perspective from Convolutional Neural Networks", "start_pos": 21, "end_pos": 67, "type": "TASK", "confidence": 0.6024751245975495}]}], "abstractContent": [{"text": "Up to now, relation extraction systems have made extensive use of features generated by linguistic analysis modules.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.944991797208786}]}, {"text": "Errors in these features lead to errors of relation detection and classification.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.862104058265686}]}, {"text": "In this work, we depart from these traditional approaches with complicated feature engineering by introducing a convolu-tional neural network for relation extraction that automatically learns features from sentences and minimizes the dependence on external toolkits and resources.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.8020626902580261}]}, {"text": "Our model takes advantages of multiple window sizes for filters and pre-trained word embeddings as an initializer on a non-static architecture to improve the performance.", "labels": [], "entities": []}, {"text": "We emphasize the relation extraction problem with an unbalanced corpus.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8981218039989471}]}, {"text": "The experimental results show that our system significantly outperforms not only the best baseline systems for relation extraction but also the state-of-the-art systems for relation classification.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.9127219319343567}, {"text": "relation classification", "start_pos": 173, "end_pos": 196, "type": "TASK", "confidence": 0.8726007342338562}]}], "introductionContent": [{"text": "Learning to extract semantic relations between entity pairs from text plays a vital role in information extraction, knowledge base population and question answering, to name a few.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.8104448318481445}, {"text": "question answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.9037701189517975}]}, {"text": "The relation extraction (RE) task can be divided into two steps: detecting if a relation utterance corresponding to some entity mention pair of interest in the same sentence represents some relation and classifying the detected relation mentions into some predefined classes.", "labels": [], "entities": [{"text": "relation extraction (RE)", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8516791582107544}]}, {"text": "If we only need to categorize the given relation mentions that are known to express some expected relation (perfect detection), we are left with the relation classification (RC) task.", "labels": [], "entities": [{"text": "relation classification (RC)", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.7978565692901611}]}, {"text": "One variation of relation classification is that one might have non-relation examples in the dataset but the number of those is comparable to the number of the other examples.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.8948356211185455}]}, {"text": "The non-relation examples, therefore, can be treated as a usual relation class.", "labels": [], "entities": []}, {"text": "Relation extraction, on the other hand, often comes with a tremendously unbalanced dataset where the number of the non-relation examples far exceeds the others, making relation extraction more challenging but more practical than relation classification.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9591855704784393}, {"text": "relation extraction", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.9010783433914185}, {"text": "relation classification", "start_pos": 229, "end_pos": 252, "type": "TASK", "confidence": 0.9326283931732178}]}, {"text": "Our present work focuses on the relation extraction task with an unbalanced corpus.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.9012150168418884}]}, {"text": "In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method and the kernel-based method (;).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.9168494343757629}]}, {"text": "The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt).", "labels": [], "entities": []}, {"text": "The linguistic analysis pipeline which is hand-designed itself includes tokenization, part of speech tagging, chunking, name tagging as well as parsing, often performed by existing natural language processing (NLP) modules.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.7421660125255585}, {"text": "name tagging", "start_pos": 120, "end_pos": 132, "type": "TASK", "confidence": 0.7374778091907501}]}, {"text": "While these methods allow the RE systems to inherit the knowledge discovered by the NLP community for the pre-processing tasks, they might be subject to the error propagation introduced by the imperfect quality of the supervised NLP toolkits.", "labels": [], "entities": []}, {"text": "For instance, all the tasks mentioned in the pipeline above are known to suffer from a performance loss when they are applied to out-of-domain data, causing the collapse of the RE systems based on them.", "labels": [], "entities": []}, {"text": "In this paper, we target an independent RE system that both avoids complicated feature engineering and minimizes the reliance on the supervised NLP modules for features, potentially alleviating the error propagation and advancing our performance in this area.", "labels": [], "entities": []}, {"text": "To be concrete, our relation extraction system is provided only with raw sentences marked with the positions of the two entities of interest 1 . The only elements we can derive from this structure are the words, the n-grams and their positions in the sentences, suggesting a paradigm in which relation mentions are represented by features that depend on these elements.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7799001038074493}]}, {"text": "Eventually, word embeddings that are capable of capturing latent semantic and syntactic properties of words () and convolutional neural networks (CNNs) that are able to recognize specific classes of n-gram and induce more abstract representations () area natural combination one should apply to obtain more effective representations for RE in this setting.", "labels": [], "entities": [{"text": "RE", "start_pos": 337, "end_pos": 339, "type": "TASK", "confidence": 0.9036901593208313}]}, {"text": "Convolutional neural networks (dating back to the 1980s) area type of feed-forward artificial neural networks whose layers are formed by a convolution operation followed by a pooling operation (.", "labels": [], "entities": []}, {"text": "Recently, with the emerging interests of the community in deep learning, CNNs have been revived and effectively applied in various NLP tasks, including semantic parsing (), search query retrieval (), sentence modeling and clas-sification (, name tagging and semantic role labeling).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.7407041043043137}, {"text": "search query retrieval", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.6746009389559428}, {"text": "sentence modeling", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.8133853673934937}, {"text": "name tagging", "start_pos": 241, "end_pos": 253, "type": "TASK", "confidence": 0.7815155982971191}, {"text": "semantic role labeling", "start_pos": 258, "end_pos": 280, "type": "TASK", "confidence": 0.6581200261910757}]}, {"text": "For relation classification and extraction, there are two very recent works on CNNs for relation classification () 2 and (; however, to the best of our knowledge, there has been no work on employing CNNs for relation extraction so far.", "labels": [], "entities": [{"text": "relation classification and extraction", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8306858092546463}, {"text": "relation classification", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.8492827117443085}, {"text": "relation extraction", "start_pos": 208, "end_pos": 227, "type": "TASK", "confidence": 0.8348935842514038}]}, {"text": "This paper is the first attempt to fill in that gap and serves as a baseline for future research in this area.", "labels": [], "entities": []}, {"text": "Our convolutional neural network is built upon that of and which are originally proposed for sentence classification and modeling.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.7799564003944397}]}, {"text": "We adapt the network for relation extraction by introducing the position embeddings to encode the relative distances of the words in the sentence to the two entities of interest.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.9521040618419647}]}, {"text": "Compared to the models in and for relation classification that apply a single window size, our model for relation extraction incorporates various window sizes for convolutional filters, allowing the network to capture wider ranges of n-grams to be helpful for relation extraction.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.893324464559555}, {"text": "relation extraction", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8094950616359711}, {"text": "relation extraction", "start_pos": 260, "end_pos": 279, "type": "TASK", "confidence": 0.9229371547698975}]}, {"text": "In addition, rather than initializing the word embeddings randomly as do and fixing the randomly generated position embeddings during training as do, we use pretrained word embeddings for initialization and optimize both word embeddings and position embeddings as model parameters.", "labels": [], "entities": []}, {"text": "More importantly, rather than using exterior features (either from human annotation or other pre-processing modules) to enrich the representation as do and, our model (adapted for RC where entity heads are given) avoids usage of manual linguistic resources and supervised NLP toolkits constructed externally, utilizing word embeddings that can be trained automatically in an unsupervised framework as the only external resource for the whole system.", "labels": [], "entities": []}, {"text": "We explore different model architectures systematically and demonstrate that the best model performance is achieved when multiple window sizes are implemented and the word embeddings, once initialized by some \"universal\" embeddings, are allowed to vary during the optimization process to reach an effective state for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 317, "end_pos": 336, "type": "TASK", "confidence": 0.8239750564098358}]}, {"text": "We evaluate our models on both relation classification and relation extraction tasks.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.8892289996147156}, {"text": "relation extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8459121584892273}]}, {"text": "For relation classification, experiments show that our model (without any external features and resources) outperforms the stateof-the-art models whether the external features are included in these models or not.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9540188908576965}]}, {"text": "For relation extraction, our model is significantly better than the baseline models that use the words and the embeddings themselves as the features.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.962075263261795}]}, {"text": "In the following, we discuss related work in Section 2 and present our model in Section 3.", "labels": [], "entities": []}, {"text": "We detail an extensive evaluation in Section 4 and finally conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on two datasets: the SemEval-2010 Task 8 dataset ( The SemEval dataset can be downloaded here 6 and contains 10,717 annotated examples, including 8,000 examples for training and 2,717 examples for testing.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 dataset", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.7090182900428772}, {"text": "SemEval dataset", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.7912797033786774}]}, {"text": "Each example is a sentence annotated fora pair of entities of interest and the corresponding relation class for this entity pair.", "labels": [], "entities": []}, {"text": "There are 9 ordered relationships (with two directions) and an undirected Other class, resulting in 19 classes.", "labels": [], "entities": []}, {"text": "A pair is counted as correct if the order of the entities in the relationship is correct.", "labels": [], "entities": []}, {"text": "For the ACE 2005 dataset, documents are annotated for 6 major relation classes and 7 entity types.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.96278315782547}]}, {"text": "In order to generate the non-relation examples or the examples for the Other class, we collect every pair of entity mentions within a single sentence and not included in the annotated relation set.", "labels": [], "entities": []}, {"text": "To reduce the noise, we truncate the generated dataset by removing all the examples whose distances between the two entity heads are greater than 15.", "labels": [], "entities": []}, {"text": "This results in a considerably unbalanced dataset of 8,365 positive examples of the 6 annotated relation classes and 79,147 negative examples of the class Other.", "labels": [], "entities": []}, {"text": "The distributions of the relation classes on the two datasets are shown in.", "labels": [], "entities": []}, {"text": "As we can see, the ACE dataset is much more biased toward the Other class than the SemEval dataset and thus more appropriate for relation extraction experiments.", "labels": [], "entities": [{"text": "ACE dataset", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.8928425014019012}, {"text": "SemEval dataset", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.7223928868770599}, {"text": "relation extraction", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.8439854681491852}]}, {"text": "model on window sizes of 2, 3, 4 and 5.", "labels": [], "entities": []}, {"text": "To understand the behavior of the model on multiple window sizes, we further test it on the following window size combinations: (4,5), (3,4,5) and (2,3,4,5).", "labels": [], "entities": []}, {"text": "These experiments are carried out for relation extraction on the ACE 2005 dataset via 5-fold cross validation.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.9230647683143616}, {"text": "ACE 2005 dataset", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9886907736460367}]}, {"text": "presents the system performance on Precision (P), Recall (R) and F1 score (F).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.9515044391155243}, {"text": "Recall (R)", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9635278582572937}, {"text": "F1 score (F)", "start_pos": 65, "end_pos": 77, "type": "METRIC", "confidence": 0.9796315908432007}]}, {"text": "The key observations from the table are 7 : (i) From rows 1, 2, 3, 4, we see that evaluating window sizes individually is quite intricate.", "labels": [], "entities": []}, {"text": "It is unclear which window size is the best size for CNNs on relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8625246584415436}]}, {"text": "For instance, on the nonstatic.rand mode, the window size 4 seems to outperform the others while on the other modes, the window sizes 3 and 5 turnout to be better.", "labels": [], "entities": []}, {"text": "Besides, the performance gaps between the window sizes are small, making it hard to draw a conclusive judgement.", "labels": [], "entities": []}, {"text": "In any case, the window size 2 seems to be the worst, suggesting that the 2-grams might be less informative than the others on representing relation mentions for CNNs on this dataset.", "labels": [], "entities": []}, {"text": "(ii) While the results on evaluating single window sizes are hard to analyze, the results for multiple window sizes are quite clear and conclusive.", "labels": [], "entities": []}, {"text": "Moving from single window sizes of 2, 3, 4 or 5 (rows 1, 2, 3 and 4 respectively) to the configuration with two window sizes 4 and 5 (row 5) gives us consistent improvements on all the model architectures.", "labels": [], "entities": []}, {"text": "The performance is then consistently enhanced when more window sizes are included, resulting in the best performance when all the window sizes 2, 3, 4 and 5 are employed.", "labels": [], "entities": []}, {"text": "This demonstrates the advantages of the models with multiple window sizes over the single window size models in and.", "labels": [], "entities": []}, {"text": "(iii) Regarding different model architectures, the picture is even clearer.", "labels": [], "entities": []}, {"text": "No matter which window size configuration is applied, we constantly seethe nonstatic.word2vec architecture performs most effectively, followed by the static.word2vect setting which is in turn followed by the nonstatic.rand model.", "labels": [], "entities": []}, {"text": "This suggests the undeniable benefits of initializing the word embeddings by some \"universal\" pre-trained values and updating the embeddings to reflex RE specific embeddings when training the models).", "labels": [], "entities": []}, {"text": "For the next experiments, we always use all the window sizes 2, 3, 4 and 5 with the nonstatic.word2vec architecture.", "labels": [], "entities": []}, {"text": "We compare our system with the traditional featurebased relation extraction systems when these system are only allowed to use the same information and resources as our systems, i.e, the words in the relation mentions, the positions of the two entity heads and the word embeddings.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7467012405395508}]}, {"text": "Given the sentences and the positions of the two entity heads, the features that the state-of-the-art feature-based systems extract in-clude: the heads of the two entity mentions; the words in the context before mention 1; after mention 2 and between two mentions; the bigrams, the word sequences between two entities, the order of two mentions, the number of words between two mentions ().", "labels": [], "entities": []}, {"text": "The feature-based system using this feature set is called Words.", "labels": [], "entities": []}, {"text": "Armed with the word embeddings, one can further introduce these embeddings into the head words or the words in the context as additional features).", "labels": [], "entities": []}, {"text": "We call the system Words augmented with the embeddings for the two heads Words-HMWed and Words augmented with the embeddings for words in the contexts Words-WC-Wed.", "labels": [], "entities": []}, {"text": "We apply the MaxEnt framework with L2 regularization in the Mallet toolkit 8 to train these feature-based models (as).", "labels": [], "entities": [{"text": "Mallet toolkit 8", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.9718019564946493}]}, {"text": "The first observation is that adding the word embeddings to the words in the context hurt the performance of the feature-based systems while augmenting the heads of the entities with word embeddings significantly improves the feature-based systems.", "labels": [], "entities": []}, {"text": "This is consistent with the results reported by and demonstrates that the ability to wisely pick the words for embeddings and avoid embeddings on specific locations is crucial to the feature-based systems.", "labels": [], "entities": []}, {"text": "More importantly, our proposed CNN significantly outperforms all the baseline models at the confidence levels \u2265 95%, an improvement of 4.96% over the best feature-based system Words-HM-Wed).", "labels": [], "entities": []}, {"text": "This result indicates that CNNs area better way to employ word embeddings for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8859466314315796}]}, {"text": "Remember that although the traditional systems can achieve a performance greater than 72% on the ACE dataset (, they come at the expense of elaborate feature engineering as well as much more expensive feature extraction.", "labels": [], "entities": [{"text": "ACE dataset", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.9726691544055939}, {"text": "feature extraction", "start_pos": 201, "end_pos": 219, "type": "TASK", "confidence": 0.7131478637456894}]}, {"text": "In particular, the feature extractors of these feature-based systems require: (i) the perfect entity and mention type information hand-labeled laboriously by human annotators; (ii) the extensive usage of the existing supervised NLP toolkits and resources (constituent and dependency parsers, dictionaries, gazetteers etc) which might be unavailable for various domains in reality.", "labels": [], "entities": []}, {"text": "The absence of the perfect (hand-annotated) entity and mention type information (i.e point (i) above) greatly impairs these feature-based systems' performance.", "labels": [], "entities": []}, {"text": "For instance, both and report a performance less than 60% on the ACE 2005 dataset when the perfect entity type and mention type features are not employed although the other features with extensive feature engineering (i.e point (ii) above) are still included.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9803232749303182}]}, {"text": "As a result, in a more realistic setting where hand-annotated features are prohibitive, the proposed CNN requires much less feature engineering and resources but still performs better than the traditional feature-based systems.", "labels": [], "entities": []}, {"text": "In order to further verify the effectiveness of the system, we test the system on the relation classification task with the SemEval 2010 dataset and compare the results with the state-of-the-art systems in this area.", "labels": [], "entities": [{"text": "relation classification task", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.879713753859202}, {"text": "SemEval 2010 dataset", "start_pos": 124, "end_pos": 144, "type": "DATASET", "confidence": 0.8797508478164673}]}, {"text": "More interestingly, even without supervised and manual features, our system can still work comparably to the other systems utilizing these features as the vital components.", "labels": [], "entities": []}, {"text": "For instance, the supervised features (dependency parse and name tagging) are crucial to FCM () to significantly improve its performance.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8052410781383514}, {"text": "name tagging", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.7563797235488892}, {"text": "FCM", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8116343021392822}]}, {"text": "We attribute our performance advantage over the closely-related system O-CNN () to the multiple window sizes, the optimization of the position embeddings during training and possibly the superiority of the embeddings word2vec we use.", "labels": [], "entities": []}, {"text": "Shifting from relation classification to relation extraction with an unbalanced corpus, we witness a large performance gap as described above.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.894976794719696}, {"text": "relation extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8834400475025177}]}, {"text": "In this section, we study the impact of the unbalanced corpus on the performance of relation extractors for both convolutional neural networks and traditional feature-based approaches (Words and Words-HM- Wed).", "labels": [], "entities": []}, {"text": "In particular, we vary the ratio of positive (true relations) and negative (the class Other) examples in the ACE 2005 dataset and see how the system performance responds to this variation.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 109, "end_pos": 125, "type": "DATASET", "confidence": 0.9780717293421427}]}, {"text": "This is a 5-fold cross validation experiment and all the comparisons are significant at confidence levels \u2265 95%.", "labels": [], "entities": []}, {"text": "From the figure, we see that all the models improve constantly with the increase of the ratio of the positive and negative examples.", "labels": [], "entities": []}, {"text": "The performance peaks with an improvement of about 20% for all models when the number of examples of the class Other is small relative to the others.", "labels": [], "entities": []}, {"text": "In other words, the systems attain their best performance when relation extraction is reduced to the relation classification problem, suggesting that relation extraction is much more challenging than relation classification.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8608396351337433}, {"text": "relation classification", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.8554039001464844}, {"text": "relation extraction", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.8938927948474884}, {"text": "relation classification", "start_pos": 200, "end_pos": 223, "type": "TASK", "confidence": 0.9133977890014648}]}, {"text": "Finally, for all the ratio values, we consistently see that the convolutional neural network is superior to the others, once again confirming its advantages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ACE 2005 and SemEval 2010 Relation Class  Distributions", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.8790740668773651}, {"text": "SemEval 2010 Relation Class  Distributions", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.8318998217582703}]}, {"text": " Table 2: System Performance on various window size combinations and architectures", "labels": [], "entities": []}, {"text": " Table 3: Performance of Relation Extraction Systems", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.9365849792957306}]}]}