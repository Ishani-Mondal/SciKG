{"title": [{"text": "A Methodology for Bilingual Lexicon Extraction from Comparable Corpora", "labels": [], "entities": [{"text": "Bilingual Lexicon Extraction from Comparable Corpora", "start_pos": 18, "end_pos": 70, "type": "TASK", "confidence": 0.8530790309111277}]}], "abstractContent": [{"text": "Dictionary extraction using parallel corpora is well established.", "labels": [], "entities": [{"text": "Dictionary extraction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8088176846504211}]}, {"text": "However, for many language pairs parallel corpora area scarce resource which is why in the current work we discuss methods for dictionary extraction from comparable corpora.", "labels": [], "entities": [{"text": "dictionary extraction", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.7201930284500122}]}, {"text": "Hereby the aim is to push the boundaries of current approaches, which typically utilize correlations between co-occurrence patterns across languages, in several ways: 1) Eliminating the need for initial lexicons by using a bootstrapping approach which only requires a few seed translations.", "labels": [], "entities": []}, {"text": "2) Implementing anew approach which first establishes alignments between comparable documents across languages, and then computes cross-ling-ual alignments between words and mul-tiword-units.", "labels": [], "entities": []}, {"text": "3) Improving the quality of computed word translations by applying an interlingua approach, which, by relying on several pivot languages, allows an effective multi-dimensional cross-check.", "labels": [], "entities": [{"text": "computed word translations", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6768826345602671}]}, {"text": "4) We investigate that, by looking at foreign citations, language translations can even be derived from a single monolin-gual text corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "The aim of this paper is to suggest new methods for automatically extracting bilingual dictionaries, i.e. dictionaries listing all possible translations of words and multiword units, from comparable corpora.", "labels": [], "entities": []}, {"text": "With comparable corpora we mean sets of text collections which cover roughly the same subject area in different languages or dialects, but which are not translations of each other.", "labels": [], "entities": []}, {"text": "Their main advantages are that they don't have a translation bias (as there is no source language which could show through) and are available in by far larger quantities and for more domains than parallel corpora, i.e. collections of translated texts.", "labels": [], "entities": []}, {"text": "The systems to be developed are supposed to have virtually no prior knowledge on word translations.", "labels": [], "entities": [{"text": "word translations", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7229886800050735}]}, {"text": "Instead, they induce this knowledge statistically using an extension of Harris' distributional hypothesis to the multilingual case.", "labels": [], "entities": []}, {"text": "The distributional hypothesis states that words occurring in similar contexts have related meanings.", "labels": [], "entities": []}, {"text": "Its application led to excellent automatically created monolingual thesauri of related words.", "labels": [], "entities": []}, {"text": "Our extension of Harris' distributional hypothesis to the multilingual case claims that the translations of words with related meanings will also have related meanings.", "labels": [], "entities": []}, {"text": "From this it can be inferred that if two words co-occur more frequently than expected in a corpus of one language, then their translations into another language will also co-occur more frequently than expected in a comparable corpus of this other language.", "labels": [], "entities": []}, {"text": "This is the primary statistical clue which is the basis for our work.", "labels": [], "entities": []}, {"text": "Starting from this our aim is to develop a methodology which is capable of deriving good quality bilingual dictionaries in a language independent fashion, i.e. which can be applied to all language pairs where comparable corpora are available.", "labels": [], "entities": []}, {"text": "In future work, to exemplify the results achieved with this method, we will generate large dictionaries comprising single words and multiword units for the following language pairs: English-German; English-French; English-Spanish; German-French; German-Spanish; French-Spanish, German-Dutch, and Spanish-Dutch.", "labels": [], "entities": []}, {"text": "Bilingual dictionaries are an indispensable resource for both human and machine translation.", "labels": [], "entities": [{"text": "human and machine translation", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.7340833097696304}]}, {"text": "For this reason, in the field of lexicography a lot of effort has been put into producing high quality dictionaries.", "labels": [], "entities": []}, {"text": "For example, in rule-based machine translation producing the dictionaries is usually by far the most time consuming and expensive part of system development.", "labels": [], "entities": [{"text": "rule-based machine translation producing the dictionaries", "start_pos": 16, "end_pos": 73, "type": "TASK", "confidence": 0.7585270206133524}]}, {"text": "But as the dictionaries are crucial in ensuring high coverage and high translation quality, a lot of effort has to be invested into them, and there are many examples where the manual creation of comprehensive dictionaries has been an ongoing process over several decades.", "labels": [], "entities": []}, {"text": "Now that some high quality dictionaries exist, why do we suggest further research in this field?", "labels": [], "entities": []}, {"text": "The reasons are manifold: 1) High quality dictionaries are only available fora few hundred common language pairs, usually involving some of the major European and Asian languages.", "labels": [], "entities": []}, {"text": "But there exist about 7000 languages worldwide), of which 600 have a written form.", "labels": [], "entities": []}, {"text": "In the interest of speakers or learners of lesser used languages, at least for all possible pairs of written languages high quality dictionaries would be desirable, which means a total of 600 * (600 -1) = 359,400 translation directions.", "labels": [], "entities": []}, {"text": "But in practice this is impossible for reasons of time, effort, and cost.", "labels": [], "entities": []}, {"text": "So the companies working in the field tend to concentrate on their major markets only.", "labels": [], "entities": []}, {"text": "2) The usage and meanings of words are adapted and modified in language of specialized domains and genres.", "labels": [], "entities": []}, {"text": "To give an example, the word memory is used differently in the life sciences and in computer science.", "labels": [], "entities": []}, {"text": "This means that in principle for each domain specific dictionaries would be desirable.", "labels": [], "entities": []}, {"text": "Again, fora few common language pairs and commercially important subject areas such as medicine or engineering such dictionaries have been developed.", "labels": [], "entities": []}, {"text": "But if we (conservatively) assumed only 20 subject areas, the total number of required dictionaries increases from 359,400 to 143,988,000.", "labels": [], "entities": []}, {"text": "3) Languages evolve overtime.", "labels": [], "entities": []}, {"text": "New topics and disciplines require the creation or borrowing (e.g. from English) of new terms (a good example is mobile computing), other terms become obsolete.", "labels": [], "entities": []}, {"text": "This means that we cannot create our dictionaries once and forever, but need to constantly track these changes, for all language pairs, and for all subject areas.", "labels": [], "entities": []}, {"text": "4) Even if some companies such as specialized publishing houses (e.g. Collins and Oxford University Press), translation companies (e.g. Systran and SDL) or global players (e.g. Google and Microsoft) can afford to compile dictionaries for some markets, these dictionaries are proprietary and often not available for other companies, institutions, academia, and individuals.", "labels": [], "entities": [{"text": "Collins and Oxford University Press", "start_pos": 70, "end_pos": 105, "type": "DATASET", "confidence": 0.8460256576538085}]}, {"text": "This is an obstacle for the advancement of the field.", "labels": [], "entities": []}, {"text": "Given this situation, it would be desirable to be able to generate dictionaries ad hoc as we need them from corpora of the text types we are interested in.", "labels": [], "entities": []}, {"text": "So a lot of thought has been spent on how to produce bilingual dictionaries more efficiently than manually in the traditional lexicographic way.", "labels": [], "entities": []}, {"text": "From these efforts, two major straits of research arose: The first is based on the exploitation of parallel corpora, i.e. collections of translated documents, as suggested by) in their seminal papers.", "labels": [], "entities": []}, {"text": "They automatically extracted a bilingual dictionary from a large parallel corpus of English-French Canadian parliamentary proceedings, and then built a machine translation system around this.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7084826827049255}]}, {"text": "The development of such systems has not been without setbacks, but finally, after 15 years of research, it led to a revolution in machine translation technology and provided the basis for machine translation systems such as Moses, Google Translate and Microsoft's Bing Translator which are used by millions of people worldwide everyday.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7865796685218811}, {"text": "machine translation", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.7027428448200226}]}, {"text": "The second strait of research is based on comparable rather than parallel corpora.", "labels": [], "entities": []}, {"text": "It was first suggested by and.", "labels": [], "entities": []}, {"text": "The motivation was that parallel corpora area scarce resource for most language pairs and subject areas, and that human performance in second language acquisition and in translation shows that there must be away of crossing the language barrier that does not require the reception of large amounts of translated texts.", "labels": [], "entities": [{"text": "second language acquisition", "start_pos": 135, "end_pos": 162, "type": "TASK", "confidence": 0.6226422190666199}]}, {"text": "We suggest hereto replace parallel by comparable corpora.", "labels": [], "entities": []}, {"text": "Comparable (written or spoken) corpora are far more abundant than parallel corpora, thus offering the chance to overcome the data acquisition bottleneck.", "labels": [], "entities": []}, {"text": "This is particularly true as, given n languages to be considered, n comparable corpora will suffice.", "labels": [], "entities": []}, {"text": "In contrast, with parallel corpora, unless translations of the same text are available in several languages, the number of required corpora c increases quadratically with the number of languages as c = (n 2 -n)/2.", "labels": [], "entities": []}, {"text": "However, the problem with comparable corpora is that it is much harder to extract a bilingual dictionary from comparable corpora than from parallel corpora.", "labels": [], "entities": []}, {"text": "As a consequence, despite intensive research carried out over two decades (to a good part taking place in international projects such as AC-CURAT, HyghTra, PRESEMT, METIS, Kelly, and TTC) no commercial breakthrough has yet been possible.", "labels": [], "entities": [{"text": "HyghTra", "start_pos": 147, "end_pos": 154, "type": "DATASET", "confidence": 0.8585999011993408}, {"text": "TTC", "start_pos": 183, "end_pos": 186, "type": "DATASET", "confidence": 0.7954915761947632}]}, {"text": "However, we feel that in recent years some remarkable improvements were suggested (e.g. dictionary extraction from aligned comparable documents and dictionary verification using cross checks based on pivot languages).", "labels": [], "entities": [{"text": "dictionary extraction from aligned comparable documents", "start_pos": 88, "end_pos": 143, "type": "TASK", "confidence": 0.8663452963034312}, {"text": "dictionary verification", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.7757870852947235}]}, {"text": "They cannot solve the problem when used in isolation, but when amended and combined they may well have the potential to lead to substantial improvements.", "labels": [], "entities": []}, {"text": "In this paper we try to come up with a roadmap for this.", "labels": [], "entities": []}], "datasetContent": [{"text": "As previous evaluations of the dictionary extraction task were usually conducted with ad hoc test sets and thus were not comparable, noted an urgent need for standard test sets.", "labels": [], "entities": [{"text": "dictionary extraction task", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.8692162235577902}]}, {"text": "In response to this, we intend to workout and publish a gold standard which covers all of our eight language pairs and will ensure that words of a wide range of frequencies are appropriately represented.", "labels": [], "entities": []}, {"text": "All results on single words are to be evaluated using this test set.", "labels": [], "entities": []}, {"text": "Little work has been done so far on multiword dictionary extraction using comparable corpora (an exception is, and no widely accepted gold standard exists.", "labels": [], "entities": [{"text": "multiword dictionary extraction", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7834522326787313}]}, {"text": "A problem is that there are many ways how to define multiword units.", "labels": [], "entities": []}, {"text": "To explore these and to provide for different needs, we aim for five types of test sets of at least 5000 multiword units and their translations.", "labels": [], "entities": []}, {"text": "The test sets are to be generated semi-automatically in the following ways: a) Multiword units connected by Wikipedia interlanguage links.", "labels": [], "entities": []}, {"text": "b) Multiword units extracted from a parallel corpus which was word-aligned using GIZA++.", "labels": [], "entities": []}, {"text": "c) Multiword units extracted from phrase tables as generated using the Moses toolkit.", "labels": [], "entities": []}, {"text": "d) Multiword units extracted with a co-occurrence based system such as Likely) and redundantly translated with several translation systems, using voting to select translations.", "labels": [], "entities": []}, {"text": "e) Multiword named entities taken from JRCNames (as provided by the European Commission's Joint Research Centre).", "labels": [], "entities": [{"text": "JRCNames", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9120639562606812}, {"text": "European Commission's Joint Research Centre", "start_pos": 68, "end_pos": 111, "type": "DATASET", "confidence": 0.7933186690012614}]}, {"text": "The results on the multiword dictionary extraction task are to be evaluated using each of these gold standards.", "labels": [], "entities": [{"text": "multiword dictionary extraction task", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.7976493239402771}]}], "tableCaptions": []}