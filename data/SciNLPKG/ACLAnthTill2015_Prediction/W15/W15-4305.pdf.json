{"title": [{"text": "A Normalizer for UGC in Brazilian Portuguese", "labels": [], "entities": [{"text": "A Normalizer", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.814330130815506}, {"text": "UGC", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.8664103150367737}]}], "abstractContent": [{"text": "User-generated contents (UGC) represent an important source of information for governments, companies, political candidates and consumers.", "labels": [], "entities": []}, {"text": "However, most of the Natural Language Processing tools and techniques are developed from and for texts of standard language, and UGC is a type of text especially full of creativity and idiosyncrasies, which represents noise for NLP purposes.", "labels": [], "entities": []}, {"text": "This paper presents UGCNormal, a lexicon-based tool for UGC normalization.", "labels": [], "entities": [{"text": "UGC normalization", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.8911547660827637}]}, {"text": "It encompasses a tokenizer, a sentence segmentation tool, a phonetic-based speller and some lexicons, which were originated from a deep analysis of a corpus of product reviews in Brazilian Portuguese.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.730290949344635}]}, {"text": "The normalizer was evaluated in two different data sets and carried out from 31% to 89% of the appropriate corrections, depending on the type of text noise.", "labels": [], "entities": [{"text": "corrections", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.9760154485702515}]}, {"text": "The use of UGCNormal was also validated in a task of POS tagging, which improved from 91.35% to 93.15% inaccuracy and in a task of opinion classification, which improved the average of F1-score measures (F1-score positive and F1-score negative) from 0.736 to 0.758.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8118215203285217}, {"text": "opinion classification", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7274647951126099}, {"text": "F1-score", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9918519854545593}, {"text": "F1-score", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.9527220129966736}, {"text": "F1-score negative)", "start_pos": 226, "end_pos": 244, "type": "METRIC", "confidence": 0.9619563817977905}]}], "introductionContent": [{"text": "The increasing volume of text posted by users on the web is regarded as an extremely useful opportunity to reveal public opinion on many issues.", "labels": [], "entities": []}, {"text": "For a variety of reasons, governments, companies, political candidates, and consumers want to explore such web content.", "labels": [], "entities": []}, {"text": "This type of text is referred to in the literature as UGC (usergenerated content) or EWoM (electronic word-ofmouth).", "labels": [], "entities": []}, {"text": "However, due to the large amount of data available, it is impossible for humans to analyze all available UGC for most issues.", "labels": [], "entities": []}, {"text": "As a result, processing and analyzing UGC became a task of NLP (Natural Language Processing).", "labels": [], "entities": []}, {"text": "The problem is that, until now, almost all NLP tools and techniques were developed from, and for, standard language text, but UGC displays a range of creative and idiosyncratic differences, which represent noise for NLP purposes.", "labels": [], "entities": []}, {"text": "In order to reuse the NLP tools to process UGC, the normalization or standardization of this genre of text became an essential preprocessing step, aiming to make UGC as close as possible to standard language.", "labels": [], "entities": []}, {"text": "The level of noise in UGC varies depending on the social media in which it is posted.", "labels": [], "entities": []}, {"text": "Short messages (SMS and microblogs, such as Twitter) tend to be much noisier than texts posted in blogs and sites of reviews, as users need to be creative to deal with character limitations (140 characters for Twitter and 160 for SMS).", "labels": [], "entities": []}, {"text": "The challenge for NLP is to determine the aspects in which UGC deviates from standard language and develop strategies to deal with the normalization of these aspects.", "labels": [], "entities": []}, {"text": "Many of UGC's deviations from standard language are motivated by wordplay (U=you, 4=for), by the need to save space (short messages have a limited length), by the influence of pronunciation, or even by a low level of literacy.", "labels": [], "entities": []}, {"text": "Regardless of the causes of UGC deviations from standard language, if they are recurrent, they need to be addressed by normalization processes.", "labels": [], "entities": []}, {"text": "Some characteristics of UGC are languageindependent, as the long vowels used to express emphasis (Gooooooooooooood) and the unconventional use of lower and upper cases (proper names in lowercase and common words in uppercase).", "labels": [], "entities": []}, {"text": "Other characteristics are languagedependent, such as the apostrophe suppression in English (wont=won't) and the omission of diacritics and cedilla under \"c\" in Portuguese (eleicao=elei\u00e7\u00e3o).", "labels": [], "entities": []}, {"text": "UGC differs from the standard language mainly in the lexical level.", "labels": [], "entities": [{"text": "UGC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8102322220802307}]}, {"text": "For this reason, the normalization problem is approached by strategies of word correction (the lexical items of the UGC are treated as \"errors\") and strategies for machine translation (the UGC is treated as source language and the standard language as target language).", "labels": [], "entities": [{"text": "normalization", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9628933072090149}, {"text": "word correction", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.75071981549263}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.7724249362945557}]}, {"text": "We address herein the normalization process as a set of procedures that deal with different types of deviation.", "labels": [], "entities": [{"text": "normalization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9821074604988098}]}, {"text": "The input consists of consumer reviews on electronic products.", "labels": [], "entities": []}, {"text": "The main purpose is to convert such texts, as closely as possible, into the form expected by NLP tools trained on corpora of standard language.", "labels": [], "entities": []}, {"text": "This work was preceded by the detection and analysis of out-of-vocabulary 1 (OOV) words in a corpus of product reviews ().", "labels": [], "entities": []}, {"text": "In another preliminary investigation, we have found other different types of deviations and their impact on a tagging task ().", "labels": [], "entities": [{"text": "tagging task", "start_pos": 110, "end_pos": 122, "type": "TASK", "confidence": 0.9099985659122467}]}, {"text": "Such diagnosis has resulted in the procedures that integrate the normalization system proposed here.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related works.", "labels": [], "entities": []}, {"text": "Section 3 describes the characteristics of the product review corpus and the problems they pose to normalization.", "labels": [], "entities": []}, {"text": "Section 4 reports the methodology used to construct the normalization tool.", "labels": [], "entities": [{"text": "normalization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9775387644767761}]}, {"text": "Section 5 describes and discusses the evaluation and validation results.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we make some final remarks and outline future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the normalization tool intrinsically, in two corpus, and extrinsically, in a POS tag task and in an Opinion Classifier.", "labels": [], "entities": [{"text": "normalization", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.9624909162521362}]}, {"text": "In the intrinsic evaluation we used two samples, one from the Buscap\u00e9 corpus, and one from another corpus of the same genre, extracted from the e-commerce website Mercado Livre, which constitutes unseen data.", "labels": [], "entities": [{"text": "Buscap\u00e9 corpus", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9690253138542175}]}, {"text": "In both cases, a sample of 60 product reviews was manually annotated with respect to punctuation errors, case use, and misspellings.", "labels": [], "entities": []}, {"text": "Our two samples (random selection from both corpora) are described in. shows the recall figures of UGCNormal in both samples.", "labels": [], "entities": [{"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9989572763442993}, {"text": "UGCNormal", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.8391873240470886}]}, {"text": "The second and third columns contain X/Y=Z, where X shows the number of items to be normalized, Y shows the number of correctly normalized items, and Z shows the corresponding accuracy rate.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 176, "end_pos": 189, "type": "METRIC", "confidence": 0.9894327223300934}]}, {"text": "As expected, the results in the Buscap\u00e9 corpus (used for diagnosis) are better than in Mercado Livre, because some lexical resources were constructed from analysis of OOV words in Buscap\u00e9.", "labels": [], "entities": [{"text": "Buscap\u00e9 corpus", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9495700597763062}, {"text": "diagnosis)", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.9293522238731384}]}, {"text": "In spite of both samples having the same number of reviews, the Mercado Livre sample contains proportionally more items to be normalized than the Buscap\u00e9 sample, that is, the reviews from Mercado Livre deviate more from standard language than those from Buscap\u00e9.", "labels": [], "entities": [{"text": "Mercado Livre sample", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.9363243182500204}, {"text": "Buscap\u00e9 sample", "start_pos": 146, "end_pos": 160, "type": "DATASET", "confidence": 0.9338600337505341}]}, {"text": "For the misspellings whose corrections are context-free, UGCNormal achieved a recall of 89% in Buscap\u00e9 corpus and 80% in Mercado Livre corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9993274211883545}, {"text": "Buscap\u00e9 corpus", "start_pos": 95, "end_pos": 109, "type": "DATASET", "confidence": 0.9854288697242737}, {"text": "Mercado Livre corpus", "start_pos": 121, "end_pos": 141, "type": "DATASET", "confidence": 0.938409149646759}]}, {"text": "This difference maybe due to the small size of both samples and the number of misspellings (in Mercado Livre there are almost twice as many misspellings as in Buscap\u00e9).", "labels": [], "entities": []}, {"text": "We evaluated the task noise removal in a single pass, identifying and correcting errors simultaneously.", "labels": [], "entities": [{"text": "task noise removal", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.6520269910494486}]}, {"text": "Therefore, cases where errors were identified but not corrected were taken to be failures just like unidentified errors.", "labels": [], "entities": []}, {"text": "However, it is worth mentioning that the normalizer failed to correct 6 true errors identified in the Buscap\u00e9 sample and 14 true errors identified in the Mercado Livre sample.", "labels": [], "entities": [{"text": "Buscap\u00e9 sample", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.9768963754177094}, {"text": "Mercado Livre sample", "start_pos": 154, "end_pos": 174, "type": "DATASET", "confidence": 0.9605633020401001}]}, {"text": "The other non-corrected errors were not even identified.", "labels": [], "entities": []}, {"text": "The normalization tool corrected 66% (138 of 209) of the manually annotated errors in the Buscap\u00e9 sample, and 63% (206 of 325) in the Mercado Livre sample.", "labels": [], "entities": [{"text": "Buscap\u00e9 sample", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.9596834480762482}, {"text": "Mercado Livre sample", "start_pos": 134, "end_pos": 154, "type": "DATASET", "confidence": 0.9573638836542765}]}, {"text": "Misspellings whose correction depends on contextual information were not expected to be corrected, as the speller is based only on lexical information.", "labels": [], "entities": []}, {"text": "However, thanks to the strategy of excluding highly infrequent words that are homographs of frequent words without diacritics, some such errors were corrected (38% of the annotated errors of such category in Buscap\u00e9 and 31% in Mercado Livre).", "labels": [], "entities": [{"text": "Buscap\u00e9", "start_pos": 208, "end_pos": 215, "type": "DATASET", "confidence": 0.9423656463623047}, {"text": "Mercado Livre", "start_pos": 227, "end_pos": 240, "type": "DATASET", "confidence": 0.9042021036148071}]}, {"text": "The case use in the start of sentences and the punctuation are treated by the sentence segmentation tool.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.7460566163063049}]}, {"text": "These procedures are simultaneous: if a punctuation mark is not inserted, the initial word after a period is consequently not converted into uppercase.", "labels": [], "entities": []}, {"text": "In the Mercado Livre corpus, the use of uppercase and lowercase is far more unconventional than in the Buscap\u00e9 corpus and this explains the deterioration of results in case use and punctuation.", "labels": [], "entities": [{"text": "Mercado Livre corpus", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9248053232828776}, {"text": "Buscap\u00e9 corpus", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.9269406795501709}]}, {"text": "For example, in Mercado Livre, unlike in Buscap\u00e9, we found reviews completely written in uppercase.", "labels": [], "entities": [{"text": "Buscap\u00e9", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9348931908607483}]}, {"text": "The conversion of proper nouns and acronyms to uppercase, as well as the conversion of Internet slangs to the standard language, are two issues that depend on the respective lexicons.", "labels": [], "entities": [{"text": "conversion of proper nouns and acronyms", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.8088636795679728}]}, {"text": "As such, lexicons resulting from the analysis of the Buscap\u00e9 corpus are not sufficient to identify all the proper nouns, acronyms and Internet slangs from the Mercado Livre corpus.", "labels": [], "entities": [{"text": "Buscap\u00e9 corpus", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9439476132392883}, {"text": "Mercado Livre corpus", "start_pos": 159, "end_pos": 179, "type": "DATASET", "confidence": 0.869580090045929}]}, {"text": "Finally, the glued words are normalized by the tokenizer only in cases where numbers are followed by units of measurement.", "labels": [], "entities": []}, {"text": "Glued words are rare in both evaluated corpora, but we need to tackle them in the future if we want to address other categories of UGC, such as chats and short messages.", "labels": [], "entities": []}, {"text": "UGCNormal made 149 corrections in the Buscap\u00e9 sample, of which 138 were true positives and 11 were false positives (well-formed words that were incorrectly modified), representing a precision of 93%.", "labels": [], "entities": [{"text": "Buscap\u00e9 sample", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.9625128507614136}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9978766441345215}]}, {"text": "In the Mercado Livre sample, UGCNormal made 220 corrections, of which 206 were true positives and 14 were false positives, also representing a precision of 93%.", "labels": [], "entities": [{"text": "Mercado Livre sample", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9483768343925476}, {"text": "corrections", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9631597399711609}, {"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9972366094589233}]}, {"text": "From the 82 OOV words in the Buscap\u00e9 sample, UGCNormal corrected 65 (79%), and the remaining 17 words are constituted of 6 (7.3%) true errors and 11 (13.4%) real words.", "labels": [], "entities": [{"text": "Buscap\u00e9 sample", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9292219877243042}]}, {"text": "In the Mercado Livre sample, UGCNormal identified 145 OOV words and appropriately corrected 117 (80.6%).", "labels": [], "entities": [{"text": "Mercado Livre sample", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9376840988794962}]}, {"text": "From the remaining 28 OOV words, 14 (9.6%) are true errors and 14 (9.6%) are real words.", "labels": [], "entities": []}, {"text": "The false positives (real words identified as errors) are mainly foreign loan words, proper nouns, acronyms and Internet slang absent from the UGCNormal's lexicons.", "labels": [], "entities": [{"text": "UGCNormal's lexicons", "start_pos": 143, "end_pos": 163, "type": "DATASET", "confidence": 0.9222401181856791}]}, {"text": "To validate the normalization tool, we evaluated its impact as a preprocessing step in two NLP tasks: POS tagging and opinion classification.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.8811116814613342}, {"text": "opinion classification", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.7663410007953644}]}, {"text": "For the first task, we used the tagger MXPOST, trained in the MAC-Morpho corpus (1.2 million tokens,.", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.6735101342201233}, {"text": "MAC-Morpho corpus", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9405497908592224}]}, {"text": "The better reported results of MXPOST are around 97%, for journalistic texts, the same genre used to train the tagger.", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.5404210686683655}]}, {"text": "For this experiment, we first randomly selected a sample often reviews from the Buscap\u00e9 corpus.", "labels": [], "entities": [{"text": "Buscap\u00e9 corpus", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.972120076417923}]}, {"text": "Then we tagged the sample with MXPOST and performed a linguistic revision of the POS tags, in order to create a gold-standard POS-tagged version of the sample.", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.7377191185951233}]}, {"text": "Subsequently, we POStagged three different versions of the same sample: 1) the original one; 2) aversion manually normalized, and 3) aversion automatically normalized by UGCNormal.", "labels": [], "entities": [{"text": "aversion", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9698113203048706}, {"text": "UGCNormal", "start_pos": 170, "end_pos": 179, "type": "DATASET", "confidence": 0.9414104223251343}]}, {"text": "The results of the three versions evaluated against the gold-standard version are presented in the.", "labels": [], "entities": []}, {"text": "The accuracy values are the ratio between the number of correct tags and the total number of tags (1226).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995250701904297}]}, {"text": "The result achieved by the automatically normalized version (UGCNormal) is almost the same as that achieved by the human normalized version.", "labels": [], "entities": []}, {"text": "We have also made a test of statistical significance to evaluate the probability that such improvement in the tagger precision could have been obtained by chance.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.8664171695709229}]}, {"text": "Given the sample size and some relevant considerations while evaluating NLP tasks (), we opted for the non-parametric test Wilcoxon Signed-Rank.", "labels": [], "entities": [{"text": "Wilcoxon Signed-Rank", "start_pos": 123, "end_pos": 143, "type": "DATASET", "confidence": 0.7772434651851654}]}, {"text": "We observed a significance of 0.05, the p-value being equal to 0.02249.", "labels": [], "entities": [{"text": "significance", "start_pos": 14, "end_pos": 26, "type": "METRIC", "confidence": 0.9680982232093811}]}, {"text": "The other extrinsic evaluation is based on a lexicon-based opinion classifier , which assigns polarity to texts (positive, negative or neutral).", "labels": [], "entities": []}, {"text": "We applied the classifier on a sample of 13,685 reviews (6,812 positives and 6,873 negatives) extracted from the Buscap\u00e9 corpus, before and after normalization by UGCNormal.", "labels": [], "entities": [{"text": "Buscap\u00e9 corpus", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.9837869703769684}, {"text": "UGCNormal", "start_pos": 163, "end_pos": 172, "type": "DATASET", "confidence": 0.9689885973930359}]}, {"text": "The average of F1-score measures (F1-score positive and F1-score negative) was 0.736 for non-normalized texts, and 0.758 for normalized texts.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9990670084953308}, {"text": "F1-score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9873666763305664}, {"text": "F1-score negative", "start_pos": 56, "end_pos": 73, "type": "METRIC", "confidence": 0.9685478806495667}]}, {"text": "The performance of a lexicon-based opinion classifier is highly dependent of the recognition of sentiment words in the text.", "labels": [], "entities": []}, {"text": "As errors like \"exelente\" (excelente=excellent) and \"otimo\" (\u00f3timo=great) are very frequent, such improvement in the precision, after normalization, was expected.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9995606541633606}]}], "tableCaptions": [{"text": " Table 2: Distribution of errors and corrections for  each UGC sample, and the recall values for each  error type.", "labels": [], "entities": [{"text": "UGC sample", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.8230438232421875}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9994137287139893}]}, {"text": " Table 3: The number of correct tags produced by  the tagger, for each sample version.", "labels": [], "entities": []}]}