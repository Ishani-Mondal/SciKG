{"title": [{"text": "Automatic dysfluency detection in dysarthric speech using deep belief networks", "labels": [], "entities": [{"text": "Automatic dysfluency detection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6765283743540446}]}], "abstractContent": [{"text": "Dysarthria is a speech disorder caused by difficulties in controlling muscles, such as the tongue and lips, that are needed to produce speech.", "labels": [], "entities": []}, {"text": "These differences in motor skills cause speech to be slurred, mumbled, and spoken relatively slowly, and can also increase the likelihood of dysfluency.", "labels": [], "entities": []}, {"text": "This includes non-speech sounds, and 'stuttering', defined here as a disruption in the fluency of speech manifested by prolongations, stopgaps , and repetitions.", "labels": [], "entities": []}, {"text": "This paper investigates different types of input features used by deep neural networks (DNNs) to automatically detect repetition stuttering and non-speech dysfluencies within dysarthric speech.", "labels": [], "entities": []}, {"text": "The experiments test the effects of dimen-sionality within Mel-frequency cepstral coefficients (MFCCs) and linear predictive cepstral coefficients (LPCCs), and explore the detection capabilities in dyarthric versus non-dysarthric speech.", "labels": [], "entities": []}, {"text": "The results obtained using MFCC and LPCC features produced similar recognition accuracies; repetition stuttering in dysarthric speech was identified correctly at approximately 86% and 84% for non-dysarthric speech.", "labels": [], "entities": [{"text": "MFCC", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.7453527450561523}, {"text": "repetition stuttering", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.885858952999115}]}, {"text": "Non-speech sounds were recognized with approximately 75% accuracy in dysarthric speakers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9990743398666382}]}], "introductionContent": [{"text": "Many studies have researched ways to improve the intelligibility of dysarthric speech, including methods that targeted particular aspects of speech to modify.", "labels": [], "entities": []}, {"text": "[1] implemented a system of transformations that focused strictly on mapping vowels from individuals with dysarthria to vowels more characteristic of non-dysarthric speech.", "labels": [], "entities": []}, {"text": "Those experiments showed an intelligibility increase of 6%.", "labels": [], "entities": []}, {"text": "In 2013, Rudzicz proposed a method that added the correction of other pronunciation errors and adjusted tempo.", "labels": [], "entities": [{"text": "tempo", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.982092022895813}]}, {"text": "Among a cohort of listeners unfamiliar with the speech of people with cerebral palsy, word recognition rates increased by 19.6%.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7590721547603607}]}, {"text": "Crucially, the Levenshtein-based detection of phoneme repetitions and non-speech dysfluencies in that work depended on full phoneme segmentation, which may itself be quite challenging for dysarthric speech.", "labels": [], "entities": [{"text": "Levenshtein-based detection of phoneme repetitions", "start_pos": 15, "end_pos": 65, "type": "TASK", "confidence": 0.5772195041179657}]}, {"text": "Chee et al. provided an overview of automatic stuttering detection, emphasizing its difficulty across a number of classification methods., e.g., implemented artificial neural networks (ANNs) and 'rough sets' to detect three types of 'stuttering': stop-gaps, vowel prolongations, and syllable repetitions, obtaining accuracies up to 73.25% with ANNs and 91% with rough sets.", "labels": [], "entities": [{"text": "stuttering detection", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8140628635883331}, {"text": "accuracies", "start_pos": 315, "end_pos": 325, "type": "METRIC", "confidence": 0.9907914400100708}]}, {"text": "Wi\u00b4sniewskiWi\u00b4sniewski et al. performed two studies that used hidden Markov models with Melfrequency cepstral coefficients (MFCCs) to detect stuttering.", "labels": [], "entities": []}, {"text": "The first focused on both prolongation of fricative phonemes and blockades with repetition of stop phonemes that produced an accuracy of 70%; the second strictly focused on prolongation of fricative phonemes and found an improvement inaccuracy to approximately 80%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9992625117301941}]}, {"text": "Rath investigated modifications to MFCC feature vectors in speaker adaptation using deep neural networks (DNNs), obtaining 3% improvements over Gaussian mixture models (GMMs) baselines.", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8409906029701233}]}, {"text": "Across various types of speech features, deep learning has shown considerable improvements across several areas of speech recognition, compared with traditional techniques such as hidden Markov models.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7171030640602112}]}, {"text": "Here, we compare MFCCs (which are the most commonly used feature set in this domain) and linear predictive cepstral coefficients (LPCCs), which are another popular but less utilized feature set.", "labels": [], "entities": []}, {"text": "An exception was Chee et al., who applied LPCCs with k-nearest-neighbors and linear discriminant analysis classifiers to automatically detect prolongations and repetition stutters, with recognition accuracy up to 89.77%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.8404621481895447}]}, {"text": "In the related field of automatic speech recognition ()ASR), MFCCs have consistently generated better results than LPCCs; to see if this trend extends to the domain of dysfluency detection, we compare these feature types with DNNs.", "labels": [], "entities": [{"text": "automatic speech recognition ()ASR)", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.7854620069265366}, {"text": "dysfluency detection", "start_pos": 168, "end_pos": 188, "type": "TASK", "confidence": 0.7947668731212616}]}], "datasetContent": [{"text": "We use two different partitioning schemes to compare results according to different categories of interest (), namely generic-vs-individual speaker models (i.e., speaker-independent vs. speaker-dependent), and dysarthric-vs-non-dysarthric individuals.", "labels": [], "entities": []}, {"text": "A total of 120 repetition stutters occurred across all 3115 recordings of dysarthric speech, and a total of 42 repetition stutters occurred across all 5641 recordings of non-dysarthric speech.", "labels": [], "entities": [{"text": "repetition", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9926395416259766}, {"text": "repetition", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9722042679786682}]}, {"text": "The male and female dysarthric speakers with the most stutter occurrences were used for individual analysis; specifically, male dysarthric speaker M04 with 32 stutters, and female dysarthric speaker F03 with 22 stutters.", "labels": [], "entities": []}, {"text": "Among the nondysarthric speakers, there is no significant difference between males and females, so the non-dysarthric speaker with the most stutter occurrences was used in further analysis, namely male control MC04 with 16 stutters.", "labels": [], "entities": []}, {"text": "All training and testing data sets were divided in the same way -70% of stutter occurrences were randomly assigned to training and paired with a random utterance without any stutter.", "labels": [], "entities": []}, {"text": "By balancing training class sizes, we avoid the problem of overfitting to devolved majority classification.", "labels": [], "entities": []}, {"text": "Testing data consisted of the remaining 30% of repetition stutters.", "labels": [], "entities": [{"text": "repetition", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9968265891075134}]}, {"text": "An empirical question is whether stutter detection is more or less difficult in dysarthric speech, compared to nondysarthric speech.", "labels": [], "entities": [{"text": "stutter detection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.966421365737915}]}, {"text": "shows the average error rates of detecting repetition stuttering using 5-fold cross validation with MFCC and LPCC features.", "labels": [], "entities": [{"text": "detecting repetition stuttering", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.8705605069796244}, {"text": "MFCC", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.8274790644645691}]}, {"text": "Clearly, across all models, accuracy increases monotonically as additional context is added.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9989664554595947}]}, {"text": "We also note that we obtain state-of-the-art accuracy for dysarthric speaker F03 using 10 frames of surrounding context, which is comparable tos work with rough sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.999042809009552}]}, {"text": "An n-way analysis of variance reveals strong effects of window size (F3 = 836.91, p < 0.001) and population (F1 = 11.80, p < 0.01), but not of the feature set (F1 = 0.12, p = 0.74).", "labels": [], "entities": [{"text": "F3", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9861230850219727}, {"text": "F1", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.988436222076416}, {"text": "F1", "start_pos": 160, "end_pos": 162, "type": "METRIC", "confidence": 0.9919443130493164}]}, {"text": "Across all experiments, LPCCs give slightly lower error than MFCCs, on average (20.17% vs. 20.32%, respectively).", "labels": [], "entities": [{"text": "error", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9928807020187378}]}, {"text": "Except for the (relatively inaccurate) case where no context frames are used, generic control models always give higher error than generic dysarthric models, by absolute differences of 2% to 2.35%.", "labels": [], "entities": [{"text": "error", "start_pos": 120, "end_pos": 125, "type": "METRIC", "confidence": 0.9885064363479614}]}, {"text": "It is important to note that we only consider main effects of these grouping variables -given the different dimensionality of MFCC and LPCC, one cannot make direct interaction comparisons across these groups and context sizes simultaneously.", "labels": [], "entities": [{"text": "MFCC", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.8111021518707275}]}, {"text": "Speaker-dependent models always outperformed associated speaker-independent models.", "labels": [], "entities": []}, {"text": "The difference in error rates between generic and individualized models is larger for dysarthric speech than non-dysarthric speech.", "labels": [], "entities": []}, {"text": "At best, the speaker-dependent dysarthric models achieved a 5.06% lower rate than the speaker-independent dysarthric models, while speaker-dependent non-dysarthric models obtained at best a difference of 2.85%.", "labels": [], "entities": []}, {"text": "Interestingly, it is easier to detect stuttering in dysarthric speech than in non-dysarthric speech.", "labels": [], "entities": []}, {"text": "In fact, error rates were consistently lower for the dysarthric speech (\u224814%) than for the non-dysarthric speech (\u224816%).", "labels": [], "entities": [{"text": "error", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.995520830154419}]}, {"text": "This suggests that the implemented method is robust to this particular speech disorder.", "labels": [], "entities": []}, {"text": "We repeated the methodology of Experiment 1, but considered instead 'lower-level' dysfluencies and non-speech vocal noise that can affect speech recognition and synthesis systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7140807956457138}]}, {"text": "Here, annotation is based on the phonetic transcriptions provided in the TORGO corpus.", "labels": [], "entities": [{"text": "TORGO corpus", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9555605947971344}]}, {"text": "Segments labeled as noi (noise) were examined and manually tagged with either none, or any combination of the following three dysfluency types: aspiration Noise related to breathing, i.e., inspiration or expiration.", "labels": [], "entities": []}, {"text": "mouth/lips Noise produced by the lips and/or mouth/tongue.", "labels": [], "entities": []}, {"text": "vocal Non-speech voicing (e.g., laughter, hesitation...).", "labels": [], "entities": []}, {"text": "The procedure of classification and evaluation is the same as in Experiment 1, except only individuals with dysarthria are considered, since the amount of occurrences of such dysfluencies in control speakers were not significant.", "labels": [], "entities": [{"text": "classification", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9609440565109253}]}, {"text": "Among all 1403 recordings of the head-worn microphones for dysarthric speakers with phonetic transcriptions, we found 706 instances of aspiration noise, 496 of mouth/lips, and 111 of vocal noise.", "labels": [], "entities": []}, {"text": "shows the average error rates of detecting the different non-speech dysfluencies using 5-fold cross validation with MFCC and LPCC features.", "labels": [], "entities": [{"text": "error", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9563004374504089}, {"text": "MFCC", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8327450156211853}]}, {"text": "The accuracy increases with the use of one or more frames of context, but adding more than one frame does not improve the results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994538426399231}]}, {"text": "These types of low-level dysfluencies are significantly localized in time or highly characterized by their spectral shape.", "labels": [], "entities": []}, {"text": "Therefore, adding more contextual information does not appear to improve classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.9678528308868408}]}, {"text": "Dysfluencies of type aspiration are consistently more accurately classified than mouth/lips, which in turn are easier to classify than vocal.", "labels": [], "entities": []}, {"text": "The aspiration dysfluencies contain a very characteristic timbre which is easier to discriminate from other speech sounds than the other classes.", "labels": [], "entities": []}, {"text": "On the other hand, vocal dysfluencies are the closest to actual speech phones, leading to a more difficult differentiation.", "labels": [], "entities": []}, {"text": "We note that aspiration dysfluencies are usually longer and since, in our current setting, an entire region is tagged with the noise type without performing segmentation, frames containing aspiration maybe systematically more accurately labelled than those with other more localized noises such as mouth/lips or vocal.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Input feature dimensions", "labels": [], "entities": [{"text": "Input", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9475293159484863}]}, {"text": " Table 3: Average error rate (%, 5-fold cross-validation) of stut- ter detection using MFCC and LPCC features across speaker  groups. Speakers F03, M04, and MC04 are also examined in- dividually due to their relatively high rates of stuttering.", "labels": [], "entities": [{"text": "error rate", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9318152964115143}, {"text": "stut- ter detection", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7465765476226807}]}, {"text": " Table 4: Average error rate (%, 5-fold cross-validation) across  other dysfluencies using MFCC and LPCC features across  speaker groups.", "labels": [], "entities": [{"text": "Average error rate", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7901774744192759}]}]}