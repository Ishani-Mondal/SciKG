{"title": [{"text": "Deep Learning Models for Sentiment Analysis in Arabic", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9761055111885071}]}], "abstractContent": [{"text": "In this paper, deep learning framework is proposed for text sentiment classification in Arabic.", "labels": [], "entities": [{"text": "text sentiment classification", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.8828208446502686}]}, {"text": "Four different architectures are explored.", "labels": [], "entities": []}, {"text": "Three are based on Deep Belief Networks and Deep Auto Encoders, where the input data model is based on the ordinary Bag-of-Words, with features based on the recently developed Arabic Sentiment Lexicon in combination with other standard lexicon features.", "labels": [], "entities": []}, {"text": "The fourth model, based on the Recursive Auto Encoder, is proposed to tackle the lack of context handling in the first three models.", "labels": [], "entities": []}, {"text": "The evaluation is carried out using Linguistic Data Consortium Arabic Tree Bank dataset, with benchmarking against the state of the art systems in sentiment classification with reported results on the same dataset.", "labels": [], "entities": [{"text": "Linguistic Data Consortium Arabic Tree Bank dataset", "start_pos": 36, "end_pos": 87, "type": "DATASET", "confidence": 0.8813500830105373}, {"text": "sentiment classification", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.9151040613651276}]}, {"text": "The results show high improvement of the fourth model over the state of the art, with the advantage of using no lexicon resources that are scarce and costly in terms of their development.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the revolution of web 2.0 and the amount of opinionated data generated by online users, personal views and opinions are no longer constrained to authors in newspapers or custom opinion surveys.", "labels": [], "entities": []}, {"text": "Instead, almost anyone can express opinions through social media.", "labels": [], "entities": []}, {"text": "The abundance of these opinions and their availability and accessibility gave birth to automated applications that use sentiment analysis (opinion mining) as a key factor in predicting stock market, evaluating products, surveying the public, etc.", "labels": [], "entities": [{"text": "sentiment analysis (opinion mining)", "start_pos": 119, "end_pos": 154, "type": "TASK", "confidence": 0.7441428204377493}, {"text": "predicting stock market", "start_pos": 174, "end_pos": 197, "type": "TASK", "confidence": 0.8531221946080526}]}, {"text": "However, automated sentiment analysis is still far from producing output with quality comparable to humans due to the complexity of the semantics.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8057057559490204}]}, {"text": "Furthermore, the Arabic language adds another dimension of difficulty to automated sentiment analysis due to its morphological richness, ambiguity, and the large number of dialectal variants.", "labels": [], "entities": [{"text": "automated sentiment analysis", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.7094085017840067}]}, {"text": "These challenges add to the complexity of the required natural language processing (NLP).", "labels": [], "entities": []}, {"text": "Many methods have been suggested in literature to address automated sentiment analysis.", "labels": [], "entities": [{"text": "automated sentiment analysis", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.6818171938260397}]}, {"text": "One of the prominent approaches is the use of machine learning (ML) techniques, where sentiment analysis is formalized as a classification task.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9350610375404358}]}, {"text": "The predicted classes are typically chosen to be positive or negative sentiment.", "labels": [], "entities": []}, {"text": "The classification tasks range from classifying the sentiment of words, phrases, sentences, or sometimes documents.", "labels": [], "entities": [{"text": "classifying the sentiment of words, phrases, sentences", "start_pos": 36, "end_pos": 90, "type": "TASK", "confidence": 0.8392730487717522}]}, {"text": "Deep learning has been recently considered for sentiment analysis).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.9845651388168335}]}, {"text": "worked on phrase level sentiment classification using the Recursive Neural Tensor Network (RNTN) over a fine grained phrase level annotated corpus (Stanford Sentiment Tree Bank).", "labels": [], "entities": [{"text": "phrase level sentiment classification", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.7946129739284515}, {"text": "Stanford Sentiment Tree Bank)", "start_pos": 148, "end_pos": 177, "type": "DATASET", "confidence": 0.9250873684883117}]}, {"text": "Other deep learning models that can potentially be used in sentiment analysis include deep neural networks (DNN), convolutional neural networks (CNN) (, Deep Belief Networks (DBN) with fast inferencing of the model parameters (), and recurrent neutral network (RNN) ( We aim in this work to investigate the merit of using deep models for sentiment analysis in Arabic, focusing on the sentence level sentiment classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.9688250124454498}, {"text": "sentiment analysis", "start_pos": 338, "end_pos": 356, "type": "TASK", "confidence": 0.9056889712810516}, {"text": "sentence level sentiment classification", "start_pos": 384, "end_pos": 423, "type": "TASK", "confidence": 0.689066007733345}]}, {"text": "To the best of our knowledge, this is the first attempt to explore deep learning models for sentiment classification in Arabic.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.9417661726474762}]}, {"text": "For the vector space representation of text, we utilize ArSenL (), a recently published sentiment lexicon.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9943578839302063}]}, {"text": "Each word in the lexicon is associated with three sentiment scores indicating levels of positivity, negativity, and neutrality.", "labels": [], "entities": []}, {"text": "ArSenL includes 28,780 Arabic lemmas with the corresponding number of 157,969 synsets.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6086320877075195}]}, {"text": "We explore four deep learning models: DNN, DBN, Deep Auto Encoder (DAE), and combined DAE with DBN.", "labels": [], "entities": []}, {"text": "DNN applies back propagation to a conventional neural network, but with several layers.", "labels": [], "entities": []}, {"text": "DBN applies generative pre-training phase before feeding a discriminative fine tuning step.", "labels": [], "entities": [{"text": "DBN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9116216897964478}, {"text": "generative", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.978270947933197}]}, {"text": "DAE provides a generative model representation for the original but with reduced dimensionality.", "labels": [], "entities": [{"text": "DAE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9376826286315918}]}, {"text": "Finally, the RAE aims at parsing the raw sentence words in the best order that minimizes the reconstruction error of re-generating the same sentence words in the same order; in other words, it aims at discovering the best parse tree that maximizes the probability of the input data.", "labels": [], "entities": [{"text": "RAE", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.7295619249343872}]}, {"text": "Both DAE and RAE models aim at providing a compact representation of the input sentence.", "labels": [], "entities": [{"text": "RAE", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.5204561352729797}]}, {"text": "Both models are based on unsupervised learning, where their objective is the minimization of reconstruction error of the input, so no manual annotation is needed.", "labels": [], "entities": []}, {"text": "The main difference is that; RAE considers the context and order of parsing of the sentence.", "labels": [], "entities": [{"text": "RAE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6193309426307678}]}, {"text": "This recursion enables parsing variable length sentences.", "labels": [], "entities": [{"text": "parsing variable length sentences", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.8893870413303375}]}, {"text": "While the DAE is parsing the whole sentence words at once in the first layer, with no consideration of the order of parsing of words, and keep feeding the representation forward in the deep architecture on the hope that useful features are extracted at each layer of depth.", "labels": [], "entities": [{"text": "DAE", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8145880103111267}]}, {"text": "This property makes it mandatory to have fixed length features vector, which promotes the Bag-of-Words (BoW) model.", "labels": [], "entities": []}, {"text": "Both DAE and RAE models require a classifier on top of their obtained representation.", "labels": [], "entities": [{"text": "RAE", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.49939170479774475}]}, {"text": "In case of DAE, the classifier is the DBN, while in case of RAE, the classifier is a softmax layer.", "labels": [], "entities": []}, {"text": "The Linguistic Data Consortium Arabic Tree Bank (LDC ATB) dataset is used to evaluate the proposed models.", "labels": [], "entities": [{"text": "Linguistic Data Consortium Arabic Tree Bank (LDC ATB) dataset", "start_pos": 4, "end_pos": 65, "type": "DATASET", "confidence": 0.8853157785805789}]}, {"text": "The input data to the first three models depend on the BoW model, with the utilization of lexicon scores.", "labels": [], "entities": [{"text": "BoW model", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.9075857698917389}]}, {"text": "In our case it is ArSenL, as special sentiment features.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.970279335975647}]}, {"text": "The rest of the paper is organized as follows: Section 2 overviews the work related to sentiment classification in Arabic.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.9195885956287384}]}, {"text": "Section 3 describes the features employed from ArSenL.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.8185790181159973}]}, {"text": "Section 4 includes a description of the proposed deep learning models.", "labels": [], "entities": []}, {"text": "Section 5 presents the results of the evaluation on LDC ATB, and section 6 concludes the paper.", "labels": [], "entities": [{"text": "LDC ATB", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.7886636257171631}]}], "datasetContent": [{"text": "To evaluate the models, LDC ATB dataset is used for training and testing.", "labels": [], "entities": [{"text": "LDC ATB dataset", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.6831703583399454}]}, {"text": "The dataset is split into 944 training sentences and 236 test sentences.", "labels": [], "entities": []}, {"text": "Only positive and negative classes are considered for the data represented by Arsenl_lemma, and Arsenl_sentence features separately.", "labels": [], "entities": []}, {"text": "The results in show that both, DBN and Auto Encoder (models 2 and 3) do not suffer over fitting while model 1 does.", "labels": [], "entities": []}, {"text": "This is inline with the observation in () which indicates that pre-training provides kind of regularization on the learned weights of the network.", "labels": [], "entities": []}, {"text": "This is expected because deep auto encoder output provides good generalization of the input data, and has even less tendency to over-fit training data.", "labels": [], "entities": []}, {"text": "With selected architectures, and inmost cases, the F1 measures were close to SVM, and sometimes superior.", "labels": [], "entities": [{"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9974466562271118}]}, {"text": "The accuracy measures were not superior.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999537467956543}]}, {"text": "The input representation in the first three models is based on the BoW encoding of the ArSenL scores, which makes the features vector very sparse with too many zeros.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9589389562606812}]}, {"text": "This hurts badly the reconstruction capability of the network, because slight errors around zeros add up.", "labels": [], "entities": []}, {"text": "This effect is reduced when the features vectors are first fed to a DAE to obtain a compact representation rather than a sparse one.", "labels": [], "entities": []}, {"text": "A better representation would be to select only the vocabulary words that are encountered in the sentence under focus.", "labels": [], "entities": []}, {"text": "However, this will make the features vector length variable.", "labels": [], "entities": []}, {"text": "A recursive model addresses this problem by parsing the sentence words recursively to obtain sentence wide representation considering only the vocabulary words that exist in the sentence.", "labels": [], "entities": []}, {"text": "This is one of the reasons why the RAE is superior to the other three models.", "labels": [], "entities": [{"text": "RAE", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.6365441083908081}]}, {"text": "The RAE model outperforms all the other models by a large margin of around 9%.", "labels": [], "entities": [{"text": "RAE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.6256431341171265}]}, {"text": "As pointed out earlier, in this model, semantic context and the parsing order of words are considered.", "labels": [], "entities": []}, {"text": "In the same time, no lexicon is used, and no special features are used, but only raw words as input..", "labels": [], "entities": []}, {"text": "Benchmark results on LDC ATB In our experiments on RAE we focus on the idea of obtaining a representation that takes into consideration the context of the word.", "labels": [], "entities": [{"text": "LDC ATB", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.4691765755414963}, {"text": "RAE", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9654074907302856}]}, {"text": "At the same time, we want to take advantage of the unsupervised nature of RAE that avoids the use of sentiment lexicon.", "labels": [], "entities": [{"text": "RAE", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.7364262342453003}]}, {"text": "Another future direction will be to consider using ArSenL lexicon to create better representation of word embeddings.", "labels": [], "entities": []}, {"text": "This can be done by creating special word embedding blocks with the objective of generating the ArSenL sentiment scores, and then use this representation as input to the RAE.", "labels": [], "entities": [{"text": "ArSenL sentiment scores", "start_pos": 96, "end_pos": 119, "type": "METRIC", "confidence": 0.9104544719060262}]}, {"text": "This is considered as a pre-training step to the embedding block rather than random initialization or n-gram validity task.", "labels": [], "entities": []}, {"text": "Also, the pre-training using ArSenL enables the consideration of the individual words sentiment in addition to the semantic words context.", "labels": [], "entities": [{"text": "ArSenL", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.8600136041641235}]}], "tableCaptions": [{"text": " Table 2. Evaluation results on LDC ATB", "labels": [], "entities": [{"text": "LDC ATB", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.6574483513832092}]}, {"text": " Table 3. Benchmark results on LDC ATB", "labels": [], "entities": [{"text": "LDC ATB", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.58663210272789}]}]}