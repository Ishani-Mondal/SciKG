{"title": [{"text": "Neural word embeddings with multiplicative feature interactions for tensor-based compositions", "labels": [], "entities": []}], "abstractContent": [{"text": "Categorical compositional distributional models unify compositional formal semantic models and distributional models by composing phrases with tensor-based methods from vector representations.", "labels": [], "entities": []}, {"text": "For the tensor-based compositions , Milajevs et al.", "labels": [], "entities": []}, {"text": "(2014) showed that word vectors obtained from the continuous bag-of-words (CBOW) model are competitive with those from co-occurrence based models.", "labels": [], "entities": []}, {"text": "However, because word vectors from the CBOW model are trained assuming additive interactions between context words, the word composition used for the training mismatches to the tensor-based methods used for evaluating the actual compositions including point-wise multiplication and tensor product of context vectors.", "labels": [], "entities": []}, {"text": "In this work, we show whether the word embeddings from extended CBOW models using multiplication or tensor product between context words, reflecting the actual composition methods, can show better performance than those from the baseline CBOW model in actual tasks of compositions with multiplication or tensor-based methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, there has been a surge of interest in using word vectors for modeling semantics.", "labels": [], "entities": []}, {"text": "introduced word2vec that includes the continuous bag-of-words (CBOW) model and the skip-gram model.", "labels": [], "entities": []}, {"text": "These models have been most widely used for generating word vectors to be used for word related tasks because of where T is the total number of words in a corpus, wt is the tth word, pt is the tth word vector, and c is the half window size.", "labels": [], "entities": []}, {"text": "showed that the word vectors generated from the CBOW model are competitive with those from co-occurrence based models for both simple arithmetic compositions and tensorbased compositions for categorical compositional distributional models (.", "labels": [], "entities": []}, {"text": "Categorical compositional distributional models represent compositional semantics with algebra of Pregroup by representing each grammatical reduction as a linear map in vector spaces ().", "labels": [], "entities": [{"text": "Pregroup", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9756702780723572}]}, {"text": "For example, cats like milk consists of a subject noun, a transitive verb requiring a subject and an object, and an object noun, respectively.", "labels": [], "entities": []}, {"text": "In Pregroup grammar, the types of the three words in this example are n, , and n, respectively, where n is a noun, n r can be combined with an in the left, n l can be combined with an in the right, and sis a declarative statement.", "labels": [], "entities": [{"text": "Pregroup grammar", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7012260556221008}]}, {"text": "Then, they can be reduced to represent the entire phrase with single entity as follows: In the reduction, n r is composed with the left n resulting in an identity element, 1.", "labels": [], "entities": []}, {"text": "Then, n l is composed with the right n resulting in another 1.", "labels": [], "entities": []}, {"text": "Because 1 is an identity element, 1s1 is reduced to s.", "labels": [], "entities": []}, {"text": "Since there is no specification of actual implementation of the composition in categorical compositional distributional models, different composition methods have been introduced; they are reviewed in Section 2.", "labels": [], "entities": []}, {"text": "However, there are few studies about the vector representation of single words regarding those compositions.", "labels": [], "entities": []}, {"text": "One issue of using the word vectors from the CBOW model as the constituent vectors for tensorbased composition is that their assumptions of the composition are different.", "labels": [], "entities": []}, {"text": "Word embeddings of the CBOW model are trained with an additive context composition, which is the mean of the context projection.", "labels": [], "entities": []}, {"text": "However, most tensor-based compositions use point-wise multiplication or tensor product as composition operators.", "labels": [], "entities": []}, {"text": "This means that there is a mismatch between the composition method used for the training of the underlying word vectors and the actual composition methods we evaluate.", "labels": [], "entities": []}, {"text": "To alleviate the mismatch, we introduce extensions of the CBOW model with multiplicative interactions between word projections to obtain word embeddings more suitable for the tensor-based compositions.", "labels": [], "entities": []}, {"text": "For four datasets, evaluating different types of compositions, we show that those extensions of the CBOW model improve the performance of the actual composition tasks with multiplication or tensor product operations.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the five different CBOW-based models proposed in Section 3, we use the following datasets: similarity of transitive verbs with multiple senses from Grefenstette and Sadrzadeh (2011a), three-word sentence similarity from , paraphrase detection from, and dialog act tagging for the Switchboard corpus) from.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 234, "end_pos": 254, "type": "TASK", "confidence": 0.8448213934898376}, {"text": "dialog act tagging", "start_pos": 265, "end_pos": 283, "type": "TASK", "confidence": 0.6605708400408427}, {"text": "Switchboard corpus", "start_pos": 292, "end_pos": 310, "type": "DATASET", "confidence": 0.8682449460029602}]}, {"text": "These are all the datasets evaluated in 's work as well.", "labels": [], "entities": []}, {"text": "Each phrase in the first two datasets is fixed as a subject, a transitive verb, and an object whereas the length of each phrase in the last two datasets is arbitrary.", "labels": [], "entities": []}, {"text": "There are several differences between our word vectors and the ones used in . First, we use BNC as the training set while Milajevs et al.", "labels": [], "entities": []}, {"text": "(2014) use pretrained word vectors from word2vec that are trained using GoogleNews dataset.", "labels": [], "entities": [{"text": "GoogleNews dataset", "start_pos": 72, "end_pos": 90, "type": "DATASET", "confidence": 0.9064580202102661}]}, {"text": "To reduce the size of projection matrices, all the words are lower-cased and words occurring 20 times or less are converted to the words' POS tags.", "labels": [], "entities": []}, {"text": "Second, instead of negative sampling, our models use hierarchical softmax as the objective function, where each word is represented as a leaf node of Huffman tree since hierarchical softmax is better for training with infrequent words ().", "labels": [], "entities": []}, {"text": "Third, we use gradient clipping for more stable training since gradient can be fluctuating when the projections are multiplied.", "labels": [], "entities": []}, {"text": "All the other parameters for the training are the same as those used for 's experiments.", "labels": [], "entities": []}, {"text": "Using the mean as the network combination function can be considered a reimplementation of Milajevs et al.", "labels": [], "entities": []}, {"text": "(2014)'s system subject to the changes mentioned above.", "labels": [], "entities": []}, {"text": "We trained the CBOW-based models and obtained 300 dimensional word vectors, which are with the same dimensionality used in shows the experiment results for the threeword phrases.", "labels": [], "entities": []}, {"text": "The first column represents the two evaluation tasks, the second column is the composition methods described in, and the third column shows the results of neural word embeddings (NWE) from previous work ( . Bold entries in the table indicate the highest scores among our models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Spearman's \u03c1 on the similarity of transitive verbs with multiple senses (top) and three-word sentence similarity  (bottom). The mean column can be considered an implementation of the Milajevs et al. (2014)'s model on the BNC  corpus.", "labels": [], "entities": [{"text": "BNC  corpus", "start_pos": 231, "end_pos": 242, "type": "DATASET", "confidence": 0.9283372163772583}]}, {"text": " Table 4: Accuracies on the paraphrase detection (top) and the dialog act tagging (bottom). The mean column can be  considered an implementation of the", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9873790144920349}, {"text": "paraphrase detection", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7741313874721527}]}]}