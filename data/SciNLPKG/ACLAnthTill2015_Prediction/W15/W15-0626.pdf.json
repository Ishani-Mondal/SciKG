{"title": [{"text": "Task-Independent Features for Automated Essay Grading", "labels": [], "entities": [{"text": "Automated Essay Grading", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7619486451148987}]}], "abstractContent": [{"text": "Automated scoring of student essays is increasingly used to reduce manual grading effort.", "labels": [], "entities": [{"text": "Automated scoring of student essays", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7108320772647858}]}, {"text": "State-of-the-art approaches use supervised machine learning which makes it complicated to transfer a system trained on one task to another.", "labels": [], "entities": []}, {"text": "We investigate which currently used features are task-independent and evaluate their transferability on English and German datasets.", "labels": [], "entities": [{"text": "English and German datasets", "start_pos": 104, "end_pos": 131, "type": "DATASET", "confidence": 0.6031926274299622}]}, {"text": "We find that, by using our task-independent feature set, models transfer better between tasks.", "labels": [], "entities": []}, {"text": "We also find that the transfer works even better between tasks of the same type.", "labels": [], "entities": []}], "introductionContent": [{"text": "Having students write an essay is a widely used method for assessment, e.g. universities use essay writing skills as a proxy for the prospects of applicants.", "labels": [], "entities": []}, {"text": "As manually grading essays is costly, automated essay grading systems are increasingly used becasue they -once developed -do not introduce additional costs for grading new essays.", "labels": [], "entities": []}, {"text": "Automated essay grading systems usually follow a supervised approach and yield a quality of holistic grading comparable to human performance ().", "labels": [], "entities": []}, {"text": "These systems make use of certain properties of essays (called features) in order to estimate the essay quality.", "labels": [], "entities": []}, {"text": "In the grading process, these features are extracted and ratings are assigned according to the manifestations of the features ().", "labels": [], "entities": []}, {"text": "In order to automatically learn the association between feature values and ratings a high amount of manually rated essays is required for training.", "labels": [], "entities": []}, {"text": "Hence, it seems desirable to develop systems that work without this initial input, which means -expressed in terms of machine learning -that features should not be defined by the present task but by general essay grading.", "labels": [], "entities": []}, {"text": "A task is defined here as prompting a group of humans to solve a particular writing task.", "labels": [], "entities": []}, {"text": "Tasks differ in attributes such as the grade-level of underlying subjects or characteristics of the prompt.", "labels": [], "entities": []}, {"text": "Many kinds of features have been proposed for essay grading ().", "labels": [], "entities": [{"text": "essay grading", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.7698328495025635}]}, {"text": "They differ in the degree of dependency to the task at hand.", "labels": [], "entities": []}, {"text": "There are features that are strongly dependent on a task, e.g. when they detect important words or topics).", "labels": [], "entities": []}, {"text": "Other features are less dependent, e.g. when they capture general characteristics of essays like the number of words in the essay), usage of connectors (), etc.", "labels": [], "entities": []}, {"text": "We assume that a system which considers only task-independent features should perform well no matter what task it is trained on.", "labels": [], "entities": []}, {"text": "However, it is unclear how much explanatory power the model might lose in this step.", "labels": [], "entities": []}, {"text": "In this paper, we test this hypothesis by performing experiments with a state-of-theart essay grading system on English and German datasets.", "labels": [], "entities": [{"text": "English and German datasets", "start_pos": 112, "end_pos": 139, "type": "DATASET", "confidence": 0.6070786714553833}]}, {"text": "We categorize features into task-dependent and task-independent ones and evaluate the difference in grading accuracy between the corresponding models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.7870147228240967}]}, {"text": "We find that the task-independent models show a better performance for both languages tested, but the resulting losses are relatively high in general.", "labels": [], "entities": []}, {"text": "Moreover, we examine the tasks more closely and group them according to whether they offer a textual source as a reference point.", "labels": [], "entities": []}, {"text": "We show that the transfer works better if the model is derived from the same task type.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe the experimental setup used to examine our research question regarding taskindependent models.", "labels": [], "entities": []}, {"text": "As we want to compare models across tasks, we need datasets that contain different tasks.", "labels": [], "entities": []}, {"text": "English A suitable English dataset is the ASAP essay grading challenge.", "labels": [], "entities": [{"text": "ASAP essay grading challenge", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.5817598551511765}]}, {"text": "The dataset contains eight independent tasks of essay-writing with each about 1,800 graded essays (except the last one with only 723).", "labels": [], "entities": []}, {"text": "The essays were written by students in grade levels between 7 and 10 of a US high-school.", "labels": [], "entities": []}, {"text": "The tasks cover a wide range of different settings and can be grouped on whether they were source-based or not: The source-based tasks have in common that the participants first received a text as input and then had to write an essay that refers to this source.", "labels": [], "entities": []}, {"text": "The following task belong to this group: \u2022 Task 3: Given a source of someone who is traveling by bicycle, students should describe how the environment influences the narrator.", "labels": [], "entities": []}, {"text": "\u2022 Task 4: On the basis of the text 'winter hibiscus' participants should explain why the text ends in a particular way.", "labels": [], "entities": []}, {"text": "\u2022 Task 5: Students were requested to describe the mood of a given memoir.", "labels": [], "entities": []}, {"text": "2 https://www.kaggle.com/c/asap-aes \u2022 Task 6: Based on an excerpt on the construction of the Empire State Building, participants had to describe the obstacles the builders faced.", "labels": [], "entities": [{"text": "Empire State Building", "start_pos": 93, "end_pos": 114, "type": "DATASET", "confidence": 0.8766493201255798}]}, {"text": "The opinion tasks ask for an opinion about a certain topic, but without referring to a specific source text.", "labels": [], "entities": []}, {"text": "\u2022 Task 1: Students should convince readers of a local newspaper of their opinion on the effects computers have on people.", "labels": [], "entities": []}, {"text": "\u2022 Task 2: Participants were asked to write about their opinion on whether certain media should be banned from libraries.", "labels": [], "entities": []}, {"text": "They were prompted to include own experiences.", "labels": [], "entities": []}, {"text": "\u2022 Task 7: Participants should freely write on 'patience'.", "labels": [], "entities": []}, {"text": "They could either write entirely free or about a situation in which they or another person proved patience.", "labels": [], "entities": []}, {"text": "\u2022 Task 8: Participants were told to tell a true story in which laughter was apart.", "labels": [], "entities": []}, {"text": "As the different tasks use different scoring schemes, we use holistic scores and normalize to a scale from 0 to 9 in order to make the trained model exchangeable.", "labels": [], "entities": []}, {"text": "German The German dataset contains two independent tasks each with 197 and 196 annotated essays.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.7985460460186005}]}, {"text": "The essays were written by first-year university students of degree programs for future teachers.", "labels": [], "entities": []}, {"text": "Both writing tasks had in common that the participants first received a text as an input.", "labels": [], "entities": []}, {"text": "After reading the given text they were supposed to write an essay by summarizing the argumentative structure of the text.", "labels": [], "entities": []}, {"text": "However, students were also asked to include their own pro and contra arguments.", "labels": [], "entities": []}, {"text": "\u2022 T1: Students were requested to summarize and to discuss a newspaper article of a national German newspaper which deals with an educational topic.", "labels": [], "entities": [{"text": "summarize", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.9676290154457092}]}, {"text": "\u2022 T2: Participants were asked to summarize and to discuss a newspaper article of a national German newspaper which focusses on the quality of contributions in the participatory media.", "labels": [], "entities": []}, {"text": "Again, we use the holistic scores.", "labels": [], "entities": []}, {"text": "No normalization was necessary as both tasks use the same 6-point scoring scale.: List of features grouped into strongly and weakly task-dependent.", "labels": [], "entities": []}, {"text": "Source-based features (marked with a *) are not used in our experiments.", "labels": [], "entities": []}, {"text": "Following the recommendation of the ASAP challenge, we use as evaluation metric quadratic weighted kappa computed as: with O i,j as the number of times one annotator graded j and the other i, with E i,j as the expected grades given a random distribution and with as the weight of the grades.", "labels": [], "entities": [{"text": "ASAP challenge", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.5325040221214294}]}, {"text": "The metric produces a value for the agreement between the human gold standard and the machine grading.", "labels": [], "entities": []}, {"text": "The preprocessing was realized with the DKPro Core 1.7.0 components used within DKPro TC: BreakIterator, TreeTagger, SnowballStemmer and StanfordParser.", "labels": [], "entities": [{"text": "DKPro Core 1.7.0", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.945226271947225}, {"text": "DKPro TC", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9194231927394867}, {"text": "SnowballStemmer", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.9199361801147461}, {"text": "StanfordParser", "start_pos": 137, "end_pos": 151, "type": "DATASET", "confidence": 0.9674435257911682}]}, {"text": "We now examine the task-adaptivity of models by training on one task and testing on another, and then compare the result to the baseline established above.", "labels": [], "entities": []}, {"text": "shows the resulting loss in performance for the full model.", "labels": [], "entities": []}, {"text": "The table rows represent the tasks on which the model has been trained and the columns the tasks on which the trained model was tested.", "labels": [], "entities": []}, {"text": "The average loss overall model transfers is .42, which shows that the full models do notwork very well when transferred to another task.", "labels": [], "entities": []}, {"text": "6 For most cases, the observed behavior is symmetric, i.e. we see a similar drop when training on task 5 and testing on 4 or training on 4 and testing on 5.", "labels": [], "entities": []}, {"text": "Though there are some remarkable exceptions.", "labels": [], "entities": []}, {"text": "The model  trained onset 1 performs even better onset 2 than its own model, while training onset 2 and testing onset 1 results in a .4 drop.", "labels": [], "entities": []}, {"text": "In addition, all source-based models (1, 2, and 7) work quite well as models for set 8 -the drop is only about .1 in all those cases.", "labels": [], "entities": []}, {"text": "However, set 8 has relatively little training data so that this might be rather an effect of the other models being generally of higher quality than a task transfer effect.", "labels": [], "entities": []}, {"text": "The same procedure was carried out for the model with the reduced feature set that excludes taskdependent features.", "labels": [], "entities": []}, {"text": "The results are shown in table 3.", "labels": [], "entities": []}, {"text": "We see that the average loss is reduced (.36 compared to .42 for the full model) which is inline with our hypothesis that the reduced feature set should transfer better between tasks.", "labels": [], "entities": []}, {"text": "However, the effect is not very strong when averaged overall tasks.", "labels": [], "entities": []}, {"text": "We also observe noticeable difference in the transferability between the groups (source-based vs. opinion tasks).", "labels": [], "entities": []}, {"text": "Looking only within the sourcebased tasks the loss falls between +.03 and -.41, while for training on the opinion tasks and yields much higher losses (from -.37 to -.57 ).", "labels": [], "entities": []}, {"text": "The same: Average loss of reduced model by task type effect can be found for the opinion tasks (with the exceptions of set 7).", "labels": [], "entities": []}, {"text": "In order to better seethe difference, we show the average loss for each group in table 4.", "labels": [], "entities": []}, {"text": "It is obvious that a transfer within source-based or opinion tasks works much better than across the groups.", "labels": [], "entities": []}, {"text": "Within a group, the loss is only half as big as between groups.", "labels": [], "entities": []}, {"text": "We perform the same set of experiments on the German data set.", "labels": [], "entities": [{"text": "German data set", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.9557435711224874}]}, {"text": "The results of the full model are shown in table 5a and the results of the reduced model are shown in figure 5b.", "labels": [], "entities": []}, {"text": "Again the losses of the reduced model are much smaller than of the full model confirming our results on the English dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 108, "end_pos": 123, "type": "DATASET", "confidence": 0.9335807263851166}]}], "tableCaptions": [{"text": " Table 3: Loss of the reduced models compared with using the tasks own model (loss >-0.3 highlighted)", "labels": [], "entities": []}]}