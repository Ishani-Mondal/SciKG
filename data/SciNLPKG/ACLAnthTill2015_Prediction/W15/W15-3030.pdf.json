{"title": [], "abstractContent": [{"text": "The log-linear combination of different features is an important component of SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9963892698287964}]}, {"text": "It allows for the easy in-tegartion of models into the system and is used during decoding as well as for n-best list rescoring.", "labels": [], "entities": []}, {"text": "With the recent success of more complex models like neural network-based translation models, n-best list rescoring attracts again more attention.", "labels": [], "entities": [{"text": "n-best list rescoring", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.5604648192723592}]}, {"text": "In this work, we present anew technique to train the log-linear model based on the ListNet algorithm.", "labels": [], "entities": []}, {"text": "This technique scales to many features, considers the whole list and not single entries during learning and can also be applied to more complex models than a log-linear combination.", "labels": [], "entities": []}, {"text": "Using the new learning approach, we improve the translation quality of a large-scale system by 0.8 BLEU points during rescoring and generate translations which are up to 0.3 BLEU points better than other learning techniques such as MERT or MIRA.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9980383515357971}, {"text": "BLEU", "start_pos": 174, "end_pos": 178, "type": "METRIC", "confidence": 0.9954991936683655}]}], "introductionContent": [{"text": "Nowadays, statistical machine translation is the most promising approach to translate from one natural language into another one, when sufficient training data is available.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7346577048301697}]}, {"text": "While there are several powerful approaches to model the translation process, nearly all of them rely on a log-linear combination of different models.", "labels": [], "entities": []}, {"text": "This approach allows the system an easy integration of additional models into the translation process and therefore a great flexibility to address the various issues and the different language pairs.", "labels": [], "entities": []}, {"text": "The log-linear model is used during decoding and for n-best list rescoring.", "labels": [], "entities": []}, {"text": "Recently, the success of rich but computationally complex models, such as neural network based translation models (), leads to an increased interest in rescoring.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 152, "end_pos": 161, "type": "TASK", "confidence": 0.9726086854934692}]}, {"text": "It was shown that the n-best list rescoring is an easy and efficient way to integrate complex models.", "labels": [], "entities": []}, {"text": "From a machine learning perspective the loglinear model is used to solve a ranking problem.", "labels": [], "entities": []}, {"text": "Given a list of candidates associated with different features, we need to find the best ranking according to a reference ranking.", "labels": [], "entities": []}, {"text": "In machine translation, this ranking is, for example, given by an automatic evaluation metric.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7592423260211945}]}, {"text": "One promising approach for this type of problems is the ListNet algorithm, which has already been applied successfully to the information retrieval task.", "labels": [], "entities": [{"text": "information retrieval task", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.8648298184076945}]}, {"text": "Using this algorithm it is possible to train many features.", "labels": [], "entities": []}, {"text": "In contrast to other algorithms, which work only on single pairs of entries, it considers the whole list during learning.", "labels": [], "entities": []}, {"text": "Furthermore, in addition to train the weights of a linear combination, it can be used for more complex models such as neural networks.", "labels": [], "entities": []}, {"text": "In this paper, we present an adaptation of this algorithm to the task of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8068786263465881}]}, {"text": "Therefore, we investigate different methods to normalize the features and adapt the algorithm to directly optimize a machine translation metric.", "labels": [], "entities": []}, {"text": "We used the algorithm to train a rescoring model and compared it to several existing training algorithms.", "labels": [], "entities": []}, {"text": "In the following section, we first review the related work.", "labels": [], "entities": []}, {"text": "Afterwards, we introduce the ListNet algorithm in Section 3.", "labels": [], "entities": []}, {"text": "The adaptation to the problem of rescoring machine translation n-best lists will be described in the next section.", "labels": [], "entities": [{"text": "rescoring machine translation n-best lists", "start_pos": 33, "end_pos": 75, "type": "TASK", "confidence": 0.8295652985572814}]}, {"text": "Finally, we will present the results on different language pairs and domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed approach is evaluated in two widely known translation tasks.", "labels": [], "entities": []}, {"text": "The first is the large scale translation task of WMT 2015 for the GermanEnglish language pair in both directions.", "labels": [], "entities": [{"text": "translation task of WMT 2015", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.6318465590476989}, {"text": "GermanEnglish language pair", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.9234989484151205}]}, {"text": "The second is the task of translating English TED lectures into German using the data from the IWSLT 2015 evaluation campaign ().", "labels": [], "entities": [{"text": "translating English TED lectures", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.8594242185354233}, {"text": "IWSLT 2015 evaluation campaign", "start_pos": 95, "end_pos": 125, "type": "DATASET", "confidence": 0.8897470235824585}]}, {"text": "The systems using the ListNet-based rescoring were submitted to this evaluation campaigns and when evaluated using the BLEU score they were all ranking within the top 3.", "labels": [], "entities": [{"text": "ListNet-based rescoring", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.8820410072803497}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9972077012062073}]}, {"text": "Before discussing the results, we summarize the translation systems used for experiments along with the additionnal features that rely on continuous space translation models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WMT Results for English to German", "labels": [], "entities": [{"text": "WMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5259926319122314}]}, {"text": " Table 3: TED Results for English to German", "labels": [], "entities": [{"text": "TED", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9286657571792603}]}]}