{"title": [{"text": "Parsing Chinese with a Generalized Categorial Grammar", "labels": [], "entities": [{"text": "Parsing Chinese", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8803776204586029}]}], "abstractContent": [{"text": "Categorial grammars are attractive because they have a clear account of unbounded dependencies.", "labels": [], "entities": []}, {"text": "This accounting is especially important in Mandarin Chi-nese which makes extensive usage of unbounded dependencies.", "labels": [], "entities": []}, {"text": "However, parsers trained on existing categorial grammar annotations (Tse and Curran, 2010) extracted from the Penn Chinese Treebank (Xue et al., 2005) are not as accurate as those trained on the original treebank, possibly because enforcing a small set of inference rules in these grammars leads to large sets of categories, which cause sparse data problems.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 110, "end_pos": 131, "type": "DATASET", "confidence": 0.9858701427777609}]}, {"text": "This work reannotates the Penn Chinese Treebank into a generalized categorial grammar which uses a larger rule set and a substantially smaller category set while retaining the capacity to model unbounded dependencies.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.9886645476023356}]}, {"text": "Experimental results show a statistically significant improvement in parsing accuracy with this categorial grammar.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9713412523269653}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.977663516998291}]}], "introductionContent": [{"text": "Categorial grammar annotations are attractive because they have a transparent syntactic-semantic interface and provide a natural account of traces (.", "labels": [], "entities": []}, {"text": "This is especially important in parsing Chinese, which generates 1.5 times as many traces as English and makes heavy use of unbounded dependencies (.", "labels": [], "entities": [{"text": "parsing Chinese", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.9028958976268768}]}, {"text": "Unfortunately, the accuracy of parsers trained on existing categorial grammar reannotations (Chinese CCGbank; of the Penn Chinese Treebank () is much lower than that of parsers trained on the original.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9995438456535339}, {"text": "Chinese CCGbank", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.8786030411720276}, {"text": "Penn Chinese Treebank", "start_pos": 117, "end_pos": 138, "type": "DATASET", "confidence": 0.870337982972463}]}, {"text": "This maybe because previous attempts used Combinatory Categorial Grammar, which is strongly lexicalized, using a small set of language-independent rules and consequently a large set of language-dependent categories.", "labels": [], "entities": []}, {"text": "This strong lexicalization may contribute to sparse data problems.", "labels": [], "entities": []}, {"text": "This work reannotates the Penn Chinese Treebank into a 'moderately lexicalized' generalized categorial grammar, similar to that defined for English by, which uses a larger set of language-specific inference rules and a substantially smaller category set.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.988874097665151}]}, {"text": "Experimental results show a statistically significant gain in parsing accuracy from this moderately lexicalized grammar over parsing with a strongly lexicalized CCG.", "labels": [], "entities": [{"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9717190861701965}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9799835085868835}]}], "datasetContent": [{"text": "We use a set of reannotation rules similar to those described by to reannotate the Penn Chinese Treebank into GCG trees.", "labels": [], "entities": [{"text": "Penn Chinese Treebank into GCG trees", "start_pos": 83, "end_pos": 119, "type": "DATASET", "confidence": 0.9097797175248464}]}, {"text": "These reannotation rules work within a perl script that traverses each bracketed sentence in the Penn Chinese Treebank by selecting each pair of matching brackets from the top of the tree to the bottom, then running a sed-like pattern substitution rule on each selection.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 97, "end_pos": 118, "type": "DATASET", "confidence": 0.9871541261672974}]}, {"text": "With around 200 annotation rules, we currently fully annotate 71% of sentences (18,505 sentences out of 26,062) from the Penn Chinese Treebank 5 and 6.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5 and 6", "start_pos": 121, "end_pos": 150, "type": "DATASET", "confidence": 0.9768007298310598}]}, {"text": "In order to evaluate the Chinese GCG annotations in terms of parsing accuracy, we compare the parsing performance of a latent-variable parser trained on Chinese GCG annotations with that of the same parser trained on Chinese CCG annotations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9639859795570374}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9176080822944641}]}, {"text": "The Chinese CCGbank is obtained by converting the Penn Chinese Treebank into CCG annotations according to.", "labels": [], "entities": [{"text": "Chinese CCGbank", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9252849817276001}, {"text": "Penn Chinese Treebank", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.990869402885437}]}, {"text": "We divided the fully annotated sentences in both grammars into training, development and test sections according to the section divisions suggested by.", "labels": [], "entities": []}, {"text": "In order to have a better understanding of how the parsing performance changes with the size of the training data, we trained the Chinese CCG parser on both the full training set (ccg.full) and the same training set used for training the Chinese GCG parser (ccg.same).", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9631443619728088}, {"text": "Chinese CCG parser", "start_pos": 130, "end_pos": 148, "type": "DATASET", "confidence": 0.8901763161023458}]}, {"text": "The detailed section divisions are shown in.", "labels": [], "entities": []}, {"text": "For the two CCG parsers, ccg.full and ccg.same, we use the Petrov and Klein (2007) latent variable PCFG trainer, with 5 split-merge cycles, which is the best setting indicated by.", "labels": [], "entities": []}, {"text": "As with CCG, we ran the Petrov et al. show the parsing performance of the parsers on the development and test sets.", "labels": [], "entities": [{"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9630998969078064}]}, {"text": "The parsing results show that a larger training set is beneficial to the parsing performance of the Chinese CCG parer; the parsing performance of the CCG parser trained on the full training set performs consistently better than the parser trained on 71% of the training set.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.967896044254303}, {"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9786645174026489}]}, {"text": "The GCG parser, trained on 71% of the training set, seems to parse reasonably well even compared with the CCG parser trained on the full training set.", "labels": [], "entities": [{"text": "GCG", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7256104946136475}]}, {"text": "It is worth noting that the GCG parser is much higher in tagging accuracy than the CCG parser, which supports our hypothesis that the CCG parser might suffer from sparse data problems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9665506482124329}]}, {"text": "However, direct comparison of the parsing performance of these two parsers is not fair because these two grammars define different categories and different tree structures.", "labels": [], "entities": []}, {"text": "In order to ensure a fair comparison between these grammars, it is necessary to have them produce exactly the same target representation.", "labels": [], "entities": []}, {"text": "In this experiment, we test the parsing performance of these two grammars on a common test set of sentences to which the two grammars assign the same tree structure when syntactic labels and unary branches are removed, see.", "labels": [], "entities": []}, {"text": "We found 984 sentences in the test set which have exactly the same unlabeled binary structures) in both grammars.", "labels": [], "entities": []}, {"text": "moving unary branches, the parses have exclusively binary tree structures and have identical results for precision, recall and F1 in parsing evaluations.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9996676445007324}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9994582533836365}, {"text": "F1", "start_pos": 127, "end_pos": 129, "type": "METRIC", "confidence": 0.999452531337738}]}, {"text": "Since both grammars predict exactly the same binary tree structures with exactly the same ('X') categories, significance testing is performed on these predictions using bootstrap resampling.", "labels": [], "entities": []}, {"text": "Results in show that the parsing performance of the parser trained on the GCG-reannotated corpus is more accurate with strong significance (p < 0.001) than the same parser trained on the CCG-reannotated corpus of the same size.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.969484806060791}, {"text": "GCG-reannotated corpus", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9756436347961426}, {"text": "CCG-reannotated corpus", "start_pos": 187, "end_pos": 209, "type": "DATASET", "confidence": 0.8980956375598907}]}, {"text": "We observe a significant improvement (p < 0.05) of the GCG parser over the CCG parser trained on the full training set.", "labels": [], "entities": []}, {"text": "We believe that the Chinese CCG parser suffers from data sparsity effects.", "labels": [], "entities": [{"text": "Chinese CCG parser", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.8221991062164307}]}, {"text": "Excluding those words which are only associated with one preterminal category, the lexical-categorial confusion rate is 3.45 for the Chinese CCG annotations and 2.59 for the Chinese GCG annotations, which is also reflected in the large gap (more than 5 points) between their tagging accuracy.", "labels": [], "entities": [{"text": "confusion rate", "start_pos": 102, "end_pos": 116, "type": "METRIC", "confidence": 0.9079536199569702}, {"text": "Chinese GCG annotations", "start_pos": 174, "end_pos": 197, "type": "DATASET", "confidence": 0.7041693528493246}]}, {"text": "Enforcing a small set of language-independent inference rules in the Chinese CCG-annotations might have some formal appeal, but it leads to a large set of syntactic categories, many of which, such as nominal or adverbial modifiers, are syntactically or semantically indistinguishable.", "labels": [], "entities": []}, {"text": "Since the GCG described in this paper uses a larger set of inference rules and consequently fewer category labels, it suffers fewer sparse data effects.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parsing results on the development set", "labels": [], "entities": []}, {"text": " Table 3: Parsing results on the test set", "labels": [], "entities": []}, {"text": " Table 4: Parsing results, error reduction ratios and  significance testing results on the common test set  of NoUnary+NoLab trees.", "labels": [], "entities": [{"text": "error reduction ratios", "start_pos": 27, "end_pos": 49, "type": "METRIC", "confidence": 0.8075629671414694}, {"text": "NoUnary+NoLab trees", "start_pos": 111, "end_pos": 130, "type": "DATASET", "confidence": 0.8222719728946686}]}]}