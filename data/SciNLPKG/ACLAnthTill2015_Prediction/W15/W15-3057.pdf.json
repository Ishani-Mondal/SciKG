{"title": [{"text": "An Investigation of Machine Translation Evaluation Metrics in Cross-lingual Question Answering", "labels": [], "entities": [{"text": "Machine Translation Evaluation Metrics", "start_pos": 20, "end_pos": 58, "type": "TASK", "confidence": 0.844048336148262}, {"text": "Cross-lingual Question Answering", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.6683171391487122}]}], "abstractContent": [{"text": "Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.873184597492218}]}, {"text": "However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA).", "labels": [], "entities": []}, {"text": "Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accuracy.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8888560056686401}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9210039377212524}]}, {"text": "However, it is not clear whether an MT system that is better for human consumption is also better for CLQA.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9631253480911255}]}, {"text": "In this paper, we investigate the relationship between manual and automatic translation evaluation metrics and CLQA accuracy by creating a data set using both manual and machine translations and perform CLQA using this created data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9647040367126465}]}, {"text": "1 As a result, we find that QA accuracy is closely related with a metric that considers frequency of words, and as a result of manual analysis, we identify 3 factors of translation results that affect CLQA accuracy .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.7695777416229248}, {"text": "accuracy", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.867041289806366}]}], "introductionContent": [{"text": "Question answering (QA) is the task of searching for an answer to question sentences using some variety of information resource.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9411398410797119}]}, {"text": "Generally, documents, web pages, or knowledge bases are used as these information resources.", "labels": [], "entities": []}, {"text": "When the language of the question differs from the language of the information resource, the task is called cross-lingual question answering (CLQA) (.", "labels": [], "entities": [{"text": "cross-lingual question answering", "start_pos": 108, "end_pos": 140, "type": "TASK", "confidence": 0.6059107482433319}]}, {"text": "Machine translation (MT) is one of the most widely used tools to achieve CLQA (.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8918448090553284}]}, {"text": "In the realm of monolingual question answering, recent years have seen a large increase in the use of structured knowledge bases such as Freebase (), as they allow for accurate answering of questions over a variety of topics (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7071626633405685}]}, {"text": "However, knowledge bases are limited to only a few major languages.", "labels": [], "entities": []}, {"text": "Thus, CLQA is particularly important for QA using knowledge bases.", "labels": [], "entities": [{"text": "QA", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9652833938598633}]}, {"text": "In contrast to the CLQA situation, where an MT system is performing translation fora downstream system to consume, in standard translation tasks the consumer of results is a human (.", "labels": [], "entities": []}, {"text": "In this case, it is important to define an evaluation measure which has high correlation with human evaluation, and the field of MT metrics has widely studied which features of MT results are correlated with human evaluation, and how to reflect these features in automatic evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 129, "end_pos": 131, "type": "TASK", "confidence": 0.9771092534065247}]}, {"text": "However, translations which are good for humans may not be suitable for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9164047539234161}]}, {"text": "For example, according to the work of Hyodo and Akiba, a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full sentences on CLQA using documents or web pages, although it is not clear whether these results will apply to more structured QA using knowledge bases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9978269934654236}]}, {"text": "There is also work on optimizing translation to improve CLQA accuracy (, but these methods require a large set of translated questionanswer pairs, which may not be available in many languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.951519250869751}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9516732692718506}]}, {"text": "Correspondingly, it is of interest to investigate which factors of translation output affect CLQA accuracy, which is the first step towards designing MT systems that achieve better accuracy on the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9298150539398193}, {"text": "MT", "start_pos": 150, "end_pos": 152, "type": "TASK", "confidence": 0.9882768988609314}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9870269894599915}]}, {"text": "In this paper, to investigate the influence of translation on CLQA using knowledge bases, we create a QA data set in which each question has been translated both manually and by a number of MT systems.", "labels": [], "entities": [{"text": "QA data set", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.7800193627675375}]}, {"text": "We then perform CLQA using this data set and investigate the relationship between translation evaluation metrics and QA accuracy.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.9015500247478485}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.6676552891731262}]}, {"text": "As a result, we find that QA accuracy is closely related to NIST score, a metric that considers the frequency of words, indicating that proper translation of infrequent words has an important role in CLQA tasks using knowledge bases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8447316884994507}]}, {"text": "In addition, as a result of fine-grained manual analysis, we identify a number of factors of translation results that affect CLQA.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we examine the effect of various features of translation quality on CLQA.", "labels": [], "entities": []}, {"text": "To do so, we use the data sets described in Section 2, and we performed QA with the system described in Section 3.", "labels": [], "entities": [{"text": "QA", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.8009184002876282}]}, {"text": "In the experiments, we suppose a situation in which Japanese question sentences are translated into English and inputted into an English-language QA system.", "labels": [], "entities": []}, {"text": "First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (), WER (), NIST) and RIBES ( and manual evaluation of acceptability ().", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9545483589172363}, {"text": "BLEU+1", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9705923597017924}, {"text": "WER", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9975709319114685}, {"text": "NIST", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.8627534508705139}, {"text": "RIBES", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9945176243782043}]}, {"text": "BLEU+1 BLEU () is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences.", "labels": [], "entities": [{"text": "BLEU+1", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8738219539324442}, {"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.6254288554191589}, {"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.742231696844101}, {"text": "BLEU+1", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9278890490531921}]}, {"text": "It is based on n-gram precision, and the score is from 0 to 1, where 0 is the worst and 1 is the best.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9673687815666199}]}, {"text": "WER Word error rate (WER) is the edit distance between the translation and reference normalized by the sentence length.", "labels": [], "entities": [{"text": "WER Word error rate (WER)", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.7749365483011518}]}, {"text": "The formula of WER is as follows: \u2022 S is the number of substitutions.", "labels": [], "entities": [{"text": "WER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.7886141538619995}]}, {"text": "\u2022 Dis the number of deletions.", "labels": [], "entities": [{"text": "Dis", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9778294563293457}]}, {"text": "\u2022 I is the number of insertions.", "labels": [], "entities": [{"text": "I", "start_pos": 2, "end_pos": 3, "type": "METRIC", "confidence": 0.9751987457275391}]}, {"text": "\u2022 N is the number of word in the reference.", "labels": [], "entities": []}, {"text": "The score is areal number more than 0, and can be over 1 when the length of the output is larger than the reference.", "labels": [], "entities": [{"text": "areal number", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.9599303603172302}]}, {"text": "Like BLEU, WER focuses on matches between words, but is less lenient with regards to word ordering, having a strong performance for linear matches between the two sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9939919114112854}, {"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.8347397446632385}, {"text": "word ordering", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.7377976775169373}]}, {"text": "WER is an error rate, thus lower WER is better.", "labels": [], "entities": [{"text": "WER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9899088144302368}, {"text": "error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9671204388141632}, {"text": "WER", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9980873465538025}]}, {"text": "To adjust direction of axis to match the other measures, we use the value of 1 \u2212 W ER.", "labels": [], "entities": [{"text": "ER", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.8563523888587952}]}, {"text": "RIBES RIBES is a metric based on rank correlation coefficient of word order in the translation and reference, and thus focuses on whether the MT system was able to achieve the correct ordering.", "labels": [], "entities": [{"text": "RIBES RIBES", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.47339335083961487}]}, {"text": "It has been shown effective for the evaluation of language pairs with greatly different structure such as Japanese and English.", "labels": [], "entities": []}, {"text": "The score is from 0 to 1, where 0 is the worst and 1 is the best.", "labels": [], "entities": []}, {"text": "NIST NIST is a metric based on n-gram precision and each n-gram's weight.", "labels": [], "entities": [{"text": "NIST NIST", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9093645215034485}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9540796279907227}]}, {"text": "Rarer n-grams have a higher weight.", "labels": [], "entities": []}, {"text": "Therefore, less frequent words such as content words are given more importance than function words such as \"of,\" \"in,\" and others.", "labels": [], "entities": []}, {"text": "The score is areal number more than 0.", "labels": [], "entities": [{"text": "areal number", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.9807933568954468}]}, {"text": "Acceptability Acceptability is a 5-grade manual evaluation metric.", "labels": [], "entities": [{"text": "Acceptability Acceptability", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.8865270018577576}]}, {"text": "It combines aspects of both fluency and adequacy, with levels 1-3 evaluating semantic content, and 3-5 evaluating syntactic correctness.", "labels": [], "entities": []}, {"text": "shows the result of the evaluation for each system.", "labels": [], "entities": []}, {"text": "Note that NIST and Acceptability have been normalized between 0 and 1 by dividing by the highest possible achievable value.", "labels": [], "entities": [{"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.7297273278236389}, {"text": "Acceptability", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9647165536880493}]}, {"text": "From this, we can see that HT has the best score on all metrics.", "labels": [], "entities": [{"text": "HT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.6446536183357239}]}, {"text": "Indicating that human translation is still more accurate than machines in this language pair and task.", "labels": [], "entities": [{"text": "human translation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.6453633010387421}]}, {"text": "Next comes commercial systems, with GT being the 2nd best on BLEU and NIST, while YT is higher than GT on RIBES and manual evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.961133599281311}, {"text": "NIST", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.9510074853897095}, {"text": "YT", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9679131507873535}, {"text": "RIBES", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.7114589810371399}]}, {"text": "This confirms previous reports) that RIBES is well correlated with human judgments of acceptability for Japanese-English translation tasks.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9226886034011841}, {"text": "Japanese-English translation tasks", "start_pos": 104, "end_pos": 138, "type": "TASK", "confidence": 0.7161700626214346}]}, {"text": "In the next section, we examine whether this observation also holds when it is not a human but a computer doing the language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.6959176659584045}]}], "tableCaptions": []}