{"title": [], "abstractContent": [{"text": "We describe the creation of anew Dan-ish resource for automated coarse-grained word sense disambiguation of running text (supersense tagging, SST).", "labels": [], "entities": [{"text": "automated coarse-grained word sense disambiguation of running text", "start_pos": 54, "end_pos": 120, "type": "TASK", "confidence": 0.7448313646018505}]}, {"text": "Based on corpus evidence we expand the sense inventory to incorporate new lexical classes.", "labels": [], "entities": []}, {"text": "We add tags for verbal satellites like col-locates, particles and reflexive pronouns, to give account for the satellite-framing properties of Danish.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the quality of our expanded sense inventory in terms of variation in F 1 on a state-of-the-art SST system.", "labels": [], "entities": [{"text": "F 1", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9873215556144714}, {"text": "SST", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9591509699821472}]}, {"text": "The SST systems uses type constraints and achieves performance just under the upper bound of inter-annotator agreement.", "labels": [], "entities": [{"text": "SST", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9648301005363464}]}, {"text": "The initial release is a 1,500-sentence corpus covering six gen-res, made available under an open-source license.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supersense tagging is a coarse-grained word sense disambiguation task, which bases its sense inventory on the top level of Princeton Wordnet), taken from lexicographer files.", "labels": [], "entities": [{"text": "Supersense tagging", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7959395051002502}, {"text": "word sense disambiguation task", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.7140287458896637}, {"text": "Princeton Wordnet", "start_pos": 123, "end_pos": 140, "type": "DATASET", "confidence": 0.8987976312637329}]}, {"text": "A supersense is more general than a synset, grouping many related sense distinctions together, while keeping important semantic distinctions.", "labels": [], "entities": []}, {"text": "The smaller number of supersenses (comparable to the size of atypical POS tag set) makes it possible for state-of-the-art taggers to be trained on datasets of moderate size.", "labels": [], "entities": []}, {"text": "Supersense tagging is similar to Named Entity Recognition (NER) in that the labels are comprised within spans of one or more tokens.", "labels": [], "entities": [{"text": "Supersense tagging", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8432823419570923}, {"text": "Named Entity Recognition (NER)", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8026134769121805}]}, {"text": "NER, however, only recognizes a handful of entity types and does not extend beyond nouns, while supersenses maybe defined for all part of speech and permit more granular semantic distinctions.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7764289379119873}]}, {"text": "While coarse-grained semantic types find use in a range of applications, such as information retrieval, question answering (QA), and relation extraction, one of the main intended uses of the annotated corpus is building a semantic concordancer in the style of.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.774156242609024}, {"text": "question answering (QA)", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.8824847340583801}, {"text": "relation extraction", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.880157470703125}]}, {"text": "We base our annotation effort on the set of supersenses derived from Princeton Wordnet, which makes our annotations interoperable across many languages through the already existing linkings to Princeton Wordnet.", "labels": [], "entities": [{"text": "Princeton Wordnet", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9271861910820007}, {"text": "Princeton Wordnet", "start_pos": 193, "end_pos": 210, "type": "DATASET", "confidence": 0.9417560696601868}]}, {"text": "However, we found several cases where the Princeton supersenses made overly broad distinctions that caused large groups of lexemes to be grouped together (e.g. buildings and vehicles falling under the ARTIFACT class).", "labels": [], "entities": []}, {"text": "The original sense inventory comprises a total of 41 senses, spread over 26 noun senses, and 15 verb senses, plus a single \"catch-all\" sense for adjectives, which is grammatically rather than semantically motivated.", "labels": [], "entities": []}, {"text": "Based on lexical data from the corpus-based Danish wordnet, we introduce seven new noun senses, two verb senses, and four adjective senses.", "labels": [], "entities": [{"text": "Danish wordnet", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9018228948116302}]}, {"text": "A complete listing is shown in.", "labels": [], "entities": []}, {"text": "Importantly, these additions do not break compatibility with supersenses, because the extended senses add more granularity to existing senses.", "labels": [], "entities": []}, {"text": "An additional sense can thus always be unambiguously mapped to an original sense.", "labels": [], "entities": []}, {"text": "For instance, a DISEASE is a STATE.", "labels": [], "entities": []}, {"text": "Details about the newly introduced senses are given in Section 2.", "labels": [], "entities": []}, {"text": "After an annotation task, we experiment with SST in order to gauge the quality of automatic supersenses annotations for the aforementioned semantic concordancer.", "labels": [], "entities": [{"text": "SST", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9265666604042053}]}], "datasetContent": [{"text": "In this section we evaluate the performance of the supersense tagging system (SST) against the MFS (most-frequent sense baseline).", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7627522051334381}]}, {"text": "All our systems have been evaluated on 5-fold cross-validation on randomly shuffled sentences.", "labels": [], "entities": []}, {"text": "All results are expressed in terms of micro-averaged F 1 -score.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9437756836414337}]}, {"text": "We have trained and test the data using two variants of the training data: one where the verbal satellites where removed from the annotation replacing them with the O tag, and another where the annotations were kept intact.", "labels": [], "entities": []}, {"text": "We evaluate only on the set of lexical supersenses (adjectives, nouns and verbs).", "labels": [], "entities": []}, {"text": "The goal of this comparison is to establish whether adding the verb-satellite tags penalizes the performance of the system.", "labels": [], "entities": []}, {"text": "This section provides tagwise evaluation in terms of precision (P), recall (R), and F 1 . In addition, we provide the number of tokens (absolute frequency), the number of types, the token-type ratio for each supersense tag in tables 8, 10, 9 and 11.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.936111330986023}, {"text": "recall (R)", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9571976810693741}, {"text": "F 1", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9938744902610779}]}, {"text": "Overall, the prediction of adjective supersenses fares fairly well, however ADJ.ALL makes up a 30% of the annotated adjectives senses, which is too large fora back-off sense.", "labels": [], "entities": []}, {"text": "Also, ADJ.ALL is a low-agreement supersense tag.", "labels": [], "entities": []}, {"text": "A further refinement of the annotation guidelines or an inclusion on an additional supersense-provided that we identify some internal semantic consistency-can reduce the amount of words labeled as ADJ.ALL.: Performance for the 10 most frequent verbs senses.", "labels": [], "entities": []}, {"text": "Overall performance for verbs is worse than for nouns.", "labels": [], "entities": []}, {"text": "Even though there are fewer verbal senses, verbs are more difficult to annotate, as shown by the verb disagreement plot in.: Performance for the 10 most frequent noun senses.", "labels": [], "entities": []}, {"text": "The sense COMMUNICATION is the second most frequent noun sense, yet it fares much worse than that first sense, namely PERSON.", "labels": [], "entities": [{"text": "COMMUNICATION", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.841263473033905}, {"text": "PERSON", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.8805865049362183}]}, {"text": "Even though COMMUNICATION has lower support, its tokentype ratio is higher than the one for PERSON, which should increase F 1 . However, PERSON has a subset of well-defined proper names that are easy to identify automatically given features like capitalization.", "labels": [], "entities": [{"text": "F 1", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9784075915813446}]}, {"text": "For NOUN.COMMUNICATION, out of its 323 examples, 10% of them are hapaxes.", "labels": [], "entities": [{"text": "NOUN.COMMUNICATION", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8213834762573242}]}, {"text": "The VERB.STATIVE class, however, with a 884 examples, is constituted by forms of the verb vaere (to be) in 76%.", "labels": [], "entities": [{"text": "VERB.STATIVE class", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.6742096543312073}]}, {"text": "The low variety of lexical elements makes it an easy-to-predict sense, and yields an F 1 of 78.39, which is very high for word-sense disambiguation tasks.", "labels": [], "entities": [{"text": "F 1", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.993371456861496}, {"text": "word-sense disambiguation tasks", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.7850255767504374}]}, {"text": "The three verbal satellites fare very differently from each other.", "labels": [], "entities": []}, {"text": "The most common tag, COLL, has a very low F 1 (14.35).", "labels": [], "entities": [{"text": "COLL", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9292181730270386}, {"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9955566823482513}]}, {"text": "Besides the already commented factors of number of tokens and token-type ratio, the predictability of these senses is also determined by how many different POS tags they can be applied to: RE-FLPRON is only for pronouns, PARTICLE encompasses prepositions and adverbs, whereas COLL can also contain nouns, verbs, and adjectives.", "labels": [], "entities": [{"text": "RE-FLPRON", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9917068481445312}, {"text": "PARTICLE", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.7477845549583435}]}], "tableCaptions": [{"text": " Table 4: Supersense tagging data sets.", "labels": [], "entities": [{"text": "Supersense tagging", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8912123739719391}]}, {"text": " Table 3: Sense inventory with new senses introduced in this article marked in bold.", "labels": [], "entities": []}, {"text": " Table 8: Performance for adjectives.", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9591496586799622}]}, {"text": " Table 9: Performance for the 10 most frequent  verbs senses.", "labels": [], "entities": []}, {"text": " Table 10: Performance for the 10 most frequent  noun senses.", "labels": [], "entities": []}, {"text": " Table 11: Performance for extended noun and verb  supersenses, and satellites.", "labels": [], "entities": []}]}