{"title": [{"text": "The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT 2015", "labels": [], "entities": [{"text": "Edinburgh/JHU", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8951709071795145}, {"text": "Phrase-based Machine Translation", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.8721932172775269}, {"text": "WMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9312925338745117}]}], "abstractContent": [{"text": "This paper describes the submission of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015).", "labels": [], "entities": [{"text": "shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015)", "start_pos": 108, "end_pos": 210, "type": "TASK", "confidence": 0.7868759947664598}]}, {"text": "We setup phrase-based statistical machine translation systems for all ten language pairs of this year's evaluation campaign, which are English paired with Czech, Finnish, French, German, and Rus-sian in both translation directions.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 9, "end_pos": 53, "type": "TASK", "confidence": 0.5862044245004654}]}, {"text": "Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task 1 are based on the open source Moses toolkit (.", "labels": [], "entities": [{"text": "Edinburgh/JHU", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8995321194330851}, {"text": "phrase-based translation", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.7391875088214874}, {"text": "WMT 2015 shared translation task 1", "start_pos": 80, "end_pos": 114, "type": "TASK", "confidence": 0.785729706287384}]}, {"text": "We built upon Edinburgh's strong baselines from WMT submissions in previous years ( ) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT 2 and EU-BRIDGE 3).", "labels": [], "entities": [{"text": "Edinburgh", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.9327373504638672}, {"text": "WMT submissions", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.4899349808692932}, {"text": "IWSLT", "start_pos": 189, "end_pos": 194, "type": "DATASET", "confidence": 0.7983710169792175}]}, {"text": "We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission.", "labels": [], "entities": [{"text": "Edinburgh/JHU submission", "start_pos": 81, "end_pos": 105, "type": "DATASET", "confidence": 0.9075541943311691}]}, {"text": "Next we give a general system overview with details on our training pipeline and decoder configuration.", "labels": [], "entities": []}, {"text": "We finally present empirical results for the individual language pairs and translation directions.", "labels": [], "entities": []}, {"text": "1 http://www.statmt.org/wmt15/ 2 http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe peculiarities of individual systems and present experimental results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Use of additional feature functions based on Och clusters (see Section 2.3). The last four  lines refer to ablation studies where one of the sets of clustered feature functions is removed from the  comprehensive setup. Note that the word-based feature functions are used in all cases. BLEU scores on  newstest2014 are reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 295, "end_pos": 299, "type": "METRIC", "confidence": 0.9946439266204834}, {"text": "newstest2014", "start_pos": 311, "end_pos": 323, "type": "DATASET", "confidence": 0.9593116641044617}]}, {"text": " Table 2: Comparison of baseline with post- submission experiments on class-based language  models, bilingual LM and NPLM. Note that  for French\u2192English the submitted system was  the same as the baseline (retuned) whilst for  English\u2192French it was the same as the third line  (retrained).", "labels": [], "entities": []}, {"text": " Table 3: Comparison of baseline with post- submission experiments on class-based language  models, bilingual LM and NPLM. Note that the  submitted system for Finnish\u2192English was the  same as the baseline (but retuned).", "labels": [], "entities": []}, {"text": " Table  5:  Experimental  results  for  German\u2192English and English\u2192German. We re- port cased BLEU scores on the newstest2013  and newstest2014 sets. Primary submission  results are highlighted in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.988523542881012}, {"text": "newstest2013", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9531576633453369}, {"text": "newstest2014 sets", "start_pos": 130, "end_pos": 147, "type": "DATASET", "confidence": 0.7963667213916779}]}]}