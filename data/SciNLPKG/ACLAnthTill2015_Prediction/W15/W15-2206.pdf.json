{"title": [{"text": "A Framework for Procedural Text Understanding", "labels": [], "entities": [{"text": "Procedural Text Understanding", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.9192671577135721}]}], "abstractContent": [{"text": "In this paper we propose a framework for procedural text understanding.", "labels": [], "entities": [{"text": "procedural text understanding", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6733645896116892}]}, {"text": "Procedural texts are relatively clear without modality nor dependence on viewpoints, etc. and have many potential applications in artificial intelligence.", "labels": [], "entities": []}, {"text": "Thus they are suitable as the first target of natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6620698173840841}]}, {"text": "As our framework we extend parsing technologies to connect important concepts in a text.", "labels": [], "entities": []}, {"text": "Our framework first tokenizes the input text, a sequence of sentences, then recognizes important concepts like named entity recognition , and finally connect them like a sentence parser but dealing all the concepts in the text at once.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.7256089051564535}]}, {"text": "We tested our framework on cooking recipe texts annotated with a directed acyclic graph as their meaning.", "labels": [], "entities": []}, {"text": "We present experimental results and evaluate our framework.", "labels": [], "entities": []}], "introductionContent": [{"text": "Among many sorts of texts in natural languages, procedural texts are clear and related to the real world.", "labels": [], "entities": []}, {"text": "Thus they are suitable for the first target of natural language understanding (NLU).", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.78050430615743}]}, {"text": "A procedural text is a sequence of sentences describing instructions to create an objector to change an object into a certain state.", "labels": [], "entities": [{"text": "procedural text is a sequence of sentences describing instructions to create an objector to change an object into a certain state", "start_pos": 2, "end_pos": 131, "type": "Description", "confidence": 0.7239429085027604}]}, {"text": "If a computer understands procedural texts, there are potentially tremendous applications: an intelligent search engine for howto texts (), more intelligent computer vision), a work help system teaching the operator what to do the next (, etc.", "labels": [], "entities": []}, {"text": "The general natural language processing (NLP) tries to solve the understanding problem by along * This work was done when the first author was at Kyoto University.", "labels": [], "entities": [{"text": "general natural language processing (NLP", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.6805936445792516}]}, {"text": "series of sub-problems: word identification, partof-speech tagging, parsing, semantic analysis, and soon.", "labels": [], "entities": [{"text": "word identification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8153404593467712}, {"text": "partof-speech tagging", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8184993267059326}, {"text": "semantic analysis", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7852950096130371}, {"text": "soon", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9514725208282471}]}, {"text": "Contrary to this design, in this paper, we propose a concise framework of NLU focusing on procedural texts.", "labels": [], "entities": []}, {"text": "There have been a few attempts at procedural text understanding.", "labels": [], "entities": [{"text": "procedural text understanding", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6296341717243195}]}, {"text": "tried to convert various procedural texts into so-called PT-chart on the background of automatic programming.", "labels": [], "entities": []}, {"text": "proposed a method for interpreting cooking instruction texts (recipes) to schedule two or more recipes.", "labels": [], "entities": [{"text": "interpreting cooking instruction texts (recipes)", "start_pos": 22, "end_pos": 70, "type": "TASK", "confidence": 0.880290653024401}]}, {"text": "Although their definition of understanding was not clear and their approach was based on domain specific heuristic rules, these pioneer works inspired us to tackle a major problem of NLP, text understanding.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.8236589133739471}]}, {"text": "As the meaning representation of a procedural text we adopt a flow graph.", "labels": [], "entities": []}, {"text": "Its vertices are important concepts consisting of word sequences denoting materials, tools, actions, etc.", "labels": [], "entities": []}, {"text": "And its arcs denote relationships among them.", "labels": [], "entities": []}, {"text": "It has a special vertex, root, corresponding to the final product.", "labels": [], "entities": []}, {"text": "The problem which we try to solve in this paper is to convert a procedural text into the appropriate flow graph.", "labels": [], "entities": []}, {"text": "The input of our NLU system is the entire text, but not a single sentence.", "labels": [], "entities": []}, {"text": "Our framework first segments sentences into words (word segmentation; abbreviated to WS hereafter).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7202657461166382}]}, {"text": "This process is only needed for some languages without clear word boundary.", "labels": [], "entities": []}, {"text": "Then we identify concepts in the texts and classify them into some categories (concept identification; abbreviated to CI hereafter).", "labels": [], "entities": [{"text": "concept identification", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7724526226520538}]}, {"text": "And finally we connect them with labeled arcs.", "labels": [], "entities": []}, {"text": "For the first process, WS, we adapt an existing tool to the target domain and achieve an enough high accuracy.", "labels": [], "entities": [{"text": "WS", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9817522764205933}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.996833860874176}]}, {"text": "The second process, CI, can be solved by the named entity recognition (NER) technique given an annotated corpus (training data).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.7806917230288187}]}, {"text": "The major difference is the definition of named entities (NE).", "labels": [], "entities": [{"text": "definition of named entities (NE)", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.6166891838823046}]}, {"text": "Contrary to many other NERs we propose a method that does not require part-of-speech (POS) tags.", "labels": [], "entities": []}, {"text": "This makes our text understanding framework simple.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8518563508987427}]}, {"text": "For the final process we extend a graph-based parsing method to deal with the entire text, a sequence of sentences, at once.", "labels": [], "entities": []}, {"text": "The difference from sentence parsing is that the vertices are concepts but not words and there are words not covered by any concept functioning as clues for the structure.", "labels": [], "entities": [{"text": "sentence parsing", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.755683034658432}]}, {"text": "As a representative of procedural texts, we selected cooking recipes, because there are many available resources not only in the NLP area but in the computer vision (CV) area.", "labels": [], "entities": []}, {"text": "For example, the TACoS dataset (, is a collection of short videos recording fundamental actions in cooking with descriptions written by Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "TACoS dataset", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.76655712723732}, {"text": "Amazon Mechanical Turk", "start_pos": 136, "end_pos": 158, "type": "DATASET", "confidence": 0.8733605146408081}]}, {"text": "Another example, the KUSK dataset (), contains 40 videos recording entire executions (20 recipes by two persons).", "labels": [], "entities": [{"text": "KUSK dataset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9552862048149109}]}, {"text": "The recipes in the KUSK dataset are taken from the r-FG corpus , in which each recipe text is annotated with its \"meaning.\"", "labels": [], "entities": [{"text": "KUSK dataset", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9380734264850616}, {"text": "r-FG corpus", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8578758239746094}]}, {"text": "We tested our framework on recipe texts manually annotated with word boundary information, concepts, and a flow graph.", "labels": [], "entities": []}, {"text": "We compare a naive application of an MST dependency parser and our extension for flow graph estimation.", "labels": [], "entities": [{"text": "MST dependency parser", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.9137018720308939}, {"text": "flow graph estimation", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.7912832697232565}]}, {"text": "We also measure the accuracy at each step with the gold input assuming the perfect preceding steps.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995689988136292}]}, {"text": "Finally we evaluate the full automatic process of building a flow graph from a raw text.", "labels": [], "entities": []}, {"text": "Our result can be a solid baseline for future improvement in the procedural text understanding problem.", "labels": [], "entities": [{"text": "procedural text understanding", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.7812811732292175}]}], "datasetContent": [{"text": "We evaluated our framework on the r-FG corpus described in.", "labels": [], "entities": [{"text": "r-FG corpus", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.7240578681230545}]}, {"text": "We executed 10-fold cross validation for more reliable results.", "labels": [], "entities": []}, {"text": "DAG estimation accuracy is measured by the F-measure of labeled arcs, which is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "DAG estimation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7701866924762726}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9542434215545654}, {"text": "F-measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9962179064750671}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9993923902511597}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9951719641685486}]}, {"text": "Let N sys , N ref , and N int be the number of the estimated arcs, the gold standard arcs, and their intersection, respectively.", "labels": [], "entities": []}, {"text": "Then precision = N int /N sys , recall = N int /N ref , and F-measure = 2N int /(N ref + N sys ).: F-measure of each task and the overall task.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9993022680282593}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9991299510002136}, {"text": "F-measure", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.995983362197876}, {"text": "F-measure", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9797198176383972}]}], "tableCaptions": [{"text": " Table 2: Concept tags with frequencies per recipe.", "labels": [], "entities": []}, {"text": " Table 3: Arc labels with frequencies per recipe.", "labels": [], "entities": []}, {"text": " Table 6: Accuracies of the baseline and proposed  methods.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9923679232597351}]}, {"text": " Table 7: F-measure of each task and the overall task.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9986036419868469}]}]}