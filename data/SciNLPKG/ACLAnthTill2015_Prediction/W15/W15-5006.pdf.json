{"title": [{"text": "KyotoEBMT System Description for the 2nd Workshop on Asian Translation", "labels": [], "entities": [{"text": "KyotoEBMT System Description", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8862987160682678}, {"text": "2nd Workshop on Asian Translation", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.4812574863433838}]}], "abstractContent": [{"text": "This paper introduces the Ky-otoEBMT example-based machine translation framework.", "labels": [], "entities": [{"text": "Ky-otoEBMT example-based machine translation", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.5246483832597733}]}, {"text": "Since last year's workshop we have replaced input trees with forests, improved alignment, added new features, and introduced bilingual neural network reranking.", "labels": [], "entities": [{"text": "alignment", "start_pos": 79, "end_pos": 88, "type": "TASK", "confidence": 0.9357997179031372}, {"text": "bilingual neural network reranking", "start_pos": 125, "end_pos": 159, "type": "TASK", "confidence": 0.6663157194852829}]}, {"text": "The major benefits of our system include online example retrieval and flexible reordering.", "labels": [], "entities": [{"text": "example retrieval", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.6231907904148102}]}, {"text": "We also use syntactic dependency analysis for both source and target languages in the hope of learning how to translate non-local structure.", "labels": [], "entities": [{"text": "syntactic dependency analysis", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6551701525847117}]}, {"text": "The system implementation (this paper refers to version 1.0) is available as open-source.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the KyotoEBMT system used in the 2nd Workshop on Asian Translation.", "labels": [], "entities": [{"text": "KyotoEBMT", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.9234417676925659}, {"text": "2nd Workshop on Asian Translation", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.7019813776016235}]}, {"text": "Our system is a fully-fledged ExampleBased Machine Translation (EBMT) platform making use of both source-language and target-language dependency structure.", "labels": [], "entities": [{"text": "ExampleBased Machine Translation (EBMT)", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.7956476807594299}]}, {"text": "This approach has been explored comparatively less in studies on syntax-based SMT/EBMT, which tend to focus on constituent trees rather than dependency trees, and on tree-to-string rather than tree-to-tree approaches.", "labels": [], "entities": [{"text": "SMT/EBMT", "start_pos": 78, "end_pos": 86, "type": "TASK", "confidence": 0.8346664110819498}]}, {"text": "Furthermore, we employ separate dependency parsers for each language rather than projecting the dependencies from one language to another, as in).", "labels": [], "entities": []}, {"text": "The dependency structure information is used end-to-end: for improving the quality of the alignment of the translation examples, for constraining the translation rule extraction and for guiding the decoding.", "labels": [], "entities": [{"text": "translation rule extraction", "start_pos": 150, "end_pos": 177, "type": "TASK", "confidence": 0.7454946835835775}]}, {"text": "We believe that dependency structure, which considers more than just local context, is important in order to generate fluent and accurate translations of complex sentences across distant language pairs.", "labels": [], "entities": [{"text": "dependency structure", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7616722285747528}]}, {"text": "The experiments described in this paper focus on technical domain translation for Japanese-Chinese and Japanese-English, however our implementation is applicable to any domain and language pair for which there exist parallel sentences and dependency parsers.", "labels": [], "entities": [{"text": "technical domain translation", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6454948683579763}]}, {"text": "A further unique characteristic of our system is that, again contrary to the majority of similar systems, it does not rely on precomputation of translation rules.", "labels": [], "entities": []}, {"text": "Instead it matches each input sentence to the full database of translation examples before extracting translation rules online.", "labels": [], "entities": []}, {"text": "This has the merit of maximizing the information available when creating and combining translation rules, while retaining the ability to produce excellent translations for input sentences similar to an existing translation example.", "labels": [], "entities": []}, {"text": "The system is mostly developed in C++ and is available as open source.", "labels": [], "entities": []}, {"text": "The code and documentation are available from http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.", "labels": [], "entities": []}, {"text": "Experiments are facilitated through the inclusion of an end-to-end experiment management system (EMS) which has been greatly improved in this version.", "labels": [], "entities": []}, {"text": "The framework is simple to use and supports model training with multiple threads or across a cluster.", "labels": [], "entities": []}, {"text": "shows the basic structure of the KyotoEBMT translation pipeline.", "labels": [], "entities": [{"text": "KyotoEBMT translation pipeline", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.738071878751119}]}], "datasetContent": [{"text": "We conducted translation experiments on the four language pairs in the scientific papers subtask: Japanese-English (JA-EN), EnglishJapanese (EN-JA), Japanese-Chinese (JA-ZH) and Chinese-Japanese (ZH-JA).", "labels": [], "entities": []}, {"text": "The proposed system used the following dependency parsers and show below their approximate parsing accuracies (micro-average), which were evaluated by hand on a random subset of sentences from the test data.", "labels": [], "entities": []}, {"text": "The parsers were trained on domains different to those used in the experiments.", "labels": [], "entities": []}, {"text": "\u2022 English: NLParser 6 (92%) \u2022 Japanese: KNP (96%) ( \u2022 Chinese: SKP (88%) For generating input for Nile we used the following constituency parsers: \u2022 English: Berkeley Parser () \u2022 Japanese: Cyklark (Oda et al., 2015) \u2022 Chinese: Berkeley Parser () Forests were created by packing the 200-best dependency parses for Japanese and English, and 50-best parses for Chinese.", "labels": [], "entities": []}, {"text": "shows the results of our proposed system (WAT15) and a comparison with the system from last year (WAT14) (Richardson et al., 2014) and official baseline (phrasebased SMT, for details see).", "labels": [], "entities": [{"text": "WAT15", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.8556531667709351}, {"text": "WAT14", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.8079265356063843}]}, {"text": "We give results for evaluation on the test set after tuning (WAT15, WAT14) and tuning plus reranking (WAT15+Rerank, WAT14+Rerank).", "labels": [], "entities": [{"text": "WAT15", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8298612833023071}]}, {"text": "Tuning was conducted over 10 iterations on the development set using an n-best list of length 500, and we used the 1000-best for reranking.", "labels": [], "entities": []}, {"text": "WAT15+Rerank was the strongest system in our comparison, outperforming the official baseline, non-reranked system (WAT15) and last year's systems in all metrics for all languages, with the minor exception of JA-ZH human evaluation for reranked vs. nonreranked.", "labels": [], "entities": [{"text": "WAT15", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8148500919342041}, {"text": "Rerank", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.4626332223415375}]}], "tableCaptions": [{"text": " Table 1: Official evaluation results for BLEU/RIBES/HUMAN. (NB: Human evaluation scores  of WAT2014 and WAT2015 are not comparable.)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9942060708999634}, {"text": "RIBES", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.47327888011932373}, {"text": "NB", "start_pos": 61, "end_pos": 63, "type": "DATASET", "confidence": 0.6509326696395874}, {"text": "WAT2014", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.882980227470398}, {"text": "WAT2015", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.8413713574409485}]}]}