{"title": [], "abstractContent": [{"text": "In conversation, interlocutors routinely indicate whether something said or done has been processed and integrated.", "labels": [], "entities": []}, {"text": "Such feedback includes backchannels such as 'okay' or 'mhm', the production of a next relevant turn, and repair initiation via clarification requests.", "labels": [], "entities": []}, {"text": "Importantly, such feedback can be produced not only at sentence/turn boundaries, but also sub-sententially.", "labels": [], "entities": []}, {"text": "In this paper, we extend an existing model of incremental semantic processing in dialogue, based around the Dynamic Syntax (DS) grammar framework, to provide a low-level, integrated account of backchannels, clarification requests and their responses; demonstrating that they can be accounted for as part of the core semantic structure-building mechanisms of the grammar, rather than via higher level pragmatic phenomena such as intention recognition, or treatment as an \"unofficial\" part of the conversation.", "labels": [], "entities": [{"text": "intention recognition", "start_pos": 428, "end_pos": 449, "type": "TASK", "confidence": 0.7156502604484558}]}, {"text": "The end result is an incremental model in which words, not turns, are seen as procedures for contextual update and backchannels serve to align participant semantic processing contexts and thus ease the production and interpretation of subsequent conversational actions.", "labels": [], "entities": []}, {"text": "We also show how clarification requests and their following responses and repair can be modelled within the same DS framework, wherein the divergence and realignment effort in participants' semantic processing drives conversations forward.", "labels": [], "entities": []}], "introductionContent": [{"text": "In conversation, interlocutors provide frequent feedback about whether something said can betaken as understood., a crucial point of advancing the joint project of dialogue is establishing that we are sufficiently coordinated to continue, a process called \"grounding\", which uses, for example, backchannel responses (such as 'mm', example 1:2127, or 'yeah') and non-linguistic cues (e.g. nods and smiles).", "labels": [], "entities": []}, {"text": "Alternative responses indicate processing difficulties or lack of coordination and signal a need for clarification or repair (example 1:2112).", "labels": [], "entities": [{"text": "repair", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9480108618736267}]}, {"text": "present a model of contributions in dialogue that consist of both a presentation and an acceptance phase.", "labels": [], "entities": []}, {"text": "In the acceptance phase, listeners can display evidence of understanding at various levels, from continued attention to verbatim repetition, with backchannels being one possible type (see also).", "labels": [], "entities": [{"text": "acceptance phase", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.9073852002620697}]}, {"text": "The acceptance phase can also be used to clear up sources of misunderstanding, as with clarification requests.", "labels": [], "entities": []}, {"text": "develops a formal model of grounding in which grounding acts at the level of individual utterances buildup discourse units, at which level core speech acts are realised through being grounded.", "labels": [], "entities": []}, {"text": "More recently, provides a substantial dialogue model based on embedding the grammar under an utterance processing protocol, modelling updates of interlocutors' information states.", "labels": [], "entities": []}, {"text": "Grounding or clarification processes rely on a notion of * We would like to thank Ruth Kempson and the anonymous IWCS reviewers for their comments.", "labels": [], "entities": [{"text": "IWCS reviewers", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.8796237111091614}]}, {"text": "Eshghi is supported by the EPSRC BABBLE project (grant number EP/M01553X/1) and Hough by the DUEL project funded by the ANR (grant number ANR-13-FRAL-0001) and the DFG (grant number SCHL 845/5-1).", "labels": [], "entities": [{"text": "EPSRC BABBLE project (grant number EP/M01553X/1", "start_pos": 27, "end_pos": 74, "type": "DATASET", "confidence": 0.789821510965174}, {"text": "Hough", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.895707905292511}, {"text": "ANR", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.7694622278213501}, {"text": "DFG (grant number SCHL 845/5-1", "start_pos": 164, "end_pos": 194, "type": "DATASET", "confidence": 0.7264354676008224}]}, {"text": "We thank them for their financial support.", "labels": [], "entities": []}, {"text": "Purver is partially supported by ConCreTe: the project ConCreTe acknowledges the financial support of the Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Commission, under FET grant number \"locutionary proposition\", a linguistic sign, specified with an appropriate illocutionary force through the grammar.", "labels": [], "entities": []}, {"text": "Such propositions become the elements manipulated during the grounding process, resulting in either acceptance in the common ground or the generation of clarification.", "labels": [], "entities": []}, {"text": "Elements providing feedback or repair are assigned lexical entries or construction types that presuppose the derivation of such propositions in the context.", "labels": [], "entities": []}, {"text": "However, importantly, such feedback can be produced sub-sententially and during an ongoing turn (as with the backchannels in (1) lines 2127 and 2129), illustrating the crucially incremental nature of coordination inhuman interaction, without the need to invoke propositional contents at all stages.", "labels": [], "entities": []}, {"text": "( Moreover, inmost dialogue models, grounding or clarification generation introduce a speaker/hearer asymmetry in their representations in that speakers are assumed to be omniscient regarding their own utterances (which seems to preclude backchannels/clarifications addressed to self), as a result of complete, clear plans/intentions guiding their production (e.g..", "labels": [], "entities": []}, {"text": "However, the joint nature of dialogue actions becomes evident in the fact that, besides listenership, turn-managing, understanding, and acceptance backchannel feedback and its elicitation, like any mechanism in interaction, have perlocutionary effects in conversation, post-hoc characterisable both as \"intended\" and \"unintended\" (2): A: John...uh...", "labels": [], "entities": []}, {"text": "yes, John, yeah?", "labels": [], "entities": []}, {"text": "B: mhm, mhm A: he went to the party yest....", "labels": [], "entities": []}, {"text": "B: yeah, yes, ok.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9565034508705139}]}, {"text": "A: He saw your sister with.", "labels": [], "entities": []}, {"text": "B: yes, yes.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9538462162017822}]}, {"text": "A: Will you stop hurrying me along?", "labels": [], "entities": []}, {"text": "I'll tell the story at my own pace.", "labels": [], "entities": []}, {"text": "Similarly, repetitions of phrases/sentences in dialogue serve various feedback functions (both for self and other) besides clarification.", "labels": [], "entities": []}, {"text": "These range from surprise or disbelief indicators to delaying functions, and, like backchannels, their content cannot always be explicated as a full propositional intention without some loss of the impression or effect shared in context (example 3).", "labels": [], "entities": []}, {"text": "Models of grounding have been recently explored in practical dialogue systems.", "labels": [], "entities": []}, {"text": "However, these systems either explore the positioning of backchannels based on low-level features (e.g.; or rely on a notion of feedback that incorporates reasoning about the intentions, mental states or goals of one's interlocutor (e.g.).", "labels": [], "entities": []}, {"text": "The former type of system may allow a dialogue model to sound 'more human', but do not give any insight into why feedback occurs where it does; and, in the latter, full intention recognition requires a level of complexity that corpus studies on repair (), backchannels, and conversational analysis of multiparty dialogue suggest are unnecessary as a prerequisite in natural conversation.", "labels": [], "entities": [{"text": "full intention recognition", "start_pos": 164, "end_pos": 190, "type": "TASK", "confidence": 0.6474786102771759}]}, {"text": "In contrast, under the view pioneered by and, all such phenomena require an account that integrates their linguistic features with their functions in dialogue because of various form-content constraints.", "labels": [], "entities": []}, {"text": "However, even if this is the general perspective assumed here, examples (1)- show that the grammar needs to be able to provide an account of such feedback and repair mechanisms that remains flexible enough to be put to various situated uses without requiring specific propositional contents to be derived for single words or phrases.", "labels": [], "entities": []}, {"text": "This is a significant requirement given that both the content and the scope of a feedback contribution is highly underspecified (as also pointed out by an anonymous IWCS reviewer), rather than determinable through use of fixed constructions.", "labels": [], "entities": [{"text": "IWCS reviewer", "start_pos": 165, "end_pos": 178, "type": "DATASET", "confidence": 0.828783243894577}]}, {"text": "Moreover, backchannels and clarification requests, like anaphora and ellipsis antecedents, can be provided through actions employing various modalities, like eye-gaze, posture, head movement and bodily gestures.", "labels": [], "entities": []}, {"text": "For this reason the grammar model needs to be able to integrate input from various sources without rules that confine such input to linguistic signs (cf Ginzburg (2012) whose locutionary propositions preclude such unification).", "labels": [], "entities": []}, {"text": "In this paper we first outline the formal model of Dynamic Syntax with Type Theory with Records (DS-TTR); we then present an extension to the model and show how it can provide a low-level, semantic model of feedback phenomena, in particular backchannels and clarification interaction, without recourse to dialogue moves/acts or reasoning about intentions.", "labels": [], "entities": []}, {"text": "The model is illustrated using variations on example (4).", "labels": [], "entities": []}, {"text": "(4) Example dialogue from BNC KPY A 1006 Er, the doctor B 1007 Chorlton?", "labels": [], "entities": [{"text": "BNC KPY A 1006", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.7502859979867935}, {"text": "Er", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.604036808013916}]}, {"text": "A 1008 Chorlton, mhm, he examined me, erm, he, he said now they were on about a slide \ud97b\udf59unclear\ud97b\udf59 on my heart.", "labels": [], "entities": [{"text": "A 1008 Chorlton", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.6647730469703674}]}, {"text": "2 Dynamic Syntax and Type Theory with Records (DS-TTR) Dynamic Syntax is an action-based grammar formalism, which models the word-by-word incremental processing of linguistic input.", "labels": [], "entities": []}, {"text": "Unlike many other formalisms, DS models the incremental linear construction of interpretations without recognising an independent level of syntactic representation.", "labels": [], "entities": []}, {"text": "Thus, the output for any given string of words is a purely semantic tree representing its predicate-argument structure; tree nodes correspond to terms in the lambda calculus, decorated with labels expressing their semantic type (e.g. T y(e)) and logical formulae as record types of the Type Theory with Records framework (TTR, see below); beta-reduction determines the type and formula at a mother node from those at its daughters ().", "labels": [], "entities": []}, {"text": "These trees can be partial, containing unsatisfied requirements potentially for any element, for example, node labels (e.g. ?T y(e), a requirement for future development to T y(e)), and contain a pointer, \u2666, labelling the node currently underdevelopment.", "labels": [], "entities": []}, {"text": "Grammaticality is defined as processability in a context: the successful incremental word-by-word construction of a tree with no outstanding requirements (a complete tree) using all information given by the words in a string.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}