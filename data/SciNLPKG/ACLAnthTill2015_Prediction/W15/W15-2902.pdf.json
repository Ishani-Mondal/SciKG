{"title": [{"text": "Sentiment Analysis on Monolingual, Multilingual and Code-Switching Twitter Corpora", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9362115561962128}]}], "abstractContent": [{"text": "We address the problem of performing polarity classification on Twitter over different languages, focusing on English and Spanish, comparing three techniques: (1) a monolingual model which knows the language in which the opinion is written, (2) a monolingual model that acts based on the decision provided by a language identification tool and (3) a multilingual model trained on a multilingual dataset that does not need any language recognition step.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.7337141633033752}]}, {"text": "Results show that multilingual models are even able to outperform the monolingual models on some monolingual sets.", "labels": [], "entities": []}, {"text": "We introduce the first code-switching corpus with sentiment labels, showing the robust-ness of a multilingual approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Noisy social media, such as Twitter, are especially interesting for sentiment analysis (SA) and polarity classification tasks, given the amount of data and their popularity in different countries, where users simultaneously publish opinions about the same topic in different languages).", "labels": [], "entities": [{"text": "sentiment analysis (SA)", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.8852458477020264}, {"text": "polarity classification", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.6925690919160843}]}, {"text": "Some expressions are written in different languages, making the polarity classification harder.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.7624020278453827}]}, {"text": "In this context, handling texts in different languages becomes areal need.", "labels": [], "entities": []}, {"text": "We evaluate three machine learning models, considering Spanish (es), English (en) and its multilingual version, English-Spanish (en-es): 1.", "labels": [], "entities": []}, {"text": "Multilingual approach (en-es model): A model does not need to recognise the language of the text.", "labels": [], "entities": []}, {"text": "The en and es training and development corpora are merged to train an unique en-es sentiment classifier.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed sets of features and models are evaluated on standard monolingual corpora, taking accuracy as the reference metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9991267323493958}]}, {"text": "These monolingual collections are then joined to create a multilingual corpus, which helps us compare the performance of the approaches when tweets come from two different languages.", "labels": [], "entities": []}, {"text": "An evaluation over a codeswitching test set is also carried out.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy (%) on the SemEval 2014", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994087219238281}, {"text": "SemEval 2014", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.7671132981777191}]}, {"text": " Table 3: Accuracy (%) on the TASS test sets", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995875954627991}, {"text": "TASS test", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.7286477386951447}]}, {"text": " Table 4: Accuracy (%) on the multilingual test set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992997646331787}]}, {"text": " Table 5: Accuracy (%) on the code-switching set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993465542793274}]}]}