{"title": [{"text": "Hierarchical Machine Translation With Discontinuous Phrases", "labels": [], "entities": [{"text": "Hierarchical Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6759697695573171}]}], "abstractContent": [{"text": "We present a hierarchical statistical machine translation system which supports discontinuous constituents.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6469459335009257}]}, {"text": "It is based on synchronous linear context-free rewriting systems (SLCFRS), an extension to synchronous context-free grammars in which synchronized non-terminals spank \u2265 1 continuous blocks on either side of the bitext.", "labels": [], "entities": []}, {"text": "This extension beyond context-freeness is motivated by certain complex alignment configurations that are beyond the alignment capacity of current translation models and their relatively frequent occurrence in hand-aligned data.", "labels": [], "entities": []}, {"text": "Our experiments for translating from German to English demonstrate the feasibility of training and decoding with more expressive translation models such as SLCFRS and show a modest improvement over a context-free baseline.", "labels": [], "entities": [{"text": "translating from German to English", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.8799839973449707}, {"text": "SLCFRS", "start_pos": 156, "end_pos": 162, "type": "DATASET", "confidence": 0.7572046518325806}]}], "introductionContent": [{"text": "In statistical machine translation, phrase-based translation models with abeam search decoder () and tree-based models with a CYK decoder represent two prominent types of approaches.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.643439253171285}, {"text": "phrase-based translation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7324022054672241}]}, {"text": "The latter usually employ some form of synchronous context-free grammar (SCFG).", "labels": [], "entities": []}, {"text": "They can be grouped into so-called hierarchical phrase-based models that are formally syntaxbased, such as in, and models where hierarchical units are somehow linguistically motivated, e.g. in and.", "labels": [], "entities": []}, {"text": "The adequacy of all of these models has been questioned, as the space of alignments that they generate is limited.", "labels": [], "entities": []}, {"text": "Inside-out alignments are beyond the alignment capacity of SCFG of rank 2 (henceforth 2-SCFG) and inversion transduc- tion grammar (Wu, 1997), but they can be generated with phrase-based translation models thanks to the reordering component of standard decoders.", "labels": [], "entities": []}, {"text": "Cross-serial discontinuous translation units (CDTU)) and bonbon configurations) in contrast can neither be generated by a phrase-based translation system nor by an SCFG-based one.", "labels": [], "entities": []}, {"text": "It is thereby assumed that a translation unit, the transitive closure of a set of nodes of the bipartite alignment graph, represents minimal translational equivalence, and therefore that an adequate translation grammar formalism should be able to generate each translation unit separately.", "labels": [], "entities": []}, {"text": "The aforementioned problematic alignment configurations are schematically depicted in Figure 1.", "labels": [], "entities": []}, {"text": "Alignment (i) is an inside-out alignment; it is formed by four translation units (a, b, c and d).", "labels": [], "entities": []}, {"text": "CDTUs (ii) and bonbons (iii) each consist of two intertwined discontinuous translation units.", "labels": [], "entities": []}, {"text": "Several studies have investigated the alignment capacity of SCFG-based and phrase-based translation models in different setups.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.6093374192714691}]}, {"text": "For example, find that inside-out alignments occur in 5% of their manually aligned English-Chinese sentence pairs.", "labels": [], "entities": []}, {"text": "In the study of, 9% of the sentence pairs in a Spanish-French data set and 5.5% of the sentence pairs in an English-German data set cannot be generated by a 2-SCFG.", "labels": [], "entities": [{"text": "English-German data set", "start_pos": 108, "end_pos": 131, "type": "DATASET", "confidence": 0.8156583507855734}]}, {"text": "In addition, qualitatively investigate the instances of the complex alignment configurations in the same English-German data set and find that even though some of them are due to annotation errors, most of them are correctly annotated phenomena that one would like to be able to generate when translating.", "labels": [], "entities": [{"text": "English-German data set", "start_pos": 105, "end_pos": 128, "type": "DATASET", "confidence": 0.8144305547078451}]}, {"text": "To be able to induce the alignment configurations in question, more expressive translation models and corresponding decoding algorithms are necessary.", "labels": [], "entities": []}, {"text": "For the phrase-based models, propose a translation model that uses discontinuous phrases and a corresponding beam search decoder.", "labels": [], "entities": []}, {"text": "For tree-based models, a grammar formalism beyond the power of context-free grammar is necessary.", "labels": [], "entities": []}, {"text": "proposes to apply range concatenation grammar; puts forward the idea of using synchronous linear context-free rewriting systems (SLCFRS), a direct extension of SCFG to discontinous constituents.", "labels": [], "entities": [{"text": "range concatenation grammar", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.747793068488439}]}, {"text": "To the best of our knowledge, neither of the two proposals have resulted in an actual machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7035724818706512}]}, {"text": "With this work, we extend the line of research proposed in, and present the first full tree-based statistical machine translation system that allows for discontinuous constituents.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.6509719491004944}]}, {"text": "It is thus able to produce the complex alignment configurations in.", "labels": [], "entities": []}, {"text": "As such, it combines the advantage of being able to learn and generate discontinuous phrases with the benefits of treebased translation models.", "labels": [], "entities": []}, {"text": "Currently, our system is hierarchical phrasebased, i.e. it does not make use of linguistically motivated syntactic annotation.", "labels": [], "entities": []}, {"text": "However, it will be straightforward to transfer methods to integrate linguistic constituency information from the SCFG-based machine translation literature (such as) to our approach.", "labels": [], "entities": [{"text": "SCFG-based machine translation", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.609200656414032}]}, {"text": "This is particularly interesting, since, in the monolingual parsing community, approaches that are able to produce constituency trees with discontinuous constituents have become increasingly popular.", "labels": [], "entities": []}, {"text": "Recently, such parsers have reached a speed with which it would actually be feasible to parse the training set of a machine translation system, which is necessary to train syntactically motivated translation grammars.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7067985385656357}, {"text": "translation grammars", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.8271511197090149}]}, {"text": "In this work, we define a translation model based on SLCFRS, explain the training of a corresponding hierarchical phrase-based grammar, provide details about a corresponding decoder and results of experiments for translating from German to English.", "labels": [], "entities": [{"text": "SLCFRS", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.7034049034118652}, {"text": "translating from German to English", "start_pos": 213, "end_pos": 247, "type": "TASK", "confidence": 0.8497611880302429}]}], "datasetContent": [{"text": "We furthermore performed a manual evaluation inform of a system comparison using our own installation of the Appraise tool.", "labels": [], "entities": [{"text": "Appraise tool", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.7021778225898743}]}, {"text": "We compare the baseline SYS(1,1) against SYS(2,1), the best-performing setup on the test set.", "labels": [], "entities": []}, {"text": "For each of them, we randomly selected one of the four configurations that lead to the reported averaged BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9774383306503296}]}, {"text": "We then selected those translations of the test set where SYS(2,1) uses at least one SLCFRS rule with a discontinuity (95 sentences).", "labels": [], "entities": []}, {"text": "We asked two native speakers of English (e1, e2) with basic knowledge of German to evaluate our test sentences.", "labels": [], "entities": []}, {"text": "They were shown the source sentence, a reference translation, the SYS(1,1) translation and the SYS(2,1) translation.", "labels": [], "entities": []}, {"text": "The latter two were presented anonymized and in random order.", "labels": [], "entities": []}, {"text": "The options for the evaluators were (a) translation A is better than B, (b) translation B is better than A, and (c) translations A and B are of equal quality.", "labels": [], "entities": []}, {"text": "We specifically asked them to use option (c) as rarely as possible.", "labels": [], "entities": []}, {"text": "While our human evaluators do not demonstrate a clear preference for one of the systems, there is, however, a slight preference for the system that uses discontinuous rules (SYS(2,1)).", "labels": [], "entities": []}, {"text": "In spite of the inter-annotator agreement being not very high (Cohen's \u03ba = 0.338), the tendency for SYS(2,1) is also perceivable for the translations for which the evaluators agree in their decisions, see.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Averaged BLEU scores over four tuning  runs; the feat column indicates whether additional  source/target gap degree features have been used", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9261609315872192}, {"text": "feat", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.978158175945282}]}, {"text": " Table 2: Result of the manual system comparison", "labels": [], "entities": []}, {"text": " Table 3: Confusion matrix of the decisions of the  manual evaluation", "labels": [], "entities": []}]}