{"title": [{"text": "Data representation methods and use of mined corpora for Indian language transliteration", "labels": [], "entities": [{"text": "Indian language transliteration", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6327875653902689}]}], "abstractContent": [{"text": "Our NEWS 2015 shared task submission is a PBSMT based transliteration system with the following corpus preprocessing enhancements: (i) addition of word-boundary markers, and (ii) language-independent, overlapping character segmentation.", "labels": [], "entities": [{"text": "NEWS 2015 shared task submission", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.8617647767066956}]}, {"text": "We show that the addition of word-boundary markers improves transliteration accuracy substantially, whereas our overlapping segmentation shows promise in our preliminary analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.951403796672821}]}, {"text": "We also compare transliteration systems trained using manually created corpora with the ones mined from parallel translation corpus for English to Indian language pairs.", "labels": [], "entities": []}, {"text": "We identify the major errors in English to Indian language transliterations by analyzing heat maps of confusion matrices.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Transliteration can be viewed as a problem of transforming a sequence of characters in one alphabet to another.", "labels": [], "entities": [{"text": "Machine Transliteration", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7289724946022034}]}, {"text": "Transliteration can be seen as a special case of the general translation problem between two languages.", "labels": [], "entities": [{"text": "general translation problem", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.72269340356191}]}, {"text": "The primary differences from the general translation problem are: (i) limited vocabulary size, and (ii) simpler grammar with no reordering.", "labels": [], "entities": []}, {"text": "Phrase based statistical machine translation (PB-SMT) is a robust and well-understood technology and can be easily adopted for application to the transliteration problem.", "labels": [], "entities": [{"text": "Phrase based statistical machine translation (PB-SMT)", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7103825584053993}]}, {"text": "Our submission to the NEWS 2015 shared task is a PBSMT system.", "labels": [], "entities": [{"text": "NEWS 2015 shared task", "start_pos": 22, "end_pos": 43, "type": "DATASET", "confidence": 0.8403778225183487}]}, {"text": "Over a baseline PBSMT system, we address two issues: (i) suitable data representation for training, and (ii) parallel transliteration corpus availability.", "labels": [], "entities": []}, {"text": "In many writing systems, the same logical/phonetic symbols can have different character representations depending on whether it occurs in initial, medial or terminal word position.", "labels": [], "entities": []}, {"text": "For instance, Indian scripts have different characters for independent vowels and vowel diacritics.", "labels": [], "entities": []}, {"text": "Independent vowels typically occurs at the beginning of the word, while diacritics occur in medial and terminal positions.", "labels": [], "entities": []}, {"text": "The pronounciation, and hence the transliteration could also depend on the position of the characters.", "labels": [], "entities": []}, {"text": "For instance, the terminal ion in nation would be pronounced differently from initial one in ionize.", "labels": [], "entities": []}, {"text": "PBSMT learning of character sequence mappings is agnostic of the position of the character in the word.", "labels": [], "entities": [{"text": "PBSMT learning of character sequence mappings", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.599274605512619}]}, {"text": "Hence, we explore to transform the data representation to encode position information.", "labels": [], "entities": []}, {"text": "did not report any benefit from such a representation for Chinese-English transliteration.", "labels": [], "entities": []}, {"text": "We investigated if such encoding useful for alphabetic and consonantal scripts as opposed to logographic scripts like Chinese.", "labels": [], "entities": []}, {"text": "It is generally believed that syllabification of the text helps improve transliteration systems.", "labels": [], "entities": []}, {"text": "However, syllabification systems are not available for all languages.", "labels": [], "entities": []}, {"text": "proposed a character-level, overlapping bigram representation in the context of machine translation using transliteration.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7830511331558228}]}, {"text": "We can view this as weak, coarse and language independent syllabification approach.", "labels": [], "entities": []}, {"text": "We explore this overlapping, segmentation approach for the transliteration task.", "labels": [], "entities": []}, {"text": "For many language pairs, parallel transliteration corpora are not publicly available.", "labels": [], "entities": []}, {"text": "However, parallel translation corpora like Europarl ( and ILCI (Jha, 2012) are available for many language pairs.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9843621253967285}, {"text": "ILCI (Jha, 2012)", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.7935245732466379}]}, {"text": "Transliteration corpora mined from such parallel corpora has been shown to be useful for machine translation, cross lingual information retrieval, etc.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.8031100928783417}, {"text": "cross lingual information retrieval", "start_pos": 110, "end_pos": 145, "type": "TASK", "confidence": 0.671075314283371}]}, {"text": "(. In this paper, we make an intrinsic evaluation of the performance of the automatically mined BrahmiNet transliteration corpus ( for transliteration between English and Indian languages.", "labels": [], "entities": [{"text": "BrahmiNet transliteration corpus", "start_pos": 96, "end_pos": 128, "type": "DATASET", "confidence": 0.7816380659739176}, {"text": "transliteration between English and Indian languages", "start_pos": 135, "end_pos": 187, "type": "TASK", "confidence": 0.7814776500066122}]}, {"text": "The BrahmiNet corpus contains transliteration corpora for 110 Indian language pairs mined from the ILCI corpus, a parallel translation corpora of 11 Indian languages.", "labels": [], "entities": [{"text": "BrahmiNet corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9324070513248444}, {"text": "ILCI corpus", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.941003829240799}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 and Section 3 describes our system and experimental setup respectively.", "labels": [], "entities": []}, {"text": "Section 4 discusses the results of various data representation methods and the use of mined corpus respectively.", "labels": [], "entities": []}, {"text": "Section 5 concludes the report.", "labels": [], "entities": []}], "datasetContent": [{"text": "For building the transliteration model with the NEWS 2015 shared task corpus as well as the BrahmiNet corpus, we used 500 word pairs for tuning and the rest for SMT training.", "labels": [], "entities": [{"text": "NEWS 2015 shared task corpus", "start_pos": 48, "end_pos": 76, "type": "DATASET", "confidence": 0.915034818649292}, {"text": "BrahmiNet corpus", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.898077517747879}, {"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9961423277854919}]}, {"text": "The experimental results are reported on the NEWS 2015 development sets in both cases.", "labels": [], "entities": [{"text": "NEWS 2015 development sets", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.9755844324827194}]}, {"text": "The details of the NEWS 2015 shared task datasets are mentioned in shared text report, while the size of the BrahmiNet datasets are listed below:", "labels": [], "entities": [{"text": "NEWS 2015 shared task datasets", "start_pos": 19, "end_pos": 49, "type": "DATASET", "confidence": 0.87847900390625}, {"text": "BrahmiNet datasets", "start_pos": 109, "end_pos": 127, "type": "DATASET", "confidence": 0.939471960067749}]}], "tableCaptions": [{"text": " Table 1: Results on NEWS 2015 development set (in %)", "labels": [], "entities": [{"text": "NEWS 2015 development set", "start_pos": 21, "end_pos": 46, "type": "DATASET", "confidence": 0.8685879409313202}]}, {"text": " Table 3: Results with BrahmiNet training on  NEWS 2105 dev set (in %)", "labels": [], "entities": [{"text": "NEWS 2105 dev set", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.955756276845932}]}]}