{"title": [{"text": "Unsupervised Learning of Meaningful Semantic Classes for Entity Aggregates", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper addresses the task of semantic class learning by introducing anew methodology to identify the set of semantic classes underlying an aggregate of instances (i.e, a set of nominal phrases observed as a particular semantic role in a collection of text documents).", "labels": [], "entities": [{"text": "semantic class learning", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6796898047129313}]}, {"text": "The aim is to identify a set of semantically coherent (i.e., interpretable) and general enough classes capable of accurately describing the full extension that the set of instances is intended to represent.", "labels": [], "entities": []}, {"text": "Thus, the set of learned classes is then used to devise a generative model for entity categorization tasks such as semantic class induction.", "labels": [], "entities": [{"text": "semantic class induction", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.6473765969276428}]}, {"text": "The proposed methods are completely unsupervised and rely on an (unlabeled) open-domain collection of text documents used as the source of background knowledge.", "labels": [], "entities": []}, {"text": "We demonstrate our proposal on a collection of news stories.", "labels": [], "entities": []}, {"text": "Specifically, we model the set of classes underlying the predicate arguments in a Proposition Store built from the news.", "labels": [], "entities": []}, {"text": "The experiments carried out show significant improvements over a (baseline) generative model of entities based on latent classes that is defined by means of Hierarchical Dirichlet Processes.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of identifying semantic classes for words in Natural Language Processing (NLP) has been shown useful to address many text processing tasks, mainly in the context of supervised and semisupervised learning, in which the development of systems suffers from data scarcity.", "labels": [], "entities": [{"text": "identifying semantic classes for words in Natural Language Processing (NLP)", "start_pos": 15, "end_pos": 90, "type": "TASK", "confidence": 0.7026773020625114}]}, {"text": "Although some semantic dictionaries and ontologies do exist such as WordNet or), their coverage is rarely complete, especially for large open classes (e.g., very specific classes of people and objects), and they fail to integrate new knowledge.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9702895283699036}]}, {"text": "Thus, it helps a lotto firstly learn word categories or classes from a large amount of (unlabeled) training data and then to use these categories as features for the supervised tasks.", "labels": [], "entities": []}, {"text": "The general task of semantic class learning, which can be broadly defined as the task of learning classes of words and their instances from text corpora, has been addressed in a variety of forms that correspond to different application scenarios.", "labels": [], "entities": [{"text": "semantic class learning", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.683130701382955}]}, {"text": "Among these forms, we can find two that have been termed as semantic class mining () and semantic class induction (;.", "labels": [], "entities": [{"text": "semantic class mining", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.6604166030883789}, {"text": "semantic class induction", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.618809849023819}]}, {"text": "These have to do respectively with (i) the expansion of (seed) sets of instances labeled with class information (Knowledge Base population), and with (ii) automatic annotation of individual instances with their semantic classes in the context of a particular text.", "labels": [], "entities": []}, {"text": "In our research, we are more focused on the later.", "labels": [], "entities": []}, {"text": "Specifically, we center on the task of providing a collection of instances with class information (what an entity is) in a given textual context, and then to eventually enrich the context with properties inherited from the semantic class.", "labels": [], "entities": []}, {"text": "Thus, an important issue addressed in our work is that of learning a set of meaningful classes to label a collection of instances composing a semantic aggregate.", "labels": [], "entities": []}, {"text": "By meaningful classes, we refer to a set of classes showing the following two properties: \u2022 be a general enough class so that it can represent other entity occurrences in similar contexts, but also \u2022 be a specific enough and coherent class so that it directly reflects the most important entity properties that can be inherited from the textual context in which the instance occurs.", "labels": [], "entities": []}, {"text": "For example, in the context \"x1 throws a touchdown pass\", entity x1 should be assigned with classes entailing football players rather than just a generic class person, and more likely, receive the class quarterback.", "labels": [], "entities": []}, {"text": "With the term semantic aggregate we refer to a set of instances not completely chosen at random, but sharing some contextual relationships (e.g., a set of nominal phrases observed in a given syntactic/semantic relationship with a specific verb in a text corpus).", "labels": [], "entities": []}, {"text": "In this way, this paper proposes anew methodology to identify/learn the set of semantic classes underlying an aggregate of entity instances.", "labels": [], "entities": []}, {"text": "The aim is to learn a set of semantically meaningful classes capable of accurately describing the full extension that the set of entity instances is intended to represent fora posterior application to semantic class induction.", "labels": [], "entities": [{"text": "semantic class induction", "start_pos": 201, "end_pos": 225, "type": "TASK", "confidence": 0.6371578971544901}]}, {"text": "Thus, we also go beyond and propose a generative model of instances based on the set of learned classes for the aggregates to allow the classification of individual occurrences of instances.", "labels": [], "entities": []}, {"text": "We evaluate our proposal from a collection of news.", "labels": [], "entities": []}, {"text": "Specifically, we model the set of semantic classes underlying the predicate arguments in a Proposition Store built from the news texts.", "labels": [], "entities": []}, {"text": "So far, it has said little about the quality of automatically learned classes beyond their coverage; which is traditionally measured when evaluating approaches in application scenarios related to the task of semantic class mining, for expanding seed sets of words.", "labels": [], "entities": [{"text": "semantic class mining", "start_pos": 208, "end_pos": 229, "type": "TASK", "confidence": 0.6561077336470286}]}, {"text": "By taking advantage of the generative method proposed to model instances, we rely on a recently introduced coherence measure to evaluate the coherence of the learned classes to classify instances.", "labels": [], "entities": []}, {"text": "Also, we measure the generalization of instances by means of the likelihood of generating held-aside data.", "labels": [], "entities": []}, {"text": "The experiments carried out show significant improvements over a (baseline) generative model of instances based on latent classes that is defined by means of Hierarchical Dirichlet Processes (HDP) ().", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our proposal, we consider a collection of 30,826 New York Times articles about US football, from which we build two proposition stores: one for training (based on the first 80% of the published articles) and the other one for testing (based on the remainder articles).", "labels": [], "entities": []}, {"text": "The aim was to learn the semantic classes underlying each predicate argument taken as semantic aggregate of instances.", "labels": [], "entities": []}, {"text": "Specifically, documents in the training set were parsed using a standard dependency parser De Marneffe and; together with TARSQI, and after collapsing some syntactic dependencies following;, we select the collection of 1,646,583 propositions corresponding to the top 1500 more frequent verb-based predicates (i.e., about the 90 percent of the total number of propositions in the training) to setup the input proposition store.", "labels": [], "entities": [{"text": "TARSQI", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9313644766807556}]}, {"text": "The same procedure was applied to gather propositions from the test set, but they were held-aside for testing purposes.", "labels": [], "entities": []}, {"text": "We applied our approach to learn the classes underlying each predicate argument from the proposition store used for training, and evaluate the obtained models by conducting two experiments.", "labels": [], "entities": []}, {"text": "In each experiment, we choose to compare the results obtained by our proposal to a baseline produced by applying HDP to infer latent distributions of distributions of instances, instead of using the PTM approach described in Section 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of the classes learned by our approach for some predicate arguments.  Predicate Arg. Classes learned", "labels": [], "entities": [{"text": "Arg", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.7017906308174133}]}, {"text": " Table 3: Averaged values and standard deviation of log-likelihood on the generation of instances for  predicate arguments in the test data.  Method  Avg. likelihood Std. dev.  HDP  -2009.77  169.52  Our approach  -1638.55  95.0238", "labels": [], "entities": [{"text": "Method  Avg. likelihood Std. dev.  HDP  -2009.77", "start_pos": 142, "end_pos": 190, "type": "DATASET", "confidence": 0.685383340716362}]}]}