{"title": [{"text": "Short Answer Grading: When Sorting Helps and When it Doesn't", "labels": [], "entities": [{"text": "Short Answer Grading", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5725132922331492}]}], "abstractContent": [{"text": "Automatic short-answer grading promises improved student feedback at reduced teacher effort both during and after instruction.", "labels": [], "entities": []}, {"text": "Automated grading is, however , controversial in high-stakes testing and complex systems can be difficult to setup by non-experts, especially for frequently changing questions.", "labels": [], "entities": []}, {"text": "We propose a versatile, domain-independent system that assists manual grading by pre-sorting answers according to their similarity to a reference answer.", "labels": [], "entities": []}, {"text": "We show near state-of-the-art performance on the task of automatically grading the answers from CREG (Meurers et al., 2011).", "labels": [], "entities": [{"text": "CREG", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.6943923234939575}]}, {"text": "To evaluate the grader assistance task, we present CSSAG (Computer Science Short Answers in Ger-man), anew corpus of German computer science questions answered by natives and highly-proficient non-natives.", "labels": [], "entities": [{"text": "Computer Science Short Answers in Ger-man)", "start_pos": 58, "end_pos": 100, "type": "TASK", "confidence": 0.5458450743130275}]}, {"text": "On this corpus , we demonstrate the positive influence of answer sorting on the slowest-graded, most complex-to-assess questions.", "labels": [], "entities": [{"text": "answer sorting", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7873865067958832}]}], "introductionContent": [{"text": "Recent research on short-answer prompts has focussed mostly on fully automatically predicting student scores).", "labels": [], "entities": []}, {"text": "While research interest has intensified, central problems in practice remain: On a technical note, teachers need to quickly setup reliable automatic grading for frequently changing questions, which is not always feasible for complex systems.", "labels": [], "entities": []}, {"text": "An even more basic concern is that the use of an automated system in summative testing (which determines pass or fail or the overall grade fora class) may not be compatible with legal constraints and with student and teacher beliefs about fair grading.", "labels": [], "entities": [{"text": "pass", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9867509007453918}]}, {"text": "Another issue with short-answer questions themselves is the objectivity of grading -will two different teachers or even the same teacher on two different days award the same number of points to the same answer?", "labels": [], "entities": []}, {"text": "Mohler, Bunescu and Mihalcea (2011) present results from the preparation of their test corpus where their judges perfectly agreed on a score 58% of the time, with differences of one point (out of five) in another 23% of cases.", "labels": [], "entities": []}, {"text": "This opens a teacher up to justified complaints from students on 19% of questions.", "labels": [], "entities": []}, {"text": "Objective, replicable grading therefore is a big concern in teaching, and of course even more so in summative testing.", "labels": [], "entities": []}, {"text": "It is also one that can be naturally addressed with the help of automated or semi-automated systems.", "labels": [], "entities": []}, {"text": "We believe that short-answer grading in realworld teaching will not profit most from fully automatic grade prediction.", "labels": [], "entities": []}, {"text": "Instead, relatively simple NLP techniques that need little or no domain adaptation to deal with new questions can assist manual grading and both improve objectivity and minimize effort.", "labels": [], "entities": []}, {"text": "We present such a grading assistance tool that presents student answers for manual correction ranked by their similarity to the reference answer (or answers).", "labels": [], "entities": [{"text": "manual correction", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.6758933663368225}]}, {"text": "The intuition is that graders will profit from seeing clearly correct and clearly incorrect answers together.", "labels": [], "entities": []}, {"text": "The similarity scores are computed on the lemma level, so that the system is portable to any other language where a lemmatiser exists.", "labels": [], "entities": []}, {"text": "Since it only relies on the lexical content of student and reference answer, it is completely independent of a question domain.", "labels": [], "entities": []}, {"text": "To further facilitate real-world use, it is packaged as a plugin to the open-source Learning Management System (LMS) Moodle 1 to allow easy use for teachers.", "labels": [], "entities": []}, {"text": "For the purpose of evaluating our system, we in-troduce Computer Science Short Answers in German (CSSAG), anew data set of nine short-answer questions from the Computer Science domain.", "labels": [], "entities": [{"text": "Computer Science Short Answers in German (CSSAG)", "start_pos": 56, "end_pos": 104, "type": "TASK", "confidence": 0.620739589134852}]}, {"text": "Answers were collected from native or near-native speakers and double-annotated (grading conflicts were resolved after annotation by discussion between the annotators).", "labels": [], "entities": []}, {"text": "We report our observations about structural differences between the answers to a native-speaker content matter task (as in CSSAG) and a reading comprehension task that primarily tests language skills (as exemplified by the German standard corpus CREG-1032, Meurers, Ziai, Ott and Kopp (2011)).", "labels": [], "entities": [{"text": "German standard corpus CREG-1032", "start_pos": 223, "end_pos": 255, "type": "DATASET", "confidence": 0.820591926574707}]}, {"text": "We evaluate our system in two ways.", "labels": [], "entities": []}, {"text": "First, we adapt our ranking task to binary classification and perform classic score prediction (as corrector wrong) for the CREG-1032 and CSSAG data sets.", "labels": [], "entities": [{"text": "CREG-1032", "start_pos": 124, "end_pos": 133, "type": "DATASET", "confidence": 0.9651985168457031}, {"text": "CSSAG data sets", "start_pos": 138, "end_pos": 153, "type": "DATASET", "confidence": 0.8789600133895874}]}, {"text": "Our shallow tool approximates the state of the art in binary classification for CREG, with a small drop in performance on CSSAG.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6644829511642456}, {"text": "CSSAG", "start_pos": 122, "end_pos": 127, "type": "DATASET", "confidence": 0.9118571281433105}]}, {"text": "This shows that the similarity scores carry relevant information for predicting human grades.", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9934002161026001}, {"text": "predicting human grades", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.8370762864748637}]}, {"text": "Our second evaluation directly addresses our intended task of grader assistance.", "labels": [], "entities": [{"text": "grader assistance", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.8465880155563354}]}, {"text": "Time and accuracy data from human graders shows that the ranking of student answers is beneficial especially for questions that are very slow to grade, at no reduction in agreement with gold grades.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9803254008293152}]}, {"text": "Further exploration shows that the slow-to-grade questions are worth more points, which indicates that the teacher expects more complex answers.", "labels": [], "entities": []}, {"text": "Higher answer complexity entails more difficult grading.", "labels": [], "entities": []}, {"text": "Presenting the answers to these questions ranked by similarity to the reference answer results in a simulated speedup of more than 10%.", "labels": [], "entities": [{"text": "speedup", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9574459195137024}]}], "datasetContent": [{"text": "Our evaluation is two-fold: Our first experiment establishes that the similarity between student and reference answers does indeed predict humanassigned grades.", "labels": [], "entities": []}, {"text": "We then goon to test the influence of ranking the student answers on grading speed and agreement with the gold grade.", "labels": [], "entities": [{"text": "grading speed", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.706735223531723}, {"text": "agreement", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9917940497398376}]}, {"text": "In Experiment 1, we classify student short answers as corrector incorrect given their similarity to the reference answer.", "labels": [], "entities": []}, {"text": "This is the classical automatic short answer grading task given a two-level scoring regime.", "labels": [], "entities": []}, {"text": "We compare our results against Hahn and Meurers (2012) who report the best results to date on CREG, the German short-answer corpus.", "labels": [], "entities": [{"text": "German short-answer corpus", "start_pos": 104, "end_pos": 130, "type": "DATASET", "confidence": 0.6376947263876597}]}, {"text": "Their system runs a deep semantic analysis to derive underspecified formal semantic representations of the question, student and reference answer and determine information structural focus.", "labels": [], "entities": []}, {"text": "Our second evaluation tests the influence of similarity ranking on grading accuracy and speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9803206920623779}, {"text": "speed", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9930866360664368}]}, {"text": "This is the task for which we designed the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Processing steps in the Grader Assistance system.", "labels": [], "entities": [{"text": "Grader Assistance", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8671404421329498}]}, {"text": " Table 2: Exp. 1: Results for binary classification  by the Grader Assistance system on the CREG- 1032 and CSSAG data sets.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.6184081435203552}, {"text": "CREG- 1032 and CSSAG data sets", "start_pos": 92, "end_pos": 122, "type": "DATASET", "confidence": 0.9073007191930499}]}, {"text": " Table 3: Corpus comparison: Average similarity  of student answers to reference answer in CREG  and CSSAG corpora. CSSAG correct answers by  strict interpretation of points assigned.", "labels": [], "entities": [{"text": "similarity", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.7083463668823242}, {"text": "CREG", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.876692533493042}, {"text": "CSSAG corpora", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.8788306415081024}]}, {"text": " Table 4: Exp. 2: Average grading times in seconds  per condition. Slowest questions gain most from  sorting.", "labels": [], "entities": []}]}