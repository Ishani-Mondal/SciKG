{"title": [{"text": "IITP: Hybrid Approach for Text Normalization in Twitter", "labels": [], "entities": [{"text": "Text Normalization", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7376294434070587}]}], "abstractContent": [{"text": "In this paper we report our work for nor-malization of noisy text in Twitter data.", "labels": [], "entities": []}, {"text": "The method we propose is hybrid in nature that combines machine learning with rules.", "labels": [], "entities": []}, {"text": "In the first step, supervised approach based on conditional random field is developed, and in the second step a set of heuristics rules is applied to the candidate wordforms for the normalization.", "labels": [], "entities": []}, {"text": "The classifier is trained with a set of features which were are derived without the use of any domain-specific feature and/or resource.", "labels": [], "entities": []}, {"text": "The overall system yields the precision, recall and F-measure values of 90.26%, 71.91% and 80.05% respectively for the test dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9998142123222351}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9989804625511169}, {"text": "F-measure", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.998392641544342}]}], "introductionContent": [{"text": "Twitter has seen a phenomenal growth in the number of users during the last few years.", "labels": [], "entities": []}, {"text": "Over 500 million user accounts have been registered with it with approx 302 million active users . Amount of user generated contents over the web would be unarguably enormous i.e. almost 500 million tweets per day 2 . The fact that Twitter data (or tweets) are typically noisy and unstructured in nature are due to several grammatical & spelling mistakes it contain.", "labels": [], "entities": [{"text": "Amount", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9886245727539062}]}, {"text": "The size limitation (constitute upto 140 characters only) is the another prominent reason.", "labels": [], "entities": []}, {"text": "It confines a user to devise different short forms (e.g. 'c u ltr.' for 'see you later.') of a valid word.", "labels": [], "entities": []}, {"text": "Interpreting such forms maybe an easier task fora human being but, is very difficult to build an accurate system for solving any problem related to natural language processing.", "labels": [], "entities": []}, {"text": "At times, user puts extra emphasis by stretching/elongating a valid word to express their feelings.", "labels": [], "entities": []}, {"text": "For example, they often use word like 'yeeessss' to show their happiness, which is a stretched form of 'yes'.", "labels": [], "entities": []}, {"text": "Normalization of noisy text is an important and necessary pre-processing task for building different applications related to text processing.", "labels": [], "entities": [{"text": "Normalization of noisy text", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8622678518295288}]}, {"text": "It is pretty obvious from various studies () that presence of noisy texts makes any natural language processing (NLP) task very tedious to achieve good accuracy levels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.993871808052063}]}, {"text": "The goal of normalization is twofold, i.e. a) identification of candidates for normalization and b) converting the candidate wordforms to the normalized form.", "labels": [], "entities": []}, {"text": "Unlike the general well-formatted corpus, like newswire, it does not always contain noisy text.", "labels": [], "entities": []}, {"text": "Its main sources are normally those platforms on which users have complete freedom to express themselves.", "labels": [], "entities": []}, {"text": "Therefore, user generated tweets are one of the major sources of noisy texts.", "labels": [], "entities": []}, {"text": "In the last couples of years researchers across worldwide are actively working for the normalization of noisy contents of twitter ().", "labels": [], "entities": []}, {"text": "In (Han and Baldwin, 2011), a linear Support Vector Machine (SVM) classifier was trained for detecting ill-formed words, and then performed normalization based on morphophonemic similarity.", "labels": [], "entities": []}, {"text": "Application of edit operations and recurrent neural embedding can be found in (Chrupala, 2014) for text normalization.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8396950960159302}]}, {"text": "Their method learns sequence of edit operations using conditional random field (CRF).", "labels": [], "entities": []}, {"text": "In another work, () investigated the human perspectives of enhanced letter transformation, visual priming and the phonetic similarity for the text normalization.", "labels": [], "entities": [{"text": "letter transformation", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8082865178585052}, {"text": "text normalization", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7167847156524658}]}, {"text": "The use of beam search decoder and finitestate transducers can be seen in ( for the word normalization.", "labels": [], "entities": [{"text": "word normalization", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.6907821148633957}]}, {"text": "These existing works are based on different setups and datasets.", "labels": [], "entities": []}, {"text": "For further advancement of research on text normalization and to provide a common benchmark setup for evaluation, a shared task \"ACL2015 W-NUT: Normalization of Noisy Text in Twitter\" 3 was organized.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7542024850845337}, {"text": "Normalization of Noisy Text in Twitter\"", "start_pos": 144, "end_pos": 183, "type": "TASK", "confidence": 0.5979227636541639}]}, {"text": "The shared task had two variants: constrained mode and unconstrained mode.", "labels": [], "entities": []}, {"text": "We participated only for the constrained mode which did not permit us to use any external resources and/or tools except few that were recommended by the organizers.", "labels": [], "entities": []}, {"text": "In this paper we report our work for normalization.", "labels": [], "entities": [{"text": "normalization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9911872744560242}]}, {"text": "We implemented a hybrid system where machine learning along with rules are utilized to perform the task.", "labels": [], "entities": []}, {"text": "We have exploited lexical and syntactic properties of a tweet as discussed in section 3.1 to derive a feature set for identification of noisy text in the first step.", "labels": [], "entities": [{"text": "identification of noisy text", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.8242602348327637}]}, {"text": "We train Conditional Random Field (CRF) () as a machine learning algorithm to identify the candidate wordforms that need to be normalized.", "labels": [], "entities": []}, {"text": "In second step, we apply some rule based methods (as defined in section 3.2) in order to normalize the wordforms which were identified in first step.", "labels": [], "entities": []}, {"text": "The organization of the paper is as follows.", "labels": [], "entities": []}, {"text": "A brief theoretical discussion on CRF is presented in section 2.", "labels": [], "entities": [{"text": "CRF", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9336736798286438}]}, {"text": "Section 3 discuss about the feature set and methodology used in the proposed work.", "labels": [], "entities": []}, {"text": "Experimental result and analysis can be found in section 4.", "labels": [], "entities": []}, {"text": "We conclude the paper in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In subsequent subsections we discuss the dataset used in the system and evaluation results, respectively.", "labels": [], "entities": []}, {"text": "Conditional Random Field (CRF)() was used as abase learning algorithm in the.", "labels": [], "entities": []}, {"text": "We closely analyze the errors encountered by our system.", "labels": [], "entities": []}, {"text": "We observed that many errors were due to the incorrect identification of the candidates that need to be normalized.", "labels": [], "entities": []}, {"text": "The jumbled words, e.g. 'liek', 'whta' etc. were not properly recognized.", "labels": [], "entities": []}, {"text": "With more accurate identification system we would have achieved better result.", "labels": [], "entities": []}, {"text": "For example in case of 100% noisy text identification, we obtained an increase of 3.75% in our final F-measure.", "labels": [], "entities": [{"text": "text identification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.741116002202034}, {"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9828129410743713}]}, {"text": "For normalization error, our method arguably lags behind in two fronts: a) ambiguities in normalization and b) many-to-one mapping cases.", "labels": [], "entities": [{"text": "normalization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9804762601852417}]}, {"text": "Many of these maybe reduced by careful design of the heuristic rules.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the dataset", "labels": [], "entities": []}, {"text": " Table 2: Result of the proposed system. All values are in %.", "labels": [], "entities": []}]}