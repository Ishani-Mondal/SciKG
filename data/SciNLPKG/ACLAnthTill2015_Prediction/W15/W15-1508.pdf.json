{"title": [{"text": "A Deep Architecture for Non-Projective Dependency Parsing", "labels": [], "entities": [{"text": "Non-Projective Dependency Parsing", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.7050687273343405}]}], "abstractContent": [{"text": "Graph-based dependency parsing algorithms commonly employ features up to third order in an attempt to capture richer syntactic relations.", "labels": [], "entities": [{"text": "Graph-based dependency parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6070098777612051}]}, {"text": "However, each level and each feature combination must be defined manually.", "labels": [], "entities": []}, {"text": "Besides that, input features are usually represented as huge, sparse binary vectors, offering limited generalization.", "labels": [], "entities": []}, {"text": "In this work, we present a deep architecture for dependency parsing based on a convolutional neural network.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8674288988113403}]}, {"text": "It can examine the whole sentence structure before scoring each head/modifier candidate pair, and uses dense embeddings as input.", "labels": [], "entities": []}, {"text": "Our model is still under ongoing work, achieving 91.6% unlabeled attachment score in the Penn Treebank.", "labels": [], "entities": [{"text": "91.6% unlabeled attachment score", "start_pos": 49, "end_pos": 81, "type": "METRIC", "confidence": 0.6688695788383484}, {"text": "Penn Treebank", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9923780262470245}]}], "introductionContent": [{"text": "Graph-based dependency parsing works by assigning scores to each possible dependency arc between two words (plus the root), and then creating a dependency tree by selecting the arcs which yield the highest score sum).", "labels": [], "entities": [{"text": "Graph-based dependency parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6116182804107666}]}, {"text": "The Chu-Liu-Edmonds algorithm is commonly used to extract the maximum spanning tree (MST) of the resulting graph in polynomial time, and inherently allows for non-projective trees.", "labels": [], "entities": []}, {"text": "Most such parsing algorithms obtain the score for an arc from word i to j as the dot product of a weight vector and a vector of binary features, s(i, j) = w \u00b7 f (i, j).", "labels": [], "entities": []}, {"text": "Their training procedure is thus essentially optimizing the weight vector.", "labels": [], "entities": []}, {"text": "The features, however, often follow redundant patterns: the same classifier may use as separate features: (i) headword and its POS tag, (ii) headword, and (iii) headword POS tag.", "labels": [], "entities": []}, {"text": "This is justified first by data sparseness, since a given word may not have been seen many times in the training set (or not with a given POS tag), and the last two features serve as a fallback.", "labels": [], "entities": []}, {"text": "Second, most approaches are based on linear classifiers, which cannot learn complex interactions between features.", "labels": [], "entities": []}, {"text": "Given that the scoring function deals with an arc at a time, graph-based parsers are usually restricted to features of local pairs.", "labels": [], "entities": []}, {"text": "This is problematic when determining the head of a given word depends on its modifiers.", "labels": [], "entities": []}, {"text": "For example, consider the two sentences in, where the preposition with maybe attached to a verb or a noun, depending on its complement.", "labels": [], "entities": []}, {"text": "Including neighboring words as features in the arc scoring function may alleviate the problem, but doesn't account for long range dependencies.", "labels": [], "entities": []}, {"text": "A more efficient solution is second or high order features, which include child or sibling arcs in the scoring function).", "labels": [], "entities": []}, {"text": "Some authors explored higher order features, including, for example, grandparents and grand-siblings ( or non-adjacent siblings).", "labels": [], "entities": []}, {"text": "However, each new level (i.e., each higher order) must be defined through manually designed features.", "labels": [], "entities": []}, {"text": "Furthermore, finding the exact non-projective MST in such cases is computationally intractable, making it necessary to resort to approximate solutions 1 . Another disadvantage of such systems is that fea-He ate spaghetti with a fork He ate spaghetti with meatballs tures are usually binary.", "labels": [], "entities": [{"text": "MST", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9254469275474548}]}, {"text": "Thus, each word in the system vocabulary is represented as a separate, independent feature.", "labels": [], "entities": []}, {"text": "By contrast, a growing trend in the NLP community is to use word embeddings, which are low dimensional, dense vectors representing words.", "labels": [], "entities": []}, {"text": "Word embeddings have the advantage to deliver similar representations to words that tend to occur in the same contexts (and usually have a related meaning), and lower out-ofvocabulary impact.", "labels": [], "entities": []}, {"text": "In this work, we address the limitations described above with a graph-based parser architecture inspired in the SENNA system . It takes word embeddings and POS tags as input, and uses a convolutional neural network that allows it to examine the whole sentence before giving a score for each head-dependent pair.", "labels": [], "entities": []}, {"text": "The complexity of the scoring procedure is O(n 3 ).", "labels": [], "entities": [{"text": "O", "start_pos": 43, "end_pos": 44, "type": "METRIC", "confidence": 0.9942724704742432}]}, {"text": "The remaining of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents relevant related work with dependency parsing, word embeddings and neural architectures.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8595063388347626}]}, {"text": "Section 3 describes our model.", "labels": [], "entities": []}, {"text": "Section 4 shows our experimental setup and results found for English, German and Dutch, and Section 5 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments with English, German and Dutch data.", "labels": [], "entities": []}, {"text": "For English, we used the default Penn Treebank data set, converted to constituency trees to CoNLL dependencies) using the LTH conversion tool We trained on sections 2-21, validated on 22, and tested on section 23.", "labels": [], "entities": [{"text": "Penn Treebank data set", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.9946573078632355}]}, {"text": "We trained and validated models using gold POS tags; for testing, we used a neural network based tagger trained on the default WSJ POS tagging data set (sections 0-18).", "labels": [], "entities": [{"text": "WSJ POS tagging data set", "start_pos": 127, "end_pos": 151, "type": "DATASET", "confidence": 0.8892550706863404}]}, {"text": "For German and Dutch, we used the CoNLL 2006 datasets.", "labels": [], "entities": [{"text": "CoNLL 2006 datasets", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.9766238331794739}]}, {"text": "We chose these two languages because they have the highest rate of non-projective edges among all languages in CoNLL 2006, and one of our method's strengths is precisely finding nonprojective edges as easily as it would find projective ones.", "labels": [], "entities": [{"text": "CoNLL 2006", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.9254327416419983}]}, {"text": "As common practice, we used gold POS tags in training, validating and testing on these languages.", "labels": [], "entities": []}, {"text": "We report results obtained with the English word embedding matrix M word initialized with data from SKIP DEP and Levy and Goldberg  The other matrices were initialized randomly.", "labels": [], "entities": [{"text": "SKIP DEP", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.8817430436611176}]}, {"text": "Since they have a relatively low number of entries, we can expect good embeddings to be obtained during supervised training.", "labels": [], "entities": []}, {"text": "summarizes the adjustable parameters in our model and their values.", "labels": [], "entities": []}, {"text": "SKIP DEP embeddings yielded slightly better accuracy than L&G, but still considerably low when compared to stateof-the-art parsers, which achieve 93.3%, 87.4% and 92.7% UAS on the WSJ, Dutch and German data, respectively ().", "labels": [], "entities": [{"text": "SKIP DEP embeddings", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5031470358371735}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9989370703697205}, {"text": "UAS", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9988353848457336}, {"text": "WSJ, Dutch and German data", "start_pos": 180, "end_pos": 206, "type": "DATASET", "confidence": 0.6340068529049555}]}, {"text": "On the other hand, the first-order parsers from have 91.94%, 84.79% and 90.54% UAS.", "labels": [], "entities": [{"text": "UAS", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9788979291915894}]}, {"text": "Thus, despite our theoretical motivation, our parser's performance is on par with that of first-order models.", "labels": [], "entities": []}, {"text": "This suggests that the simpler, local features commonly used by such models are just as effective as examining the whole sentence before issuing each local decision.", "labels": [], "entities": []}, {"text": "Training time is another drawback, with each epoch in edge detection for the WSJ taking around 4 hours (running on an Intel Xeon E7 2.4 GHz).", "labels": [], "entities": [{"text": "edge detection", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7090016454458237}]}, {"text": "How-6 L&G embeddings originally had 300 dimensions.", "labels": [], "entities": [{"text": "How-6 L&G embeddings", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9207628726959228}]}, {"text": "We applied Principal Component Analysis in order to reduce them to 100.", "labels": [], "entities": [{"text": "Principal Component Analysis", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.5539515316486359}]}, {"text": "The maximum distance is counted separately to the right and to the left.", "labels": [], "entities": [{"text": "distance", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9384769201278687}]}, {"text": "In other words, there are 10 different vectors encoding distance before a head/modifier, and 10 encoding distance after.", "labels": [], "entities": []}, {"text": "Additionally, there is a vector for distance 0 and two for 11 or more, totaling 23 vectors.: Accuracy values ever, as this was preliminary work on evaluating the architecture, we didn't focus on speeding up execution (e.g., using pruning).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9883894920349121}]}, {"text": "On the other hand, memory consumption is low: training uses around 1.5 GB of RAM and running a model needs around 320 MB.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parameter values used in experiments. (U) indi- cates the unlabeled stage, and (L) the labeled one. When  neither is present, the same configuration was used in  both.", "labels": [], "entities": []}]}