{"title": [{"text": "The Impact of Training Data on Automated Short Answer Scoring Performance *", "labels": [], "entities": [{"text": "Automated Short Answer Scoring", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.5027366057038307}]}], "abstractContent": [{"text": "Automatic evaluation of written responses to content-focused assessment items (automated short answer scoring) is a challenging educational application of natural language processing.", "labels": [], "entities": [{"text": "Automatic evaluation of written responses to content-focused assessment items", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.6543349756134881}, {"text": "automated short answer scoring)", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.6199142754077911}, {"text": "natural language processing", "start_pos": 155, "end_pos": 182, "type": "TASK", "confidence": 0.6792117953300476}]}, {"text": "It is often addressed using supervised machine learning by estimating models to predict human scores from detailed linguistic features such as word n-grams.", "labels": [], "entities": []}, {"text": "However, training data (i.e., human-scored responses) can be difficult to acquire.", "labels": [], "entities": []}, {"text": "In this paper, we conduct experiments using scored responses to 44 prompts from 5 diverse datasets in order to better understand how training set size and other factors relate to system performance.", "labels": [], "entities": []}, {"text": "We believe this will help future researchers and practitioners working on short answer scoring to answer practically important questions such as, \"How much training data do I need?\"", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.6380790770053864}]}], "introductionContent": [{"text": "Automated short answer scoring is a challenging educational application of natural language processing that has received considerable attention in recent years, including a SemEval shared task (), a public competition on the Kaggle data science website (https://www.kaggle.com/ c/asap-sas), and various other research papers).", "labels": [], "entities": [{"text": "Automated short answer scoring", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7005530074238777}]}, {"text": "The goal of short answer scoring is to create a predictive model that can take as input a text response to a given prompt (e.g., a question about a reading passage) and produce a score representing the accuracy * Michael Heilman is now a data scientist at Civis Analytics. or correctness of that response.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.5943332513173422}, {"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9985994696617126}, {"text": "correctness", "start_pos": 276, "end_pos": 287, "type": "METRIC", "confidence": 0.9635068774223328}]}, {"text": "One well-known approach is to learn a prompt-specific model using detailed linguistic features such as word n-grams from a large training set of responses that have been previously scored by humans.", "labels": [], "entities": []}, {"text": "This approach works very well when large sets of training data are available, such as in the ASAP 2 competition, where there were thousands of labeled responses per prompt.", "labels": [], "entities": [{"text": "ASAP 2 competition", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7572498917579651}]}, {"text": "However, little work has been done to investigate the extent to which short answer scoring performance depends on the availability of large amounts of training data.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7034733891487122}]}, {"text": "This is important because short answer scoring is different from tasks where one dataset can be used to train models fora wide variety of inputs, such as syntactic parsing.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.6739716430505117}, {"text": "syntactic parsing", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.7304405570030212}]}, {"text": "2 Current short answer scoring approaches depend on having training data for each new prompt.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6774876415729523}]}, {"text": "Here, we investigate the effects on performance of training sample size and a few other factors, in order to help answer extremely practical questions like, \"How much data should I gather and label before deploying automated scoring fora new prompt?\"", "labels": [], "entities": []}, {"text": "Specifically, we explore the following research questions: \u2022 How strong is the association between training sample size and automated scoring performance?", "labels": [], "entities": []}, {"text": "\u2022 If the training set size is doubled, how much improvement in performance should we expect?", "labels": [], "entities": []}, {"text": "\u2022 Are there other factors such as the number of score levels that are strongly associated with performance?", "labels": [], "entities": []}, {"text": "\u2022 Can we create a model to predict scoring model performance from training sample size and other factors (and how confident would we be of its estimates)?", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments using responses to 44 prompts from five different datasets.", "labels": [], "entities": []}, {"text": "The data for each of the 44 prompts was split into a training set and a testing set.", "labels": [], "entities": []}, {"text": "provides an overview of the datasets.", "labels": [], "entities": []}, {"text": "The ASAP 2 dataset is from the 2012 public competition hosted on Kaggle (https:// www.kaggle.com/c/asap-sas) and is publicly available.", "labels": [], "entities": [{"text": "ASAP 2 dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.6980262597401937}, {"text": "Kaggle", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.7842555046081543}]}, {"text": "The Math and Reading 1 datasets were developed as part of the Educational Testing Service's \"Cognitively Based Assessment of, for, and as Learning\" research initiative.", "labels": [], "entities": [{"text": "Math and Reading 1 datasets", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.5919828832149505}, {"text": "Educational Testing Service", "start_pos": 62, "end_pos": 89, "type": "DATASET", "confidence": 0.9316847324371338}]}, {"text": "The Reading 2 dataset was developed as part of the \"Reading for Understanding\" framework ().", "labels": [], "entities": [{"text": "Reading 2 dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8005260427792867}]}, {"text": "The Science dataset was developed and scored as part of the Knowledge Integration framework).", "labels": [], "entities": [{"text": "Science dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9325416684150696}, {"text": "Knowledge Integration", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.6389880478382111}]}, {"text": "Note that only the ASAP 2 dataset is publicly available.", "labels": [], "entities": [{"text": "ASAP 2 dataset", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.8275280992190043}]}, {"text": "For all prompts, there are at least 359 training examples (at most 2,633).", "labels": [], "entities": []}, {"text": "For each prompt, we trained a model on the full training set for that prompt and evaluated on the testing set.", "labels": [], "entities": []}, {"text": "In addition, we trained models from randomly selected subsamples of the training set and evaluated on the full testing set.", "labels": [], "entities": []}, {"text": "Specifically, we created 20 replications of samples (without replacement) of sizes 2 n * 100 (i.e., 100, 200, 400, . .", "labels": [], "entities": []}, {"text": ") up to the full training sample size.", "labels": [], "entities": []}, {"text": "We trained models on these subsamples and evaluated each on the full testing set.", "labels": [], "entities": []}, {"text": "Following the ASAP 2 competition (https://www.kaggle.com/c/asap-sas/ details/evaluation), we evaluated models using quadratically weighted \u03ba.", "labels": [], "entities": [{"text": "ASAP 2", "start_pos": 14, "end_pos": 20, "type": "TASK", "confidence": 0.6647022664546967}]}, {"text": "For subsamples of the training data, we averaged the results across the 20 replications before further analyses.", "labels": [], "entities": []}, {"text": "We used the Fisher Transformation z(\u03ba) when averaging because of its variance-stabilization properties.", "labels": [], "entities": []}, {"text": "The same transformation was also used  by the ASAP 2 competition as part of its official evaluation.", "labels": [], "entities": [{"text": "ASAP 2 competition", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6569499969482422}]}, {"text": "This gives us a dataset of averaged \u03ba values for different combinations of prompts and sample sizes.", "labels": [], "entities": []}, {"text": "For each data point, in addition to the \u03ba value and prompt, we compute the following: \u2022 log2SampleSize: log 2 of the training sample size, Variable r log2SampleSize .550 log2MinSampleSizePerScore .392 meanLog2NumChar -.365 numLevels .033: Pearson's r correlations between training set characteristics and human-machine \u03ba.", "labels": [], "entities": [{"text": "meanLog2NumChar", "start_pos": 201, "end_pos": 216, "type": "METRIC", "confidence": 0.9583377242088318}]}, {"text": "\u2022 log2MinSampleSizePerScore:log 2 of the minimum number of examples fora score level (e.g., log 2 (16) if the least frequent score level in the training sample had 16 examples), \u2022 meanLog2NumChar: The mean, across training sample responses, of log 2 of the number of characters (a measure of response length), \u2022 numLevels: The number of score levels.", "labels": [], "entities": []}, {"text": "For each of these variables, we first compute Pearson's r to measure the association between \u03ba and each variable.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9444337288538615}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Not surprisingly, the variable most strongly associated with performance (i.e., \u03ba) is the log 2 of the number of responses used for training.", "labels": [], "entities": []}, {"text": "However, having a large sample does not ensure high humanmachine agreement: the correlation between \u03ba and log2SampleSize was only r = .550.", "labels": [], "entities": [{"text": "agreement", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.8737764954566956}]}, {"text": "Performance varies considerably across prompts, as illus- trated in.", "labels": [], "entities": []}, {"text": "Next, we tested whether we could predict humanmachine agreement for different size training sets for new prompts.", "labels": [], "entities": []}, {"text": "We used the dataset of \u03ba values for different prompts and training set sizes described above (N = 224).", "labels": [], "entities": []}, {"text": "We iteratively held out each dataset and used it as a test set to evaluate performance of a model trained on the remaining datasets.", "labels": [], "entities": []}, {"text": "For the model, we used a simple ordinary least squares linear regression model, with the variables from as features.", "labels": [], "entities": []}, {"text": "For labels, we used z(\u03ba) instead of \u03ba, and then converted the models predictions back to \u03ba values using the inverse of the z function (Eq. 1).", "labels": [], "entities": []}, {"text": "We report two measures of correlation (Pearson's and Spearman's) and two measures of error (root mean squared error and mean absolute error).", "labels": [], "entities": [{"text": "correlation", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.989702582359314}, {"text": "error", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9944827556610107}, {"text": "root mean squared error", "start_pos": 92, "end_pos": 115, "type": "METRIC", "confidence": 0.7659059911966324}, {"text": "mean absolute error", "start_pos": 120, "end_pos": 139, "type": "METRIC", "confidence": 0.7874508301417033}]}, {"text": "The results are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptions of the datasets. The Response Length column shows the range of average response lengths (in  number of words) across all prompts in a dataset.", "labels": [], "entities": [{"text": "Response Length", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.79811692237854}]}, {"text": " Table 2: Descriptive statistics about performance in terms  of averaged quadratically weighted \u03ba for different train- ing sample sizes (N ), aggregated across all prompts.  \"med.\" = median, \"s.d.\" = standard deviation", "labels": [], "entities": []}, {"text": " Table 4: Results for the predictive model of human- machine \u03ba.", "labels": [], "entities": [{"text": "predictive model of human- machine \u03ba", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.6540435935769763}]}]}