{"title": [{"text": "A Weighted Combination of Text and Image Classifiers for User Gender Inference", "labels": [], "entities": [{"text": "User Gender Inference", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.6594639420509338}]}], "abstractContent": [{"text": "Demographic attribute inference of social networking service (SNS) users is a valuable application for marketing and for targeting advertisements.", "labels": [], "entities": [{"text": "Demographic attribute inference of social networking service (SNS) users", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.85326134074818}]}, {"text": "Several studies have examined Twitter-user gender inference in natural language processing, image recognition, and other research domains.", "labels": [], "entities": [{"text": "Twitter-user gender inference", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.6326939264933268}, {"text": "image recognition", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7175766378641129}]}, {"text": "Reportedly, a combined approach using text data and image data outper-forms an individual data approach.", "labels": [], "entities": []}, {"text": "This paper presents a proposal of a novel hybrid approach.", "labels": [], "entities": []}, {"text": "A salient benefit of our system is that features provided from a text classi-fier and from an image classifier are combined appropriately to infer male or female gender using logistic regression.", "labels": [], "entities": []}, {"text": "The experimentally obtained results demonstrate that our approach markedly improves an existing combination-based method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Concomitantly with rapid growth in SNS, consumers increasingly use SNS to exchange and share their opinions related to products, services, politics, and other matters.", "labels": [], "entities": []}, {"text": "Many companies are motivated to use SNS data for marketing or advertisement to satisfy needs for improvements of their products or services in real time with low cost.", "labels": [], "entities": []}, {"text": "However, in many cases, SNS user information such as gender, age or residence is not openly available, although such information is extremely important for marketing.", "labels": [], "entities": []}, {"text": "To meet that objective, several studies have been conducted to infer demographic information of anonymous users using text or image data posted on Twitter, and community membership).", "labels": [], "entities": []}, {"text": "demonstrated that a hybridbased method outperformed other approaches using individual sources.", "labels": [], "entities": []}, {"text": "However we observed an important issue: each probability score output from the image classifiers and the text classifier was simply summed, although the degree of their respective contributions to the inference is presumably different.", "labels": [], "entities": []}, {"text": "As described herein, considering that issue, we propose a novel method with a hybrid approach using logistic regression.", "labels": [], "entities": []}, {"text": "In addition, from examination of experimentally obtained results, we show which image contents contribute strongly to the inference of a Twitter user being male or female.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a gender classification on Twitter.", "labels": [], "entities": [{"text": "gender classification", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.6610791236162186}]}, {"text": "Experimental data are of two levels: a tweet level and an image level.", "labels": [], "entities": []}, {"text": "We prepared a huge number of annotation data as a training corpus using Yahoo Crowd Sourcing (http://crowdsourcing. yahoo.co.jp/Yahoo).", "labels": [], "entities": []}, {"text": "Tweet level annotation: Tweet level annotation process refers to rules proposed by, who defined the tweet level labels as male and female.", "labels": [], "entities": [{"text": "Tweet level annotation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6518940230210623}]}, {"text": "Workers annotated the labels using many sources of potentially discriminative metadata, including user preferences, icons, text, and images.", "labels": [], "entities": []}, {"text": "Image level annotation: The image level annotation process refers to rules proposed by , who defined image labels as the combination of the gender of users who had posted images and the contents that the images are likely to express.", "labels": [], "entities": [{"text": "Image level annotation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6087316274642944}]}, {"text": "The image labels include two parts.", "labels": [], "entities": []}, {"text": "The first is a gender category: female, male, and unknown.", "labels": [], "entities": []}, {"text": "The former two are used to label images, for which people can infer the uploader gender.", "labels": [], "entities": []}, {"text": "For images of which the uploader gender is unrecognizable, we use unknown.", "labels": [], "entities": []}, {"text": "The second part defined in the image label is the category that expresses the classification of contents included in images.", "labels": [], "entities": []}, {"text": "We designate the combination of these categories as sub-category.", "labels": [], "entities": []}, {"text": "shows typical contents of the sub-category.", "labels": [], "entities": []}, {"text": "Finally, we obtained 6000 tweet level annotations and 8162 image level annotations.", "labels": [], "entities": []}, {"text": "As shown in, the tweet level annotation data were split up into three subsets.", "labels": [], "entities": []}, {"text": "Subset A was used for training the text-based method.", "labels": [], "entities": []}, {"text": "Subset B was used for training the hybrid-based method.", "labels": [], "entities": []}, {"text": "Subset C was used for evaluation.", "labels": [], "entities": []}, {"text": "Image level annotation data were used for the training-image-based method.", "labels": [], "entities": []}, {"text": "LIBSVM () was used as the implementation of SVM.", "labels": [], "entities": [{"text": "LIBSVM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9164351224899292}]}, {"text": "The linear kernel was selected.", "labels": [], "entities": []}, {"text": "Then LIBLINEAR ( was used as the implementation of logistic regression.", "labels": [], "entities": [{"text": "LIBLINEAR", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9669069051742554}]}, {"text": "Cost parameter C was set to 1.0.", "labels": [], "entities": [{"text": "Cost parameter C", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9262393315633138}]}, {"text": "As comparative methods, we selected the method presented by using the combination approach of text and image data and also the selected text-based and image-based method using the approach of a single source.", "labels": [], "entities": []}, {"text": "In the experiment, classifiers of were replaced with the proposed classifiers to compare the performances of hybrid methods.", "labels": [], "entities": []}, {"text": "The alpha value necessary for the method of to combine probability scores was set to 0.74 based on preliminary experiments.", "labels": [], "entities": []}, {"text": "shows the precision, recall, F -measure, and accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9997817873954773}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9996999502182007}, {"text": "F -measure", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9881572524706522}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9995176792144775}]}, {"text": "The accuracy of our proposed method achieved 80.25, which is 2.95 pt higher than that of the text-based method, 8.25 pt higher than that of the image-based method, and 1.35 pt higher than that of the method described by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995172023773193}]}, {"text": "Especially, the female F -measure associated with our proposed method achieved 77.07, which is 5.03 pt higher than that of the text based method, 13.69 pt higher than that of the image based method, and 2.0 pt higher than that of the method described by.", "labels": [], "entities": [{"text": "F -measure", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9924638867378235}]}, {"text": "We conducted a binomial test to assess our proposed method and the method described by.", "labels": [], "entities": []}, {"text": "Results confirmed that the p value is 0.0031, which indicates that the results obtained using our method are significantly better than those obtained using the existing combinationbased method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Number of users included in each set.  Conventional approach denotes the method of  Sakaki et al. (2014).", "labels": [], "entities": []}, {"text": " Table 4: Top three absolute weights of the image content categories.", "labels": [], "entities": []}]}