{"title": [{"text": "Response Generation in Dialogue using a Tailored PCFG Parser", "labels": [], "entities": [{"text": "Response Generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8797973692417145}]}], "abstractContent": [{"text": "This paper presents a parsing paradigm for natural language generation task, which learns a tailored probabilistic context-free grammar for encoding meaning representation (MR) and its corresponding natural language (NL) expression, then decodes and yields natural language sentences at the leaves of the optimal parsing tree fora target meaning representation.", "labels": [], "entities": [{"text": "natural language generation task", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.7045195251703262}]}, {"text": "The major advantage of our method is that it does not require any prior knowledge of the M-R syntax for training.", "labels": [], "entities": []}, {"text": "We deployed our method in response generation fora Chi-nese spoken dialogue system, obtaining results comparable to a strong baseline both in terms of BLEU scores and human evaluation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8440749049186707}, {"text": "BLEU scores", "start_pos": 151, "end_pos": 162, "type": "METRIC", "confidence": 0.9743496775627136}]}], "introductionContent": [{"text": "Grammar based natural language generation (NL-G) have received considerable attention over the past decade.", "labels": [], "entities": [{"text": "Grammar based natural language generation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5646621346473694}]}, {"text": "Prior work has mainly focused on hand-crafted generation grammar, which is extensive, but also expensive.", "labels": [], "entities": []}, {"text": "Recent work automatically learns a probabilistic regular grammar describing Markov dependency among fields and word strings (, or extracts a tree adjoining grammar provided an alignment lexicon is available which projects the input semantic variables up the syntactic tree of their natural language expression (.", "labels": [], "entities": []}, {"text": "Although it is a consensus that at a rather abstract level natural language generation can benefit a lot from its counterpart natural language understanding (NLU), the problem of leveraging NLU resources for NLG still leaves much room for investigation.", "labels": [], "entities": []}, {"text": "In this paper, we propose a purely data-driven natural language generation model which exploits a probabilistic context-free grammar (PCFG) parser to assist natural language generation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 157, "end_pos": 184, "type": "TASK", "confidence": 0.6761419276396433}]}, {"text": "The basic idea underlying our method is that the generated sentence is licensed by a context-free-grammar, and thus can be deduced from a parsing tree which encodes hidden structural associations between meaning representation and its sentence expression.", "labels": [], "entities": []}, {"text": "A tailored PCFG, i.e., a PCFG easily tailored to application-specific concepts, is learned from pairs of structured meaning representation and its natural language sentence and then used to guide generation processes for other previously unseen meaning representations.", "labels": [], "entities": []}, {"text": "exemplifies a record from the application under consideration.", "labels": [], "entities": []}, {"text": "Our model is closest to ( and) who reformulate the Markov structure between a meaning representation and a string of text depicted in) into a set of CFG rewrite rules, and then deduce the best derivation tree fora database record.", "labels": [], "entities": []}, {"text": "Although this Markov structure can capture a few elements of rudimentary syntax, it is essentially not linguistic grammars.", "labels": [], "entities": []}, {"text": "Thus the sentences produced by this model are usually ungrammatically informed (for instance, its 1-BEST model produces grammatically illegal sentences like \"Milwaukee Phoenix on Saturday on Saturday on Saturday on Saturday\").", "labels": [], "entities": [{"text": "Milwaukee Phoenix", "start_pos": 158, "end_pos": 175, "type": "DATASET", "confidence": 0.8702665865421295}]}, {"text": "() claims that long range dependency is an efficient complementary to CFG grammar, and incorporates syntactic dependency between words into the reranking procedure to enhance the performance.", "labels": [], "entities": [{"text": "CFG grammar", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.6852559745311737}]}, {"text": "Although conceptually similar, our model directly learns more grammatical rewrite rules from hybrid syntactic trees whose nonterminal nodes are comprised of phrasal nodes inheriting from a common syntactic parser, and conceptual nodes designed for encoding target meaning representation.", "labels": [], "entities": [{"text": "encoding target meaning representation", "start_pos": 248, "end_pos": 286, "type": "TASK", "confidence": 0.7837211042642593}]}, {"text": "Therefore, the learning aspect of two models is fundamentally different.", "labels": [], "entities": []}, {"text": "We have a single CFG grammar that applies throughout, where- as they train different CFG grammar and dependency grammar respectively.", "labels": [], "entities": []}, {"text": "The major advantage of our approach is that it learns a tailored PCFGs directly from MR and NL pairs, without the need to manually define CFG derivations, which is one of the most important prerequisites in and, and thus porting our method to another applications is relatively easy.", "labels": [], "entities": []}, {"text": "We demonstrate the versatility and effectiveness of our method on response generation fora Chinese spoken dialogue system (SDS) 1 .", "labels": [], "entities": [{"text": "response generation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7924985289573669}, {"text": "Chinese spoken dialogue system (SDS)", "start_pos": 91, "end_pos": 127, "type": "TASK", "confidence": 0.6198180658476693}]}], "datasetContent": [{"text": "We conducted experiments on a Chinese spoken dialogue system (SDS) for booking meeting room.", "labels": [], "entities": [{"text": "Chinese spoken dialogue system (SDS)", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.6637496309620994}]}, {"text": "Our NLG module receives structured input from dialogue management (DM) module and generates natural language response to user.", "labels": [], "entities": [{"text": "dialogue management (DM)", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7476218581199646}]}, {"text": "The structured input includes dialogue actions (e.g., greet, request, confirm), objects (e.g., date, budget, location) and object values which can be a null.", "labels": [], "entities": []}, {"text": "The SDS corpus consists of 1,406 formal meaning representations, along with their Chinese NL expressions written by 3 Chinese native speakers.", "labels": [], "entities": []}, {"text": "The average sentence length for the example data is 15.7 Chinese words.", "labels": [], "entities": []}, {"text": "We randomly select 1,000 record pairs as training data, and the remaining 406 is used as testing data.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the generated sentences, the BLEU score () is computed by comparing system-generated sentences with human-written sentences.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9861383736133575}]}, {"text": "In addition, we evaluated the generated text via a human judgment as designed in.", "labels": [], "entities": []}, {"text": "The subjects were presented with a MR and were asked to rate its corresponding NL expression along two dimensions: grammatical fluency and semantic correctness.", "labels": [], "entities": [{"text": "MR", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9704070687294006}]}, {"text": "A five point rating scale is designed where a higher number indicates better performance.", "labels": [], "entities": []}, {"text": "The averaged score of three human evaluators was computed.", "labels": [], "entities": []}, {"text": "In order to compare our work with previous related work, summarizes results achieved using the proposed tailored PCFGs with that using the grammar described in ().", "labels": [], "entities": []}, {"text": "1-BEST signifies results obtained from the basic decoder described in Section 2.3, and k-BEST is results of the k-best decoder reranked with a bigram language model.", "labels": [], "entities": []}, {"text": "Here we set k = 20 without more fine-tuning work.", "labels": [], "entities": []}, {"text": "To make intensive comparisons, the length of the generated sentence is not restricted as a fixed number, while varying from 1 to a length of the longest sentence in the training data.", "labels": [], "entities": []}, {"text": "The sentences with different length are overall sorted to obtain the 1-BEST and the k-BEST.", "labels": [], "entities": []}, {"text": "From, we find that differences in BLEU scores between 1-BEST-Konstas and 1-BEST-Our are statistically significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9991030693054199}]}, {"text": "Since the only difference between these two results is the grammar used, we have reason to justify that the tailored grammar learnt from the hybrid phrase-concept trees is superior for modeling NL and MR correspondence to that used in (.", "labels": [], "entities": []}, {"text": "It is interesting to notice that k-BEST-Konstas observes substantial increase in performance compared to 1-BEST-Konstas, while k-BEST-Our only achieves a slight increase compared to 1-BEST-Our.", "labels": [], "entities": []}, {"text": "Statistical language model offers potentially significant advantages for the sequential Markov grammar as reported in, but it contributes little to the tailored PCFGs.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 161, "end_pos": 166, "type": "DATASET", "confidence": 0.8888784050941467}]}, {"text": "This also verifies the robustness of the proposed method.", "labels": [], "entities": []}, {"text": "also summarizes the human ratings for each system and the gold-standard humanauthored sentences.", "labels": [], "entities": []}, {"text": "From we can observe that our method consistently produce good Chinese sentences in terms of both grammatical coherence and semantic soundness, which is consistent with the results of automatic evaluation.", "labels": [], "entities": []}, {"text": "Another major advantage of our method over method of is that it does not require any prior knowledge of the MR syntax for training.", "labels": [], "entities": []}, {"text": "Therefore, transplanting our method to other NLG application is relatively easy.", "labels": [], "entities": []}, {"text": "shows the generations of the four models on an example.", "labels": [], "entities": []}, {"text": "1-BEST-Konstas is only able to form Markov but not grammatical associations.", "labels": [], "entities": []}, {"text": "k-BEST-Konstas improves it by accounting for more possible associations, but errors are still made due to the lack of syntactic structure.", "labels": [], "entities": []}, {"text": "1-BEST-Our and k-BEST-Our remedies this.", "labels": [], "entities": []}, {"text": "However, unexpected sentences are still produced in the cases of long rang correlation.", "labels": [], "entities": []}, {"text": "For example, k-BEST-Our produced a sentence \" \u00acAEF\u00cf\u0178o\u017e\u00ff\u00de1 Q\u00ba(When is the meeting date held?)\" which is a grammatically well-formed sentence but has poor fluency and meaning.", "labels": [], "entities": [{"text": "\u00acAEF\u00cf\u0178o\u017e\u00ff\u00de1 Q\u00ba(When", "start_pos": 46, "end_pos": 65, "type": "METRIC", "confidence": 0.8892426490783691}]}, {"text": "As perceived in the work of syntactic parsing, PCFG is very difficult to capture long range dependency of word strings.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7625585198402405}]}], "tableCaptions": [{"text": " Table 1: Examples of meaning representation input as a structured database and its corresponding natural  language expression. Each meaning representation has several fields, each field has a value.", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores, and human ratings for syn- tactic fluency (SF) and semantic correctness (SC)  of different systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981416463851929}, {"text": "syn- tactic fluency (SF)", "start_pos": 45, "end_pos": 69, "type": "METRIC", "confidence": 0.5246109792164394}, {"text": "semantic correctness (SC)", "start_pos": 74, "end_pos": 99, "type": "METRIC", "confidence": 0.6515105664730072}]}]}