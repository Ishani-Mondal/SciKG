{"title": [{"text": "Image with a Message: Towards detecting non-literal image usages by visual linking", "labels": [], "entities": [{"text": "detecting non-literal image usages", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.7664623856544495}]}], "abstractContent": [{"text": "A key task to understand an image and its corresponding caption is not only to find out what is shown on the picture and described in the text, but also what is the exact relationship between these two elements.", "labels": [], "entities": []}, {"text": "The long-term objective of our work is to be able to distinguish different types of relationship, including literal vs. non-literal usages, as well as fine-grained non-literal usages (i.e., symbolic vs. iconic).", "labels": [], "entities": []}, {"text": "Here, we approach this challenging problem by answering the question: 'How can we quantify the degrees of similarity between the literal meanings expressed within images and their cap-tions?'.", "labels": [], "entities": []}, {"text": "We formulate this problem as a ranking task, where links between entities and potential regions are created and ranked for relevance.", "labels": [], "entities": []}, {"text": "Using a Ranking SVM allows us to leverage from the preference ordering of the links, which help us in the similarity calculation for the cases of visual or textual ambiguity, as well as misclassified data.", "labels": [], "entities": []}, {"text": "Our experiments show that aggregating different features using a supervised ranker achieves better results than a baseline knowledge-base method.", "labels": [], "entities": []}, {"text": "However, much work still lies ahead, and we accordingly conclude the paper with a detailed discussion of a short-and long-term outlook on how to push our work on relationship classification one step further.", "labels": [], "entities": [{"text": "relationship classification", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.8772181570529938}]}], "introductionContent": [{"text": "Despite recent major advances in vision and language understanding, the classification of usage relationships between images and textual captions is still an open challenge, which is still to be addressed from a computational point of view.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7407195866107941}, {"text": "classification of usage relationships between images and textual captions", "start_pos": 72, "end_pos": 145, "type": "TASK", "confidence": 0.7665819923082987}]}, {"text": "Relationships between images and texts can be classified from a general perspective into three different types, namely literal, non-literal and no-relationship.", "labels": [], "entities": []}, {"text": "Literal relations cover captions and/or longer corresponding texts that have a descriptive character with respect to the associated image.", "labels": [], "entities": []}, {"text": "Non-literal refers instead to images and captions having a relationship that arouses broad associations to other topics, e.g., abstract topics.", "labels": [], "entities": []}, {"text": "The class of non-literal relationships itself can be further divided: Symbolic photographs area common example of non-literal relations.", "labels": [], "entities": []}, {"text": "Pictures of this kind can be used without any further explanation on the basis of common socially-mediated understanding, e.g., a heart as a symbol of love, an apple and the snake as an symbol of original sin, or the peace symbol.", "labels": [], "entities": []}, {"text": "Social media typically use another type of language and sometimes can only be understood by insiders or people, who attended to the situation the photo has been taken, e.g., \"Kellogs in a pizza box\", with a photo showing a cat sleeping in a pizza box.", "labels": [], "entities": []}, {"text": "Without the image, it would have been only clear to those who know Kellogs that a cat is meant by this caption.", "labels": [], "entities": []}, {"text": "To the ordinary reader, this would rather suggest a typo and thus, cereals in the pizza box.", "labels": [], "entities": []}, {"text": "Those types of relationships can often be found on Flickr, e.g., in the SBU 1M dataset (.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.960623025894165}, {"text": "SBU 1M dataset", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.8643674651781718}]}, {"text": "A third category is the one of Media icons, which is typically focused on hot, sensitive, and abstract topics, which are hard to depict directly.", "labels": [], "entities": []}, {"text": "Pictures of this kind are often used by news agencies, politicians, and organizations, e.g., a polar bear on an ice floe for global warming.", "labels": [], "entities": []}, {"text": "This type of non-literal relationship uses a combination of descriptive parts and language beyond a literal meaning, which assumes fine-grained domain and background knowledge, e.g., the ice floe melting as a result of global warming.", "labels": [], "entities": []}, {"text": "When knowledge of this kind is not readily available to the reader, it can be Figure 1: Caption: \"A girl with a black jacket and a bluejeans is sitting on a brown donkey; another person is standing behind it; a brown, bald slope in the background.\" still acquired by reading associated articles or, in general, by getting to know further facts about a topic.", "labels": [], "entities": [{"text": "Caption", "start_pos": 88, "end_pos": 95, "type": "TASK", "confidence": 0.8583155274391174}]}, {"text": "This way the readers are able to create the association of the topic to the image-caption pair.", "labels": [], "entities": []}, {"text": "In our work, we aim at developing methods for automatic understanding of relations between natural language text and pictures beyond literal meanings and usages.", "labels": [], "entities": [{"text": "automatic understanding of relations between natural language text and pictures beyond literal meanings and usages", "start_pos": 46, "end_pos": 160, "type": "TASK", "confidence": 0.7655337691307068}]}, {"text": "In particular, we ultimately aim to automatically understand the cultural semantics of iconic pictures in textual contexts (i.e., captions, associated texts, etc.).", "labels": [], "entities": []}, {"text": "Far from being an abstract research topic, our work has the potential to impact real-world applications like mixed image-text search (, especially in cases of ambiguous or abstract topics in textual queries.", "labels": [], "entities": [{"text": "mixed image-text search", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.6387054324150085}]}, {"text": "Even if current state-of-theart search engines perform very well, not every search query is answered with what a user expects, e.g., in cases of ambiguity or image and text pairs with non-literal meaning.", "labels": [], "entities": []}, {"text": "Being able to assess if a caption and an image are in literal, nonliteral, or no relationship can have positive effects to search results.", "labels": [], "entities": []}, {"text": "Another, more specific use case is the training of image detectors with the use of captions, which are available in large amounts on the World Wide Web.", "labels": [], "entities": [{"text": "image detectors", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7308953702449799}]}, {"text": "Training image detectors requires image-caption pairs of the literal class, so being able to reliably identify such instances will arguably produce better, more reliable, and precise objector scene detection models.", "labels": [], "entities": [{"text": "objector scene detection", "start_pos": 183, "end_pos": 207, "type": "TASK", "confidence": 0.63282510638237}]}, {"text": "This is particularly of interest in the news and social media: Non-literal caption: \"Deforestation to make way for palm oil plantations has threatened the biodiversity of Borneo, placing species such as the orangutan at risk.\".", "labels": [], "entities": []}, {"text": "Literal caption: \"Two orangutans hugging each other on afield with green leaves.", "labels": [], "entities": []}, {"text": "A wooden trunk lays in the background.\".", "labels": [], "entities": []}, {"text": "Photograph: BOSF I VIER PFOTEN domain, where customizing image detectors for trending entities is of high interest.", "labels": [], "entities": [{"text": "BOSF I VIER PFOTEN", "start_pos": 12, "end_pos": 30, "type": "METRIC", "confidence": 0.7585875391960144}]}, {"text": "Most of the datasets used for training and testing methods from natural language processing, computer vision, or both, are focusing on images with literal textual description.", "labels": [], "entities": []}, {"text": "When humans are asked to annotate images with a description, they tend to use a literal caption (cf., e.g.,).", "labels": [], "entities": []}, {"text": "However, captions in real world news articles are devised to enhance the message and build bridges to a more abstract topic, thus have a non-literal or iconic meaning -cf., e.g., the caption of on deforestation in combination with an image showing the orangutan mother with her baby in an open field without trees.", "labels": [], "entities": []}, {"text": "Note that image-captions of this kind are typically designed to arouse an emotive response in the reader: in this case, the non-literal usage aims at leading the reader to focus on an abstract topic such as the negative impacts of palm oil plantations.", "labels": [], "entities": []}, {"text": "In contrast, the literal caption for this image would rather be \"Two orangutans hugging each other on afield with green leaves.", "labels": [], "entities": []}, {"text": "A wooden trunk lays in the background.\"", "labels": [], "entities": []}, {"text": "The literal image-caption pair, without further background knowledge, does not trigger this association.", "labels": [], "entities": []}, {"text": "Existing methods from Natural Language Processing (NLP), Computer Vision (CV) do not, and are not meant to find a difference between the same images being used in another context or the same textual contexts depicted with other viewpoints of an abstract topic.", "labels": [], "entities": []}, {"text": "In the case of image detection there is no difference between the image with the literal or non-literal caption -it is still the same image, classified as e.g., orangutans.", "labels": [], "entities": [{"text": "image detection", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.8163518309593201}]}, {"text": "Only when the caption is incorporated into the prediction process, we are able to identify the imagecaption pair into the appropriate usage classes, either in a coarse-grained (i.e., 'literal' versus 'nonliteral') or fine-grained (e.g., 'media icons' versus 'symbolic photographs').", "labels": [], "entities": []}, {"text": "Spinning our example further, if we would replace the image of with a picture showing a supermarket shelf with famous palm-oil-rich products, it should still be classified as non-literal.", "labels": [], "entities": []}, {"text": "However, when regarding the caption as arbitrary text without the context of a picture, this does not have any iconic meaning.", "labels": [], "entities": []}, {"text": "Likewise, image processing without considering text cannot predict the relationship to this abstract topic.", "labels": [], "entities": [{"text": "image processing", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.816916435956955}]}, {"text": "Therefore, the classification into 'literal' or 'non-literal' (respectively 'media iconic') needs to integrate NLP and CV together.", "labels": [], "entities": []}, {"text": "Our working assumption is that the iconic meaning reveals itself through the mismatches between objects mentioned in the caption and objects present in the image.", "labels": [], "entities": []}, {"text": "In this paper we set to find methods and measures to being able to classify these different image-text usage relationships.", "labels": [], "entities": []}, {"text": "Consequently, we aim at answering the following research questions: \u2022 What constitutes a literal class of imagecaption pair?", "labels": [], "entities": []}, {"text": "\u2022 Which method or measure is required to classify a pair as being literal?", "labels": [], "entities": []}, {"text": "\u2022 Are we able to derive methods and measures to approach the detection of non-literal pairs?", "labels": [], "entities": []}, {"text": "\u2022 How to differentiate literal, non-literal, and not-related classes from each other?", "labels": [], "entities": []}, {"text": "As a first step towards answering these questions, we focus hereon detecting literal text-image usages.", "labels": [], "entities": []}, {"text": "Therefore, we focus on a dataset of images and captions with literal usages.", "labels": [], "entities": []}, {"text": "Our hunch is that the more links between entities from the caption and regions in the image we can create, the more literal the relationship becomes.", "labels": [], "entities": []}, {"text": "In order to verify this hypothesis, we need to create links between entities from the text and regions with an object in the image, a problem we next turn to.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on the SAIAPR-TC12 dataset (  An example image of the SAIPR-TC12 with segmentation masks and the affiliated caption is given in.", "labels": [], "entities": [{"text": "SAIAPR-TC12 dataset", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9096294343471527}, {"text": "SAIPR-TC12", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.8564911484718323}]}, {"text": "The example also shows that due to the limited amount of labels objects are not inevitably represented by the same word in images and captions.", "labels": [], "entities": []}, {"text": "Links between entities of the captions and corresponding image segments are not given by default.", "labels": [], "entities": []}, {"text": "Due to the topics, covered by the dataset, which are similar to other datasets, the SAIAPR-TC12 can be used as training data.", "labels": [], "entities": [{"text": "SAIAPR-TC12", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.7443979382514954}]}, {"text": "Whereas other, non segmented datasets can be used as testing data, e.g., MediaEval Benchmarking ().", "labels": [], "entities": []}, {"text": "We manually created links between the 240 segments and 231 entities of the originally 281 extracted ones.", "labels": [], "entities": []}, {"text": "Since some entities are abstract words, describing images, e.g. 'background',: Results of the baseline and the different ranking SVM with the two metrics for relevance (Precision), diversity (Recall), and mean of relevance and diversity (F1-Measure).", "labels": [], "entities": [{"text": "F1-Measure", "start_pos": 238, "end_pos": 248, "type": "METRIC", "confidence": 0.9969333410263062}]}, {"text": "those entities are filtered in advance (already in the baseline).", "labels": [], "entities": []}, {"text": "Overall, 98 color names, that are further describing entities, can be extracted.", "labels": [], "entities": []}, {"text": "All links are rated with respect to the query.", "labels": [], "entities": []}, {"text": "Within a leave-one-out approach we cross validated every method.", "labels": [], "entities": []}, {"text": "As color features are low level features, and rather supposed to enrich the HOG model, it is not separately evaluated.", "labels": [], "entities": []}, {"text": "All Ranking SVM results are evaluated for Precision (P), Recall (R) and F1-Measure (F1).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 42, "end_pos": 55, "type": "METRIC", "confidence": 0.9477914273738861}, {"text": "Recall (R)", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9589960128068924}, {"text": "F1-Measure (F1)", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.9130803346633911}]}, {"text": "The text-based Baseline achieves precision and F1 with around 50% (cf).", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.999698281288147}, {"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9998325109481812}]}, {"text": "The also textbased Cosine Similarity of GloVe achieves around one and a half better results than the baseline, but these results are reduced for around 10% after integrating the cosine similarities of color names and labels.", "labels": [], "entities": []}, {"text": "Vice versa, the two visual feature approaches show better results when integrating both feature types -HOG and color (P: 63.79% vs. 54.59%, F1:40.59% vs. 35.12%).", "labels": [], "entities": [{"text": "F1:40.59", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.99762362241745}]}, {"text": "The results indicate, that visual feature selection and extraction needs further improvement, but they also show, that a post-processing, e.g., reranking with aggregation can have positive impacts.", "labels": [], "entities": [{"text": "visual feature selection", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6136433482170105}]}], "tableCaptions": [{"text": " Table 1: Most common 10 labels and entities of  test data selection.", "labels": [], "entities": []}, {"text": " Table 2: Results of the baseline and the different ranking SVM with the two metrics for relevance (Pre- cision), diversity (Recall), and mean of relevance and diversity (F1-Measure).", "labels": [], "entities": [{"text": "diversity (Recall)", "start_pos": 114, "end_pos": 132, "type": "METRIC", "confidence": 0.8933603018522263}, {"text": "F1-Measure", "start_pos": 171, "end_pos": 181, "type": "METRIC", "confidence": 0.99544358253479}]}]}