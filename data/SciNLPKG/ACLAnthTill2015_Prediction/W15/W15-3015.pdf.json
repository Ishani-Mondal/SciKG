{"title": [{"text": "GF Wide-coverage English-Finnish MT system for WMT 2015", "labels": [], "entities": [{"text": "GF Wide-coverage English-Finnish", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.7632743318875631}, {"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.635402500629425}, {"text": "WMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.6206505298614502}]}], "abstractContent": [{"text": "This paper describes the GF Wide-coverage MT system submitted to WMT 2015 for translation from English to Finnish.", "labels": [], "entities": [{"text": "GF Wide-coverage MT", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.45616328716278076}, {"text": "WMT 2015", "start_pos": 65, "end_pos": 73, "type": "TASK", "confidence": 0.5742814242839813}, {"text": "translation from English to Finnish", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.8204381704330445}]}, {"text": "Our system uses a interlingua based approach, in which the interlingua is a shared formal representation, that abstracts syntactic structures over multiple languages.", "labels": [], "entities": []}, {"text": "Our final submission is a re-ranked system in which we combine this baseline MT system with a factored LM model.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9587612748146057}]}], "introductionContent": [{"text": "Interlingual translation is an old idea that has been suggested numerous times and refuted almost as many times.", "labels": [], "entities": [{"text": "Interlingual translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8316740691661835}]}, {"text": "A typical criticism is that the very idea is utopistic: that one can never build an interlingua that faithfully represents meaning in all languages of the world.", "labels": [], "entities": []}, {"text": "However, as the focus in machine translation has shifted from the perfect rendering of meaning to less modest goals, the idea of an interlingua can be reconsidered.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7387707531452179}]}, {"text": "In the current paper, we describe our system submission to the WMT shared task in the English-Finnish track.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7687087059020996}]}, {"text": "Our system is an interlingua-based system, the interlingua based on an abstract syntax in the sense of Grammatical Framework (GF).", "labels": [], "entities": []}, {"text": "GF has been previously shown to work for domain-specific MT outperforming state-of-art systems using semantic interlinguas ).", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9352502226829529}]}, {"text": "Departing from this, the GF wide-coverage Translator is an attempt following the current mainstream in the field of MT: we are content with browsing quality in the output of the MT systems, while achieving the low cost of interlingual MT systems.", "labels": [], "entities": [{"text": "GF wide-coverage Translator", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.6107565462589264}, {"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9857407212257385}]}, {"text": "As such, the shared abstract syntax is mapped to different \"surface\" languages representing an abstraction of the deep syntactic structure for each of the languages.", "labels": [], "entities": []}, {"text": "The abstraction from word order, morphology and certain deep syntactic phenomena, allows the interlingua to cope with unrelated languages.", "labels": [], "entities": []}, {"text": "At the same time, these systems are scalable beyond toy examples, into wide-coverage systems.", "labels": [], "entities": []}, {"text": "We submit this system as our baseline over the English Finnish language pair for the WMT shared task.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.698003351688385}]}, {"text": "In addition, we also submitted a \"re-ranked\" variant of the same system as our primary submission, using statistical language models to re-score the translations from the baseline.", "labels": [], "entities": []}, {"text": "Automatic evaluation metrics have shown small improvements from re-ranking our baseline system . The paper is organized as follows: we describe our baseline system in Section 2 and the re-ranked variant in Section 3.", "labels": [], "entities": []}, {"text": "We present our experiments and relevant discussion in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "As part of the shared task contest, we carried out experiments with the wide-coverage translator and its re-ranked variant on the English-Finnish track.", "labels": [], "entities": []}, {"text": "shows the scores obtained by automatic evaluation for our system submissions.", "labels": [], "entities": []}, {"text": "On the devel set, the baseline system takes 27 minutes to carryout the translation pipeline i.e. the 1-best parsing of the English sentences combined with the 1-best linearization into Finnish.", "labels": [], "entities": []}, {"text": "In comparison, the test set takes about 22 minutes for the pipeline.", "labels": [], "entities": []}, {"text": "Of the 1500 sentences in the devel dataset, 600 sentences are parsed by the full RGL grammar, while the rest of the 900 sentences are parsed using the chunking grammar.", "labels": [], "entities": [{"text": "RGL grammar", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.8128867745399475}]}, {"text": "We obtained similar statistics on the test dataset, where 560 sentences were parsed by the RGL and 810 sentences using the chunking grammar.", "labels": [], "entities": [{"text": "RGL", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9649032950401306}]}, {"text": "This version of our translation pipeline is available online 2 . Manual evaluation and error analysis on a small sample from the devel dataset showed that the loss in MT quality from the chunking grammar was small, but significant.", "labels": [], "entities": [{"text": "MT", "start_pos": 167, "end_pos": 169, "type": "TASK", "confidence": 0.97120201587677}]}, {"text": "This is because the chunking grammar still allows for local agreement and reordering, while relaxing the RGL grammar.", "labels": [], "entities": []}, {"text": "Nonetheless, we decided to use this version of the chunking grammar, without extending the RGL with new syntactic constructions.", "labels": [], "entities": [{"text": "RGL", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.6929317712783813}]}, {"text": "One reason for this decision was the speedup in the pipeline obtained by relaxing the full RGL grammar and adding the chunking grammar.", "labels": [], "entities": []}, {"text": "It should be noted here that the quality of the MT system can be further improved by adding the full RGL at an additional computational cost.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.976921021938324}]}, {"text": "Evaluation experiments also showed that automatic evaluation metrics like BLEU substantially under-evaluate the perform of our system when used with a static translation as reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9981065988540649}]}, {"text": "In the next round of experiments, we ran the parser and the linearizer in K-best modes, collecting the 50-best abstract syntax trees and the 30-best linearizations for each abstract syntax tree.", "labels": [], "entities": []}, {"text": "Since the parsing and the linearization are carried out independent of one another, the 1500 hypothesis obtained from this run often contained identical translations.", "labels": [], "entities": []}, {"text": "The overall number of distinct hypothesis in the K-best lists was typically found to be between 300 and 400.", "labels": [], "entities": []}, {"text": "Collecting the K-best lists took about 93 minutes on the devel dataset and 80 minutes on the test dataset.", "labels": [], "entities": [{"text": "devel dataset", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8602147102355957}]}, {"text": "We reorder these K-best lists using our reranking models, which consists of a re-scoring the hypothesis translations using a language model (LM) and estimating the mixed score for each hypothesis.", "labels": [], "entities": []}, {"text": "The reordering combined with the re-scoring takes about 3-4 minutes on our lists of 1500-best hypotheses.", "labels": [], "entities": [{"text": "re-scoring", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9339799284934998}]}, {"text": "The LM for Finnish was trained on the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9929322004318237}]}, {"text": "Finnish sentences were morphologically analyzed and converted into a lemmatized corpus with morphological factors tagged along with the lemmas.", "labels": [], "entities": []}, {"text": "We train a factored language model on this corpus, using the lemma and the part-ofspeech and suffix as factors.", "labels": [], "entities": []}, {"text": "In our current experiments, the hypothesis are re-scored using the Finnish language model alone, though in principle the re-scoring can be carried out using language models for multiple languages.", "labels": [], "entities": []}, {"text": "We train a ordinal regression model using the parse tree probability estimated using the GF disambiguation model and the factored LM score to re-order the K-best lists.", "labels": [], "entities": [{"text": "factored LM score", "start_pos": 121, "end_pos": 138, "type": "METRIC", "confidence": 0.6757028301556905}]}, {"text": "A small set of 2500 sentences from the Europarl corpus were randomly taken and used as training samples for the regres- Experiments with the devel dataset showed small improvements from using the LM to rescore the hypothesis.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9899918138980865}, {"text": "devel dataset", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.6767344623804092}]}, {"text": "Comparatively, reranking resulted in even smaller improvements on the the test dataset.", "labels": [], "entities": []}, {"text": "At this point, we carried out a analysis of the K-best lists on the devel set.", "labels": [], "entities": []}, {"text": "We found that there was a very small variation in the K-best lists given the number of distinct hypothesis that were considered.", "labels": [], "entities": []}, {"text": "Most of the variation was attributed to punctuation and orthography rather than word senses or word order as we initially expected.", "labels": [], "entities": []}, {"text": "Following this, we experimented with random sampling in the parse forests to evaluate the oracle quality of our translation system.", "labels": [], "entities": []}, {"text": "The results of this study are pending error analysis and evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU (11b) and TER scores obtained on  the newstest2015 dataset", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993979930877686}, {"text": "TER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9986447691917419}, {"text": "newstest2015 dataset", "start_pos": 53, "end_pos": 73, "type": "DATASET", "confidence": 0.9776532053947449}]}]}