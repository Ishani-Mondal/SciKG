{"title": [{"text": "A Discriminative Model for Perceptually-Grounded Incremental Reference Resolution", "labels": [], "entities": [{"text": "Perceptually-Grounded Incremental Reference", "start_pos": 27, "end_pos": 70, "type": "TASK", "confidence": 0.6061752239863077}]}], "abstractContent": [{"text": "A large part of human communication involves referring to entities in the world, and often these entities are objects that are visually present for the interlocutors.", "labels": [], "entities": []}, {"text": "A computer system that aims to resolve such references needs to tackle a complex task: objects and their visual features must be determined , the referring expressions must be recognised, extra-linguistic information such as eye gaze or pointing gestures must be incorporated-and the intended connection between words and world must be reconstructed.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a discriminative model of reference resolution that processes incrementally (i.e., word for word), is perceptually-grounded, and improves when interpolated with information from gaze and pointing gestures.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8122242093086243}]}, {"text": "We evaluated our model and found that it performed robustly in a realistic reference resolution task, when compared to a generative model.", "labels": [], "entities": [{"text": "reference resolution task", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7857091228167216}]}], "introductionContent": [{"text": "Reference to entities in the world via definite description makes up a large part of human communication).", "labels": [], "entities": []}, {"text": "In task-oriented situations, these references are often to entities that are visible in the shared environment.", "labels": [], "entities": []}, {"text": "In such co-located settings, interlocutors can make use of extra-linguistic cues such as gaze or pointing gestures.", "labels": [], "entities": []}, {"text": "Furthermore, listeners resolve references as they unfold, often identifying the referred entity before the end of the reference (as found, inter alia, by;).", "labels": [], "entities": []}, {"text": "Computational research on reference resolution, however, has mostly focused on offline processing of full, completed referring expressions, and not attempted to model this online nature of human reference resolution.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.914383739233017}, {"text": "reference resolution", "start_pos": 195, "end_pos": 215, "type": "TASK", "confidence": 0.7052508145570755}]}, {"text": "On a more technical level, most of the models making use of stochastic information (see discussion below) have been generative models; even though such models are known to often have certain disadvantages compared to discriminative models.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a discriminative model of reference resolution that is incremental in that it does not wait until the end of an utterance to process, rather it updates its interpretation at each word.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7385864406824112}]}, {"text": "Moreover, the semantics of each word is perceptually grounded in visual information from the world.", "labels": [], "entities": []}, {"text": "We evaluated our model and found that it works robustly when compared to a similar generative approach.", "labels": [], "entities": []}, {"text": "In the following section we explain the task of reference resolution and discuss related work.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.9640716910362244}]}, {"text": "That is followed by an explanation of our model and evaluation experiment.", "labels": [], "entities": []}, {"text": "We end with some analyses of the model's strengths and areas of improvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will now explain our evaluation experiment, including the data we used, the pre-processing performed on it, a generative model that we will compare to, and the metrics that we will use in our evaluation.", "labels": [], "entities": []}, {"text": "We end this section with the results.", "labels": [], "entities": []}, {"text": "To give a picture of the overall performance of the model, we report accuracy (how often was the argmax the intended referent) after the full referring expression has been processed and average rank (position of intended referent among the 15 candidates on ordered distribution; ideal would bean average rank of 1, which would also correspond to 100% accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9996637105941772}, {"text": "accuracy", "start_pos": 351, "end_pos": 359, "type": "METRIC", "confidence": 0.9842957258224487}]}, {"text": "Together, these metrics give an impression of how interpretable the full distribution is, beyond just the argmax.", "labels": [], "entities": [{"text": "argmax", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.976107120513916}]}, {"text": "We report results for testing the model only given speech information (and no interpolation with the other models), and the other modalities added separately and jointly.", "labels": [], "entities": []}, {"text": "We also computed results for SIUM given speech information.", "labels": [], "entities": [{"text": "SIUM", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.9222079515457153}]}, {"text": "We also look into how the model performs incrementally.", "labels": [], "entities": []}, {"text": "For this, we followed previously used metrics (, where the predicted referent is compared to the gold refererent at each increment: \u2022 first correct: how deep into the RE (%) does the model predict the referent for the first time?", "labels": [], "entities": [{"text": "RE", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.9954380393028259}]}, {"text": "\u2022 first final: if the final prediction is correct, how deep into the RE was it reached and not changed?", "labels": [], "entities": [{"text": "RE", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.876379132270813}]}, {"text": "\u2022 edit overhead: how often did the model unnecessarily change its prediction (the only necessary prediction change happens when it first makes a correct prediction)?", "labels": [], "entities": [{"text": "edit overhead", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.7904792428016663}]}], "tableCaptions": [{"text": " Table 1: % edit overhead  and never correct", "labels": [], "entities": [{"text": "edit overhead", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9569657146930695}, {"text": "never correct", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.8482828140258789}]}]}