{"title": [{"text": "Sampling-based Alignment and Hierarchical Sub-sentential Alignment in Chinese-Japanese Translation of Patents", "labels": [], "entities": [{"text": "Sampling-based Alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9222397208213806}, {"text": "Hierarchical Sub-sentential Alignment in Chinese-Japanese Translation of Patents", "start_pos": 29, "end_pos": 109, "type": "TASK", "confidence": 0.6770842522382736}]}], "abstractContent": [{"text": "This paper describes Chinese-Japanese translation systems based on different alignment methods using the JPO corpus and our submission (ID: WASUIPS) to the subtask of the 2015 Workshop on Asian Translation.", "labels": [], "entities": [{"text": "JPO corpus", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.9820171594619751}, {"text": "2015 Workshop on Asian Translation", "start_pos": 171, "end_pos": 205, "type": "TASK", "confidence": 0.48886131048202514}]}, {"text": "One of the alignment methods used is bilingual hierarchical sub-sentential alignment combined with sampling-based multilingual alignment.", "labels": [], "entities": [{"text": "bilingual hierarchical sub-sentential alignment", "start_pos": 37, "end_pos": 84, "type": "TASK", "confidence": 0.5688392147421837}]}, {"text": "We also accelerated this method and in this paper, we evaluate the translation results and time spent on several machine translation tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.6906339824199677}]}, {"text": "The training time is much faster than the standard baseline pipeline (GIZA++/Moses) and MGIZA/Moses.", "labels": [], "entities": [{"text": "MGIZA/Moses", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.7474099596341451}]}], "introductionContent": [{"text": "Phrase-based Statistical Machine Translation (PB-SMT) as a data-oriented approach to machine translation has been widely used for over 10 years.", "labels": [], "entities": [{"text": "Phrase-based Statistical Machine Translation (PB-SMT)", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7485964000225067}, {"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7637657821178436}]}, {"text": "The Moses () open source statistical machine translation toolkit was developed by the Statistical Machine Translation Group at the University of Edinburgh.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.5503566563129425}, {"text": "Statistical Machine Translation", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.6495373845100403}]}, {"text": "During the three processes (training, tuning and decoding) for building a phrase-based translation system using Moses, training is the most important step as it creates the core knowledge used in machine translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.6006017923355103}, {"text": "machine translation", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.7514682710170746}]}, {"text": "Word or phrase alignment in the training step allows to obtain translation relationships among the words or phrases in a sentence-aligned bi-corpus.", "labels": [], "entities": [{"text": "Word or phrase alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5699340552091599}]}, {"text": "Word or phrase alignment affects the quality of translation.", "labels": [], "entities": [{"text": "Word or phrase alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5523666962981224}]}, {"text": "It is also one of the most time-consuming processing step.", "labels": [], "entities": []}, {"text": "The probabilistic approach attempts at determining the best set of alignment links between source and target words or phrases in parallel sentences.", "labels": [], "entities": []}, {"text": "IBM models ( and HMM alignment models (, which are typical implementation of the EM algorithm, are the most widely used representatives in this category.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7199429869651794}]}, {"text": "GIZA++ implemented IBM Models, it aligns words based on statistical models.", "labels": [], "entities": [{"text": "GIZA++", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9169987738132477}]}, {"text": "It is a global optimization process simultaneously considers all possible associations in the entire corpus and estimates the parameters of the parallel corpus.", "labels": [], "entities": []}, {"text": "Several improvements were made: MGIZA () is a parallel implementation of IBM models.", "labels": [], "entities": [{"text": "MGIZA", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.4799438714981079}]}, {"text": "However, the parallelization may lead to slightly different final alignment results, thus preventing reproduction of results to a certain extent.", "labels": [], "entities": []}, {"text": "The associative approaches, introduced in (, do not rely on an alignment model, but on independence statistical measures.", "labels": [], "entities": []}, {"text": "The Dice coefficient, mutual information (, and likelihood ratio are representative cases of this approach.", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.6697631180286407}, {"text": "likelihood ratio", "start_pos": 48, "end_pos": 64, "type": "METRIC", "confidence": 0.9879526197910309}]}, {"text": "The associative approaches use a local maximization process in which each sentence is processed independently.", "labels": [], "entities": []}, {"text": "Sampling-based multilingual alignment (Anymalign) ( and hierarchical sub-sentential alignment (Cutnalign) () are two associative approaches.", "labels": [], "entities": [{"text": "Sampling-based multilingual alignment", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7706455588340759}]}, {"text": "Anymalign is an open source multilingual associative aligner.", "labels": [], "entities": []}, {"text": "This method samples large numbers of sub-corpora randomly to obtain source and target word or phrase occurrence distributions.", "labels": [], "entities": []}, {"text": "The more often two words or phrases have the same occurrence distribution overparticular sub-corpora, the higher the association between them.", "labels": [], "entities": []}, {"text": "We can run Anymalign by setting with -t (running time) option and stop it at anytime, and the option -i allows to to extract longer phrases by enforcing n-grams to be considered as tokens.", "labels": [], "entities": []}, {"text": "For pre-segmented texts, option -i allows to group words into phrases more easily.", "labels": [], "entities": []}, {"text": "Cutnalign is a bilingual hierarchical subsentential alignment method (.", "labels": [], "entities": [{"text": "Cutnalign", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8917202949523926}]}, {"text": "It is based on a recursive binary segmentation process of the alignment matrix between a source sentence and its corresponding target sentence.", "labels": [], "entities": []}, {"text": "We make use of this method in combination with Anymalign.", "labels": [], "entities": [{"text": "Anymalign", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.8474535346031189}]}, {"text": "In the experiments, reported in this paper, we extend the work to decrease time costs in the training step.", "labels": [], "entities": []}, {"text": "We obtained comparable results in only one fifth of the training time required by the GIZA++/Moses baseline pipeline.", "labels": [], "entities": [{"text": "GIZA++/Moses baseline pipeline", "start_pos": 86, "end_pos": 116, "type": "DATASET", "confidence": 0.9126268267631531}]}], "datasetContent": [{"text": "Here, we basically perform experiments with GIZA++ or MGIZA.", "labels": [], "entities": [{"text": "MGIZA", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.8675284385681152}]}, {"text": "The phrase tables are extracted from the alignments obtained using the grow-diag-final-and heuristic) integrated in the Moses toolkit.", "labels": [], "entities": []}, {"text": "Our sampling-based alignment method and hierarchical sub-sentential alignment method are also evaluated within a PB-SMT system built by using the Moses toolkit, the Ken Language Modeling toolkit (Heafield, 2011) and a lexicalized reordering model ().", "labels": [], "entities": []}, {"text": "We built systems from Chinese to Japanese.", "labels": [], "entities": []}, {"text": "Each experiment was run using the same data sets (see Section 2).", "labels": [], "entities": []}, {"text": "Translations were evaluated using BLEU) and RIBES (.", "labels": [], "entities": [{"text": "Translations", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9599984884262085}, {"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.998428225517273}, {"text": "RIBES", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9647325277328491}]}, {"text": "We used Anymalign (i=2, two words can be considered as one token) and Cutnalign to build phrase tables.", "labels": [], "entities": []}, {"text": "As a timeout (-t) should be given, we set two different timeouts (5400 sec. and 1200 sec.).", "labels": [], "entities": []}, {"text": "We also use different Cutnalign versions where core components are implemented in C or Python.", "labels": [], "entities": []}, {"text": "We passed word-to-word associations output by Anymalign (i=2) to Cutnalign which produces sub-sentential alignments, which are in turn passed to the grow-dial-final-and heuristic of the Moses toolkit to build phrase tables.", "labels": [], "entities": [{"text": "Anymalign", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.8316148519515991}]}], "tableCaptions": [{"text": " Table 1: Statistics of our baseline training data of  JPC.", "labels": [], "entities": [{"text": "JPC", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.6629395484924316}]}, {"text": " Table 4: Steps in recursive segmentation and alignment result using sampling-based alignment and hier- archical sub-sentential alignment method.", "labels": [], "entities": [{"text": "recursive segmentation", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7256982028484344}]}]}