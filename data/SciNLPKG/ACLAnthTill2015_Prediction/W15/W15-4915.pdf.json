{"title": [{"text": "Truly Exploring Multiple References for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Truly Exploring Multiple References", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9111347645521164}, {"text": "Machine Translation Evaluation", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.8562888503074646}]}], "abstractContent": [{"text": "Multiple references in machine translation evaluation are usually under-explored: they are ignored by alignment-based met-rics and treated as bags of n-grams in string matching evaluation metrics, none of which take full advantage of the recurring information in these references.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.8455506761868795}]}, {"text": "By exploring information on the n-gram distribution and on divergences in multiple references, we propose a method of n-gram weighting and implement it to generate new versions of the popular BLEU and NIST metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9505845308303833}, {"text": "NIST metrics", "start_pos": 201, "end_pos": 213, "type": "DATASET", "confidence": 0.8590426743030548}]}, {"text": "Our metrics are tested in two into-English machine translation datasets.", "labels": [], "entities": []}, {"text": "They lead to a significant increase in Pearson's correlation with human fluency judgements at system-level evaluation.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 39, "end_pos": 60, "type": "METRIC", "confidence": 0.9463575085004171}]}, {"text": "The new NIST metric also out-performs the standard NIST for document-level evaluation.", "labels": [], "entities": [{"text": "NIST", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.8238425850868225}, {"text": "NIST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9373747706413269}]}], "introductionContent": [{"text": "Quality evaluation plays a critical role in Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8694357514381409}]}, {"text": "Since its conception, the BLEU metric () has had a significant impact on MT development.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9652661979198456}, {"text": "MT development", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.9752118289470673}]}, {"text": "Although human evaluation has been used in recent evaluation campaigns such as WMT (Workshop on Statistical MT) () and other forms of reference-less metrics have been proposed, the merit of language and resource-independent n-gram based metrics such as BLEU is undeniable.", "labels": [], "entities": [{"text": "WMT (Workshop on Statistical MT)", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.718361667224339}, {"text": "BLEU", "start_pos": 253, "end_pos": 257, "type": "METRIC", "confidence": 0.9900818467140198}]}, {"text": "Despite its criticisms, BLEU is thus still considered the de facto or at least a baseline metric for MT quality evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9983161687850952}, {"text": "MT quality evaluation", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.8927270968755087}]}, {"text": "Due to the cost of human translation, often only one reference translation is available at evaluation time.", "labels": [], "entities": [{"text": "human translation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.6555666327476501}]}, {"text": "However, generally there are numerous valid translations fora given sentence or document.", "labels": [], "entities": []}, {"text": "Different references provide valid variations in linguistic aspects such as style, word choice and word order.", "labels": [], "entities": [{"text": "word choice", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.7115837931632996}]}, {"text": "Therefore, having multiple reference translations is key to improve the reliability of n-gram based evaluation metrics: the more references, the more chances for n-grams correctly translated to be captured.", "labels": [], "entities": []}, {"text": "HyTER, an n-gram matching metric based on an exponential number of reference translations fora given target sentence, demonstrates the potential for better machine translation evaluation results from having as many references as possible.", "labels": [], "entities": [{"text": "HyTER", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9757757782936096}, {"text": "machine translation evaluation", "start_pos": 156, "end_pos": 186, "type": "TASK", "confidence": 0.7957164247830709}]}, {"text": "Nevertheless, in the more realistic case where only a few references are available, if these are simply taken as bags of n-grams, increasing the number of references will not lead to the best possible results, as pointed out by.", "labels": [], "entities": []}, {"text": "In this paper we explore how to use multiple references by means other than simply viewing them as bags of n-gram like BLEU, NIST) and other n-gram co-occurrence based metrics do.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9884210824966431}, {"text": "NIST", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.8945481181144714}]}, {"text": "Our assumption is that each reference reflects the complete meaning of the source segment.", "labels": [], "entities": []}, {"text": "The semantic entirety of the translation will be adversely affected if all the n-grams from various references are simply put together.", "labels": [], "entities": []}, {"text": "We propose a method of modifying the weight assignment strategy in BLEU and NIST by taking into account the n-gram distributions and divergences over different references.", "labels": [], "entities": [{"text": "weight assignment", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.668609008193016}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.5355601906776428}, {"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9052707552909851}]}, {"text": "Experiments were performed on two into-English translation datasets released by LDC, leading to promising results.", "labels": [], "entities": [{"text": "LDC", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.6784048080444336}]}, {"text": "In the remainder of this paper we will first review BLEU and related ngram based evaluation metrics (Section 2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9913190007209778}]}, {"text": "We then describe the method we propose to explore multiple references by reassigning the weights of n-grams that are common in system translations and references (Section 3), and the experiments performed and their results (Section 4).", "labels": [], "entities": []}, {"text": "These illustrate how the modified BLEU and NIST scores compare against standard BLEU and NIST scores at the system, document and sentence levels.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.991388738155365}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9784960150718689}]}], "datasetContent": [{"text": "The BLEU metric applies a straightforward method of counting the n-grams that overlap in the system translation and given human translations under the assumption that human translations precisely reproduce the meaning of the source text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9931493401527405}]}, {"text": "The closer to the reference, the higher the translation quality of the system translation will be.", "labels": [], "entities": []}, {"text": "The core formula is given in Eq.", "labels": [], "entities": []}, {"text": "1 (), so that we can subsequently compare it to our approach.", "labels": [], "entities": []}, {"text": "where w n is a weighting factor usually set as 1/N , where N is the longest possible n-gram considered by the matching method.", "labels": [], "entities": []}, {"text": "N is usually set to 4 to avoid data sparseness issues resulting from longer n-grams.", "labels": [], "entities": []}, {"text": "P n is the n-gram precision at a given n and in essence represents the proportion of ngrams in the candidate translation that also appear in the reference translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.8492389917373657}]}, {"text": "BP is a penalty factor for shorter segments.", "labels": [], "entities": [{"text": "BP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9920152425765991}]}, {"text": "c and rare the length of the candidate segment and reference segment, respectively.", "labels": [], "entities": [{"text": "length", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9649890065193176}]}, {"text": "When multiple references are available, Count clip (ngram) is clipped at the maximum count of n-grams which occurs in a single reference, and r is set as the length of the reference closest in size to the candidate translation.", "labels": [], "entities": [{"text": "Count clip (ngram)", "start_pos": 40, "end_pos": 58, "type": "METRIC", "confidence": 0.9666584253311157}]}, {"text": "Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation.", "labels": [], "entities": [{"text": "precisions", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9348698854446411}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9955494403839111}]}, {"text": "Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing () and combinations of smoothing techniques.", "labels": [], "entities": []}, {"text": "A great deal of methods have been proposed to improve the performance of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.7951809763908386}]}, {"text": "These include metrics such as m-bleu ( and Amber ().", "labels": [], "entities": [{"text": "Amber", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.8855043649673462}]}, {"text": "However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Kappa agreement of human judgement on  system translations", "labels": [], "entities": []}, {"text": " Table 2: Pearson correlation at system level", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7593988180160522}]}, {"text": " Table 3: Doc-level Pearson correlation on fluency", "labels": [], "entities": [{"text": "Doc-level Pearson correlation", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.8207742969195048}]}, {"text": " Table 4: Doc-level Pearson correlation on accu- racy", "labels": [], "entities": [{"text": "Doc-level Pearson correlation", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.7510946194330851}]}]}