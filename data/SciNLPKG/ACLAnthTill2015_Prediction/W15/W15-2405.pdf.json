{"title": [{"text": "Towards a Model of Prediction-based Syntactic Category Acquisition: First Steps with Word Embeddings", "labels": [], "entities": [{"text": "Syntactic Category Acquisition", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.827695349852244}]}], "abstractContent": [{"text": "We present a prototype model, based on a combination of count-based distri-butional semantics and prediction-based neural word embeddings, which learns about syntactic categories as a function of (1) writing contextual, phonological, and lexical-stress-related information to memory and (2) predicting upcoming context words based on memorized information.", "labels": [], "entities": [{"text": "predicting upcoming context words", "start_pos": 291, "end_pos": 324, "type": "TASK", "confidence": 0.872667670249939}]}, {"text": "The system is a first step towards utilizing recently popular methods from Natural Language Processing for exploring the role of prediction in childrens' acquisition of syntactic categories.", "labels": [], "entities": [{"text": "childrens' acquisition of syntactic categories", "start_pos": 143, "end_pos": 189, "type": "TASK", "confidence": 0.7770702123641968}]}], "introductionContent": [{"text": "Evidence is mounting that during language processing, the brain is predicting upcoming elements at different levels of granularity.", "labels": [], "entities": []}, {"text": "This could serve at least two purposes: (1) to facilitate understanding in dialogue and (2) to acquire abstract syntactic structure.", "labels": [], "entities": []}, {"text": "With respect to (1), review evidence suggesting that people predict upcoming elements in their interlocutors' speech streams using the production system.", "labels": [], "entities": []}, {"text": "This is thought to facilitate understanding in dialogue.", "labels": [], "entities": []}, {"text": "One reason to postulate is that length of memory span for syntactically well-formed sequences is positively correlated with an individual's ability to predict upcoming words, for further arguments).", "labels": [], "entities": []}, {"text": "We thus have evidence that people predict linguistic elements, and there is reason to suspect that this could be linked to the acquisition of syntactic The work reported here was implemented in the Theano framework (.", "labels": [], "entities": [{"text": "Theano framework", "start_pos": 198, "end_pos": 214, "type": "DATASET", "confidence": 0.9557746648788452}]}, {"text": "The code is freely available at: https://github.com/RobGrimm/prediction based structure.", "labels": [], "entities": [{"text": "RobGrimm", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.964588463306427}]}, {"text": "Models of prediction in language processing should therefore aim to demonstrate the emergence of such structure as a function of learning to predict upcoming elements.", "labels": [], "entities": []}, {"text": "Perhaps the most explicit account of such a process can be found in the work of, who use a recurrent neural network in combination with an event semantics, in order to generate sentences with unseen bindings between words and semantic roles -i.e., the types of novel sentential constructions that could be afforded by abstract syntactic structure.", "labels": [], "entities": []}, {"text": "It is noteworthy, given this line of work, that prediction is central to recently popular methods from Natural Language Processing (NLP) for obtaining distributional representations of words ().", "labels": [], "entities": []}, {"text": "Vector representations (often called word embeddings) obtained using these methods cluster closely in terms of semantic and syntactic typesan achievement due to engineering efforts, without emphasis on psychological constraints.", "labels": [], "entities": []}, {"text": "Thus, if these methods are to be used for modelling aspects of human language processing, they should be modified to reflect such constraints.", "labels": [], "entities": []}, {"text": "Here, we attempt to take a first step into this direction: we modify the skipgram model from the word2vec family of models ( -which predicts both the left and right context of a word -to predict only the righ context.", "labels": [], "entities": []}, {"text": "Word counts from the left context form the basis for prediction and are tuned to maximize the likelihood of correctly predicting words from the right context.", "labels": [], "entities": [{"text": "prediction", "start_pos": 53, "end_pos": 63, "type": "TASK", "confidence": 0.9580279588699341}]}, {"text": "Throughout, we measure the organization of word embeddings in terms of syntactic categories -and find that embeddings of the same category cluster more closely after each training stage.", "labels": [], "entities": []}, {"text": "In addition to word frequencies from the left context, we experiment with phonological information and features related to lexical stress as the basis of predicting words from the right context.", "labels": [], "entities": []}], "datasetContent": [{"text": "While the task is to predict words, we are interested in aside effect of the learning process: the induction of representations whose organization in vector space reflects syntactic categories.", "labels": [], "entities": []}, {"text": "To measure this, we train a 10-NN classifier on the embeddings after each training epoch, with embeddings labeled by syntactic category, and we stop training as soon as the micro F 1 score does not increase anymore.", "labels": [], "entities": [{"text": "micro F 1 score", "start_pos": 173, "end_pos": 188, "type": "METRIC", "confidence": 0.7378394454717636}]}, {"text": "To avoid premature termination of training due to fluctuations in F 1 scores during stage 1, we keep track of the epoch E at which we got the best score A.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9656003713607788}]}, {"text": "If scores stay smaller than or equal to A for 10 epochs, we terminate training and obtain the dimensionality-reduced embeddings for further training in stage 2 from the dAE's state at E.", "labels": [], "entities": []}, {"text": "In stage 2, as there are no such fluctuations, it is safe to terminate as soon as there is no increase anymore.", "labels": [], "entities": []}, {"text": "This procedure allows for as many training epochs as are necessary for achieving the best results -between 22 and 30 in the first and between 4 and 5 epochs in the second stage.", "labels": [], "entities": []}, {"text": "Performance is compared across stages as well as to a majority vote baseline (each data point is assigned the most common class) and a stratified sampling baseline (class labels are assigned in accordance with the class distribution).", "labels": [], "entities": []}, {"text": "The expected pattern is that performance at each training stage is both above baseline and significantly bet- ter than performance at the previous stage.", "labels": [], "entities": []}, {"text": "Significance of differences is computed via approximate randomization testing, 4 a statistical test suitable for comparing evaluation metrics such as F-scores (cf.).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9687609076499939}]}, {"text": "Results are based on a dAE with 400 hidden units, trained with a learning rate of 0.01, and a corruption level of 0.1.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.9488080143928528}, {"text": "corruption level", "start_pos": 94, "end_pos": 110, "type": "METRIC", "confidence": 0.9685032367706299}]}, {"text": "The softmax model was trained with a learning rate of 0.008, with context words sampled from a sentence-internal window oft = 3 words to the right.", "labels": [], "entities": []}, {"text": "Both models were optimized via true stochastic gradient descent.", "labels": [], "entities": []}, {"text": "shows precision, recall and F 1 scores based on a 10-NN classifier trained on the word embeddings at three different points in time: (a) before training begins, with scores based on the input embeddings, (b) after stage 1, with embeddings projected into a lower-dimensional space, and (c) after stage 2, with embeddings modified as a result of predicting words from the right context.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9995144605636597}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9993834495544434}, {"text": "F 1 scores", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9798160195350647}]}], "tableCaptions": [{"text": " Table 1: Precision and recall (in percent), together with micro and macro F1 scores, based on a 10-NN classifier trained on", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9977542757987976}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9991451501846313}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9196237325668335}]}]}