{"title": [{"text": "IHS_RD: Lexical Normalization for English Tweets", "labels": [], "entities": [{"text": "IHS_RD", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.4556575318177541}, {"text": "Lexical Normalization for English Tweets", "start_pos": 8, "end_pos": 48, "type": "TASK", "confidence": 0.7978231906890869}]}], "abstractContent": [{"text": "This paper describes the Twitter lexical nor-malization system submitted by IHS R&D Belarus team for the ACL 2015 workshop on noisy user-generated text.", "labels": [], "entities": [{"text": "IHS R&D Belarus team for the ACL 2015 workshop", "start_pos": 76, "end_pos": 122, "type": "DATASET", "confidence": 0.8688374324278398}]}, {"text": "The proposed system consists of two components: a CRF-based approach to identify possible normali-zation candidates, and a post-processing step in an attempt to normalize words that do not have normalization variants in the lexicon.", "labels": [], "entities": []}, {"text": "Evaluation on the test data set showed that our unconstrained system achieved the F-measure of 0.8272 (rank 1 out of 5 submissions for the unconstrained mode, rank 2 out of all 11 submissions).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9995385408401489}]}], "introductionContent": [{"text": "Social media texts found in such services as Twitter or Facebook have a great data-mining potential, as they offer real-time data that can be useful to monitor public opinion on brands, products, events, etc.", "labels": [], "entities": []}, {"text": "However, current Natural Language Processing systems are usually optimized for clean data, which is not the type of data found in social media texts, as they are often noisy, containing a lot of slang, typos, and abbreviations.", "labels": [], "entities": []}, {"text": "Normalizing such text is challenging.", "labels": [], "entities": []}, {"text": "We want to achieve high recall, making as many corrections as possible, but not at the expense of precision -words should not be incorrectly normalized.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.99928218126297}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9992080330848694}]}, {"text": "Previous approaches to this task incorporated different tools and methods: dictionaries, language models, finite state transducers, and machine translation models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7366720139980316}]}, {"text": "Some of the methods are unsupervised, though often requiring adjustment of parameters based on annotated data (Han and Baldwin (2011),).", "labels": [], "entities": []}, {"text": "Some are supervised, like that in, making use of a Conditional Random Field () to learn the sequences of edit operations from labelled data.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach based on the usage of normalization lexicons and a CRF model for identifying potential candidates.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus provided by the organizers consists of 2950 annotated tweets.", "labels": [], "entities": []}, {"text": "The annotations follow these guidelines (): \uf0b7 Non-standard words are normalized to one or more canonical English words based on a pre-defined lexicon.", "labels": [], "entities": []}, {"text": "For instance, lo v e should be normalized to love (many-to-one normalization), tmrw to tomorrow (one-toone normalization), and cu to see you (one-to-many normalization).", "labels": [], "entities": []}, {"text": "Additionally, IBM should be left untouched as it is in the lexicon and it is in its canonical form, and the informal lol should be expanded to laughing out loud.", "labels": [], "entities": [{"text": "IBM", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.5898383855819702}]}, {"text": "\uf0b7 Non-standard words maybe either out-ofvocabulary (OOV) tokens (e.g., tmrw for tomorrow) or in-vocabulary (IV) tokens (e.g., wit for within \"I will come wit you\").", "labels": [], "entities": []}, {"text": "\uf0b7 Only alphanumeric tokens (e.g., 2, 4eva and tmrw) and apostrophes used in contractions (e.g., yoou've) are considered for normalization.", "labels": [], "entities": []}, {"text": "Tokens including hyphens, single quotes and other types of contractions should be ignored.", "labels": [], "entities": []}, {"text": "\uf0b7 Domain specific entities are ignored even if they are in non-standard forms, e.g., #ttyl, @nyc \uf0b7 It is possible fora tweet to have no nonstandard tokens but still require normalization (e.g., the example of wit above), and it is also possible for the tweet to require no normalization whatsoever.", "labels": [], "entities": []}, {"text": "\uf0b7 Proper nouns should be left untouched, even if they are not in the given lexicon (e.g., Twitter).", "labels": [], "entities": []}, {"text": "\uf0b7 All normalizations should use the American spelling (e.g., tokenize rather than tokenise).", "labels": [], "entities": []}, {"text": "Evaluation was to be carried out according to Precision, Recall, and F1 metrics.", "labels": [], "entities": [{"text": "Precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9947339296340942}, {"text": "Recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.966289758682251}, {"text": "F1", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9995521903038025}]}, {"text": "First, a normalization lexicon was generated from the given training data, enriched with the data from several sources: \uf0b7 Word pairs extracted from the datasets used for lexical normalization (Han, 2011; Liu, 2011) \uf0b7 The online social media abbreviation list of Beal 1 . Compared to the previous workshops with one-to-one normalizations, the current task also considers oneto-many normalizations, and obviously not all abbreviations are present in the training data, so the use of a list of social media abbreviations can be vital to the system.", "labels": [], "entities": []}, {"text": "At the current stage of development the system is unable to differentiate between several normalization variants; thus, entries with multiple possible variants were reviewed to make the most suitable variant first in the list (entries that are most frequent in datasets are placed first, any ties were manually reviewed).", "labels": [], "entities": []}, {"text": "Second, a CRF model was trained.", "labels": [], "entities": [{"text": "CRF", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8433986306190491}]}, {"text": "The labels chosen were CAND and NOT_CAND, reflecting potential normalization candidates and words that should not be normalized, respectively.", "labels": [], "entities": [{"text": "CAND", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9860365390777588}, {"text": "NOT_CAND", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.814337432384491}]}, {"text": "The following features were used: Token: This feature represents the string of the current token.", "labels": [], "entities": []}, {"text": "Context Feature: The token to the left and the token to the right are used as two context features.", "labels": [], "entities": []}, {"text": "The surrounding words usually convey useful information about a token which helps in predicting the correct tag for each token.", "labels": [], "entities": []}, {"text": "Alphanumeric feature: This feature checks whether the token adheres to the annotation guidelines and makes sure that non-adhering tokens are not marked as potential candidates.", "labels": [], "entities": []}, {"text": "Normalization dictionary feature: This feature checks whether the token is present in the generated normalization lexicon.", "labels": [], "entities": []}, {"text": "Canonical lexicon feature: This feature indicates whether or not the token is present in the canonical lexicon provided by the workshop organizers.", "labels": [], "entities": []}, {"text": "Word length and number of vowels: Two separate features as well as their correlation, allowing to tag words with uncommon lengthvowel correlation, like bcz, pls, etc.", "labels": [], "entities": []}, {"text": "Edit distance feature: marks a token that is within an edit distance of 2 or less from any word in the canonical lexicon.", "labels": [], "entities": []}, {"text": "Third, the text is normalized: \uf0b7 All tokens tagged as potential candidates by the CRF model are normalized to their lexicon variants.", "labels": [], "entities": []}, {"text": "\uf0b7 All alphanumeric words are normalized to the American spelling with the VarCon tool (Atkinson, 2015) 2 . This includes the tokens which are already normalized using the lexicon.", "labels": [], "entities": [{"text": "VarCon tool (Atkinson, 2015)", "start_pos": 74, "end_pos": 102, "type": "DATASET", "confidence": 0.8772052441324506}]}, {"text": "\uf0b7 We have also tried to improve the normalization results by using a did-you-mean (DYM) module that is currently being developed at IHS R&D team.", "labels": [], "entities": [{"text": "normalization", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9334657788276672}, {"text": "IHS R&D team", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.8217036247253418}]}, {"text": "The DYM module corrects user queries/sentences with misspellings by providing corrected variant(s) with a confidence measure (including no correction variant with the corresponding confidence measure).", "labels": [], "entities": [{"text": "DYM module corrects user queries/sentences with misspellings", "start_pos": 4, "end_pos": 64, "type": "TASK", "confidence": 0.6474370658397675}]}, {"text": "The DYM module is an SVM model trained on a set of features for each of the multiple candidates generated for an input query/sentence.", "labels": [], "entities": []}, {"text": "We used the following features: error model score, Levenshtein distance, language model score, the ratio of common noun vocabulary words, the ratio of proper noun vocabulary words, and the number of changes in non-lowercase words.", "labels": [], "entities": [{"text": "error model score", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.9430077075958252}, {"text": "Levenshtein distance", "start_pos": 51, "end_pos": 71, "type": "METRIC", "confidence": 0.6996845304965973}]}, {"text": "An error model score was obtained from an autocompletion and autocorrection module (AAM) for which an index was built from 12.4M documents (scientific papers -42.1%, Wikipedia articles -23.5%, patents -19.4%, social texts -8%, and news -7%).", "labels": [], "entities": []}, {"text": "The 2-gram language model was built from 177K patents (1.36G words and 2.6M vocabulary).", "labels": [], "entities": []}, {"text": "Since we did not have enough time to tailor both DYM and AAM modules for social text processing, DYM and AAM modules were used for this Twitter lexical normalization system as is, being actually tailored for technical and scientific texts.", "labels": [], "entities": [{"text": "Twitter lexical normalization", "start_pos": 136, "end_pos": 165, "type": "TASK", "confidence": 0.6042880614598592}]}], "tableCaptions": [{"text": " Table 1. Result metrics of candidate CRF model  with different features (and its impact on the re- sult after normalization using a submitted sys- tem).", "labels": [], "entities": []}, {"text": " Table 2. Result metrics of two normalization sys- tem configurations.", "labels": [], "entities": []}]}