{"title": [{"text": "ParFDA for Fast Deployment of Accurate Statistical Machine Translation Systems, Benchmarks, and Statistics", "labels": [], "entities": [{"text": "ParFDA", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6661211848258972}, {"text": "Statistical Machine Translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.608513206243515}]}], "abstractContent": [{"text": "We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and obtain results close to the top with an average of 3.176 BLEU points difference using significantly less resources for building SMT systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.8101110855738322}, {"text": "statistical machine translation", "start_pos": 126, "end_pos": 157, "type": "TASK", "confidence": 0.643432637055715}, {"text": "WMT15) translation task", "start_pos": 180, "end_pos": 203, "type": "TASK", "confidence": 0.7665628492832184}, {"text": "BLEU", "start_pos": 265, "end_pos": 269, "type": "METRIC", "confidence": 0.998551070690155}, {"text": "SMT", "start_pos": 336, "end_pos": 339, "type": "TASK", "confidence": 0.9730337858200073}]}, {"text": "ParFDA is a parallel implementation of feature decay algorithms (FDA) developed for fast deployment of accurate SMT systems (Bi\u00e7ici, 2013; Bi\u00e7ici et al., 2014; Bi\u00e7ici and Yuret, 2015).", "labels": [], "entities": [{"text": "ParFDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6808165311813354}, {"text": "SMT", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.96098792552948}]}, {"text": "ParFDA Moses SMT system we built is able to obtain the top TER performance in French to English translation.", "labels": [], "entities": [{"text": "ParFDA Moses", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.6726101487874985}, {"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.5207133293151855}, {"text": "TER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9936659932136536}]}, {"text": "We make the data for building ParFDA Moses SMT systems for WMT15 available: https://github.", "labels": [], "entities": [{"text": "ParFDA Moses SMT", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.5621127982934316}, {"text": "WMT15", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8232827186584473}]}, {"text": "com/bicici/ParFDAWMT15.", "labels": [], "entities": [{"text": "ParFDAWMT15", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.6353396773338318}]}, {"text": "1 Parallel FDA5 (ParFDA) Statistical machine translation performance is influenced by the data: if you already have the translations for the source being translated in your training set or even portions of it, then the translation task becomes easier.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6488388081391653}]}, {"text": "If some token does not appear in your language model (LM), then it becomes harder for the SMT engine to find its correct position in the translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9864555597305298}]}, {"text": "The importance of ParFDA increases with the proliferation of training material available for building SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9902920126914978}]}, {"text": "Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 (Bojar et al., 2015) as well as the statistics of the ParFDA selected training and LM data.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.9004999399185181}]}, {"text": "ParFDA (Bi\u00e7ici, 2013; Bi\u00e7ici et al., 2014) runs separate FDA5 (Bi\u00e7ici and Yuret, 2015) models on randomized subsets of the training data and combines the selections afterwards.", "labels": [], "entities": [{"text": "ParFDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5323099493980408}]}, {"text": "FDA5 is available at http://github.com/bicici/FDA.", "labels": [], "entities": [{"text": "FDA5", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9475911855697632}]}, {"text": "We run ParFDA SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT15 (Bojar et al., 2015) and obtain SMT performance close to the top constrained Moses systems.", "labels": [], "entities": [{"text": "ParFDA SMT", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.5432572960853577}, {"text": "WMT15", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.9349463582038879}, {"text": "SMT", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.9884202480316162}]}, {"text": "ParFDA allows rapid prototyping of SMT systems fora given target domain or task.", "labels": [], "entities": [{"text": "ParFDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6949832439422607}, {"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9699673056602478}]}, {"text": "We use ParFDA for selecting parallel training data and LM data for building SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9898152351379395}]}, {"text": "We select the LM training data with ParFDA based on the following observation (Bi\u00e7ici, 2013): No word not appearing in the training set can appear in the translation.", "labels": [], "entities": [{"text": "ParFDA", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.8058093190193176}]}, {"text": "Thus we are only interested in correctly ordering the words appearing in the training corpus and collecting the sentences that contain them for building the LM.", "labels": [], "entities": []}, {"text": "At the same time, a compact and more relevant LM corpus is also useful for modeling longer range dependencies with higher order n-gram models.", "labels": [], "entities": []}, {"text": "We use 3-grams for selecting training data and 2-grams for LM corpus selection.", "labels": [], "entities": [{"text": "LM corpus selection", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7442389329274496}]}, {"text": "2 Results We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task (Bojar et al., 2015), which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru).", "labels": [], "entities": [{"text": "ParFDA SMT", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.5191131681203842}, {"text": "WMT15 translation task", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7856642206509908}]}, {"text": "We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (Stolcke, 2002) with-unk option.", "labels": [], "entities": []}, {"text": "For GIZA++ (Och and Ney, 2003), max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word 74", "labels": [], "entities": [{"text": "max-fertility", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9960368275642395}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data statistics for the available training and LM corpora in the constrained (C) setting compared  with the ParFDA selected training and LM data. #words is in millions (M) and #sents in thousands (K).", "labels": [], "entities": []}, {"text": " Table 2: The space and time required for building the ParFDA Moses SMT systems. The sizes are in  MB and time in minutes. PT stands for the phrase table. ALL does not contain the size of the LM.", "labels": [], "entities": [{"text": "ParFDA Moses", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.772394061088562}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.49809393286705017}, {"text": "PT", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9904436469078064}]}, {"text": " Table 3: BLEUc for ParFDA results, for the top constrained result in WMT15 (TopWMTC, from  matrix.statmt.org), their difference, and the ParFDA LM order used are presented. Average  difference is 3.176 BLEU points", "labels": [], "entities": [{"text": "BLEUc", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9991312623023987}, {"text": "WMT15", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.9289515614509583}, {"text": "Average  difference", "start_pos": 174, "end_pos": 193, "type": "METRIC", "confidence": 0.9570298790931702}, {"text": "BLEU", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9981562495231628}]}, {"text": " Table 4: Perplexity comparison of the LM built from the training corpus (train), ParFDA selected training  data (FDA5 train), and the ParFDA selected LM data (FDA5 LM). %red is proportion of reduction.", "labels": [], "entities": [{"text": "ParFDA selected training  data", "start_pos": 82, "end_pos": 112, "type": "DATASET", "confidence": 0.78423111140728}, {"text": "ParFDA selected LM data", "start_pos": 135, "end_pos": 158, "type": "DATASET", "confidence": 0.8086583018302917}, {"text": "reduction", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.8365629315376282}]}]}