{"title": [{"text": "Evaluation of Crowdsourced User Input Data for Spoken Dialog Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "Using the Internet for the collection of data is quite common these days.", "labels": [], "entities": []}, {"text": "This process is called crowdsourcing and enables the collection of large amounts of data at reasonable costs.", "labels": [], "entities": []}, {"text": "While being an inexpensive method, this data typically is of lower quality.", "labels": [], "entities": []}, {"text": "Filtering data sets is therefore required.", "labels": [], "entities": []}, {"text": "The occurring errors can be classified into different groups.", "labels": [], "entities": []}, {"text": "There are technical issues and human errors.", "labels": [], "entities": []}, {"text": "For speech recording, technical issues could be a noisy background.", "labels": [], "entities": [{"text": "speech recording", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8883520662784576}]}, {"text": "Human errors arise when the task is misunderstood.", "labels": [], "entities": []}, {"text": "We employ several techniques for recognizing errors and eliminating faulty data sets in user input data fora Spoken Dialog System (SDS).", "labels": [], "entities": [{"text": "Spoken Dialog System (SDS)", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.5275818556547165}]}, {"text": "Furthermore, we compare three different kinds of questionnaires (QNRs) fora given set of seven tasks.", "labels": [], "entities": []}, {"text": "We analyze the characteristics of the resulting data sets and give a recommendation which type of QNR might be the most suitable one fora given purpose.", "labels": [], "entities": []}], "introductionContent": [{"text": "Similar to research in other areas, Automatic Speech Recognition (ASR) systems and SDSs are facing the challenge how to get new training data, e. g., if there is the urge to cover new domains.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.7697928696870804}]}, {"text": "Until several years ago, a common procedure was to record the required audio samples in an anechoic chamber and let experts (e. g., linguistics students) create the transcriptions.", "labels": [], "entities": []}, {"text": "Although the data collected via this method is of high quality and can be used as a gold standard, researchers found that this approach is very time-consuming and results in quite little data related to the effort.", "labels": [], "entities": []}, {"text": "A few years ago, companies like Amazon Mechanical Turk started to offer so-called crowdsourcing approaches, which means that Human Intelligence Tasks (HITs) are performed by a group of non-experts.", "labels": [], "entities": []}, {"text": "Furthermore, these tasks are open calls and are assigned to the different crowdsource workers.", "labels": [], "entities": []}, {"text": "Especially in industrial contexts, crowdsourcing seems to be the means to choose because development cycles are short and much data for ASR or SDS development can be generated right as it is needed, although the data collected needs to be checked for quality.", "labels": [], "entities": [{"text": "ASR or SDS development", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7867792397737503}]}, {"text": "Our work analyzes crowdsourced data collected by the company Clickworker (.", "labels": [], "entities": []}, {"text": "The data consists of user input to an in-car SDS, where the crowdworkers had to input one German utterance for each of the seven tasks, after which they had to transcribe the utterance themselves.", "labels": [], "entities": []}, {"text": "This procedure was conducted for three different types of QNRs: pictures, semantics, and text.", "labels": [], "entities": []}, {"text": "We show the differences among these QNRs as well as an overall quality evaluation of the collected data.", "labels": [], "entities": []}, {"text": "For this, we make use of Natural Language Processing (NLP) tools.", "labels": [], "entities": []}], "datasetContent": [{"text": "To be able to tell the overall quality of the underlying corpus, we had to analyze the self-entered transcripts, too.", "labels": [], "entities": []}, {"text": "For this purpose, we developed an NLP analysis chain which contains a large part of preprocessing (i. e. mainly cleaning the text) apart from the actual analysis.", "labels": [], "entities": []}, {"text": "Concerning preprocessing, we first applied a basic tokenizer to split the punctuation marks from the rest of the text.", "labels": [], "entities": []}, {"text": "Second, we went over the transcripts with a spellchecker called LanguageTool (https://www.languagetool.org/).", "labels": [], "entities": [{"text": "LanguageTool", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9178525805473328}]}, {"text": "For all misspelled words, we checked whether it equals one of the predefined, special keywords which should be entered for the current task (e. g., \"Michael Jackson\", \"Stieglitzweg\").", "labels": [], "entities": []}, {"text": "If such a keyword was found, we processed the next word; if not, we checked which of the correct alternatives proposed by LanguageTool is most similar to one of the words on our \"synonymously used words\" list by using the Levenshtein distance.", "labels": [], "entities": []}, {"text": "Third, after deciding which spelling is the most appropriate one for each word, we store the corrected utterances and use them for further analysis.", "labels": [], "entities": []}, {"text": "The latter included Part-of-Speech (POS) Tagging with the TreeTagger to investigate, which and how many different POS patterns, i.e. types of sentence patterns, occur in the corpus and how the QNRs differ from each other on this level.", "labels": [], "entities": [{"text": "Part-of-Speech (POS) Tagging", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6255083799362182}]}, {"text": "Further, we investigated the most frequent words used in each task, and how many words in total are used in a specific task and in a specific QNR.", "labels": [], "entities": []}, {"text": "With our analysis, we provide answers to the following questions: (a) How large is the linguistic variation in the data set (on sentence and word level)?", "labels": [], "entities": []}, {"text": "(b) Which pros and cons do the presented QNRs have?", "labels": [], "entities": []}, {"text": "(c) Which QNR is the right one fora certain purpose?", "labels": [], "entities": []}, {"text": "We present the results in Section 4.2.", "labels": [], "entities": []}, {"text": "To determine the usability of the recordings, we compared the length of the recordings and analyzed them using an ASR system.", "labels": [], "entities": []}, {"text": "Generally, we assume that most recordings are done appropriately and that their quality resembles a normal distribution.", "labels": [], "entities": []}, {"text": "We conducted our analysis using the Janus Recognition Toolkit (JRTk) which features the IBIS decoder ().", "labels": [], "entities": [{"text": "IBIS decoder", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.8693702816963196}]}, {"text": "For each task, a certain answer length is expected.", "labels": [], "entities": []}, {"text": "This length may vary, but a significantly shorter or longer audio file indicates an error.", "labels": [], "entities": []}, {"text": "Whether due to a technically false recording setup or a misunderstanding of the task description, in both cases the recording needs to be discarded.", "labels": [], "entities": []}, {"text": "Even if the length is within a suitable range, the transcription of the audio might be wrong.", "labels": [], "entities": []}, {"text": "To see if the transcription matches the spoken words, we use JRTk to perform a forced alignment.", "labels": [], "entities": [{"text": "JRTk", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8546654582023621}]}, {"text": "We use a GMM/HMM-based recognizer for German with 6,000 context-dependent quintphone states for aligning a phoneme sequence to the audio using forced Viterbi alignment.", "labels": [], "entities": []}, {"text": "If there is a mismatch between audio and transcriptions, there will be phonemes covering unusual long or short parts of the audio.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average length of recordings.", "labels": [], "entities": [{"text": "Average length", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9141411483287811}]}, {"text": " Table 2: Picture QNR with its dismissal rate.", "labels": [], "entities": [{"text": "dismissal rate", "start_pos": 31, "end_pos": 45, "type": "METRIC", "confidence": 0.9868246018886566}]}, {"text": " Table 3: Semantics QNR with its dismissal rate.", "labels": [], "entities": [{"text": "Semantics QNR", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8648341000080109}, {"text": "dismissal rate", "start_pos": 33, "end_pos": 47, "type": "METRIC", "confidence": 0.9858798384666443}]}, {"text": " Table 4: Variance of used words across all QNRs.", "labels": [], "entities": []}, {"text": " Table 5: Most common sentences for tasks 1, 4, 5.", "labels": [], "entities": []}]}