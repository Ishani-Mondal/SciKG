{"title": [{"text": "A Discriminative Model for Semantics-to-String Translation", "labels": [], "entities": [{"text": "Semantics-to-String Translation", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.8803817927837372}]}], "abstractContent": [{"text": "We present a feature-rich discriminative model for machine translation which uses an abstract semantic representation on the source side.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8054618835449219}]}, {"text": "We include our model as an additional feature in a phrase-based de-coder and we show modest gains in BLEU score in an n-best re-ranking experiment.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9827536940574646}]}], "introductionContent": [{"text": "The goal of machine translation is to take source language utterances and convert them into fluent target language utterances with the same meaning.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7744883000850677}]}, {"text": "Most recent approaches learn transformations using statistical techniques on parallel data.", "labels": [], "entities": []}, {"text": "Meaning equivalent representations of words and phrases are learned directly from natural data, as are other syntactic operations such as reordering.", "labels": [], "entities": []}, {"text": "However, commonly used methods have a very simple view of the linguistic data.", "labels": [], "entities": []}, {"text": "Each word is generally modeled independently, for instance, and the relations between words are generally captured only in fixed phrases or as syntactic relationships.", "labels": [], "entities": []}, {"text": "Recently there has been a resurgence of interest in unified semantic representations: deep analyses with heavy normalization of morphology, syntax, and even semantic representations.", "labels": [], "entities": []}, {"text": "In particular, Abstract Meaning Representation (AMR,) is a novel representation of (sentential) semantics.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.822381055355072}]}, {"text": "Such representations could influence a number of natural language understanding and generation tasks, particularly machine translation.", "labels": [], "entities": [{"text": "natural language understanding and generation", "start_pos": 49, "end_pos": 94, "type": "TASK", "confidence": 0.708702278137207}, {"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7844059765338898}]}, {"text": "Deeper models can be used for multiple aspects of the translation modeling problem.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.9854900240898132}]}, {"text": "Building translation models that rely on a deeper representation of the input allows fora more parsimonious translation model: morphologically related words can be handled in a unified manner; semantically related concepts are immediately adjacent and available for modeling, etc.", "labels": [], "entities": []}, {"text": "Language models using deep representations might help us model which interpretations are more plausible.", "labels": [], "entities": []}, {"text": "We present an initial discriminative method for modeling the likelihood of a target language surface string given source language deep semantics.", "labels": [], "entities": []}, {"text": "This approach relies on an automatic parser for source language semantics.", "labels": [], "entities": []}, {"text": "We use a system that parses into AMR-like structures, and apply the resulting model as an additional feature in a translation system.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our model in an n-best re-ranking experiment.", "labels": [], "entities": []}, {"text": "We began by training a basic phrase-based MT system for English\u2192French on 1 million parallel sentence pairs and produced 1000-best lists for three test sets provided for the Workshop on Statistical Machine Translation (.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9071431756019592}, {"text": "Statistical Machine Translation", "start_pos": 186, "end_pos": 217, "type": "TASK", "confidence": 0.6873498857021332}]}, {"text": "This system had a set of 13 commonly used features: four channel model scores (forward and backward MLE and lexical weighting scores), a 5-gram language model, five lexicalized reordering model scores (corresponding to different ordering outcomes), linear distortion penalty, word count, and phrase count.", "labels": [], "entities": [{"text": "linear distortion penalty", "start_pos": 249, "end_pos": 274, "type": "METRIC", "confidence": 0.7990199327468872}]}, {"text": "The system was optimized using minimum error rate training: BLEU scores of n-best reranking in English\u2192French translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9994710087776184}]}, {"text": "For reranking, we gathered 1000-best lists for the development and test sets.", "labels": [], "entities": [{"text": "reranking", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9664515256881714}]}, {"text": "We added six scores from our model to each translation in the n-best lists.", "labels": [], "entities": []}, {"text": "We included the total log probability, the sum of unnormalized scores, and the rank of the given output.", "labels": [], "entities": []}, {"text": "In addition, we had count features indicating the number of words that were not in the GEN set of the model, the number of NULLs (effectively deleted nodes), and a count of times a target word appeared in a stopword list.", "labels": [], "entities": [{"text": "GEN set", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9066944420337677}]}, {"text": "In the end, each translation had a total of 19 features: 13 from the original features and 6 from this approach.", "labels": [], "entities": []}, {"text": "Next, we ran one iteration of the MERT optimizer on these 1000-best lists for all of the features.", "labels": [], "entities": []}, {"text": "Because this was a reranking experiment rather than decoding, we did not repeatedly gather n-best lists as in decoding.", "labels": [], "entities": []}, {"text": "The resulting feature weights were used to rescore the test n-best lists and evaluated the using BLEU; shows the results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9986409544944763}]}, {"text": "We obtained a modest but consistent improvement.", "labels": [], "entities": []}, {"text": "Once the model is used directly in the decoder, the gains should increase as it will be able to influence decoding.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores of n-best reranking in  English\u2192French translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994052648544312}]}]}