{"title": [{"text": "INVITED TALK: How Much Information Does a Human Translator Add to the Original?", "labels": [], "entities": [{"text": "INVITED", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5821763277053833}, {"text": "TALK", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.5290336608886719}]}], "abstractContent": [{"text": "It is well-known that natural language has built-in redundancy.", "labels": [], "entities": []}, {"text": "By using context, we can often guess the next word or character in a text.", "labels": [], "entities": []}, {"text": "Two practical communities have independently exploited this fact.", "labels": [], "entities": []}, {"text": "First, automatic speech and translation researchers build language models to distinguish fluent from non-fluent outputs.", "labels": [], "entities": [{"text": "automatic speech and translation", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.5965534299612045}]}, {"text": "Second, text compression researchers convert predictions into short encodings, to save disk space and bandwidth.", "labels": [], "entities": [{"text": "text compression", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7994582653045654}]}, {"text": "I will explore what these two communities can learn from each others' (interestingly different) solutions.", "labels": [], "entities": []}, {"text": "Then I will look at the less-studied question of redundancy in bilingual text, addressing questions like \"How well can we predict human translator behavior?\" and \"How much information does a human translator add to the original?\"", "labels": [], "entities": []}, {"text": "(This is joint work with Barret Zoph and Marjan Ghazvininejad.)", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}