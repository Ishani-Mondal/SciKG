{"title": [{"text": "Translating Negation: A Manual Error Analysis", "labels": [], "entities": [{"text": "Translating Negation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8568501770496368}]}], "abstractContent": [{"text": "Statistical Machine Translation has come along way improving the translation quality of a range of different linguistic phenomena.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7439466118812561}]}, {"text": "With negation however, techniques proposed and implemented for improving translation performance on negation have simply followed from the developers' beliefs about why performance is worse.", "labels": [], "entities": []}, {"text": "These beliefs, however, have never been validated by an error analysis of the translation output.", "labels": [], "entities": []}, {"text": "In contrast, the current paper shows that an informative empirical error analysis can be formulated in terms of (1) the set of semantic elements involved in the meaning of negation, and (2) a small set of string-based operations that can char-acterise errors in the translation of those elements.", "labels": [], "entities": []}, {"text": "Results on a Chinese-to-English translation task confirm the robustness of our analysis cross-linguistically and the basic assumptions can inform an automated investigation into the causes of translation errors.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.7291972537835439}]}, {"text": "Conclusions drawn from this analysis should guide future work on improving the translation of negative sentences.", "labels": [], "entities": [{"text": "translation of negative sentences", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.883531853556633}]}], "introductionContent": [{"text": "In recent years, there has been increasing interest in improving the quality of SMT systems over a wide range of linguistic phenomena, including coreference resolution) and modality (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9938302636146545}, {"text": "coreference resolution", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.9223351180553436}]}, {"text": "Amongst these, however, translating negation is still a problem that has not been researched thoroughly.", "labels": [], "entities": [{"text": "translating negation", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.9024057388305664}]}, {"text": "This paper takes an empirical approach towards understanding why negation is a problem in SMT.", "labels": [], "entities": [{"text": "negation", "start_pos": 65, "end_pos": 73, "type": "TASK", "confidence": 0.9700852036476135}, {"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9937607645988464}]}, {"text": "More specifically, we try to answer two main questions: 1.", "labels": [], "entities": []}, {"text": "What kind of errors are involved in translating negation?", "labels": [], "entities": [{"text": "translating negation", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.9052392840385437}]}, {"text": "2. What are the causes of these errors during decoding?", "labels": [], "entities": []}, {"text": "While previous work (section 2) has shown that translating negation is a problem, it has not addressed either of these questions.", "labels": [], "entities": [{"text": "translating negation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.9187575280666351}]}, {"text": "The present paper focuses on the first one; we show that tailoring to a semantic task, string-based error categories standardly used to evaluate the quality of the machine translation output, allows us to cover the wide range of errors occurring while translating negative sentences (section 3).", "labels": [], "entities": []}, {"text": "We report the results of the analysis of a Hierarchical Phrase Based Model on a Chineseto-English translation task (section 4), where we show that all error categories occur to some extent with scope reordering being the most frequent (section 5).", "labels": [], "entities": [{"text": "Chineseto-English translation task", "start_pos": 80, "end_pos": 114, "type": "TASK", "confidence": 0.6193082332611084}]}, {"text": "Addressing question (2) requires connecting the assumptions behind this manual error analysis to errors occurring along the translation pipeline.", "labels": [], "entities": []}, {"text": "As such, we complete the analysis by briefly introduce an automatic method to investigate the causes of the errors at decoding time (section 6).", "labels": [], "entities": []}, {"text": "Conclusion and future works are reported in section 7 and 8.", "labels": [], "entities": []}, {"text": "In recent years, automatic recognition of negation has been the focus of considerable work.", "labels": [], "entities": [{"text": "automatic recognition of negation", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.740844301879406}]}, {"text": "The *SEM 2012 shared task represented a first attempt to apply machine learning methods to the problem of automatically detect the aforementioned elements in English.", "labels": [], "entities": [{"text": "*SEM 2012 shared task", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.6029022634029388}]}, {"text": "In particular CRFs and SVMs, making use of syntactic (both constituent and dependency based) clues, were shown to lead to the best results in a supervised machine learning setting.", "labels": [], "entities": []}, {"text": "The shared task also saw the release of a fully annotated corpus in the literature domain, which represents, along with the BioScope corpus (, the only resource specifically annotated for negation.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.9326843023300171}, {"text": "negation", "start_pos": 188, "end_pos": 196, "type": "TASK", "confidence": 0.9579142332077026}]}, {"text": "There were also a few attempts in automatically detecting negation in Chinese texts.", "labels": [], "entities": [{"text": "automatically detecting negation in Chinese texts", "start_pos": 34, "end_pos": 83, "type": "TASK", "confidence": 0.7897511223951975}]}, {"text": "designed a negation detection algorithm based on syntactic patterns; similarly, implemented an FSA for automatic recognition of negation structures in Chinese medical texts, using a list of manually defined cues and the syntactic structures they appear in.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9618817269802094}, {"text": "FSA", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.8953213691711426}, {"text": "automatic recognition of negation structures in Chinese medical texts", "start_pos": 103, "end_pos": 172, "type": "TASK", "confidence": 0.8047528001997206}]}, {"text": "Ina bilingual setting such as the SMT, however, most work has only considered negation as aside problem.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9763135313987732}]}, {"text": "For this reason, no actual analysis on the type of errors involved in translating negation or their causes has been specifically carried out.", "labels": [], "entities": [{"text": "translating negation", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8729269802570343}]}, {"text": "Given that all these works assess the quality of translation of negative sentences using an n-gram overlap metric, there is no certainty whether any improvement derives from a better rendering of negation or from other, non-negation related elements.", "labels": [], "entities": []}, {"text": "Evaluating the semantic adequacy of the SMT output has also stimulated interest in recent years.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9938825368881226}]}, {"text": "Traditional error categories, such as the ones presented in), are mostly based on n-gram overlap between hypothesis and reference and so are the most widely used automatic evaluation metrics used in SMT (e.g. BLEU () and TER ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 199, "end_pos": 202, "type": "TASK", "confidence": 0.9934808015823364}, {"text": "BLEU", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.9956364035606384}, {"text": "TER", "start_pos": 221, "end_pos": 224, "type": "METRIC", "confidence": 0.9852787256240845}]}, {"text": "In contrast, MEANT) and its human counterpart, HMEANT, attempt to abstract from simple string matching and assess the degree of semantic similarity between machine output and reference sentence.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.7259057760238647}]}, {"text": "To do so, both sides are annotated using Propbank-like semantic labels, and the fillers matched if both sides contain the same event.", "labels": [], "entities": []}, {"text": "To assign a score to the test set evaluated, an F 1 measure over precision and recall of matched fillers is then computed.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9835471312204996}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.997931957244873}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9982138872146606}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results from the error analysis of the 250 sen- tences randomly extracted from the IWSLT2014 test set.", "labels": [], "entities": [{"text": "IWSLT2014 test set", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.9699066877365112}]}]}