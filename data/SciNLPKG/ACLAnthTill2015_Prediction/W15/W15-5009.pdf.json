{"title": [{"text": "An Awkward Disparity between BLEU / RIBES Scores and Human Judgements in Machine Translation", "labels": [], "entities": [{"text": "Awkward Disparity", "start_pos": 3, "end_pos": 20, "type": "METRIC", "confidence": 0.8903389573097229}, {"text": "BLEU / RIBES Scores", "start_pos": 29, "end_pos": 48, "type": "METRIC", "confidence": 0.8274136185646057}, {"text": "Human Judgements in Machine Translation", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.6728990077972412}]}], "abstractContent": [{"text": "Automatic evaluation of machine translation (MT) quality is essential in developing high quality MT systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.8274532973766326}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9898920059204102}]}, {"text": "Despite previous criticisms, BLEU remains the most popular machine translation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9976785778999329}, {"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7711420357227325}]}, {"text": "Previous studies on the schism between BLEU and manual evaluation highlighted the poor correlation between MT systems with low BLEU scores and high manual evaluation scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9821500182151794}, {"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.9798682332038879}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9979206919670105}]}, {"text": "Alternatively, the RIBES metric-which is more sensitive to reordering-has shown to have better correlations with human judgements, but in our experiments it also fails to correlate with human judgements.", "labels": [], "entities": [{"text": "RIBES metric-which", "start_pos": 19, "end_pos": 37, "type": "METRIC", "confidence": 0.8502436876296997}]}, {"text": "In this paper we demonstrate, via our submission to the Workshop on Asian Translation 2015 (WAT 2015), a patent translation system with very high BLEU and RIBES scores and very poor human judgement scores.", "labels": [], "entities": [{"text": "Asian Translation 2015 (WAT 2015)", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.8032216855457851}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9993298053741455}, {"text": "RIBES", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9692168831825256}]}], "introductionContent": [{"text": "Automatic Machine Translation (MT) evaluation metrics have been criticized fora variety of reasons ().", "labels": [], "entities": [{"text": "Automatic Machine Translation (MT) evaluation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8068914456026894}]}, {"text": "However, the relatively consistent correlation of higher BLEU scores () and better human judgements in major machine translation shared tasks has led to the conventional wisdom that translations with significantly higher BLEU scores generally suggests a better translation than its lower scoring counterparts ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9982112646102905}, {"text": "BLEU", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9908434152603149}]}, {"text": "Callison- has anecdotally presented possible failures of BLEU by showing examples of translations with the same BLEU score but of different translation quality.", "labels": [], "entities": [{"text": "Callison", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9351357221603394}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.8567877411842346}, {"text": "BLEU score", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9787969291210175}]}, {"text": "Through meta-evaluation 1 of BLEU scores and human judgements scores of the 2005 NIST MT Evaluation exercise, they have also showed high correlations of R 2 = 0.87 (for adequacy) and R 2 = 0.74 (for fluency) when an outlier rule-based machine translation system with poor BLEU score and high human score is excluded; when included the correlations drops to 0.14 for adequacy and 0.74 for fluency.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9961400628089905}, {"text": "NIST MT Evaluation exercise", "start_pos": 81, "end_pos": 108, "type": "DATASET", "confidence": 0.8582288920879364}, {"text": "R 2", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9810340106487274}, {"text": "R 2", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.9772221148014069}, {"text": "BLEU score", "start_pos": 272, "end_pos": 282, "type": "METRIC", "confidence": 0.9739844501018524}]}, {"text": "Despite showing the poor correlation between BLEU and human scores, had only empirically meta-evaluated a scenario where low BLEU score does not necessary result in a poor human judgement score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9977753758430481}, {"text": "BLEU score", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9864852726459503}]}, {"text": "In this paper, we demonstrate a real-world example of machine translation that yielded high automatic evaluation scores but failed to obtain a good score on manual evaluation in an MT shared task submission.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8043540120124817}, {"text": "MT shared task submission", "start_pos": 181, "end_pos": 206, "type": "TASK", "confidence": 0.8939919322729111}]}, {"text": "originally define BLEU n-gram precision p n by summing the n-gram matches for every hypothesis sentence S in the test corpus C:", "labels": [], "entities": [{"text": "BLEU n-gram precision", "start_pos": 18, "end_pos": 39, "type": "METRIC", "confidence": 0.8837229013442993}]}], "datasetContent": [{"text": "We describe our system submission 4 to the WAT 2015 shared task () for Korean to Japanese patent translation.", "labels": [], "entities": [{"text": "WAT 2015 shared task", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.5179453194141388}, {"text": "Korean to Japanese patent translation", "start_pos": 71, "end_pos": 108, "type": "TASK", "confidence": 0.5869268596172332}]}, {"text": "The Japan Patent Office (JPO) Patent Corpus is the official resource provided for the shared task.", "labels": [], "entities": [{"text": "Japan Patent Office (JPO) Patent Corpus", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.9382515028119087}]}, {"text": "The training dataset is made up of 1 million sentences (250k each from the chemistry, electricity, mechanical engineering and physics do- \u2022 Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations \u2022 To minimize the computing load on the translation model, we compressed the phrasetable and lexical reordering model (JunczysDowmunt, 2012) \u2022 Language modeling is trained using KenLM using 5-grams, with modified Kneser-Ney smoothing).", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 388, "end_pos": 405, "type": "TASK", "confidence": 0.7261497378349304}]}, {"text": "The language model is quantized to reduce filesize and improve querying speed ().", "labels": [], "entities": []}, {"text": "\u2022 Minimum Error Rate Training (MERT) to tune the decoding parameters.", "labels": [], "entities": [{"text": "Minimum Error Rate Training (MERT", "start_pos": 2, "end_pos": 35, "type": "METRIC", "confidence": 0.9320274889469147}]}, {"text": "dev.txt and devtest.txt  The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform.", "labels": [], "entities": [{"text": "WAT evaluations", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.885136067867279}]}, {"text": "Human evaluators were randomly assigned documents from the test set.", "labels": [], "entities": []}, {"text": "They were shown the source document, the hypothesis translation and a baseline translation generated by the baseline phrase-based MT system.", "labels": [], "entities": [{"text": "hypothesis translation", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7193074524402618}, {"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9226810932159424}]}, {"text": "We perform a segment level meta-evaluation by calculating the BLEU and RIBES score difference for each hypothesis-baseline translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9992936849594116}, {"text": "RIBES score difference", "start_pos": 71, "end_pos": 93, "type": "METRIC", "confidence": 0.9705402851104736}]}, {"text": "show the correlations of the BLEU and RIBES score difference against the positive and negative human judgements score for every sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9993507266044617}, {"text": "RIBES score difference", "start_pos": 38, "end_pos": 60, "type": "METRIC", "confidence": 0.9720053275426229}]}, {"text": "presents the considerable incongruity between our system's high BLEU improvements (>+60 BLEU) being rated marginally better than the baseline translation, indicated by the orange and blue bubbles on the top right corner.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9991028308868408}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9980242252349854}]}, {"text": "There were even translations from our system with >+40 BLEU improvements that tied with the organizer's baseline translations, indicated by the grey bubbles at around the +40 BLEU and +5 RIBES region.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9975026249885559}, {"text": "BLEU", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9972698092460632}, {"text": "RIBES", "start_pos": 187, "end_pos": 192, "type": "METRIC", "confidence": 0.9729361534118652}]}, {"text": "Except for the a portion of segments that scored worse than the baseline system (lower right part of the graph where BLEU and RIBES falls below 0), the overall trend in presents the conventional wisdom that the BLEU improvements from our systems reflects positive human judgement scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9985333681106567}, {"text": "RIBES", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.9818097352981567}, {"text": "BLEU", "start_pos": 211, "end_pos": 215, "type": "METRIC", "confidence": 0.9924690127372742}]}, {"text": "However, presents the awkward disparity where many segments with BLEU improvements were rated strongly as poorer translations when compared against the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9974271655082703}]}, {"text": "Also, many segments with high BLEU improvements were tied with the baseline translations, indicated by the grey bubbles across the positive BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9988368153572083}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9980093836784363}]}, {"text": "As shown in the examples in Section 2, a number of prominent factors contribute to these disparity in high BLEU / RIBES improvements and low HUMAN judgement scores: \u2022 Minor lexical differences causing a huge difference in n-gram precision \u2022 Crowd-sourced vs. expert preferences on terminology, especially for patents code instead of Unicode.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9988957643508911}, {"text": "RIBES", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.5175208449363708}, {"text": "HUMAN judgement scores", "start_pos": 141, "end_pos": 163, "type": "METRIC", "confidence": 0.9409732421239217}, {"text": "precision", "start_pos": 229, "end_pos": 238, "type": "METRIC", "confidence": 0.8403118252754211}]}, {"text": "But the decoder could still output Unicode since our Japanese data was successfully tokenized using MeCab, we submitted this output under the submission name Byte2String; the Byte2String submission is not reported in this paper.", "labels": [], "entities": [{"text": "MeCab", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9334893822669983}]}, {"text": "Later we rectified the encoding problem by using KoNLPy and re-ran the alignment, phrase extraction, MERT and decoding, hence the submission name, Unicode2String, i.e. the system reported in  \u2022 Minor MT evaluation metric differences not reflecting major translation inadequacy Each of these failures contributes to an increased amount of disparity between the automatic translation metric improvements and human judgement scores.", "labels": [], "entities": [{"text": "KoNLPy", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9055619835853577}, {"text": "phrase extraction", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7534965872764587}, {"text": "MERT", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9832354784011841}, {"text": "MT evaluation", "start_pos": 200, "end_pos": 213, "type": "TASK", "confidence": 0.8745653927326202}]}], "tableCaptions": [{"text": " Table 1: Differences between Organizer's and our Phrase-based SMT system", "labels": [], "entities": [{"text": "Phrase-based SMT", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.4752969592809677}]}, {"text": " Table 2: BLEU and HUMAN scores for WAT  2015", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993083477020264}, {"text": "HUMAN", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9845110177993774}, {"text": "WAT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9148434996604919}]}]}