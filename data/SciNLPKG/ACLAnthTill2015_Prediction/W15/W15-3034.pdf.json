{"title": [{"text": "Investigations on Phrase-based Decoding with Recurrent Neural Network Language and Translation Models", "labels": [], "entities": [{"text": "Phrase-based Decoding", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.9180318713188171}]}], "abstractContent": [{"text": "This work explores the application of recurrent neural network (RNN) language and translation models during phrase-based decoding.", "labels": [], "entities": [{"text": "recurrent neural network (RNN) language and translation", "start_pos": 38, "end_pos": 93, "type": "TASK", "confidence": 0.6079090237617493}]}, {"text": "Due to their use of unbounded context, the decoder integration of RNNs is more challenging compared to the integration of feedforward neural models.", "labels": [], "entities": []}, {"text": "In this paper, we apply approximations and use caching to enable RNN de-coder integration, while requiring reasonable memory and time resources.", "labels": [], "entities": [{"text": "RNN de-coder integration", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.7805522878964742}]}, {"text": "We analyze the effect of caching on translation quality and speed, and use it to integrate RNN language and translation models into a phrase-based decoder.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, no previous work has discussed the integration of RNN translation models into phrase-based decoding.", "labels": [], "entities": [{"text": "RNN translation", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.8869173228740692}]}, {"text": "We also show that a special RNN can be integrated efficiently without the need for approximations.", "labels": [], "entities": []}, {"text": "We compare decoding using RNNs to rescoring n-best lists on two tasks: IWSLT 2013 German\u2192English, and BOLT Arabic\u2192English.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English", "start_pos": 71, "end_pos": 96, "type": "DATASET", "confidence": 0.9028432369232178}, {"text": "BOLT", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.8387230634689331}]}, {"text": "We demonstrate that the performance of decoding with RNNs is at least as good as using them in rescoring.", "labels": [], "entities": []}], "introductionContent": [{"text": "Applying neural networks to statistical machine translation has been gaining increasing attention recently.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.709037572145462}]}, {"text": "Neural network language and translation models have been successfully applied to rescore the first-pass decoding output ().", "labels": [], "entities": []}, {"text": "These models include feedforward and recurrent neural networks.", "labels": [], "entities": []}, {"text": "A more ambitious move is to apply neural networks directly during decoding, which in principle should give the models a better chance to influence translation in comparison to rescoring, as rescoring is limited to scoring and reranking fixed nbest lists.", "labels": [], "entities": []}, {"text": "Recently, neural networks were used for standalone decoding using a simple beam-search word-based decoder.", "labels": [], "entities": []}, {"text": "Another approach is to apply neural models directly in a phrase-based decoder.", "labels": [], "entities": []}, {"text": "We focus on this approach, which is challenging since phrase-based decoding typically involves generating tensor even hundreds of millions of partial hypotheses.", "labels": [], "entities": []}, {"text": "Scoring such a number of hypotheses using neural models is expensive, mainly due to the usually large output layer.", "labels": [], "entities": []}, {"text": "Nevertheless, decoder integration has been done in) for feedforward neural language models.", "labels": [], "entities": [{"text": "decoder integration", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.8672719895839691}]}, {"text": "integrate feedforward translation models into phrase-based decoding reporting major improvements, which highlight the strength of the underlying models.", "labels": [], "entities": [{"text": "feedforward translation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.5922974944114685}]}, {"text": "In related fields like e. g. language modeling, RNNs has been shown to perform considerably better than standard feedforward architectures).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7342005372047424}]}, {"text": "also show that RNN translation models outperform feedforward networks in rescoring.", "labels": [], "entities": [{"text": "RNN translation", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.8807751834392548}]}, {"text": "Given the success of feedforward translation models in phrase-based decoding, it is natural to ask how RNN translation models perform if they are integrated in decoding.", "labels": [], "entities": [{"text": "RNN translation", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.7809269428253174}]}, {"text": "This paper investigates the performance of RNN language and translation models in phrasebased decoding.", "labels": [], "entities": [{"text": "RNN language and translation", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.5945029854774475}]}, {"text": "For RNNs that depend on an unbounded target context, their integration into a phrase-based decoder employing beam search requires relaxing the pruning parameters, which makes translation inefficient.", "labels": [], "entities": []}, {"text": "Therefore, we apply approximations to integrate RNN translation models during phrase-based decoding.", "labels": [], "entities": [{"text": "RNN translation", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.8575736880302429}]}, {"text": "use approximate scoring to integrate an RNN language model (LM), but to the best of our knowledge, no work yet has explored the integration of RNN translation models.", "labels": [], "entities": [{"text": "RNN translation", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.7615120112895966}]}, {"text": "In addition to approximate models, we integrate a special RNN model that only depends on the source context, allowing for exact, yet efficient integration into the decoder.", "labels": [], "entities": []}, {"text": "We provide a detailed comparison between using the RNN models in decoding vs. rescoring on two tasks: IWSLT 2013 German\u2192English and BOLT Arabic\u2192English.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English", "start_pos": 102, "end_pos": 127, "type": "DATASET", "confidence": 0.8991385579109192}, {"text": "BOLT", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.8415378332138062}]}, {"text": "In addition, we analyze the approximation effect on translation speed and quality.", "labels": [], "entities": []}, {"text": "Our integration follows ( ), which uses caching strategies to apply an RNN LM in speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7906352877616882}]}, {"text": "This can be viewed as a modification of the approximation introduced by , allowing fora flexible choice between translation quality and speed.", "labels": [], "entities": []}, {"text": "We choose to integrate the word-based RNN translation models that were introduced in), due to their success in rescoring nbest lists.", "labels": [], "entities": [{"text": "RNN translation", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7233514189720154}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we review the related work.", "labels": [], "entities": []}, {"text": "The RNN LM integration and caching strategies are discussed in Section 3, while Section 4 discusses the integration of exact and approximate RNN translation models.", "labels": [], "entities": [{"text": "RNN LM integration", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7909958362579346}]}, {"text": "We analyze the effect of approximation and caching on translation quality and speed in Section 5.", "labels": [], "entities": []}, {"text": "The section also includes the translation experiments comparing decoding vs. rescoring.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.963419497013092}]}, {"text": "Finally we conclude with Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: A comparison between storing the RNN  state in the search nodes (last entry) or caching it  using different caching orders (remaining entries).  We report the BLEU [%] scores for the IWSLT  2013 German\u2192English task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9994831085205078}, {"text": "IWSLT  2013 German\u2192English task", "start_pos": 193, "end_pos": 224, "type": "DATASET", "confidence": 0.8784539600213369}]}, {"text": " Table 3: IWSLT 2013 German\u2192English results.  Caching orders: n = 8 (LM), n = 5 (JM).", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English results", "start_pos": 10, "end_pos": 43, "type": "DATASET", "confidence": 0.8932927449544271}]}, {"text": " Table 4: BOLT Arabic\u2192English results. Caching  orders: n = 8 (LM), n = 10 (JM).", "labels": [], "entities": [{"text": "BOLT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.895912766456604}]}, {"text": " Table 5: The in-domain baseline has a translation  model trained on the TED portion of the data only,  while RNN denotes decoding with the BTM, in  addition to 4 offline word-and phrase-based neu- ral scores in the phrase table . The phrase-based  models were trained on forced-aligned phrase- pairs rather than full sentences.", "labels": [], "entities": [{"text": "RNN", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9276151061058044}]}]}