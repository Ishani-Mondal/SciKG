{"title": [{"text": "Hyper-parameter Optimisation of Gaussian Process Reinforcement Learning for Statistical Dialogue Management", "labels": [], "entities": [{"text": "Statistical Dialogue Management", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.9204200903574625}]}], "abstractContent": [{"text": "Gaussian processes reinforcement learning provides an appealing framework for training the dialogue policy as it takes into account correlations of the objective function given different dialogue belief states, which can significantly speedup the learning.", "labels": [], "entities": []}, {"text": "These correlations are modelled by the kernel function which may depend on hyper-parameters.", "labels": [], "entities": []}, {"text": "So far, for real-world dialogue systems the hyper-parameters have been hand-tuned, relying on the designer to adjust the correlations, or simple non-parametrised kernel functions have been used instead.", "labels": [], "entities": []}, {"text": "Here, we examine different kernel structures and show that it is possible to optimise the hyper-parameters from data yielding improved performance of the resulting dialogue policy.", "labels": [], "entities": []}, {"text": "We confirm this in areal user trial.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken dialogue systems enable human-computer interaction via speech.", "labels": [], "entities": []}, {"text": "The dialogue management component has two aims: to maintain the dialogue state based on the current spoken language understanding input and the conversation history, and choose a response according to its dialogue policy.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8406395316123962}]}, {"text": "To provide robustness to the input errors, a number of statistical approaches are proposed to track a distribution overall dialogue states at every dialogue turn, called the belief state ().", "labels": [], "entities": []}, {"text": "The system response is then based on the belief state, rather than an inaccurate estimate of the most likely dialogue state.", "labels": [], "entities": []}, {"text": "The state-of-art statistical methods for policy learning are based on reinforcement learning (RL) (, which makes it possible to learn from interaction with the users.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.8772844076156616}]}, {"text": "However, most RL methods take too many dialogues for policy training.", "labels": [], "entities": [{"text": "RL", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9890103936195374}]}, {"text": "In Gaussian process reinforcement learning (GPRL) the kernel function defines prior correlations of the objective function given different belief states, which can significantly speeds up the policy optimisation).", "labels": [], "entities": [{"text": "Gaussian process reinforcement learning (GPRL)", "start_pos": 3, "end_pos": 49, "type": "TASK", "confidence": 0.779450535774231}]}, {"text": "Alternative methods include Kalman temporal difference (KTD) reinforcement learning).", "labels": [], "entities": []}, {"text": "Typically, statistical approaches to dialogue management rely on the belief state space compression into a form of a summary space, where the policy learning can be tractably performed.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8349437415599823}]}, {"text": "GPRL allows the learning to be performed directly on the full belief state.", "labels": [], "entities": [{"text": "GPRL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9616199731826782}]}, {"text": "However, only non-parametrised kernel functions have been considered for this purpose).", "labels": [], "entities": []}, {"text": "Here we address the important problem of how to define the structure of the kernel function fora real-world dialogue task and learn the hyperparameters from data fora policy that operates on the full belief state.", "labels": [], "entities": []}, {"text": "Using only a small-size dataset for hyper-parameter optimisation, we show that the policy with the optimised kernel function outperforms both the policy with hand specified kernel parameters and the one with a standard nonparametrised kernel function.", "labels": [], "entities": []}, {"text": "This is particularly beneficial for policy training with real users.", "labels": [], "entities": []}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "In section 2, we briefly review GP-Sarsa and the hyperparameter optimisation.", "labels": [], "entities": [{"text": "GP-Sarsa", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.8602622151374817}]}, {"text": "Section 3 introduces the kernel functions examined here.", "labels": [], "entities": []}, {"text": "The experimental results are shown in section 4, followed by conclusions and future work directions in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training data for hyper-parameter optimisation was generated using an agenda-based user simulator ().", "labels": [], "entities": [{"text": "hyper-parameter optimisation", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.8735977411270142}]}, {"text": "The dialogue policy training and evaluation is performed both on the user simulator and human users.", "labels": [], "entities": []}, {"text": "The system operates on the TopTable dialogue domain which consists of about 150 restaurants in Cambridge, UK.", "labels": [], "entities": [{"text": "TopTable dialogue domain", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.9236383040746053}]}, {"text": "Each restaurant has 9 slots, e.g. food, area, phone, address and soon.", "labels": [], "entities": [{"text": "soon", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9838010668754578}]}, {"text": "The state decomposes into 21 concepts.", "labels": [], "entities": []}, {"text": "Each concept takes from 4 to 150 values.", "labels": [], "entities": []}, {"text": "Each value is given a belief in by the BUDS state tracker . The summary action space consists of 20 summary actions.", "labels": [], "entities": [{"text": "BUDS state tracker", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.737951397895813}]}, {"text": "The hyper-parameter are optimised as follows.", "labels": [], "entities": []}, {"text": "1. Training data generation: We used a random policy to generate simulated dialogues out of which a small number of successful dialogues were used as training data.", "labels": [], "entities": [{"text": "Training data generation", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6903135776519775}]}, {"text": "Interval estimation: We estimated appropriate intervals for concept independent hyperparameters according to the properties of Gaussian kernel as described in section 3.1.", "labels": [], "entities": []}, {"text": "In our experiments, we only restricted the range of lengthscale l.", "labels": [], "entities": []}, {"text": "For the sum Gaussian kernel, the belief state for each concept is a probability distribution, so the lengthscale l is in interval (0, \u221a 2].", "labels": [], "entities": []}, {"text": "For product Gaussian kernel, the product of Gaussian kernels is still a Gaussian kernel, therefore the lengthscale l should be less than the maximum distance between two whole belief states (5.29 in the TopTable domain).", "labels": [], "entities": [{"text": "TopTable domain", "start_pos": 203, "end_pos": 218, "type": "DATASET", "confidence": 0.9274851679801941}]}, {"text": "We used 73 dialogues for concept independent and 147 dialogues for concept dependent hyper-parameter optimisation, with respectively 505 and 1004 dialogue turns.", "labels": [], "entities": []}, {"text": "We found that the smaller data set was not sufficient to capture correlations for the concept dependent kernels.: Summary of kernels 3.", "labels": [], "entities": []}, {"text": "Concept independent hyper-parameter optimisation: We sampled initial concept independent hyper-parameters from the estimated intervals and then minimised the negative log likelihood to find the concept independent optimised hyper-parameters.", "labels": [], "entities": []}, {"text": "We repeated this N times, and the hyperparameters with the overall smallest negative log likelihood was chosen as the final concept independent hyper-parameters.", "labels": [], "entities": []}, {"text": "4. Concept dependent hyper-parameter optimisation: We initialised them as concept independent hyper-parameters, then minimised the negative log likelihood to get the concept dependent optimised hyper-parameters.", "labels": [], "entities": []}, {"text": "After the hyper-parameters are obtained, we trained and evaluated the policies with these optimised kernels.", "labels": [], "entities": []}, {"text": "For comparison, the policies with hand-tuned Gaussian kernel hyper-parameters and linear kernel were also trained and evaluated.", "labels": [], "entities": []}, {"text": "In order to further evaluate the effect of the optimised kernels, policies were trained using crowdsourcing via the Amazon Mechanical Turk service in a set-up similar to.", "labels": [], "entities": []}, {"text": "At the end of each dialogue, a recurrent neural network (RNN) model was used to predict the dialogue success used as the reinforcement feedback (.", "labels": [], "entities": []}, {"text": "The GLSD kernel and the GLPD kernel were selected for on-line policy training and compared to the LS kernel.", "labels": [], "entities": [{"text": "GLSD kernel", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8162148296833038}, {"text": "GLPD kernel", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9195476472377777}]}, {"text": "shows the learning curve of the reward during training, demonstrating the advantage of Gaussian kernels over the simple linear kernel.", "labels": [], "entities": []}, {"text": "To confirm this result, all optimised policies were evaluated after 500 training dialogues.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "It can be seen that the policies with optimised kernels, especially the GLPD kernel, perform much better than the policy with linear kernel.", "labels": [], "entities": [{"text": "GLPD kernel", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9162547290325165}]}, {"text": "10.52 \u00b1 0.47 82.3 \u00b1 1.9: Evaluation of policies with three kernels.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. It can be seen that  the policies with optimised kernels, especially the  GLPD kernel, perform much better than the policy  with linear kernel.", "labels": [], "entities": [{"text": "GLPD kernel", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.8995252251625061}]}, {"text": " Table 2: Evaluation of policies with three kernels.", "labels": [], "entities": []}]}