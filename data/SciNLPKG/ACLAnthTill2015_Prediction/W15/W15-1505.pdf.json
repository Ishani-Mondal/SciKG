{"title": [{"text": "Distributional Representations of Words for Short Text Classification", "labels": [], "entities": [{"text": "Distributional Representations of Words for Short Text Classification", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.8504489213228226}]}], "abstractContent": [{"text": "Traditional supervised learning approaches to common NLP tasks depend heavily on manual annotation, which is labor intensive and time consuming, and often suffer from data s-parseness.", "labels": [], "entities": []}, {"text": "In this paper we show how to mitigate the problems in short text classification (STC) through word embeddings-distribu-tional representations of words learned from large unlabeled data.", "labels": [], "entities": [{"text": "short text classification (STC)", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.8146913548310598}]}, {"text": "The word embeddings are trained from the entire English Wikipedia text.", "labels": [], "entities": [{"text": "English Wikipedia text", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.8607293764750162}]}, {"text": "We assume that a short text document is a specific sample of one distribution in a Bayesian framework.", "labels": [], "entities": []}, {"text": "A Gaussian process approach is used to model the distribution of words.", "labels": [], "entities": []}, {"text": "The task of classification becomes a simple problem of selecting the most probable Gaussian distribution.", "labels": [], "entities": [{"text": "classification", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.9770414233207703}]}, {"text": "This approach is compared with those based on the classical maximum entropy (MaxEnt) model and the Latent Dirichlet Allocation (LDA) approach.", "labels": [], "entities": []}, {"text": "Our approach achieved better performance and also showed advantages in dealing with unseen words.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the boom of e-commerce and social media, short texts, such as instant messages, microblogs and product reviews, become more available in diverse forms than before.", "labels": [], "entities": []}, {"text": "These short forms of documents have become convenient presentations of information.", "labels": [], "entities": []}, {"text": "It is becoming more and more important to understand those short text documents and to efficiently detect what users are interested in.", "labels": [], "entities": []}, {"text": "Unlike long documents such as news articles and blogs, it is hard to measure similarities among these short texts since they do not share much in common (.", "labels": [], "entities": []}, {"text": "This poses a great challenge to short text classification (STC).", "labels": [], "entities": [{"text": "short text classification (STC)", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.7858736962080002}]}, {"text": "The task of short text classification can be described as follows: given a short text S, the aim is to identify its target theme T.", "labels": [], "entities": [{"text": "short text classification", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.6654557387034098}]}, {"text": "Several supervised learning approaches have been proposed for short text classification.", "labels": [], "entities": [{"text": "short text classification", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.7971200545628866}]}, {"text": "They have been shown to be effective and yielded good performance.", "labels": [], "entities": []}, {"text": "These approaches are effective because they leverage a large body of linguistic knowledge and related corpora.", "labels": [], "entities": []}, {"text": "However, the supervised learning approaches depend heavily on manual annotation, which is labor intensive and time consuming, and often suffer from data sparseness.", "labels": [], "entities": []}, {"text": "To tackle the above problems, we exploit word embeddings.", "labels": [], "entities": []}, {"text": "A word embedding W:words\u2192R n is a distributed representation fora word which is usually learned from a large corpus.", "labels": [], "entities": []}, {"text": "Many researches have found that the learned word vectors capture linguistic regularities and collapse similar words into groups (.", "labels": [], "entities": []}, {"text": "In this paper, we apply an information theoretic approach which assumes that the short text is generated from a predefined parametric model, and estimate its optimal parameters from training data.", "labels": [], "entities": []}, {"text": "We use Gaussian models to describe the distribution of words embeddings since it can describe any continuous distribution in common practice.", "labels": [], "entities": []}, {"text": "Then, we classify new short texts using the Bayesian rule to get the posterior probability.", "labels": [], "entities": [{"text": "posterior probability", "start_pos": 69, "end_pos": 90, "type": "METRIC", "confidence": 0.953246682882309}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Some related work is presented in Section 2.", "labels": [], "entities": []}, {"text": "The word embedding based approach to short text classification is presented in Section 3.", "labels": [], "entities": [{"text": "short text classification", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7170533537864685}]}, {"text": "The dataset and evaluation metrics are described in Section 4.", "labels": [], "entities": []}, {"text": "Experimental results on short text classification are given in Section 5.", "labels": [], "entities": [{"text": "short text classification", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.6750599642594656}]}, {"text": "Some conclusions are drawn in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted three sets of experiments.", "labels": [], "entities": []}, {"text": "In the first set of experiments, we compare the performance of our approach with the previous studies.", "labels": [], "entities": []}, {"text": "The second is to test the capability of our approach in dealing with the unseen words using different size of training data.", "labels": [], "entities": []}, {"text": "The third is to investigate the effect of the word representation dimension on STC.", "labels": [], "entities": [{"text": "word representation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7272226214408875}, {"text": "STC", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.8972986340522766}]}], "tableCaptions": [{"text": " Table 1: Statistics of the Web Snippets data", "labels": [], "entities": []}, {"text": " Table 2: The number of unseen words", "labels": [], "entities": []}, {"text": " Table 3: Short Text Classification Performance", "labels": [], "entities": [{"text": "Short Text Classification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7546135783195496}]}]}