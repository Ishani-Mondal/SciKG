{"title": [], "abstractContent": [{"text": "We present a method to store additional information in a minimal automaton so that it is possible to compute a corresponding tree node number fora state.", "labels": [], "entities": []}, {"text": "The number can then be used to retrieve additional information.", "labels": [], "entities": []}, {"text": "The method works for minimal (and any other) deterministic acyclic finite state automata (DFAs).", "labels": [], "entities": []}, {"text": "We also show how to compute the inverse mapping.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deterministic finite state automata and transducers are widely used in natural language processing and computational linguistics.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6519675751527151}]}, {"text": "The most frequent uses include dictionaries (, acoustic and language models (the latter indirectly, using perfect hashing to number words), as well as hidden Markov models used for tagging and chunking.", "labels": [], "entities": []}, {"text": "Acyclic minimal automata recognize the same languages as automata inform of trees, but they take considerably less space.", "labels": [], "entities": []}, {"text": "Therefore, minimization is an obligatory step inmost applications.", "labels": [], "entities": [{"text": "minimization", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.98592209815979}]}, {"text": "In LZ-style compression of automata, it is a trie (a letter-tree) that is compressed without going through minimization.", "labels": [], "entities": []}, {"text": "However, the compression finds identical sequences of transitions, so it also finds isomorphic trees, which is what minimization does.", "labels": [], "entities": []}, {"text": "In speech recognition, trees are often used (Ortmanns et al., 1997) instead of minimal automata, because lookahead probabilities are computed for individual nodes of the trees.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7488750517368317}]}, {"text": "At a given moment in time represented by anode (a state) in a treelike automaton, one wants to know the probability of the most probable word that is recognized by going through that state.", "labels": [], "entities": []}, {"text": "Minimal perfect hashing can deliver a range of numbers for all words going through that state.", "labels": [], "entities": []}, {"text": "It is then possible to use them to access probabilities of each word, and find the maximal one.", "labels": [], "entities": []}, {"text": "This can be timeconsuming, especially close to the root of the tree.", "labels": [], "entities": []}, {"text": "What is needed for lookahead probabilities is a storage for the probability of the most probable word recognized by going through that state, i.e. one probability for each state.", "labels": [], "entities": []}, {"text": "This is trivial to implement in a tree -one simply stores the probability in the state.", "labels": [], "entities": []}, {"text": "Ina minimal DFA, a state can represent many sets of words as it maybe equivalent to several nodes in a tree.", "labels": [], "entities": []}, {"text": "Therefore in a state of a minimal DFA, one must refer somehow to the corresponding node in the tree, or at least to a place associated with that node.", "labels": [], "entities": []}, {"text": "The solution is to provide a dynamically computed mapping from prefixes of words to node numbers in the tree.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}