{"title": [{"text": "Annotating Entailment Relations for Shortanswer Questions", "labels": [], "entities": [{"text": "Annotating Entailment Relations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8992176055908203}]}], "abstractContent": [{"text": "This paper presents an annotation project that explores the relationship between tex-tual entailment and short answer scoring (SAS).", "labels": [], "entities": [{"text": "tex-tual entailment", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.6951255798339844}, {"text": "short answer scoring (SAS)", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.7253496944904327}]}, {"text": "We annotate entailment relations between learner and target answers in the Corpus of Reading Comprehension Exercises for German (CREG) with a fine-grained label inventory and compare them in various ways to correctness scores assigned by teachers.", "labels": [], "entities": []}, {"text": "Our main finding is that although both tasks are clearly related, not all of our entailment tags can be directly mapped to SAS scores and that especially the area of partial entailment covers instances that are problematic for automatic scoring and need further investigation .", "labels": [], "entities": [{"text": "automatic scoring", "start_pos": 227, "end_pos": 244, "type": "TASK", "confidence": 0.6778587102890015}]}], "introductionContent": [{"text": "Reading comprehension exercises area standard task in foreign language education: Students read a text in the language they are learning and answer questions about it.", "labels": [], "entities": []}, {"text": "With the advent of computerbased language learning courses, the automatic scoring of such shortanswer questions has become an important research topic (for an overview see; ), not only for reading and listening comprehension in the context of foreign language learning, but also e.g. in science questions for native speaker students.", "labels": [], "entities": []}, {"text": "It has been often noted that the SAS task is related to the task of recognizing textual entailment (RTE, e.g.,,).", "labels": [], "entities": [{"text": "SAS task", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.8314916789531708}, {"text": "recognizing textual entailment (RTE", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.6266468703746796}]}, {"text": "RTE is the task to decide whether there is an inference relation between two texts; in the case of SAS, these texts are the learner answer (LA), given by a student, and a teacher-specified target answer (TA, i.e. a sample solution).", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7995747327804565}, {"text": "SAS", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9377971887588501}, {"text": "learner answer (LA)", "start_pos": 124, "end_pos": 143, "type": "METRIC", "confidence": 0.7618882060050964}, {"text": "TA", "start_pos": 204, "end_pos": 206, "type": "METRIC", "confidence": 0.8852855563163757}]}, {"text": "An entailment relation between two texts A and B is given if people reading A and B would infer that whenever A is true, B is most likely true as.", "labels": [], "entities": []}, {"text": "Consider the following example: (1) Q: Why did Julchen come to the kitchen?", "labels": [], "entities": [{"text": "Q", "start_pos": 36, "end_pos": 37, "type": "METRIC", "confidence": 0.9779821634292603}, {"text": "Julchen", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9628390073776245}]}, {"text": "TA: She came to the kitchen because of the noise her parents made.", "labels": [], "entities": [{"text": "TA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.897519052028656}]}, {"text": "LA: She came to the kitchen because Mr. and Mrs. Muschler became out of breath from laughing.", "labels": [], "entities": [{"text": "Mr. and Mrs. Muschler", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.706423819065094}]}, {"text": "In this example, the LA textually (but not logically) entails the TA.", "labels": [], "entities": [{"text": "LA", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9824672341346741}, {"text": "TA", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.736680805683136}]}, {"text": "Ina strictly logical sense of entailment, laughing until you are out of breath does not entail making noise.", "labels": [], "entities": []}, {"text": "However, it seems plausible to many people that laughing in that way makes a lot of noise.", "labels": [], "entities": []}, {"text": "Such a learner answer that is more specific than the target answer -and thus entails the target answer -is likely to be scored as correct by a teacher.", "labels": [], "entities": []}, {"text": "In some aspects, SAS for reading comprehension in a language learning scenario differs from a standard textual entailment scenario: Whereas in standard RTE, two texts are compared, in the SAS scenario the additional context of the question has to be accounted for in terms of information structure and resolution of anaphora and ellipses.", "labels": [], "entities": [{"text": "SAS", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8065603375434875}]}, {"text": "Additionally, when processing learner language one often has to deal with ungrammatical sentences and orthographical variance that are challenging for many NLP tools, up to the extent that it is sometimes difficult to understand what the learner wanted to express with an answer (the so-called target hypothesis).", "labels": [], "entities": []}, {"text": "In this study, we want to explicitly assess the relation between RTE labels and correctness scores assigned by teachers.", "labels": [], "entities": [{"text": "RTE labels", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.838049441576004}]}, {"text": "We assume that they are related, but we expect that the relation is not a direct mapping.", "labels": [], "entities": []}, {"text": "We expect, for example, that, if a LA entails a TA and vice versa at the same time, i.e. if they are paraphrases, then the LA will probably be scored as correct by a teacher.", "labels": [], "entities": [{"text": "TA", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.8388816118240356}]}, {"text": "On the other hand, the fact that there is only some partial conceptual overlap between a LA and a TA does not constitute entailment, but is in some instances enough for an answer to be scored as correct by a teacher.", "labels": [], "entities": []}, {"text": "We present in this paper the first part of an annotation project that aims at investigating the relationship between SAS and RTE and that compares existing binary correctness scores annotated by teachers to RTE annotations that have been conducted without the correctness or quality of the learner answer in mind.", "labels": [], "entities": [{"text": "SAS", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.753319501876831}]}, {"text": "(In future work, we will also look at the relation between reading texts and learner answers.)", "labels": [], "entities": []}, {"text": "Understanding these relations better will potentially help us to leverage techniques from RTE for the task of SAS in a more efficient way and to shed light on the the way teachers score shortanswer questions.", "labels": [], "entities": [{"text": "SAS", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9713397026062012}]}, {"text": "This paper makes the following contributions: \u2022 We provide a fine-grained annotation of the Corpus of Reading Comprehension Exercises in German (CREG) (Ott et al., 2012) with 7 textual entailment labels that specify the entailment relations between learner answers and target answers.", "labels": [], "entities": [{"text": "Reading Comprehension Exercises in German (CREG) (Ott et al., 2012)", "start_pos": 102, "end_pos": 169, "type": "TASK", "confidence": 0.5800215224424998}]}, {"text": "\u2022 We provide an evaluation of our annotations that compares how our label distribution corresponds to the distribution of binary teacher scores in CREG.", "labels": [], "entities": [{"text": "CREG", "start_pos": 147, "end_pos": 151, "type": "DATASET", "confidence": 0.8650659918785095}]}, {"text": "\u2022 State of the art binary scoring approaches label only about 86% of the corpus correctly (.", "labels": [], "entities": []}, {"text": "In order to understand the challenges of automatic scoring better, we evaluate which instances in terms of our entailment annotation labels are most problematic for automatic scoring with a binary label.", "labels": [], "entities": [{"text": "automatic scoring", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7195142805576324}]}, {"text": "\u2022 We will further explore the relation between textual entailment and SAS by comparing, how well features from shortanswer scoring tasks can be used to learn our classification.", "labels": [], "entities": [{"text": "SAS", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8222700357437134}]}], "datasetContent": [{"text": "This section presents an analysis of our RTE annotations and comparisons to SAS scores.", "labels": [], "entities": [{"text": "RTE", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6701822876930237}]}, {"text": "We explore the relation between RTE and SAS through a series of machine learning evaluations.", "labels": [], "entities": [{"text": "RTE", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9556183218955994}, {"text": "SAS", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.6560435891151428}]}, {"text": "In the first part, we evaluate a SAS classifier asking which LAs in terms of entailment type are most difficult for automatic labeling.", "labels": [], "entities": [{"text": "SAS classifier", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.8364723920822144}]}, {"text": "We then present a modeling experiment that explores the impact of using our entailment labels as features fora SAS system and finally a series of experiments that aim at testing how well entailment information is modeled by alignment-based machine learning features.", "labels": [], "entities": []}, {"text": "For all experiments, we used the Logistic classifier in the Weka package, that is based on a logistic regression algorithm ().", "labels": [], "entities": [{"text": "Weka package", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.8477239012718201}]}, {"text": "We use alignment-based features in a re-implementation of that reaches an accuracy of 86% on CREG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9991732239723206}, {"text": "CREG", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.8889688849449158}]}, {"text": "All experiments were evaluated via leave-one-out cross validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Confusion matrix between the two annotators for our labels. Abbreviations: paraphrase, en- tailment, reverse entailment, partial entailment, contradiction, topical non-entailment, off-topic.", "labels": [], "entities": []}, {"text": " Table 3: Confusion matrix for teacher assessments  and entailment labels.", "labels": [], "entities": []}, {"text": " Table 5: Overview of the classifier performances  Abbreviations: teacher scores as class with align- ment features and alignment+entailment fea- tures. 7-way entailment type as class and col- lapsed entailment class sets by combining entail- ment types into 5, 3 or 2 classes, all with alignment  features", "labels": [], "entities": []}, {"text": " Table 4: Confusion matrix for the machine learner on our labels with precision and recall for all classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9995208978652954}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9995335340499878}]}]}