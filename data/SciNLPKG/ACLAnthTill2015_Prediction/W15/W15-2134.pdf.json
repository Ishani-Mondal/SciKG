{"title": [{"text": "Does Universal Dependencies need a parsing representation? An investigation of English", "labels": [], "entities": [{"text": "Universal Dependencies", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.6551826894283295}]}], "abstractContent": [{"text": "This paper investigates the potential of defining a parsing representation for En-glish data in Universal Dependencies, a crosslingual dependency scheme.", "labels": [], "entities": []}, {"text": "We investigate structural transformations that change the choices of headedness in the dependency tree.", "labels": [], "entities": []}, {"text": "The transformations make auxiliaries, copulas, subordinating conjunctions and prepositions heads, while in UD they are dependents of a lexical head.", "labels": [], "entities": []}, {"text": "We show experimental results for the performance of MaltParser, a data-driven transition-based parser, on the product of each transformation.", "labels": [], "entities": []}, {"text": "While some transformed representations favor performance, inverting the transformations to obtain UD for the final product propagates errors, in part due to the nature of lexical-head representations.", "labels": [], "entities": []}, {"text": "This prevents the transformations from being profitably used to improve parser performance in that representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is a considerable amount of research suggesting that the choice of syntactic representation can have an impact on parsing performance, in constituency () as well as dependency (; parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 120, "end_pos": 127, "type": "TASK", "confidence": 0.9762210845947266}]}, {"text": "Recently, this has led designers of dependency representations ) to suggest the use of an alternative parsing representation to support the performance of statistical learners.", "labels": [], "entities": []}, {"text": "While it is clear that, at the limit, trivializing a linguistic representation in order to make it easier to parse is undesirable -for example, by making each word depend on the previous one -there certainly exists a variety of choice points in which more than one type of design is defensible.", "labels": [], "entities": []}, {"text": "In the dependency tradition, semantic and syntactic criteria have been recognized to motivate headedness, and there are well-known examples of conflicts between those criteria ().", "labels": [], "entities": []}, {"text": "Here we investigate four syntactic constructions that are loci of such conflicts: verb groups, prepositional phrases, copular clauses and subordinate clauses.", "labels": [], "entities": []}, {"text": "The baseline representation is Universal Dependencies (), a multilingual dependency scheme that strongly prefers lexical heads.", "labels": [], "entities": []}, {"text": "For each target construction, structural transformations are defined that demote the lexical head and make it dependent on a functional head.", "labels": [], "entities": []}, {"text": "We show experimental results for the performance of MaltParser, a data-driven transitionbased parser, on the product of each transformation.", "labels": [], "entities": []}, {"text": "While some transformed representations are in fact easier to learn, error propagation when inverting the transformations to obtain UD prevents them from being profitably used to improve parser performance in that representation. is a systematic study of how representation choices in dependency annotation schemes affect their learnability for parsing.", "labels": [], "entities": []}, {"text": "The choice points investigated, much like in the current paper, relate to the issue of headedness.", "labels": [], "entities": []}, {"text": "Each combination of these binary choices is tested with 5 different parsers, which represent different paradigms in dependency parsing.The edges in the representation are unlabeled, unlike the common practice in NLP.", "labels": [], "entities": [{"text": "dependency parsing.The edges", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.8270612955093384}]}, {"text": "The results show a learnability bias towards a conjunct in (1), a noun in (3), and a preposition in (5) in all the parsers.", "labels": [], "entities": []}, {"text": "Furthermore, a bias towards the modal heads in (6) and towards the head-initial representation in (4) is seen with some parsers.", "labels": [], "entities": []}, {"text": "No significant results are found for (2).", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments in this paper fit the following template: aversion of the training and development data from the EWT corpus was used to optimize a MaltParser model.", "labels": [], "entities": [{"text": "EWT corpus", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.9745708405971527}]}, {"text": "Then that model was used to parse the test set (of 25k tokens) and evaluated on the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9234755337238312}]}, {"text": "In the 12 experiments where the training data had undergone a transformation, the output of the parser was converted back into the original UD representation with the inverse transformation, so that it could be compared to the actual gold standard.", "labels": [], "entities": []}, {"text": "An important concern with this type of experiment is that the default feature sets for the algorithms maybe implicitly biased towards a particular type of representation.", "labels": [], "entities": []}, {"text": "Therefore, it was crucial to explore different hyperparameters and feature sets.", "labels": [], "entities": []}, {"text": "This was done in two steps.", "labels": [], "entities": []}, {"text": "The MaltParser model was obtained via an optimization heuristic: MaltOptimizer (Ballesteros and Nivre, 2012) was used on the different versions of the training set to obtain models optimized for the different transformation.", "labels": [], "entities": []}, {"text": "This generates 13 models: one for the baseline, and one for each of the three versions of the four transformations.", "labels": [], "entities": []}, {"text": "Ina second step, all 13 representations of the dev set data were parsed with all the 13 models that MaltOptimizer produced in the previous step.", "labels": [], "entities": []}, {"text": "Note that MaltOptimizer did not use the dev set.", "labels": [], "entities": []}, {"text": "The model that performed best on the dev set for each transformation was chosen.", "labels": [], "entities": []}, {"text": "Interestingly, it came out that the best-performing model fora representation was never the one recommended by MaltOptimizer for that representation.", "labels": [], "entities": [{"text": "MaltOptimizer", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.9378724694252014}]}, {"text": "For all the representations, the models that effectively performed best on the dev set consistently used the stackproj algorithm, coupled with different pseudo-projectivization strategies.", "labels": [], "entities": []}, {"text": "Throughout this procedure, the metric being maximized was the labeled attachment score (excluding punctuation), which seems to be the crucial measure of performance for most client applications.", "labels": [], "entities": [{"text": "labeled attachment score", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.7013498644034067}]}, {"text": "Each Xhead transformation targets a different construction, and the frequency of those in the data varies.", "labels": [], "entities": []}, {"text": "Additionally, the three versions of the transformations change the data to different extents.", "labels": [], "entities": []}, {"text": "To give a measure of these differences, Table 1 shows the percentage of tokens in the training data that are changed by a transformation, for all 12 transformations.", "labels": [], "entities": []}, {"text": "These counts make it clear that, in the case of casehead and markhead, there is little difference between the partial and simple transformations.", "labels": [], "entities": []}, {"text": "This is because in the case of these transforma-: Percentage of non-projective dependencies perversion of the data.", "labels": [], "entities": []}, {"text": "tions, the lexical head is unlikely (in English) to have dependents which occur to the left of the functional head.", "labels": [], "entities": []}, {"text": "The transformations are also very different in terms of how much non-projectivity they introduce.", "labels": [], "entities": []}, {"text": "shows how that proportion changes with each transformation, which helps understand their performance.", "labels": [], "entities": []}, {"text": "The labeled attachment scores of the bestperforming models for each representation on the test set are given in.", "labels": [], "entities": []}, {"text": "These results were obtained by comparing the output of parsers trained on transformed representation to a transformed version of the gold-standard test set.", "labels": [], "entities": []}, {"text": "These scores will be referred to as the within-representation performance.", "labels": [], "entities": []}, {"text": "Statistical significance was assessed using Dan Bikel's parsing evaluation comparator 2 , at the 0.05 significance level.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.6418302059173584}]}, {"text": "Our interest here is not to guide the design of   anew representation, but rather to find strategies that will improve parser performance for the existing UD representation.", "labels": [], "entities": []}, {"text": "For this reason, we also present results on the actual UD representation.", "labels": [], "entities": []}, {"text": "These results are obtained by transforming the output of a parser with the inverse of the transformation applied to the training data, and comparing that to the actual gold standard annotation.", "labels": [], "entities": []}, {"text": "The labeled accuracy scores are in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.971376359462738}]}], "tableCaptions": [{"text": " Table 1: Percentage of tokens changed with rela- tion to the gold standard by each transformation.", "labels": [], "entities": []}, {"text": " Table 2: Percentage of non-projective dependen- cies per version of the data.", "labels": [], "entities": []}, {"text": " Table 3. These results were ob- tained by comparing the output of parsers trained  on transformed representation to a transformed  version of the gold-standard test set. These scores  will be referred to as the within-representation  performance. Statistical significance was assessed  using Dan Bikel's parsing evaluation comparator 2 ,  at the 0.05 significance level.", "labels": [], "entities": []}, {"text": " Table 3: Labeled accuracy scores for within- representation evaluations. The scores marked  with * have a significant difference from the base- line.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9924589991569519}]}, {"text": " Table 4: Labeled accuracy scores for evaluations  on UD. The scores marked with * have a signifi- cant difference from the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9950855374336243}]}]}