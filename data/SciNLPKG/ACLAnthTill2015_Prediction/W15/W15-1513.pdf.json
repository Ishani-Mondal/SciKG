{"title": [{"text": "Combining Distributed Vector Representations for Words", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent interest in distributed vector representations for words has resulted in an increased diversity of approaches, each with strengths and weaknesses.", "labels": [], "entities": []}, {"text": "We demonstrate how diverse vector representations maybe inexpensively composed into hybrid representations, effectively leveraging strengths of individual components, as evidenced by substantial improvements on a standard word analogy task.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 222, "end_pos": 239, "type": "TASK", "confidence": 0.7991118828455607}]}, {"text": "We further compare these results over different sizes of training sets and find these advantages are more pronounced when training data is limited.", "labels": [], "entities": []}, {"text": "Finally, we explore the relative impacts of the differences in the learning methods themselves and the size of the contexts they access.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed vector representations allow words to be represented in a continuous space.", "labels": [], "entities": []}, {"text": "By learning these representations using unsupervised methods overlarge corpora, these models capture key distributional aspects of word function and meaning.", "labels": [], "entities": []}, {"text": "In particular, such representations provide a valuable response to issues of data sparsity by providing simple similarity measures between terms.", "labels": [], "entities": []}, {"text": "Whether used indirectly in terms of those similarity measures (e.g. for smoothing in language models) or directly as features to a model for tasks such as parsing (), these representations have proved increasingly valuable to a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Given these benefits, a number of approaches have been explored for generating these representations beginning with early work in connectionist modeling ( and expanding into applications in text analysis.", "labels": [], "entities": [{"text": "connectionist modeling", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.9623867273330688}, {"text": "text analysis", "start_pos": 190, "end_pos": 203, "type": "TASK", "confidence": 0.8227227926254272}]}, {"text": "Recently, in part spurred by the resurgence of neural network methods, vector representations have enjoyed renewed attention, expanding beyond their previous scope with the appearance of a variety of new techniques for their generation and applications for their use.", "labels": [], "entities": []}, {"text": "The various approaches have been shown to have a variety of strengths and weaknesses and it is precisely in the context of this proliferation that our work is focused.", "labels": [], "entities": []}, {"text": "Presented with diversity of techniques, other areas of machine learning have found excellent results with the use of ensemble methods), combining multiple techniques to capture the strengths of each.", "labels": [], "entities": []}, {"text": "We examine whether similar gains are available here through the combination of multiple existing techniques for generating semantic vector representations.", "labels": [], "entities": []}, {"text": "Recent work has shown that relationships in these models (such as gender differences or pluralization) are often linear ().", "labels": [], "entities": []}, {"text": "Drawing on this, we explore composition through linear combinations of these representational spaces.", "labels": [], "entities": []}, {"text": "In particular, we explore combinations of a popular neural network method (Word2Vec) () with Distributed Vector Representations in Sigma (DVRS) (), a method based on prior work in holographic representation.", "labels": [], "entities": [{"text": "holographic representation", "start_pos": 180, "end_pos": 206, "type": "TASK", "confidence": 0.7154431641101837}]}, {"text": "We demonstrate that various methods of composing these vectors can produce hybrid representations which perform significantly better than either method in isolation.", "labels": [], "entities": []}, {"text": "This leap in performance is particularly pronounced when working with smaller datasets, opening up intriguing possibilities for domains which lack large corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following upon the initial experiment, we began to explore how much of the differences came from the structures of the algorithms versus the data available to the models.", "labels": [], "entities": []}, {"text": "We varied the size of the context available to both Word2Vec and DVRS and the window size used by Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9679871797561646}, {"text": "Word2Vec", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9680967926979065}]}, {"text": "In particular, we varied the parameters available to the best performing combinations from the previous experiment.", "labels": [], "entities": []}, {"text": "Given the results of our prior experiment, we focused on concatenation rather than additive combinations.", "labels": [], "entities": []}, {"text": "We trained vectors against two versions of the enwik9 dump, one divided into paragraphs, the other into sentences.", "labels": [], "entities": [{"text": "enwik9 dump", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.8345221877098083}]}, {"text": "For Word2Vec, we varied the window size from 10 to 50, testing against both paragraph and sentence versions.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9480897188186646}]}, {"text": "Given the expanded window size, we chose to include the continuous bag of words (CBOW) model for Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.944876492023468}]}, {"text": "This is an alternative to the skip gram model which does not take into account word ordering.", "labels": [], "entities": []}, {"text": "Given this, it seemed like this model might do better given the larger window size.", "labels": [], "entities": []}, {"text": "DVRS structurally makes use of the full context available, so running it against paragraphs and sentences provided an analogous shift in effective window size.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on word analogy problems with vectors trained against the first 10 8 bytes of Wikipedia.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7698849439620972}]}, {"text": " Table 2: Performance on word analogy problems with vectors trained against the first 10 9 bytes of Wikipedia.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.767562597990036}]}, {"text": " Table 3: Variations on window size and data structure with vectors trained against the first 10 9 bytes of Wikipedia.", "labels": [], "entities": []}]}