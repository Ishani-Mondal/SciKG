{"title": [{"text": "Passive and Pervasive Use of a Bilingual Dictionary in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.7775563597679138}]}], "abstractContent": [{"text": "There are two primary approaches to the use bilingual dictionary in statistical machine translation: (i) the passive approach of appending the parallel training data with a bilingual dictionary and (ii) the pervasive approach of enforcing translation as per the dictionary entries when decoding.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6584974328676859}]}, {"text": "Previous studies have shown that both approaches provide external lexical knowledge to statistical machine translation thus improving translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.675187756617864}]}, {"text": "We empirically investigate the effects of both approaches on the same dataset and provide further insights on how lexical information can be reinforced in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 155, "end_pos": 186, "type": "TASK", "confidence": 0.7041715582211813}]}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) obtains the best translation, e best , by maximizing the conditional probability of the foreign sentence given the source sentence, p(f|e), and the a priori probability of the translation, p LM (e).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7395263671875}]}, {"text": "State-of-art SMT systems rely on (i) large bilingual corpora to train the translation model p(f|e) and (ii) monolingual corpora to build the language model, p LM (e).", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9916378855705261}]}, {"text": "One approach to improve the translation model is to extend the parallel data with a bilingual dictionary prior to training the model.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9674075841903687}]}, {"text": "The primary motivation to use additional lexical information for domain adaptation to overcome the out-ofvocabulary words during decoding.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7095683962106705}]}, {"text": "Alternatively, adding in-domain lexicon to parallel data has also shown to improve SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9951139688491821}]}, {"text": "The intuition is that by adding extra counts of bilingual lexical entries, the word alignment accuracy improves, resulting in a better translation model (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.7094235867261887}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9298511743545532}]}, {"text": "Another approach to use a bilingual dictionary is to hijack the decoding process and force word/phrase translations as per the dictionary entries.", "labels": [], "entities": [{"text": "word/phrase translations", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7091108411550522}]}, {"text": "Previous researches used this approach to explore various improvements in industrial and academic translation experiments.", "labels": [], "entities": []}, {"text": "For instance, Tezcan and Vandeghinste (2011) injected a bilingual dictionary in the SMT decoding process and integrated it with Computer Assisted Translation (CAT) environment to translate documents in the technical domain.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.9363366067409515}, {"text": "Computer Assisted Translation (CAT)", "start_pos": 128, "end_pos": 163, "type": "TASK", "confidence": 0.8071555495262146}]}, {"text": "They showed that using a dictionary in decoding improves machine translation output and reduces post-editing time of human translators.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7386184632778168}]}, {"text": "Carpuat (2009) experimented with translating sentences in discourse context by using a discourse specific dictionary annotations to resolve lexical ambiguities and showed that this can potentially improve translation quality.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the improvements made by both approaches to use a bilingual dictionary in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9908766150474548}]}, {"text": "We refer to the first approach of extending the parallel data with dictionary as the passive use and the latter approach of hijacking the decoding process as the pervasive use of dictionary in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 193, "end_pos": 224, "type": "TASK", "confidence": 0.684331476688385}]}, {"text": "Different from the normal use of a dictionary for the purpose of domain adaptation where normally, a domain-specific lexicon is appended to a translation model trained on generic texts, we are investigating the use of an in-domain dictionary in statistical machine translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.753768652677536}, {"text": "statistical machine translation", "start_pos": 245, "end_pos": 276, "type": "TASK", "confidence": 0.6592260003089905}]}, {"text": "More specifically, we seek to understand how much improvement can be made by skewing the lexical information towards the passive and pervasive use of the dictionary in statistical machine 30 translation.", "labels": [], "entities": [{"text": "statistical machine 30 translation", "start_pos": 168, "end_pos": 202, "type": "TASK", "confidence": 0.6197512149810791}]}], "datasetContent": [{"text": "We experimented the passive and pervasive uses of dictionary in SMT using the Japanese-English dataset provided in the Workshop for Asian Translation ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9751978516578674}, {"text": "Japanese-English dataset", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.6755709797143936}, {"text": "Asian Translation", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.6699087917804718}]}, {"text": "We used the Asian Scientific Paper Excerpt Corpus (ASPEC) as the training corpus used in the experiments.", "labels": [], "entities": [{"text": "Asian Scientific Paper Excerpt Corpus (ASPEC)", "start_pos": 12, "end_pos": 57, "type": "DATASET", "confidence": 0.8364022970199585}]}, {"text": "The AS-PEC corpus consists of 3 million parallel sentences extracted from Japanese-English scientific abstracts from Japan's Largest Electronic Journal Platform for Academic Societies (J-STAGE).", "labels": [], "entities": [{"text": "AS-PEC corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.6736235022544861}]}, {"text": "In our experiments we follow the setup of the WAT shared task with 1800 development and test sentences each from the ASPEC corpus.", "labels": [], "entities": [{"text": "WAT shared task", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.8786880175272623}, {"text": "ASPEC corpus", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.8345838785171509}]}, {"text": "We use the Japanese-English (JA-EN) translation dictionaries) from the Japan Science and Technology Corporation.", "labels": [], "entities": [{"text": "Japan Science and Technology Corporation", "start_pos": 71, "end_pos": 111, "type": "DATASET", "confidence": 0.9406587362289429}]}, {"text": "It contains 800,000 entries 2 for technical terms extracted from scientific and technological documents.", "labels": [], "entities": []}, {"text": "Both the parallel data and the bilingual dictionary are tokenized with the MeCab segmenter ().", "labels": [], "entities": [{"text": "MeCab segmenter", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.8605518639087677}]}, {"text": "2.1M 1.7M presents the number of tokens in the AS-PEC corpus and the JICST dictionary.", "labels": [], "entities": [{"text": "AS-PEC corpus", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7369371205568314}, {"text": "JICST dictionary", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.9181996583938599}]}, {"text": "On average 3-4 dictionary entries are found for each sentence Code to automatically convert sentences into XML-input with pervasive dictionary translations for the Moses toolkit is available at http://tinyurl.com/pervasive-py.", "labels": [], "entities": []}, {"text": "2.1M JA and 1.7M EN tokens 31 in the WAT development set.", "labels": [], "entities": [{"text": "WAT development set", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.7796463767687479}]}, {"text": "For all experiments we used the phrase-based SMT implemented in the Moses toolkit () with the following experimental settings: \u2022 MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction ( \u2022 Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations ( \u2022 Language modeling is trained using KenLM with maximum phrase length of 5 with Kneser-Ney smoothing \u2022 Minimum Error Rate Training (MERT) to tune the decoding parameters.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.6732639074325562}, {"text": "IBM word alignment", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.5264827956755956}, {"text": "word alignment", "start_pos": 225, "end_pos": 239, "type": "TASK", "confidence": 0.7773497700691223}, {"text": "Minimum Error Rate Training (MERT", "start_pos": 476, "end_pos": 509, "type": "METRIC", "confidence": 0.8915572166442871}]}, {"text": "For the pervasive use of the dictionary, we used the xml-input function in the Moses toolkit to force lexical knowledge in the decoding process . presents the BLEU scores of the Japanese to English (JA-EN) translation outputs from the phrase-based SMT system on the WAT test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9991793036460876}, {"text": "SMT", "start_pos": 248, "end_pos": 251, "type": "TASK", "confidence": 0.8780653476715088}, {"text": "WAT test set", "start_pos": 266, "end_pos": 278, "type": "DATASET", "confidence": 0.8128041128317515}]}, {"text": "The leftmost columns indicate the number of times a dictionary is appended to the parallel training data (Baseline = 0 times, Passive x1 = 1 time).", "labels": [], "entities": []}, {"text": "The rightmost columns present the results from both the passive and pervasive use of dictionary translations, with exception to the top-right cell which shows the baseline result of the pervasive dictionary usage without appending any dictionary.", "labels": [], "entities": []}, {"text": "By repeatedly appending the dictionary to the parallel data, the BLEU scores significantly 4 improves from 16.75 to 17.31.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9986931681632996}]}, {"text": "Although the system's performance degrades when adding the dictionary passively thrice, the score remains significantly better than baseline.", "labels": [], "entities": []}, {"text": "The pervasive use of the dictionary improves the baseline without the passive of the dictionary.", "labels": [], "entities": []}, {"text": "The best performance is achieved when the dictionary is passively added four times with the pervasive use of the dictionary during decoding.", "labels": [], "entities": []}, {"text": "The fluctuations in improvement from coupling the passive and pervasive use of an in-domain dictionary give no indication of how both approaches should be used in tandem.", "labels": [], "entities": []}, {"text": "However, using either or both the approaches improves the translation quality of the baseline system.", "labels": [], "entities": [{"text": "translation", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.9501726627349854}]}, {"text": "presents the BLEU scores of the English to Japanese (EN-JA) translation outputs from the phrase-based SMT system on the WAT test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9993081092834473}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.8930925726890564}, {"text": "WAT test set", "start_pos": 120, "end_pos": 132, "type": "DATASET", "confidence": 0.8545140027999878}]}, {"text": "Similarly, the passive use of dictionary outperforms the baseline but the pervasive use of dictionary consistently reported worse BLEU scores significantly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9995738863945007}]}, {"text": "Different from the JA-EN translation the pervasive use of dictionary consistently performs worse  than the baseline.", "labels": [], "entities": [{"text": "JA-EN translation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.5781373977661133}]}, {"text": "Upon random manual checking of the MT output, there are many instances where the technical/scientific term in the dictionary is translated correctly with only the passive use of the dictionary.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9549918174743652}]}, {"text": "However, it unclear whether the overall quality of the translations have degraded from the pervasive use of the dictionary given the slight, though significant, decrease in BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.9991992115974426}]}], "tableCaptions": [{"text": " Table 2: BLEU Scores for Passive and Pervasive  Use of the Dictionary in SMT (Japanese to En- glish)", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9752012193202972}]}, {"text": " Table 3: BLEU Scores for Passive and Pervasive  Use of Dictionary in SMT (English to Japanese)", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9758589863777161}, {"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8324472904205322}]}]}