{"title": [{"text": "Dependency-based Reordering Model for Constituent Pairs in Hierarchical SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.5348406434059143}]}], "abstractContent": [{"text": "We propose a novel dependency-based reordering model for hierarchical SMT that predicts the translation order of two types of pairs of constituents of the source tree: head-dependent and dependent-dependent.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.802036702632904}]}, {"text": "Our model uses the dependency structure of the source sentence to capture the medium-and long-distance reorder-ings between these pairs of constituents.", "labels": [], "entities": []}, {"text": "We describe our reordering model in detail and then apply it to a language pair in which the languages involved follow different word order patterns, English (SVO) and Farsi (free word order being SOV the most frequent pattern).", "labels": [], "entities": []}, {"text": "Our model out-performs a baseline (standard hierarchical SMT) by 0.78 BLEU points absolute, statistically significant at p = 0.01.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8949281573295593}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9987996816635132}]}], "introductionContent": [{"text": "Reordering is a fundamental problem in machine translation (MT) that significantly affects translation quality, especially between languages with major differences in word order.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8572960019111633}]}, {"text": "While a great deal of work has been carried out to address this problem, none of the existing approaches can perform all the required types of reordering operations in a principled manner.", "labels": [], "entities": []}, {"text": "In general, there are four main approaches to address the reordering problem in statistical machine translation (SMT): distortion models, lexical phrase-based models, hierarchical phrase-based models and syntax-based models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.7864071975151697}]}, {"text": "Despite the relative success of each of these approaches in improving the overall performance of the SMT systems, they suffer from a number of shortcomings: \u2022 Inability to capturing long-distance reordering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9929702877998352}]}, {"text": "Distortion and lexical phrase-based models assign probability only to the adjacent word or phrase pairs, so they can only perform local reordering between adjacent units and fail to capture long distance reordering.", "labels": [], "entities": []}, {"text": "This weakness has motivated research on tree-based models, such as the hierarchical phrase-based model (HPB).", "labels": [], "entities": []}, {"text": "Although HPB models outperform phrase-based models (PB-SMT) on medium-range reordering, they still perform weakly on handling long distance reordering due to complexity constraints.", "labels": [], "entities": []}, {"text": "Most of the approaches can perform the reordering of common words or phrases, but they usually cannot be generalized to unseen patterns which have the same linguistic structure.", "labels": [], "entities": []}, {"text": "For example, if the object follows the verb in the source language and preceeds the verb in the target language, we still need to see a particular instance of a verb and an object in the training data to be able to perform reordering between them.", "labels": [], "entities": []}, {"text": "Lexical and hierarchical phrase-based models determine the ordering of the phrases based solely on the lexical items in those phrases.", "labels": [], "entities": []}, {"text": "However, a phrase might have different orderings in different contexts, so it is essential to include more context in order to capture the reordering behaviour.", "labels": [], "entities": []}, {"text": "Compared to the other reordering models, syntax-based models have access to the necessary structural information to perform long-distance reordering.", "labels": [], "entities": []}, {"text": "However, due to the complexity of the decoding algorithm, they have very low performance on large-scale translations.", "labels": [], "entities": []}, {"text": "In order to overcome some of these deficiencies, we propose a dependency-based reordering model for HPB-SMT.", "labels": [], "entities": [{"text": "HPB-SMT", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.9588053226470947}]}, {"text": "Our model uses the dependency structure of the source sentence to capture the medium-and long-distance reorderings between the dependent parts of the sentence.", "labels": [], "entities": []}, {"text": "Unlike the syntax-based models that impose harsh syntactic limits on rule extraction and require serious efforts to be optimised (, we use syntactic information only in the reordering model and augment the HPB model with soft dependency constraints.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7634769380092621}]}, {"text": "We report experimental results on a large-scale Engish-to-Farsi translation task.", "labels": [], "entities": [{"text": "Engish-to-Farsi translation task", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.7934107383092245}]}, {"text": "The rest of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the related work and contextualises our work.", "labels": [], "entities": []}, {"text": "Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi.", "labels": [], "entities": []}, {"text": "Section 4 presents our reordering model, which is then evaluated in Section 5.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes the paper and outlines avenues of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the Mizan parallel English-Farsi corpus 1 (Supreme Council of Information and Communication Technology, 2013) which contains nearly 1 million sentence pairs.", "labels": [], "entities": [{"text": "Mizan parallel English-Farsi corpus 1 (Supreme Council of Information and Communication Technology, 2013)", "start_pos": 12, "end_pos": 117, "type": "DATASET", "confidence": 0.8630807287991047}]}, {"text": "This corpus is extracted from English novel books (mostly in their classical literature domain) and their translations in Farsi.", "labels": [], "entities": []}, {"text": "3,000 sentence pairs were held out for development and 1,000 for testing.", "labels": [], "entities": []}, {"text": "These sentence pairs were randomly selected from the corpus.", "labels": [], "entities": []}, {"text": "The remaining content of the corpus is used for training.", "labels": [], "entities": []}, {"text": "presents the details about this dataset.", "labels": [], "entities": []}, {"text": "We parsed the source side (English) of the corpus using the Stanford dependency parser (Chen) and used the \"collapsed representation\" of the parser output to obtain direct dependencies between the words in the source sentences.", "labels": [], "entities": []}, {"text": "We used GIZA++ to align the words in the corpus.", "labels": [], "entities": []}, {"text": "Then we extracted 6,391,255 head\u2212dependent pairs and 5,247,137 dependent\u2212dependent pairs from train dataset and determined the orientation for each pair based on Equation 1.", "labels": [], "entities": [{"text": "Equation", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9762528538703918}]}, {"text": "In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classifier with standard settings from the Weka machine learning toolkit ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9992194175720215}, {"text": "Weka machine learning toolkit", "start_pos": 199, "end_pos": 228, "type": "DATASET", "confidence": 0.8981563001871109}]}, {"text": "We trained the classifier separately for head\u2212dependent and dependent\u2212dependent pairs.", "labels": [], "entities": []}, {"text": "Our baseline MT system was the Moses implementation of HPM model with default settings (.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9692193269729614}]}, {"text": "We used a 5-gram target language model trained on the Farsi side of the training data.", "labels": [], "entities": []}, {"text": "In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT .", "labels": [], "entities": [{"text": "MERT", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9550037980079651}]}], "tableCaptions": [{"text": " Table 4: Mizan parallel corpus statistics", "labels": [], "entities": [{"text": "Mizan parallel corpus", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6000967025756836}]}, {"text": " Table 7: Scores of the MT systems according to different automatic metrics. The best score according  to each metric is shown in bold. Statistically significant results, calculated with paired bootstrap resam- pling", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9841772317886353}]}]}