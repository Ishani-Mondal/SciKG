{"title": [{"text": "Target-side Generation of Prepositions for SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9896787405014038}]}], "abstractContent": [{"text": "We present a translation system that models the selection of prepositions in a target-side generation component.", "labels": [], "entities": []}, {"text": "This novel approach allows the modeling of all subcategorized elements of a verb as either NPs or PPs according to target-side requirements relying on source and target side features.", "labels": [], "entities": []}, {"text": "The BLEU scores are encouraging, but fail to surpass the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9990517497062683}]}, {"text": "We additionally evaluate the preposition accuracy fora carefully selected subset and discuss how typical problems of translating prepositions can be modeled with our method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9639418721199036}]}], "introductionContent": [{"text": "The translation of prepositions is a difficult task for machine translation; a preposition must convey the source-side meaning while also meeting targetside constraints.", "labels": [], "entities": [{"text": "translation of prepositions", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8809395035107931}, {"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.749591737985611}]}, {"text": "This requires information that is not always directly accessible in an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9892648458480835}]}, {"text": "Prepositions are typically determined by governors, such as verbs (to believe in sth.) or nouns (interest in sth.).", "labels": [], "entities": []}, {"text": "Functional prepositions tend to convey little meaning and mostly depend on targetside restrictions, whereas content-bearing prepositions are largely determined by the source-side, but may also be subject to target-side requirements, as in the following example: go to the cinema/to the beach \u2192 ins Kino/an den Strand gehen.", "labels": [], "entities": []}, {"text": "In this paper, we treat prepositions as a targetside generation problem and move the selection of prepositions out of the translation system into a post-processing component.", "labels": [], "entities": [{"text": "targetside generation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7678541839122772}]}, {"text": "During translation, we use an abstract representation of prepositions as a place-holder that serves as a basis for the generation of prepositions in the post-processing step.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.964996874332428}]}, {"text": "In this step, all subcategorized elements of a verb are considered and allotted to their respective functions -as PPs with an overt preposition, but also as NPs with an \"empty\" preposition, e.g. to call for sth.", "labels": [], "entities": []}, {"text": "\u2192 \u2205 etw. erfordern.", "labels": [], "entities": []}, {"text": "Ina standard SMT system, subcategorization is difficult to capture in the language model or by the translation rules if the verb and its subcategorized elements are not adjacent.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.983391284942627}]}, {"text": "In the following, we outline a method to handle prepositions with a target-side generation model in an English-German morphology-aware SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.8223335146903992}]}, {"text": "We study two aspects: (i) features fora meaningful abstract representation of prepositions and (ii) how to predict prepositions in the translation output using a combination of source and targetside information.", "labels": [], "entities": []}, {"text": "In addition, we compare prepositions in the machine translation output with those in the reference translation fora selected subset.", "labels": [], "entities": []}, {"text": "Finally, we discuss examples illustrating typical problems of translating prepositions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The success of generating-prepositions in SMT depends to a large extent on the quality of the prediction component.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9938390851020813}]}, {"text": "Before beginning with the MT experiments, we thus evaluate the quality of predicting prepositions on clean data, the tuning-set.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9789113998413086}]}, {"text": "We use the Wapiti toolkit (see section 6.1) to train a CRF to predict prepositions.", "labels": [], "entities": [{"text": "Wapiti toolkit", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.8355105519294739}]}, {"text": "We opted fora sequence model to take into account decisions from previous positions.", "labels": [], "entities": []}, {"text": "Even though it only looks at previous decisions on bigram-level, the annotation of case on all elements of noun phrases should prevent that two adjacent noun phrases be assigned the same value for case.", "labels": [], "entities": []}, {"text": "shows the performance of predicting prepositions on clean data.", "labels": [], "entities": [{"text": "predicting prepositions", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8815108835697174}]}, {"text": "In the column \"prep+case\", we evaluate the accuracy of the prediction of both the preposition and its grammatical case, whereas the column \"prep\" gives the accuracy when only looking at the predicted preposition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9993921518325806}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9987598657608032}]}, {"text": "We compare a model using source-side and projected source-side features (1) and a model with additional subcategorization information (2).", "labels": [], "entities": []}, {"text": "Source-side information and its target-side pro-    jection are crucial -without source-information, content-conveying prepositions would need to be guessed -the addition of subcategorization information does not lead to further gains, though.", "labels": [], "entities": []}, {"text": "lists the prediction results for some of the prepositions to be modeled, ranging from 95% to 22%.", "labels": [], "entities": []}, {"text": "The realization as empty preposition constitutes by far the majority.", "labels": [], "entities": []}, {"text": "In the list of the top-3 predicted prepositions, it becomes obvious that the realization as \u2205 instead of an overt preposition is also the most frequent error; similarly, the prepositions von/in (of/in), all high-frequency prepositions, are often output instead of the correct preposition.", "labels": [], "entities": []}, {"text": "Here, we present the setup and results of our experiments.", "labels": [], "entities": []}, {"text": "In addition to the traditional metric BLEU, we assess the quality of the translated prepositions fora subset where relevant elements (verb, noun) match with the reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9959128499031067}]}, {"text": "Finally, we discuss some examples before concluding the paper.", "labels": [], "entities": []}, {"text": "We trained a standard phrase-based Moses system on 4.3M lines of EN-DE data (WMT'14) with a 10.3M sentence language model.", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.4731077253818512}]}, {"text": "For the lemmatized representation of the morphologyaware SMT system, the German part was parsed with BitPar) and analyzed with the morphological tool SMOR ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.7258997559547424}]}, {"text": "The models for predicting inflectional features and prepositions were built with the Wapiti toolkit (.", "labels": [], "entities": [{"text": "predicting inflectional features and prepositions", "start_pos": 15, "end_pos": 64, "type": "TASK", "confidence": 0.8721357822418213}, {"text": "Wapiti toolkit", "start_pos": 85, "end_pos": 99, "type": "DATASET", "confidence": 0.8961257040500641}]}, {"text": "The inflectional models (case, number, gender strong/weak) were trained on lemma and tag information of the German part of the parallel data.", "labels": [], "entities": []}, {"text": "The models to predict prepositions were trained on half of the parallel data due to the considerably larger amount of labels that can be predicted.", "labels": [], "entities": []}, {"text": "The subcategorization tuples were extracted from German web data (,) and Europarl.", "labels": [], "entities": [{"text": "German web data", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.9667132099469503}, {"text": "Europarl", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.9718986749649048}]}, {"text": "We used WMT'13 as tuning and WMT'14 as test sets 3 . shows the results of experiments with the baseline system (a), a morphology-aware SMT system with no special treatment for prepositions . As a variant of the baseline system (b), we removed all prepositions from the translation output to be re-predicted.", "labels": [], "entities": [{"text": "WMT'13", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.9535259008407593}, {"text": "WMT'14", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9510504007339478}, {"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9021745324134827}]}, {"text": "This does not lead to much change in BLEU, illustrating that the prediction step itself is not harmful.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9962002635002136}]}, {"text": "However, only changing existing prepositions is not sufficient and it is not possible to model empty vs. overt prepositions.", "labels": [], "entities": []}, {"text": "shows results for the variants of the place-holder systems.", "labels": [], "entities": []}, {"text": "Using a basic place-holder (\u2737) representation (S1) leads to a considerably drop in relation to the baseline in table 4.", "labels": [], "entities": []}, {"text": "Annotating the place-holder with case (S2) leads to an improvement of ca.", "labels": [], "entities": []}, {"text": "0.4, indicating that the abstract representation of the place-holders plays a significant role here.", "labels": [], "entities": []}, {"text": "In (S3), we mark whether the preposition is governed by a verb or a noun, to no avail.", "labels": [], "entities": []}, {"text": "As an extension, we annotate the status of the placeholder: subcategorized or non-subcategorized in (S4), which seems to slightly help, even though the observed differences are very small.", "labels": [], "entities": []}, {"text": "Assuming that functional prepositions contribute only little in terms of meaning, only subcategorized prepositions are represented by place-holders, whereas non-functional prepositions are kept.", "labels": [], "entities": []}, {"text": "Again, we show two variants: in (S5a), all prepositions are re-predicted, while in (S5b), the forms of nonfunctional prepositions in the MT output are kept and only those for functional prepositions are predicted -this last result reaches the baseline level.", "labels": [], "entities": [{"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.8565359115600586}]}, {"text": "While none of the variants outperforms the baseline, we consider the results encouraging as they illustrate (i) that the representation of prepositions during the translation step considerably influences the MT quality (S2) and (ii) that applying the prediction step to a carefully selected subset of prepo-   sitions improves the results (S5a vs. S5b).", "labels": [], "entities": [{"text": "MT", "start_pos": 208, "end_pos": 210, "type": "TASK", "confidence": 0.9084656238555908}]}, {"text": "BLEU is known to not capture subtle differences between two translation systems very well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9558766484260559}]}, {"text": "Thus, we present a second evaluation in which we analyze the translation accuracy of prepositions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.8711415529251099}]}, {"text": "It is difficult to automatically assess the quality of the translation of prepositions as the choice of a preposition depends on its context, mainly the verbs and/or nouns it occurs with.", "labels": [], "entities": []}, {"text": "It is not sufficient to compare the prepositions occurring in the reference translation with those in the translation output, as the used verbs/nouns or even the entire structure of the sentence might differ.", "labels": [], "entities": []}, {"text": "We will thus restrict the evaluation to cases where the relevant parts, namely the governing verb and the noun governed by the preposition are the same in the reference sentence and in the translation output 5 : in such cases, an automatic comparison of the preposition in the MT output with the preposition in the reference sentence is possible.", "labels": [], "entities": []}, {"text": "To obtain the set for which to evaluate the prepositions, we took each preposition in the reference sentence 6 governing a proper noun or named entity.", "labels": [], "entities": []}, {"text": "The governing verb is identified relying on dependency parses of the reference translation.", "labels": [], "entities": []}, {"text": "For extracting the equivalents of the relevant parts (preposition, noun, verb) in the translation output, we made use of the alignments with the English source sentence as pivot.", "labels": [], "entities": []}, {"text": "The matching is made on lemma-level.", "labels": [], "entities": []}, {"text": "gives an overview of the amount of cases where the reference contains a preposition and its noun and governing verb are the same in the MT output; in the set of 3003 sentences, this is the case fora subset of 270 (baseline), 260 (S2, the best place-holder-only system) and 271 (S5).", "labels": [], "entities": [{"text": "MT", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.9018351435661316}]}, {"text": "Note that the slightly less prep-noun-verb triples of S2 that match the reference compared to the baseline are not per-se a sign for inferior translation quality as we did not consider the possibility of synonymous translations.", "labels": [], "entities": []}, {"text": "shows the amount of prepositions for the respective subsets that were considered correct, i.e. match with the reference.", "labels": [], "entities": []}, {"text": "While the difference is very small, the percentage of correct prepositions is slightly higher for the systems S2/5a.", "labels": [], "entities": []}, {"text": "Systems 5a/b are based on the same MT output; however, 5a fares better in this evaluation even though 5b had a higher BLEU score.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.7869033217430115}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9992695450782776}]}, {"text": "We thus assume that BLEU did not improve based on the examined subset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9992062449455261}]}, {"text": "This analysis also shows that the translation quality of prepositions is a problem in need of more attention . It has to be noted, though, that this evaluation only gives partial insights into the performance of the systems.", "labels": [], "entities": []}, {"text": "The main problem is that the evaluation is centered around prepositions in the reference translation, which often is (structurally) different from the source sentence and consequently also the translation output.", "labels": [], "entities": []}, {"text": "Thus, sentences with prepositions in the translation, but not in the reference, are not considered.", "labels": [], "entities": []}, {"text": "Nevertheless, we regard this evaluation as suitable to evaluate the correctness of prepositions in an automatic way.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on clean data (3000 sentences).", "labels": [], "entities": []}, {"text": " Table 4: Baseline variants (3003 sentences).", "labels": [], "entities": []}, {"text": " Table 5: Results for place-holder systems.", "labels": [], "entities": []}, {"text": " Table 6: Subsets where governing verb/governed  noun are the same in MT output and reference.", "labels": [], "entities": [{"text": "MT output", "start_pos": 70, "end_pos": 79, "type": "TASK", "confidence": 0.8362973630428314}]}, {"text": " Table 7: Percentage of correct prepositions for the  subsets from table 6.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.979472279548645}]}]}