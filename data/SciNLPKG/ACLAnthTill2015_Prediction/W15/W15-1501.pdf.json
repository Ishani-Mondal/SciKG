{"title": [{"text": "A Simple Word Embedding Model for Lexical Substitution", "labels": [], "entities": [{"text": "Lexical Substitution", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.8065253794193268}]}], "abstractContent": [{"text": "The lexical substitution task requires identifying meaning-preserving substitutes fora target word instance in a given sentential context.", "labels": [], "entities": [{"text": "lexical substitution task", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7743044396241506}]}, {"text": "Since its introduction in SemEval-2007, various models addressed this challenge, mostly in an unsupervised setting.", "labels": [], "entities": []}, {"text": "In this work we propose a simple model for lexical substitution , which is based on the popular skip-gram word embedding model.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7653330862522125}]}, {"text": "The novelty of our approach is in leveraging explicitly the context embeddings generated within the skip-gram model, which were so far considered only as an internal component of the learning process.", "labels": [], "entities": []}, {"text": "Our model is efficient, very simple to implement , and at the same time achieves state-of-the-art results on lexical substitution tasks in an unsupervised setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical substitution tasks have become very popular for evaluating context-sensitive lexical inference models since the introduction of the original task in and additional later variants).", "labels": [], "entities": [{"text": "Lexical substitution tasks", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8754638036092123}]}, {"text": "In these tasks, systems are required to predict substitutes fora target word instance, which preserve its meaning in a given sentential context.", "labels": [], "entities": []}, {"text": "Recent models addressed this challenge mostly in an unsupervised setting.", "labels": [], "entities": []}, {"text": "They typically generated a word instance representation, which is biased towards its given context, and then identified substitute words based on their similarity to this biased representation.", "labels": [], "entities": []}, {"text": "Various types of models were proposed, from sparse syntax-based vector models), to probabilistic graphical models and LDA topic models.", "labels": [], "entities": []}, {"text": "Word embeddings are low-dimensional vector representations of word types that recently gained much traction in various semantic tasks.", "labels": [], "entities": []}, {"text": "Probably the most popular word embedding model today is skip-gram, introduced in and available as part of the word2vec toolkit.", "labels": [], "entities": []}, {"text": "1 word2vec learns for every word type two distinct representations, one as a target and another as a context, both embedded in the same space.", "labels": [], "entities": []}, {"text": "However, the context representations are considered internal to the model and are discarded after training.", "labels": [], "entities": []}, {"text": "The output word embeddings represent context-insensitive target word types.", "labels": [], "entities": []}, {"text": "Few recent models extended word embeddings by learning a distinct representation for each sense of a target word type, as induced by clustering the word's contexts).", "labels": [], "entities": []}, {"text": "They then identify the relevant sense(s) fora given word instance, in order to measure contextsensitive similarities.", "labels": [], "entities": []}, {"text": "Although these models maybe considered for lexical substitution, they have so far been applied only to 'softer' word similarity tasks which include topical relations.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.766424834728241}]}, {"text": "In this work we propose a simple approach for directly utilizing the skip-gram model for contextsensitive lexical substitution.", "labels": [], "entities": []}, {"text": "Instead of discarding the learned context embeddings, we use them in conjunction with the target word embeddings to model target word instances.", "labels": [], "entities": []}, {"text": "A suitable substitute fora 1 target word instance is then identified via its combined similarity to the embeddings of both the target and its given context.", "labels": [], "entities": []}, {"text": "Our model is efficient, can be implemented literally in a few lines of code, and at the same time achieves state-of-the-art results on two lexical substitution datasets in an unsupervised setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the original lexical substitution task all of the participating sys-   tems predicted substitutes by first using manuallyconstructed thesauri to generate substitute candidates and then developing candidate ranking models to choose the most appropriate ones.", "labels": [], "entities": []}, {"text": "Later works focused mostly on the candidate ranking part, where candidates are provided as part of the datasets.", "labels": [], "entities": []}, {"text": "In this section we present the evaluation of our model both on the substitute candidates ranking task, and on the original substitutes prediction task (no candidates provided), using two different lexical substitution datasets.", "labels": [], "entities": [{"text": "substitutes prediction task", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.759251226981481}]}, {"text": "The dataset introduced in the lexical substitution task of, denoted here LS-SE, is the most widely used for the evaluation of lexical substitution.", "labels": [], "entities": []}, {"text": "It consists of 10 sentences extracted from a web corpus for each of 201 target words (nouns, verbs, adjectives and adverbs), or altogether 2,010 word instances in sentential context, split into 300 trial sentences and 1,710 test sentences.", "labels": [], "entities": []}, {"text": "The gold standard provided with this dataset is a weighted lemmatized substitute list for each word instance, based on manual annotations.", "labels": [], "entities": []}, {"text": "A more recent large-scale 'all-words' dataset, called 'Concepts in Context', was introduced in and denoted here LS-CIC.", "labels": [], "entities": []}, {"text": "This dataset provides the same kind of data as LS-SE, but instead of choosing specific target words that tend to be ambiguous as done in LS-SE, the target words here are all the content words in text documents extracted from news and fiction corpora, and are therefore more naturally distributed.", "labels": [], "entities": []}, {"text": "LS-CIC is also much larger than LS-SE with over 15K target word instances.", "labels": [], "entities": [{"text": "LS-CIC", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8386536240577698}]}], "tableCaptions": [{"text": " Table 3: GAP scores for compared methods on the can- didate ranking task. Resources used by these meth- ods: ukWaC, Wikipedia, Gigaword (Parker et al., 2011),  WN = WordNet", "labels": [], "entities": [{"text": "GAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8632619380950928}, {"text": "ukWaC", "start_pos": 110, "end_pos": 115, "type": "DATASET", "confidence": 0.9818181395530701}, {"text": "Wikipedia", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.9088590145111084}, {"text": "WordNet", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9052954316139221}]}, {"text": " Table 4: best and oot subtasks scores for all compared  methods on the substitute prediction task.", "labels": [], "entities": [{"text": "substitute prediction task", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.8162105282147726}]}]}