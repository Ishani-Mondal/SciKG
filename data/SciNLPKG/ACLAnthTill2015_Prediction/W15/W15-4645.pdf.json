{"title": [{"text": "A Statistical Approach for Non-Sentential Utterance Resolution for Interactive QA System", "labels": [], "entities": [{"text": "Non-Sentential Utterance Resolution", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.762132316827774}]}], "abstractContent": [{"text": "Non-Sentential Utterances (NSUs) are short utterances that do not have the form of a full sentence but nevertheless convey a complete sentential meaning in the context of a conversation.", "labels": [], "entities": [{"text": "Non-Sentential Utterances (NSUs) are short utterances", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7454705089330673}]}, {"text": "NSUs are frequently used to ask followup questions during interactions with question answer (QA) systems resulting into incorrect answers being presented to their users.", "labels": [], "entities": []}, {"text": "Most of the current methods for resolving such NSUs have adopted rule or grammar based approach and have limited applicability.", "labels": [], "entities": []}, {"text": "In this paper, we present a data driven statistical method for resolving such NSUs.", "labels": [], "entities": []}, {"text": "Our method is based on the observation that humans identify keyword appearing in an NSU and place them in the context of conversation to construct a meaningful sentence.", "labels": [], "entities": []}, {"text": "We adapt the keyword to question (K2Q) framework to generate natural language questions using keywords appearing in an NSU and its context.", "labels": [], "entities": []}, {"text": "The resulting questions are ranked using different scoring methods in a statistical framework.", "labels": [], "entities": []}, {"text": "Our evaluation on a data-set collected using mTurk shows that the proposed method perform significantly better than the previous work that has largely been rule based.", "labels": [], "entities": [{"text": "mTurk", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.9476962089538574}]}], "introductionContent": [{"text": "Recently Question Answering (QA) systems have been built with high accuracies.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.8769392251968384}]}, {"text": "The obvious next step for them is to assist people by improving their experience in seeking day today information needs like product support and troubleshooting.", "labels": [], "entities": []}, {"text": "For QA systems to be effective * D.", "labels": [], "entities": [{"text": "D", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.9956501126289368}]}, {"text": "Raghu and S. Indurthi contributed equally to this work and usable they need to evolve into conversational systems.", "labels": [], "entities": []}, {"text": "One extra challenge that conversational systems throw is that users tend to form successive queries that allude to the entities and concepts made in the past utterances.", "labels": [], "entities": []}, {"text": "Therefore, among other things, such systems need to be equipped with the ability to understand what are called NonSentential Utterances (NSUs).", "labels": [], "entities": []}, {"text": "NSUs are utterances that do not have the form of a full sentence, according to the most traditional grammars, but nevertheless convey a complete sentential meaning.", "labels": [], "entities": []}, {"text": "Consider for example, the conversation between a sales staff of a mobile store (S) and one of their customers (C), where C:2 and C:3 are examples of NSUs.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the datasets, evaluation approaches and results.", "labels": [], "entities": []}, {"text": "We also present the comparative analysis of the performance obtained when we employ a rule-based baseline approach (Section 3) for this task.", "labels": [], "entities": []}, {"text": "In this section, we describe the data that we use for evaluating the performance of the proposed method for NSU resolution.", "labels": [], "entities": [{"text": "NSU resolution", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.9243691861629486}]}, {"text": "We used a subset of the data that was collected using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 54, "end_pos": 76, "type": "DATASET", "confidence": 0.9645602504412333}]}, {"text": "For collecting this data a question answer pair (Q,A) was presented to an mTurk worker and who was then asked to conceive another question Q 2 related to the pair (Q, A).", "labels": [], "entities": []}, {"text": "The Q 2 was to be given in two different versions, an elliptical version Q 2e and a fully resolved version Q 2r . The original data contains 7400 such entries and contains examples for NSUs as well as anaphora in Q 2 . We selected a subset of 500 entries from this dataset for our evaluation.", "labels": [], "entities": []}, {"text": "We present our evaluations based on the following three different configurations to investigate the importance of various scoring and ranking modules.", "labels": [], "entities": []}, {"text": "The configurations used are, 1.", "labels": [], "entities": []}, {"text": "Rule Based: This configuration is used as a baseline system, as described in section 3.", "labels": [], "entities": []}, {"text": "As rule based methodologies are dominant in the field of NSU resolutions, we compare to clearly illustrate the limitations of just using rules.", "labels": [], "entities": [{"text": "NSU resolutions", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.9052433371543884}]}, {"text": "2. Semantic Similarity: We investigate how well the semantic similarity score as described in Section 4.3 works when we sort the candidate questions generated based on this feature alone.", "labels": [], "entities": [{"text": "Semantic Similarity", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.6719180941581726}]}, {"text": "Given the input conversation {U tt1, Ans1, U tt2}, system generated resolved utterance Q (corresponding to NSU U tt2) and the intended utterance Q r , the goal of the evaluation metric is to judge how similar Q is to Q r . We use BLEU score and human judgments for the purpose of this evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 230, "end_pos": 240, "type": "METRIC", "confidence": 0.9758878350257874}]}, {"text": "BLEU score is often used for evaluation of machine translation systems to judge the goodness of the translated text with the reference text.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9762176871299744}, {"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.706993818283081}]}, {"text": "Please note that we also used the BLEU score as one of the features as mentioned in Section 4.3.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9544825553894043}]}, {"text": "There, it was computed between the generated question Q and the preceding utterance U tt1.", "labels": [], "entities": []}, {"text": "Whereas, for evaluation purposes, this score is computed between the generated question Q and the intended question provided by the ground truth Q r . To account for the paraphrasing errors, as the same utterance can be said in several different ways, we also use human judgment for the evaluation.", "labels": [], "entities": []}, {"text": "Our test set comprises only of those utterances ({U tt2}) which require a resolution and therefore Recall@N captures how many of these NSUs were correctly resolved if candidates only up to top N are to be considered.", "labels": [], "entities": []}, {"text": "We compute the BLEU score between the candidate resolution Q and the ground truth utterance Q rand compare it across the three configurations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9733562171459198}]}, {"text": "shows the comparison of the average BLEU score at position 1.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9661253690719604}]}, {"text": "A low score for the rule based approach is expected as it resolves only those cases in which rules fire.", "labels": [], "entities": []}, {"text": "The semantic similarity configuration gains over the rule based approach as it is able to utilize the template database generated using the WikiAnswers corpus.", "labels": [], "entities": [{"text": "WikiAnswers corpus", "start_pos": 140, "end_pos": 158, "type": "DATASET", "confidence": 0.8709392249584198}]}, {"text": "Finally, the SVM Rank uses various other scores (LM, BLEU score) on top of rule-based and semantic similarity score and therefore achieves higher BLEU Score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9844557344913483}, {"text": "BLEU Score", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.9830638468265533}]}, {"text": "Finally, to account for the paraphrasing artifacts manifested inhuman language, we use human judgments to make a true comparison between the rule based approach and the SVM Rank configuration.", "labels": [], "entities": []}, {"text": "For human judgments, we presented just the resolved Q and the ground truth Q r . For all the 200 data points in the test set, top 5 candidates were presented to human annotators who were asked to decide if it was a correct resolution or not.", "labels": [], "entities": []}, {"text": "We choose just the top 5 just to analyze the quality of the candidates generated at various positions by the system.", "labels": [], "entities": []}, {"text": "shows the Recall@1 for the the two configurations.", "labels": [], "entities": [{"text": "Recall@1", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9515854716300964}]}, {"text": "A better recall for the proposed SVM configuration signifies the better coverage of the proposed approach beyond a pre-defined set of rules.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9915174841880798}]}, {"text": "The Recall@1 was used for this comparison since the rule-based approach can only yield a single candidate.", "labels": [], "entities": [{"text": "Recall@1", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8595261971155802}]}, {"text": "To further seethe behavior of the proposed approach as more candidates are considered, Recall@N is presented in.", "labels": [], "entities": [{"text": "Recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.8822699189186096}]}, {"text": "The shows that a recall of 42.5% can be achieved when results up to top 5 are considered.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9988797307014465}]}, {"text": "The objective of this experiment is to study the quality of top (1-5) ranked generated questions.", "labels": [], "entities": []}, {"text": "This experiment helps us conclude that improving the ranking module has the potential to improve the overall performance of the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparing Recall@1 using Human Judgments", "labels": [], "entities": []}]}