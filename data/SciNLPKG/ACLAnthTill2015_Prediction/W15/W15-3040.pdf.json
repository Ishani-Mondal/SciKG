{"title": [{"text": "USHEF and USAAR-USHEF Participation in the WMT15 Quality Estimation Shared Task", "labels": [], "entities": [{"text": "USHEF", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9692226052284241}, {"text": "USAAR-USHEF", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.9697532653808594}, {"text": "WMT15 Quality Estimation Shared Task", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.6997403025627136}]}], "abstractContent": [{"text": "We present the results of the USHEF and USAAR-USHEF submissions for the WMT15 shared task on document-level quality estimation.", "labels": [], "entities": [{"text": "USHEF", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9639458060264587}, {"text": "USAAR-USHEF", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.7915021181106567}, {"text": "WMT15 shared task", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.614538848400116}, {"text": "document-level quality estimation", "start_pos": 93, "end_pos": 126, "type": "TASK", "confidence": 0.5723971426486969}]}, {"text": "The USHEF submissions explored several document and discourse-aware features.", "labels": [], "entities": [{"text": "USHEF", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8895329833030701}]}, {"text": "The USAAR-USHEF submissions used an exhaustive search approach to select the best features from the official baseline.", "labels": [], "entities": [{"text": "USAAR-USHEF submissions", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.9548572897911072}]}, {"text": "Results show slight improvements over the baseline with the use of discourse features.", "labels": [], "entities": []}, {"text": "More interestingly , we found that a model of comparable performance can be built with only three features selected by the exhaustive search procedure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating the quality of Machine Translation (MT) systems outputs is a challenging topic.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8553074955940246}]}, {"text": "Several metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU)) or error rates (such as TER ().", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.979826033115387}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9975576400756836}, {"text": "TER", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.989891767501831}]}, {"text": "However, in some scenarios, human references are not available.", "labels": [], "entities": []}, {"text": "For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7222182601690292}]}, {"text": "Another example is machine translation for gisting by users of online systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.823197066783905}]}, {"text": "Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7448097348213196}, {"text": "MT outputs", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.8822231292724609}]}, {"text": "Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to train supervised machine learning models (classifiers or regressors).", "labels": [], "entities": []}, {"text": "A number of data points need to be annotated for quality (by humans or automatically) for training, using a given quality metric.", "labels": [], "entities": []}, {"text": "Most QE research is done at sentence level.", "labels": [], "entities": [{"text": "QE", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9570604562759399}]}, {"text": "This task has been a track at WMT shared task for the last four years).", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.7716723879178365}]}, {"text": "In addition to sentence level, the current edition offers for the first time a track on paragraph-level QE.", "labels": [], "entities": []}, {"text": "Exploring quality beyond sentence level is interesting for completely automatic translation applications, i.e. without human review.", "labels": [], "entities": []}, {"text": "For instance, consider a user looking for information on a product that has several reviews automatically translated into his/her language.", "labels": [], "entities": []}, {"text": "This user have no knowledge about the source language.", "labels": [], "entities": []}, {"text": "To ensure that the main message of the review is preserved, for this user the quality of each word or sentence individually is not as important as the quality of the review as a whole.", "labels": [], "entities": []}, {"text": "Therefore, predicting the quality of the whole document (or paragraph, considering paragraph as short documents) becomes necessary.", "labels": [], "entities": [{"text": "predicting", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.9577004909515381}]}, {"text": "This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking.", "labels": [], "entities": [{"text": "WMT15 QE shared task", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.6595462709665298}]}, {"text": "We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN).", "labels": [], "entities": []}, {"text": "Little previous research has been done to address document-level QE.", "labels": [], "entities": [{"text": "document-level QE", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.5039998590946198}]}, {"text": "proposed document-aware features in order to rank machine translated documents.", "labels": [], "entities": []}, {"text": "Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.7214818000793457}]}, {"text": "Finally, and introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents.", "labels": [], "entities": []}, {"text": "Previous work led to some improvements over the baselines used.", "labels": [], "entities": []}, {"text": "However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by . Our approach focuses on extracting various features and building models with different combination of these features.", "labels": [], "entities": [{"text": "QE", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.6588884592056274}]}, {"text": "Two feature selection approaches are considered.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.700426772236824}]}, {"text": "The first one is based on Random Forests and backward feature selection.", "labels": [], "entities": []}, {"text": "The second performs an exhaustive search on the entire feature space.", "labels": [], "entities": []}, {"text": "Features are either based on previous work for sentence-level QE (e.g. number of tokens in the target document) or are discourseaware (e.g. lexical repetition counts).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our systems use only the data provided by the task organisers.", "labels": [], "entities": []}, {"text": "For features that require corpora or resources, only those provided by the organisers were used.", "labels": [], "entities": []}, {"text": "Tasks we participate in Task 3 (paragraph-level QE) in both subtasks, scoring and ranking.", "labels": [], "entities": []}, {"text": "The evaluation for the scoring task was done using Mean Absolute Error (MAE) and the evaluation for the ranking task was done by DeltaAvg (official metrics of the competition).", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 51, "end_pos": 76, "type": "METRIC", "confidence": 0.9437916378180186}, {"text": "DeltaAvg", "start_pos": 129, "end_pos": 137, "type": "DATASET", "confidence": 0.89482581615448}]}, {"text": "Data the official data of Task 3 -WMT15 QE shared task consist of 1215 paragraphs for EN-DE and DE-EN, extracted from the corpora of WMT13 machine translation shared task ().", "labels": [], "entities": [{"text": "WMT15 QE shared task", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.8671712875366211}, {"text": "WMT13 machine translation shared task", "start_pos": 133, "end_pos": 170, "type": "TASK", "confidence": 0.6425198197364808}]}, {"text": "For training, 800 paragraphs were used and, for test, 415 paragraphs were considered.", "labels": [], "entities": []}, {"text": "METEOR () was used as quality labels.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.619265079498291}]}, {"text": "Feature combination we experimented with different feature sets: \u2022 baseline Exhaustive search We investigate the efficacy of the baseline features by learning one Bayesian Ridge classifier for each feature and evaluating the classifiers based on MAE.", "labels": [], "entities": [{"text": "MAE", "start_pos": 246, "end_pos": 249, "type": "METRIC", "confidence": 0.9007372856140137}]}, {"text": "To examine the best set of features among the baseline features, we implemented an exhaustive feature selection search by enumerating all possible feature combinations.", "labels": [], "entities": []}, {"text": "Given n number of features, S, there are 2 n -1 number of possible feature combinations since a k-combination of a set forms a subset of k distinct elements of S.", "labels": [], "entities": []}, {"text": "The set of n elements, the number of k-combination is equal to the binomial coefficient: And the sum of all possible k-combinations: We note that the exhaustive search for feature selection is only possible in low feature space but from the results above, it is possible to approximate the best feature combination by using the N-best performing features when the classifier is trained solely on each of the feature.", "labels": [], "entities": []}, {"text": "For both languages, the exhaustive search selected three features only.", "labels": [], "entities": [{"text": "exhaustive", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9383332133293152}]}, {"text": "For EN-DE: \u2022 average source token length \u2022 percentage of unigrams in quartile 4 of frequency of source words in a corpus of the source language \u2022 percentage of trigrams in quartile 4 of frequency of source words in a corpus of the source language.", "labels": [], "entities": []}, {"text": "For DE-EN: \u2022 type/token ratio \u2022 percentage of unigrams in quartile 1 of frequency of source words in a corpus of the source language \u2022 percentage of trigrams in quartile 1 of frequency of source words in a corpus of the source language.", "labels": [], "entities": []}, {"text": "Machine learning algorithms for the feature combination experiments (with backward feature selection) we used the SVR implementation in the scikit-learn toolkit with parameters optimised via grid search.", "labels": [], "entities": []}, {"text": "shows the results of all experiments, for both language directions (EN-DE and DE-EN) and for scoring (MAE) and ranking (DeltaAvg) subtasks.", "labels": [], "entities": [{"text": "scoring (MAE)", "start_pos": 93, "end_pos": 106, "type": "METRIC", "confidence": 0.8825841993093491}]}, {"text": "For EN-DE, BFF showed the best result for scoring, and Baseline + discourse repetition showed the best result for ranking.", "labels": [], "entities": [{"text": "BFF", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9982439279556274}]}, {"text": "For DE-EN, Backward feature selection showed the best results for both scoring and ranking (although BFF showed similar results for scoring).", "labels": [], "entities": [{"text": "BFF", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.848227322101593}]}], "tableCaptions": [{"text": " Table 1: Results of all combinations of features", "labels": [], "entities": []}, {"text": " Table 2: MAE of classifiers trained with one baseline feature -the top three features are shown in bold", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7484694719314575}]}, {"text": " Table 3: Top five feature combinations with the  lowest MAE", "labels": [], "entities": [{"text": "MAE", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.997138261795044}]}]}