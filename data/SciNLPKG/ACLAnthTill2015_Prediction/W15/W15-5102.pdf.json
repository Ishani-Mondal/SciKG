{"title": [{"text": "Bridging the gap between sign language machine translation and sign language animation using sequence classification", "labels": [], "entities": [{"text": "sign language machine translation", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6312708631157875}, {"text": "sign language animation", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.6387813091278076}, {"text": "sequence classification", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.7145657241344452}]}], "abstractContent": [{"text": "To date, the non-manual components of signed utterances have rarely been considered in automatic sign language translation.", "labels": [], "entities": [{"text": "automatic sign language translation", "start_pos": 87, "end_pos": 122, "type": "TASK", "confidence": 0.7188788056373596}]}, {"text": "However, these components are capable of carrying important linguistic information.", "labels": [], "entities": []}, {"text": "This paper presents work that bridges the gap between the output of a sign language translation system and the input of a sign language animation system by incorporating non-manual information into the final output of the translation system.", "labels": [], "entities": [{"text": "sign language translation", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.661027709643046}]}, {"text": "More precisely, the generation of non-manual information is scheduled after the machine translation step and treated as a sequence classification task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7048041969537735}, {"text": "sequence classification task", "start_pos": 122, "end_pos": 150, "type": "TASK", "confidence": 0.7485904594262441}]}, {"text": "While sequence classification has been used to solve automatic spoken language processing tasks, we believe this to be the first work to apply it to the generation of non-manual information in sign languages.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.815603107213974}, {"text": "automatic spoken language processing tasks", "start_pos": 53, "end_pos": 95, "type": "TASK", "confidence": 0.7374557137489319}]}, {"text": "All of our experimental approaches outperformed lower base-line approaches, consisting of unigram or bigram models of non-manual features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sign languages are often the preferred means of communication of deaf and hard-of-hearing persons, making it vital to provide access to information in these languages.", "labels": [], "entities": []}, {"text": "Technologies for automatically translating written text (in a spoken language 1 ) into a sign language would therefore increase the accessibility of information sources for many people.", "labels": [], "entities": []}, {"text": "Sign languages are natural languages and, as such, fully developed linguistic systems.", "labels": [], "entities": []}, {"text": "While there area variety of sign languages used internationally, they share several key properties: Utterances in sign languages are produced with the hands/arms (the manual activity) and the shoulders, head, and face (the nonmanual activity).", "labels": [], "entities": []}, {"text": "Manual and non-manual components together form the sublexical components.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the experiments described here was to predict the most probable sequence of non-manual features fora sequence of glosses output by the machine translation system (cf..", "labels": [], "entities": []}, {"text": "As stated in Section 3.1, CRFs allow the prediction of one outcome layer at a time.", "labels": [], "entities": []}, {"text": "Hence, the two label layers head and eyebrows in our corpus (cf. Section 2) could either be collapsed into a single label (Configuration G\u2192H+E,), or a separate classifier could be trained for each feature (Configurations G\u2192H and G\u2192E,).", "labels": [], "entities": []}, {"text": "A downside of Configuration G\u2192H+E is that there is a potential for data sparseness, as the number of possible outcome labels is equivalent to the number of cross-combinations of head and eyebrow labels occurring in the training data.", "labels": [], "entities": [{"text": "Configuration G\u2192H", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.8538021743297577}]}, {"text": "However, even with this approach, the risk of data sparseness is lower than that of appending the non-manual features to the sign language glosses during the machine translation task, as described at the beginning of Section 3.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 158, "end_pos": 182, "type": "TASK", "confidence": 0.81109090646108}]}], "tableCaptions": [{"text": " Table 8: Sequence classification experiments: Results", "labels": [], "entities": [{"text": "Sequence classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9760975539684296}]}]}