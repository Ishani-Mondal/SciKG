{"title": [{"text": "CASICT-DCU Participation in WMT2015 Metrics Task", "labels": [], "entities": [{"text": "WMT2015 Metrics", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.5448435246944427}]}], "abstractContent": [{"text": "Human-designed sub-structures are required by most of the syntax-based machine translation evaluation metrics.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.7856226563453674}]}, {"text": "In this paper, we propose a novel evaluation metric based on dependency parsing model, which does not need this human involvement.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8029305338859558}]}, {"text": "Experimental results show that the new single metric gets better correlation than METEOR on system level and is comparable with it on sentence level.", "labels": [], "entities": [{"text": "correlation", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.986681342124939}, {"text": "METEOR", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.8633652925491333}]}, {"text": "To introduce more information, we combine the new metric with many other metrics.", "labels": [], "entities": []}, {"text": "The combined metric obtains state-of-the-art performance on both system level evaluation and sentence level evaluation on WMT 2014.", "labels": [], "entities": [{"text": "WMT 2014", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.9491057395935059}]}], "introductionContent": [{"text": "Automatic evaluation metrics play an important role in machine translation research.", "labels": [], "entities": [{"text": "machine translation research", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.8882763584454855}]}, {"text": "At present, most of the automatic evaluation metrics evaluate the translation quality by comparing the similarity between the hypothesis and the reference.", "labels": [], "entities": []}, {"text": "The lexicon-based metrics can only use lexical information, such as BLEU (), NIST) and METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.998646080493927}, {"text": "METEOR", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.8487613201141357}]}, {"text": "To evaluate the hypothesis on syntactic level, some researchers proposed the syntax-based metrics.", "labels": [], "entities": []}, {"text": "proposed a constituent-tree-based metric STM and a dependency-tree-based metric HWCM.", "labels": [], "entities": [{"text": "HWCM", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.8714301586151123}]}, {"text": "The syntax-based metric proposed by uses the Lexical-Functional Grammar (LFG) dependency tree.", "labels": [], "entities": []}, {"text": "Some metrics introduce the syntactic information on the basis of lexical information, such as MAXSIM () and the metric proposed by.", "labels": [], "entities": [{"text": "MAXSIM", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.7463986277580261}]}, {"text": "These metrics evaluate the syntactic similarity by comparing the sub-structures extracted from the trees of hypothesis and reference.", "labels": [], "entities": []}, {"text": "To avoid parsing the hypothesis in order to prevent translation error propagation, some researchers propose a kind of syntax-based evaluation metric which only uses the tree of reference, such as BLEU\u02c6ATREBLEU\u02c6 BLEU\u02c6ATRE ( and RED ().", "labels": [], "entities": [{"text": "translation error propagation", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.8784700036048889}]}, {"text": "The syntax-based metrics either use the substructures of both the reference and the hypothesis tree, or only use that on the reference side.", "labels": [], "entities": []}, {"text": "Therefore, for these metrics, sub-structures designed by human are required.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel dependency-parsing-model-based metric in the view of dependency tree generation, which completely avoids this human involvement.", "labels": [], "entities": [{"text": "dependency tree generation", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.7395709156990051}]}, {"text": "A dependency parsing model is trained by the reference dependency tree, through which we can obtain the dependency tree of the hypothesis and the corresponding score.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.8474146127700806}]}, {"text": "The syntactic similarity between the hypothesis and the reference can be evaluated by this score.", "labels": [], "entities": []}, {"text": "In order to obtain the lexicon similarity, we also introduce the unigram F-score to the new metric.", "labels": [], "entities": [{"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.7988992929458618}]}, {"text": "The experimental results show that the new metric gets the state-of-the-art performance in the single metrics on system level evaluation, and gets the comparable correlation with METEOR on sentence level evaluation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.8853768110275269}]}, {"text": "We also propose a combined metric 1 which combines the new metric with many other metrics together.", "labels": [], "entities": []}, {"text": "The combined metric obtains state-of-the-art performance on both system level and sentence level.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 describes the dependency-parsingmodel-based metric; Section 3 presents the combined metric; Section 4 gives the experiment results; Conclusions are discussed in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the syntactic similarity via the Dependency Paring Model score of hypothesis and evaluate the lexical similarity via the unigram F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.6400601267814636}]}, {"text": "So we name the new metric as DPMF.", "labels": [], "entities": [{"text": "DPMF", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8381664752960205}]}, {"text": "There are four steps to obtain the dependency parsing model score of the hypothesis.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.772683709859848}]}, {"text": "1) Obtain the reference dependency tree which can be generated by the automatic parsing tools or labeled by human.", "labels": [], "entities": []}, {"text": "2) Train a dependency parsing model using the reference dependency tree.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8326765596866608}]}, {"text": "3) Parse the hypothesis using the dependency parsing model and get the probability of the hypothesis dependency tree.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7398167252540588}]}, {"text": "4) Normalize the probability of the hypothesis dependency tree.", "labels": [], "entities": []}, {"text": "We define the normalized probability of the hypothesis dependency tree as the dependency parsing model score.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.6880834102630615}]}, {"text": "After obtaining the dependency parsing model score of a hypothesis, we multiply this score by unigram Fscore to get the final score of DPMF.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7502957582473755}, {"text": "DPMF", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.7836647629737854}]}, {"text": "The detailed description of our metric will be found in paper.", "labels": [], "entities": []}, {"text": "We only give the experiment results in this paper.", "labels": [], "entities": []}, {"text": "From the published results of WMT 2014, we can see that the combined metrics such as DISCOTK-PARTY () and UPC-STOUT () obtained great success which can make use of many single metrics.", "labels": [], "entities": [{"text": "WMT 2014", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.7838311493396759}, {"text": "DISCOTK-PARTY", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.9340818524360657}, {"text": "UPC-STOUT", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.46712613105773926}]}, {"text": "In most of the cases, combined metrics can obtain good correlations, so we also propose a combined metric which combines DPMF with some other single metrics.", "labels": [], "entities": []}, {"text": "The combined metric is named as DPMF comb and it involves DPMF, REDp, ENTFp 2 and some metrics included in the open source toolkit Asiya 3 . We introduce REDp, ENTFp and Asiya briefly in the rest of this section.", "labels": [], "entities": [{"text": "REDp", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.8761337995529175}]}, {"text": "To evaluate the performance of DPMF and DPMF comb , we carryout the experiments on both system level evaluation and sentence level evaluation.", "labels": [], "entities": []}, {"text": "In this section, we first describe the data sets and the baseline metrics in the experiments, and then give and analyse the experimental results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of translation systems  for each language pair on WMT 2014. cs-en  means Czech-to-English. de-en means German- to-English. fr-en means French-to-English. ru-en  means Russian-to-English.", "labels": [], "entities": [{"text": "WMT 2014", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9713071882724762}]}, {"text": " Table 2: System level correlations on WMT 2014. Asiya represents the combined metric only using the  metrics in Asiya. The value in bold is the best result in each column. average stands for the average  result of all the language pairs for each metric on WMT 2014.", "labels": [], "entities": [{"text": "WMT 2014. Asiya", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.8709560334682465}, {"text": "WMT 2014", "start_pos": 257, "end_pos": 265, "type": "DATASET", "confidence": 0.9267339110374451}]}, {"text": " Table 3: Sentence level correlations on WMT 2014. Asiya represents the combined metric only using  the metrics in Asiya. The value in bold is the best result in each column. average stands for the average  result of all the language pairs for each metric on WMT 2014.", "labels": [], "entities": [{"text": "WMT 2014. Asiya", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.8923669010400772}, {"text": "WMT 2014", "start_pos": 259, "end_pos": 267, "type": "DATASET", "confidence": 0.9175890684127808}]}]}