{"title": [{"text": "Computational Integration of Human Vision and Natural Language through Bitext Alignment", "labels": [], "entities": [{"text": "Computational Integration of Human Vision", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7785278797149658}, {"text": "Bitext Alignment", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7361067235469818}]}], "abstractContent": [{"text": "Multimodal integration of visual and linguistic data is a longstanding but crucial challenge for modeling human understanding.", "labels": [], "entities": []}, {"text": "We propose a framework that uses an unsupervised bitext alignment method to integrate visual and linguistic data.", "labels": [], "entities": []}, {"text": "We present an empirical study of the various parameters of the framework.", "labels": [], "entities": []}, {"text": "Our results exceed baselines using both exact and delayed temporal correspondence.", "labels": [], "entities": []}, {"text": "The resulting alignments can be used for image classification and retrieval.", "labels": [], "entities": [{"text": "image classification", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8696199357509613}]}], "introductionContent": [{"text": "Modeling and characterizing human expertise is a major bottleneck in advancing image-based application systems.", "labels": [], "entities": [{"text": "characterizing human expertise", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.733857274055481}]}, {"text": "We propose a framework for integrating experts' eye movements and verbal narrations as they examine and describe images in order to understand images semantically.", "labels": [], "entities": []}, {"text": "Eye movements can act as pointers to important image regions, while the co-captured descriptions provide conceptual labels associated with those regions.", "labels": [], "entities": []}, {"text": "Although successful when applied to scenic images in controlled experiments, many multimodal integration techniques do not transfer directly to scenarios requiring domain-specific expertise.", "labels": [], "entities": [{"text": "multimodal integration", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7264207303524017}]}, {"text": "Our approach is inspired by, who combine NLP methods with eye movements to generate linguistic descriptions of videos, and, who use image features to match words to the corresponding pictures.", "labels": [], "entities": []}, {"text": "We expand hereon earlier work) exploring multimodal integration in medical image annotation.", "labels": [], "entities": [{"text": "medical image annotation", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6255068282286326}]}, {"text": "Because an exact temporal match between the visual and verbal modalities cannot be assumed, our framework integrates the two modalities without enforcing strict temporal correspondence.", "labels": [], "entities": []}, {"text": "We use a bitext word alignment algorithm, originally developed for word alignment in machine translation, to align an expert's fixations on an image with the words in that expert's description of that image.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.6869089305400848}, {"text": "word alignment in machine translation", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.6640788555145264}]}, {"text": "The resulting alignments are then used to annotate image regions with corresponding conceptual labels, which in turn may aid image labeling and captioning applications.", "labels": [], "entities": [{"text": "image labeling", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.7394442856311798}]}, {"text": "In this paper we discuss the parameters of our framework and their effects on alignment accuracy.", "labels": [], "entities": [{"text": "alignment", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.9463903307914734}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9122761487960815}]}], "datasetContent": [], "tableCaptions": []}