{"title": [{"text": "Combining Relational and Distributional Knowledge for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7433051566282908}]}], "abstractContent": [{"text": "We present anew approach to word sense disambiguation derived from recent ideas in distributional semantics.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7567484180132548}]}, {"text": "The input to the algorithm is a large unlabeled corpus and a graph describing how senses are related; no sense-annotated corpus is needed.", "labels": [], "entities": []}, {"text": "The fundamental idea is to embed meaning representations of senses in the same continuous-valued vector space as the representations of words.", "labels": [], "entities": []}, {"text": "In this way, the knowledge encoded in the lexical resource is combined with the information derived by the distributional methods.", "labels": [], "entities": []}, {"text": "Once this step has been carried out, the sense representations can be plugged back into e.g. the skip-gram model, which allows us to compute scores for the different possible senses of a word in a given context.", "labels": [], "entities": []}, {"text": "We evaluated the new word sense dis-ambiguation system on two Swedish test sets annotated with senses defined by the SALDO lexical resource.", "labels": [], "entities": [{"text": "Swedish test sets", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.8847532272338867}, {"text": "SALDO lexical resource", "start_pos": 117, "end_pos": 139, "type": "DATASET", "confidence": 0.9172491033871969}]}, {"text": "In both evaluations , our system soundly outperformed random and first-sense baselines.", "labels": [], "entities": []}, {"text": "Its accuracy was slightly above that of a well-known graph-based system, while being computationally much more efficient.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995697140693665}]}], "introductionContent": [{"text": "For NLP applications such as word sense disambiguation (WSD), it is crucial to use some sort of representation of the meaning of a word.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.8335241923729578}]}, {"text": "There are two broad approaches commonly used in NLP to represent word meaning: representations based on the structure of a formal knowledge representation, and those derived from co-occurrence statistics in corpora (distributional representations).", "labels": [], "entities": []}, {"text": "Ina knowledge-based word meaning representation, the meaning of a word string is defined by mapping it to a symbolic concept defined in a knowledge base or ontology, and the meaning of the concept itself is defined in terms of its relations to other concepts, which can be used to deduce facts that were not stated explicitly: a mouse is a type of rodent, so it has prominent teeth.", "labels": [], "entities": [{"text": "knowledge-based word meaning representation", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.7048159092664719}]}, {"text": "On the other hand, in a data-driven meaning representation, the meaning of a word in defined as a point in a geometric space, which is derived from the word's cooccurrence patterns so that words with a similar meaning end up near each other in the vector space.", "labels": [], "entities": []}, {"text": "The most important relation between the meaning representations of two words is typically similarity: a mouse is something quite similar to a rat.", "labels": [], "entities": []}, {"text": "Similarity of meaning is often operationalized in terms of the geometry of the vector space, e.g. by defining a distance metric.", "labels": [], "entities": []}, {"text": "These two broad frameworks obviously have very different advantages: while the symbolic representations contain explicit and very detailed relational information, the data-driven representations handle the notion of graded similarity in a very natural way, and the fact that they typically have a wide vocabulary coverage makes it attractive to integrate them in NLP systems for additional robustness.", "labels": [], "entities": []}, {"text": "However, there are many reasons to study how these two very dissimilar approaches can complement each other.", "labels": [], "entities": []}, {"text": "showed that vector spaces represent more structure than previously thought: they implicitly encode a wide range of syntactic and semantic relations, which can be recovered using simple linear algebra operations.", "labels": [], "entities": []}, {"text": "For instance, the geometric relation between Rome and Italy is similar to that between Cairo and Egypt.", "labels": [], "entities": []}, {"text": "further analyzed how this property can be explained.", "labels": [], "entities": []}, {"text": "One aspect where symbolic representations seem to have an advantage is in describing word sense ambiguity: the fact that one surface form may correspond to more than one underlying concept.", "labels": [], "entities": []}, {"text": "For instance, the word mouse can refer to a rodent or an electronic device.", "labels": [], "entities": []}, {"text": "Except for scenarios where a small number of senses are used, lexical-semantic resources such as WordNet) for English and SALDO ( for Swedish are crucial in applications that rely on sense meaning, WSD above all.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9161759614944458}, {"text": "SALDO", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.6458209753036499}]}, {"text": "Corpus-derived representations on the other hand typically have only one representation per surface form, which makes it hard to search e.g. fora group of words similar to the rodent sense of mouse 1 or to reliably use the vector in machine learning methods that generalize from the semantics of the word.", "labels": [], "entities": []}, {"text": "One straightforward solution could be to build a vector-space semantic representation from a sense-annotated corpus, but this is infeasible since fairly large corpora are needed to induce data-driven representations of a high quality, while sense-annotated corpora are small and scarce.", "labels": [], "entities": []}, {"text": "Instead, there have been several attempts to create vectors representing the senses of ambiguous words, most of them based on some variant of the idea first proposed by: that senses can be seen as clusters of similar contexts.", "labels": [], "entities": []}, {"text": "Further examples where this idea has reappeared include the work by, as well as a number of recent papers.", "labels": [], "entities": []}, {"text": "However, sense distributions are often highly imbalanced, it is not clear that context clusters can be reliably created for senses that occur rarely.", "labels": [], "entities": []}, {"text": "In this work, we build a word sense disambiguation system by combining the two approaches to representing meaning.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.6757699449857076}]}, {"text": "The crucial steppingstone is the recently developed algorithm by, which derives vector-space representations of word senses by embedding the graph structure of a semantic network in the word vector space.", "labels": [], "entities": []}, {"text": "A scoring function for selecting a sense can then be derived from a word-based distributional model in a very intuitive way simply by reusing the scoring function used to construct the original word-based vector space.", "labels": [], "entities": []}, {"text": "This approach to WSD is attractive because it can leverage corpus statistics similar to a supervised method trained on an annotated corpus, but also use the lexical-semantic resource for generalization.", "labels": [], "entities": [{"text": "WSD", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9691555500030518}]}, {"text": "Moreover, the sense representation algorithm also estimates how common the different senses are; finding the predominant sense of a word also gives a strong baseline for WSD (, and is of course also interesting from a lexicographical perspective.", "labels": [], "entities": [{"text": "sense representation", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.6962998360395432}, {"text": "WSD", "start_pos": 170, "end_pos": 173, "type": "TASK", "confidence": 0.4692363739013672}]}, {"text": "We applied the algorithm to derive vector representations for the senses in SALDO, a Swedish semantic network (, and we used these vectors to build a disambiguation system that can assign a SALDO sense to ambiguous words occurring in free text.", "labels": [], "entities": []}, {"text": "To evaluate the system, we created two new benchmark sets by processing publicly available datasets.", "labels": [], "entities": []}, {"text": "On these benchmarks, our system outperforms a random baseline by a wide margin, but also a first-sense baseline significantly.", "labels": [], "entities": []}, {"text": "It achieves a slightly higher score than UKB, a highly accurate graph-based WSD system, but is several orders of magnitude faster.", "labels": [], "entities": [{"text": "UKB", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.9327999949455261}]}, {"text": "The highest disambiguation accuracy was achieved by combining the probabilities output by the two systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9952635765075684}]}, {"text": "Furthermore, in a qualitative inspection of the most ambiguous words in SALDO for each word class, we see that the sense distribution estimates provided by the sense embedding algorithm are good for nouns, adjectives, and adverbs, although less so for verbs.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our new WSD system, we applied it to two test sets and first compared it to a number of baselines, and finally to UKB, a well-known graph-based WSD system.", "labels": [], "entities": [{"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9797376990318298}, {"text": "UKB", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.9773707389831543}]}, {"text": "Our two test sets were the SALDO examples (SALDO-ex) and the Swedish FrameNet examples (SweFN-ex) . Both resources consist of sentences selected by lexicographers for illustration of word senses.", "labels": [], "entities": []}, {"text": "At the time of our experiments, SALDO-ex contained 4,489 sentences.", "labels": [], "entities": [{"text": "SALDO-ex contained 4,489 sentences", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.5178307592868805}]}, {"text": "In each sentence, one of the tokens (the target word) has been marked up by a lexicographer and assigned a SALDO sense.", "labels": [], "entities": [{"text": "SALDO", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9581639766693115}]}, {"text": "SweFN-ex contained 7,991 sentences, and as in SALDO-ex the annotation consists of disambiguated target words: the difference is that instead of a SALDO sense, the target word is assigned a FrameNet frame.", "labels": [], "entities": []}, {"text": "However, using the Swedish FrameNet lexicon (Friberg Heppin and Toporowska Gronostaj, 2012), frames can inmost cases be deterministically mapped to SALDO senses: for instance, the first SALDO sense of the noun stam ('trunk' or 'stem') belongs to the frame PLANT SUBPART, while the second sense ('tribe') is in the frame AGGREGATE.", "labels": [], "entities": [{"text": "Swedish FrameNet lexicon", "start_pos": 19, "end_pos": 43, "type": "DATASET", "confidence": 0.7161204020182291}]}, {"text": "We preprocessed these two test sets using Spr\u00e5kbanken's annotation services 9 to tokenize, compound-split, and lemmatize the texts and to determine the set of possible senses in a given context.", "labels": [], "entities": []}, {"text": "All unambiguous instances were removed from the sets, and we also excluded sentences where the target consisted of more than one word.", "labels": [], "entities": []}, {"text": "We then ended up with 1,177 and 1,429 instances in SALDO-ex and SweFN-ex, respectively.", "labels": [], "entities": [{"text": "SALDO-ex", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8578595519065857}, {"text": "SweFN-ex", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.7462096214294434}]}, {"text": "shows the distribution of the number of senses for target word in the combination of the two sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Predominant sense selection accuracy.", "labels": [], "entities": [{"text": "Predominant sense selection", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.5075532992680868}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9729523062705994}]}, {"text": " Table 2: Comparison to baselines.", "labels": [], "entities": []}, {"text": " Table 3: Results for different parts of speech.", "labels": [], "entities": []}, {"text": " Table 4. This ta- ble also includes the result of a combined system  where we simply added Eq. 2 to the log of the  probability output by UKB.", "labels": [], "entities": [{"text": "Eq", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9825021028518677}, {"text": "UKB", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.9769496917724609}]}, {"text": " Table 4: Comparison to the UKB system.", "labels": [], "entities": [{"text": "UKB system", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9844411015510559}]}]}