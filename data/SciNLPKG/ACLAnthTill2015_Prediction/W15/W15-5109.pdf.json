{"title": [{"text": "Recognizing Dysarthric Speech due to Amyotrophic Lateral Sclerosis with Across-Speaker Articulatory Normalization", "labels": [], "entities": [{"text": "Recognizing Dysarthric Speech", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8071218729019165}]}], "abstractContent": [{"text": "Recent dysarthric speech recognition studies using mixed data from a collection of neurological diseases suggested articula-tory data can help to improve the speech recognition performance.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7263081520795822}, {"text": "speech recognition", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7669601738452911}]}, {"text": "This project was specifically designed for the speaker-independent recognition of dysarthric speech due to amy-otrophic lateral sclerosis (ALS) using articulatory data.", "labels": [], "entities": [{"text": "speaker-independent recognition of dysarthric speech due to amy-otrophic lateral sclerosis (ALS)", "start_pos": 47, "end_pos": 143, "type": "TASK", "confidence": 0.669754262153919}]}, {"text": "In this paper, we investigated three across-speaker normalization approaches in acoustic, articulatory, and both spaces: Procrustes matching (a physiological approach in articulatory space), vocal tract length normalization (a data-driven approach in acoustic space), and feature space maximum likelihood linear regression (a model-based approach for both spaces), to address the issue of high degree of variation of articulation across different speakers.", "labels": [], "entities": [{"text": "vocal tract length normalization", "start_pos": 191, "end_pos": 223, "type": "TASK", "confidence": 0.5761602520942688}]}, {"text": "A preliminary ALS data set was collected and used to evaluate the approaches.", "labels": [], "entities": [{"text": "ALS data set", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.8763349056243896}]}, {"text": "Two recognizers, Gaussian mixture model (GMM)-hidden Markov model (HMM) and deep neural network (DNN)-HMM, were used.", "labels": [], "entities": []}, {"text": "Experimental results showed adding articulatory data significantly reduced the phoneme error rates (PERs) using any or combined normalization approaches.", "labels": [], "entities": [{"text": "phoneme error rates (PERs)", "start_pos": 79, "end_pos": 105, "type": "METRIC", "confidence": 0.7977966070175171}]}, {"text": "DNN-HMM outperformed GMM-HMM in all configurations.", "labels": [], "entities": [{"text": "DNN-HMM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9003890752792358}, {"text": "GMM-HMM", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9544840455055237}]}, {"text": "The best performance (30.7% PER) was obtained by triphone DNN-HMM + acoustic and articulatory data + all three normal-ization approaches, a 15.3% absolute PER reduction from the baseline using triphone GMM-HMM + acoustic data.", "labels": [], "entities": [{"text": "PER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9904078841209412}, {"text": "PER", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.7913436889648438}]}], "introductionContent": [{"text": "Although automatic speech recognition (ASR) technologies have been commercially available for healthy talkers, these technologies did not perform satisfactorily well when directly used for talkers with dysarthria, a motor speech disorder due to neurological or other injury.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.8077618181705475}]}, {"text": "Dysarthric speech is always with degraded speech intelligibility due to impaired voice and articulation functions.", "labels": [], "entities": []}, {"text": "For example, Parkinson's disease and amyotrophic lateral sclerosis (ALS) impact the patient's motor functions and therefore impair their speech.", "labels": [], "entities": []}, {"text": "Only a few studies have been focused on dysarthric speech recognition.", "labels": [], "entities": [{"text": "dysarthric speech recognition", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6165459752082825}]}, {"text": "Recent studies using mixed data from a variety of neurological diseases indicated articulatory data can improve the speech recognition performance.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.829074889421463}]}, {"text": "However, dysarthric speech recognition particularly for ALS has rarely been studied.", "labels": [], "entities": [{"text": "dysarthric speech recognition", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.6309339900811514}, {"text": "ALS", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9328070878982544}]}, {"text": "ALS, also known as Lou Gehrig's disease, is the most common motor neuron disease that causes the death of both upper and lower motor neurons.", "labels": [], "entities": []}, {"text": "The cause of the disease is unknown for most of the patients and only a small portion (5-10%) of patients is inherited.", "labels": [], "entities": []}, {"text": "As the disease progresses, the patient's speech intelligibility declines.", "labels": [], "entities": []}, {"text": "Eventually all patients have degraded speech and need an assistive device for communication.", "labels": [], "entities": []}, {"text": "Normal speech recognition technology (typically trained on healthy talkers' data) does notwork satisfactorily well for the patients.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7182759195566177}]}, {"text": "Therefore, ALS patients' ability to use modern speech technology (e.g., smart home environment control driven by speech recognition) is limited.", "labels": [], "entities": [{"text": "smart home environment control driven by speech recognition)", "start_pos": 72, "end_pos": 132, "type": "TASK", "confidence": 0.6598242488172319}]}, {"text": "This project, to our best knowledge, is the first one specifically designed to improve speech recognition performance for ALS using articulatory data.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.813246488571167}]}, {"text": "The high degree of variation in articulatory patterns across speakers has been a barrier for speaker-independent speech recognition with articulatory data.", "labels": [], "entities": [{"text": "speaker-independent speech recognition", "start_pos": 93, "end_pos": 131, "type": "TASK", "confidence": 0.6188279589017233}]}, {"text": "Multiple sources contributed to the inter-talker variation including gender, dialect, individual vocal tract anatomy, and different co-articulation patterns.", "labels": [], "entities": []}, {"text": "However, speaker-independent approaches are important for reducing the amount of training data required from each user.", "labels": [], "entities": []}, {"text": "Only limited articulatory data samples are often available from individuals with ALS (even with healthy talkers) due to the logistic difficulty of articulatory data collection.", "labels": [], "entities": []}, {"text": "For example, in data collection using electromagnetic articulograph (EMA), small sensors have to be attached on the tongue using dental glue.", "labels": [], "entities": []}, {"text": "The procedure requires the patient to hold his/her tongue to a position fora while so that the glue can take effect.", "labels": [], "entities": []}, {"text": "To reduce speaker-specific difference, researchers have tried different approaches to normalize the articulatory movements including data-driven approaches (e.g., principal component analysis) or physiological approaches including aligning the tongue position when producing vowels, consonants, and pseudo-words to a reference (e.g., palate, or a general tongue shape  Procrustes matching, a bidimensional shape analysis technique, has been used to minimize the translational, scaling, and rotational effects of articulatory data across speakers.", "labels": [], "entities": [{"text": "Procrustes matching", "start_pos": 369, "end_pos": 388, "type": "TASK", "confidence": 0.9120509028434753}]}, {"text": "Recent studies indicated Procrustes matching was effective for speaker-independent silent speech recognition (i.e., recognizing speech from articulatory data only).", "labels": [], "entities": [{"text": "Procrustes matching", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6256354600191116}, {"text": "speaker-independent silent speech recognition", "start_pos": 63, "end_pos": 108, "type": "TASK", "confidence": 0.642890177667141}]}, {"text": "Procrustes matching, however, has rarely been used in dysarthric speech recognition with articulatory data.", "labels": [], "entities": [{"text": "Procrustes matching", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7316039502620697}, {"text": "dysarthric speech recognition", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.6622403164704641}]}, {"text": "In addition, we adopted two other representative approaches for across-speaker data normalization.", "labels": [], "entities": [{"text": "across-speaker data normalization", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.6632954378922781}]}, {"text": "Vocal tract length normalization (VTLN) which has been widely used in acoustic speech recognition, a data-driven approach in acoustic space, was used to extract normalized acoustic features.", "labels": [], "entities": [{"text": "Vocal tract length normalization (VTLN)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6401360758713314}, {"text": "acoustic speech recognition", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.643866628408432}]}, {"text": "The third approach, feature space maximum likelihood linear regression (fMLLR), a model-based adaptation, was used for both acoustic and articulatory data.", "labels": [], "entities": [{"text": "feature space maximum likelihood linear regression (fMLLR)", "start_pos": 20, "end_pos": 78, "type": "TASK", "confidence": 0.46483399470647174}]}, {"text": "In this paper, we investigated the use of 1) articulatory data as additional information source for speech, 2) Procrustes matching, VTLN, and fMLLR as feature normalization approaches individually or combined, 3) two machine learning classifiers, GMM-HMM and DNN-HMM.", "labels": [], "entities": [{"text": "fMLLR", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.845665454864502}]}, {"text": "The effectiveness of these speaker-independent dysarthric speech recognition approaches were evaluated with a preliminary data collected from multiple early diagnosed ALS patients.", "labels": [], "entities": [{"text": "speaker-independent dysarthric speech recognition", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.6306094154715538}]}], "datasetContent": [{"text": "The long-standing GMM-HMM and recently available DNN-HMM were used as the recognizers.", "labels": [], "entities": [{"text": "GMM-HMM", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9582695364952087}]}, {"text": "In this experiment, window size was 25 ms for acoustic features and frame rate was 10 ms for both acoustic and articulatory features.", "labels": [], "entities": [{"text": "frame rate", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.957436740398407}]}, {"text": "For each frame, static features plus derivative and acceleration form 39-dimensional mel-frequency cepstral coefficient and approximately 200 dimensions (varies for each configuration in triphone model) for monophone and triphone models, respectively.", "labels": [], "entities": [{"text": "acceleration", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.9655259251594543}, {"text": "mel-frequency cepstral coefficient", "start_pos": 85, "end_pos": 119, "type": "METRIC", "confidence": 0.8050113717714945}]}, {"text": "We used 1 to 6 hidden layers and each layer had 512 nodes.", "labels": [], "entities": []}, {"text": "The best performance obtained using 1 to 6 layers was  Note: The degree indicates a counterclockwise rotation.", "labels": [], "entities": []}, {"text": "Radians converted from degrees were actually used in the rotation. reported.", "labels": [], "entities": []}, {"text": "shows the detailed experimental setup.", "labels": [], "entities": []}, {"text": "The training and decoding were performed using the Kaldi speech recognition toolkit.", "labels": [], "entities": [{"text": "Kaldi speech recognition", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.5921235680580139}]}, {"text": "Phoneme error rate (PER) was used as the measure of dysarthric speech recognition performance.", "labels": [], "entities": [{"text": "Phoneme error rate (PER)", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.8040645271539688}, {"text": "speech recognition", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.675443708896637}]}, {"text": "PER is the summation of substitution, insertion, and deletion errors of phonemes divided by the number of all phonemes.", "labels": [], "entities": [{"text": "PER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.982324481010437}]}, {"text": "Leave-one-subject-out cross validation was used in the experiment.", "labels": [], "entities": []}, {"text": "In each execution, all samples from one subject were used for testing and the samples from the rest subjects were used for training.", "labels": [], "entities": []}, {"text": "The average performance of executions was calculated as the overall performance.", "labels": [], "entities": []}, {"text": "shows detailed parameters (angles and centroids) for Procrustes matching, which varies for different speakers. and show the warping factors for each speaker and their histogram.", "labels": [], "entities": [{"text": "Procrustes matching", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7927931547164917}]}, {"text": "The histogram of ALS patients follows general trend of warping factor distribution for females (typically < 1.0) and males (typically > 1.0).", "labels": [], "entities": [{"text": "warping factor distribution", "start_pos": 55, "end_pos": 82, "type": "METRIC", "confidence": 0.9262463251749674}]}], "tableCaptions": [{"text": " Table 1: ALS participants and data size information.", "labels": [], "entities": [{"text": "ALS", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9149415493011475}]}, {"text": " Table 4: Warping factor (\u03b1) for each speaker in testing or train- ing stages.", "labels": [], "entities": [{"text": "Warping factor (\u03b1)", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9607748746871948}]}]}