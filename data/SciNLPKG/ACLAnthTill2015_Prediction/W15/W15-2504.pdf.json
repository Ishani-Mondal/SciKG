{"title": [{"text": "Document-Level Machine Translation Evaluation with Gist Consistency and Text Cohesion", "labels": [], "entities": [{"text": "Document-Level Machine Translation Evaluation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7256564423441887}]}], "abstractContent": [{"text": "Current Statistical Machine Translation (SMT) is significantly affected by Machine Translation (MT) evaluation metric.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.8187256654103597}, {"text": "Machine Translation (MT) evaluation", "start_pos": 75, "end_pos": 110, "type": "TASK", "confidence": 0.8499517937501272}]}, {"text": "Nowadays the emergence of document-level MT research increases the demand for corresponding evaluation metric.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.8254388570785522}]}, {"text": "This paper proposes two superior yet low-cost quantitative objective methods to enhance traditional MT metric by modeling document-level phenomena from the perspectives of gist consistency and text cohesion.", "labels": [], "entities": [{"text": "MT metric", "start_pos": 100, "end_pos": 109, "type": "TASK", "confidence": 0.9664739370346069}]}, {"text": "The experimental results show the proposed metrics can obtain better correlation with human judgments than traditional metrics on evaluating document-level translation quality.", "labels": [], "entities": [{"text": "document-level translation quality", "start_pos": 141, "end_pos": 175, "type": "TASK", "confidence": 0.6865513920783997}]}], "introductionContent": [{"text": "Since most of current SMT models impose strong independence assumptions on words and sentences, most of these systems only work at sentence level and cannot employ useful relationships among sentences during decoding.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.991161584854126}]}, {"text": "However, a text rather than individual words or fragments of sentences is the basic unit of communication.", "labels": [], "entities": []}, {"text": "define that text is a communicative occurrence which meets seven standards, such as textuality cohesion, coherence.", "labels": [], "entities": []}, {"text": "Text is constituted by sentences, but there exist separate principles of text-construction beyond the rules for making sentences.", "labels": [], "entities": []}, {"text": "Document is the carrier of text in modern computer system.", "labels": [], "entities": []}, {"text": "Currently more researching work focus on document-level SMT; Xiong et al, * *Corresponding author.).", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.6944761872291565}]}, {"text": "However, most of these researches show their improvements by using system-level metrics, such as BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9984009861946106}]}, {"text": "Whether improvements in performance at system level are really able to reflect the change of text-level translation quality is still to doubt.", "labels": [], "entities": [{"text": "text-level translation", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.5467758029699326}]}, {"text": "Nowadays, the study of real document-level MT metrics has been drawing more and more attention.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.8983437418937683}]}, {"text": "Based on Discourse Representation Theory (, propose to use co-reference and discourse relations to build evaluation metrics.", "labels": [], "entities": []}, {"text": "The metrics by extending traditional metrics with lexical cohesion devices show some positive experimental results).", "labels": [], "entities": []}, {"text": "Bilingual topic model () is applied to do MT quality estimation().", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.8829795916875204}]}, {"text": "use two discourse-aware similarity measures based on discourse structure to improve existing MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 93, "end_pos": 106, "type": "TASK", "confidence": 0.9192741811275482}]}, {"text": "According to the afore-mentioned definition of text, the most important standard of evaluating translation quality for one document should be to what degree the MT output correctly communicates the main idea of origin text.", "labels": [], "entities": []}, {"text": "From this regard, this paper first proposes to measure gist consistency of text via topic model.", "labels": [], "entities": []}, {"text": "Topic model is a statistical model which assumes each document can be characterized by a particular set of topics.", "labels": [], "entities": []}, {"text": "Currently a variety of probabilistic topic models ( have been used to analyze the content of documents and the meaning of words.", "labels": [], "entities": []}, {"text": "Our experimental results show the MT evaluation metrics with robust topic model can effectively capture change of translation quality between reference and MT output at document level.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.8942973911762238}]}, {"text": "Furthermore, cohesion and coherence are important standards of textuality.", "labels": [], "entities": []}, {"text": "Coherence interprets meaning connectedness in the underlying text while cohesion can be formulated quite explicitly on the basis of grammatical and lexical properties.", "labels": [], "entities": []}, {"text": "This paper describes a simple yet effective cohesion function to measure text cohesion via lexical chain.", "labels": [], "entities": []}, {"text": "Our experimental results show that the number of matching lexical chain between reference and MT output can reflect the goodness of translation at document level.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 and 3 respectively describes how to model two kinds of document-level features.", "labels": [], "entities": []}, {"text": "Section 4 shows the framework of combing document-level scores with traditional metrics.", "labels": [], "entities": [{"text": "combing document-level", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8284456133842468}]}, {"text": "Section 5 presents the experimental results and Section 6 gives out discussion.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "For fair comparison and possible integration of our proposed document-level features, this section gives a brief introduction on two widely adopted MT evaluation metrics: BLEU () and METEOR ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 148, "end_pos": 161, "type": "TASK", "confidence": 0.8829730153083801}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9984539747238159}, {"text": "METEOR", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9551938772201538}]}, {"text": "As the most famous evaluation metric, BLEU is based on n-gram matching.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9977877140045166}]}, {"text": "Given a system translation, BLEU first collects all n-grams and count how many of them exist in one or more references (sentence by sentence), and then integrate the precisions of n-grams with different lengths into one score as follows: where p n is the precision of n-gram and BP is a penalty factor, preventing BLEU from favoring short segments due to the lack of direct consideration of recall.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9735783338546753}, {"text": "precision", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.9864885210990906}, {"text": "BP", "start_pos": 279, "end_pos": 281, "type": "METRIC", "confidence": 0.9934470057487488}, {"text": "BLEU", "start_pos": 314, "end_pos": 318, "type": "METRIC", "confidence": 0.9662169814109802}, {"text": "recall", "start_pos": 391, "end_pos": 397, "type": "METRIC", "confidence": 0.9951960444450378}]}, {"text": "It is obvious that, although BLEU takes all n-grams into consideration, the importance of different n-grams is ignored except their lengths.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9889041781425476}]}, {"text": "METEOR is based on unigram alignment of references and MT output.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5006341934204102}, {"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9184958338737488}]}, {"text": "Each unigram in one system translation is at most mapped to one unigram in the references first and then three successive stages of \"exact\", \"porter stem\" and \"WN synonymy\" are used to create alignment in turn.", "labels": [], "entities": [{"text": "exact", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9826712012290955}]}, {"text": "Once the final alignment is produced, unigram precision (P ) and recall (R) are calculated and combined into one F mean score: Finally, the METEOR score is obtained as follows: Where penis a penalty factor.", "labels": [], "entities": [{"text": "unigram precision (P )", "start_pos": 38, "end_pos": 60, "type": "METRIC", "confidence": 0.906083607673645}, {"text": "recall (R)", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9721846878528595}, {"text": "F mean score", "start_pos": 113, "end_pos": 125, "type": "METRIC", "confidence": 0.9621405998865763}, {"text": "METEOR score", "start_pos": 140, "end_pos": 152, "type": "METRIC", "confidence": 0.9698306024074554}]}, {"text": "METEOR is explicitly designed to improve the correlation with human judgments of MT quality at the sentence level and the performance of METEOR outperforms BLEU at sentence level.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7177805304527283}, {"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9668111801147461}, {"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9978439807891846}]}, {"text": "Based on the formula 5 or 7, document-level BLEU/METEOR score can be generated by aggregating sentences in a document rather than simply averaging scores at sentence level.", "labels": [], "entities": [{"text": "BLEU/METEOR score", "start_pos": 44, "end_pos": 61, "type": "METRIC", "confidence": 0.8297394514083862}]}, {"text": "To avoid the bias in the distributions of different judges' assessments in the evaluation data, we normalize the scores following.", "labels": [], "entities": []}, {"text": "It is worth noting that, due to the lack of document-level human assessments on the two evaluation dataset, document-level human assessments are averaged over sentence scores, weighted by sentence length.", "labels": [], "entities": []}, {"text": "This method is also adopted by famous MetricsMaTr (the NIST Metrics for Machine Translation Challenge) and approximated in and", "labels": [], "entities": [{"text": "Machine Translation Challenge", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.7626530528068542}]}], "tableCaptions": [{"text": " Table 3: The Kendall correlation between human", "labels": [], "entities": [{"text": "Kendall correlation", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.9479125142097473}]}, {"text": " Table 2: The correlation between the proposed metrics combining with gist consistency/text cohesion with human judgments", "labels": [], "entities": []}, {"text": " Table 4: The number of lexical chains extracted  from human translation and MT output on MTC4  and MTC2 (MTC2 only involves 3 MT systems)", "labels": [], "entities": [{"text": "MT output", "start_pos": 77, "end_pos": 86, "type": "TASK", "confidence": 0.8824191689491272}]}, {"text": " Table 5: The number of lexical chains( chain)", "labels": [], "entities": []}]}