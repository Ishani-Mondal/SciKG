{"title": [{"text": "Learning Salient Samples and Distributed Representations for Topic-Based Chinese Message Polarity Classification", "labels": [], "entities": [{"text": "Topic-Based Chinese Message Polarity Classification", "start_pos": 61, "end_pos": 112, "type": "TASK", "confidence": 0.6821260929107666}]}], "abstractContent": [{"text": "We describe our participation in the Topic-Based Chinese Message Polarity Classification Task, based on the restricted and unrestricted resources respectively.", "labels": [], "entities": [{"text": "Topic-Based Chinese Message Polarity Classification Task", "start_pos": 37, "end_pos": 93, "type": "TASK", "confidence": 0.7368276218573252}]}, {"text": "In the restricted resource based classification, we focus on the selection of parameters in a multi-class classification model with highly-biased training data.", "labels": [], "entities": []}, {"text": "In the unrestricted resource based classification, we explore the distributed representation of Chinese words through unsupervised feature learning and the annotation of salient samples through active learning, with a raw corpus of over 90 million messages extracted from Chinese Weibo Platform.", "labels": [], "entities": [{"text": "Chinese Weibo Platform", "start_pos": 272, "end_pos": 294, "type": "DATASET", "confidence": 0.9079103072484335}]}, {"text": "For two classification subtasks, our submitted results ranked the 4th and the 2nd respectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ZWK team participates in the Topic-Based Chinese Message Polarity Classification Task, the purpose of which is to predict the message polarities in the Positive, Negative, and Neutral classes towards particular topics.", "labels": [], "entities": [{"text": "Topic-Based Chinese Message Polarity Classification Task", "start_pos": 33, "end_pos": 89, "type": "TASK", "confidence": 0.7313686261574427}]}, {"text": "Learning classification models on the training corpus with bag-of-words features is very challenging, given the fact that the class labels are highly-biased in the corpus and that the number of training samples is an order of magnitude lower than the number of observed word features.", "labels": [], "entities": []}, {"text": "Therefore, our work focuses on the active learning and unsupervised feature learning algorithms, to avoid over-fitting the parameters of a linear classification model.", "labels": [], "entities": []}, {"text": "To predict polarities with respect to specific topics, we re-evaluate the features with respect to their distances to topical words in a message.", "labels": [], "entities": []}, {"text": "* These authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "Because the class labels are highly-biased in the training corpus, most of which are Neutral, we explore an active learning algorithm to incrementally obtain the knowledge of different polarities from a large raw corpus.", "labels": [], "entities": []}, {"text": "In the iterative procedure of active learning, salient samples are firstly selected from a large raw corpus, based on the amount of information in their polarity predictions, their representativeness within the raw corpus, and their distinctiveness in the selection.", "labels": [], "entities": []}, {"text": "The selection procedure ensures that samples of the minor classes are more probably selected than samples of the major class(es) and that the extension of training data with these samples has the most potential to improve the current classification model.", "labels": [], "entities": []}, {"text": "Then, class labels are annotated to the salient samples by querying oracles, and all labeled samples are appended to the training corpus to update the classification model before the next iteration in active learning.", "labels": [], "entities": []}, {"text": "We select and append the salient samples in a batch-mode, to efficiently re-balance the training corpus and incrementally improve the polarity classification model.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.6832857429981232}]}, {"text": "And because the number of training messages (around 5K) turns much smaller than the number of unique words (17.5K), a linear classification model can be easily over-fitted with bag-ofword features.", "labels": [], "entities": []}, {"text": "To avoid over-fitting, we project the 17.5K-dimensional word space to a 200-dimensional vector space through an unsupervised feature learning.", "labels": [], "entities": []}, {"text": "We employ word2vec () as the unsupervised feature learning algorithm, based on a raw corpus of over 90 million messages extracted from Chinese Weibo Platform.", "labels": [], "entities": [{"text": "Chinese Weibo Platform", "start_pos": 135, "end_pos": 157, "type": "DATASET", "confidence": 0.8823654850323995}]}, {"text": "One of the most significant advantage of learning with word2vec is that the vector representations are additively composable, which means we can represent the semantic composition of multiple words by adding the respective vector representations.", "labels": [], "entities": []}, {"text": "For the topic-based polarity classifica-tion problem, we only compose words around the specific topics as features, with an exponentially decreasing weight along the word sequence.", "labels": [], "entities": []}, {"text": "The rest of this paper is arranged as follows: section 2 reviews the related work of polarity classification, section 3 describes our active learning algorithm for retrieving salient samples, section 4 illustrates the unsupervised learning algorithm for reducing feature dimensions, section 5 shows our experiment results on polarity classification and discusses the over-fitting problem, and section 6 concludes our work.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.7195682972669601}, {"text": "polarity classification", "start_pos": 325, "end_pos": 348, "type": "TASK", "confidence": 0.7395783513784409}]}], "datasetContent": [{"text": "The We employ a raw corpus of 90 million messages from Chinese Weibo Platform, for developing salient samples with active learning and for learning distributed representation of words with unsupervised feature learning.", "labels": [], "entities": [{"text": "Chinese Weibo Platform", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.9095682899157206}]}, {"text": "All these messages are randomly collected from April to September in 2013.", "labels": [], "entities": []}, {"text": "Based on the One-vs-All Logistic Regression algorithm from scikit-learn 4 , we construct several polarity classifiers clf i with different features.", "labels": [], "entities": []}, {"text": "For the basic classifier clf 0 , we explore the bagof-word feature by collecting words which occur more than \"min occur\" times in the training corpus and by removing the most frequent \"stop num\" words in the collection.", "labels": [], "entities": []}, {"text": "We select model parameters \"C\", \"penalty\", \"class weight\" and feature parameters \"min occur\", \"stop num\" through grid search with 5-fold cross validation on the training corpus.", "labels": [], "entities": []}, {"text": "We employ an active learning algorithm to generate a less-biased training corpus as shown in.", "labels": [], "entities": []}, {"text": "Class labels have been significantly balanced after 14 loops of sample selection.", "labels": [], "entities": []}, {"text": "Classifier clf 1 is trained on this corpus, with a similar parameter selection procedure as clf 0 . We employ the word2vec algorithm to project the large word space to a small vector space.", "labels": [], "entities": []}, {"text": "The algorithm has learned a 200-dimensional distributed representation for 1 million different words in the raw corpus.", "labels": [], "entities": []}, {"text": "Classifier clf 2 is trained on the basic corpus with composed word2vec features as in Eq.", "labels": [], "entities": []}, {"text": "7. To evaluate the classification results, we calculate precision, recall, and F1 scores for each polarity, the macro average of these scores, and the overall accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9997472167015076}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9978833794593811}, {"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9998706579208374}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9988822340965271}]}, {"text": "shows the evaluation of 4 http://scikit-learn.org/dev/index.html  \u2022 RUN0 generates predictions from clf 0 . \u2022 RUN1 generates predictions from clf 1 . \u2022 RUN2 summarizes probabilistic predictions by where pclf i generates the probabilistic predictions over (negative, neutral, positive) for clf i , and arg max generates the class label with the largest accumulated probabilistic prediction.", "labels": [], "entities": []}, {"text": "\u2022 RUN3 combines probabilistic predictions by where clf 3 takes three probabilistic predictions as features and generates polarity predictions in Y . clf 3 has been trained on the labeled corpus, with parameters optimized over classification accuracy.", "labels": [], "entities": []}, {"text": "RUN3 achieves the highest accuracy since its classifier is optimized over classification accuracy on training data.", "labels": [], "entities": [{"text": "RUN3", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5797001719474792}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9989774227142334}]}, {"text": "RUN1 yields the highest macro recall and F1 scores, which suggests that our active learning has effectively selected salient samples for training the polarity classifier.", "labels": [], "entities": [{"text": "RUN1", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.860722541809082}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9830142259597778}, {"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9903119802474976}]}, {"text": "RUN2 yields the highest macro precision by summarizing the probabilistic predictions from three classifiers.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9654994606971741}]}, {"text": "Among the results from all participants for the restricted and unrestricted source based classifications, our submitted results in RUN0 and RUN3 have been ranked the 4th and the 2nd, respectively.", "labels": [], "entities": [{"text": "RUN0", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.8048962950706482}, {"text": "RUN3", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.7777273654937744}]}, {"text": "To further examine the problems in learning procedure we plot learning curves for each class label.", "labels": [], "entities": []}, {"text": "A learning curve represents the error rates of a classifier, trained with different sizes of data.", "labels": [], "entities": []}, {"text": "Learning curves of clf 0 and clf 1 suggest an overfitting problem since the models fit well on the training data but generalize poorly on the test data.", "labels": [], "entities": []}, {"text": "Compard to clf 0 , clf 1 is more generalizable with extra samples selected by active learning.", "labels": [], "entities": []}, {"text": "The learning curves of clf 2 on negative and positive labels suggest an under-fitting problem, which implies that the composed word2vec features have lost some important information for predicting these labels.", "labels": [], "entities": []}, {"text": "Improvement is possible to be achieved by increasing the dimension of word vectors in the word2vec algorithm.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Polarity classification results.", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7394523918628693}]}]}