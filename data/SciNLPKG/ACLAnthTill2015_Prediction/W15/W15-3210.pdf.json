{"title": [{"text": "Annotating Targets of Opinions in Arabic using Crowdsourcing", "labels": [], "entities": [{"text": "Annotating Targets of Opinions", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8276816010475159}, {"text": "Crowdsourcing", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.768010675907135}]}], "abstractContent": [{"text": "We present a method for annotating targets of opinions in Arabic in a two-stage process using the crowdsourcing tool Amazon Mechanical Turk.", "labels": [], "entities": []}, {"text": "The first stage consists of identifying candidate targets \"entities\" in a given text.", "labels": [], "entities": []}, {"text": "The second stage consists of identifying the opinion polarity (posi-tive, negative, or neutral) expressed about a specific entity.", "labels": [], "entities": []}, {"text": "We annotate a corpus of Arabic text using this method, selecting our data from online commentaries in different domains.", "labels": [], "entities": []}, {"text": "Despite the complexity of the task, we find high agreement.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important task in subjectivity analysis of text is the identification of targets -also often called topics or subjects -of opinionated text.", "labels": [], "entities": [{"text": "subjectivity analysis of text", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7726502567529678}]}, {"text": "Knowledge of the target is important for making sense of an opinion (e.g in 'The will of the people will prevail over the regime's brutality', the opinion is positive towards 'the people' and negative towards 'the regime').", "labels": [], "entities": []}, {"text": "An opinion system which can identify both targets and polarities of opinions, and which can summarize the opinions of writers towards different targets, will be more informative than one which only identifies the overall sentiment of the text.", "labels": [], "entities": []}, {"text": "This problem has started gaining interest in the product review domain (), news and social media (, and in general language and discourse.", "labels": [], "entities": []}, {"text": "Annotating targets of opinion is a difficult and expensive task, requiring definition of what constitutes a target, whether targets are linked to opinion expressions, and how the boundaries of target spans should be defined (e.g 'the people' vs. 'the will of the people' or 'the regime' vs. 'the regime's brutality'), a problem which annotators often disagree on (.", "labels": [], "entities": [{"text": "Annotating targets of opinion", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.900513082742691}]}, {"text": "Additionally, it is not always straightforward to attribute a target to a specific opinion phrase.", "labels": [], "entities": []}, {"text": "Consider for example the following statement: 'The Lebanese PM said he was convinced that there would be a consensus on the presidential election, because since the moment the US and Iran had reached an understanding in the region, things were starting to look positive.'", "labels": [], "entities": []}, {"text": "Which is the opinion expression that leads us to believe that the PM is optimistic about the target presidential election?", "labels": [], "entities": []}, {"text": "Is it 'convinced', 'consensus', 'reached an understanding', or 'look positive', or a combination of the above?", "labels": [], "entities": []}, {"text": "Such decisions are difficult for annotators to agree on; many studies have noted these challenges () which can make the task complex.", "labels": [], "entities": []}, {"text": "Compared to the amount of resources available for sentiment and subjectivity analysis, there is much less annotated data available for this more fine-grained type of analysis.", "labels": [], "entities": [{"text": "sentiment and subjectivity analysis", "start_pos": 50, "end_pos": 85, "type": "TASK", "confidence": 0.8857564181089401}]}, {"text": "Due to the difficulty of the task, most of the available datasets of fine-grained subjectivity have been annotated by trained annotators or expert linguists, making the process slower and more expensive.", "labels": [], "entities": []}, {"text": "In this work, we consider annotation of targets using a sequence of simple crowdsourced substeps.", "labels": [], "entities": []}, {"text": "We focus on Arabic, where subjectivity analysis is of growing interest, and where there are no publicly available resources for fine-grained opinion analysis.", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7348040044307709}, {"text": "fine-grained opinion analysis", "start_pos": 128, "end_pos": 157, "type": "TASK", "confidence": 0.6832819779713949}]}, {"text": "We assume targets of opinions to be noun phrase entities: people, places, things or ideas.", "labels": [], "entities": []}, {"text": "We develop a two-stage annotation process for annotating targets of opinions using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.9609402418136597}]}, {"text": "In the first, annotators list all important 'entities', and in the second, they choose the polarity expressed (positive, negative, or neutral) towards any given entity.", "labels": [], "entities": []}, {"text": "We select online data from multiple domains: politics, sports, and culture; and we provide anew publicly available resource for Arabic by annotating it for targets of opinions along with their polarities.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the quality of the data at different stages, obtaining majority agreement on sentiment for 91.8% of entities in a corpus of 1177 news article comments.", "labels": [], "entities": []}, {"text": "We also find that the morphology and grammar of Arabic lends itself to even more variations in identifying the boundaries of targets.", "labels": [], "entities": []}, {"text": "Section 2 describes related annotation work.", "labels": [], "entities": []}, {"text": "Section 3 describes the Amazon Mechanical Turk tasks design, the data selection, and the annotation process.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk tasks design", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.73807373046875}, {"text": "data selection", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.6975133866071701}]}, {"text": "In Section 4, we examine and analyze the annotations, evaluate the inter-annotator agreement, and provide detailed examples.", "labels": [], "entities": []}, {"text": "We conclude in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes results and analyses of the crowdsourced annotations.", "labels": [], "entities": []}, {"text": "We report the interannotator agreement at each of the two annotation stages, the distribution of the sentiment of collected targets by domain, and a manual analysis of our target entities.", "labels": [], "entities": []}, {"text": "We also provide examples of our final annotations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of article comments by do- main", "labels": [], "entities": []}, {"text": " Table 2: Agreement on entity-level sentiment an- notation", "labels": [], "entities": []}, {"text": " Table 3: Distribution of sentiment in final targets", "labels": [], "entities": [{"text": "Distribution of sentiment", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.788965662320455}]}]}