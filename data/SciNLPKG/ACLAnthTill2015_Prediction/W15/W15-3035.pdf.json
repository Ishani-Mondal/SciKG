{"title": [{"text": "Referential Translation Machines for Predicting Translation Quality and Related Statistics", "labels": [], "entities": [{"text": "Referential Translation Machines", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9366551240285238}, {"text": "Predicting Translation Quality and Related Statistics", "start_pos": 37, "end_pos": 90, "type": "TASK", "confidence": 0.8661664028962454}]}], "abstractContent": [{"text": "We use referential translation machines (RTMs) for predicting translation performance.", "labels": [], "entities": [{"text": "referential translation machines (RTMs)", "start_pos": 7, "end_pos": 46, "type": "TASK", "confidence": 0.7626165946324667}, {"text": "predicting translation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.8921734392642975}]}, {"text": "RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource.", "labels": [], "entities": []}, {"text": "We improve our RTM models with the ParFDA instance selection model (Bi\u00e7ici et al., 2015), with additional features for predicting the translation performance, and with improved learning models.", "labels": [], "entities": [{"text": "ParFDA instance selection", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.5295308232307434}]}, {"text": "We develop RTM models for each WMT15 QET (QET15) subtask and obtain improvements over QET14 results.", "labels": [], "entities": [{"text": "WMT15 QET (QET15) subtask", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.8874622285366058}]}, {"text": "RTMs achieve top performance in QET15 ranking 1st in document-and sentence-level prediction tasks and 2nd in word-level prediction task.", "labels": [], "entities": [{"text": "QET15", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8553531169891357}, {"text": "document-and sentence-level prediction tasks", "start_pos": 53, "end_pos": 97, "type": "TASK", "confidence": 0.6510146930813789}, {"text": "word-level prediction", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.75491663813591}]}, {"text": "1 Referential Translation Machine (RTM) Referential translation machines area computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to in-terpretants.", "labels": [], "entities": [{"text": "Referential Translation Machine (RTM) Referential translation", "start_pos": 2, "end_pos": 63, "type": "TASK", "confidence": 0.7627751007676125}]}, {"text": "RTMs achieve top performance in automatic , accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.", "labels": [], "entities": [{"text": "language independent prediction of machine translation", "start_pos": 58, "end_pos": 112, "type": "TASK", "confidence": 0.6069417695204417}]}, {"text": "Prediction of translation performance can help in estimating the effort required for correcting the translations during post-editing by human translators.", "labels": [], "entities": []}, {"text": "We present top results with Referential Translation Machines (Bi\u00e7ici, 2015; Bi\u00e7ici and Way, 2014) at quality estimation task (QET15) in WMT15 (Bojar et al., 2015).", "labels": [], "entities": [{"text": "Referential Translation Machines", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8525551756223043}, {"text": "WMT15", "start_pos": 136, "end_pos": 141, "type": "DATASET", "confidence": 0.7989277839660645}]}, {"text": "RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics.", "labels": [], "entities": []}, {"text": "RTMs use Machine Translation Performance Prediction (MTPP) System (Bi\u00e7ici et al., 2013; Bi\u00e7ici, 2015), which is a state-of-the-art performance predictor of translation even without using the translation by using only the source.", "labels": [], "entities": [{"text": "Machine Translation Performance Prediction (MTPP)", "start_pos": 9, "end_pos": 58, "type": "TASK", "confidence": 0.8143315783568791}]}, {"text": "We use ParFDA for selecting the interpretants (Bi\u00e7ici et al., 2015; Bi\u00e7ici and Yuret, 2015) and build an MTPP model.", "labels": [], "entities": []}, {"text": "MTPP derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.", "labels": [], "entities": [{"text": "data transformation", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7825658023357391}]}, {"text": "We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss, 2012).", "labels": [], "entities": []}, {"text": "Our encouraging results in QET provides a greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation.", "labels": [], "entities": []}, {"text": "RTMs are powerful enough to be applicable in different domains and tasks while achieving top performance.", "labels": [], "entities": [{"text": "RTMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7754231691360474}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of sentences in different tasks.", "labels": [], "entities": []}, {"text": " Table 2: Training performance of the top 2 individual RTM models prepared for different tasks.", "labels": [], "entities": []}, {"text": " Table 3: RTM-DCU Task 2 training results.", "labels": [], "entities": [{"text": "RTM-DCU Task 2 training", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.5153000801801682}]}, {"text": " Table 4. Rank lists the overall ranking in the  task out of about 9 submissions. We obtain the  rankings by sorting according to the predicted", "labels": [], "entities": []}, {"text": " Table 4: Test performance of the top 2 individual RTM models prepared for different tasks.", "labels": [], "entities": []}, {"text": " Table 5: RTM-DCU Task 2 results on the test set.  wF 1 is the average weighted F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9595264196395874}]}, {"text": " Table 6: Test performance of the top individual RTM results when predicting HTER or METEOR also  including results from QET14 (", "labels": [], "entities": [{"text": "predicting HTER", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.6906470358371735}, {"text": "METEOR", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.8491061925888062}, {"text": "QET14", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.928439736366272}]}]}