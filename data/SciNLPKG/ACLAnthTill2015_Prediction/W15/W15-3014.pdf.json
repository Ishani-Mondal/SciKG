{"title": [{"text": "Montreal Neural Machine Translation Systems for WMT'15", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7724985877672831}, {"text": "WMT'15", "start_pos": 48, "end_pos": 54, "type": "TASK", "confidence": 0.7754294872283936}]}], "abstractContent": [{"text": "Neural machine translation (NMT) systems have recently achieved results comparable to the state of the art on a few translation tasks, including English\u2192French and English\u2192German.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8112574319044749}]}, {"text": "The main purpose of the Montreal Institute for Learning Algorithms (MILA) submission to WMT'15 is to evaluate this new approach on a greater variety of language pairs.", "labels": [], "entities": [{"text": "Montreal Institute for Learning Algorithms (MILA) submission to WMT'15", "start_pos": 24, "end_pos": 94, "type": "DATASET", "confidence": 0.5681999109008096}]}, {"text": "Furthermore, the human evaluation campaign may help us and the research community to better understand the behaviour of our systems.", "labels": [], "entities": []}, {"text": "We use the RNNsearch architecture, which adds an attention mechanism to the encoder-decoder.", "labels": [], "entities": []}, {"text": "We also leverage some of the recent developments in NMT, including the use of large vocabularies, unknown word replacement and, to a limited degree, the inclusion of monolingual language models.", "labels": [], "entities": [{"text": "NMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9013930559158325}, {"text": "unknown word replacement", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.6071183184782664}]}], "introductionContent": [{"text": "Neural machine translation (NMT) is a recently proposed approach for machine translation that relies only on neural networks.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7938362210988998}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.753535807132721}]}, {"text": "The NMT system is trained end-to-end to maximize the conditional probability of a correct translation given a source sentence.", "labels": [], "entities": []}, {"text": "Although NMT has only recently been introduced, its performance has been found to be comparable to the state-of-the-art statistical machine translation (SMT) systems on a number of translation tasks (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 120, "end_pos": 157, "type": "TASK", "confidence": 0.8110701938470205}]}, {"text": "The main purpose of our submission to WMT'15 is to test the NMT system on a greater equal contribution variety of language pairs.", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.8405328989028931}]}, {"text": "As such, we trained systems on Czech\u2194English, German\u2194English and Finnish\u2192English.", "labels": [], "entities": []}, {"text": "Furthermore, the human evaluation campaign of WMT'15 will help us better understand the quality of NMT systems which have mainly been evaluated using the automatic evaluation metric such as BLEU ().", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.9124839305877686}, {"text": "BLEU", "start_pos": 190, "end_pos": 194, "type": "METRIC", "confidence": 0.9980946183204651}]}, {"text": "Most NMT systems are based on the encoderdecoder architecture (.", "labels": [], "entities": []}, {"text": "The source sentence is first read by the encoder, which compresses it into a real-valued vector.", "labels": [], "entities": []}, {"text": "From this vector representation the decoder may then generate a translation word-by-word.", "labels": [], "entities": []}, {"text": "One limitation of this approach is that a source sentence of any length must be encoded into a fixedlength vector.", "labels": [], "entities": []}, {"text": "To address this issue, our systems for WMT'15 use the RNNsearch architecture from (.", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.8093740344047546}]}, {"text": "In this case, the encoder assigns a context-dependent vector, or annotation, to every source word.", "labels": [], "entities": []}, {"text": "The decoder then selectively combines the most relevant annotations to generate each target word.", "labels": [], "entities": []}, {"text": "NMT systems often use a limited vocabulary of approximately 30, 000 to 80, 000 target words, which leads them to generate many outof-vocabulary tokens (UNK).", "labels": [], "entities": []}, {"text": "This may easily lead to the degraded quality of the translations.", "labels": [], "entities": []}, {"text": "To sidestep this problem, we employ a variant of importance sampling to help increase the target vocabulary size (.", "labels": [], "entities": []}, {"text": "Even with a larger vocabulary, there will almost assuredly be words in the test set that were unseen during training.", "labels": [], "entities": []}, {"text": "As such, we replace generated out-ofvocaulbary tokens with the corresponding source words with a technique similar to those proposed by.", "labels": [], "entities": []}, {"text": "Most NMT systems rely only on parallel data, ignoring the wealth of information found in large monolingual corpora.", "labels": [], "entities": []}, {"text": "On Finnish\u2192English, we combine our systems with a recurrent neural net-work (RNN) language model by recently proposed deep fusion.", "labels": [], "entities": []}, {"text": "For the other language pairs, we tried reranking n-best lists with 5-gram language models).", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe the settings of our experiments.", "labels": [], "entities": []}, {"text": "Except for minor differences, all the settings were similar across all the considered language pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the official WMT'15 test sets for single models and primary ensemble submissions.  All our own systems are constrained. When ranking by BLEU, we only count one system from each  submitter. Human rankings include all primary and online systems, but exclude those used in the Cs\u2194En  tuning task.", "labels": [], "entities": [{"text": "WMT'15 test sets", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9160190423329672}, {"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9983125925064087}]}]}