{"title": [{"text": "Strategy-Based Technology for Estimating MT Quality", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.7843185067176819}]}], "abstractContent": [{"text": "This paper introduces our SAU-KERC system that achieved F1 score of 0.39 in the world-level quality estimation task in WMT2015.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9889867007732391}, {"text": "world-level quality estimation task", "start_pos": 80, "end_pos": 115, "type": "TASK", "confidence": 0.5831599533557892}, {"text": "WMT2015", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.8167437314987183}]}, {"text": "The goal is to assign each translated word a \"OK\" or \"BAD\" label indicating translation quality.", "labels": [], "entities": [{"text": "OK\" or \"BAD\" label", "start_pos": 46, "end_pos": 64, "type": "METRIC", "confidence": 0.8202381730079651}]}, {"text": "We adopt the sequence labeling model, conditional random fields (CRF), to predict the labels.", "labels": [], "entities": []}, {"text": "Since \"BAD\" labels are rare in the training and development sets, recognition rate of \"BAD\" is low.", "labels": [], "entities": [{"text": "BAD", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.7438361644744873}, {"text": "recognition rate of \"BAD", "start_pos": 66, "end_pos": 90, "type": "METRIC", "confidence": 0.7817489027976989}]}, {"text": "To solve this problem, we propose two strategies.", "labels": [], "entities": []}, {"text": "One is to replace \"OK\" label with sub-labels to balance label distribution.", "labels": [], "entities": []}, {"text": "The other is to reconstruct the training set to include more \"BAD\" words.", "labels": [], "entities": [{"text": "BAD", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.7489556670188904}]}], "introductionContent": [{"text": "QE task is proposed to estimate the quality of machine translation without relying on reference translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7355527877807617}]}, {"text": "It contains three levels --word, sentence, and document and our work focuses on the word-level task.", "labels": [], "entities": []}, {"text": "The word-level task was proposed in 2013 and was divided into binary classification and multi-class classification.", "labels": [], "entities": []}, {"text": "This year only binary classification was considered in WMT2015.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.6532182097434998}, {"text": "WMT2015", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.8763752579689026}]}, {"text": "OK/BAD: If a word need editing, then it is BAD.", "labels": [], "entities": [{"text": "BAD", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.996380627155304}, {"text": "BAD", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9896407723426819}]}, {"text": "It is OK, otherwise.", "labels": [], "entities": []}, {"text": "As a confidence estimation problem, methods aim to confidence estimation before 2013.", "labels": [], "entities": [{"text": "confidence estimation", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.6678804010152817}, {"text": "confidence estimation", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.750645250082016}]}, {"text": "A lot of researchers started to investigate confidence measures for machine translation for nearly a decade ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.806294858455658}]}, {"text": "Many different confidence measures are investigated in(.", "labels": [], "entities": []}, {"text": "They are based on source and target language models features, n-best list, word-lattices, translation tables, and soon.", "labels": [], "entities": []}, {"text": "The authors also present efficient ways of classifying words as \"correct\" or \"incorrect\" by using native Bayes, single-or multi-layer perceptron.", "labels": [], "entities": []}, {"text": "( combines several features and use neural network and na\u00ef ve Bayes learning algorithms to predict whether a word is ok or bad.", "labels": [], "entities": []}, {"text": "() combines syntax feature, vocabulary feature and word posterior probability feature, which are extracted based on LG parsing, and use the binary classifier based on Maximum Entropy Model to predict the label of each word in machine translation(ok or bad).", "labels": [], "entities": [{"text": "word posterior probability feature", "start_pos": 51, "end_pos": 85, "type": "METRIC", "confidence": 0.6716948971152306}]}, {"text": "Some good ideas are proposed in word-level QE task of WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.6188480854034424}]}, {"text": "() use both internal and external features into a conditional random fields(CRF) model to predict the label for each word in the MT hypothesis.) rely on a random forest classifier and 16 features to predict the label of a word.", "labels": [], "entities": [{"text": "MT hypothesis.", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.8641214072704315}]}, {"text": "() train two classifier models by using bidirectional long short-term memory recurrent neural networks and CRF to complete word level QE Task.", "labels": [], "entities": []}, {"text": "In WMT2015, the high ratio of OK labels in the training set and development set makes the task an unbalanced classification problem.", "labels": [], "entities": [{"text": "WMT2015", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.7656117677688599}]}, {"text": "Generally, it is hard to solve unbalanced classification problem effectively using common machine learning algorithms and features.", "labels": [], "entities": []}, {"text": "To balance the label distribution, we propose two strategies: refining OK label(ROL) and changing training set structure(CTS).", "labels": [], "entities": [{"text": "OK label(ROL)", "start_pos": 71, "end_pos": 84, "type": "METRIC", "confidence": 0.7897902131080627}]}, {"text": "We augment the CRF model with these two strategies to improve the performance.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives the selected features.", "labels": [], "entities": []}, {"text": "Section 3 introduces the learning algorithm and the strategies we used.", "labels": [], "entities": []}, {"text": "Section 4 shows the structure of experimental data.", "labels": [], "entities": []}, {"text": "Section 5 analyzes the exper-iment results.", "labels": [], "entities": []}, {"text": "The last part is our summary of this task.", "labels": [], "entities": [{"text": "summary", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9405723214149475}]}], "datasetContent": [{"text": "There are four comparative experiments to prove the validity of the strategies proposed in this paper.", "labels": [], "entities": []}, {"text": "Experiment names are as follows: WY: do not change the structure of train set, not refine OK label.", "labels": [], "entities": [{"text": "WY", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9044276475906372}]}, {"text": "WF: do not change the structure of train set, refine OK with OK_B, OK_I, OK_E, OK.", "labels": [], "entities": []}, {"text": "ZY: change the structure of train set, do not refine OK label.", "labels": [], "entities": []}, {"text": "ZF: change the structure of train set, refine OK label with OK_B, OK_I, OK_E, OK.", "labels": [], "entities": []}, {"text": "In QE task of WMT2015, Label distribution disequilibrium phenomenon can lead to Paranoid problem, which impacts the performance of QE system seriously.", "labels": [], "entities": [{"text": "WMT2015", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.4772418439388275}, {"text": "Paranoid", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9274981021881104}]}, {"text": "As shown in, the strategies that refine OK label and change structure of train set can solve label disequilibrium problem to a certain degree.", "labels": [], "entities": []}, {"text": "The F_BAD is 34.28 when using the strategy of refining OK label alone, and the F_BAD is 32.69 when using the strategy of changing structure of training set.", "labels": [], "entities": [{"text": "F_BAD", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9434048334757487}, {"text": "F_BAD", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9676881233851115}]}, {"text": "The strategy that refines OK label is more effective than the one that change the structure of the training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus structural information", "labels": [], "entities": []}, {"text": " Table 2: Training data information after change", "labels": [], "entities": []}, {"text": " Table 4: The results on development corpus", "labels": [], "entities": []}, {"text": " Table 5: The results on test corpus", "labels": [], "entities": []}]}