{"title": [{"text": "Building Monolingual Word Alignment Corpus for the Greater China Region", "labels": [], "entities": [{"text": "Monolingual Word Alignment Corpus", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.6821102648973465}]}], "abstractContent": [{"text": "For a single semantic meaning, various linguistic expressions exist the Mainland China, Hong Kong and Taiwan variety of Mandarin Chinese, a.k.a., the Greater China Region (GCR).", "labels": [], "entities": [{"text": "Greater China Region (GCR)", "start_pos": 150, "end_pos": 176, "type": "DATASET", "confidence": 0.7360823750495911}]}, {"text": "Differing from the current bilingual word alignment corpus, in this paper, we have constructed two monolingual GCR corpora.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.6912933140993118}]}, {"text": "One is a 11,623-triple GCR word dictionary corpora which is automatically extracted and manually annotated from 30 million sentence pairs from Wikipedia.", "labels": [], "entities": []}, {"text": "The other one is a manually annotated 12,000 sentence pairs GCR word alignment corpus from Wikipedia and news website.", "labels": [], "entities": [{"text": "GCR word alignment", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.6303775509198507}]}, {"text": "In addition, we present a rule-based word alignment model which systematically explores the different word alignment case, e.g. 1-1, 1-n and m-n mapping, from Mainland China to Hong Kong or Taiwan.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7198596298694611}]}, {"text": "Evaluation results on our two different GCR word alignment corpora verify the effectiveness of our model, which significantly outper-forms the current Hidden Markov Model (HMM) based method, GIZA++ and their enhanced versions.", "labels": [], "entities": [{"text": "GCR word alignment", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.6283785303433737}]}], "introductionContent": [{"text": "There are different expressions fora single concept among the Mainland China, Hong Kong and Taiwan variety of Mandarin Chinese.", "labels": [], "entities": []}, {"text": "For example, \" \u4fe1\u606f/xin xi/information\" and \" \u5206\u8bcd/fen ci/word segmentation\" are the valid expressions in Mainland China, while \"\u8d44\u8baf/zi xun/information\", and \"\u65ad\u8bcd/duan ci/word segmentation\" are the corresponding expressions in Chinese Hong Kong and Taiwan, respectively.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7054629921913147}, {"text": "word segmentation", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.757029265165329}]}, {"text": "Although these expressions are different, they have the same semantic meanings.", "labels": [], "entities": []}, {"text": "Generally, the automatic word alignment task is to find word-level translation correspondences in the parallel text or sentences.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7103319019079208}, {"text": "word-level translation correspondences in the parallel text or sentences", "start_pos": 56, "end_pos": 128, "type": "TASK", "confidence": 0.7761594222651588}]}, {"text": "In specific, given a source sentence e consisting of words e1, e2,\u2026, eland a target sentence f consisting of words f1, f2,\u2026, fm, one needs to infer an alignment a, a sequence of indices a1, a2,\u2026, am corresponding to source words eai or a null word.", "labels": [], "entities": []}, {"text": "Automatic word alignment plays a critical role in statistical machine translation.", "labels": [], "entities": [{"text": "Automatic word alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5826812585194906}, {"text": "statistical machine translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.7407814959685007}]}, {"text": "Basically, the source sentence and the target sentence are usually written in different languages in the conventional word alignment corpora.", "labels": [], "entities": []}, {"text": "Therefore, most current word alignment models are designed for bilingual word alignment corpus, such as Chinese-English (), Japanese-English () and French-English (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7565959692001343}, {"text": "word alignment corpus", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7722859382629395}]}, {"text": "However, little work focuses on the word alignment only in one language but with different script, e.g. Mandarin with simplified and traditional scripts, or different Mandarin dialects.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7393635809421539}]}, {"text": "Motivated by the above observation, we have constructed two GCR corpora in this work.", "labels": [], "entities": []}, {"text": "One is a 11,623-triple GCR word dictionary corpus which is automatically extracted and manually annotated from 30 million sentence pairs from Wikipedia.", "labels": [], "entities": []}, {"text": "The other one is a manually annotated 12,000 sentence pairs GCR word alignment corpora obtained from Wikipedia and news website, respectively.", "labels": [], "entities": []}, {"text": "Furthermore, we present a rulebased word alignment model which systematically explores the different word alignment case, e.g. 1-1, 1-n, and m-n mapping, from Chinese Mainland to Hong Kong or Taiwan.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7013636231422424}]}, {"text": "Evaluation results on our GCR word alignment corpora verify the effectiveness of our model, which significantly outperforms the current HMM based method, GIZA++ and their enhanced versions.", "labels": [], "entities": [{"text": "GCR word alignment", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.6207243104775747}]}, {"text": "Actually, our corpora maybe used as a linguistic resources to test whether automatic mining of Mandarin words across different regions.", "labels": [], "entities": []}, {"text": "Or, it maybe used as a resource to transliterate between simplified and traditional variant of Mandarin, like a tool offered by ICU (International Components for Unicode) . The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 overviews the related work.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the annotation framework and scheme.", "labels": [], "entities": []}, {"text": "Section 4 illustrates the annotation and statistics of the GCR triples (word dictionary) corpus.", "labels": [], "entities": [{"text": "GCR triples (word dictionary) corpus", "start_pos": 59, "end_pos": 95, "type": "DATASET", "confidence": 0.5875587293079921}]}, {"text": "Section 5 presents the annotation of our GCR word alignment corpus, along with a rule-based word alignment model.", "labels": [], "entities": [{"text": "GCR word alignment", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.633804033199946}, {"text": "word alignment", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.6986130177974701}]}, {"text": "In Section 6, we evaluate our model and the current representative word alignment models on the two corpora, and we conclude this work in Section 7 and present future directions.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7038378715515137}]}], "datasetContent": [{"text": "In this section, we present the experiment settings including the benchmark datasets and baseline systems, and the experiment results for the different word segmentation pairs and the all sentence pairs accordingly.", "labels": [], "entities": []}, {"text": "https://code.google.com/p/berkeleyaligner/  Dataset.", "labels": [], "entities": []}, {"text": "Currently, we take the proposed two different GCR word alignment corpora as our benchmark datasets.", "labels": [], "entities": [{"text": "GCR word alignment", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6171647111574808}]}, {"text": "We choose several baseline methods.", "labels": [], "entities": []}, {"text": "They are the Berkeley aligner utility 9 with HMM (), SYN_HMM: Alignment performance for the different mapping case (1-1 mapping accounts for 71.87%,1-n mapping accounts for 25.55%, m-n mapping accounts for 2.58%) for Wikipedia corpora between Chinese Mainland and Hong Kong, and \"-\" stands for 0.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.956421434879303}]}, {"text": "GIZA++ (Och and Ney, 2003) and Moses () with union, intersecct, grow, growfinal, grow-diag, grow-diag-final, and grow-diagfinal-and parameters for harmonizing the GIZA++ 1-n and m-1 alignment to m-n alignment.", "labels": [], "entities": []}, {"text": "Meanwhile, we employ Stanford parser 10 to generate constituent parser tree for the SYN_HMM-based model.", "labels": [], "entities": []}, {"text": "Besides, we also verify the word alignment direction for the GIZA++ and PI-ALIGN.", "labels": [], "entities": [{"text": "word alignment direction", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7288649082183838}, {"text": "GIZA++", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.794025182723999}]}, {"text": "In this section, we report the experiment results for the different word segmentation pairs and the all sentence pairs accordingly.", "labels": [], "entities": []}, {"text": "shows the alignment performance for the different word segmentation pairs.", "labels": [], "entities": [{"text": "word segmentation pairs", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7834664583206177}]}, {"text": "In, \"\uf0e0\" refers to the direction from HK/TW to ML, while \"\uf0df\" stands for the direction from ML to HK/TW instead.", "labels": [], "entities": [{"text": "HK/TW", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8639012177785238}, {"text": "ML", "start_pos": 46, "end_pos": 48, "type": "DATASET", "confidence": 0.48027604818344116}, {"text": "HK/TW", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.8379116058349609}]}, {"text": "As it is shown, our rule-based system significantly outperforms the HMM-based, http://nlp.stanford.edu/software/lex-parser.shtml SYN_HMM-based, GIZA++ and PIALIGN systems under the two different corpus with p<0.01 using paired t-test for significance.", "labels": [], "entities": []}, {"text": "The best parameter for the alignment performance of Moses is grow, marking with Moses_grow in.", "labels": [], "entities": []}, {"text": "We don't list other parameter's performance of Moses for the limited space consideration.", "labels": [], "entities": []}, {"text": "As shown, our simple method is comparable with Moses_grow under wikipedia corpus.", "labels": [], "entities": [{"text": "Moses_grow under wikipedia corpus", "start_pos": 47, "end_pos": 80, "type": "DATASET", "confidence": 0.7266552746295929}]}, {"text": "But our system also significantly outperforms the Moses_grow system under News corpus.", "labels": [], "entities": [{"text": "News corpus", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9388129115104675}]}, {"text": "The first reason is that the strictness characteristic of the News website, while the looseness property of the Wikipedia.", "labels": [], "entities": [{"text": "News website", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.9202718138694763}]}, {"text": "The second reason is that the Moses_grow adopts many heuristic rules to improve its recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9973111152648926}]}, {"text": "This will be one of our future works.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The statistics of the initial and final GCR tri- ples", "labels": [], "entities": []}, {"text": " Table 2: Some GCR word dictionary examples", "labels": [], "entities": []}, {"text": " Table 3: The difference between Chinese Mainland,  Hong Kong and Taiwan", "labels": [], "entities": []}, {"text": " Table 6: Precision, Recall and F1 scores of the different word segmentation pairs", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9981992840766907}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9939016103744507}, {"text": "F1 scores", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9711310565471649}]}, {"text": " Table 7: Precision, Recall and F1 scores of the all sentence pairs", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999116837978363}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9954432249069214}, {"text": "F1 scores", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9716866910457611}]}, {"text": " Table 8: Alignment performance for the different mapping case (1-1 mapping accounts for  71.87%,1-n mapping accounts for 25.55%, m-n mapping accounts for 2.58%) for Wikipedia  corpora between Chinese Mainland and Hong Kong, and \"-\" stands for 0.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.843277633190155}]}]}