{"title": [{"text": "Crowdsourced Word Sense Annotations and Difficult Words and Examples", "labels": [], "entities": [{"text": "Crowdsourced Word Sense Annotations", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.589310958981514}]}], "abstractContent": [{"text": "Word Sense Disambiguation has been stuck for many years.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8357766071955363}]}, {"text": "The recent availability of crowd-sourced data with large numbers of sense annotations per example facilitates the exploration of new perspectives.", "labels": [], "entities": []}, {"text": "Previous work has shown that words with uniform sense distribution have lower accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9972310662269592}]}, {"text": "In this paper we show that the agreement between annotators has a stronger correlation with performance, and that it can also be used to detect problematic examples.", "labels": [], "entities": []}, {"text": "In particular, we show that, for many words, such examples are not useful for training, and that they are more difficult to disam-biguate.", "labels": [], "entities": []}, {"text": "The manual analysis seems to indicate that most of the problematic examples correspond to occurrences of subtle sense distinctions where the context is not enough to discern which is the sense that should be applied.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense ambiguity is a major obstacle for accurate information extraction, summarization and machine translation, but there is still alack of high performance Word Sense Disambiguation systems (WSD).", "labels": [], "entities": [{"text": "Word sense ambiguity", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6389054358005524}, {"text": "accurate information extraction", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6103182733058929}, {"text": "summarization", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.987736165523529}, {"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7824970185756683}, {"text": "Word Sense Disambiguation", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.5431641042232513}]}, {"text": "The current state-of-the-art is around the high 60s accuracy for words in full documents, and high 70s for words with larger number of training examples (lexical sample).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9941372871398926}]}, {"text": "The lack of large, high-quality, annotated corpora and the fine-grainedness of the sense inventories (typically WordNet) are thought to be the main reasons for the poor performance ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9512976408004761}]}, {"text": "The situation of WSD is in stark contrast to the progresses made on Named-Entity Disambiguation, where performance over 80% accuracy is routinely reported.", "labels": [], "entities": [{"text": "WSD", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9429560899734497}, {"text": "Named-Entity Disambiguation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6672060042619705}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9984757304191589}]}, {"text": "In this paper we focus on the recent release of crowdsourced data with large numbers of sense annotations per example (, and try to shed some light in the factors that affect the performance of a supervised WSD system.", "labels": [], "entities": [{"text": "WSD", "start_pos": 207, "end_pos": 210, "type": "TASK", "confidence": 0.9151174426078796}]}, {"text": "In particular, we extend the analysis setup in, and show that the agreement between annotators has a strong correlation with the performance fora particular word, stronger than previously used factors like the number of senses of the word and the sense distribution for the word.", "labels": [], "entities": []}, {"text": "In addition, we show that crowdsourced data can be used to detect problematic examples.", "labels": [], "entities": []}, {"text": "In particular, our results indicate that, for many words, such examples are not useful for training, and that they are more difficult to disambiguate.", "labels": [], "entities": []}, {"text": "The last section shows some examples.", "labels": [], "entities": []}, {"text": "2 Previous work on factors affecting WSD performance WSD is a problem which differs from other natural language processing tasks in that each target word is a different classification problem, in contrast to, for instance, document classification, PoS tagging or parsing, where one needs to train one single classifier for all.", "labels": [], "entities": [{"text": "WSD performance WSD", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.6802570323149363}, {"text": "document classification", "start_pos": 223, "end_pos": 246, "type": "TASK", "confidence": 0.7593551576137543}, {"text": "PoS tagging or parsing", "start_pos": 248, "end_pos": 270, "type": "TASK", "confidence": 0.6639584377408028}]}, {"text": "Furthermore, it is already known that, given a fixed amount of training data, the performance of a supervised WSD algorithm varies from one word to another).", "labels": [], "entities": [{"text": "WSD algorithm", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.9018834233283997}]}, {"text": "Their analysis was based on the annotated examples fora handful of words, as released in the SensEval2 lexical sample task).", "labels": [], "entities": []}, {"text": "In particular they found that the performance of all systems decreased for words with higher number of senses (as opposed to words with few senses) and for those with more uniform distributions of senses (as opposed to words with skewed distributions of senses).", "labels": [], "entities": []}, {"text": "The distribution of senses was measured using the entropy of the probability distribution of the senses, normalized by the number of senses 1 H r (P ) = H(P )/log 2 (#senses), where H(P ) = \u2212 i\u2208senses p(i)log 2 p(i) (.", "labels": [], "entities": []}, {"text": "In this paper we quantify the correlation of those two factors with the performance of a WSD system, in order to compare their contribution.", "labels": [], "entities": [{"text": "WSD", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9615000486373901}]}, {"text": "In addition, we analyse anew factor, agreement between annotators, which can be used not only to know which words are more difficult, but also to characterize which examples are more difficult to disambiguate.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first work which quantifies the contribution of each of these factors towards the performance on WSD, and the only one which analyses example difficulty for WSD on empirical grounds.", "labels": [], "entities": [{"text": "WSD", "start_pos": 127, "end_pos": 130, "type": "TASK", "confidence": 0.8924569487571716}, {"text": "WSD", "start_pos": 187, "end_pos": 190, "type": "TASK", "confidence": 0.955146312713623}]}, {"text": "Our work is also related to), which showed that multiple crowdsourced annotations of the same item allow to improve the performance in PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 135, "end_pos": 146, "type": "TASK", "confidence": 0.8200134336948395}]}, {"text": "They incorporate the uncertainty of the annotators into the loss function of the model by measuring the inter-annotator agreement on a small sample of data, with good results.", "labels": [], "entities": []}, {"text": "Our work can be seen as preliminary evidence that such a method can be also applied to WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.979933500289917}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Regression analysis summary. The first three rows refer to the simplest models, where each  factor (annotation entropy, sense entropy and number of senses) is taken in isolation. The full model takes  all three factors with no interaction, and the full interaction includes all three factors and interactions.  R 2 and F-test indicate whether the model is fitted.", "labels": [], "entities": [{"text": "R 2", "start_pos": 321, "end_pos": 324, "type": "METRIC", "confidence": 0.9538451135158539}, {"text": "F-test", "start_pos": 329, "end_pos": 335, "type": "METRIC", "confidence": 0.9917820692062378}]}, {"text": " Table 2: Average results for the 30 words which get improvement using thresholds. Legend (cf. Section  5): no-filt for using full train and test data, train-filt for filtered train, test-filt for filtered test, and t&t-filt  for filtered train and test. See table 4 for statistics and results of individual words.", "labels": [], "entities": []}, {"text": " Table 4: Statistics for 45 words, with results across different evaluation conditions. Legend: ns for  number of senses, s-ent for sense entropy, a-ent for annotation entropy, thr for threshold (cf. Section 5).  The remaining columns report results on the following conditions (cf. Section 5): no-filt for using full  train and test data, train-filt for filtered train, test-filt for filtered test, and t&t-filt for filtered train and test.", "labels": [], "entities": []}]}