{"title": [{"text": "MT Tuning on RED: A Dependency-Based Evaluation Metric", "labels": [], "entities": [{"text": "MT Tuning", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9005937278270721}, {"text": "RED", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6181874871253967}]}], "abstractContent": [{"text": "In this paper, we describe our submission to WMT 2015 Tuning Task.", "labels": [], "entities": [{"text": "WMT 2015 Tuning Task", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.6244366094470024}]}, {"text": "We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8593199849128723}, {"text": "RED", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9564639925956726}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9990353584289551}, {"text": "METEOR", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9068431854248047}, {"text": "MERT", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9708476066589355}, {"text": "MIRA", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9910356998443604}]}, {"text": "Experiments are conducted using hierarchical phrase-based models on Czech-English and English-Czech tasks.", "labels": [], "entities": []}, {"text": "Our results show that MIRA performs better than MERT inmost cases.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9025008082389832}, {"text": "MERT", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9395512938499451}]}, {"text": "Using RED performs similarly to METEOR when tuning is performed using MIRA.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.8733118176460266}, {"text": "MIRA", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.7641633749008179}]}, {"text": "We submit our system tuned by MIRA towards RED to WMT 2015.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9020832180976868}, {"text": "RED to WMT 2015", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.699752002954483}]}, {"text": "In human evaluations, we achieve the 1st rank in all 7 systems on the English-Czech task and 6/9 on the Czech-English task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) is modeled as a weighted combination of several features.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8408696055412292}]}, {"text": "Tuning in SMT refers to learning a set of optimized weights, which minimize a defined translation error on a tuning set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9923661351203918}]}, {"text": "Typically, the error is measured by an automatic evaluation metric.", "labels": [], "entities": [{"text": "error", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9413461685180664}]}, {"text": "Thanks to its simplicity and language independence, BLEU () has served as the optimization objective since the 2000s.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9954816102981567}]}, {"text": "Although various lexical metrics, such as TER () and METEOR etc., have been proposed, none of them can truly replace BLEU in a phrase-based system.", "labels": [], "entities": [{"text": "TER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9965230226516724}, {"text": "METEOR", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.98463374376297}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9937872886657715}]}, {"text": "However, BLEU has no proficiency to deal with synonyms, paraphrases, and syntactic equivalent etc.).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9703054428100586}]}, {"text": "In addition, as a lexical and n-gram-based metric, BLEU maybe not suitable for optimization in a syntax-based model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9969910383224487}]}, {"text": "In this paper, we integrate a reference dependency-based MT evaluation metric, RED 1 (), into the hierarchical phrasebased model) in Moses (.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.8670287132263184}, {"text": "RED 1", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9434939920902252}]}, {"text": "In doing so, we explore whether a syntax-based translation system will perform better when it is optimized towards a syntaxbased evaluation criteria.", "labels": [], "entities": []}, {"text": "We compare RED with two other evaluation metrics, BLEU and ME-TEOR (Section 2).", "labels": [], "entities": [{"text": "RED", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9602094888687134}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9990739822387695}, {"text": "ME-TEOR", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9711493253707886}]}, {"text": "Two tuning algorithms are used (Section 3).", "labels": [], "entities": []}, {"text": "They are MERT, MIRA.", "labels": [], "entities": [{"text": "MERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9891282916069031}, {"text": "MIRA", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9895310401916504}]}, {"text": "Experiments are conducted on Czech-English and English-Czech translation (Section 4).", "labels": [], "entities": [{"text": "English-Czech translation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.675532341003418}]}], "datasetContent": [{"text": "An evaluation metric, which has a higher correlation with human judgments, maybe used to train a better system.", "labels": [], "entities": []}, {"text": "In this paper, we compare three metrics: BLEU, METEOR, and RED.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9993382096290588}, {"text": "METEOR", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.988013744354248}, {"text": "RED", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9916610717773438}]}, {"text": "We conduct experiments on Czech-English and English-Czech hierarchical phrase-based translation systems built using Moses with default configurations and default feature functions.", "labels": [], "entities": [{"text": "English-Czech hierarchical phrase-based translation", "start_pos": 44, "end_pos": 95, "type": "TASK", "confidence": 0.6086765006184578}]}, {"text": "We use WMT newstest2014 as our development data, while our test data consists of the concatenation of newstest2012 and newstest2013, which", "labels": [], "entities": [{"text": "WMT newstest2014", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.9533346593379974}]}], "tableCaptions": [{"text": " Table 1: Czech-English evaluation performance.  In each column, the intensity of shades indicates  the rank of values.", "labels": [], "entities": []}, {"text": " Table 2: English-Czech evaluation performance.  In each column, the intensity of shades indicates  the rank of values.", "labels": [], "entities": []}]}