{"title": [{"text": "Multi-source Cross-lingual Delexicalized Parser Transfer: Prague or Stanford?", "labels": [], "entities": [{"text": "Parser Transfer", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6542102694511414}]}], "abstractContent": [{"text": "We compare two annotation styles, Prague dependencies and Universal Stanford Dependencies , in their adequacy for parsing.", "labels": [], "entities": []}, {"text": "We specifically focus on comparing the adposition attachment style, used in these two formalisms, applied in multi-source cross-lingual delexicalized dependency parser transfer performed by parse tree combination.", "labels": [], "entities": [{"text": "cross-lingual delexicalized dependency parser transfer", "start_pos": 122, "end_pos": 176, "type": "TASK", "confidence": 0.6368394613265991}]}, {"text": "We show that in our setting , converting the adposition annotation to Stanford style in the Prague style training treebanks leads to promising results.", "labels": [], "entities": [{"text": "Prague style training treebanks", "start_pos": 92, "end_pos": 123, "type": "DATASET", "confidence": 0.6850742250680923}]}, {"text": "We find that best results can be obtained by parsing the target sentences with parsers trained on treebanks using both of the ad-position annotation styles in parallel, and combining all the resulting parse trees together after having converted them to the Stanford adposition style (+0.39% UAS over Prague style baseline).", "labels": [], "entities": [{"text": "UAS", "start_pos": 291, "end_pos": 294, "type": "METRIC", "confidence": 0.9448091387748718}]}, {"text": "The score improvements are considerably more significant when using a smaller set of diverse source treebanks (up to +2.24% UAS over the baseline).", "labels": [], "entities": [{"text": "UAS", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.8141757845878601}]}], "introductionContent": [{"text": "Dependency treebanks are annotated in various styles, with annotations based on Prague dependencies ( and (Universal) Stanford Dependencies) being the most popular and widespread.", "labels": [], "entities": [{"text": "Dependency treebanks", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.7777652442455292}]}, {"text": "In last years, several treebank collections with unified annotation have been published.", "labels": [], "entities": []}, {"text": "The largest of them, HamleDT, currently offers 30 treebanks, semiautomatically converted both to Prague dependen- We use the term annotation style to refer to the set of annotation conventions, as applied in annotating a given treebank, typically also defined by an annotation manual.", "labels": [], "entities": [{"text": "HamleDT", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9731927514076233}]}, {"text": ": Stanford style (above) and Prague style (below) analysis of the phrases \"bar of chocolate\" and \"chocolate bar\".", "labels": [], "entities": []}, {"text": "Note that in Stanford style, these phrases have a more similar structure, both featuring an nmod edge directly from \"bar\" to \"chocolate\".", "labels": [], "entities": []}, {"text": "This shows the principle of constructions with a similar meaning also having a similar dependency structure.", "labels": [], "entities": []}, {"text": "cies and Universal Stanford Dependencies (, and featuring morphological annotation using Interset.", "labels": [], "entities": []}, {"text": "Another collection, Google Universal Treebanks, contains 11 treebanks, generally annotated from scratch using aversion of Stanford Dependencies (  and Universal POS (.", "labels": [], "entities": [{"text": "Google Universal Treebanks", "start_pos": 20, "end_pos": 46, "type": "DATASET", "confidence": 0.9069058100382487}, {"text": "Stanford Dependencies", "start_pos": 122, "end_pos": 143, "type": "DATASET", "confidence": 0.9209610521793365}]}, {"text": "Recently, these efforts have joined to produce Universal Dependencies (UD), which currently contain 18 treebanks annotated with a newly defined annotation scheme based on Universal Stanford Dependencies, Universal POS tags and Interset.", "labels": [], "entities": []}, {"text": "UD are now becoming the de facto standard; however, we used the HamleDT collection for our experiments, as at the time of performing the experiments, HamleDT was much larger than UD, as well as more diverse in terms of language families represented.", "labels": [], "entities": [{"text": "UD", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7694483399391174}, {"text": "HamleDT collection", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.9493818581104279}]}], "datasetContent": [{"text": "We use the HamleDT 2.0 collection of 30 dependency treebanks, which had been semiautomatically harmonized to Prague dependencies and then Stanfordized into Universal Stanford Dependencies.", "labels": [], "entities": [{"text": "HamleDT 2.0 collection of 30 dependency treebanks", "start_pos": 11, "end_pos": 60, "type": "DATASET", "confidence": 0.8888975637299674}]}, {"text": "We list the treebanks and their sizes in.", "labels": [], "entities": []}, {"text": "More information about the treebanks contained in the dataset, as well as the dataset itself, can be obtained online.", "labels": [], "entities": []}, {"text": "In most experiments, we use the Prague style version of HamleDT, as the Stanford version performs much worse for parsing (see Section 3.1).", "labels": [], "entities": [{"text": "HamleDT", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.7112379670143127}, {"text": "parsing", "start_pos": 113, "end_pos": 120, "type": "TASK", "confidence": 0.9683462381362915}]}, {"text": "Instead of using the full Stanford version, we only focus on one of its prominent features -adposition attachment.", "labels": [], "entities": []}, {"text": "Thus, we alternate between Prague adposition attachment as head (denoted \"P\"), and, this shows a case where our conversion is imperfect, as we are unable to obtain the original structure after the conversion roundtrip.", "labels": [], "entities": []}, {"text": "Stanford adposition attachment as leaf node (denoted \"S\"), using simple conversion scripts.", "labels": [], "entities": []}, {"text": "\u2022 The conversion from P to S takes each adposition and attaches it as a dependent of its left-most non-adpositional child, together with all of its other non-adpositional children.", "labels": [], "entities": []}, {"text": "Thus, the adposition becomes a leaf node, unless it has adpositional dependent nodes (typically this signifies a compound adposition).", "labels": [], "entities": []}, {"text": "Coordinating conjunctions are passed through (recursively) -if the left-most non-adpositional child is a coordinating conjunction, then its dependent leftmost nonadpositional conjunct is used instead as the new head of the adposition (see).", "labels": [], "entities": []}, {"text": "\u2022 In the conversion from S to P, each adposition with a non-adpositional head is attached as a dependent of its head's head, and its original head is attached as its dependent (see 3).", "labels": [], "entities": []}, {"text": "The roundtrip of the conversion (UAS after converting from P to Sand back) is around 98% in total, and around 94% for adposition nodes alone.", "labels": [], "entities": [{"text": "UAS", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9965292811393738}]}, {"text": "We use the training sections of the treebanks for parser training and their testing sections for evaluation.", "labels": [], "entities": [{"text": "parser training", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.9627275466918945}]}, {"text": "We report the results using UAS (unlabelled attachment score).", "labels": [], "entities": [{"text": "UAS (unlabelled attachment score", "start_pos": 28, "end_pos": 60, "type": "METRIC", "confidence": 0.7424684226512909}]}], "tableCaptions": [{"text": " Table 1: List of HamleDT 2.0 treebanks.", "labels": [], "entities": [{"text": "HamleDT 2.0 treebanks", "start_pos": 18, "end_pos": 39, "type": "DATASET", "confidence": 0.9324064254760742}]}, {"text": " Table 2: Prague versus full Stanford annotation  style, UAS averaged over 30 target languages.", "labels": [], "entities": []}, {"text": " Table 3: Average UAS of supervised monolingual  parsers, both lexicalized and delexicalized.", "labels": [], "entities": [{"text": "UAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.6923570036888123}]}, {"text": " Table 4: Pairs of supervised parser setups.", "labels": [], "entities": []}, {"text": " Table 6: UAS of supervised lexicalized monolingual parsers, supervised delexicalized monolingual  parsers, and delexicalized transfer parsers.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6089999079704285}]}, {"text": " Table 8: Subsets of source treebanks, selected ac- cording to their frequency of adposition tokens.", "labels": [], "entities": []}, {"text": " Table 9: UAS of delexicalized parser transfer, av- eraged over 21 target languages, with the specified  subset treebanks as sources.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7095289826393127}, {"text": "parser transfer", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7610656321048737}]}, {"text": " Table 10: Pairs of delexicalized transfer setups us- ing specific source treeebank subsets.", "labels": [], "entities": []}]}