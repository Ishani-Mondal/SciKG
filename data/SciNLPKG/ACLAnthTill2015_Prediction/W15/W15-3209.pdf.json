{"title": [{"text": "A Pilot Study on Arabic Multi-Genre Corpus Diacritization Annotation", "labels": [], "entities": [{"text": "Arabic Multi-Genre Corpus Diacritization Annotation", "start_pos": 17, "end_pos": 68, "type": "TASK", "confidence": 0.6812182843685151}]}], "abstractContent": [{"text": "Arabic script writing is typically under-specified for short vowels and other markup, referred to as diacritics.", "labels": [], "entities": []}, {"text": "Apart from the lexical ambiguity found in words, similar to that exhibited in other languages, the lack of diacritics in written Arabic script adds another layer of ambiguity which is an artifact of the orthography.", "labels": [], "entities": []}, {"text": "Diacritiza-tion of written text has a significant impact on Arabic NLP applications.", "labels": [], "entities": []}, {"text": "In this paper, we present a pilot study on building a diacritized multi-genre corpus in Arabic.", "labels": [], "entities": []}, {"text": "We annotate a sample of non-diacritized words extracted from five text genres.", "labels": [], "entities": []}, {"text": "We explore different annotation strategies: Basic where we present only the bare undiacritized forms to the annota-tors, Intermediate (Basic forms+their POS tags), and Advanced (automatically dia-critized words).", "labels": [], "entities": [{"text": "Advanced", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9851158261299133}]}, {"text": "We present the impact of the annotation strategy on annotation quality.", "labels": [], "entities": []}, {"text": "Moreover, we study different diacriti-zation schemes in the process.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the characteristics of writing in Modern Standard Arabic (MSA) is that the commonly used orthography is mostly consonantal and does not provide full vocalization of the text.", "labels": [], "entities": [{"text": "writing in Modern Standard Arabic (MSA)", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.6617620252072811}]}, {"text": "It sometimes includes optional diacritical marks (henceforth, diacritics or vowels).", "labels": [], "entities": []}, {"text": "Diacritics are extremely useful for text readability and understanding.", "labels": [], "entities": []}, {"text": "Their absence in Arabic text adds another layer of lexical and morphological ambiguity.", "labels": [], "entities": []}, {"text": "Naturally occurring Arabic text has some percentage of these diacritics present depending on genre and domain.", "labels": [], "entities": []}, {"text": "For instance, religious text such as the Quran is fully diacritized to minimize chances of reciting it incorrectly.", "labels": [], "entities": []}, {"text": "So are children's educational texts.", "labels": [], "entities": []}, {"text": "Classical poetry tends to be diacritized as well.", "labels": [], "entities": []}, {"text": "However, news text and other genre are sparsely diacritized (e.g., around 1.5% of tokens in the United Nations Arabic corpus bear at least one diacritic ().", "labels": [], "entities": [{"text": "United Nations Arabic corpus", "start_pos": 96, "end_pos": 124, "type": "DATASET", "confidence": 0.8416315168142319}]}, {"text": "From an NLP perspective, the two universal problems for processing language that affect the performance of (usually statistically motivated) NLP tools and tasks are: (1) sparseness in the data where not enough instances of a word type are observed in a corpus, and (2) ambiguity where a word has multiple readings or interpretations.", "labels": [], "entities": []}, {"text": "Undiacritized surface forms of an Arabic word might have as many as 200 readings depending on the complexity of its morphology.", "labels": [], "entities": []}, {"text": "The lack of diacritics usually leads to considerable lexical ambiguity, as shown in the example in, a reason for which diacritization, aka vowel/diacritic restoration, has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR)) and statistical machine translation (SMT) (.", "labels": [], "entities": [{"text": "vowel/diacritic restoration", "start_pos": 139, "end_pos": 166, "type": "TASK", "confidence": 0.691919356584549}, {"text": "speech recognition (ASR))", "start_pos": 244, "end_pos": 269, "type": "TASK", "confidence": 0.8392415821552277}, {"text": "statistical machine translation (SMT)", "start_pos": 274, "end_pos": 311, "type": "TASK", "confidence": 0.8264143466949463}]}, {"text": "Hence, diacritization has been receiving increased attention in several Arabic NLP applications.", "labels": [], "entities": [{"text": "diacritization", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.9557679295539856}]}, {"text": "In general, building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem.", "labels": [], "entities": []}, {"text": "The currently available diacritized MSA corpora are generally limited to the newswire genres (as distributed by the LDC) or religion related texts such as the Quran or the Tashkeela corpus.", "labels": [], "entities": []}, {"text": "In this paper we present a pilot study where we annotate a sample of non-diacritized text extracted from five different text genres.", "labels": [], "entities": []}, {"text": "We explore different annotation strategies where we present the data to the annotator in three modes: Basic (only forms with no diacritics), Intermediate (Basic forms+POS tags), and Advanced (a list of forms that is automatically diacritized).", "labels": [], "entities": [{"text": "Advanced", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9945383667945862}]}, {"text": "We show the impact of the annotation strategy on the annota- show that full diacritization has a detrimental effect on SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9913018941879272}]}, {"text": "Hence, we are interested in eventually discovering an effective optimal level of diacritization.", "labels": [], "entities": []}, {"text": "Accordingly, we explore different levels of diacritization.", "labels": [], "entities": []}, {"text": "In this work, we limit our study to two diacritization schemes: FULL and MIN.", "labels": [], "entities": [{"text": "FULL", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.987285315990448}, {"text": "MIN", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.8131025433540344}]}, {"text": "For FULL, all diacritics are explicitly specified for every word.", "labels": [], "entities": [{"text": "FULL", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.5686779022216797}]}, {"text": "For MIN, we explore what a minimum and optimal number of diacritics that needs to be added in order to disambiguate a given word in context would be with the objective of making a sentence easily readable and unambiguous for any NLP application.", "labels": [], "entities": [{"text": "MIN", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9569827914237976}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: IAA in terms of WER", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9831762313842773}, {"text": "WER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9956595301628113}]}, {"text": " Table 6: Annotations accuracy for the different corpora per mode", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9366411566734314}]}, {"text": " Table 9: IAA WER scores against gold (Annot 4 )  for the MIN annotation scheme", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.977997362613678}, {"text": "WER", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9319150447845459}, {"text": "MIN", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.6250352263450623}]}]}