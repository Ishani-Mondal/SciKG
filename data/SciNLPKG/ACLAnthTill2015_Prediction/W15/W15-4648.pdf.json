{"title": [{"text": "Dialogue Management based on Multi-domain Corpus", "labels": [], "entities": [{"text": "Dialogue Management", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6930603235960007}]}], "abstractContent": [{"text": "Dialogue Management (DM) is a key issue in Spoken Dialogue System.", "labels": [], "entities": [{"text": "Dialogue Management (DM)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8627360701560974}, {"text": "Spoken Dialogue System", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.8800865411758423}]}, {"text": "Most of the existing data-driven DM schemes train the dialogue policy for some specific domain (or vertical domain), only using the dialogue corpus in this domain, which might suffer from the scarcity of dialogue corpus in some domains.", "labels": [], "entities": []}, {"text": "In this paper, we divide Dialogue Act (DA), as semantic representation of utterance, into DA type and slot parameter, where the former one is domain-independent and the latter one is domain-specific.", "labels": [], "entities": []}, {"text": "Firstly, based on multiple-domain dialogue corpus, the DA type prediction model is trained via Recurrent Neutral Networks (RNN).", "labels": [], "entities": [{"text": "DA type prediction", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8467115759849548}]}, {"text": "Moreover , DA type decision problem is mod-eled as a multi-order POMDP, and transformed to be a one-order MDP with continuous states, which is solved by Natural Actor Critic (NAC) algorithm and applicable for every domain.", "labels": [], "entities": [{"text": "DA type decision", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.8434914946556091}]}, {"text": "Furthermore, a slot parameter selection scheme is designed to generate a complete machine DA according to the features of specific domain , which yields the Multi-domain Corpus based Dialogue Management (MCD-M) scheme.", "labels": [], "entities": [{"text": "Multi-domain Corpus based Dialogue Management (MCD-M)", "start_pos": 157, "end_pos": 210, "type": "TASK", "confidence": 0.6799112297594547}]}, {"text": "Finally, extensive experimental results illustrate the performance improvement of the MCDM scheme, compared with the existing schemes.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the fast development of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP), a lot of Spoken Dialogue Systems (SDS) appear in our lives as information assistants.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.793246199687322}]}, {"text": "In SDS, Dialogue Management (DM), as one of the most important modules, not only determines the current machine reaction, but also controls the process of future dialogue.", "labels": [], "entities": [{"text": "Dialogue Management (DM)", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.8315488696098328}]}, {"text": "Thus, it is important to study DM in the establishment of SD-S.", "labels": [], "entities": [{"text": "DM", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9166508316993713}]}, {"text": "A lot of studies have been done on DM.) introduces anew POMDP-based framework for building spoken dialogue systems by using Bayesian updates of the dialogue state.) explores the possibility of using a set of approximate dynamic programming algorithms for policy optimization in SDS, which are combined to a method for learning a sparse representation of the value function.", "labels": [], "entities": [{"text": "policy optimization", "start_pos": 255, "end_pos": 274, "type": "TASK", "confidence": 0.740744024515152}]}, {"text": "analyzes current dialogue management in operating unmanned systems and develops a more advanced way of dialogue management and accompanying dialogue manager.) proposes a task ontology model for domain independent dialogue management, where the knowledge of a specific task is modeled in its task ontology which is independent from dialogue control.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7279572486877441}, {"text": "dialogue management", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7168655842542648}, {"text": "domain independent dialogue management", "start_pos": 194, "end_pos": 232, "type": "TASK", "confidence": 0.6610412821173668}]}, {"text": "proposes to apply the Kalman Temporal Differences (KTD) framework to the problem of dialogue strategy optimization so as to address all these issues in a comprehensive manner with a single framework.", "labels": [], "entities": [{"text": "Kalman Temporal Differences (KTD)", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.7244110802809397}, {"text": "dialogue strategy optimization", "start_pos": 84, "end_pos": 114, "type": "TASK", "confidence": 0.8124073545138041}]}, {"text": "proposes a scheme to utilize a socially-based reward function for Reinforcement Learning and uses it to fit the user adaptation issue for dialogue management.", "labels": [], "entities": [{"text": "Reinforcement Learning", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.9289824962615967}, {"text": "dialogue management", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.8306204080581665}]}, {"text": "describes an architecture fora dialogue management system to be employed in serious games for natural language interaction with non-player characters.", "labels": [], "entities": []}, {"text": "() provides an overview of the current state of the art in the development of POMDP-based spoken dialog systems.) presents a dialogue manager based on a log-linear probabilistic model and uses context-free grammars to impart hierarchical structure to variables and features.) uses single-agent Reinforcement Learning and multi-agent Reinforcement Learning for learning dialogue policies in a resource allocation negotiation scenario.", "labels": [], "entities": []}, {"text": "To sum up, most of these previous studies establish a specific-domain DM model, only using the dialogue corpus in this domain, which might suffer from scarcity of dialogue corpus in some vertical domains.", "labels": [], "entities": []}, {"text": "In this paper, we mainly consider the domains about slot-filling tasks such as hotel reservation, flight ticket booking, and shopping guidance.", "labels": [], "entities": [{"text": "hotel reservation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.6940659284591675}, {"text": "flight ticket booking", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7153408924738566}, {"text": "shopping guidance", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7659385502338409}]}, {"text": "We utilize dialogue act (DA) as semantic representation of utterance, and divide it into DA type and slot parameter, where the former one is domainindependent and the latter one is domain-specific.", "labels": [], "entities": []}, {"text": "Based on the dialogue corpus in multiple domains, we train the current machine DA type prediction model and the next user DA type prediction model via Recurrent Neutral Networks (RNN).", "labels": [], "entities": [{"text": "DA type prediction", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.5812333623568217}, {"text": "DA type prediction", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.615274598201116}]}, {"text": "With these two prediction models, the current machine DA type decision problem is modeled as a multi-order POMDP, and transformed to be a oneorder MDP with continuous states, which could be solved by Natural Actor Critic (NAC) algorithm.", "labels": [], "entities": []}, {"text": "This general DA type decision model could be applied to multiple domains.", "labels": [], "entities": [{"text": "DA type decision", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.8479903936386108}]}, {"text": "After calculating the machine DA type, we design a slot parameter selection scheme to generate a complete machine DA according to the features of vertical domain, which yields the Multi-domain Corpus based Dialogue Management (MCDM) scheme.", "labels": [], "entities": [{"text": "Multi-domain Corpus based Dialogue Management (MCDM)", "start_pos": 180, "end_pos": 232, "type": "TASK", "confidence": 0.7064969800412655}]}, {"text": "The advantages of this scheme are as follows.", "labels": [], "entities": []}, {"text": "\u2022 The MCDM scheme separates DA into DA type and slot parameter, where DA type is domain-independent.", "labels": [], "entities": []}, {"text": "It utilizes multidomain corpus to train a general DA type decision model that is applicable to every domain.", "labels": [], "entities": []}, {"text": "Namely, it extracts general dialogue knowledge from all the domains and put it into vertical domain DM model.", "labels": [], "entities": []}, {"text": "Even for some vertical domain with insufficient dialogue corpus, it could work well.", "labels": [], "entities": []}, {"text": "\u2022 The MCDM scheme encodes the dialogue historical information into history vector via RNN, and utilizes this history vector to estimate the distribution over possible current machine DA type and the distribution over possible next user DA.", "labels": [], "entities": []}, {"text": "Theoretically, the history vector contains the whole dialogue history, even the information of utterances in the first turn.", "labels": [], "entities": []}, {"text": "\u2022 The MCDM scheme models the machine DA type decision problem as a POMDP, which makes a decision in the limitation of unreliable ASR and NLP, and achieves a tradeoff between dialogue popularity (frequency of dialogue pattern) and slot-filling efficiency.", "labels": [], "entities": [{"text": "machine DA type decision", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.63752381503582}]}, {"text": "\u2022 The MCDM scheme designs a slot parameter selection method for generated machine DA type, according to the features of vertical domain.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, system model is introduced.", "labels": [], "entities": []}, {"text": "Section 3 establishes the current machine DA type prediction model and the next user DA type prediction model via RNN, and Section 4 models the DA type decision problem as a POMDP.", "labels": [], "entities": [{"text": "DA type prediction", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6394604047139486}]}, {"text": "Section 5 describes slot selection scheme for the given DA type and slot filling process.", "labels": [], "entities": [{"text": "slot selection", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.9252297282218933}, {"text": "slot filling", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.8158648014068604}]}, {"text": "Extensive experimental results are provided in Section 6 to illustrate the performance comparison, and Section 7 concludes this study.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare the performance of the proposed DM schemes and the existing DM scheme.", "labels": [], "entities": []}, {"text": "The DM scheme proposed in this paper is named as the RNN-MCDM scheme.", "labels": [], "entities": []}, {"text": "In the NGram-MCDM scheme, the DA type is estimated by N-gram model, and other parts are the same as the RNN-MCDM scheme.", "labels": [], "entities": [{"text": "NGram-MCDM", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9078799486160278}, {"text": "DA type", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.8470275104045868}]}, {"text": "In the existing scheme, the DM model in each domain is designed according to, using the dialogue corpus in its own domain.", "labels": [], "entities": []}, {"text": "Namely, fora given domain, the existing scheme does not utilize dialogue corpus in other domains.", "labels": [], "entities": []}, {"text": "The dialogue corpus for experiments covers five vertical domains, including hotel reservation (171 dialogues), shopping guidance (71 dialogues), banking service (64 dialogues), restaurant catering (46 dialogues), and taxi service (33 dialogues).", "labels": [], "entities": [{"text": "shopping guidance", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.7094153612852097}]}, {"text": "Several slots are defined for each vertical domain.", "labels": [], "entities": []}, {"text": "Besides, we also define 8 slots for shopping guidance, 9 slots for banking service, 6 slots for restaurant catering and 4 slots for taxi service.", "labels": [], "entities": [{"text": "shopping guidance", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7396490573883057}]}, {"text": "The details of these slots are not described due to the limitation of pages.", "labels": [], "entities": []}, {"text": "Besides, K dis set to be 10.", "labels": [], "entities": []}, {"text": "The dialogues in corpus are divided into two parts: 70% corpus for training the DM model and 30% corpus for user simulation to test the systems.", "labels": [], "entities": []}, {"text": "The simulated users are built via Bayesian Networks according to).", "labels": [], "entities": []}, {"text": "There are two performance indices for SDS evaluation: average turn and success rate.", "labels": [], "entities": [{"text": "SDS evaluation", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.9256654977798462}, {"text": "average turn", "start_pos": 54, "end_pos": 66, "type": "METRIC", "confidence": 0.7454752326011658}]}, {"text": "Average turn is defined as the average dialogue turn cost for task completion.", "labels": [], "entities": [{"text": "Average turn", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.910491943359375}]}, {"text": "Generally, in different vertical domains, the dialogue turns are directly proportional to the quantities of slots.", "labels": [], "entities": []}, {"text": "Thus, we define the normalized average turn as the ratio of average dialogue turn to slot number.", "labels": [], "entities": []}, {"text": "In addition, success rate is defined as the ratio of the dialogues that complete the task in the threshold turns to all the dialogues.", "labels": [], "entities": [{"text": "success rate", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.9911248087882996}]}, {"text": "Here, we define the threshold as double of slot number.", "labels": [], "entities": []}, {"text": "The reasons are as follows.", "labels": [], "entities": []}, {"text": "The existing scheme only uses the dialogue corpus in one domain.", "labels": [], "entities": []}, {"text": "Its trained DM model might not contain the abundant states if the size of dialogue corpus is small.", "labels": [], "entities": []}, {"text": "Thus, when being in a unknown state, it could not calculate the optimal action, which might be detrimental to the efficiency of slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 128, "end_pos": 140, "type": "TASK", "confidence": 0.8865021765232086}]}, {"text": "However, the MCDM schemes have stable and better performance of normalized average turn, which should be ascribed to the fact that the proposed schemes train the general DM model based on the dialogue corpus in all the domains, and leaning general dialogue knowledge to guide the dialogue evolution.", "labels": [], "entities": []}, {"text": "In addition, the N-Gram-MCDM scheme has lower normalized average turn than the existing scheme.", "labels": [], "entities": [{"text": "normalized average turn", "start_pos": 46, "end_pos": 69, "type": "METRIC", "confidence": 0.5593777199586233}]}, {"text": "Especially in the vertical domain with less dialogue corpus, performance improvement is more obvious.", "labels": [], "entities": []}, {"text": "The reason is that the N-Gram-MCDM scheme could learn the general dialogue knowledge from all the domains, especially in the domain with less corpus it could use apart of other domain knowledge to train its optimal dialogue policy.", "labels": [], "entities": []}, {"text": "Furthermore, the RNN-MCDM scheme has the lowest normalized average turn in every domain, because the RNN-MCDM scheme use RNN to learning history vector for DA prediction that takes the whole dialogue history into account.", "labels": [], "entities": [{"text": "DA prediction", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.9594874978065491}]}, {"text": "Namely, the RNN-MCDM scheme utilizes dialogue historical information more efficiently than the N-Gram-MCDM scheme, and RNN-based prediction model is smoother than NGram-based prediction model.", "labels": [], "entities": []}, {"text": "compares the success rate among the RNN-MCDM scheme, the N-Gram-MCDM scheme and the existing DM scheme.", "labels": [], "entities": []}, {"text": "From this picture, we can find out that the RNN-MCDM scheme has the highest success rate, and the success rate in the existing scheme is lower than the N-Gram-MCDM scheme, the gap become huge in the vertical domain with less dialogue corpus, which should be ascribed to the same reasons in.", "labels": [], "entities": []}], "tableCaptions": []}