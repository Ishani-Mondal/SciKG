{"title": [{"text": "Generating Image Descriptions with Gold Standard Visual Inputs: Motivation, Evaluation and Baselines", "labels": [], "entities": [{"text": "Generating Image Descriptions", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7508953213691711}, {"text": "Gold Standard Visual Inputs", "start_pos": 35, "end_pos": 62, "type": "METRIC", "confidence": 0.7989994436502457}, {"text": "Evaluation", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9047658443450928}]}], "abstractContent": [{"text": "In this paper, we present the task of generating image descriptions with gold standard visual detections as input, rather than directly from an image.", "labels": [], "entities": []}, {"text": "This allows the Natural Language Generation community to focus on the text generation process, rather than dealing with the noise and complications arising from the visual detection process.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6281170348326365}, {"text": "text generation", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.8154964447021484}]}, {"text": "We propose a fine-grained evaluation metric specifically for evaluating the content selection capabilities of image description generation systems.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.7477195461591085}]}, {"text": "To demonstrate the evaluation metric on the task, several baselines are presented using bounding box information and textual information as priors for content selection.", "labels": [], "entities": [{"text": "content selection", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.6679272651672363}]}, {"text": "The baselines are evaluated using the proposed metric, showing that the fine-grained metric is useful for evaluating the content selection phase of an image description generation system.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 151, "end_pos": 179, "type": "TASK", "confidence": 0.7435334523518881}]}], "introductionContent": [{"text": "There has been increased interest in the task of automatically generating full-sentence natural language image descriptions in recent years.", "labels": [], "entities": [{"text": "automatically generating full-sentence natural language image descriptions", "start_pos": 49, "end_pos": 123, "type": "TASK", "confidence": 0.6176771138395581}]}, {"text": "Compared to earlier work that annotates images with isolated concept labels (), such detailed annotations are much more informative and discriminating, and are important for improved text and image retrieval.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 192, "end_pos": 207, "type": "TASK", "confidence": 0.6776761710643768}]}, {"text": "They also pose an interesting and difficult challenge for natural language generation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6480039358139038}]}, {"text": "Previous work on generating image descriptions concentrates on solving the problem 'end-to-end', that is to generate a description given an image as input ().", "labels": [], "entities": []}, {"text": "Recent advances in large scale visual object recognition, especially in deep learning techniques, have reached a reasonably high level of accuracy in the last few years.", "labels": [], "entities": [{"text": "large scale visual object recognition", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.6804589569568634}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9989633560180664}]}, {"text": "For the task of classifying an image into one of 1,000 object categories (i.e. does the image contain an object of category X, yes or no?) on the ImageNet Large Scale Visual Recognition Challenge dataset), the state-ofthe-art currently performs at a 4.82% top-5 error rate ( comparable to the 5.1% error rate of a human annotator who trained himself to recognise the object categories).", "labels": [], "entities": [{"text": "ImageNet Large Scale Visual Recognition Challenge dataset", "start_pos": 146, "end_pos": 203, "type": "DATASET", "confidence": 0.6916656196117401}, {"text": "error rate", "start_pos": 262, "end_pos": 272, "type": "METRIC", "confidence": 0.941233217716217}]}, {"text": "For the more challenging object category detection task (i.e. draw a bounding box around each instance of objects of the given categories), the state-of-the-art achieved a mean average precision of 43.9%.", "labels": [], "entities": [{"text": "object category detection", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7442299723625183}, {"text": "precision", "start_pos": 185, "end_pos": 194, "type": "METRIC", "confidence": 0.9355911016464233}]}, {"text": "However, even at this level of performance, the errors from the visual output are still problematic when used as input to an image description generation system, especially when considering a large pool of candidate object categories to be mentioned in the description.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.7107657392819723}]}, {"text": "What if we were to assume that visual object recognisers have already achieved close to perfect detection rates, and that the object instances have already been identified and localised in an image?", "labels": [], "entities": []}, {"text": "This then raises many interesting questions with regards to generating a description for an image, including: (i) how do we decide which objects are to be mentioned?", "labels": [], "entities": []}, {"text": "(ii) how should these objects be ordered in the description?", "labels": [], "entities": []}, {"text": "(iii) how do we infer and describe activities or actions?", "labels": [], "entities": []}, {"text": "(iv) how to we describe spatial relations between objects?", "labels": [], "entities": []}, {"text": "(v) how and when do we describe the object attributes?", "labels": [], "entities": []}, {"text": "Many of these questions could be explored if we had a 'perfect' visual input to our image description generator.", "labels": [], "entities": []}, {"text": "To be able to begin to answer these questions, we proposed a pilot task, which has formed part of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task bench-: We present the task of generating textual descriptions given gold standard labelled bounding boxes as input.", "labels": [], "entities": [{"text": "Sentence Generation task", "start_pos": 161, "end_pos": 185, "type": "TASK", "confidence": 0.7941513458887736}]}, {"text": "This allows researchers to focus on the text generation aspects of the image description generation task, rather than dealing with the noise arising from visual detection.", "labels": [], "entities": [{"text": "text generation", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.745527982711792}, {"text": "image description generation task", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.7909244149923325}]}, {"text": "This task also allows us to evaluate specific phases of the conventional generation pipeline, providing insights into which specific phases of the generation pipeline contribute to the performance of an image description generation system.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 203, "end_pos": 231, "type": "TASK", "confidence": 0.7053647836049398}]}, {"text": "More specifically, we assume that perfectly labelled object instances and their localisations are available to image description generation systems, as done in and.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 111, "end_pos": 139, "type": "TASK", "confidence": 0.6873770356178284}]}, {"text": "Given this knowledge, we would like to evaluate how well image description generation systems perform through the various stages of Natural Language Generation): content determination (what objects to describe), microplanning (how to describe objects) and realisation (generating the complete sentence).", "labels": [], "entities": [{"text": "image description generation", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.748086134592692}, {"text": "Natural Language Generation", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.6495558718840281}, {"text": "content determination", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.7281503230333328}]}, {"text": "This pilot task is an attempt at encouraging fine-grained evaluation specifically for image descriptions, compared to general-purpose metrics like METEOR) that evaluates text at a global, coarsegrained level.", "labels": [], "entities": []}, {"text": "For our pilot, we concentrated on just one fine-grained metric: a content selection measure to evaluate how well a text generation system selects the correct object instances to be mentioned in the resulting image description.", "labels": [], "entities": []}, {"text": "A dataset has been introduced for this particular challenge.", "labels": [], "entities": []}, {"text": "This paper will not discuss in great detail how the dataset has been collected and annotated; we instead refer readers to  for more details about the challenge.", "labels": [], "entities": []}, {"text": "The main purpose of this paper, instead, is to: (i) present and discuss the task of generating image descriptions with a gold standard visual input; (ii) propose a fine-grained metric specifically for evaluating the content selection capabilities of image description generation systems; (iii) introduce several baselines for this task and evaluate the baselines using the proposed fine-grained metric.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 250, "end_pos": 278, "type": "TASK", "confidence": 0.6981033484141032}]}, {"text": "In section 2, we discuss the motivations for introducing the pilot task and the finegrained metric in the ImageCLEF 2015 challenge, positioning them in relation to existing work.", "labels": [], "entities": [{"text": "ImageCLEF 2015 challenge", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.6096091469128927}]}, {"text": "In section 3, we describe the task of generating image descriptions given gold standard visual inputs, along with a discussion on evaluating image description generation systems with regards to their content selection abilities.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 141, "end_pos": 169, "type": "TASK", "confidence": 0.7012195189793905}]}, {"text": "Section 4 presents several baselines for this task, while section 5 evaluates these baselines using the proposed content selection metric.", "labels": [], "entities": []}, {"text": "Finally, we discuss further challenges with the proposed task, and introduce possible fine-grained metrics to be considered in the future.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned above, we introduced as a benchmarking challenge the task of generating image descriptions for 450 test images given gold standard, labelled bounding box annotations as input).", "labels": [], "entities": []}, {"text": "The category labels were restricted to 251 WordNet synsets selected specifically for the challenge.", "labels": [], "entities": []}, {"text": "To enable evaluation with our proposed fine-grained metric, participants were also asked to annotate, within their generated descriptions, the bounding box ID to which a term in the description corresponds.", "labels": [], "entities": []}, {"text": "A development dataset of 500 images was provided with labelled bounding box annotations and correspondence annotations between textual terms and bounding boxes.", "labels": [], "entities": []}, {"text": "shows an example annotation of bounding boxes and the correspondences between bounding box instances and terms in the image descriptions.", "labels": [], "entities": []}, {"text": "Note that correspondence was annotated at word level (unigram) rather than at phrase level (higher-order n-grams) to avoid possible complications with multiple correspondences within the same phrase (woman in a white dress).", "labels": [], "entities": []}, {"text": "As a pilot, we propose a fine-grained metric to evaluate the content selection capabilities of an image description system.", "labels": [], "entities": []}, {"text": "This content selection metric is the F 1 score averaged across all 450 test images, where each F 1 score is computed from the precision and recall averaged overall gold standard descriptions for the image.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9877081910769144}, {"text": "F 1 score", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9812154968579611}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9972620010375977}, {"text": "recall averaged overall gold standard descriptions", "start_pos": 140, "end_pos": 190, "type": "METRIC", "confidence": 0.9070970416069031}]}, {"text": "Formally, let I = {I 1 , I 2 , ...I N } be the set of test images.", "labels": [], "entities": []}, {"text": "The advantage of the macro-averaging process in equations and is that it implicitly captures the relative importance of the bounding box instances based on how frequently they occur across the gold standard descriptions.", "labels": [], "entities": []}, {"text": "For example, in, both woman and car are referenced in all seven gold standard descriptions, while boot is mentioned four times and dress twice.", "labels": [], "entities": []}, {"text": "Thus, a generated description that references woman and car will naturally result in a higher score than one that references only woman and dress.", "labels": [], "entities": []}, {"text": "Note that for this metric, we are only concerned with evaluating the generation system's content selection capabilities, rather than its referring expression generation.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 137, "end_pos": 168, "type": "TASK", "confidence": 0.6356004675229391}]}, {"text": "As such, systems are free to generate any referring expression for each selected bounding box instance.", "labels": [], "entities": []}, {"text": "We consider the evaluation of referring expressions as a potentially separate fine-grained evaluation task to be introduced in the future.", "labels": [], "entities": []}, {"text": "In addition, we do not evaluate terms outside those that refer to bounding box instances, and as for the pilot task of the challenge use the global METEOR metric to cover evaluation of other aspects of image description generation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9094247221946716}, {"text": "image description generation", "start_pos": 202, "end_pos": 230, "type": "TASK", "confidence": 0.8008720874786377}]}, {"text": "The generation systems described in Section 4 were evaluated using the proposed content selection metric (Section 3.1).", "labels": [], "entities": []}, {"text": "We also compared the proposed systems to a baseline that selects bounding boxes at random, up to a pre-defined threshold k of the maximum allowed number of bounding boxes per image.", "labels": [], "entities": []}, {"text": "We explore different values of this threshold by varying k from 1 to 15.", "labels": [], "entities": []}, {"text": "We take min(k, N box ) for images with fewer thank bounding boxes, where N box is the total number of bounding boxes for the image.", "labels": [], "entities": []}, {"text": "As an upper bound to how well humans perform content selection, we evaluated the gold standard descriptions by evaluating one description against the other descriptions of the image and repeating the process for all descriptions.", "labels": [], "entities": [{"text": "content selection", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.6948083937168121}]}, {"text": "The upper bound is computed to be F = 0.74 \u00b1 0.12, with P = 0.77 \u00b1 0.11 and R = 0.77 \u00b1 0.11.", "labels": [], "entities": [{"text": "F", "start_pos": 34, "end_pos": 35, "type": "METRIC", "confidence": 0.998676598072052}, {"text": "R", "start_pos": 76, "end_pos": 77, "type": "METRIC", "confidence": 0.9713729619979858}]}, {"text": "shows the F -scores of our proposed generation systems.", "labels": [], "entities": [{"text": "F -scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9865835309028625}]}, {"text": "Firstly, we examine the effects of varying the threshold k on the number of instances to be selected.", "labels": [], "entities": []}, {"text": "The F -score peaks at k between 3 and 4 across all systems except the random baseline, and then drops or remains stagnant beyond that.", "labels": [], "entities": [{"text": "F -score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9953768650690714}]}, {"text": "gives an insight about this observation when the precision P and recall R are examined separately.", "labels": [], "entities": [{"text": "precision P", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.928697794675827}, {"text": "recall R", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9761125445365906}]}, {"text": "As expected, P decreases while R increases when k is increased.", "labels": [], "entities": [{"text": "R", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.9776071906089783}]}, {"text": "The two graphs intersect at about k between 3 to 4, suggesting that these values are an optimal tradeoff between precision and recall (the mean number of unique instances per description is 2.89 in the development dataset).", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9992351531982422}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9985281229019165}]}, {"text": "Comparing the baselines based on visual and textual cues, the F -score in suggests that baselines using textual cues perform better when k is small, and visual cues perform better with larger k's.", "labels": [], "entities": [{"text": "F -score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9917356371879578}]}, {"text": "However, gives a clearer picture, where the bigram-based system obtained the best precision regardless of k), while the systems based on bounding box cues relied on the increased recall when increasing k to obtain a high F -score).", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9980577826499939}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9987429976463318}, {"text": "F -score", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9913144508997599}]}, {"text": "Note that the bigrambased generation system is less sensitive to larger k's as the model itself contains an internal stopping criterion when no suitable concept is likely to follow a selected concept, resulting in a lower but stable recall rate compared to other systems, when k is increased.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 233, "end_pos": 244, "type": "METRIC", "confidence": 0.9852579534053802}]}, {"text": "shows some example sentences generated by our baseline systems, for k=3.", "labels": [], "entities": []}, {"text": "We can infer from the results that (i) using prior knowledge on the ordering of concepts (i.e. bigrams) is helpful for concept selection; (ii) frequency of concepts (i.e. unigrams) are helpful when there are only one or two instances to be described, possibly because the remaining objects are not mentioned as frequently as the main actors; (iii) visual cues are helpful for concept selection, although the precision is reduced ask increases.", "labels": [], "entities": [{"text": "concept selection", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7667576670646667}, {"text": "concept selection", "start_pos": 376, "end_pos": 393, "type": "TASK", "confidence": 0.8196945488452911}, {"text": "precision", "start_pos": 408, "end_pos": 417, "type": "METRIC", "confidence": 0.998223602771759}]}], "tableCaptions": [{"text": " Table 1: P , R and F scores (with standard devia- tions) of the content selection metric, as evaluated  on different baselines at varying levels of k (1 to  10).", "labels": [], "entities": [{"text": "F", "start_pos": 20, "end_pos": 21, "type": "METRIC", "confidence": 0.9753066897392273}]}]}