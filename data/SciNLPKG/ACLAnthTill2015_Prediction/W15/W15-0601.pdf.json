{"title": [{"text": "Candidate Evaluation Strategies for Improved Difficulty Prediction of Language Tests", "labels": [], "entities": [{"text": "Improved Difficulty Prediction of Language Tests", "start_pos": 36, "end_pos": 84, "type": "TASK", "confidence": 0.7499887595574061}]}], "abstractContent": [{"text": "Language proficiency tests area useful tool for evaluating learner progress, if the test difficulty fits the level of the learner.", "labels": [], "entities": [{"text": "Language proficiency", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6558826863765717}]}, {"text": "In this work, we describe a generalized framework for test difficulty prediction that is applicable to several languages and test types.", "labels": [], "entities": [{"text": "test difficulty prediction", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.5964430471261343}]}, {"text": "In addition, we develop two ranking strategies for candidate evaluation inspired by automatic solving methods based on language model probability and semantic relatedness.", "labels": [], "entities": []}, {"text": "These ranking strategies lead to significant improvements for the difficulty prediction of cloze tests.", "labels": [], "entities": []}], "introductionContent": [{"text": "In learning scenarios, evaluating the learner's proficiency is crucial to assess differences in learner groups and also individual learner progress.", "labels": [], "entities": []}, {"text": "This kind of evaluation is usually performed over the learner's results on certain tasks or tests.", "labels": [], "entities": []}, {"text": "For informative results, it is important that the test difficulty is suitable for the learner.", "labels": [], "entities": []}, {"text": "It needs to be challenging enough to avoid boredom and stagnation, but the learner should still be able to solve the task at least partially.", "labels": [], "entities": []}, {"text": "In this work, we focus on language proficiency tests and aim at predicting the difficulty for five different test datasets.", "labels": [], "entities": [{"text": "language proficiency tests", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.7895223100980123}]}, {"text": "Understanding the challenging elements of a task is an essential prerequisite for learner support.", "labels": [], "entities": []}, {"text": "In natural language processing, human performance is usually considered as the gold standard for automatic approaches.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6568436225255331}]}, {"text": "The models are tuned and adjusted to reach human-like results.", "labels": [], "entities": []}, {"text": "In learning settings, the human performance is flawed because of limited knowledge and lack of experience.", "labels": [], "entities": []}, {"text": "In this work, we thus apply a reverse approach: we exploit strategies from automatic solving to model human difficulties.", "labels": [], "entities": []}, {"text": "To enable the experiments, we retrieved datasets from various testing institutions and conducted a learner study to obtain error rates for an additional test type.", "labels": [], "entities": []}, {"text": "For a better understanding of the differences between test types, we first calculate the candidate space of potential answers and compare it to learner answers.", "labels": [], "entities": []}, {"text": "We assume that higher answer ambiguity leads to higher difficulty.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9779865145683289}]}, {"text": "As all datasets allow binary scoring (correct/wrong), the difficulty of an item is interpreted as the proportion of wrong answers, also referred to as the error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9755286276340485}]}, {"text": "We then build a generalized difficulty prediction framework based on an earlier approach we presented in which was limited to English and to one specific test type.", "labels": [], "entities": [{"text": "difficulty prediction", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.6602517813444138}]}, {"text": "We evaluate the prediction for different test types and languages and obtain remarkable results for French and German.", "labels": [], "entities": []}, {"text": "Many language tests are designed as multiple choice questions.", "labels": [], "entities": []}, {"text": "The generalized prediction approach lacks predictive power for this format because the evaluation strategy for the answer candidates is solely based on word frequency.", "labels": [], "entities": [{"text": "generalized prediction", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7121066153049469}]}, {"text": "We develop two strategies for more sophisticated candidate ranking that are inspired by automatic solving methods based on language models and semantic relatedness.", "labels": [], "entities": []}, {"text": "We show that the candidate ranking can successfully model human evaluation strategies and leads to improved difficulty prediction for cloze tests.", "labels": [], "entities": []}, {"text": "In order to establish common ground, we first introduce the concept of reduced redundancy testing and the most popular test types.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main challenge for solving a reduced redundancy test consists in identifying the most suitable candidate in the candidate space.", "labels": [], "entities": []}, {"text": "The context fitness of a candidate can be evaluated based on language model probabilities and on semantic relatedness between the candidate and the context.", "labels": [], "entities": []}, {"text": "LM-based approach A probabilistic language model (LM) calculates the probability of a phrase based on the frequencies of lower order n-grams extracted from training data.", "labels": [], "entities": []}, {"text": "This can be used to predict the fitness of a word for the sentential context., for example, evaluate the use of probabilistic language models to support auto-completion of sentences in writing editors.", "labels": [], "entities": []}, {"text": "In the completion scenario, only the left context is available, while the learner can also consider the right context in language tests.", "labels": [], "entities": []}, {"text": "thus model the problem of solving cloze tests by applying methods from lexical substitution to evaluate and rank the candidates.", "labels": [], "entities": []}, {"text": "The part to be substituted is a gap and the set of \"substitution candidates\" is already provided by the answer options.", "labels": [], "entities": []}, {"text": "Unfortunately, we cannot rely on static sentences for the open test formats as the context needs to be determined by solving the surrounding gaps.", "labels": [], "entities": []}, {"text": "For each gap, we take all candidates into account and generate all possible sentences resulting from the combinations with the candidates of subsequent gaps.", "labels": [], "entities": []}, {"text": "This can lead to strong dependencies between items, i.e. solving a subsequent item is facilitated, if the previous one has been solved correctly.", "labels": [], "entities": []}, {"text": "As a consequence, we need to evaluate a combinatorial search space that grows exponentially with the number of gaps in the sentence (see).", "labels": [], "entities": []}, {"text": "We thus use a pruning step after each gap that scores the generated sub-sentences using a language model and only keeps then best.", "labels": [], "entities": []}, {"text": "For the closed cloze test, the number of generated sentences is of course limited to the number of candidates because each sentence contains only one gap.", "labels": [], "entities": []}, {"text": "We use 5-gram language models that are trained on monolingual news corpora using berkeleylm with Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "8  trained their models explicitly on training data only from Sherlock Holmes novels.", "labels": [], "entities": []}, {"text": "In order to better simulate learner knowledge, we use rather small and controlled training data from the Leipzig collection () consisting of one million sentences for each language.", "labels": [], "entities": [{"text": "Leipzig collection", "start_pos": 105, "end_pos": 123, "type": "DATASET", "confidence": 0.9522062242031097}]}, {"text": "For solving the test, we then select the generated sentence with the highest log-probability in the language model and count how many gaps are solved correctly.", "labels": [], "entities": []}, {"text": "If several sentences obtain the same probability, we pick one at random.", "labels": [], "entities": []}, {"text": "We run this strategy ten times and average the results.", "labels": [], "entities": []}, {"text": "For comparison, we implement a baseline that always selects the most frequent candidate without considering the context.", "labels": [], "entities": []}, {"text": "Semantic relatedness approach Language models cannot capture relations between distant words in the sentence.", "labels": [], "entities": []}, {"text": "To account for this constraint,  include information from latent semantic analysis  Results The accuracy of the automatic solving strategies and the average human performance in shows that the LM-based solving strategy strongly outperforms the baseline and can also beat the average human solver for the open test formats.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9996024966239929}]}, {"text": "Even the large candidate space of the prefix deletion test can be disambiguated quite well.", "labels": [], "entities": []}, {"text": "For the cloze tests, the candidate ambiguity seems to be more challenging.", "labels": [], "entities": []}, {"text": "The LM-based candidate evaluation only performs slightly better than the baseline due to the fact that the distractor generation approach assured comparable context frequency of all candidates.", "labels": [], "entities": [{"text": "distractor generation", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.6439735144376755}]}, {"text": "The semantic relatedness approach works slightly better, but also fails to select the correct candidate inmost cases.", "labels": [], "entities": []}, {"text": "Not surprisingly, our results for the cloze tests are worse than those obtained with domain-specific corpora in previous work.", "labels": [], "entities": []}, {"text": "However, we are not interested in developing a perfect solving method, but aim at modelling the difficulty for the learner.", "labels": [], "entities": [{"text": "perfect solving", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.5454684197902679}]}, {"text": "A question is less likely to be solved if the context fitness of a distractor is rated higher than that of the solution.", "labels": [], "entities": []}, {"text": "The failures of the automatic solving might hence be indicative for the difficulty prediction for cloze tests.", "labels": [], "entities": [{"text": "automatic solving", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.6602700352668762}]}], "tableCaptions": [{"text": " Table 1: Vocabulary size and mean word length for dif- ferent languages", "labels": [], "entities": [{"text": "mean word length", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.7587924599647522}]}, {"text": " Table 2: Overview of test data", "labels": [], "entities": [{"text": "Overview", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8811181783676147}]}, {"text": " Table 5: Improved prediction results for cloze tests. Sig- nificant differences to the result with the standard features  are indicated with * (p<0.01).", "labels": [], "entities": [{"text": "Sig", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9697684049606323}]}]}