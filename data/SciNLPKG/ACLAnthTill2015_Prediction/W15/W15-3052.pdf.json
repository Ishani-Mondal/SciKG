{"title": [{"text": "LeBLEU: N-gram-based Translation Evaluation Score for Morphologically Complex Languages", "labels": [], "entities": [{"text": "LeBLEU", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5214385390281677}, {"text": "Translation Evaluation Score", "start_pos": 21, "end_pos": 49, "type": "METRIC", "confidence": 0.756791353225708}]}], "abstractContent": [{"text": "This paper describes the LeBLEU evaluation score for machine translation, submitted to WMT15 Metrics Shared Task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8278757035732269}, {"text": "WMT15 Metrics Shared Task", "start_pos": 87, "end_pos": 112, "type": "DATASET", "confidence": 0.7602091282606125}]}, {"text": "LeBLEU extends the popular BLEU score to consider fuzzy matches between word n-grams.", "labels": [], "entities": [{"text": "LeBLEU", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9034774899482727}, {"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9806909263134003}]}, {"text": "While there are several variants of BLEU that allow to non-exact matches between words either by character-based distance measures or morphological pre-processing, none of them use fuzzy comparison between longer chunks of text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9854228496551514}]}, {"text": "The results on WMT data sets show that fuzzy n-gram matching improves correlations to human evaluation especially for highly compounding languages.", "labels": [], "entities": [{"text": "WMT data sets", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.8305079142252604}, {"text": "fuzzy n-gram matching", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.6605058113733927}]}], "introductionContent": [{"text": "The quality of machine translation has improved to the level that the translation hypotheses are useful starting points for human translators for almost any language pair.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7531144618988037}]}, {"text": "In the post-editing task, the ultimate way to evaluate the machine translation quality is to measure the editing time.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6406264156103134}]}, {"text": "Editing times are naturally related to the number and types of the edits-and thus the number of keystrokesthe post-editor needs to get the final translation from the hypothesis.", "labels": [], "entities": []}, {"text": "If we compare the raw translation hypothesis and its post-edited version, an appropriate edit distance measure should correlate to the edit time.", "labels": [], "entities": []}, {"text": "However, implementing such a measure is far from trivial.", "labels": [], "entities": []}, {"text": "In automatic speech recognition, common evaluation measures are Word Error Rate (WER) and Letter Error Rate (LER) that are based on the Levenshtein edit distance.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6525947550932566}, {"text": "Word Error Rate (WER)", "start_pos": 64, "end_pos": 85, "type": "METRIC", "confidence": 0.8887808620929718}, {"text": "Letter Error Rate (LER)", "start_pos": 90, "end_pos": 113, "type": "METRIC", "confidence": 0.9282838006814321}]}, {"text": "LER is more reasonable measure than WER for morphologically complex languages, in which the same word can occur in many inflected and derived forms (.", "labels": [], "entities": [{"text": "LER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9880967140197754}, {"text": "WER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9355450868606567}]}, {"text": "However, both give too high penalty for the variations in word ordering, which are frequent in translations.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.7283593267202377}]}, {"text": "Even in English, there are often at least two grammatically correct orders fora complex sentence.", "labels": [], "entities": []}, {"text": "For languages in which the grammatical roles are marked by morphology and not the word order, there maybe many more options.", "labels": [], "entities": []}, {"text": "An edit distance measure suitable for machine translation would require move operations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7923867702484131}]}, {"text": "However, such measures are computationally very expensive: finding the minimum edit distance with moves is NP-hard (), making it cumbersome for evaluation and unsuitable for automatic tuning of the translation models.", "labels": [], "entities": []}, {"text": "Possible solutions include limiting the move operations or searching only for an approximate solution.", "labels": [], "entities": []}, {"text": "For example, Translation Edit Rate (TER) by uses a shift operation that moves a contiguous sequence of words to another location, as well as a greedy search algorithm to find the minimum distance.", "labels": [], "entities": [{"text": "Translation Edit Rate (TER", "start_pos": 13, "end_pos": 39, "type": "METRIC", "confidence": 0.8244398355484008}]}, {"text": "Stanford Probabilistic Edit Distance Evaluation (SPEDE) by applies a probabilistic push-down automaton that captures nonnested, limited distance word swapping.", "labels": [], "entities": [{"text": "Probabilistic Edit Distance Evaluation (SPEDE", "start_pos": 9, "end_pos": 54, "type": "TASK", "confidence": 0.6030794779459635}, {"text": "limited distance word swapping", "start_pos": 128, "end_pos": 158, "type": "TASK", "confidence": 0.7548055350780487}]}, {"text": "A different approach to avoid the requirement of exactly same word order in the hypothesis and reference translations is to concentrate on comparing only small parts of the full texts.", "labels": [], "entities": []}, {"text": "For example, the popular BLEU metric by considers only local ordering of words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9946715235710144}]}, {"text": "To be precise, it calculates the geometric mean precision of the n-grams of length between one and four.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.5255563259124756}]}, {"text": "As high precision is easy to obtain by providing a very short hypothesis translation, hypotheses that are shorter than the reference are penalized by a brevity penalty.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9917743802070618}]}, {"text": "BLEU, TER and many other word-based methods assume that a single word (or n-gram) is either corrector incorrect, nothing in between.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9776917695999146}, {"text": "TER", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9883497953414917}]}, {"text": "This is problematic for inflected or derived words (e.g. \"translate\" and \"translated\" are considered two different words) as well as compound words (e.g. \"salt-and-pepper\" vs. \"salt and pepper\").", "labels": [], "entities": []}, {"text": "This is a minor issue for English, but it makes the evaluation unreliable for many other languages.", "labels": [], "entities": []}, {"text": "For example, in English-German translation, producing \"Arbeits Geberverband\" from \"employers' organization\" would give no hits if the reference had the compound \"Arbeitgeberverband\".", "labels": [], "entities": []}, {"text": "A common approach to the problem of inflected word forms-as well as to the simpler issues of uppercase letters and punctuation characters-is preprocessing.", "labels": [], "entities": []}, {"text": "For example, METEOR (Banerjee and) uses a stemmer.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9183983206748962}]}, {"text": "Popovi\u00b4cPopovi\u00b4c (2011) applies and combines BLEU-style scores based on part-of-speech (POS) tags as well as morphemes induced by the unsupervised method by.", "labels": [], "entities": [{"text": "BLEU-style", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9965577721595764}]}, {"text": "Also the AMBER score by combines many BLEU variants, and in some variants, the words are heuristically segmented.", "labels": [], "entities": [{"text": "AMBER score", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9556832909584045}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9973984956741333}]}, {"text": "Our approach is to extend the BLEU metric to work better on morphologically complex languages without using any language-specific resources.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9920182824134827}]}, {"text": "Instead of giving one point for exactly same n-gram or zero points for any difference, we include \"soft\" or \"fuzzy\" hits for word n-grams based on letter edit distance.", "labels": [], "entities": []}, {"text": "We call the score LeBLEU; this name can be interpreted either as \"Letter-edit-BLEU\" or \"Levenshtein-BLEU\".", "labels": [], "entities": []}, {"text": "Le-BLEU has two main parameters, n-gram length and fuzzy match threshold, that are easy to tune for different types of languages.", "labels": [], "entities": [{"text": "fuzzy match threshold", "start_pos": 51, "end_pos": 72, "type": "METRIC", "confidence": 0.8034783999125162}]}, {"text": "There are at least three previous approaches that resemble LeBLEU in that they try not to overpenalize different word orderings and word forms, but do not require any preprocessing tools or resources.", "labels": [], "entities": []}, {"text": "simply use the standard BLEU score on the level of characters, treating word delimiters as any other characters.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9768892228603363}]}, {"text": "In order to capture long enough sequences of text, they increase the maximum n-gram length to 18.", "labels": [], "entities": []}, {"text": "Compared to word-based BLEU, their method does not increase the correlations to human evaluation in English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9345154762268066}]}, {"text": "propose a score that is a weighted combination of two measures: an alignment score that applies letter edit distances be-tween the word forms and a structural score that measures the differences in word order.", "labels": [], "entities": []}, {"text": "In contrast to LeBLEU, it still strongly penalizes errors in compounding, as the alignment is word-to-word and fuzzy matches are accepted only if the LER between a pair of words is lower than 15%.", "labels": [], "entities": [{"text": "LER", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9858616590499878}]}, {"text": "More recently, Libovick\u00b4yLibovick\u00b4y and Pecina (2014) have proposed \"tolerant BLEU\", a variant of BLEU that similarly to LeBLEU finds fuzzy matches between hypothesis and reference words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9550158381462097}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9888734817504883}]}, {"text": "Instead of Levenshtein edit distance, they apply a specific affix distance measure that requires an exact match in the middle of the words.", "labels": [], "entities": []}, {"text": "Moreover, they apply a more complex procedure, in which the words between the hypothesis and reference are first aligned using the Munkres algorithm.", "labels": [], "entities": []}, {"text": "Then the hypothesis words are replaced by the matched reference words while applying a penalty based on the affix distance, and finally standard BLEU calculations are performed on the modified hypothesis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9971489310264587}]}, {"text": "Similarly to the method by, there is no matching between word n-grams of different lengths.", "labels": [], "entities": []}], "datasetContent": [{"text": "We study the proposed evaluation score using the data sets from the shared tasks of the Workshops on Statistical Machine Translation (WMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 101, "end_pos": 138, "type": "TASK", "confidence": 0.7815314481655756}]}, {"text": "The data sets contain human evaluations for different machine translation systems and system combination outputs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7392989695072174}]}, {"text": "The translation hypotheses are ranked both in the level of segments (individual sentences) and systems.", "labels": [], "entities": []}, {"text": "The translation hypotheses and references were used as inputs to the Le-BLEU score as such: no preprocessing was performed on the texts.", "labels": [], "entities": [{"text": "Le-BLEU score", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.8001691699028015}]}], "tableCaptions": [{"text": " Table 1. For the Finnish language that was  not present in the 2013 and 2014 shared tasks, we  took the best parameters for German, another lan- guage with complex morphology and long com- pound words.", "labels": [], "entities": []}, {"text": " Table 2: Performance of LeBLEU in recent WMT metrics shared tasks. Pearson's correlation coefficients  (system-level data) and average Kendall's tau correlation coefficients (segment-level data) for LeBLEU  with default parameters (def.), LeBLEU with optimized parameters (opt.), and topline method for the  shared task (top). For WMT 2014 data, also two reference methods are included: BLEU (ref-B) and  AMBER (ref-A).", "labels": [], "entities": [{"text": "WMT metrics shared tasks", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8225719481706619}, {"text": "WMT 2014 data", "start_pos": 332, "end_pos": 345, "type": "DATASET", "confidence": 0.8430445591608683}, {"text": "BLEU", "start_pos": 388, "end_pos": 392, "type": "METRIC", "confidence": 0.9983170032501221}, {"text": "AMBER", "start_pos": 406, "end_pos": 411, "type": "METRIC", "confidence": 0.7000769376754761}]}]}