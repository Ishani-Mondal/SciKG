{"title": [{"text": "Evaluating a Dynamic Time Warping Based Scoring Algorithm for Facial Expressions in ASL Animations", "labels": [], "entities": [{"text": "ASL Animations", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.8576552867889404}]}], "abstractContent": [{"text": "Advancing the automatic synthesis of linguistically accurate and natural-looking American Sign Language (ASL) anima-tions from an easy-to-update script would increase information accessibility for many people who are deaf by facilitating more ASL content to websites and media.", "labels": [], "entities": []}, {"text": "We are investigating the production of ASL grammatical facial expressions and head movements coordinated with the manual signs that are crucial for the interpretation of signed sentences.", "labels": [], "entities": [{"text": "ASL grammatical facial expressions", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.891892820596695}, {"text": "interpretation of signed sentences", "start_pos": 152, "end_pos": 186, "type": "TASK", "confidence": 0.869173988699913}]}, {"text": "It would be useful for researchers to have an automatic scoring algorithm that could be used to rate the similarity of two animation sequences of ASL facial movements (or an animation sequence and a motion-capture recording of a human signer).", "labels": [], "entities": []}, {"text": "We present a novel, sign-language specific similarity scoring algorithm, based on Dynamic Time Warping (DTW), for facial expression performances and the results of a user-study in which the predictions of this algorithm were compared to the judgments of ASL sign-ers.", "labels": [], "entities": []}, {"text": "We found that our algorithm had significant correlations with participants' comprehension scores for the animations and the degree to which they reported noticing specific facial expressions .", "labels": [], "entities": []}], "introductionContent": [{"text": "Access to understandable information on websites and other media is necessary for full participation in society.", "labels": [], "entities": []}, {"text": "Yet, the vast majority of information content online is in the form of written language text, and there are many users who have difficulty reading this material.", "labels": [], "entities": []}, {"text": "For many people who are deaf and hardof-hearing, there are educational factors that may lead to lower levels of written language literacy.", "labels": [], "entities": []}, {"text": "In the U.S., standardized testing has revealed that a majority of deaf high school graduates (students who are age 18 and older) have a fourth-grade English reading level or below.", "labels": [], "entities": []}, {"text": "(U.S. students in the fourth grade of school are typically age 10.)", "labels": [], "entities": []}, {"text": "While they may have difficulty with written English, many of these users have sophisticated fluency in another language: American Sign Language (ASL).", "labels": [], "entities": []}, {"text": "More than 500,000 people in the U.S. use ASL as a primary means of communication.", "labels": [], "entities": []}, {"text": "However, fluency in ASL does not entail fluency in written English since the two are distinct natural languages: with their own word order, linguistic structure, and vocabulary.", "labels": [], "entities": []}, {"text": "Thus, information content can be easier to understand for many deaf users if it is presented in ASL.", "labels": [], "entities": []}, {"text": "A spontaneous approach to presenting ASL online would be to upload videos of human signers on website and other media, but this is not ideal: re-filming a human performing ASL for frequently updated information is often prohibitively expensive, and the real-time generation of content from a query is not possible.", "labels": [], "entities": [{"text": "ASL online", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.7952966392040253}]}, {"text": "Software is needed that given an easy-to-update script as input can automatically synthesize ASL signing performed by a virtual human character.", "labels": [], "entities": [{"text": "ASL signing", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.9376445114612579}]}, {"text": "This software must internally coordinate the movements of the virtual human character such that the animated ASL message is linguistically accurate, understandable, and acceptable among users.", "labels": [], "entities": []}, {"text": "The creation of such software is the focus our research.", "labels": [], "entities": []}, {"text": "An ASL utterance consists of the movement of the hands, arms, torso, head, eye-gaze, and facial expressions.", "labels": [], "entities": [{"text": "ASL utterance", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9712728261947632}]}, {"text": "In fact, facial expressions are essential to the understandability and meaning of ASL sentences (see section 2).", "labels": [], "entities": [{"text": "ASL sentences", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.7970853745937347}]}, {"text": "Our research focuses on the automatic synthesis of facial expression movements for an ASL-signing virtual human character such that the resulting animations are judged to be clear and understandable by deaf users.", "labels": [], "entities": [{"text": "ASL-signing virtual human character", "start_pos": 86, "end_pos": 121, "type": "TASK", "confidence": 0.8029383271932602}]}, {"text": "In addition to our ongoing research in this area, other groups have studied issues related to the synthesis of facial expressions for sign language animation, whose methods and contributions we compare and survey in.", "labels": [], "entities": [{"text": "sign language animation", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.635980874300003}]}, {"text": "For researchers like ourselves, who are interested in designing software that generates linguistically-accurate ASL facial expressions performed by virtual human characters, the most comprehensive way to evaluate the quality of the software is to conduct user studies.", "labels": [], "entities": [{"text": "ASL facial expressions performed by virtual human characters", "start_pos": 112, "end_pos": 172, "type": "TASK", "confidence": 0.8178752958774567}]}, {"text": "Typically, we generate animations using the facial expression selection software, setup an experiment in which deaf participants view and evaluate the animations, and compare the scores of animations produced using the software (to some baselines or to prior versions of the software).", "labels": [], "entities": [{"text": "facial expression selection", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6645051836967468}]}, {"text": "Of course, conducting such studies with users is time-consuming and resource-intensive; so, these studies cannot be conducted on a frequent basis (e. g., weekly) during the development of ASL facial-expression synthesis software.", "labels": [], "entities": [{"text": "ASL facial-expression synthesis", "start_pos": 188, "end_pos": 219, "type": "TASK", "confidence": 0.857893705368042}]}, {"text": "For this reason, it would be useful to have some automatic method for quickly evaluating whether the facial expression produced by the software for some specific ASL sentence is accurate.", "labels": [], "entities": []}, {"text": "In this paper, we present an automatic scoring algorithm that can compare two facial expression performances to rate their similarity.", "labels": [], "entities": []}, {"text": "In principle, this automatic scoring tool could be used to quickly evaluate whether the output of facial expression synthesis software is producing a result that is similar to ASL utterances recorded from actual human ASL signers.", "labels": [], "entities": [{"text": "facial expression synthesis", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.6412164469559988}, {"text": "ASL utterances recorded from actual human ASL signers", "start_pos": 176, "end_pos": 229, "type": "TASK", "confidence": 0.8494121879339218}]}, {"text": "The proposed algorithm could be incorporated into a data-driven facial expression synthesis architecture, an approach which is also favored by other sign language animation researchers, e. g.: that use computer vision to extract facial features and produce facial expressions that occur during specific signs, and that map facial motion-capture data to animation blend-shapes using machine-learning methods.", "labels": [], "entities": [{"text": "facial expression synthesis", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.663310835758845}]}, {"text": "The at any moment in time can be conceptualized as a vector of numbers, specifying joint angles and facial-control parameters at that moment in time.", "labels": [], "entities": []}, {"text": "Thus, an animation is a stream of such vectors.", "labels": [], "entities": []}, {"text": "While there area variety of techniques that can be used to measure the similarity between two time-streams of vectors, this paper will specifically explore an approach based on a Dynamic Time Warping (DTW) algorithm.", "labels": [], "entities": []}, {"text": "Section 5 describes DTW and discusses how some researchers have begun to use this algorithm to rate the similarity of non-sign-language emotional facial expressions for animated characters; however, no user-study had been performed to verify that such scores actually matched human judgments of similarity -nor has this technique yet been applied to sign-language facial expressions.", "labels": [], "entities": []}, {"text": "This paper presents a novel, sign-language specific scoring algorithm based on DTW, which takes into account the timing of words in the sentence.", "labels": [], "entities": []}, {"text": "This paper reflects our first efforts at designing a DTW-based scoring tool, and the goal of this paper is to determine if the technique holds promise -if so, then we intend to investigate further variations of the scoring algorithm, to optimize it for ASL.", "labels": [], "entities": [{"text": "ASL", "start_pos": 253, "end_pos": 256, "type": "TASK", "confidence": 0.9719011187553406}]}, {"text": "In order to determine if our scoring tool is useful, we must determine whether the scores it provides actually correlate with the judgments of human ASL signers who evaluate ASL animations in an experiment.", "labels": [], "entities": [{"text": "ASL signers who evaluate ASL animations", "start_pos": 149, "end_pos": 188, "type": "TASK", "confidence": 0.7336502571900686}]}, {"text": "This paper presents a user study we conducted in which human ASL signers evaluated animations with facial expressions of different levels of quality (as rated by the automatic scoring tool), and we measure how well our automatic scoring correlates with the human judgments.", "labels": [], "entities": [{"text": "ASL signers", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9034892022609711}]}, {"text": "The remainder of this paper is organized as follows: Section 2 describes the linguistics of various ASL facial expressions, and section 3 describes how we time-warp a motion-capture recording of a facial expression performance to suit the synthesis of an ASL animation of a sentence with a different time duration.", "labels": [], "entities": [{"text": "ASL facial expressions", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.8275068402290344}]}, {"text": "Section 4 describes how the movements of the face of a virtual human character can be parameterized and controlled, and Section 5 defines our new DTW-based automatic scoring algorithm.", "labels": [], "entities": [{"text": "DTW-based automatic scoring", "start_pos": 146, "end_pos": 173, "type": "TASK", "confidence": 0.5579207241535187}]}, {"text": "Section 6 presents our research questions and hypotheses, which were evaluated in a user-study presented in section 7.", "labels": [], "entities": []}, {"text": "Finally, section 8 discusses these results and identifies future directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}