{"title": [{"text": "Extended Translation Models in Phrase-based Decoding", "labels": [], "entities": [{"text": "Extended Translation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6723061501979828}, {"text": "Phrase-based Decoding", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8639039695262909}]}], "abstractContent": [{"text": "We propose a novel extended translation model (ETM) to counteract some problems in phrase-based translation: The lack of translation context when using single-word phrases and uncaptured dependencies beyond phrase boundaries.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.7621304392814636}]}, {"text": "The ETM operates on word-level and augments the IBM models by an additional bilingual word pair and a reordering operation.", "labels": [], "entities": []}, {"text": "Its implementation in a phrase-based decoder introduces translation and reordering dependencies for single-word phrases and dependencies across phrase boundaries.", "labels": [], "entities": []}, {"text": "More, the model incorporates an explicit treatment of multiple and empty alignments.", "labels": [], "entities": []}, {"text": "Its integration outperforms competitive systems that include lexical and phrase translation models as well as hierarchical reordering models on 4 language pairs significantly by +0.7% BLEU on average.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7122697979211807}, {"text": "BLEU", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9994621872901917}]}, {"text": "Although simpler and using fewer dependencies, the ETM proves to be on par with 7-gram operation sequence models (Durrani et al., 2013b).", "labels": [], "entities": []}], "introductionContent": [{"text": "The first successful steps in Statistical Machine Translation have been taken by applying word-based models in a source-channel approach (.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8898852666219076}]}, {"text": "Within this framework, the language model (LM) is estimated on monolingual n-grams, whereas the translation models IBM-1 to IBM-5 are trained on bilingual data using word alignments.", "labels": [], "entities": []}, {"text": "The disadvantage of word-to-word translation is overcome by phrase-based translation (PBT) and log-linear model combination ).", "labels": [], "entities": [{"text": "word-to-word translation", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.7141371965408325}, {"text": "phrase-based translation", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.744059145450592}]}, {"text": "The open question is how much actual lexical context is included in decoding.", "labels": [], "entities": []}, {"text": "depicts the relative word frequencies plotted against the length of the phrase they were translated with for the IWSLT 2014 1 German\u2192English and English\u2192French tasks.", "labels": [], "entities": [{"text": "IWSLT 2014 1 German\u2192English", "start_pos": 113, "end_pos": 140, "type": "DATASET", "confidence": 0.8906854093074799}]}, {"text": "For English\u2192French, more than 40% of the words are translated using single-or two-word phrases, i.e. with a lexical context of at most one word.", "labels": [], "entities": []}, {"text": "For the German\u2192English task, more reorderings occur and lead to less monotone alignments.", "labels": [], "entities": []}, {"text": "Here, even 60% of all words are translated with a lexical context of at most one single word and over 20% are translated without any lexical context at all.", "labels": [], "entities": []}, {"text": "We address this problem by developing two variants of extended translation models (ETM), the direct (EdTM) for the Source\u2192Target and the inverse (EiTM) for the Target\u2192Source direction.", "labels": [], "entities": []}, {"text": "They operate on word-level and augment the IBM models by an additional bilingual word pair and a reordering operation.", "labels": [], "entities": []}, {"text": "We introduce them into the log-linear framework of a PBT system.", "labels": [], "entities": []}, {"text": "Thus, the decoding of single-word phrases can benefit from lexical and reordering context.", "labels": [], "entities": []}, {"text": "Moreover, the ETM allows to capture dependencies across phrase boundaries and long-range source dependencies.", "labels": [], "entities": []}, {"text": "It incorporates reordering information for non-monotone and multiple alignments including unaligned words.", "labels": [], "entities": []}, {"text": "As a first step, we implement the ETM as a count model with interpolated Kneser-Ney smoothing) using the Viterbi alignment and apply it in phrase-based decoding.", "labels": [], "entities": []}, {"text": "Nevertheless, the long-term goal of this approach is to replace the phrases used in decoding by translation units that predict a single target word, but may depend on several source words, previously translated target words and the reordering context.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on the largescale IWSLT 2014 2 () German\u2192English, English\u2192French and the large-scale DARPA BOLT Chinese\u2192English, Arabic\u2192English tasks.", "labels": [], "entities": [{"text": "IWSLT 2014 2", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.8429613510767618}, {"text": "DARPA", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.586079478263855}, {"text": "BOLT", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.5350771546363831}]}, {"text": "As mentioned in Section 4, all baseline systems include phrasal and lexical smoothing scores trained in both directions.", "labels": [], "entities": []}, {"text": "Word alignments are trained with GIZA ++ , by sequentially running 5 iterations each for the IBM-1, HMM and IBM-4 alignment models.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6605376899242401}, {"text": "IBM-1", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.8971666693687439}]}, {"text": "The domain of IWSLT consists of lecture-type talks presented at TED conferences which are also available online 3 . The baseline systems are trained on all provided bilingual data.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.7438328862190247}, {"text": "TED", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8530337810516357}]}, {"text": "All systems are optimized on the dev2010 and evaluated on the test2010 corpus.", "labels": [], "entities": [{"text": "test2010 corpus", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9342767298221588}]}, {"text": "The ETM is trained on the TED portions of the data: 138K sentences for German\u2192English and 185K sentences for English\u2192French.", "labels": [], "entities": [{"text": "ETM", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9044809937477112}]}, {"text": "For German\u2192English, to estimate the 4-gram LM, we additionally make use of parts of the Shuffled News, LDC English Gigaword and 10 9 -French-English corpora, selected by a crossentropy difference criterion.", "labels": [], "entities": [{"text": "Shuffled News", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.8231437504291534}]}, {"text": "In total, 1.7 billion running words are taken for LM training.", "labels": [], "entities": [{"text": "LM training", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9261438846588135}]}, {"text": "For English\u2192French, we use a large general domain 5-gram LM and an indomain 5-gram LM.", "labels": [], "entities": []}, {"text": "Both are estimated with the KenLM toolkit () using interpolated Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "KenLM toolkit", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8016755282878876}]}, {"text": "For the general domain LM, we first select 1 2 of the English Shuffled News, 4 of the French Shuffled News as well as both the English and French Gigaword corpora by the same cross-entropy difference criterion.", "labels": [], "entities": [{"text": "English Shuffled News", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.9021528363227844}, {"text": "French Shuffled News", "start_pos": 86, "end_pos": 106, "type": "DATASET", "confidence": 0.9135764241218567}]}, {"text": "By concatenating this selection with all available remaining monolingual data, we build an unpruned LM.", "labels": [], "entities": []}, {"text": "The BOLT tasks are evaluated on the \"discussion forum\" domain.", "labels": [], "entities": [{"text": "BOLT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6495887637138367}]}, {"text": "For Chinese\u2192English, the baseline is trained on 4.08M general domain sentence pairs and the 5-gram LM on 2.9 billion running words in total.", "labels": [], "entities": []}, {"text": "The ETM is trained on an indomain subset of 67.8K sentences and the test set contains 1844 sentences.", "labels": [], "entities": []}, {"text": "For the Arabic\u2192English BOLT task, we use only the in-domain data for training the baseline and the ETM.", "labels": [], "entities": [{"text": "BOLT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.8024704456329346}]}, {"text": "The training and test sets contain text drawn from discussion forums in Egyptian Arabic.", "labels": [], "entities": []}, {"text": "The evaluation set contains 1510 bilingual sentence pairs.", "labels": [], "entities": []}, {"text": "The baseline systems for all tasks -except the Arabic\u2192English BOLT task, where preliminary experiments showed no improvement -contain a 7-gram word cluster language model) and for comparison, we also experiment with a hierarchical reordering model (HRM).", "labels": [], "entities": []}, {"text": "When integrated into a phrase-based decoder, have shown the OSM to outperform bilingual LMs on MTUs.", "labels": [], "entities": []}, {"text": "Therefore, we directly compare ourselves with a 7-gram OSM implemented into our phrasebased decoder as an additional feature.", "labels": [], "entities": []}, {"text": "The OSM is trained on the same data as the ETM for all tasks.", "labels": [], "entities": []}, {"text": "Bilingual data statistics for all tasks are shown in.", "labels": [], "entities": []}, {"text": "For each system setting we evaluate three MERT runs using multeval.", "labels": [], "entities": [{"text": "MERT", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.6178279519081116}]}, {"text": "Results are reported in BLEU () and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9987687468528748}, {"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9955726861953735}]}, {"text": "The optimization criterion for all experiments is BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9989439845085144}]}], "tableCaptions": [{"text": " Table 2: The number of model parameters for the  BOLT Arabic\u2192English bilingual training data af- ter filtering.", "labels": [], "entities": [{"text": "BOLT Arabic\u2192English bilingual training data af- ter filtering", "start_pos": 50, "end_pos": 111, "type": "DATASET", "confidence": 0.6906373040242628}]}, {"text": " Table 3: Results for the German\u2192English IWSLT  data. The systems are optimized with MERT on  the dev2010 set. All results are statistically sig- nificant with \u226599% confidence.", "labels": [], "entities": [{"text": "IWSLT  data", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.7489529550075531}, {"text": "MERT", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.996256947517395}]}, {"text": " Table 4: Comparison of ETM to the HRM and  OSM measured in BLEU. Statistically significant  improvements with \u226599% confidence are printed  in boldface.", "labels": [], "entities": [{"text": "HRM", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.7863600850105286}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9969049096107483}]}]}