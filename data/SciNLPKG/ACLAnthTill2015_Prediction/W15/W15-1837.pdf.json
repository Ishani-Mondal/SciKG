{"title": [{"text": "Using sub-word n-gram models for dealing with OOV in large vocabulary speech recognition for Latvian", "labels": [], "entities": [{"text": "OOV in large vocabulary speech recognition", "start_pos": 46, "end_pos": 88, "type": "TASK", "confidence": 0.7238233387470245}, {"text": "Latvian", "start_pos": 93, "end_pos": 100, "type": "TASK", "confidence": 0.5602458119392395}]}], "abstractContent": [{"text": "In the Latvian language, one word can have tensor even hundreds of surface forms.", "labels": [], "entities": []}, {"text": "This is a serious problem for large vocabulary speech recognition.", "labels": [], "entities": [{"text": "large vocabulary speech recognition", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.6333466544747353}]}, {"text": "Inclusion of every form in vocabulary will make it intractable, but, on the other hand, even with a vocabulary of 400K, the out-of-vocabulary (OOV) rate will be very high.", "labels": [], "entities": [{"text": "out-of-vocabulary (OOV) rate", "start_pos": 124, "end_pos": 152, "type": "METRIC", "confidence": 0.9534241676330566}]}, {"text": "In this paper, the authors investigate the possibility of using sub-word vocabularies where words are split into frequent and common parts.", "labels": [], "entities": []}, {"text": "The results of our experiment show that this allows to significantly reduce the OOV rate.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9760786294937134}]}], "introductionContent": [{"text": "The Latvian language is a moderately inflected language, with complex nominal and verbal morphology.", "labels": [], "entities": []}, {"text": "Latvian also has a selection of prefixes that can modify nouns, adjectives, adverbs, and verbs either in a qualitative or a spatial sense.", "labels": [], "entities": []}, {"text": "There is no definite or indefinite article in Latvian, but definiteness can be indicated by the endings of adjectives.", "labels": [], "entities": []}, {"text": "Because of these properties, one word in Latvian can have tensor even hundreds (in the case of verbs) of surface forms.", "labels": [], "entities": []}, {"text": "A successful large vocabulary speech recognition system must be able to recognize most (if not all) of these forms.", "labels": [], "entities": [{"text": "large vocabulary speech recognition", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.6658714115619659}]}, {"text": "This means that the vocabulary of the system must be really huge and contain about a million or more source forms.", "labels": [], "entities": []}, {"text": "Speech recognition with such a vocabulary can be computationally intractable on most consumer hardware.", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8315938115119934}]}, {"text": "On the other hand, reducing vocabulary size increases the OOV rate and significantly degrades the quality of recognition.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9822349548339844}]}, {"text": "For example, an out-of-vocabulary word is known to generate between 1.5 and 2 errors (.", "labels": [], "entities": []}, {"text": "In this paper, the authors explore the sub-word approach, i.e., prefixes and endings, which are mostly common for all words, are split and treated as separate words.", "labels": [], "entities": []}, {"text": "This splitting greatly reduces vocabulary size, but can introduce other problems.", "labels": [], "entities": [{"text": "vocabulary size", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.5994253009557724}]}, {"text": "There have been many efforts in using word decomposition and sub-word based language models (LM) for dealing with OOV in inflective languages such as Arabic (), Czech (), Estonian), Finnish (, Russian,, and Slovenian ().", "labels": [], "entities": [{"text": "word decomposition", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7523237764835358}]}, {"text": "However, the authors could not find any reports on similar efforts for Latvian.", "labels": [], "entities": []}, {"text": "Significant improvement in the OOV rate was reported in all cases, but the changes in WER were not as dramatic.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.965469092130661}, {"text": "WER", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9571646451950073}]}, {"text": "The exception was the Finnish language (, where an astonishing improvement from 56% to 31% was achieved.", "labels": [], "entities": []}, {"text": "Significant improvement was also observed for the Estonian language; WER dropped by about 6.5% absolute with the morpheme language model (LM) and by more than 10% absolute when using the interpolated morpheme and class LM.", "labels": [], "entities": [{"text": "WER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9875828623771667}]}, {"text": "Different approaches for selecting sub-word units have been explored.", "labels": [], "entities": []}, {"text": "These can be divided into two groups: (1) data-driven methods () and (2) supervised methods with some embedded language knowledge (e.g., morphological analyzers, stemmers); El-Desoky et at.,.", "labels": [], "entities": []}, {"text": "In this work the authors investigate methods from both groups.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, the authors used a 22 million sentence text corpus, which was collected by crawling Latvian web news portals.", "labels": [], "entities": []}, {"text": "The corpus is used for training the Morfessor model and extracting vocabularies.", "labels": [], "entities": []}, {"text": "Sub-word vocabularies are extracted by performing a word decomposition on the text corpus and taking the 100 thousand most frequent units.", "labels": [], "entities": []}, {"text": "For comparison, the full vocabulary of this corpus contains approximately 1.5 million surface forms.", "labels": [], "entities": []}, {"text": "Also, vocabularies of the 100, 200, and 400 thousand most frequent surface forms were extracted as a baseline.", "labels": [], "entities": []}, {"text": "For evaluation, a small 23-minute long annotated speech corpus from 10 speakers was used.", "labels": [], "entities": []}, {"text": "First, OOV rates for different methods were calculated on the evaluation corpus transcripts.", "labels": [], "entities": [{"text": "OOV", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9783112406730652}]}, {"text": "As shown in, even with a 400K vocabulary, OOV is still very high -7.15%.", "labels": [], "entities": [{"text": "OOV", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9985578656196594}]}, {"text": "Both of the proposed methods, which use sub-word units instead of words, show a significant reduction of the OOV rate in the test corpus, while using a much smaller vocabulary.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9917608797550201}]}, {"text": "Vocabulary containing sub-word units from Morfessor output almost completely solves the OOV problem, despite being the smallest among other vocabularies.", "labels": [], "entities": [{"text": "OOV problem", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.6392194330692291}]}, {"text": "In order to evaluate the influence of sub-word vocabularies on the speech recognition task, the authors setup the following speech recognition system: For systems with sub-word vocabularies, training set transcripts were also split using the previously described models and tools and were used during the retraining of the acoustic models, so that the model is more adapted for recognizing sub-word units.", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8390651345252991}, {"text": "speech recognition", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7014231830835342}]}, {"text": "For language modeling, we used the same 22 million sentence text corpus.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7544385194778442}]}, {"text": "3-gram models were used in all experiments, except for the Morfessor-based system, where 6-gram models were also trained.", "labels": [], "entities": []}, {"text": "All models are pruned with equal parameters.", "labels": [], "entities": []}, {"text": "The results of the speech recognition are shown in.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8552106022834778}]}, {"text": "Despite the reduction in the OOV rate, no significant improvement in WER has been achieved.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9861287474632263}, {"text": "WER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.882656455039978}]}, {"text": "On the contrary, the baseline system with a 200K vocabulary showed the best results, while the Morfessor based system showed only a very small improvement (1%) in comparison to the baseline 100K system.", "labels": [], "entities": [{"text": "Morfessor based system", "start_pos": 95, "end_pos": 117, "type": "DATASET", "confidence": 0.8893718520800272}]}, {"text": "For systems using decomposition with a stemmer, an increase in the WER was observed.", "labels": [], "entities": [{"text": "WER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9960401058197021}]}], "tableCaptions": []}