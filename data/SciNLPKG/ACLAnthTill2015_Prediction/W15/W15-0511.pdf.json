{"title": [{"text": "Automatic Claim Negation: Why, How and When", "labels": [], "entities": [{"text": "Automatic Claim Negation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5160714288552603}]}], "abstractContent": [{"text": "The main goal of argumentation mining is to analyze argumentative structures within an argument-rich document, and reason about their composition.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.9161859452724457}]}, {"text": "Recently, there is also interest in the task of simply detecting claims (sometimes called conclusion) in general documents.", "labels": [], "entities": [{"text": "detecting claims (sometimes called conclusion) in general documents", "start_pos": 55, "end_pos": 122, "type": "TASK", "confidence": 0.7303378105163574}]}, {"text": "In this work we ask how this set of detected claims can be augmented further, by adding to it the negation of each detected claim.", "labels": [], "entities": []}, {"text": "This presents two NLP problems: how to automatically negate a claim, and when such a negated claim can plausibly be used.", "labels": [], "entities": []}, {"text": "We present first steps into solving both these problems, using a rule-based approach for the former and a statistical one towards the latter.", "labels": [], "entities": []}], "introductionContent": [{"text": "In Monty Python's famous Argument Clinic Sketch, Michael Palin is seeking a good argument, and John Cleese, too lazy to provide areal argument, simply contradicts whatever Mr. Palin is saying.", "labels": [], "entities": []}, {"text": "MP: An argument isn't just contradiction.", "labels": [], "entities": []}, {"text": "MP: No it can't!", "labels": [], "entities": [{"text": "MP", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8997992277145386}]}, {"text": "An argument is a connected series of statements intended to establish a proposition.", "labels": [], "entities": []}, {"text": "JC: No it isn't!", "labels": [], "entities": []}, {"text": "MP: Yes it is!", "labels": [], "entities": [{"text": "MP", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8801354765892029}]}, {"text": "It isn't just contradiction!", "labels": [], "entities": []}, {"text": "JC: Look, if I argue with you, I must take up a contrary position!", "labels": [], "entities": []}, {"text": "MP: Yes, but it isn't just saying 'no it isn't'.", "labels": [], "entities": []}, {"text": "In this work we aim to explore this last statement from the perspective of an automatic system, aiming to refute an examined claim.", "labels": [], "entities": []}, {"text": "Specifically, given a claim, how should we contradict it?", "labels": [], "entities": []}, {"text": "Is it enough to say \"No it isn't\", or is a more complicated algorithm required?", "labels": [], "entities": []}, {"text": "And when can we plausibly use an automatically generated contradiction?", "labels": [], "entities": []}, {"text": "When would it be considered a valid counterclaim, and when would it seem as an even less comprehensible version of John Cleese?", "labels": [], "entities": []}, {"text": "The answers to these questions turnout to be less simple than one might expect at first glance.", "labels": [], "entities": []}, {"text": "The main goal of argumentation mining is to analyze argumentative structures within a document.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.9132939875125885}]}, {"text": "Typically, documents in which such structures are abundant, such as from the legal domain, are analyzed, and compound argumentative structures, or argumentation schemes, are sought.", "labels": [], "entities": []}, {"text": "More recently, there is also interest in automatically detecting simple argumentative structures, or the building blocks of such structures, in documents which are not argumentative by nature.", "labels": [], "entities": []}, {"text": "For example, in) it was shown that context-dependent Claims and Evidences (sometimes called Conclusion and Grounds, respectively) are fairly common in Wikipedia articles, and can be detected automatically.", "labels": [], "entities": [{"text": "Conclusion", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9215519428253174}]}, {"text": "In this setting, detection is done within a given context of a pre-specified debatable topic.", "labels": [], "entities": [{"text": "detection", "start_pos": 17, "end_pos": 26, "type": "TASK", "confidence": 0.9614766836166382}]}, {"text": "Then, the objective is to search a given set of documents, and mine Claims and Evidence pertaining to this topic.", "labels": [], "entities": []}, {"text": "One motivation for such context-dependent argumentation mining is that it serves as the first component in a debate-support system.", "labels": [], "entities": [{"text": "context-dependent argumentation mining", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.631102571884791}]}, {"text": "Ina second stage, Claims and Evidence can be combined into full fledged Arguments, highlighting to the user the various opinions surrounding the debatable topic.", "labels": [], "entities": []}, {"text": "In order to provide a comprehensive view of these various opinions, it might not be sufficient to rely on the initial set of detected argumentative elements.", "labels": [], "entities": []}, {"text": "For example, for practical reasons, an automatic Claim detection system as in ( will present to the user only its top scoring predictions, which will probably represent only a subset of the relevant Claims.", "labels": [], "entities": [{"text": "Claim detection", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.8267057538032532}]}, {"text": "Furthermore, the examined corpus might be biased, hence enriched with claims supporting only one side of the debate.", "labels": [], "entities": []}, {"text": "Thus, it is of interest to augment the initial set of predictions made by such systems through various means.", "labels": [], "entities": []}, {"text": "Here, motivated by the observation that negating previous arguments has an important function in argumentation, we suggest a system to augment a given set of relevant Claims by automatically suggesting a meaningful negation per mentioned Claim.", "labels": [], "entities": []}, {"text": "More specifically, we require that the automatically suggested negation will be not only grammatically correct, but also plausible to use in a discussion about the given topic.", "labels": [], "entities": []}, {"text": "As we discuss and demonstrate, this latter requirement poses a nontrivial challenge.", "labels": [], "entities": []}, {"text": "Accordingly, we propose a Machine Learning approach that exploits NLP-based features in order to determine if it is plausible to use the suggested negation.", "labels": [], "entities": []}, {"text": "Our results demonstrate the feasibility and practical potential of the suggested approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We started with a data set of 1,240 Claims, collected in the context of various debatable topics, using the same protocol described in (.", "labels": [], "entities": []}, {"text": "Given this data, the algorithm described in section 4.2 was used to generate 1,240 pairs of the form (Claim, candidate negation).", "labels": [], "entities": []}, {"text": "Each pair was annotated by 5 annotators, out of a set of 11 annotators.", "labels": [], "entities": []}, {"text": "Specifically, each annotator was asked to assess the candidate negation according to the 4 criteria mentioned in section 3 -i.e., whether the candidate negation is grammatically correct; clear; states the opposite of the original Claim; and usable in a debate to rebut the Claim.", "labels": [], "entities": [{"text": "Claim", "start_pos": 273, "end_pos": 278, "type": "DATASET", "confidence": 0.9517276883125305}]}, {"text": "Taking the majority over the 5 annotators determined the candidate negation's label.", "labels": [], "entities": []}, {"text": "Thus, a candidate negation was considered \"usable\" if at least three annotators determined it was such.", "labels": [], "entities": []}, {"text": "We note that each pair of annotators either considered no pairs of (Claim, candidate negation) in common, or at least 250.", "labels": [], "entities": []}, {"text": "This was important when measuring agreement (section 5.2), ensuring a reasonable sample size.", "labels": [], "entities": []}, {"text": "Next, a logistic-regression classifier was trained and tested based on the features described in section 4.3, in a 10-fold cross validation framework, using the \"usable\" (yes/no) annotation as the class label.", "labels": [], "entities": []}, {"text": "That is, the data set was divided into 10 chunks of consecutive Claims.", "labels": [], "entities": []}, {"text": "At each of the 10 iterations, a logistic-regression classifier was trained on 9 of the chunks, and predicted whether or not each of the candidate negations in the remaining chunk should be considered \"usable\".", "labels": [], "entities": []}, {"text": "There is a caveat here -on the one hand each fold should be of the same size, while on the other hand including claims from the same topic in both train and test set may conceivably create a bias (if deciding successful negation is somehow topic-dependant).", "labels": [], "entities": []}, {"text": "As a compromise we ordered the claims according to topic.", "labels": [], "entities": []}, {"text": "This way folds are of the same size, and at most two topics are split between the train and test sets.", "labels": [], "entities": []}, {"text": "The weights assigned to each train sample were the product of two numbers -a normalizing factor and a confidence score.", "labels": [], "entities": []}, {"text": "The normalization factor is assigned so that the total weight for positive samples is the same as that of negative samples.", "labels": [], "entities": []}, {"text": "Namely, if k out of n samples are positive, then the normalization factor for positive samples is (n \u2212 k)/k (and 1 for negative samples).", "labels": [], "entities": []}, {"text": "The confidence score was defined as the size of the majority which determined the label, divided by 5.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9783750772476196}]}, {"text": "So 0.6 in the case of a 3-2 split, 0.8 in the case of a 4-1 split and 1.0 in the case of a unanimous vote.", "labels": [], "entities": []}, {"text": "The complete data-set, including the Claims, candidate negations, and associated annotations, are available upon request for research purposes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Fraction of negated claims which passed each  criteria according to majority vote, and mean pairwise  agreement among annotators. Pairwise agreement is de- fined as the fraction of candidate negations for which the  two annotators give the same \"yes/no\" label.", "labels": [], "entities": []}, {"text": " Table 2: Fraction of claims which pass both criteria from  those which pass the one listed on the left column. If n 1  claims pass criterion i, and n 2 pass both i and j, the (i, j)  entry in the table is n2/n1.", "labels": [], "entities": []}, {"text": " Table 3: Pairwise Cohen's kappa statistics among anno- tators (first line), and comparing annotators to classifier  (second line).", "labels": [], "entities": []}]}