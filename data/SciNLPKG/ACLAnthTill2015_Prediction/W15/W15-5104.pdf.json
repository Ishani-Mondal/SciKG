{"title": [{"text": "Contour-based Hand Pose Recognition for Sign Language Recognition", "labels": [], "entities": [{"text": "Contour-based Hand Pose Recognition", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7123932465910912}, {"text": "Sign Language Recognition", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6564721862475077}]}], "abstractContent": [{"text": "We are developing a real-time Japanese sign language recognition system that employs abstract hand motions based on three elements familiar to sign language: hand motion, position, and pose.", "labels": [], "entities": [{"text": "Japanese sign language recognition", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.5767030492424965}]}, {"text": "This study considers the method of hand pose recognition using depth images obtained from the Kinect v2 sensor.", "labels": [], "entities": [{"text": "hand pose recognition", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.8095748623212179}]}, {"text": "We apply the contour-based method proposed by Keogh to hand pose recognition.", "labels": [], "entities": [{"text": "hand pose recognition", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7541279792785645}]}, {"text": "This method recognizes a contour by means of discriminators generated from contours.", "labels": [], "entities": []}, {"text": "We conducted experiments on recognizing 23 hand poses from 400 Japanese sign language words.", "labels": [], "entities": []}], "introductionContent": [{"text": "In Japan, Japanese sign language is usually used among hearing impaired people to communicate.", "labels": [], "entities": []}, {"text": "In addition, these people often communicate with others through a third person who understands both oral and sign language.", "labels": [], "entities": []}, {"text": "The alternative is to use a computer that acts as an interpreter.", "labels": [], "entities": []}, {"text": "However, no practical sign language recognition system exists, even one that recognizes isolated words.", "labels": [], "entities": [{"text": "sign language recognition", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6848377188046774}]}, {"text": "The difficulties lie in the nature of visual language and its complex structure.", "labels": [], "entities": []}, {"text": "Compared with speech recognition, sign language recognition incorporates various visual components, such as hand motions, hand poses and facial expressions.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.782345175743103}, {"text": "sign language recognition", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6867431998252869}]}, {"text": "In addition, no established study exists on representing the structure of Japanese sign language in a similar manner to that of spoken language.", "labels": [], "entities": []}, {"text": "Therefore, few attempts recognize sign language by units such as hand motions and hand poses.", "labels": [], "entities": []}, {"text": "Our study develops with real-time recognition of sign language words.", "labels": [], "entities": [{"text": "real-time recognition of sign language words", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.7091534634431204}]}, {"text": "In Japanese sign language, a sentence consists of several words and non-manual signals such as facial expressions.", "labels": [], "entities": []}, {"text": "To recognize words is a first step and essential to recognize sentences.", "labels": [], "entities": []}, {"text": "The number of Japanese sign language words is said to be 3,000 or more.", "labels": [], "entities": []}, {"text": "Recognition by discriminators that are independent of every word has proven ineffective.", "labels": [], "entities": [{"text": "Recognition", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9628890752792358}]}, {"text": "To produce a practical system, analysis and reconstruction of sign language words are critical.", "labels": [], "entities": []}, {"text": "We want to emphasize that database of sign language words is required when we analyze such words.", "labels": [], "entities": []}, {"text": "However, no established database currently exists for sign language recognition.", "labels": [], "entities": [{"text": "sign language recognition", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7124192317326864}]}, {"text": "Therefore, we employ a database from a computerized sign language word dictionary instead.", "labels": [], "entities": []}, {"text": "Our system is based on three elements of sign language: hand motion, position, and pose.", "labels": [], "entities": []}, {"text": "This study considers the method of hand pose recognition for our system.", "labels": [], "entities": [{"text": "hand pose recognition", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7904975414276123}]}, {"text": "Speeding up hand pose recognition is difficult, because of the number and variety of hand poses caused by rotations, altering the angle from the sensor, and diversities in bone structures.", "labels": [], "entities": [{"text": "hand pose recognition", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7732457717259725}]}, {"text": "This study considers a hand pose recognition using depth images obtained from a single depth sensor.", "labels": [], "entities": [{"text": "hand pose recognition", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8002743721008301}]}, {"text": "We apply the contour-based method proposed by to hand pose recognition.", "labels": [], "entities": [{"text": "hand pose recognition", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7126087148984274}]}, {"text": "This method recognizes a contour by means of discriminators learned from contours.", "labels": [], "entities": []}, {"text": "We conducted experiments to recognize 23 hand poses from 400 Japanese sign language words.", "labels": [], "entities": []}, {"text": "shows the flowchart of the entire system.", "labels": [], "entities": []}, {"text": "We use Kinect v2 sensor to obtain data from sign motions produced by an actual person.", "labels": [], "entities": []}, {"text": "First, data obtained from the sensor is segmented into sign language words.", "labels": [], "entities": []}, {"text": "Second, the three aforementioned elements are recognized individually.", "labels": [], "entities": []}, {"text": "Finally, the recognition result is determined by the weighted sum of each score.", "labels": [], "entities": [{"text": "recognition", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.8815972208976746}]}, {"text": "The recognition process of the hand pose and other two components employs depth data of the hand region and coordinates of joints, respectively.", "labels": [], "entities": []}, {"text": "This study partially considers the method of hand pose recognition and does not discuss other processes on the flowchart.", "labels": [], "entities": [{"text": "hand pose recognition", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7469842831293741}]}], "datasetContent": [{"text": "We conducted experiments on recognizing 23 hand poses in 400 Japanese sign language words in the national sign language test grade 5.", "labels": [], "entities": [{"text": "national sign language test grade 5", "start_pos": 97, "end_pos": 132, "type": "DATASET", "confidence": 0.7132918188969294}]}, {"text": "To recognize these 400 words requires to distinguish 23 hand poses in defined by hand types and palm directions.", "labels": [], "entities": []}, {"text": "Some words have the same hand poses but different position and motion.", "labels": [], "entities": []}, {"text": "Our system distinguish each word after recognizing 3 components and unifying recognition results.", "labels": [], "entities": []}, {"text": "Because hand shapes transform with motions, each hand type remains independent even if the palm direction is different.", "labels": [], "entities": []}, {"text": "However, some exceptions exist to distinguish sign language words that have the same motion, position, and hand type, but have a different palm direction.", "labels": [], "entities": []}, {"text": "For example, Groups 3 and 4 in should be distinguished even though the hand type is the same.", "labels": [], "entities": []}, {"text": "To simplify the collection of data in our experiments, we used depth images of stationary hands instead of those obtained during natural sign motions.", "labels": [], "entities": []}, {"text": "We conducted four experiments examining the robustness of the recognition method about the variety of hand shapes and the computation time.", "labels": [], "entities": []}, {"text": "The objectives of the experiments are described as follows.", "labels": [], "entities": []}, {"text": "Experiment 1 Recognize 100 hand images by wedges produced from the same 100 images per hand type, palm direction, and tester (close-dataset, close-tester).", "labels": [], "entities": []}, {"text": "Experiment 2 Recognize 50 hand images by wedges produced from the other 50 hand images per hand type, palm direc-  Experiment 4 was for checking the computation time.", "labels": [], "entities": []}, {"text": "shows the relationship between the computation time required to recognize a hand image and the average recognition rate in Experiment 2.", "labels": [], "entities": []}, {"text": "The speed-up process did not affect the recognition rate.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.987656831741333}, {"text": "recognition", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9072328805923462}]}, {"text": "When the person is known, 88 ms (corresponding to 11 fps) was required to recognize a hand image with 70 % accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9972832202911377}]}, {"text": "Recognizing all hand images obtained from the sensor with a frame rate of 30 fps is impossible.", "labels": [], "entities": []}, {"text": "However, the number of frames required to specify a hand pose is limited because the hand pose does not change at every frame.", "labels": [], "entities": []}, {"text": "We can recognize in real-time selected hand images by means of comparison method employing a small calculation such as image moment.", "labels": [], "entities": []}, {"text": "This experiment has been implemented in a single-thread.", "labels": [], "entities": []}, {"text": "The processing speed can be improved by utilizing a high-speed technique such as multi-threading.", "labels": [], "entities": []}, {"text": "Experiment 3 was tester-independent setup.", "labels": [], "entities": []}, {"text": "shows the results of Experiment 3.", "labels": [], "entities": []}, {"text": "The recognition rates shown are the results when the length of distance vectors is 30.", "labels": [], "entities": []}, {"text": "If we change the length to 180, recognition rates do not change significantly.", "labels": [], "entities": [{"text": "length", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9906531572341919}, {"text": "recognition", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.7403486967086792}]}, {"text": "We specified causes of erroneous recognition when the number of wedges is 30 per hand type and palm direction.", "labels": [], "entities": []}, {"text": "The results show the same tendency as in Experiments 1 and 2, that is, 13 % of all data were misrecognized as Groups 4 and 5.", "labels": [], "entities": []}, {"text": "The detailed findings for each hand pose group reveal the following: 41 % of Group 6 were misrecognized as Group 0, 53 % of Group 19 were misrecognized as Group 0, 45 % of Group 12 were misrecognized as Group 5.", "labels": [], "entities": []}, {"text": "The low recognition rate is due to individual differences in hand shapes caused by differences in bone structure and posing of hand shown in.", "labels": [], "entities": []}, {"text": "Wedges produced from the hand images of various people include other hand types.", "labels": [], "entities": []}, {"text": "Per person details show that the recognition rate was lowest when the system attempted to recognize hand poses of tester A, whose hand size was the smallest.", "labels": [], "entities": [{"text": "recognition rate", "start_pos": 33, "end_pos": 49, "type": "METRIC", "confidence": 0.7452930808067322}]}, {"text": "When the number of wedges increases, the recognition rate of tester B, whose hand size is between that of A and C is higher than that of other testers.", "labels": [], "entities": [{"text": "recognition rate", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.852314293384552}]}, {"text": "Although we normalized the scale of distance vectors according to each hand size, hand pose recognition by contours possesses other difficulties when the bone structures are considered.", "labels": [], "entities": [{"text": "hand pose recognition", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7127728859583536}]}, {"text": "The accuracy diminishes when the system recognizes hand images of a person whose bone structure is dissimilar to any learning data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994766116142273}]}, {"text": "When we want to recognize hand poses of an unknown person, wedges generated from people who have similar bone structure should be used.", "labels": [], "entities": []}, {"text": "Therefore, additional hand images that reveal various characteristics in bone structures should be collected.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Portion of the database in the dictionary.", "labels": [], "entities": []}, {"text": " Table 2: Sign Language (SL) types.", "labels": [], "entities": []}, {"text": " Table 3: List of 23 hand pose groups.", "labels": [], "entities": []}]}