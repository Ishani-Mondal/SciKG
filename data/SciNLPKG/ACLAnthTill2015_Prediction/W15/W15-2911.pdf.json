{"title": [{"text": "Analysing domain suitability of a sentiment lexicon by identifying distributionally bipolar words", "labels": [], "entities": []}], "abstractContent": [{"text": "Contemporary sentiment analysis approaches rely heavily on lexicon based methods.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9506195485591888}]}, {"text": "This is mainly due to their simplicity , although the best empirical results can be achieved by more complex techniques.", "labels": [], "entities": []}, {"text": "We introduce a method to assess suitability of generic sentiment lexicons fora given domain, namely to identify frequent bigrams where a polar word switches polarity.", "labels": [], "entities": []}, {"text": "Our bigrams are scored using Lexicographers Mutual Information and leveraging large automatically obtained corpora.", "labels": [], "entities": []}, {"text": "Our score matches human perception of polarity and demonstrates improvements in classification results using our enhanced context-aware method.", "labels": [], "entities": []}, {"text": "Our method enhances the assessment of lexicon based sentiment detection algorithms and can be further used to quantify ambiguous words.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8150762021541595}]}], "introductionContent": [{"text": "Sentiment prediction from microblogging posts is of the utmost interest for researchers as well as commercial organizations.", "labels": [], "entities": [{"text": "Sentiment prediction from microblogging posts", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.9115723609924317}]}, {"text": "State-of-the-art sentiment research often focuses on in-depth semantic understanding of emotional constructs or neural network models.", "labels": [], "entities": [{"text": "State-of-the-art sentiment research", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7604840099811554}]}, {"text": "However, recent sentiment prediction challenges show that the vast majority of currently used systems is still based on supervised learning techniques with the most important features derived from preexisting sentiment lexica (.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.9271190166473389}]}, {"text": "Sentiment lexicons were initially developed as general-purpose resources; * Project carried out during a research stay at the University of Pennsylvania).", "labels": [], "entities": []}, {"text": "Recently, there has been an increasing amount of work on platform-specific lexicons such as Twitter.", "labels": [], "entities": []}, {"text": "However, even customized platform-and domain-specific lexica still suffer from ambiguities at a contextual level, e.g. cold beer (+) or cold food (-), dark chocolate (+) or dark soul.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to assess the suitability of an established lexicon fora new platform or domain by leveraging automatically collected data approximating sentiment labels (silver standard).", "labels": [], "entities": []}, {"text": "We present a method for creating switched polarity bigram lists to explicitly reveal and address the issues of a lexicon in question (e.g. the positivity of cold beer, dark chocolate or limited edition).", "labels": [], "entities": []}, {"text": "Note that the contextual polarity switch does not necessarily happen on sense level, but within one word sense.", "labels": [], "entities": []}, {"text": "We demonstrate that the explicit usage of such inverse polarity bigrams and replacement of the words with high ambiguity improves the performance of the classifier on unseen test data and that this improvement exceeds the performance of simply using all in-domain bigrams.", "labels": [], "entities": []}, {"text": "Further, our bigram ranking method is evaluated by human raters, showing high face validity.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the quality of our bigrams, we perform two studies.", "labels": [], "entities": []}, {"text": "First, we rate our inverted polarity bigrams intrinsically using crowdsourced annotations.", "labels": [], "entities": []}, {"text": "Second, we assess the performance of the original and adjusted lexicons on a distinct expertconstructed data set of 1,600 Facebook messages annotated for sentiment.", "labels": [], "entities": [{"text": "expertconstructed data set of 1,600 Facebook messages", "start_pos": 86, "end_pos": 139, "type": "DATASET", "confidence": 0.8469767144748143}]}, {"text": "The disambiguated bigram lexicons are available on author's website.", "labels": [], "entities": []}, {"text": "We crowdsource ratings for the inverted polarity bigrams found using both the HL and MPQA lexicon.", "labels": [], "entities": [{"text": "MPQA lexicon", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.8197988271713257}]}, {"text": "The raters were presented a list of 100 bigrams of each lexicon, with 25% having the same positive polarity as in the original lexicon, 25% the same negative polarity, 25% switching polarity from positive unigram to negative bigram and the remaining  quarter vice versa.", "labels": [], "entities": []}, {"text": "They had to answer the question 'Which polarity does this word pair have?', given positive, negative and also neutral as options.", "labels": [], "entities": []}, {"text": "Each bigram is rated by three annotators and the majority vote is selected.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement is measured using weighted Cohen's \u03ba, which is especially useful for ordered annotations, as it accounts not only for chance, but also for the seriousness of a disagreement between annotators.", "labels": [], "entities": []}, {"text": "\u03ba can range from -1 to 1, where the value of 0 represents an agreement equal to chance while 1 equals to a perfect agreement, i.e. identical annotation values.", "labels": [], "entities": []}, {"text": "We obtained an agreement of weighted Cohen's \u03ba = 0.55, which represents a \"moderate agreement\".", "labels": [], "entities": []}, {"text": "The confusion matrix of average human judgement compared to our computed bigram polarity is shown in.", "labels": [], "entities": []}, {"text": "Some of the bigrams, especially for the MPQA lexicon, were assessed as objective, which our LMI method unfortunately does not reflect beyond the score value (neutral words are less polar).", "labels": [], "entities": [{"text": "MPQA lexicon", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9064796566963196}]}, {"text": "However, the confusion between negatively and positively labeled bigrams was very low.", "labels": [], "entities": [{"text": "confusion", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9620857834815979}]}, {"text": "We evaluate our method on a data set of Facebook posts annotated for positive and negative sentiment by two psychologists.", "labels": [], "entities": []}, {"text": "The posts are annotated on a scale from 1 to 9, with 1 indicating strong negative sentiment and 9 strong positive sentiment.", "labels": [], "entities": []}, {"text": "An average rating between annotators is considered to be the final message score.", "labels": [], "entities": []}, {"text": "Ratings follow a normal distribution, i.e. with more messages having less polar score.", "labels": [], "entities": [{"text": "polar score", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9323641061782837}]}, {"text": "An inter-annotator agreement of weighted Cohen's \u03ba = 0.61 on exact score was reached, representing a \"substantial agreeement\".", "labels": [], "entities": [{"text": "exact score", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9614705741405487}]}, {"text": "Given our task, in which we attempt to improve on misleading bipolar words, we removed the posts annotated as neutral (rating 5.0).", "labels": [], "entities": []}, {"text": "This left us with 2,087 posts, of which we use only those containing at least one word from the polarity lexicons of our interest, i.e., 1,601 posts for MPQA and 1,526 posts for HL.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 153, "end_pos": 157, "type": "DATASET", "confidence": 0.9331001043319702}]}, {"text": "We then compute a sentiment score of a post as a difference of positive and negative word counts present in the post.", "labels": [], "entities": []}, {"text": "If a bigram containing the lexicon word is found, its LMI SO score is used instead of the lexicon word polarity score.", "labels": [], "entities": [{"text": "LMI SO score", "start_pos": 54, "end_pos": 66, "type": "METRIC", "confidence": 0.8656540314356486}]}, {"text": "For the two lexicons and their modifications, we employ two evaluation measuresPearson correlation of the sentiment score of a post with the affect score, and classification accuracy on binary label, i.e., distinguishing if the affect is negative (1-4) or positive (6-9).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.8357915878295898}]}, {"text": "Using McNemar's two-tailed test, there is a significant difference on p<0.05 level between the runs 1 and 2, 5 and 6 and 1 and 5 for BL, and between the runs 1 and 6 for MPQA.", "labels": [], "entities": [{"text": "BL", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.7681174278259277}, {"text": "MPQA", "start_pos": 170, "end_pos": 174, "type": "DATASET", "confidence": 0.9260334968566895}]}, {"text": "shows that adding contextual bigrams brings a consistent improvement (1 vs. 2, 5 vs. 6).", "labels": [], "entities": []}, {"text": "Especially the negative part of the bigram lexica, including bigrams of negative words which have positive orientation, consistently improves results (1 vs. 4, 5 vs. 8).", "labels": [], "entities": []}, {"text": "Likewise, pruning of the lexicon with the polar entropy score (1 vs. 5) enhances the sentiment prediction performance.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.9533635675907135}]}, {"text": "For both polarity lexicons the best performance is achieved by combining the two effects.", "labels": [], "entities": []}, {"text": "In case of the first lexicon, the performance is even higher than in case of applying for the same data a fully in-domain bigram lexicon, generated from the same large public Twitter corpus.", "labels": [], "entities": []}, {"text": "The correction of negative unigrams to positive bigrams does not improve the prediction as much as its counterpart.", "labels": [], "entities": []}, {"text": "The main cause appears to be the fact that those expressions with shifted polarity shall be rather neutral -as discussed in section 4.1 and by some recent research ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Most ambiguous sentiment lexicon words. Table dis- plays the proportion of their positive and negative contexts  and the original lexicon polarity.", "labels": [], "entities": []}, {"text": " Table 3: Confusion matrix for the majority vote of word polar- ity by three annotators.", "labels": [], "entities": []}, {"text": " Table 4: Predictive performance using lexicon based methods,  displaying the classification accuracy and linear correlation  of the affect score to LMI. Using McNemar's two-tailed test,  there is a significant difference on p<0.05 level between the  runs 1 and 2, 5 and 6 and 1 and 5 for BL, and between the  runs 1 and 6 for MPQA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.8678027987480164}, {"text": "BL", "start_pos": 289, "end_pos": 291, "type": "METRIC", "confidence": 0.7051700353622437}, {"text": "MPQA", "start_pos": 327, "end_pos": 331, "type": "DATASET", "confidence": 0.9255682229995728}]}]}