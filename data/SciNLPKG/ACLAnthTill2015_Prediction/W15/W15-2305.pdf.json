{"title": [{"text": "Lexical Semantics and Model Theory: Together at Last?", "labels": [], "entities": [{"text": "Lexical Semantics and Model Theory", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7075833320617676}]}], "abstractContent": [{"text": "We discuss the model theory of two popular approaches to lexical semantics and their relation to transcendental logic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent advances informal and computational linguistics have brought forth two classes of theories, algebraic conceptual representation (ACR) and continuous vector space (CVS) models.", "labels": [], "entities": [{"text": "algebraic conceptual representation (ACR)", "start_pos": 99, "end_pos": 140, "type": "TASK", "confidence": 0.7552774051825205}]}, {"text": "Together with Montague grammar (MG) and its lineal descendants (Discourse Representation Theory, Dynamic Predicate Logic, etc.) we now have three broad families of semantic theories competing in the same space.", "labels": [], "entities": [{"text": "Montague grammar (MG)", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.547510701417923}, {"text": "Discourse Representation Theory", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.7555148402849833}]}, {"text": "MG and related theories fit well with most versions of transformational and post-transformational grammar and retain a strong presence in theoretical linguistics, but have long been abandoned in computational work as too brittle.", "labels": [], "entities": [{"text": "MG", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8224402070045471}, {"text": "transformational and post-transformational grammar", "start_pos": 55, "end_pos": 105, "type": "TASK", "confidence": 0.7245637327432632}]}, {"text": "As we have argued elsewhere, MG-like theories fail not just as performance grammar but, perhaps more surprisingly, on competence grounds as well ().", "labels": [], "entities": []}, {"text": "Nevertheless, MG will be our starting point, as it is familiar to virtually all linguists.", "labels": [], "entities": [{"text": "MG", "start_pos": 14, "end_pos": 16, "type": "DATASET", "confidence": 0.35868343710899353}]}, {"text": "From an abstract point of view we should distinguish between a framework for compositionality and a commitment to a particular brand of semantics.", "labels": [], "entities": []}, {"text": "While we still want to uphold the idea of compositionality, we are less enthusiastic about the dominance of standard first order models, even if suitably intensionalized, in explaining or representing meanings.", "labels": [], "entities": []}, {"text": "Luckily, other choices can be made, though they come with a different conception of meaning.", "labels": [], "entities": []}, {"text": "The main difference between ACR, CVS, and the standard MG treatment is in fact the choice of model structures: both ACR and CVS aim at modeling 'concepts in the head' rather than 'things in the world', and thus clash strongly with the ostensive anti-psychologism of MG.", "labels": [], "entities": []}, {"text": "How can we make sense of such theories after without being attacked for promulgating yet another version of markerese?", "labels": [], "entities": []}, {"text": "The answer proposed in this paper is that we divest model theory from the narrow meaning it has acquired in linguistics, as being about formulas in some first-or higher-order calculus, and interpret natural language expressions either directly in the models, the original approach of, or through some convenient knowledge representation language, still composed of formulas, but without the standard logical baggage.", "labels": [], "entities": [{"text": "model theory", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.7790727615356445}]}, {"text": "The main novelty is that the formulas themselves will be very close to the models, though not quite like in Herbrand models for reasons that will become clear as we develop the theory.", "labels": [], "entities": []}, {"text": "Section 2 provides a brief justification for the enterprise, and sketches as much of ACR and CVS as we will need for Section 3, where essential properties of their models are discussed.", "labels": [], "entities": [{"text": "ACR", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9648675918579102}, {"text": "CVS", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.6568741798400879}]}, {"text": "Our focus will be on CVS, and we shall discuss the challenge of compositionality, which appears to be nontrivial for CVSs.", "labels": [], "entities": []}, {"text": "ACR graphs are simple discrete structures, very attractive for representing meaning (indeed, they have along history in Knowledge Representation), but more clumsy for syntax.", "labels": [], "entities": [{"text": "Knowledge Representation", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.7038700878620148}]}, {"text": "CVS representations, finite dimensional vectors over R, are primarily about distribution (syntactic cooccurrence), and meaning, especially the linear structures that encode analogy such as king:queen = man:woman will arise in them chiefly as a result of probabilistic regularities (.", "labels": [], "entities": []}, {"text": "We take the view that CVS models 'concepts in the head' and to understand how these can be similar across speakers we need to invoke 'concepts in the world' as described by ACR.", "labels": [], "entities": [{"text": "ACR", "start_pos": 173, "end_pos": 176, "type": "DATASET", "confidence": 0.9692358374595642}]}, {"text": "Section 4 discusses the challenge posed by changing to mentalist semantics.", "labels": [], "entities": []}, {"text": "If meanings are in the head, we are losing, or so it appears, the objectivity of meanings.", "labels": [], "entities": []}, {"text": "However, we think that this is not so.", "labels": [], "entities": []}, {"text": "Instead, our working hypothesis of this paper is what we call 'One Reality': meanings describe a common reality so that anything that is true of the world must be compatible with anything else that is true.", "labels": [], "entities": []}, {"text": "The section explores some immediate consequences of this hypothesis.", "labels": [], "entities": []}, {"text": "We close with some speculative remarks in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 Parsing performance as a function of the  number of coefficients kept in \u2297 definitions", "labels": [], "entities": [{"text": "Parsing", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.9241161346435547}]}, {"text": " Table 2 Cooccurrence predictors for frequent, rare,  and nonexistent adjective+noun combinations", "labels": [], "entities": []}]}