{"title": [{"text": "Semantic Information Extraction for Improved Word Embeddings", "labels": [], "entities": [{"text": "Semantic Information Extraction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7921380798021952}]}], "abstractContent": [{"text": "Word embeddings have recently proven useful in a number of different applications that deal with natural language.", "labels": [], "entities": []}, {"text": "Such embeddings succinctly reflect semantic similarities between words based on their sentence-internal contexts in large corpora.", "labels": [], "entities": []}, {"text": "In this paper, we show that information extraction techniques provide valuable additional evidence of semantic relationships that can be exploited when producing word embeddings.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7947385013103485}]}, {"text": "We propose a joint model to train word embeddings both on regular context information and on more explicit semantic extractions.", "labels": [], "entities": []}, {"text": "The word vectors obtained from such an augmented joint training show improved results on word similarity tasks, suggesting that they can be useful in applications that involve word meanings.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7923969030380249}]}], "introductionContent": [{"text": "In recent years, the idea of embedding words in a vector space has gained enormous popularity.", "labels": [], "entities": []}, {"text": "This success of such word embeddings as semantic representations has been driven in part by the development of novel methods to efficiently train word vectors from large corpora, such that words with similar contexts end up having similar vectors.", "labels": [], "entities": []}, {"text": "While it is indisputable that context plays a vital role in meaning acquisition, it seems equally plausible that some contexts would be more helpful for this than others.", "labels": [], "entities": [{"text": "meaning acquisition", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8174181878566742}]}, {"text": "Consider the following sentence, taken from This research was partially funded by Wikipedia, a commonly used training corpus for word representation learning: Although Roman political authority in the West was lost, Roman culture would last inmost parts of the former Western provinces into the 6th century and beyond.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.8376522660255432}]}, {"text": "In this example sentence, the token \"parts\" does not seem to bear any particularly close relationship with the meaning of some of the other tokens, e.g. \"Roman\" and \"culture\".", "labels": [], "entities": []}, {"text": "In contrast, the occurrence of an expression such as \"Greek and Roman mythology\" in a corpus appears to indicate that the two tokens \"Roman\" and \"Greek\" likely share certain commonalities.", "labels": [], "entities": []}, {"text": "There is a large body of work on information extraction techniques to discover text patterns that reflect semantic relationships.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7684146165847778}]}, {"text": "In this paper, we propose injecting semantic information into word embeddings by training them not just on general contexts but paying special attention to stronger semantic connections that can be discovered in specific contexts on the Web or in corpora.", "labels": [], "entities": []}, {"text": "In particular, we investigate mining information of this sort from enumerations and lists, as well as from definitions.", "labels": [], "entities": []}, {"text": "Our training procedure can exploit any source of knowledge about pairs of words being strongly coupled to improve over word embeddings trained just on generic corpus contexts.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to investigate the impact of extractions on word embeddings, we conduct an empirical analysis based on semantic relatedness assessments.", "labels": [], "entities": []}, {"text": "We use the wordsim-353 () and MEN () datasets to evaluate the semantic similarities reflected in the final word embeddings.", "labels": [], "entities": [{"text": "MEN () datasets", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.7881556351979574}]}, {"text": "Wordsim-353 and MEN are datasets of English word pairs with human-assigned similarity judgements.", "labels": [], "entities": [{"text": "Wordsim-353", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8577097654342651}]}, {"text": "They are often used to train or test semantic similarity measures of words.", "labels": [], "entities": []}, {"text": "We calculate the cosine distance of word embeddings for the word pairs in wordsim-353 and MEN and compare them to the scores from human annotations.", "labels": [], "entities": [{"text": "MEN", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.7010358572006226}]}, {"text": "shows the Spearman's correlation coefficients for the wordsim-353 dataset.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.6190119683742523}, {"text": "wordsim-353 dataset", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.9457310736179352}]}, {"text": "Even fora learning rate \u03b1 as low as 0.001 for the additional threads, we can obtain some improvement over the CBOW baseline (which corresponds to an \u03b1 setting of 0.0).", "labels": [], "entities": [{"text": "CBOW baseline", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.6530973315238953}]}, {"text": "As \u03b1 increases, the result gets better.", "labels": [], "entities": []}, {"text": "The best result we get for synonyms and definitions is 0.706, while for lists from UKWaC, it is 0.693.", "labels": [], "entities": [{"text": "synonyms and definitions", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8781460920969645}, {"text": "UKWaC", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9838141202926636}]}, {"text": "The best learn- ingrate for the definitions and synonyms is 0.020, while for the list extractions, it is 0.040.", "labels": [], "entities": []}, {"text": "Both lead to noticeably better results than the CBOW baseline's correlation coefficient of 0.642.", "labels": [], "entities": [{"text": "CBOW baseline", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8589603006839752}, {"text": "correlation", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9132936596870422}]}, {"text": "Note that for large \u03b1, the augmentation performs worse than the baseline.", "labels": [], "entities": [{"text": "augmentation", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.9847924709320068}]}, {"text": "This is expected, as an overly high learning rate causes information from the related words to overwhelm the original CBOW model, leading to excessively biased final embeddings.", "labels": [], "entities": []}, {"text": "plots the results on the MEN dataset.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.9744069576263428}]}, {"text": "The best-performing learning rate is different from that for wordsim-353.", "labels": [], "entities": []}, {"text": "In particular, well-performing learning rates are slightly smaller.", "labels": [], "entities": []}, {"text": "For the definitions and synonyms, the best is 0.002, while for the lists, the best learning rate is 0.020.", "labels": [], "entities": []}, {"text": "After training jointly, we obtain higher correlation coefficients for the MEN similarity task.", "labels": [], "entities": [{"text": "correlation", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9566110968589783}, {"text": "MEN similarity task", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7572222550710043}]}, {"text": "provides a summary of the best results that we obtain on the two similarity tasks.", "labels": [], "entities": []}, {"text": "It can be seen that we obtain higher correlation coefficients for these tasks.", "labels": [], "entities": [{"text": "correlation", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.964778482913971}]}, {"text": "This suggests that the word vectors  capture more semantic properties of words and thus maybe used in applications that benefit from semantic information.", "labels": [], "entities": []}, {"text": "Finally, we plot a sample of the word embeddings obtained from the joint training with definitions and synonyms using t-SNE.", "labels": [], "entities": []}, {"text": "t-SNE is a technique for visualization of high-dimensional datasets using dimensionality reduction.", "labels": [], "entities": []}, {"text": "The perplexity of the Gaussian kernel for the t-SNE is set to 15.", "labels": [], "entities": []}, {"text": "shows a plot for 26 words: levity, lewd, and merry, and their synonyms.", "labels": [], "entities": []}, {"text": "We see that our model successfully distinguishes the different meanings of these words while reflecting semantic relationships.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Best results for the word similarity tasks", "labels": [], "entities": [{"text": "word similarity", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7486400902271271}]}]}