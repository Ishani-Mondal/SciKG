{"title": [{"text": "Enhancing the Inside-Outside Recursive Neural Network Reranker for Dependency Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.728909432888031}]}], "abstractContent": [{"text": "We propose solutions to enhance the Inside-Outside Recursive Neural Network (IORNN) reranker of Le and Zuidema (2014).", "labels": [], "entities": []}, {"text": "Replacing the original softmax function with a hierarchical softmax using a binary tree constructed by combining output of the Brown clustering algorithm and frequency-based Huffman codes, we significantly reduce the reranker's computational complexity.", "labels": [], "entities": []}, {"text": "In addition, enriching contexts used in the reranker by adding subtrees rooted at (ancestors') cousin nodes, the accuracy is increased.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9995303153991699}]}], "introductionContent": [{"text": "Using neural networks for syntactic parsing has become popular recently, thanks to promising results that those neural-net-based parsers achieved.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6923888027667999}]}, {"text": "For constituent parsing, using a recursive neural network (RNN) got an F1 score close to the state-of-the-art on the Penn WSJ corpus.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8168342709541321}, {"text": "F1 score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9809629917144775}, {"text": "Penn WSJ corpus", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.9634692271550497}]}, {"text": "For dependency parsing, the inside-outside recursive neural net (IORNN) reranker proposed by is among the top systems, including the Chen and Manning (2014)'s extremely fast transition-based parser employing a traditional feed-forward neural network.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8329001069068909}]}, {"text": "There are many reasons why neural-net-based systems perform very well.", "labels": [], "entities": []}, {"text": "First, showed that using word-embeddings can lead to significant improvement for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8878596723079681}]}, {"text": "Interestingly, those neural-net-based systems can transparently and easily employ wordembeddings by initializing their networks with those vectors.", "labels": [], "entities": []}, {"text": "Second, by comparing a countbased model with their neural-net-based model on perplexity, suggested that predicting with neural nets is an effective solution for the problem of data sparsity.", "labels": [], "entities": [{"text": "predicting", "start_pos": 104, "end_pos": 114, "type": "TASK", "confidence": 0.9707565307617188}]}, {"text": "Last but not least, as showed in the work of Socher and colleagues on RNNs, e.g., neural networks are capable of 'semantic transfer', which is essential for disambiguation.", "labels": [], "entities": [{"text": "semantic transfer", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.7384631633758545}]}, {"text": "We focus on how to enhance the IORNN reranker of by both reducing its computational complexity and increasing its accuracy.", "labels": [], "entities": [{"text": "IORNN reranker", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.8106265068054199}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.998181939125061}]}, {"text": "Although this reranker is among the top systems inaccuracy, its computation is very costly due to its softmax function used to compute probabilities of generating tokens: all possible words in the vocabulary are taken into account.", "labels": [], "entities": []}, {"text": "Solutions for this are to approximate the original softmax function by using a hierarchical softmax), noise-contrastive estimation, or factorization using classes.", "labels": [], "entities": []}, {"text": "A cost of using those approximations is, however, drop of the system performance.", "labels": [], "entities": []}, {"text": "To reduce the drop, we use a hierarchical softmax with a binary tree constructed by combining Brown clusters and Huffman codes.", "labels": [], "entities": []}, {"text": "We show that, thanks to the reranking framework and the IORNN's ability to overcome the problem of data sparsity, more complex contexts can be employed to generate tokens.", "labels": [], "entities": []}, {"text": "We introduce anew type of contexts, named full-history.", "labels": [], "entities": []}, {"text": "By employing both the hierarchical softmax and the new type of context, our new IORNN reranker has significantly lower complexity but higher accuracy than the original reranker.", "labels": [], "entities": [{"text": "IORNN reranker", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.8758895695209503}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9989712238311768}]}], "datasetContent": [{"text": "We use the same setting reported in Le and Zuidema (2014, Section 5.3).", "labels": [], "entities": []}, {"text": "The Penn WSJ Treebank is converted to dependencies using the Yamada and Matsumoto (2003)'s head rules.", "labels": [], "entities": [{"text": "Penn WSJ Treebank", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9406814575195312}]}, {"text": "Sections 2-21 are for training, section 22 for development, and section 23 for testing.", "labels": [], "entities": []}, {"text": "The development and test sets are tagged by the Stanford POStagger trained on the whole training data, whereas 10-way jackknifing is used to generate tags for the training set.", "labels": [], "entities": [{"text": "Stanford POStagger", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.728571891784668}]}, {"text": "For the new IORNN reranker, we set n = 200, initialise it with the 50-dim word embeddings from.", "labels": [], "entities": [{"text": "IORNN reranker", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.9253379106521606}]}, {"text": "We use the MSTParser (with the 2nd-order feature mode)) to generate k-best lists, and optimize k and \u03b1 (Equation 3) on the development set.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 11, "end_pos": 20, "type": "DATASET", "confidence": 0.9322494268417358}]}, {"text": "shows the comparison of our new reranker against other systems.", "labels": [], "entities": []}, {"text": "It is a surprise that our reranker with the proposed hierarchical softmax alone can achieve an almost equivalent score with Le and Zuidema's reranker.", "labels": [], "entities": []}, {"text": "We conjecture that drawbacks of the hierarchical softmax compared to the original can be lessened by probabilities of generating other elements like POS-tags, System UAS MSTParser (baseline) 92.06 93.04 93.06 93.07 93.39 Reranking 93.12 93.12 Our reranker (h-softmax only, k = 45) 93.10 Our reranker (k = 47) 93.27: Comparison with other systems on section 23 (excluding punctuation).", "labels": [], "entities": [{"text": "UAS MSTParser (baseline) 92.06 93.04 93.06 93.07 93.39 Reranking 93.12 93.12", "start_pos": 166, "end_pos": 242, "type": "DATASET", "confidence": 0.7913097991393163}]}, {"text": "Adding enriched contexts, our reranker achieves the second best accuracy among those systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9989839196205139}]}, {"text": "Because in this experiment no words have paths longer than 20 n = 200, our new reranker has a significantly lower complexity than the one of Le and Zuidema's reranker.", "labels": [], "entities": []}, {"text": "On a computer with an Intel Core-i5 3.3GHz CPU and 8GB RAM, it takes 20 minutes to train this reranker, which is implemented in C++, and 2 minutes to evaluate it on the test set.", "labels": [], "entities": []}], "tableCaptions": []}