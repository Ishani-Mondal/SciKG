{"title": [{"text": "Interpreting Questions with a Log-Linear Ranking Model in a Virtual Patient Dialogue System", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a log-linear ranking model for interpreting questions in a virtual patient dialogue system and demonstrate that it substantially outperforms a more typical multiclass classifier model using the same information.", "labels": [], "entities": []}, {"text": "The full model makes use of weighted and concept-based matching features that together yield a 15% error reduction over a strong lexical overlap baseline.", "labels": [], "entities": []}, {"text": "The accuracy of the ranking model approaches that of an extensively handcrafted pattern matching system, promising to reduce the authoring burden and make it possible to use confidence estimation in choosing dialogue acts; at the same time, the effectiveness of the concept-based features indicates that manual development resources can be productively employed with the approach in developing concept hierarchies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9983238577842712}]}], "introductionContent": [{"text": "In this paper, we present a log-linear ranking model for interpreting questions in a virtual patient dialogue system, along with initial experiments to determine effective sets of features with this model.", "labels": [], "entities": [{"text": "interpreting questions", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8871890902519226}]}, {"text": "Learning to take a medical history is fundamental to becoming a successful physician.", "labels": [], "entities": []}, {"text": "Most methods for assessing history taking skills involve interaction with Standardized Patients (SP) who are actors portraying real patients.", "labels": [], "entities": [{"text": "history taking", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.8393338918685913}]}, {"text": "SP interviews are effective, but they require significant faculty effort and institutional support.", "labels": [], "entities": [{"text": "SP interviews", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9383537769317627}]}, {"text": "As an alternative, virtual standardized patients (VSPs) can be valuable tools that offer a practical and accessible means of simulating standardized patient encounters.", "labels": [], "entities": []}, {"text": "VSP simulations have the potential to allow students to practice their communication and history taking skills before working with Standardized Patients.", "labels": [], "entities": [{"text": "history taking", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.7793058753013611}]}, {"text": "Students can rehearse interviewing skills in a risk-free environment, providing additional opportunities for practice prior to standardized or real-world patient encounters.", "labels": [], "entities": []}, {"text": "Our VSP system closely models the interaction between doctors and patients.", "labels": [], "entities": []}, {"text": "Our virtual patients are avatars representing standardized patients that students can interview and communicate with using natural language.", "labels": [], "entities": []}, {"text": "Students take a medical history and develop a differential diagnosis of the virtual standardized patient, much as they would a standardized or actual patient.", "labels": [], "entities": []}, {"text": "As shown in, the dialogue system is embedded in an immersive learning environment designed to provide student doctors with a sense of presence, allowing them to \"suspend disbelief\" and behave as if the virtual patient is areal patient.", "labels": [], "entities": []}, {"text": "The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices.", "labels": [], "entities": []}, {"text": "The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems ().", "labels": [], "entities": [{"text": "question matching", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7540312707424164}, {"text": "question answering", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.7656837403774261}]}, {"text": "This approach allows for easier authoring than, for example, systems that use deep natural language understanding () or semantic parsing, and yet still achieves the desired learning objectives of the virtual patient system.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7487841248512268}]}, {"text": "To date, the VSP system has been based on the ChatScript 1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development.", "labels": [], "entities": [{"text": "ChatScript 1 pattern matching", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.5013216882944107}]}, {"text": "In an evaluation where a group of third-year medical students were asked to complete a focused history of present illness of a patient with back pain and develop a differential diagnosis, the VSP system answered 83% of the questions correctly.", "labels": [], "entities": []}, {"text": "This level of accuracy sufficed for all students to correctly identify the appropriate differential diagnosis, confirming that the virtual patient can effectively communicate and answer complaintspecific questions in a simulated encounter between a doctor and a patient (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9991846680641174}]}, {"text": "A limitation of rule-based pattern matching approaches, however, is the need to create all patterns manually and extensively test and refine the system to allow it to answer questions correctly, with no ability to use confidence estimation in making dialogue act decisions.", "labels": [], "entities": [{"text": "rule-based pattern matching", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6822682321071625}]}, {"text": "With our log-linear ranking model, we aim to substantially reduce the burden of designing new virtual patients, as well as to make it possible to use confidence estimation to decide when the system should ask the user to clarify or restate his or her question.", "labels": [], "entities": []}, {"text": "To create a corpus for developing our statistical interpretation model, the ChatScript patterns were refined to correct errors found during the evaluation and then run on a set of 32 representative dialogues, with the interpretation of all questions hand-verified for correctness.", "labels": [], "entities": [{"text": "statistical interpretation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.709877997636795}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss related work.", "labels": [], "entities": []}, {"text": "In Section 3, we present the log-linear ranking model formally, comparing it to more typical multiclass classifica- tion models.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the features we investigate in detail, with experimental results and analysis appearing in Section 5.", "labels": [], "entities": []}, {"text": "Finally, in Section 6 we conclude with a summary and discussion of avenues for future investigation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus consists of 32 dialogues, which include 918 user turns, with a mean dialogue length of 29 turns.", "labels": [], "entities": []}, {"text": "For each turn, the asked question, canonical question, current topic and a question response are Cross-fold validation is run on a per-dialogue basis.Total system accuracy is measured as the mean overall individual cross-fold accuracies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9718881845474243}]}, {"text": "Results of system accuracy by model are shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9955770969390869}]}, {"text": "The weighted, concept-based, topicbased, and lexical features model (Full-no-meteor) shows a significant improvement over the LexOverlap baseline model, using a McNemar paired chisquare test (chi-square=16.5, p=4.86e-05).", "labels": [], "entities": [{"text": "LexOverlap baseline", "start_pos": 126, "end_pos": 145, "type": "DATASET", "confidence": 0.968664139509201}]}, {"text": "At an overall accuracy of 78.6%, this represents an error reduction of 15% over the baseline and approaches the performance of the handcrafted patterns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9994969367980957}, {"text": "error", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.972987174987793}]}, {"text": "Of interest, the LexOverlap+concept shows a significant improvement over LexOverlap alone (chi-square=18.3, p=1.95e-05).", "labels": [], "entities": [{"text": "LexOverlap", "start_pos": 73, "end_pos": 83, "type": "DATASET", "confidence": 0.953294575214386}]}, {"text": "Meteor features do not show a significant difference when comparing the Full vs. Full-no-meteor model (chi-square=3.2, p=.073), indicating that the concept-based features largely suffice to supply the information provided by WordNet synsets and pivot-method paraphrases in Meteor.", "labels": [], "entities": []}, {"text": "Training with variants as acceptable matches is a useful strategy for this domain, reducing error by 47%, as compared to training without variants.", "labels": [], "entities": [{"text": "error", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9978616833686829}]}, {"text": "This allows for comparison attest time to not only the   canonical version of a question, but also each correct variant of the canonical version.", "labels": [], "entities": []}, {"text": "Matching the correct canonical question or any of its variants results in a correct system response.", "labels": [], "entities": []}, {"text": "In addition, accuracy is higher in cases where the model is most confident, suggesting that confidence can be successfully employed to trigger useful clarification requests, and that training with question variants acquired in previous dialogues yields a large reduction in error.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9995481371879578}, {"text": "error", "start_pos": 274, "end_pos": 279, "type": "METRIC", "confidence": 0.9854387044906616}]}, {"text": "Lastly, an error analysis reveals that many question interpretation errors yield matches that are close enough for the purposes of the dialogue, though some errors remain that reflect misleading lexical overlap, lack of world knowledge or the lack of a dedicated anaphora resolution component.", "labels": [], "entities": [{"text": "question interpretation", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7298615574836731}, {"text": "anaphora resolution", "start_pos": 263, "end_pos": 282, "type": "TASK", "confidence": 0.7899784743785858}]}, {"text": "A measure of system confidence can be obtained from test items' probabilities, and can be compared to accuracy to show that higher confidence system responses are more accurate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.999470055103302}]}, {"text": "Confidence is defined as follows: In, test items' answer probability is binned by decile.", "labels": [], "entities": [{"text": "Confidence", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9765893816947937}]}, {"text": "Mean response accuracy is then calculated for each bin of test items.", "labels": [], "entities": [{"text": "Mean response accuracy", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.7644708951314291}]}, {"text": "Future work will use confidence to make discourse management decisions, such as when to answer a question, ask for clarification between close candidates, or give a generic response.", "labels": [], "entities": []}, {"text": "Additionally, higher system accuracy is possible if the system is limited to answering higher confidence quantiles.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9951625466346741}]}, {"text": "As an alternative to the log-linear ranking model employed here, a baseline multiclass classifier 8 trained on 1-to 3-gram word and stem indicator features obtains an accuracy of 67%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9994562268257141}]}, {"text": "The ranking system performs better when trained on essentially the same information (LexOverlap), with 75% accuracy.", "labels": [], "entities": [{"text": "LexOverlap", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.8224998712539673}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9988248944282532}]}, {"text": "A ranking model using SVMRank) was also tried, but performance (not shown) was similar to the log-linear model.", "labels": [], "entities": [{"text": "SVMRank", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8270494341850281}]}, {"text": "Future work might explore other machine learning models such as neural networks.", "labels": [], "entities": []}, {"text": "System errors largely fall into a few categories.", "labels": [], "entities": []}, {"text": "First, some responses are actually acceptable, but reported as incorrect due to a topic mismatch.", "labels": [], "entities": []}, {"text": "For example, the same question have you ever had this type of pain before could be labeled as have you ever had this pain before or have you ever had back pain before, depending on the topic.", "labels": [], "entities": []}, {"text": "If the topic was currentbackpain or currentpain, the gold label could differ.", "labels": [], "entities": [{"text": "currentbackpain", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.9344869256019592}, {"text": "currentpain", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.9385164976119995}]}, {"text": "Topics, therefore, exist at varying levels of specificity.", "labels": [], "entities": []}, {"text": "Including nearly identical questions in multiple topics promotes question reuse across virtual patients but can be a source of error if the topic is not tracked well.", "labels": [], "entities": [{"text": "question reuse", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7097508311271667}]}, {"text": "A second class of errors comes from superficially similar questions, where the most meaningful word or words in the question are not matched.", "labels": [], "entities": []}, {"text": "For example does the pain ever go away vs. does rest make the pain go away would have high lexical overlap, but this does not reflect the fact that the most informative words do not match.", "labels": [], "entities": []}, {"text": "Interestingly, we expect that questions that match primarily on common n-grams and not on rarer n-grams have relatively low confidence scores, since the common n-grams would match multiple other questions.", "labels": [], "entities": []}, {"text": "Using confidence scoring could help mitigate this error class.", "labels": [], "entities": []}, {"text": "For the previous example, the correct question is actually, is the pain constant, which highlights a third kind of error, where some inference or worldknowledge is necessary.", "labels": [], "entities": []}, {"text": "Understanding that things that go away are not constant is an entailment involving negation and is more complicated to capture than using a paraphrase resource.", "labels": [], "entities": []}, {"text": "While room exists for absolute improvement inaccuracy, the results are encouraging, given the relatively small dataset and fact that the full model approaches ChatScript pattern-matching system performance (83%).", "labels": [], "entities": []}, {"text": "Larger datasets will likely improve accuracy, but given the expense and limited availability of large corpora, we focus on exploring features that maximize limited training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9988755583763123}]}, {"text": "Annotation is in progress fora larger corpus of 100 dialogues with approximately 5500 user turns.", "labels": [], "entities": []}, {"text": "Qualitatively, the ranking system is less laborintensive than ChatScript and can use confidence values to drive dialogue act decisions, such as asking the user to rephrase, or to choose between multiple candidate question interpretations.", "labels": [], "entities": []}, {"text": "Additionally, the ranking system could potentially be combined with ChatScript to provide ranking when multiple ChatScript patterns match, or to provide a question when no existing ChatScript pattern matches the input.", "labels": [], "entities": []}, {"text": "Better anaphor resolution could help address errors from uninformative pronouns that might not match the canonical question form.", "labels": [], "entities": [{"text": "anaphor resolution", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7361746430397034}]}, {"text": "Zero-anaphors are missed by the current features and could occur in a dialogue setting such as: What medications are you taking, followed by ok, how often.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Model results, with a description of their included features", "labels": [], "entities": []}]}