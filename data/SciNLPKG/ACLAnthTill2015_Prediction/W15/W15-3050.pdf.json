{"title": [{"text": "BEER 1.1: ILLC UvA submission to metrics and tuning task", "labels": [], "entities": [{"text": "BEER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9234597682952881}]}], "abstractContent": [{"text": "We describe the submissions of ILLC UvA to the metrics and tuning tasks on WMT15.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9405739903450012}]}, {"text": "Both submissions are based on the BEER evaluation metric originally presented on WMT14 (Stanojevi\u00b4cStanojevi\u00b4c and Sima'an, 2014a).", "labels": [], "entities": [{"text": "BEER", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9962882995605469}, {"text": "WMT14", "start_pos": 81, "end_pos": 86, "type": "DATASET", "confidence": 0.9707150459289551}]}, {"text": "The main changes introduced this year are: (i) extending the learning-to-rank trained sentence level metric to the corpus level (but still decom-posable to sentence level), (ii) incorporating syntactic ingredients based on dependency trees, and (iii) a technique for finding parameters of BEER that avoid \"gam-ing of the metric\" during tuning.", "labels": [], "entities": [{"text": "BEER", "start_pos": 289, "end_pos": 293, "type": "METRIC", "confidence": 0.9642841815948486}]}], "introductionContent": [{"text": "In the 2014 WMT metrics task, BEER turned up as the best sentence level evaluation metric on average over 10 language pairs).", "labels": [], "entities": [{"text": "WMT metrics task", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.6772199074427286}, {"text": "BEER", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9990059733390808}]}, {"text": "We believe that this was due to: 1.", "labels": [], "entities": []}, {"text": "learning-to-rank -type of training that allows a large number of features and also training on the same objective on which the model is going to be evaluated : ranking of translations 2.", "labels": [], "entities": []}, {"text": "dense features -character n-grams and skipbigrams that are less sparse on the sentence level than word n-grams 3.", "labels": [], "entities": []}, {"text": "permutation trees -hierarchical decomposition of word order based on ( A deeper analysis of is presented in (Stanojevi\u00b4cjevi\u00b4c and Sima'an, 2014c) and of (3) in.", "labels": [], "entities": []}, {"text": "Here we modify BEER by. incorporating a better scoring function that give scores that are better scaled 2.", "labels": [], "entities": [{"text": "BEER", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9907683730125427}]}, {"text": "including syntactic features and 3.", "labels": [], "entities": []}, {"text": "removing the recall bias from BEER . In Section 2 we give a short introduction to BEER after which we move to the innovations for this year in Sections 3, 4 and 5.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9986849427223206}, {"text": "BEER", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.7069266438484192}, {"text": "BEER", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.6965255737304688}]}, {"text": "We show the results from the metric and tuning tasks in Section 6, and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Tuning results with BEER without bias  on WMT14 as tuning and WMT13 as test set", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.982754647731781}, {"text": "BEER", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9995551705360413}, {"text": "WMT14", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8678238987922668}, {"text": "WMT13", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.8160248398780823}]}, {"text": " Table 6: Results on Czech-English tuning", "labels": [], "entities": [{"text": "Czech-English tuning", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.8995401859283447}]}, {"text": " Table 2: System-level correlations of automatic evaluation metrics and the official WMT human scores  when translating into English.", "labels": [], "entities": [{"text": "WMT human scores", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.6439052224159241}]}, {"text": " Table 3: System-level correlations of automatic evaluation metrics and the official WMT human scores  when translating out of English.", "labels": [], "entities": [{"text": "WMT human scores", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.6237585544586182}]}]}