{"title": [{"text": "Sentence selection for automatic scoring of Mandarin proficiency", "labels": [], "entities": [{"text": "Sentence selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9003703594207764}]}], "abstractContent": [{"text": "A central problem in research on automatic proficiency scoring is to differentiate the variability between and within groups of standard and non-standard speakers.", "labels": [], "entities": [{"text": "proficiency scoring", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8255700767040253}]}, {"text": "Along with the effort to improve the robustness of techniques and models, we can also select test sentences that are more reliable for measuring the between-group variability.", "labels": [], "entities": []}, {"text": "This study demonstrated that the performance of an automatic scoring system could be significantly improved by excluding \"bad\" sentences from the scoring procedure.", "labels": [], "entities": []}, {"text": "The experiments on a dataset of Putonghua Shuiping Ceshi (Mandarin proficiency test) showed that, compared to all available sentences, using only best-performed sentences improved the speaker-level correlation between human and automatic scores from r = .640 tor = .824.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic scoring of spoken language proficiency has been widely applied in language tests and computer assisted language learning (CALL) (.", "labels": [], "entities": [{"text": "computer assisted language learning (CALL)", "start_pos": 95, "end_pos": 137, "type": "TASK", "confidence": 0.7072837012154716}]}, {"text": "A central problem in this research area is to differentiate the variability between and within groups of standard and non-standard speakers.", "labels": [], "entities": []}, {"text": "One way to tackle the problem is, as done inmost previous studies, to improve the robustness and reliability of techniques and models.", "labels": [], "entities": [{"text": "reliability", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9595672488212585}]}, {"text": "There is also another way to look at the problem: not every sentence is equally good for revealing a speaker's language proficiency.", "labels": [], "entities": []}, {"text": "The purpose of this study is to demonstrate that, given an automatic scoring technique, we can significantly improve the performance of the technique by selecting well-performed sentences (with respect to the given technique) as input for scoring.", "labels": [], "entities": []}, {"text": "Most of the automatic scoring systems rely on automatic speech recognition (ASR).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.7494750916957855}]}, {"text": "The common practice is to build HMM-based acoustic models using a large amount of \"standard\" speech data.", "labels": [], "entities": []}, {"text": "To assess an utterance, pronunciation scores such as log likelihood scores and posterior probabilities are calculated by performing speech recognition (or forced alignment if the sentence is known) to the utterance based on the pre-trained acoustic models ().", "labels": [], "entities": [{"text": "log likelihood scores", "start_pos": 53, "end_pos": 74, "type": "METRIC", "confidence": 0.7757677435874939}, {"text": "speech recognition", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7083009630441666}]}, {"text": "Prosody scores, e.g., duration, F 0 , and pauses, have also been shown important ().", "labels": [], "entities": [{"text": "duration", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9992305040359497}, {"text": "F 0", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9941916465759277}, {"text": "pauses", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9842295050621033}]}, {"text": "These individual scores are combined with statistical models such as linear regression, SVM, and neural network to produce an overall score for the test utterance (.", "labels": [], "entities": []}, {"text": "The performance of model-based automatic scoring systems much depends on the amount and quality of the training data.", "labels": [], "entities": []}, {"text": "For the purpose of this study, we adopted a simple, comparisonbased approach.", "labels": [], "entities": []}, {"text": "This approach is to measure the goodness of a test utterance by directly comparing it to a standard version of the same sentence and calculating the distance between the two ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Speaker-level correlations between SVM- predicted and human scores.", "labels": [], "entities": []}]}