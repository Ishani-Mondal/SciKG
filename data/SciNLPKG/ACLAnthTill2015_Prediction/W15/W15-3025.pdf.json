{"title": [{"text": "The FBK Participation in the WMT15 Automatic Post-editing Shared Task", "labels": [], "entities": [{"text": "FBK", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7833382487297058}, {"text": "WMT15 Automatic Post-editing Shared Task", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.5951318800449371}]}], "abstractContent": [{"text": "In this paper, we describe the \"FBK English-Spanish Automatic Post-editing (APE)\" systems submitted to the APE shared task at the WMT 2015.", "labels": [], "entities": [{"text": "FBK English-Spanish Automatic Post-editing (APE)\"", "start_pos": 32, "end_pos": 81, "type": "TASK", "confidence": 0.6318487439836774}, {"text": "APE shared task at the WMT 2015", "start_pos": 107, "end_pos": 138, "type": "DATASET", "confidence": 0.7365560787064689}]}, {"text": "We explore the most widely used statistical APE technique (monolingual) and its most significant variant (context-aware).", "labels": [], "entities": [{"text": "APE", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.6076750755310059}]}, {"text": "In this exploration, we introduce some novel task-specific dense features through which we observe improvements over the default setup of these approaches.", "labels": [], "entities": []}, {"text": "We show these features are useful to prune the phrase table in order to remove unreliable rules and help the decoder to select useful translation options during decoding.", "labels": [], "entities": []}, {"text": "Our primary APE system submitted at this shared task performs significantly better than the standard APE baseline.", "labels": [], "entities": [{"text": "APE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.8001728653907776}, {"text": "APE baseline", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.7829517126083374}]}], "introductionContent": [{"text": "Over the last decade a lot of research has been carried out to mimic the human post-editing process in the field of Automatic Post-Editing (APE).", "labels": [], "entities": [{"text": "Automatic Post-Editing (APE)", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.6712749600410461}]}, {"text": "The objective of APE is to learn how to correct machine translation (MT) errors leveraging the human post-editing feedback.", "labels": [], "entities": [{"text": "APE", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.854587972164154}, {"text": "correct machine translation (MT) errors", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.7745311600821358}]}, {"text": "The variety of data generated by human feedback, in terms of post editing, possess an unprecedented wealth of knowledge about the dynamics (practical and cognitive) of the translation process.", "labels": [], "entities": [{"text": "translation process", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.917296975851059}]}, {"text": "APE leverages the potential of this knowledge to improve MT quality.", "labels": [], "entities": [{"text": "APE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7222353219985962}, {"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9971740245819092}]}, {"text": "The problem is appealing for several reasons.", "labels": [], "entities": []}, {"text": "On one side, as shown by, APE systems can improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage.", "labels": [], "entities": [{"text": "APE", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8764507174491882}, {"text": "MT output", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.9206630885601044}]}, {"text": "On the other side, APE represents the only way to rectify errors present in the \"black-box\" scenario where the MT system is unknown or its internal decoding information is not available.", "labels": [], "entities": [{"text": "APE", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.8304970264434814}, {"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.8912079334259033}]}, {"text": "The goal of the APE task is to challenge the research groups to improve the MT output quality by the use of a dataset consisting of triplets of sentences (source, MT output, human post-edition).", "labels": [], "entities": [{"text": "APE task", "start_pos": 16, "end_pos": 24, "type": "TASK", "confidence": 0.8415155112743378}, {"text": "MT output", "start_pos": 76, "end_pos": 85, "type": "TASK", "confidence": 0.8912947475910187}]}, {"text": "We are facing the \"MT-as-Black-box\" scenario, so neither we have access to the MT engine nor do we have any decoding trace.", "labels": [], "entities": [{"text": "MT-as-Black-box\"", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7617060244083405}, {"text": "MT engine", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.7951735556125641}]}, {"text": "The data for this pilot task belongs to generic news domain which reflects data sparseness, and the post-edition of the MT output is obtained through crowdsourcing which makes it vulnerable to noise thus making this task even more challenging.", "labels": [], "entities": [{"text": "MT", "start_pos": 120, "end_pos": 122, "type": "TASK", "confidence": 0.8618860244750977}]}, {"text": "To begin with, \u00a72 discusses the statistical APE methods used to implement the APE systems.", "labels": [], "entities": [{"text": "APE", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.800223708152771}]}, {"text": "\u00a73 describes the data set available for this shared task, and provides detail of the experimental setup.", "labels": [], "entities": []}, {"text": "\u00a74 is our major contribution which discusses the FBK-APE pipeline and shows that incorporation of task-specific dense features can be useful to enhance APE systems.", "labels": [], "entities": [{"text": "FBK-APE pipeline", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.8719680905342102}, {"text": "APE", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.9859375357627869}]}, {"text": "Our final submitted system is reported in \u00a75 followed by conclusion in \u00a76.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: In this shared task we are provided with a tri-parallel corpus consisting of source (src), MT output (mt), and human post-edits (pe).", "labels": [], "entities": []}, {"text": "While APE-1 uses only the last two elements of the triplet, all of them are used in the context-aware APE-2.", "labels": [], "entities": [{"text": "APE-1", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.7052451372146606}]}, {"text": "To obtain joint representation (f #e) in APE-2, word alignment model is trained on src-mt parallel corpus of the training data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.8209697306156158}]}, {"text": "The training set consist of \u223c11K triplets, we divide the development set into dev and test set consisting of 500 triplets each.", "labels": [], "entities": []}, {"text": "Our evaluation is based on the performance achieved on this test set.", "labels": [], "entities": []}, {"text": "We tokenize the data set using the tokenizer available in the MOSES( toolkit.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.8481547236442566}]}, {"text": "Training and evaluation of our APE systems are performed on the true-case data.", "labels": [], "entities": [{"text": "APE", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9073258638381958}]}, {"text": "Experiment Settings: To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES(.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation toolkit MOSES", "start_pos": 59, "end_pos": 117, "type": "TASK", "confidence": 0.6067089686791102}]}, {"text": "For all the experiments mentioned in this paper we use \"grow-diag-final-and\" as alignment heuristic and \"msd-bidirectional-fe\" heuristic for reordering model.", "labels": [], "entities": []}, {"text": "MGIZA++ () is used for word alignment.", "labels": [], "entities": [{"text": "MGIZA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.740709125995636}, {"text": "word alignment", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.8117655515670776}]}, {"text": "The APE systems are tuned to optimize TER() with MERT.", "labels": [], "entities": [{"text": "TER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.996209979057312}, {"text": "MERT", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9056457281112671}]}, {"text": "We follow an incremental strategy to develop the APE systems, at each stage of the APE pipeline we find the best configuration of a component and then proceed to explore the next component.", "labels": [], "entities": []}, {"text": "Our APE pipeline consist of various stages like language model selection, phrase table pruning, and feature designing as discussed in the following sections.", "labels": [], "entities": [{"text": "APE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8720178008079529}, {"text": "language model selection", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6427761713663737}, {"text": "feature designing", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.705217644572258}]}, {"text": "Evaluation Metric: We select TER () as our evaluation metric because it mimics the human post-editing effort by measuring the edit operation needed to translate the MT output into its human-revised version.", "labels": [], "entities": [{"text": "TER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9941199421882629}]}, {"text": "Apart from TER as an evaluation metric we also compute number of sentences being modified 1 in the test set and then compute the precision as follow: Precision = N umberof SentencesImproved N umberof SentencesM odif ied Baseline: Our baseline is the MT output asis.", "labels": [], "entities": [{"text": "TER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9937870502471924}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9989940524101257}, {"text": "Precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9862561821937561}]}, {"text": "To evaluate, we use the corresponding human post-edited corpus which gives us 23.10 TER score.", "labels": [], "entities": [{"text": "TER score", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9737440347671509}]}], "tableCaptions": [{"text": " Table 1: Performance (TER score) of the APE sys- tems using various LMs", "labels": [], "entities": [{"text": "Performance (TER score)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8595588326454162}, {"text": "APE sys- tems", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.7543893158435822}]}, {"text": " Table 1. We notice that the performance of the  APE systems do not show much variation for dif- ferent LMs. This can come from the fact that the  news commentary and new crawl data might not  resemble well the shared task data. For this rea- son, the in-domain LM1 is selected and used in  the next stages.", "labels": [], "entities": []}, {"text": " Table 2: Performance (TER score) of the APE-1- LM1 after pruning at various threshold values", "labels": [], "entities": [{"text": "Performance (TER score)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8667022228240967}, {"text": "APE-1- LM1", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.5107575158278147}]}, {"text": " Table 3: Performance (TER score) of the APE-2- LM1 after pruning at various threshold values", "labels": [], "entities": [{"text": "Performance (TER score)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8664013862609863}, {"text": "APE-2", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.6551980376243591}]}, {"text": " Table 4: Performance (TER score) of the APE-1- LM1-Prun0.4 for different features", "labels": [], "entities": [{"text": "TER score)", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9211380084355673}, {"text": "APE-1- LM1-Prun0.4", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.5714264710744222}]}, {"text": " Table 5: Performance (TER score) of the APE-2- LM1-Prun0.2 for different features", "labels": [], "entities": [{"text": "TER score)", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9547915458679199}, {"text": "APE-2- LM1-Prun0.2", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.5729647179444631}]}, {"text": " Table 6: APE shared task evaluation score (TER)", "labels": [], "entities": [{"text": "APE shared task evaluation score (TER)", "start_pos": 10, "end_pos": 48, "type": "METRIC", "confidence": 0.7827941887080669}]}]}