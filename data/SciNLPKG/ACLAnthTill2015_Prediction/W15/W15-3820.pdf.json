{"title": [{"text": "Evaluating distributed word representations for capturing semantics of biomedical concepts", "labels": [], "entities": [{"text": "Evaluating distributed word representations", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7642257809638977}]}], "abstractContent": [{"text": "Recently there is a surge in interest in learning vector representations of words using huge corpus in unsupervised manner.", "labels": [], "entities": []}, {"text": "Such word vector representations, also known as word embedding, have been shown to improve the performance of machine learning models in several NLP tasks.", "labels": [], "entities": []}, {"text": "However efficiency of such representation has not been systematically evaluated in biomedical domain.", "labels": [], "entities": []}, {"text": "In this work our aim is to compare the performance of two state-of-the-art word embedding methods, namely word2vec and GloVe on a basic task of reflecting semantic similarity and relatedness of biomedical concepts.", "labels": [], "entities": [{"text": "reflecting semantic similarity and relatedness of biomedical concepts", "start_pos": 144, "end_pos": 213, "type": "TASK", "confidence": 0.7686392366886139}]}, {"text": "For this, vector representations of all unique words in the corpus of more than 1 million full-length research articles in biomedical domain are obtained from the two methods.", "labels": [], "entities": []}, {"text": "These word vectors are evaluated for their ability to reflect semantic similarity and semantic related-ness of word-pairs in a benchmark data set of manually curated semantic similar and related words available at http:// rxinformatics.umn.edu.", "labels": [], "entities": []}, {"text": "We observe that parameters of these models do affect their ability to capture lexico-semantic properties and word2vec with particular language modeling seems to perform better than others.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the crucial step in machine learning (ML) based NLP models is how we represent word as an input to our model.", "labels": [], "entities": []}, {"text": "Most of earlier works were treating word as atomic symbol and were assigning one hot vector to each word.", "labels": [], "entities": []}, {"text": "Length of the vector in this representation was equal to the size of the vocabulary and the element at the word index is 1 while the other elements are 0s.", "labels": [], "entities": [{"text": "Length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9868764877319336}]}, {"text": "Two major drawbacks with this representation are: first, length of the vector is huge and the second, there is no notion of similarity between words.", "labels": [], "entities": [{"text": "length", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.977207601070404}]}, {"text": "The inability of one-hot vector representation to embody lexico-semantic properties prompted researchers to develop methods which are based on the notion that the \"similar words appear in similar contexts\".", "labels": [], "entities": []}, {"text": "These methods can broadly be classified into two categories, namely, distributional representation and distributed representation.", "labels": [], "entities": [{"text": "distributional representation", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.6987959146499634}]}, {"text": "Both group of methods works in unsupervised manner with huge corpus.", "labels": [], "entities": []}, {"text": "Distributional representations are mainly based on co-occurrence matrix O of words in the vocabulary and their contexts.", "labels": [], "entities": []}, {"text": "Here, among other possibilities, contexts can be documents or words within a particular window.", "labels": [], "entities": []}, {"text": "Each entry O ij in the matrix may indicate either frequency of word i in the context j or simply whether the word i has appeared in the context j at least once.", "labels": [], "entities": []}, {"text": "Co-occurrence matrix can be designed in variety of ways).", "labels": [], "entities": []}, {"text": "The major issue with such methods is size of the matrix O and reducing its size generally tends to be computationally very expensive.", "labels": [], "entities": []}, {"text": "Nevertheless, the requirement of constructing and storing the matrix O are always there.", "labels": [], "entities": []}, {"text": "The second group of methods are mainly based on language modeling (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7048007547855377}]}, {"text": "We discuss more about these methods in the section 3.", "labels": [], "entities": []}, {"text": "Outside the biomedical domain, this kind of representation has shown significant improvement in the performance of many NLP tasks.", "labels": [], "entities": []}, {"text": "For example, have improved the performance of chunking and named entity recognition by using word embedding also as one of the features in their CRF model.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6152399281660715}]}, {"text": "In one study, have formulated the NLP tasks of parts of speech tagging, chunking, named entity recognition and semantic role labeling as multi-task learning problem.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.7312915176153183}, {"text": "named entity recognition", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.6108735998471578}, {"text": "semantic role labeling", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.6537930170694987}]}, {"text": "They have shown improvement in the performance when word vectors are learned together with other NLP tasks.", "labels": [], "entities": []}, {"text": "improved the performance of sentiment analysis task and semantic relation classification task using recursive neural network.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.9533016880353292}, {"text": "semantic relation classification task", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.8397223800420761}]}, {"text": "One common step among these models is: learning of word embedding from huge unannotated corpus like Wikipedia, and later use them as features.", "labels": [], "entities": []}, {"text": "Motivated by the above results, we evaluate performance of the two word embedding models, word2vec) and GloVe ( for their ability to capture syntactic as well as semantic properties of words in biomedical domain.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.7260809540748596}]}, {"text": "We have used full-length articles obtained from PubMed Central (PMC) open access subset 1 as our corpus for learning word embedding.", "labels": [], "entities": [{"text": "PubMed Central (PMC) open access subset 1", "start_pos": 48, "end_pos": 89, "type": "DATASET", "confidence": 0.9258557226922777}]}, {"text": "For evaluation we have used publicly available validated reference dataset () containing semantic similarity and relatedness scores of around 500 word-pairs.", "labels": [], "entities": []}, {"text": "Our results indicate that the word2vec word embedding is capturing semantic similarity between words better than the GloVe word embedding in the biomedical domain, whereas for the task of semantic relatedness, there does not seem to be any statistical significant difference among different word-embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Pakhomov et al. have constructed a reference dataset of semantically similar and related word-pairs.", "labels": [], "entities": []}, {"text": "These words are clinical and biomedical terms obtained from control vocabularies maintained in the Unified Medical Language System(UMLS).", "labels": [], "entities": []}, {"text": "This reference dataset contains 566 pairs of UMLS concepts which were manually rated for their semantic similarity and 587 pairs of UMLS concepts for semantic relatedness.", "labels": [], "entities": []}, {"text": "We removed all pairs in which at least one word has less than 10 occurrences in the entire corpus as such words are removed while building vocabulary from the corpus.", "labels": [], "entities": []}, {"text": "After removing less frequent words in both reference sets, we obtain 462 pairs for semantic similarity having 278 unique words, and 465 pairs for semantic relatedness having 285 unique words.", "labels": [], "entities": []}, {"text": "In both cases, each concept pair is given a score in the range of 0 \u2212 1600, with higher score implies similar or more related judgments of manual annotators.", "labels": [], "entities": []}, {"text": "The semantic relatedness score span the four relatedness categories: completely unrelated, somewhat unrelated, somewhat related, closely related.", "labels": [], "entities": []}, {"text": "We generate the word vectors using the two word embedding techniques under different settings of their parameters and compare their performance in semantic similarity and relatedness tasks.", "labels": [], "entities": []}, {"text": "Dimension of word-vector is varied under the two different language models, CBOW and SKIP-GRAM, for word2vec word embedding.", "labels": [], "entities": []}, {"text": "For GloVe, only dimension of word vector is changed.", "labels": [], "entities": []}, {"text": "For each model, word vectors of 25, 50, 100, and 200 dimensions are generated.", "labels": [], "entities": []}, {"text": "Due to limited computing power, we could not go for higher dimensions.", "labels": [], "entities": []}, {"text": "For window size, we did not perform any experiment and simply considered 9 as window size for all models.", "labels": [], "entities": []}, {"text": "As discussed earlier, both reference data have provided a score for each word-pair in them.", "labels": [], "entities": []}, {"text": "We calculate cosine similarity between the two words of each pair present in the reference data using learned word vectors.", "labels": [], "entities": []}, {"text": "Now, each word pair has two scores: one given in the dataset and the other cosine similarity based on learned word vectors.", "labels": [], "entities": []}, {"text": "We calculate Pearson's correlation between these two scores.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 13, "end_pos": 34, "type": "METRIC", "confidence": 0.9693808952967325}]}, {"text": "Further we visualize a limited number of manually selected words for qualitative evaluation.", "labels": [], "entities": []}, {"text": "For this we use the t-SNE (van der tool to project our high dimensional word vectors into two-dimensional subspace.", "labels": [], "entities": []}, {"text": "t-SNE is being widely used for this purpose.: Correlation between cosine similarity and the score provided in the benchmark dataset.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.6550098955631256}]}, {"text": "shows the correlation values in all cases.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9914994835853577}]}, {"text": "We observe that increasing the dimension of word vectors improve their ability to capture semantic properties of words.", "labels": [], "entities": []}, {"text": "The above results indicate that less than d = 200 dimension will likely to be a bad choice for any NLP tasks.", "labels": [], "entities": []}, {"text": "Due to the limited computing power, we could not complete our experiments with 500 and 1000 dimensional vector representations.", "labels": [], "entities": []}, {"text": "We have also calculated the Spearman and Kendall-Tau's correlation in each case and have observed similar trends in all cases.", "labels": [], "entities": [{"text": "Spearman and Kendall-Tau's correlation", "start_pos": 28, "end_pos": 66, "type": "METRIC", "confidence": 0.4608947217464447}]}, {"text": "Skip-gram model seems to be better than both CBOW and GloVe models in the semantic similarity task for all dimensions.", "labels": [], "entities": []}, {"text": "However this does not seem to be the case with the relatedness task.", "labels": [], "entities": []}, {"text": "So we perform the statistical significance test to check whether correlation corresponding to word2vec skip-gram model is significantly higher than correlation corresponding to other two models.", "labels": [], "entities": [{"text": "statistical significance test", "start_pos": 18, "end_pos": 47, "type": "METRIC", "confidence": 0.716107189655304}]}, {"text": "In the statistical test, we evaluate the nullhypothesis \" correlation corresponding to alternate model (CBOW or GloVe) is equal to that corresponding to the skip-gram model\" at significance level \u03b1 = 0.05.", "labels": [], "entities": [{"text": "GloVe)", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.8715237379074097}]}, {"text": "We use cocor package for statistical comparison of dependent correlations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlation between cosine similarity and the score provided in the benchmark dataset.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9513789415359497}, {"text": "cosine similarity", "start_pos": 30, "end_pos": 47, "type": "METRIC", "confidence": 0.6474196016788483}]}, {"text": " Table 2: 10 Nearest neighbors of selected seed-words.", "labels": [], "entities": []}]}