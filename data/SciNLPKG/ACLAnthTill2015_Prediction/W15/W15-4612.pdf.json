{"title": [{"text": "PDTB Discourse Parsing as a Tagging Task: The Two Taggers Approach", "labels": [], "entities": [{"text": "PDTB Discourse Parsing as a Tagging Task", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7195493578910828}]}], "abstractContent": [{"text": "Full discourse parsing in the PDTB framework is a task that has only recently been attempted.", "labels": [], "entities": [{"text": "Full discourse parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5934166411558787}, {"text": "PDTB framework", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.8338993787765503}]}, {"text": "We present the Two Tag-gers approach, which reformulates the discourse parsing task as two simpler tagging tasks: identifying the relation within each sentence, and identifying the relation between each pair of adjacent sentences.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.743115246295929}]}, {"text": "We then describe a system that uses two CRFs to achieve an F1 score of 39.33, higher than the only previously existing system, at the full discourse parsing task.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.987507551908493}, {"text": "discourse parsing", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.6914569139480591}]}, {"text": "Our results show that sequential information is important for discourse relations, especially cross-sentence relations, and that a simple approach to argument span identification is enough to achieve state of the art results.", "labels": [], "entities": [{"text": "argument span identification", "start_pos": 150, "end_pos": 178, "type": "TASK", "confidence": 0.6657540599505106}]}, {"text": "We make our easy to use, easy to extend parser publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse structure is an important part of what makes a text coherent.", "labels": [], "entities": [{"text": "Discourse structure", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7826316952705383}]}, {"text": "Parts of the text are connected to one another by what is known as discourse relations, such as causality, contrast, and specification.", "labels": [], "entities": []}, {"text": "Discourse parsing is the task of automatically determining the discourse structure of a text according to a particular theory of discourse.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7960623502731323}]}, {"text": "The ability to parse an entire document is crucial for understanding its linguistic structure and the intentions of its authors.", "labels": [], "entities": []}, {"text": "Discourse parsing is a difficult task.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9082881212234497}]}, {"text": "While some discourse relations have explicit lexical cues called discourse connectives or markers, such as \"because\" and \"but\", these are often ambiguous: they may apply to more than one relation category, or they maybe used in away that has nothing to do with discourse at all.", "labels": [], "entities": []}, {"text": "In addition, many relations are not marked by connectives in text, and disambiguating these implicit relations is difficult even when it is known a relation exists.", "labels": [], "entities": []}, {"text": "Adding to the difficulty is the fact that the arguments of the relation (there are usually two, although some frameworks allow more for certain relations) do not necessarily correspond to sentences or clauses, and may not even be contiguous under some theories.", "labels": [], "entities": []}, {"text": "Over the years, multiple theories of discourse have been proposed.", "labels": [], "entities": []}, {"text": "Most recently, the Penn Discourse Treebank (PDTB) ( has been introduced, featuring hierarchical relation categories which generalize over other theories such as Rhetorical Structure Theory (RST) and SDRT, as well as a relatively large annotated corpus aligned with the WSJ section of the Penn Treebank (PTB).", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 19, "end_pos": 49, "type": "DATASET", "confidence": 0.9348368446032206}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 161, "end_pos": 194, "type": "TASK", "confidence": 0.7979330023129781}, {"text": "WSJ section", "start_pos": 269, "end_pos": 280, "type": "DATASET", "confidence": 0.9447070956230164}, {"text": "Penn Treebank (PTB)", "start_pos": 288, "end_pos": 307, "type": "DATASET", "confidence": 0.911113166809082}]}, {"text": "While the relation categories in PDTB are hierarchical, unlike RST and other frameworks, the discourse structure of a PDTB document is not fully hierarchical so that documents in general do not have a tree-like discourse structure.", "labels": [], "entities": []}, {"text": "This is a crucial detail which allows our proposed method to work on PDTB documents.", "labels": [], "entities": []}, {"text": "While there has been much work recently on disambiguating discourse relations in the PDTB, most have not been full parsing systems.", "labels": [], "entities": []}, {"text": "That is, they operate in an experimental environment where some information is given (for example, some systems disambiguate only implicit relations, where it is assumed that the arguments of the relation have been identified and that the relation is known to be implicit).", "labels": [], "entities": []}, {"text": "Full systems, in contrast, operate on unannotated text documents producing the full discourse structure of the text, including both implicit and explicit relations, and so can be realistically used in NLP applications.", "labels": [], "entities": []}, {"text": "Although not strictly parsing in the case of PTDB, such systems perform what has been called the end-to-end discourse parsing task.", "labels": [], "entities": [{"text": "discourse parsing task", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.747761865456899}]}, {"text": "Interest in full discourse parsing in the PDTB has been increasing, and it is this year's CoNLL shared task.", "labels": [], "entities": [{"text": "full discourse parsing in the PDTB", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.5949605703353882}]}, {"text": "The only work, to our knowledge, which provides end-to-end PDTB discourse parsing is (; they use a four-stage architecture where each stage carries out one subtask in identifying discourse relations (e.g., explicit or implicit).", "labels": [], "entities": [{"text": "PDTB discourse parsing", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.732550044854482}]}, {"text": "The parser is evaluated in terms of exact match and partial match.", "labels": [], "entities": [{"text": "exact match", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9622921347618103}]}, {"text": "Unlike exact match results, which are considered correct only if both the relation type and the exact span of its arguments are identified correctly, partial match results are correct as long as the relation type is correctly identified and each proposed argument shares at least one noun and verb with the true argument.", "labels": [], "entities": []}, {"text": "We believe that partial match results are best to focus on at this point in time, since current performance on exact match results is too low to be useful.", "labels": [], "entities": []}, {"text": "Many current NLP applications (such as summarization and question answering) focus on sentences or clauses anyway and would find this formulation natural.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9906641244888306}, {"text": "question answering", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8120838701725006}]}, {"text": "In this paper, we present a simple yet powerful sequential approach to PDTB discourse parsing, utilizing two CRFs and features that are designed to discriminate both explicit and implicit relations.", "labels": [], "entities": [{"text": "PDTB discourse parsing", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.8183690905570984}]}, {"text": "We surpass state-of-the-art performance with a simpler structure, less hand-crafted rules for special scenarios and with an approach that makes adding new features extremely easy.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following and other previous work, we use sections 2-21 of the PDTB as the training set, section 22 as the development set, and section 23 as the test set.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9049281477928162}]}, {"text": "Since we use an automatic parser for our syntactic features, our results are equivalent to Lin et al.'s \"Partial, Auto + EP\" overall results for partial match, and to their \"Exact, Auto + EP\" results for exact match.", "labels": [], "entities": []}, {"text": "We consider the results using gold standard parses to be less important for an end-to-end system, the main function of which is an out of the box document parsing tool.", "labels": [], "entities": []}, {"text": "The evaluation metric in all experiments, following Lin et al., is the micro-averaged F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9547703266143799}]}, {"text": "We show our final partial match results on the test set in, compared with the Lin Parser performance.", "labels": [], "entities": []}, {"text": "We also compare our approach with the results achieved by using the exact same formulation and features (other than the sequential features, of course) in two Logistic Regression classifiers, to show that the sequential approach is in fact helpful.", "labels": [], "entities": []}, {"text": "To illustrate the effect of our simplistic argument span identification rules, we also show results without span matching, where argument spans are presumed to always partially match if the sentence/sentences and relation type are correctly identified.", "labels": [], "entities": [{"text": "argument span identification", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.7117147445678711}]}, {"text": "The results of each tagger individually are shown in.", "labels": [], "entities": []}, {"text": "Note that the overall results are compared against all true relations in the document, including those that our method inherently cannot identify (hence the upper bound), while the individual tagger results are only in the context of the individual tagging task.", "labels": [], "entities": []}, {"text": "This is why the recall of the end-to-end results is smaller than the recall of either of the individual taggers.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9992175102233887}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9906094670295715}]}, {"text": "While we are focused on partial match results,: Results for each of the two taggers separately we also show exact match results in.", "labels": [], "entities": []}, {"text": "In error analysis we noticed that many of our errors on exact match arise because we include in the span another discourse connective, or an initial word like \"Eventually\" or \"Admittedly\" in a nondiscourse usage.", "labels": [], "entities": []}, {"text": "We therefore include another set of results we call \"almost-exact match\" which allows a match if there is at most one word at the beginning or the end of the span that does not match.", "labels": [], "entities": []}, {"text": "Using this less strict definition, we reach a performance that comes close to the Lin parser exact match results.", "labels": [], "entities": []}, {"text": "To emphasize how much harder it is to identify the level 2 relation types than it is to identify the level 1 classes, we also provide results on the class-level discourse parsing task in.", "labels": [], "entities": [{"text": "class-level discourse parsing", "start_pos": 149, "end_pos": 178, "type": "TASK", "confidence": 0.6488589445749918}]}], "tableCaptions": [{"text": " Table 2: Partial match results on all relations in  the PDTB. The Lin parser paper does not report  precision and recall", "labels": [], "entities": [{"text": "PDTB", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.9604750871658325}, {"text": "Lin parser paper", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.8045620322227478}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9996552467346191}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9982051849365234}]}, {"text": " Table 3: Results for each of the two taggers sepa- rately", "labels": [], "entities": [{"text": "sepa- rately", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.7152302066485087}]}, {"text": " Table 4: Exact match results on all relations in the  PDTB. The Lin parser paper does not report pre- cision and recall", "labels": [], "entities": [{"text": "PDTB", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9545913338661194}, {"text": "Lin parser paper", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.8119803667068481}, {"text": "pre- cision", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.9071898659070333}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.8758904933929443}]}, {"text": " Table 5: Results for the same task when using the  level 1 classes instead of the level 2 type relation  categories", "labels": [], "entities": []}]}