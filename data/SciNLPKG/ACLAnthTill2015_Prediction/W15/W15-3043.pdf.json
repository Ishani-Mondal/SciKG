{"title": [{"text": "UGENT-LT3 SCATE System for Machine Translation Quality Estimation", "labels": [], "entities": [{"text": "UGENT-LT3 SCATE", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5267983973026276}, {"text": "Machine Translation Quality Estimation", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.86972975730896}]}], "abstractContent": [{"text": "This paper describes the submission of the UGENT-LT3 SCATE system to the WMT15 Shared Task on Quality Estimation (QE), viz.", "labels": [], "entities": [{"text": "UGENT-LT3 SCATE", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.5434715449810028}, {"text": "WMT15 Shared Task on Quality Estimation (QE)", "start_pos": 73, "end_pos": 117, "type": "TASK", "confidence": 0.6813802421092987}]}, {"text": "English-Spanish word and sentence-level QE.", "labels": [], "entities": []}, {"text": "We conceived QE as a supervised Machine Learning (ML) problem and designed additional features and combined these with the baseline feature set to estimate quality.", "labels": [], "entities": []}, {"text": "The sentence level QE system re-uses the word level predictions of the word-level QE system.", "labels": [], "entities": []}, {"text": "We experimented with different learning methods and observe improvements over the baseline system for word-level QE with the use of the new features and by combining learning methods into ensembles.", "labels": [], "entities": []}, {"text": "For sentence-level QE we show that using a single feature based on word-level predictions can perform better than the baseline system and using this in combination with additional features led to further improvements in performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Translation (MT) Quality Estimation (QE) is the task of providing a quality indicator for unseen automatically translated sentences without relying on reference translations.", "labels": [], "entities": [{"text": "Machine Translation (MT) Quality Estimation (QE)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8265790194272995}]}, {"text": "Predicting the quality of MT output has many applications in computer-aided translation workflows that utilize MT, including error analysis (, filtering translations for human post-editing () and comparing the quality of different MT systems ().", "labels": [], "entities": [{"text": "MT output", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.8852560818195343}, {"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9258652925491333}, {"text": "error analysis", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.7356019020080566}]}, {"text": "The most common approach is to treat the QE problem as a supervised Machine Learning (ML) task, using standard regression or classification algorithms.", "labels": [], "entities": [{"text": "QE problem", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.8740798532962799}]}, {"text": "A considerable amount of related work on both word and sentence-level QE is described in the WMT shared tasks of previous years (.", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.5316867033640543}]}, {"text": "The WMT 2015 QE shared task proposes three evaluation tasks: (1) scoring and ranking sentences according to predicted post-editing effort given a source sentence and its translation; (2) predicting the individual words that require post-editing; and (3) predicting the quality at document level.", "labels": [], "entities": [{"text": "WMT 2015 QE shared task", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8490008592605591}]}, {"text": "In this paper, we describe the UGENT-LT3 SCATE submissions to task 1 (sentence-level QE) and task 2 (word-level QE).", "labels": [], "entities": [{"text": "UGENT-LT3 SCATE submissions", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.5767143766085306}]}, {"text": "Sentence-level and word-level QE are related tasks.", "labels": [], "entities": [{"text": "word-level QE", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.4265841990709305}]}, {"text": "Sentence-level QE assigns a global score to an automatically translated sentence whereas word-level QE is more fine-grained and tries to detect the problematic word sequences.", "labels": [], "entities": []}, {"text": "Therefore we first developed a word-level QE system and incorporate the word-level predictions as additional features in the sentence-level QE system.", "labels": [], "entities": []}, {"text": "The usefulness of including word-level predictions in sentence-level QE has already been demonstrated by For both tasks, we extracted additional features and combine these with the baseline feature set to estimate quality.", "labels": [], "entities": []}, {"text": "The new features try to capture either accuracy or fluency errors, where accuracy is concerned with how much of the meaning expressed in the source is also expressed in the target text, and fluency is concerned with to what extent the translation is wellformed, regardless of sentence meaning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9988194108009338}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9986535310745239}]}, {"text": "This distinction is well known in quality assessment schemes for MT).", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9863470196723938}]}, {"text": "Some of the additional features are based on ideas that were explored in previous work on QE, such as; context features for the target word and of POS tags, (, alignment context features () and adequacy and fluency indicators ( ).", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 and Section 3 give an overview of the shared task on word-level QE and sentence-level QE respectively and describe also the features we extracted, the learning methods and the additional language resources we used and the experiments we conducted.", "labels": [], "entities": []}, {"text": "Finally, in Section 4, we discuss the results we obtained and the observations we made.", "labels": [], "entities": []}], "datasetContent": [{"text": "Ina first set of experiments we compare the performance of a system using the baseline features with three systems using only a single feature ( \u00ed \u00b5\u00ed\u00b1\u009d !\"# ), that is the percentage of predicted \"BAD\" tokens in the target sentence.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1\u009d !\"# )", "start_pos": 145, "end_pos": 157, "type": "METRIC", "confidence": 0.8416201353073121}, {"text": "BAD", "start_pos": 196, "end_pos": 199, "type": "METRIC", "confidence": 0.9176064729690552}]}, {"text": "We extract this feature from three different word-level QE systems \"TiMBL m\", \"CRF m-uni\" and \"HY-BRID_2B\".", "labels": [], "entities": []}, {"text": "The performance of these sentencelevel QE systems are measured with Mean Squared Error (MSE), Squared Correlation Coefficient (\u00ed \u00b5\u00ed\u00b1\u009f ! ) and Mean Average Error (MAE), which are defined as follows: where \u00ed \u00b5\u00ed\u00b1\u0093 \u00ed \u00b5\u00ed\u00b1\u00a5 ! , \u2026 , \u00ed \u00b5\u00ed\u00b1\u0093 \u00ed \u00b5\u00ed\u00b1\u00a5 ! are the decision values predicted by LibSVM and \u00ed \u00b5\u00ed\u00b1\u00a6 ! , \u2026 , \u00ed \u00b5\u00ed\u00b1\u00a6 ! are the true values.", "labels": [], "entities": [{"text": "Mean Squared Error (MSE)", "start_pos": 68, "end_pos": 92, "type": "METRIC", "confidence": 0.912488708893458}, {"text": "Mean Average Error (MAE)", "start_pos": 142, "end_pos": 166, "type": "METRIC", "confidence": 0.9763161540031433}, {"text": "LibSVM", "start_pos": 279, "end_pos": 285, "type": "DATASET", "confidence": 0.9665534496307373}]}, {"text": "We train the systems with default values for hyper-parameters and perform evaluation on the development set provided for the sentencelevel QE task.", "labels": [], "entities": []}, {"text": "summarizes the performance of baseline features in comparison with \u00ed \u00b5\u00ed\u00b1\u0083 !\"# , which is obtained from different wordlevel QE systems.", "labels": [], "entities": []}, {"text": "In addition to the systems above, we build a final system, which uses the given reference labels to extract \u00ed \u00b5\u00ed\u00b1\u0083 !\"# (\u00ed \u00b5\u00ed\u00b1\u0083 !\"# -ReferenceLabels).", "labels": [], "entities": []}, {"text": "The purpose of building and evaluating this system is to show an upper boundary for the performance of \u00ed \u00b5\u00ed\u00b1\u0083 !\"# , as a single feature.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1\u0083 !\"#", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.6252529248595238}]}, {"text": "As a second set of experiments we enrich the baseline feature set by combining it with the additional features that are described in Section 3.1.", "labels": [], "entities": []}, {"text": "For the feature \u00ed \u00b5\u00ed\u00b1\u0083 !\"# we use the best output, coming from the system \"HYBRID_2B\".", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1\u0083", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.6597957412401835}, {"text": "HYBRID_2B", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.5436763962109884}]}, {"text": "shows the impact of the different feature sets on the overall performance.", "labels": [], "entities": []}, {"text": "We apply grid search to optimize the \u03b3, \u03b5 and C parameters using 5-fold cross validation prior to building SVM models to use for our submissions.", "labels": [], "entities": []}, {"text": "We perform sentence ranking based on the predicted HTER scores for both systems.", "labels": [], "entities": [{"text": "sentence ranking", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7539454102516174}, {"text": "HTER", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.7512143850326538}]}, {"text": "gives an overview of the performance of the two optimized systems we submit on the development set.", "labels": [], "entities": []}, {"text": "On the test set, the performance (MAE) of both of these systems was 0.14, based on the official results.", "labels": [], "entities": [{"text": "MAE)", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9858532547950745}]}], "tableCaptions": [{"text": " Table 1: Distribution of the binary labels on the  training and development set for word-level QE", "labels": [], "entities": [{"text": "QE", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.4423178732395172}]}, {"text": " Table 2: Classification performance of the best  TiMBL system, in comparison with the ensemble  systems on the development set.", "labels": [], "entities": []}, {"text": " Table 3: Sentence-level QE performance of SVM  systems using baseline features vs. \u00ed \u00b5\u00ed\u00b1\u009d !\"# extract- ed from three different systems.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1\u009d !\"# extract- ed", "start_pos": 84, "end_pos": 106, "type": "METRIC", "confidence": 0.7107158175536564}]}, {"text": " Table 4: Performance of the SVM systems on  sentence-level QE, using different feature sets", "labels": [], "entities": []}, {"text": " Table 5: Performance of the submitted sentence- level QE systems on development set, compared  with the baseline system.", "labels": [], "entities": []}]}