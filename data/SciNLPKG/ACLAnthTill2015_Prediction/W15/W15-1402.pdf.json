{"title": [{"text": "Supervised Word-Level Metaphor Detection: Experiments with Concreteness and Reweighting of Examples", "labels": [], "entities": [{"text": "Word-Level Metaphor Detection", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.6212055583794912}]}], "abstractContent": [{"text": "We present a supervised machine learning system for word-level classification of all content words in a running text as being metaphori-cal or non-metaphorical.", "labels": [], "entities": [{"text": "word-level classification of all content words in a running text", "start_pos": 52, "end_pos": 116, "type": "TASK", "confidence": 0.8161879330873489}]}, {"text": "The system provides a substantial improvement upon a previously published baseline, using re-weighting of the training examples and using features derived from a concreteness database.", "labels": [], "entities": []}, {"text": "We observe that while the first manipulation was very effective , the second was only slightly so.", "labels": [], "entities": []}, {"text": "Possible reasons for these observations are discussed.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present a set of experiments aimed at improving on previous work on the task of supervised word-level detection of linguistic metaphor in running text.", "labels": [], "entities": [{"text": "word-level detection of linguistic metaphor", "start_pos": 109, "end_pos": 152, "type": "TASK", "confidence": 0.8182461559772491}]}, {"text": "The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, fora review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (, datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (, and recent annotation efforts with other kinds of data).", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.9162391722202301}, {"text": "VU Amsterdam corpus", "start_pos": 305, "end_pos": 324, "type": "DATASET", "confidence": 0.9377639293670654}, {"text": "metaphor identification and interpretation", "start_pos": 421, "end_pos": 463, "type": "TASK", "confidence": 0.8115560114383698}]}, {"text": "Some of these data are publicly available, allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper.", "labels": [], "entities": []}, {"text": "We start with a baseline set of features and training regime from Beigman, and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations.", "labels": [], "entities": [{"text": "Beigman", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.8592061996459961}]}, {"text": "The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman, which we use as a baseline.", "labels": [], "entities": [{"text": "word-level metaphor detection", "start_pos": 187, "end_pos": 216, "type": "TASK", "confidence": 0.7796880006790161}]}], "datasetContent": [{"text": "In this study, each content-word token in a text is an instance that is classified as either a metaphor or not a metaphor.", "labels": [], "entities": []}, {"text": "We use the logistic regression classifier as implemented in the SKLL package, which is based on scikitlearn (), with F1 optimization (\"metaphor\" class).", "labels": [], "entities": [{"text": "SKLL package", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.8429772555828094}, {"text": "F1 optimization", "start_pos": 117, "end_pos": 132, "type": "METRIC", "confidence": 0.9614742994308472}]}, {"text": "Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (\"metaphor\") class.", "labels": [], "entities": [{"text": "Precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.998894989490509}, {"text": "Recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9654116034507751}, {"text": "F-1", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9955541491508484}]}, {"text": "As a baseline, we use the best performing feature set from Beigman, who investigated supervised word-level identification of metaphors.", "labels": [], "entities": [{"text": "word-level identification of metaphors", "start_pos": 96, "end_pos": 134, "type": "TASK", "confidence": 0.8011514991521835}]}, {"text": "We investigate the effect of reweighting of examples, as well as the effectiveness of features related to the notion of concreteness.", "labels": [], "entities": []}, {"text": "Given that the category distribution is generally heavily skewed towards the non-metaphor category (see), we experimented with cost-sensitive machine learning techniques to try to correct for the imbalanced class distribution ().", "labels": [], "entities": []}, {"text": "The first technique uses AutoWeight (as implemented in the auto flag in scikitlearn toolkit), where we assign weights that are inversely proportional to the class frequencies.", "labels": [], "entities": []}, {"text": "Table 2 shows the results.", "labels": [], "entities": []}, {"text": "The effect of auto-weighting on the VUA data is quite dramatic: A 14-point drop in precision is offset by a 32-point increase in recall, on average, along with a 10-point average increase in F1 score.", "labels": [], "entities": [{"text": "VUA data", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.8337568640708923}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9994024038314819}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9996476173400879}, {"text": "F1 score", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.987717479467392}]}, {"text": "The precision-recall balance for VUA data changed from P=0.: Performance of a model with AutoWeighted training examples in comparison to the unweighted baseline, in terms of Precision (P), Recall (R), and F-1 score (F) for the positive (\"metaphor\") class.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9991323351860046}, {"text": "VUA data", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.7564659118652344}, {"text": "Precision (P)", "start_pos": 174, "end_pos": 187, "type": "METRIC", "confidence": 0.9577285200357437}, {"text": "Recall (R)", "start_pos": 189, "end_pos": 199, "type": "METRIC", "confidence": 0.9546059370040894}, {"text": "F-1 score (F)", "start_pos": 205, "end_pos": 218, "type": "METRIC", "confidence": 0.9735307097434998}]}, {"text": "A-B and B-A correspond to training-testing scenarios where the system is trained on Set A and tested on Set B and vice versa, respectively.", "labels": [], "entities": []}, {"text": "All other figures report average performance across the cross-validation folds.", "labels": [], "entities": []}, {"text": "The effect on essay data is such that the average drop in precision is larger than for VUA data (19 points) while the improvement in recall is smaller (26 points).", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9973220229148865}, {"text": "VUA data", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.852280467748642}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9993321299552917}]}, {"text": "The average increase in F-1 score is about 3 points, with the maximum of up to 13 F-1 points (A-B evaluation) and a 3-point drop for B-A evaluation.", "labels": [], "entities": [{"text": "F-1 score", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9758599698543549}, {"text": "F-1", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9736141562461853}, {"text": "A-B evaluation)", "start_pos": 94, "end_pos": 109, "type": "METRIC", "confidence": 0.9191310604413351}]}, {"text": "Overall, this experiment shows that the feature set can support a radical change in the balance between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9991008043289185}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9921157956123352}]}, {"text": "When precision is a priority (as in a situation where feedback to the user is provided in the form of highlighting of the metaphorically used words, for example), it is possible to achieve nearly 70% precision, while recovering about half the metaphors.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9985491633415222}, {"text": "precision", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.9983078241348267}]}, {"text": "When recall is a priority (possibly when an overall per-essay metaphoricity rate is estimated and used as a feature in an essay scoring system), it is possible to recover about 3 out of every 4 metaphors, with about 50% precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9959946870803833}, {"text": "precision", "start_pos": 220, "end_pos": 229, "type": "METRIC", "confidence": 0.9979845285415649}]}, {"text": "For VUA data, a similar trend is observed, with somewhat worse performance, on average, than on essay data.", "labels": [], "entities": [{"text": "VUA data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.866400808095932}]}, {"text": "The performance on the VUA News and Academic data is inline with the findings for the cross-prompt generalization in the essay data, whereas Conversation and Fiction genres are more difficult for the current system.", "labels": [], "entities": [{"text": "VUA News and Academic data", "start_pos": 23, "end_pos": 49, "type": "DATASET", "confidence": 0.93850656747818}]}, {"text": "Having observed the results of the auto-weighting experiments, we conjectured that perhaps a more even balance of precision and recall can be obtained if the re-weighting gives extra weight to \"metaphor\" class, but not to the extent that the auto-weighting scheme does.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9993671774864197}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9987674951553345}]}, {"text": "In the second experiment, we tune the weight parameter using grid search on the training data (through a secondary 3-fold cross-validation within training data) to find the optimal weighting in terms of F-score (OptiWeight); the best-performing weight was then evaluated on the test data (for cross-prompt evaluations) or the test fold (crossvalidations).", "labels": [], "entities": [{"text": "F-score", "start_pos": 203, "end_pos": 210, "type": "METRIC", "confidence": 0.9982085227966309}, {"text": "OptiWeight)", "start_pos": 212, "end_pos": 223, "type": "METRIC", "confidence": 0.8850677609443665}]}, {"text": "We used the grid from 1:1 weighting up to 8:1, with increments of 0.33.", "labels": [], "entities": []}, {"text": "The first finding of note is that the optimal weighting for the \"metaphor\" class is lower than the autoweight.", "labels": [], "entities": []}, {"text": "For example, given that metaphors constitute 11-12% of instances in the essay data, the auto-weighting scheme for the A-B and B-A evaluations would choose the weights to be about 8:1, whereas the grid search settled on 3:1 when trained on prompt A and 3.33:1 when trained on prompt B.", "labels": [], "entities": []}, {"text": "A similar observation pertains to the VUA data: The auto-weighting is expected to be about 4.5:1 for News data, yet the grid search settled on 4:1, on average across folds.", "labels": [], "entities": [{"text": "VUA data", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9684257507324219}, {"text": "News data", "start_pos": 101, "end_pos": 110, "type": "DATASET", "confidence": 0.908217191696167}]}, {"text": "These observations suggest that the auto-weighting scheme might not be the optimal re-weighting strategy when optimizing for F1 score with equal importance of precision and recall.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9801730811595917}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9994996786117554}, {"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9977826476097107}]}, {"text": "shows the performance of the optimized weighting scheme.", "labels": [], "entities": []}, {"text": "For VUA data, the changes in performance are generally positive albeit slight -the F1 score increases by one point for 3 out of 4 evaluations).", "labels": [], "entities": [{"text": "VUA data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.878520280122757}, {"text": "F1 score", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9904024302959442}]}, {"text": "For essay data, it is clear that the imbalance between precision and recall is substantially reduced (from the average difference between recall and precision of 0.24 for the auto-weighted scheme to the average difference of 0.08 for the optimized weights; see column D in the.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9986854195594788}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9924750924110413}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9686981439590454}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.8396567702293396}]}, {"text": "The best effect was observed for the B-A evaluation (train onset B, test onset A) -a 6-point increase in preci-  sion compensated well for the 2-point drop in recall, relative to the auto-weighting scheme, with a resulting 4-point increase in F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9992691874504089}, {"text": "F-score", "start_pos": 243, "end_pos": 250, "type": "METRIC", "confidence": 0.9976422190666199}]}, {"text": "The worst effect was observed for the A-B evaluation, where the increase of 6 points in precision was offset by a 16-point drop in recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9988038539886475}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9983978867530823}]}, {"text": "We conclude, therefore, that a gridbased optimization of weighting can help improve the precision-recall balance of the learning system and also improve the overall score in some cases.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 88, "end_pos": 104, "type": "METRIC", "confidence": 0.9964814186096191}]}, {"text": "In this paper, we use mean concreteness scores for words as published in the large-scale norming study by.", "labels": [], "entities": []}, {"text": "The dataset has a reasonable coverage for our data; thus, 78% of tokens in Set A have a concreteness rating.", "labels": [], "entities": []}, {"text": "The ratings are real numbers on the scale of 1 through 5; for example, essentialness has the concreteness of 1.04, while sled has the concreteness of 5.", "labels": [], "entities": []}, {"text": "The representation used by the baseline system bins the continuous values into 17 bins, starting with 1 and incrementing by 0.25 (the topmost bin has words with concreteness value of 5).", "labels": [], "entities": []}, {"text": "Compared to a representation using a single continuous variable, the binned representation allows the machine-learner to provide different weights to dif- ferent bins, thus modeling a non-linear relationship between concreteness and metaphoricity.", "labels": [], "entities": []}, {"text": "Indeed, the logistic regression classifier has made precisely such use of this representation; shows the weights assigned by the classifier to the various bins, in a baseline model with unweighted examples trained on Set A data.", "labels": [], "entities": []}, {"text": "Specifically, it is clear that abstract words receive a negative weight (predict the class \"non-metaphor\"), while concreteness values above 2.5 generally receive a positive weight (apart form the top bin, which happens to have only a single word in it).", "labels": [], "entities": []}, {"text": "One potential problem with binning as above is that some of the features become quite sparse; sparseness, in turn, makes them vulnerable to overfitting.", "labels": [], "entities": []}, {"text": "Since the relationship between concreteness and feature weight is mostly monotonic (between bins 2 and 13), we experimented with defining bins that would encode various thresholds.", "labels": [], "entities": []}, {"text": "Thus, bin b5 = [2, 2.5] would fire whenever the value of the instance is at least 2 (x \u2208 [2, 5]) or whenever the value of the instance is at most 2.5 (x \u2208 [1, 2.5]); we call these theshold-up and threshold-down, respectively.", "labels": [], "entities": []}, {"text": "Thus, instead of a set of 17 binary bins coding for intervals, we now have a set of 34 binary bins coding for upward and downward thresholds.", "labels": [], "entities": []}, {"text": "The effect of this manipulation on the performance was generally small, yet this version of the concreteness feature yielded more robust performance.", "labels": [], "entities": []}, {"text": "Specifically, the finding above of a drop in A-B performance in the optimal-weighting scheme is now largely mitigated, with precision staying the same (0.58), while recall improving from 0.55 to 0.60, and the resulting F1 score going up from 0.57 to 0.59, just one point below the auto-weighted version.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9995214939117432}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9996432065963745}, {"text": "F1 score", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9837141931056976}]}, {"text": "The improved performance on B-A is preserved and even further improved, with P=0.50, R=0.62, F=0.55.", "labels": [], "entities": [{"text": "P", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9763492345809937}, {"text": "R=0.62", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9672608176867167}, {"text": "F", "start_pos": 93, "end_pos": 94, "type": "METRIC", "confidence": 0.996674656867981}]}, {"text": "For the rest of the datasets and weighting regimes, the performance was within one F-score point of the performance of the baseline feature set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9966893196105957}]}, {"text": "In this section, we present results of experiments trying to incorporate contextual information about the difference in concreteness between the adjective and its head noun (AdjN) and between the verb and its direct object (VN).", "labels": [], "entities": []}, {"text": "The intuition behind this approach is that a metaphor is often used to describe an abstract concept in more familiar, physical terms.", "labels": [], "entities": []}, {"text": "A concrete adjective modifying an abstract noun is likely to be used metaphorically (as in soft revolution or dark thought); similarly, a concrete verb with an abstract direct object is likely to be a metaphor (as in pour consolation or drive innovation).", "labels": [], "entities": []}, {"text": "(2011) introduced a method for acquiring estimates of concreteness of words automatically, and measuring difference in concreteness in AdjN and VN constructions.", "labels": [], "entities": []}, {"text": "They reported improved metaphor classification accuracies on constructed sets of AdjN and VN pairs.", "labels": [], "entities": [{"text": "metaphor classification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7775087952613831}]}, {"text": "We implemented a difference-in-concreteness feature using the values from database.", "labels": [], "entities": []}, {"text": "We parsed texts using Stanford Dependency Parser (de), and identified all instances of amod, dobj, and rcmod relations that connect an adjective to a noun (amod), a verb to its direct object (dobj), and a verb in a relative clause to its head noun (rcmod).", "labels": [], "entities": []}, {"text": "For example, in the sentence \"I read the wonderful book that you recommended,\" the following pairs would be extracted: wonderful-book (amod), read-book (dobj), and recommended-book (rcmod).", "labels": [], "entities": []}, {"text": "The difference-inconcreteness features are calculated for the adjectives and the verbs participating in the above constructions, as follows.", "labels": [], "entities": []}, {"text": "Let (adj,n) be a pair of words in the amod relation; then the value of the difference in concreteness (DC) for the adjective is given by: DC(adj) = Concr(adj) \u2212 Concr(n) (1) DC(v) for pairs (v,n) in dobj or rcmod relations is defined analogously.", "labels": [], "entities": []}, {"text": "Features based on DC apply only to adjectives and verbs participating in the eligible constructions specified above.", "labels": [], "entities": []}, {"text": "To represent the difference in concreteness information for the machine learner, we utilize the binned thresholded representation introduced in section 6.", "labels": [], "entities": []}, {"text": "shows the incremental improvement as a result of adding the DCUpDown features to the system with UPT+CUpDown.", "labels": [], "entities": [{"text": "UPT+CUpDown", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.7726537585258484}]}, {"text": "The improvement in recall and in F-score is very small -up to 0.4 F1 points on average across the evaluations.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.999840259552002}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.96292644739151}, {"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9964202642440796}]}, {"text": "The largest increase in performance is observed for the VUA Fiction data (1.8 F1 points), with increases in both precision and recall.", "labels": [], "entities": [{"text": "VUA Fiction data", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.945917010307312}, {"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9862848520278931}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9995530247688293}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9971489310264587}]}, {"text": "Since unweighted training scenario generally leads to high-precision low-recall models, an improvement in recall without drop in precision is helping the system to achieve a more balanced performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.999169111251831}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9917313456535339}]}, {"text": "shows the incremental improvements in performance when the system is trained in the auto-  weighting regime.", "labels": [], "entities": []}, {"text": "Here the effect of the difference in concreteness features is somewhat more pronounced for the essay data, with an average F1-score increase of 0.5 points, due to a 1.1 point average increase in precision along with 0.6-point drop in recall.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9987742304801941}, {"text": "precision", "start_pos": 195, "end_pos": 204, "type": "METRIC", "confidence": 0.9989784955978394}, {"text": "recall", "start_pos": 234, "end_pos": 240, "type": "METRIC", "confidence": 0.9983543157577515}]}, {"text": "Since auto-weighting generally leads to highrecall low-precision performance, improvement in precision is helping the system to achieve a more balanced performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.999444305896759}]}, {"text": "The effect of the difference in concreteness features on the performance in the optimized weighting regime) is less consistent across datasets; while we observe an improvement in precision in VUA data, the precision has dropped in the essay data, and vice versa with recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9982941746711731}, {"text": "VUA data", "start_pos": 192, "end_pos": 200, "type": "DATASET", "confidence": 0.8248073160648346}, {"text": "precision", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.9992944002151489}, {"text": "recall", "start_pos": 267, "end_pos": 273, "type": "METRIC", "confidence": 0.9974231719970703}]}], "tableCaptions": [{"text": " Table 1: The sizes of the datasets used in this study, and  the proportion of metaphors. Content tokens are nouns,  adjectives, adverbs, and verbs.", "labels": [], "entities": []}, {"text": " Table 2: Performance of a model with AutoWeighted  training examples in comparison to the unweighted base- line, in terms of Precision (P), Recall (R), and F-1 score  (F) for the positive (\"metaphor\") class. A-B and B-A cor- respond to training-testing scenarios where the system is  trained on Set A and tested on Set B and vice versa, re- spectively. All other figures report average performance  across the cross-validation folds.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 126, "end_pos": 139, "type": "METRIC", "confidence": 0.9555867612361908}, {"text": "Recall (R)", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.9543459713459015}, {"text": "F-1 score  (F)", "start_pos": 157, "end_pos": 171, "type": "METRIC", "confidence": 0.9670778989791871}]}, {"text": " Table 3: Performance of a model with optimally weighted  training examples in comparison to the auto-weighted  scheme, in terms of Precision (P), Recall (R), F-1 score  (F), and the difference between Recall and Precision (D).  A-B and B-A correspond to training-testing scenarios  where the system is trained on Set A and tested on Set  B and vice versa, respectively. All other figures report  average performance across the cross-validation folds.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 147, "end_pos": 157, "type": "METRIC", "confidence": 0.9471103996038437}, {"text": "F-1 score  (F)", "start_pos": 159, "end_pos": 173, "type": "METRIC", "confidence": 0.9364303231239319}]}, {"text": " Table 4: Performance of a model trained with unweighted  examples with and without DC (difference in concrete- ness) features.", "labels": [], "entities": [{"text": "DC", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9558693170547485}]}, {"text": " Table 5: Performance of a model trained with auto- weighted examples with and without DC (difference in  concreteness) features.", "labels": [], "entities": [{"text": "DC", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9435757398605347}]}, {"text": " Table 7: Performance the baseline model UPT+CBins  in the baseline configuration (unweighted) the  UPT+CUpDown+DCUpDown model in opti-weighted  configuration.", "labels": [], "entities": []}, {"text": " Table 8: Instances of the adjective full in Set B that  are predicted to be non-metaphors by the UPT model  trained on Set A in the unweighted regime, while the  UPT+CUpDown+DCUpDown model classifies these as  metaphors. The noun that is recognized as being in  the amod relation with full is shown in square brackets.  FULL (small caps) indicates an instance that is annotated  as a metaphor; lowercase version corresponds to a non- metaphor annotation.", "labels": [], "entities": [{"text": "UPT", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8688370585441589}, {"text": "UPT", "start_pos": 163, "end_pos": 166, "type": "DATASET", "confidence": 0.8846110105514526}, {"text": "FULL", "start_pos": 321, "end_pos": 325, "type": "METRIC", "confidence": 0.9958589673042297}]}, {"text": " Table 9: Performance of a model without any  concreteness features (UPT) and the model  UPT+CUpDown+DCUpDown,  in no-reweighting  regime (top), auto-weighting (middle), and optimal  weighting (bottom).", "labels": [], "entities": []}]}