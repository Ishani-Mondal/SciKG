{"title": [{"text": "MultiLing 2015: Multilingual Summarization of Single and Multi-Documents, On-line Fora, and Call-center Conversations", "labels": [], "entities": [{"text": "Multilingual Summarization of Single and Multi-Documents", "start_pos": 16, "end_pos": 72, "type": "TASK", "confidence": 0.8114174207051595}]}], "abstractContent": [], "introductionContent": [{"text": "Initially text-summarization research was fostered by the evaluation exercises, or tasks, at the Document Understanding and Text Analysis Conferences that started in 2001.", "labels": [], "entities": [{"text": "Document Understanding and Text Analysis Conferences", "start_pos": 97, "end_pos": 149, "type": "TASK", "confidence": 0.8363524675369263}]}, {"text": "But within the past five years a community of researchers have formed that push forward the development of text-summarization methods by creating evaluation tasks, dubbed MultiLing, that involve many languages (not just English) and/or many topical domains (not just news).", "labels": [], "entities": []}, {"text": "The MultiLing 2011 and 2013 tasks evolved into a community-driven initiative that pushes the state-of-the-art in Automatic Summarization by providing data sets and fostering further research and development of summarization systems.", "labels": [], "entities": [{"text": "Automatic Summarization", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7989345490932465}, {"text": "summarization", "start_pos": 210, "end_pos": 223, "type": "TASK", "confidence": 0.973820686340332}]}, {"text": "The aim of MultiLing () at SIGdial 2015 is the same: provide tasks for single and multi-document multilingual summarization and introduce pilot tasks to promote research in summarizing human dialog in online fora and customer call centers.", "labels": [], "entities": [{"text": "SIGdial 2015", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.7657836079597473}, {"text": "summarizing human dialog in online fora and customer call centers", "start_pos": 173, "end_pos": 238, "type": "TASK", "confidence": 0.8006789982318878}]}, {"text": "This report provides an outline of the four tasks MultiLing supported at SIGdial; specifically the objective of each task, the data sets used by each task, and the level of participation and success by the research community within the task.", "labels": [], "entities": [{"text": "SIGdial", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.7794551253318787}]}, {"text": "The remainder of the paper is organised as follows: section \u00a72 briefly presents the Multilingual Single-document Summarization task, section \u00a73 the Multilingual Multi-document summarization task, section \u00a74 the Online Forum Summarization task, section \u00a75 the Call-center Conversation summarization task, and finally we draw conclusions on the overall endeavour in section \u00a76.", "labels": [], "entities": [{"text": "Multilingual Single-document Summarization task", "start_pos": 84, "end_pos": 131, "type": "TASK", "confidence": 0.6649241372942924}, {"text": "Multilingual Multi-document summarization task", "start_pos": 148, "end_pos": 194, "type": "TASK", "confidence": 0.6095442697405815}, {"text": "Call-center Conversation summarization task", "start_pos": 259, "end_pos": 302, "type": "TASK", "confidence": 0.750524565577507}]}], "datasetContent": [{"text": "Ten teams submitted 18 systems to the MMS task.", "labels": [], "entities": [{"text": "MMS task", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.9465553164482117}]}, {"text": "Three randomly chosen topics (namely topics M001, M002, M003) out of the 15 topics, were provided as training sets to the participants for the task and were excluded when ranking of the systems.", "labels": [], "entities": []}, {"text": "The ranking was based on automatic evaluations methods using human model summaries provided by fluent speakers of each corresponding language (native speakers in the general case).", "labels": [], "entities": []}, {"text": "ROUGE variations (ROUGE-1, ROUGE-2)) and the AutoSummENG-MeMoG ( and NPowER) methods were applied to automatically evaluate the summarization systems.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.940739631652832}]}, {"text": "There was a clear indication that ROUGE measures were extremely sensitive to different preprocessing types and that different implementations (taking into account multilinguality or not during tokenization) may offer significantly different results (even different order of magnitude in the score).", "labels": [], "entities": []}, {"text": "Thus, the evaluation was based on the language-independent MeMoG method.", "labels": [], "entities": []}, {"text": "On average 12 system runs were executed per language, with the least popular language being Chinese, and the most popular being English.", "labels": [], "entities": []}, {"text": "On average across all languages, except for Chinese, 13 of the 18 systems surpassed the baseline, according to the automatic evaluation.", "labels": [], "entities": []}, {"text": "The systems employed a variety of approaches to tackle the multi-document summarization challenge as described in the following paragraphs.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.5858221352100372}]}, {"text": "The approaches contained various types of preprocessing, from POS tagging and extraction of POS patterns, to the representation of documents to language-independent latent spaces before the summarization or reduced vector spaces (e.g. through PCA).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.7800372242927551}]}, {"text": "It is also interesting to note that more than 10 different tools were used in various preprocessing steps, such as stemming, tokenization, sentence splitting, due to the language dependence limitations of many such tools.", "labels": [], "entities": [{"text": "stemming", "start_pos": 115, "end_pos": 123, "type": "TASK", "confidence": 0.9692335724830627}, {"text": "tokenization", "start_pos": 125, "end_pos": 137, "type": "TASK", "confidence": 0.9495363831520081}, {"text": "sentence splitting", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7652103900909424}]}, {"text": "Overall, in comparison to the previous MultiLing MMS challenge, this time it appears that reuse of existing tools for such preprocessing was increased (as detailed in individual system reports).", "labels": [], "entities": [{"text": "MultiLing MMS challenge", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7883749405543009}]}, {"text": "Subtopics were identified in some cases through various methods, such as the use of bag-ofword vector space representation of sentences and cosine-similarity-based clustering, or probabilistic clustering methods (e.g. hLDA).", "labels": [], "entities": []}, {"text": "For the sentence scoring, cosine similarity was also used as a means for sentence selection, where the topic(s) of a document group was projected in a vector space (either bag-of-words or latent topic space).", "labels": [], "entities": [{"text": "sentence scoring", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7712046504020691}, {"text": "sentence selection", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7491138577461243}]}, {"text": "Some of the MMS participants' systems used supervised optimization methods (e.g. polytope model optimization, genetic algorithms) on rich feature spaces to either maximize coverage of the output summaries, or train models for sentence scoring.", "labels": [], "entities": [{"text": "MMS", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9596492648124695}, {"text": "polytope model optimization", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.752882202466329}, {"text": "sentence scoring", "start_pos": 226, "end_pos": 242, "type": "TASK", "confidence": 0.7591375410556793}]}, {"text": "The feature spaces went beyond words to linguistic features, position features, etc.", "labels": [], "entities": []}, {"text": "Other systems used graph methods, relying on the \"importance\" of sentences as indicated by methods such as PageRank ().", "labels": [], "entities": []}, {"text": "Finally, redundancy was tackled through cosine similarity between sentences, or in the sentence selection process itself as penalty to optimization cost functions.", "labels": [], "entities": [{"text": "sentence selection process", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.7586376865704855}]}, {"text": "Overall, once again the multi-document, multilingual task showed that multilinguality implies a need for many linguistic resources, but is significantly helped by the application of machine learning methods.", "labels": [], "entities": []}, {"text": "It appears that these latter approaches transfer the burden to the annotation of good training corpora.", "labels": [], "entities": []}, {"text": "Four systems have been submitted to this first edition of the CCCS task, by two research groups.", "labels": [], "entities": []}, {"text": "In addition, three extractive baselines were evaluated for comparison purposes.", "labels": [], "entities": []}, {"text": "The official metric was ROUGE-2.", "labels": [], "entities": [{"text": "metric", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.97037672996521}, {"text": "ROUGE-2", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9841540455818176}]}, {"text": "Evaluation on each of the languages shows that the submitted systems had difficulties beating the extractive baselines, and that human annotators are consistent in their synopsis production (for more details see ).", "labels": [], "entities": []}, {"text": "We will focus on extending the evaluation in order to overcome the limitations of ROUGE, and assess the abstractiveness of the generated synopses.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.875472903251648}]}], "tableCaptions": []}