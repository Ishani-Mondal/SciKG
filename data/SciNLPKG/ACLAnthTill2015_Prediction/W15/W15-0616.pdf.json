{"title": [{"text": "Annotation and Classification of Argumentative Writing Revisions", "labels": [], "entities": [{"text": "Classification of Argumentative Writing Revisions", "start_pos": 15, "end_pos": 64, "type": "TASK", "confidence": 0.8131659507751465}]}], "abstractContent": [{"text": "This paper explores the annotation and classification of students' revision behaviors in argumentative writing.", "labels": [], "entities": [{"text": "classification of students' revision behaviors in argumentative writing", "start_pos": 39, "end_pos": 110, "type": "TASK", "confidence": 0.7868117988109589}]}, {"text": "A sentence-level revision schema is proposed to capture why and how students make revisions.", "labels": [], "entities": []}, {"text": "Based on the proposed schema, a small corpus of student essays and revisions was annotated.", "labels": [], "entities": []}, {"text": "Studies show that manual annotation is reliable with the schema and the annotated information helpful for revision analysis.", "labels": [], "entities": [{"text": "revision analysis", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.9773977994918823}]}, {"text": "Furthermore , features and methods are explored for the automatic classification of revisions.", "labels": [], "entities": [{"text": "automatic classification of revisions", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.698064424097538}]}, {"text": "Intrinsic evaluations demonstrate promising performance in high-level revision classification (surface vs. text-based).", "labels": [], "entities": [{"text": "revision classification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.8968524634838104}]}, {"text": "Extrinsic evaluations demonstrate that our method for automatic revision classification can be used to predict a writer's improvement.", "labels": [], "entities": [{"text": "automatic revision classification", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.6802738110224406}]}], "introductionContent": [{"text": "Rewriting is considered as an important factor of successful writing.", "labels": [], "entities": []}, {"text": "Research shows that expert writers revise in ways different from inexperienced writers.", "labels": [], "entities": []}, {"text": "Recognizing the importance of rewriting, more and more efforts are being made to understand and utilize revisions.", "labels": [], "entities": []}, {"text": "There are rewriting suggestions made by instructors (, studies modeling revisions for error correction) and tools aiming to help students with rewriting.", "labels": [], "entities": [{"text": "error correction", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.6625847965478897}]}, {"text": "While there is increasing interest in the improvement of writers' rewriting skills, there is still alack of study on the details of revisions.", "labels": [], "entities": []}, {"text": "First, to find out what has been changed (defined as revision extraction in this paper), atypical approach is to extract and analyze revisions at the word/phrase level based on edits extracted with character-level text comparison.", "labels": [], "entities": [{"text": "revision extraction", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8438362777233124}]}, {"text": "The semantic information of sentences is not considered in the character-level text comparison, which can lead to errors and loss of information in revision extraction.", "labels": [], "entities": [{"text": "revision extraction", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.9332965016365051}]}, {"text": "Second, the differentiation of different types of revisions (defined as revision categorization) is typically not fine-grained.", "labels": [], "entities": []}, {"text": "A common categorization is a binary classification of revisions according to whether the information of the essay is changed or not (e.g. text-based vs. surface as defined by).", "labels": [], "entities": []}, {"text": "This categorization ignores potentially important differences between revisions under the same high-level category.", "labels": [], "entities": []}, {"text": "For example, changing the evidence of a claim and changing the reasoning of a claim are both considered as text-based changes.", "labels": [], "entities": []}, {"text": "Usually changing the evidence makes a paper more grounded, while changing the reasoning helps with the paper's readability.", "labels": [], "entities": []}, {"text": "This could indicate different levels of improvement to the original paper.", "labels": [], "entities": []}, {"text": "Finally, for the automatic differentiation of revisions (defined as revision classification), while there are works on the classification of Wikipedia revisions, there is alack of work on revision classification in other datasets such as student writings.", "labels": [], "entities": [{"text": "automatic differentiation of revisions", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.6631199270486832}, {"text": "revision classification", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.717608854174614}, {"text": "revision classification", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.8109507858753204}]}, {"text": "It is not clear whether current features and methods can still be adapted or new features and methods are required.", "labels": [], "entities": []}, {"text": "To address the issues above, this paper makes .: In the example, words in sentence 1 of Draft 1 are rephrased and reordered to sentence 3 of Draft 2.", "labels": [], "entities": []}, {"text": "Sentences 1 and 2 in Draft 2 are newly added.", "labels": [], "entities": []}, {"text": "Our method first marks 1 and 3 as aligned and the other two sentences of Draft 2 as newly added based on semantic similarity of sentences.", "labels": [], "entities": []}, {"text": "The purposes and operations are then marked on the aligned pairs.", "labels": [], "entities": []}, {"text": "In contrast, previous work extracts differences between drafts at the character level to get edit segments.", "labels": [], "entities": []}, {"text": "The revision is extracted as a set of sentences covering the contiguous edit segments.", "labels": [], "entities": []}, {"text": "Sentence 1 in Draft 1 is wrongly marked as being modified to 1, 2, 3 in Draft 2 because character-level text comparison could not identify the semantic similarity between sentences.", "labels": [], "entities": []}, {"text": "First, we propose that it is better to extract revisions at a level higher than the character level, and in particular, explore the sentence-level.", "labels": [], "entities": []}, {"text": "This avoids the misalignment errors of character-level text comparisons.", "labels": [], "entities": []}, {"text": "Finer-grained studies can still be done on the sentence-level revisions extracted, such as fluency prediction, error correction (, statement strength identification, etc.", "labels": [], "entities": [{"text": "fluency prediction", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7842503786087036}, {"text": "error correction", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.7469068467617035}, {"text": "statement strength identification", "start_pos": 131, "end_pos": 164, "type": "TASK", "confidence": 0.6711021463076273}]}, {"text": "Second, we propose a sentence-level revision schema for argumentative writing, a common form of writing in education.", "labels": [], "entities": []}, {"text": "In the schema, categories are defined for describing an author's revision operations and revision purposes.", "labels": [], "entities": []}, {"text": "The revision operations can be directly decided according to the results of sentence alignment, while revision purposes can be reliably manually annotated.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.708824872970581}]}, {"text": "We also do a corpus study to demonstrate the utility of sentence-level revisions for revision analysis.", "labels": [], "entities": [{"text": "revision analysis", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.9682305157184601}]}, {"text": "Finally, we adapt features from Wikipedia revision classification work and explore new features for our classification task, which differs from prior work with respect to both the revision classes to be predicted and the sentence-level revision extraction method.", "labels": [], "entities": [{"text": "Wikipedia revision classification", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.5333792765935262}, {"text": "sentence-level revision extraction", "start_pos": 221, "end_pos": 255, "type": "TASK", "confidence": 0.6622795263926188}]}, {"text": "Our models are able to distinguish whether the revisions are changing the content or not.", "labels": [], "entities": []}, {"text": "For fine-grained classification, our models also demonstrate good performance for some categories.", "labels": [], "entities": [{"text": "fine-grained classification", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.5832468867301941}]}, {"text": "Beyond the classification task, we also investigate the pipelining of revision extraction and classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9656088352203369}, {"text": "revision extraction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.9421547055244446}]}, {"text": "Results of an extrinsic evaluation show that the automatically extracted and classified revisions can be used for writing improvement prediction.", "labels": [], "entities": [{"text": "writing improvement prediction", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.9232283631960551}]}], "datasetContent": [{"text": "Experiment 1: Surface vs. text-based As the corpus study in Section 3 shows that only text-based revisions predict writing improvement, our first experiment is to check whether we can distinguish between the surface and text-based categories.", "labels": [], "entities": []}, {"text": "The classification is done on all the non-identical aligned sentence pairs with Modify operations 6 . We choose 10-fold (student) cross-validation for our experi-  Experiment 2: Binary classification for each revision purpose category In this experiment, we test whether the system could identify if revisions of each specific category exist in the aligned sentence pair or not.", "labels": [], "entities": []}, {"text": "The same experimental setting for surface vs. text-based classification is applied.", "labels": [], "entities": [{"text": "surface vs. text-based classification", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.633520096540451}]}, {"text": "Experiment 3: Pipelined revision extraction and classification In this experiment, revision extraction and Experiment 1 are combined together as a pipelined approach 7 . The output of sentence alignment is used as the input of the classification task.", "labels": [], "entities": [{"text": "Pipelined revision extraction", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.7482292652130127}, {"text": "revision extraction", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8619980216026306}, {"text": "sentence alignment", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.7130379229784012}]}, {"text": "The accuracy of sentence alignment is 0.9177 on C1 and 0.9112 on C2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997881054878235}, {"text": "sentence alignment", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.719341591000557}]}, {"text": "The predicted Add and Delete revisions are directly classified as text-based changes.", "labels": [], "entities": []}, {"text": "Features are used as in Experiment 1.", "labels": [], "entities": []}, {"text": "In the intrinsic evaluation, we compare different feature groups' importance.", "labels": [], "entities": []}, {"text": "Paired t-tests are utilized to compare whether there are significant differences in performance.", "labels": [], "entities": []}, {"text": "Performance is measured using unweighted F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9837126731872559}]}, {"text": "In the extrinsic evaluation, we repeat the corpus study from Section 3 using the predicted counts of revision.", "labels": [], "entities": []}, {"text": "If the results in the intrinsic evaluation are solid, we expect that a similar conclusion could be drawn with the results from either predicted or manually annotated revisions.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation present the results of the classification between surface and text- We leave pipelined fine-grained classification to the future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Agreement of annotation on each category.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9439576268196106}]}, {"text": " Table 3: Distribution of revisions of corpus C1.", "labels": [], "entities": [{"text": "Distribution of revisions of corpus C1", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.8203894793987274}]}, {"text": " Table 4: Distribution of revisions of corpus C2.", "labels": [], "entities": [{"text": "Distribution of revisions of corpus C2", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.8221847911675771}]}, {"text": " Table 5: Partial correlation between number of re- visions and Draft 2 score on corpus C1 (partial cor- relation regresses out Draft 1 score); rebuttal is not  evaluated as there is only 1 occurrence.", "labels": [], "entities": []}, {"text": " Table 6: Experiment 1 on corpus C1 (Surface vs.  Text-based): average unweighted precision, recall,  F-score from 10-fold cross-validation;  *  indicates  significantly better than majority and unigram.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9663997888565063}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9994348883628845}, {"text": "F-score", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9975610971450806}]}, {"text": " Table 7: Experiment 1 on corpus C2.", "labels": [], "entities": []}, {"text": " Table 8: Experiment 2 on corpus C1: average unweighted F-score from 10-fold cross-validation;  *  indicates  significantly better than majority and unigram baselines. Rebuttal is removed as it only occurred once.", "labels": [], "entities": [{"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9385073781013489}]}, {"text": " Table 9: Experiment 2 on corpus C2; Organization is removed as it only occurred once.", "labels": [], "entities": []}, {"text": " Table 10: Partial correlation between number of pre- dicted revisions and Draft 2 score on corpus C1.  (Upper: Experiment 1, Lower: Experiment 3)", "labels": [], "entities": [{"text": "Draft 2 score", "start_pos": 75, "end_pos": 88, "type": "METRIC", "confidence": 0.6876145303249359}]}]}