{"title": [{"text": "A Snapshot of NLG Evaluation Practices 2005 -2014", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a snapshot of end-to-end NLG system evaluations as presented in conference and journal papers 1 over the last ten years in order to better understand the nature and type of evaluations that have been undertaken.", "labels": [], "entities": [{"text": "NLG system evaluations", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7616824905077616}]}, {"text": "We find that researchers tend to favour specific evaluation methods, and that their evaluation approaches are also correlated with the publication venue.", "labels": [], "entities": []}, {"text": "We further discuss what factors may influence the types of evaluation used fora given NLG system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation plays a crucial role in helping to understand whether a given approach fora text generating Natural Language Generation (NLG) system has expressed particular properties (such as quality, speed, etc.) or whether it has meta particular potential (domain utility).", "labels": [], "entities": [{"text": "text generating Natural Language Generation (NLG)", "start_pos": 87, "end_pos": 136, "type": "TASK", "confidence": 0.7412555925548077}, {"text": "speed", "start_pos": 198, "end_pos": 203, "type": "METRIC", "confidence": 0.9563906788825989}]}, {"text": "Past work within the NLG community has looked at the issues of evaluating NLG techniques and systems, the challenges unique to the NLG context in comparison to Natural Language Analysis (, and the comparisons between evaluation approaches ().", "labels": [], "entities": [{"text": "Natural Language Analysis", "start_pos": 160, "end_pos": 185, "type": "TASK", "confidence": 0.640707383553187}]}, {"text": "Whilst there has been a better understanding of the types of evaluations that can be conducted fora given NLG technique or system) there is little understanding on the frequency or types of evaluation that is typically conducted fora given system within the NLG community.", "labels": [], "entities": []}, {"text": "In this paper, we shed some light on the frequency of the types of evaluations conducted for NLG systems.", "labels": [], "entities": []}, {"text": "In particular, we have focused only on end-to-end complete NLG system as opposed to NLG components (referring expression generation, surface realisers, etc.) in our meta-analysis Dataset available from here: https://github.com/ Saad-Mahamood/ENLG2015 of published NLG systems from a variety of conferences, workshops, and journals for the last ten years since 2005.", "labels": [], "entities": [{"text": "expression generation", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.764672726392746}, {"text": "ENLG2015", "start_pos": 242, "end_pos": 250, "type": "DATASET", "confidence": 0.9363879561424255}]}, {"text": "For the purpose of this research, we created a corpus consisting of these papers (Section 3).", "labels": [], "entities": []}, {"text": "We then investigated three questions 4: (1) which is the most preferred evaluation method; (2) how does the method use changeover time; and (3) whether the publication venue influences the evaluation type.", "labels": [], "entities": []}, {"text": "In Section 5, we discuss the results of the meta analysis and finally in Section 6 we conclude the paper and we discuss directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "It was found that the majority of papers report an intrinsic evaluation method (74.7%), whereas a very small proportion of the papers report an extrinsic (15.1%) or both types of evaluation (10.1%), see also.", "labels": [], "entities": []}, {"text": "Regarding intrinsic evaluation, we further observed that papers report User like measures significantly more often than Output Quality measures (see also  We speculate that intrinsic methods are inherently easier, cheaper and quicker to be performed than extrinsic evaluations (), and therefore researchers opt for these significantly more often than extrinsic methods.", "labels": [], "entities": []}, {"text": "In addition, intrinsic methods can be domain-independent which allows comparisons between methods.", "labels": [], "entities": []}, {"text": "Finally, not all systems can be assessed for user task or system purpose success, e.g. commercial weather forecast systems.", "labels": [], "entities": []}, {"text": "Next, we investigated whether there was a change in the selection of evaluation metrics between the present and the past.", "labels": [], "entities": []}, {"text": "For this analysis, the data was separated into three groups.", "labels": [], "entities": []}, {"text": "The first group consisted of papers published between 2005 -2008 (25 papers), the second group consists of publications between 2009 -2011 (24 papers) and the last one contains papers published from 2012 to 2015 (30 papers).", "labels": [], "entities": []}, {"text": "We used only the first and the last group in order to identify whether there are differences in the application of evaluation methods.", "labels": [], "entities": []}, {"text": "We observed that papers published after 2012 are significantly (p < 0.04) more likely to include System Purpose evaluations.", "labels": [], "entities": []}, {"text": "We can also observe a trend towards intrinsic evaluations, as well as a reduction in using User Task Success evaluations, however the differences are not statistically significant (see also  We assume that this shift in evaluation metrics is correlated with the system design -more specific systems with well defined end users.", "labels": [], "entities": []}, {"text": "In addition, more general purpose systems such as adult humour generation systems ( have been recently developed which can be evaluated with a System Purpose metric in a straightforward way.", "labels": [], "entities": [{"text": "adult humour generation", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.606374611457189}]}, {"text": "Methods and Publication Venue Finally, we looked into whether papers published in specific venues \"prefer\" specific types of evaluation.", "labels": [], "entities": []}, {"text": "We used Pearson's \u03c7 2 to identify relations between the publication venues and the evaluation methods.", "labels": [], "entities": []}, {"text": "We found that more than half of the papers published at ACL, COLING, EMNLP and NAACL contain an Output Quality study, whereas for EACL, ENLG and INLG these percentages are below 50%.", "labels": [], "entities": [{"text": "ACL", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9283444881439209}, {"text": "COLING", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.787972092628479}, {"text": "EMNLP", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8556433916091919}, {"text": "NAACL", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.8934536576271057}, {"text": "Output Quality study", "start_pos": 96, "end_pos": 116, "type": "METRIC", "confidence": 0.9250832796096802}, {"text": "ENLG", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.8488084673881531}, {"text": "INLG", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.5587894916534424}]}, {"text": "Most papers published at ACL, EACL, NAACL, ENLG and INLG also contain a \"User Like\" study.", "labels": [], "entities": [{"text": "ACL", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9073455333709717}, {"text": "EACL", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.6999942064285278}, {"text": "NAACL", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9169660210609436}, {"text": "ENLG", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.861030638217926}, {"text": "INLG", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.6644029021263123}]}, {"text": "Extrinsic evaluation seems not to be popular across all venues (see also).", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8358929753303528}]}, {"text": "We further investigated whether there was a difference between ACL (including EACL, COL-ING, NAACL and EMNLP) publications and NLG publications (including ENLG and INLG).", "labels": [], "entities": [{"text": "ACL", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.8782857060432434}, {"text": "NAACL and EMNLP) publications", "start_pos": 93, "end_pos": 122, "type": "DATASET", "confidence": 0.7155490279197693}, {"text": "ENLG", "start_pos": 155, "end_pos": 159, "type": "DATASET", "confidence": 0.9463716745376587}, {"text": "INLG", "start_pos": 164, "end_pos": 168, "type": "DATASET", "confidence": 0.5990289449691772}]}, {"text": "From this analysis, journal papers have been omitted due to their low frequency.", "labels": [], "entities": []}, {"text": "Possible speculation for this significant difference in the use of the Output Quality evaluation type between the two sets of conference venues could be related to the fact that the ACL venues are patronised by a majority NLU audience.", "labels": [], "entities": []}, {"text": "Therefore, NLG papers submitted to these conferences would be more likely to use automatic metrics  (such as BLEU or ROUGE) as these measures are widely used by the NLU community as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9987648725509644}, {"text": "ROUGE", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.9640967845916748}]}], "tableCaptions": [{"text": " Table 1: High level descriptive statistics. * denotes signifi- cance at p < .016, using Z-test (after Bonferroni correction).", "labels": [], "entities": []}, {"text": " Table 2: Detailed descriptive statistics. * denotes signifi- cance at p < .008, using Z-test (after Bonferroni correction).", "labels": [], "entities": []}, {"text": " Table 3: Proportions of evaluation metrics in papers. Note  that some papers contain more than one type of evaluation.  * denotes significance at p < .05, using T-test in pair-wise  comparisons.", "labels": [], "entities": [{"text": "T-test", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9575456976890564}]}, {"text": " Table 4: Proportions of papers that report specific evaluation  metrics. Note that some papers contain more than one type of  evaluation. * denotes significance at p < .05, using Pearson's  \u03c7 2 test.", "labels": [], "entities": [{"text": "significance", "start_pos": 149, "end_pos": 161, "type": "METRIC", "confidence": 0.9538735151290894}]}, {"text": " Table 5: Proportions of ACL vs NLG papers that report  specific evaluation metrics. Note that some papers contain  more than one type of evaluation. * denotes significance at  p < .05, using Pearson's \u03c7 2 test.", "labels": [], "entities": [{"text": "significance", "start_pos": 160, "end_pos": 172, "type": "METRIC", "confidence": 0.9546900987625122}]}]}