{"title": [{"text": "Comparing attribute classifiers for interactive language grounding", "labels": [], "entities": [{"text": "interactive language grounding", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.6156888902187347}]}], "abstractContent": [{"text": "We address the problem of interactively learning perceptually grounded word meanings in a multimodal dialogue system.", "labels": [], "entities": []}, {"text": "We design a semantic and visual processing system to support this and illustrate how they can be integrated.", "labels": [], "entities": []}, {"text": "We then focus on comparing the performance (Precision, Recall, F1, AUC) of three state-of-the-art attribute classifiers for the purpose of interactive language grounding (MLKNN, DAP, and SVMs), on the aPascal-aYahoo datasets.", "labels": [], "entities": [{"text": "Precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9929444193840027}, {"text": "Recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9694440364837646}, {"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9027257561683655}, {"text": "AUC", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.906013011932373}, {"text": "aPascal-aYahoo datasets", "start_pos": 201, "end_pos": 224, "type": "DATASET", "confidence": 0.83672234416008}]}, {"text": "In prior work, results were presented for object classification using these methods for attribute labelling, whereas we focus on their performance for attribute labelling itself.", "labels": [], "entities": [{"text": "object classification", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.8007890284061432}]}, {"text": "We find that while these methods can perform well for some of the attributes (e.g. head, ears, furry) none of these models has good performance over the whole attribute set, and none supports incremental learning.", "labels": [], "entities": []}, {"text": "This leads us to suggest directions for future work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identifying, classifying and talking about objects or events in the surrounding environment are key capabilities for intelligent, goal-driven systems that interact with other agents and the external world (e.g. smart phones, robots, and other automated systems), as well as for image search/retrieval systems.", "labels": [], "entities": [{"text": "Identifying, classifying and talking about objects or events in the surrounding environment", "start_pos": 0, "end_pos": 91, "type": "TASK", "confidence": 0.7364389025248014}, {"text": "image search/retrieval", "start_pos": 278, "end_pos": 300, "type": "TASK", "confidence": 0.8367614597082138}]}, {"text": "To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions).", "labels": [], "entities": [{"text": "generation of Natural Language (NL) descriptions of images", "start_pos": 128, "end_pos": 186, "type": "TASK", "confidence": 0.7668835163116455}]}, {"text": "Another strand of work has focused on learning to generate object descriptions and object classification based on low level concepts/features (such as colour, shape and material), enabling systems to identify and describe novel, unseen images (.", "labels": [], "entities": [{"text": "object classification", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.7126124054193497}]}, {"text": "Our goal is to build interactive systems that can learn grounded word meanings relating to their perceptions of real-world objects -rather than abstract coloured shapes as in some previous work e.g.).", "labels": [], "entities": []}, {"text": "For example, we aim to build multimodal interfaces for Human-Robot Interaction which can learn object descriptions and references in interaction with humans.", "labels": [], "entities": []}, {"text": "In contrast to recent work on image description using 'deep learning' methods, this setting means that the system must be trainable from little data, compositional, able to handle dialogue, and adaptive -for instance so that it can learn visual concepts suitable for specific tasks/domains, and even new idiosyncratic language usage for particular users.", "labels": [], "entities": [{"text": "image description", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7387003004550934}]}, {"text": "However, most of the existing systems for image description rely on training data of both high quantity and high quality with no possibility of online error correction.", "labels": [], "entities": [{"text": "image description", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8841225504875183}]}, {"text": "Furthermore, they are unsuitable for robots and multimodal systems that need to continuously, and incrementally learn from the environment, and may encounter objects they haven't seen in training data.", "labels": [], "entities": []}, {"text": "These limitations are likely to be alleviated if systems can learn concepts, as and when needed, from situated dialogue with humans.", "labels": [], "entities": []}, {"text": "Interaction with a human tutor enables systems to take initiative and seek the particular information they need or lack by e.g. asking questions with the highest information gain (see e.g. (), and).", "labels": [], "entities": []}, {"text": "For example, a robot could ask questions to learn the color of a \"mug\" or to request to be presented with more \"red\" things to improve its performance on the concept (see e.g.).", "labels": [], "entities": []}, {"text": "Furthermore, such systems could allow for meaning negotiation in the form of clarification interactions T: What can you see?", "labels": [], "entities": [{"text": "meaning negotiation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7435017824172974}]}, {"text": ": red(x1) p2 : see(sys, x1) with the tutor.", "labels": [], "entities": []}, {"text": "This paper presents initial work in a larger programme of research with the aim of developing dialogue systems that learn (visual) conceptsword meanings -through situated dialogue with a human tutor.", "labels": [], "entities": []}, {"text": "Specifically, we compare several existing state-of-the-art classifiers with regard to their suitability for interactive language grounding tasks.", "labels": [], "entities": [{"text": "interactive language grounding tasks", "start_pos": 108, "end_pos": 144, "type": "TASK", "confidence": 0.6859013140201569}]}, {"text": "We compare the performance of MLKNN (, DAP (zero-shot learning (), and SVMs) on the image datasets aPascal (for training) and aYahoo (testing) -see section 4.", "labels": [], "entities": []}, {"text": "To our knowledge, this paper is the first to compare these attribute classifiers in terms of their suitability for interactive language grounding.", "labels": [], "entities": [{"text": "interactive language grounding", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.6383642752965292}]}, {"text": "Our other contribution is to integrate an incremental semantic grammar suited to dialogue processing -DS-TTR 1, see section 3 -with visual classification algorithms that provide perceptual grounding for the basic semantic atoms in the representations produced by the parser through the course of a dialogue (see).", "labels": [], "entities": [{"text": "dialogue processing", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7668565213680267}]}, {"text": "In effect, the dialogue with the tutor continuously provides semantic information about objects in the scene which is then fed to an online classifier in the form of training instances.", "labels": [], "entities": []}, {"text": "Conversely, the system can utilise the grammar and its existing knowledge about the world, encoded in its classifiers, to make reference to and formulate questions about the different attributes of an object identified in the scene.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compare the different classifiers with previous work, we perform our experiments on a benchmark dataset of natural object-based images with attribute annotations -the aPascal-aYahoo data set 3 -which is introduced by Farhadi et al.", "labels": [], "entities": [{"text": "aPascal-aYahoo data set 3", "start_pos": 179, "end_pos": 204, "type": "DATASET", "confidence": 0.7876267656683922}]}, {"text": "The aPascal-aYahoo data set has two subsets: the Pascal VOC 2008 dataset and the aYahoo dataset.", "labels": [], "entities": [{"text": "aPascal-aYahoo data set", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.7847385704517365}, {"text": "Pascal VOC 2008 dataset", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.7475626915693283}, {"text": "aYahoo dataset", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.9222785532474518}]}, {"text": "The Pascal VOC 2008 dataset is created for visual object classifications and detections.", "labels": [], "entities": [{"text": "Pascal VOC 2008 dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8466060757637024}, {"text": "visual object classifications and detections", "start_pos": 43, "end_pos": 87, "type": "TASK", "confidence": 0.7469143867492676}]}, {"text": "The aPascal data set covers 20 attribute-labelled classes and each class contains a number of samples, ranging from 150 to 1000.", "labels": [], "entities": [{"text": "aPascal data set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.900002916653951}]}, {"text": "The aYahoo dataset, as a supplement of the aPascal dataset, contains objects similar to aPascal, but with different correlations between attributes.", "labels": [], "entities": [{"text": "aYahoo dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9458004534244537}, {"text": "aPascal dataset", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.8990411460399628}]}, {"text": "The aYahoo dataset only contains 12 objects classes.", "labels": [], "entities": [{"text": "aYahoo dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9746007919311523}]}, {"text": "Images in both aPascal and aYahoo sets are annotated with 64 binary attributes, covering shape and material as well as object components (see).", "labels": [], "entities": []}, {"text": "We use the 6340 images selected by) from the aPascal dataset for training and use the whole aYahoo dataset with 2644 images as the test set.", "labels": [], "entities": [{"text": "aPascal dataset", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9787134230136871}, {"text": "aYahoo dataset", "start_pos": 92, "end_pos": 106, "type": "DATASET", "confidence": 0.9860276579856873}]}, {"text": "As both aPascal and aYahoo data sets are imbalanced in the number of positive 3 http://vision.cs.uiuc.edu/attributes/ instances for each attribute, as shown in table 1, this might affect the performance of the models on attribute classification.", "labels": [], "entities": [{"text": "aYahoo data sets", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.9035102923711141}, {"text": "attribute classification", "start_pos": 220, "end_pos": 244, "type": "TASK", "confidence": 0.7250813543796539}]}, {"text": "We test how well the different classifiers work on learning object attributes.", "labels": [], "entities": []}, {"text": "We implemented several classification models -MLkNN, DAP, and SVMs as described in Section 3.1.", "labels": [], "entities": [{"text": "MLkNN", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.7611889839172363}]}, {"text": "Most work on attribute classification reports the Precision and Recall only for object classes -which are computed using the attribute labels -but we are directly interested in the performance of the attribute classifiers themselves.", "labels": [], "entities": [{"text": "attribute classification", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7385762333869934}, {"text": "Precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9966465830802917}, {"text": "Recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.8707916140556335}]}, {"text": "Thus we report Precision, Recall, and F1-Score for the attribute labels for each model.", "labels": [], "entities": [{"text": "Precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9983886480331421}, {"text": "Recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9810246229171753}, {"text": "F1-Score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.999195396900177}]}, {"text": "We also show the average scores across all attributes in table 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average scores across attribute labels for each method, trained on aPascal and tested on aYahoo", "labels": [], "entities": [{"text": "aYahoo", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.8957555890083313}]}]}