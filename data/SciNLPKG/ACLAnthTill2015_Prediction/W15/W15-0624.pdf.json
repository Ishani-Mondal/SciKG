{"title": [{"text": "Generating Reference Texts for Short Answer Scoring Using Graph-based Summarization", "labels": [], "entities": [{"text": "Short Answer Scoring", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7001159191131592}, {"text": "Summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.5323634147644043}]}], "abstractContent": [{"text": "Automated scoring of short answers often involves matching a students response against one or more sample reference texts.", "labels": [], "entities": [{"text": "Automated scoring of short answers", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7253409087657928}]}, {"text": "Each reference text provided contains very specific instances of correct responses and may not cover the variety of possibly correct responses.", "labels": [], "entities": []}, {"text": "Finding or hand-creating additional references can be very time consuming and expensive.", "labels": [], "entities": []}, {"text": "In order to overcome this problem we propose a technique to generate alternative reference texts by summarizing the content of top-scoring student responses.", "labels": [], "entities": []}, {"text": "We use a graph-based cohesion technique that extracts the most representative answers from among the top-scorers.", "labels": [], "entities": []}, {"text": "We also use a state-of-the-art extractive summarization tool called MEAD.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8259139657020569}, {"text": "MEAD", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.6499999165534973}]}, {"text": "The extracted set of responses maybe used as alternative reference texts to score student responses.", "labels": [], "entities": []}, {"text": "We evaluate this approach on short answer data from Semeval 2013's Joint Student Response Analysis task.", "labels": [], "entities": [{"text": "Semeval 2013's Joint Student Response Analysis task", "start_pos": 52, "end_pos": 103, "type": "TASK", "confidence": 0.6410290226340294}]}], "introductionContent": [{"text": "Short answer scoring is a critical task in the field of automated student assessment.", "labels": [], "entities": [{"text": "Short answer scoring", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7454185287157694}]}, {"text": "Short answers contain brief responses restricted to specific terms or concepts.", "labels": [], "entities": []}, {"text": "There is a great demand for new techniques to handle large-scale development of shortanswer scoring engines.", "labels": [], "entities": [{"text": "shortanswer scoring engines", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.7133071919282278}]}, {"text": "For example an individual state assessment may involve building scoring algorithms for over two hundred prompts (or questions).", "labels": [], "entities": []}, {"text": "The past few years have seen a growth in the amount of research involved in developing better features and scoring models that would help improve short answer scoring (; Leacock and: Question text, sample reference and some top-scoring answers from a prompt in the ASAP-SAS (2012) competition.", "labels": [], "entities": [{"text": "ASAP-SAS (2012) competition", "start_pos": 265, "end_pos": 292, "type": "DATASET", "confidence": 0.7342766523361206}]}, {"text": "Prompt question: \"Explain how pandas in China are similar to koalas in Australia and how they both are different from pythons.", "labels": [], "entities": []}, {"text": "Support your response with information from the article.\"", "labels": [], "entities": []}, {"text": "Sample reference answer: \"Specialists are limited geographically to the area of their exclusive food source.", "labels": [], "entities": []}, {"text": "Pythons are different in both diet or eating habits and habitat from koalas.", "labels": [], "entities": []}, {"text": "Generalists are favored over specialists.", "labels": [], "entities": []}, {"text": "Koalas and pandas are herbivores and pythons are carnivores.\"", "labels": [], "entities": []}, {"text": "Some top-scoring student responses: \"A panda and a koala are both vegetarians.", "labels": [], "entities": []}, {"text": "Pandas eat bamboo, and koalas eat eucalyptus leaves.", "labels": [], "entities": []}, {"text": "Pythons are not vegetarians they eat meat, and they kill there pray by strangling them or putting venom into them.\"", "labels": [], "entities": []}, {"text": "\"Pandas and koalas are both endangered animals.", "labels": [], "entities": []}, {"text": "They can only be found in certain places where their food supply is.", "labels": [], "entities": []}, {"text": "They are different from pythons because they move to anew environment and adapt as well.", "labels": [], "entities": []}, {"text": "They beat a loss of food and climate change.\").", "labels": [], "entities": []}, {"text": "The Automated Student Assessment Prize (ASAP-SAS (2012)) competition had a short answer scoring component.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP-SAS (2012))", "start_pos": 4, "end_pos": 56, "type": "DATASET", "confidence": 0.5501191582944658}]}, {"text": "Short answer datasets are typically provided with one or more sample human references, which are representative of ideal responses.", "labels": [], "entities": []}, {"text": "Student responses that have a high text overlap with these human references are likely to get a higher score than those that have a poor overlap.", "labels": [], "entities": []}, {"text": "However often these sample human references are not representative of all possible correct responses.", "labels": [], "entities": []}, {"text": "For instance consider the question, sample reference and a set of top-scoring student responses fora prompt from the ASAP-SAS (2012) competition in.", "labels": [], "entities": [{"text": "ASAP-SAS (2012) competition", "start_pos": 117, "end_pos": 144, "type": "DATASET", "confidence": 0.6690490961074829}]}, {"text": "The human reference provided does not encompass all possible alternative ways of expressing the correct response.", "labels": [], "entities": []}, {"text": "A number of approaches have been used to extract regular expressions and score student responses.", "labels": [], "entities": []}, {"text": "use hand-crafted patterns to capture different ways of expressing the correct answer.", "labels": [], "entities": []}, {"text": "extract tags from a model answer, which are matched with stu-dent responses to determine their scores.", "labels": [], "entities": []}, {"text": "use a mark scheme consisting of a set of acceptable or unacceptable answers.", "labels": [], "entities": []}, {"text": "This marking scheme is similar to a sample reference.", "labels": [], "entities": []}, {"text": "Each student response is matched with these marking schemes and scored accordingly.", "labels": [], "entities": []}, {"text": "The winner of the ASAP competition spent a lot of time and effort hand-coding regular expressions from the human samples provided, in order to obtain better matches between student responses and references.", "labels": [], "entities": [{"text": "ASAP competition", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.9175568521022797}]}, {"text": "Although hand-crafting features might seem feasible fora few prompts, it is not an efficient technique when scoring large datasets consisting of thousands of prompts.", "labels": [], "entities": []}, {"text": "Hence there is a need to develop automated ways of generating alternate references that are more representative of topscoring student responses.", "labels": [], "entities": []}, {"text": "We use two summarization techniques to identify alternative references from top-scoring student responses fora prompt.", "labels": [], "entities": []}, {"text": "use summarization to generate content importance models from student essays.", "labels": [], "entities": []}, {"text": "We propose a graph-based cohesion technique, which uses text structure and semantics to extract representative responses.", "labels": [], "entities": []}, {"text": "We also use a state-of-the-art summarization technique called MEAD (), which extracts a summary from a collection of top-scoring responses.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9758151769638062}]}, {"text": "The novelty of our work lies in the utilization of summarization to the task of identifying suitable references to improve short-answer scoring.", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9735288619995117}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparing performance of system-generated summaries of top-scoring short answers with the performance of sample reference texts", "labels": [], "entities": []}, {"text": " Table 3: Comparing f -measures (f ) and mean cosines (cos) of every class for features generated by graph and MEAD summaries.", "labels": [], "entities": []}]}