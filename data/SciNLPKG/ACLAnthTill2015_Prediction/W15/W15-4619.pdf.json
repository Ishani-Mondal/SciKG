{"title": [{"text": "Information Theoretical and Statistical Features for Intrinsic Plagiarism Detection", "labels": [], "entities": [{"text": "Intrinsic Plagiarism Detection", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.5814512670040131}]}], "abstractContent": [{"text": "In this paper we present some information theoretical and statistical features including function word skip n-grams for detecting plagiarism intrinsically.", "labels": [], "entities": []}, {"text": "We train a binary classifier with different feature sets and observe their performances.", "labels": [], "entities": []}, {"text": "Basically, we propose a set of 36 features for classifying plagiarized and non-plagiarized texts in suspicious documents.", "labels": [], "entities": [{"text": "classifying plagiarized and non-plagiarized texts in suspicious documents", "start_pos": 47, "end_pos": 120, "type": "TASK", "confidence": 0.7531144246459007}]}, {"text": "Our experiment finds that entropy, relative en-tropy and correlation coefficient of function word skip n-gram frequency profiles are very effective features.", "labels": [], "entities": [{"text": "entropy", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9407330751419067}, {"text": "correlation coefficient", "start_pos": 57, "end_pos": 80, "type": "METRIC", "confidence": 0.9641862213611603}]}, {"text": "The proposed feature set achieves F-Score of 85.10%.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9995228052139282}]}], "introductionContent": [{"text": "Extrinsic plagiarism detection attempts to detect whether a document is plagiarised relative to reference documents.", "labels": [], "entities": [{"text": "Extrinsic plagiarism detection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8271370728810629}]}, {"text": "IPD (intrinsic plagiarism detection), which is relatively new, detects the plagiarised section(s) in a suspicious document without using any reference document.", "labels": [], "entities": [{"text": "intrinsic plagiarism detection)", "start_pos": 5, "end_pos": 36, "type": "TASK", "confidence": 0.804171696305275}]}, {"text": "The basic hypothesis behind IPD is different writers have their own styles and they maintain these in their writings consciously or subconsciously.", "labels": [], "entities": []}, {"text": "Sometimes it is very difficult to define the reference set for the task of external plagiarism detection.", "labels": [], "entities": [{"text": "external plagiarism detection", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.6545588771502177}]}, {"text": "Additionally, the source of the plagiarized text may not be available in digitized format.", "labels": [], "entities": []}, {"text": "Therefore, researchers are trying to answer whether it is possible to detect plagiarism without using any reference.", "labels": [], "entities": []}, {"text": "In this paper, we investigate some information theoretical and statistical measurements for IPD as a binary classification task.", "labels": [], "entities": [{"text": "IPD", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9070439338684082}]}, {"text": "A set of 36 features has been proposed for classifying plagiarized and non-plagiarized segments in the suspicious documents.", "labels": [], "entities": []}, {"text": "We use the PAN-PC-11 () corpus compiled for IPD task.", "labels": [], "entities": [{"text": "PAN-PC-11 () corpus compiled", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.8289670050144196}]}, {"text": "The PAN corpus is artificially plagiarised and it provides a meta-file mentioning the offsets of plagiarised and non-plagiarized parts for each suspicious document.", "labels": [], "entities": [{"text": "PAN corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8976736068725586}]}, {"text": "We consider that each suspicious document is written by single author and it is either partially plagiarised or not plagiarised and we try to identify the text-segments that differ in writing style compared to the whole document.", "labels": [], "entities": []}, {"text": "We train an SMO classifier in Weka3.) by using 10 fold cross-validation.", "labels": [], "entities": [{"text": "SMO classifier", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.903566986322403}, {"text": "Weka3.", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9879226088523865}]}, {"text": "Then the classification performances are observed with different feature sets according to the standard precision, recall and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9974892139434814}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9993709921836853}, {"text": "F-score", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9973236322402954}]}, {"text": "The next sections are organized as follows: section 2 discusses related works and section 3 briefly describes information theoretical and statistical features.", "labels": [], "entities": []}, {"text": "The text segmentation and windowing process is summarized in section 4 while the experimental framework and baseline feature sets are discussed in section 5.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7789356708526611}]}, {"text": "Section 6 compares the classification performances with different feature sets and finally, the paper concludes in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section illustrates the experimental framework of IPD task by combining the preprocessing and classification tools, the framework is graphically described in.", "labels": [], "entities": [{"text": "IPD task", "start_pos": 55, "end_pos": 63, "type": "TASK", "confidence": 0.8641559779644012}]}, {"text": "After extracting and windowing the corpus, we calculate different feature values for generating the feature vectors.", "labels": [], "entities": []}, {"text": "Before calculating the features, several text preprocessing tasks, for example, tokenizing, sentence detection and POS-tagging are employed.", "labels": [], "entities": [{"text": "tokenizing", "start_pos": 80, "end_pos": 90, "type": "TASK", "confidence": 0.979692816734314}, {"text": "sentence detection", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8372107148170471}]}, {"text": "We gen-: Experimental framework erate several feature vectors for different baseline feature sets and proposed feature set.", "labels": [], "entities": []}, {"text": "Then a classifier model is trained with the feature sets, we train SMO classifier with 10 fold cross validation in Weka 3.6 explorer interface.", "labels": [], "entities": [{"text": "SMO classifier", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.6008317470550537}, {"text": "Weka 3.6 explorer", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.9329457879066467}]}, {"text": "Equal number of plagiarized and non-plagiarized text samples are trained with the classifier.", "labels": [], "entities": []}, {"text": "We train the classifier with 8, 100 text segments from each class where each segment initially contains 5, 000 characters.", "labels": [], "entities": []}, {"text": "Finally, the classification performances are observed for different feature sets.", "labels": [], "entities": []}, {"text": "We observe that the proposed feature set achieves the highest F-Score compared to the baseline feature sets as illustrated in.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9991098046302795}]}, {"text": "All the feature sets together obtain a promising F-Score of 91% while the three baselines combined result in an F-Score around 89%.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9994341731071472}, {"text": "F-Score", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.9989810585975647}]}, {"text": "The proposed feature set achieves an 85% F-Score which is the highest compared to the three baseline feature sets.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9984146356582642}]}, {"text": "Baseline-1 and baseline-2 obtain F-Score around 68% and 62% while baseline-3 surprisingly results in an 84% F-Score as a single feature.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9993394017219543}, {"text": "F-Score", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9957049489021301}]}, {"text": "We pair feature sets and observe their performances, shows that the proposed feature set increases the F-Score with the combination of baseline feature sets.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.9985276460647583}]}, {"text": "Figure 5 depicts separate observations of entropy, relative entropy, correlation coefficient and distance function of function word skip n-gram frequency profiles.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 69, "end_pos": 92, "type": "METRIC", "confidence": 0.9821481108665466}]}, {"text": "Here we notice that relative entropy achieves a very good F-Score of 72%, entropy and correlation coefficient also obtain better F-Scores than the distance function.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9994449019432068}, {"text": "correlation coefficient", "start_pos": 86, "end_pos": 109, "type": "METRIC", "confidence": 0.9713134765625}, {"text": "F-Scores", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9967204928398132}]}, {"text": "Though distance function results in very good F-Score with the character tri-gram frequency profile it does not perform good enough with the function word skip n-gram frequency profile.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9986717700958252}]}, {"text": "Distance function with function word skip n-gram frequency profile obtains around a 35% F-Score which is the lowest compared to other functions with function word skip n-gram frequency profile.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.999669075012207}]}, {"text": "We also observe the effect of different window lengths (discussed in section 4) on classification performance, the classification performance increases for each feature set if the window length is increased.", "labels": [], "entities": [{"text": "classification", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.9545434713363647}]}, {"text": "All the feature sets combined result in F-Score of 82% and 87% for window lengths of 1000 and 2000 characters accordingly while a 91% F-Score is achieved with the window length of 5000 characters.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9993459582328796}, {"text": "F-Score", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.9986419081687927}]}], "tableCaptions": []}