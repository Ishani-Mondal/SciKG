{"title": [{"text": "Neural context embeddings for automatic discovery of word senses", "labels": [], "entities": []}], "abstractContent": [{"text": "Word sense induction (WSI) is the problem of automatically building an inventory of senses fora set of target words using only a text corpus.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8267350594202677}]}, {"text": "We introduce anew method for embedding word instances and their context, for use in WSI.", "labels": [], "entities": [{"text": "WSI", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.8498063683509827}]}, {"text": "The method, Instance-context embedding (ICE), leverages neural word embed-dings, and the correlation statistics they capture , to compute high quality embeddings of word contexts.", "labels": [], "entities": [{"text": "Instance-context embedding (ICE)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.637141740322113}]}, {"text": "In WSI, these context embed-dings are clustered to find the word senses present in the text.", "labels": [], "entities": []}, {"text": "ICE is based on a novel method for combining word embeddings using continuous Skip-gram, based on both semantic and a temporal aspects of context words.", "labels": [], "entities": [{"text": "ICE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5363972783088684}]}, {"text": "ICE is evaluated both in anew system , and in an extension to a previous system for WSI.", "labels": [], "entities": [{"text": "ICE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5565163493156433}, {"text": "WSI", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.6972464919090271}]}, {"text": "In both cases, we surpass previous state-of-the-art, on the WSI task of SemEval-2013, which highlights the generality of ICE.", "labels": [], "entities": [{"text": "WSI task of SemEval-2013", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.6589566841721535}]}, {"text": "Our proposed system achieves a 33% relative improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ambiguity is pervasive in natural language and this is particularly true of word meaning: a word string may refer to several different concepts or senses.", "labels": [], "entities": []}, {"text": "Word sense induction (WSI) is the problem of using a text corpus to automatically determine 1) the inventory of senses, and 2) which sense a particular occurrence of a word belongs to.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8355119675397873}]}, {"text": "This stands in contrast to the related task of word sense disambiguation (WSD), which is concerned with linking an occurrence of a word to an external sense inventory, e.g. WordNet.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.7885218362013499}, {"text": "WordNet", "start_pos": 173, "end_pos": 180, "type": "DATASET", "confidence": 0.943455696105957}]}, {"text": "The result of a WSI system is a set of local sense labels, consistent within the system but not linked to a universal set of labels.", "labels": [], "entities": []}, {"text": "A wide range of applications have been proposed where WSI could be useful, ranging from basic linguistic and lexicographical research), machine reading () and information retrieval.", "labels": [], "entities": [{"text": "WSI", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9646356701850891}, {"text": "machine reading", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.8416838049888611}, {"text": "information retrieval", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.8763332962989807}]}, {"text": "WSI is of particular interest in situations where standard lexical resources are unreliable or inapplicable, such as when tracking changes of word meaning overtime ().", "labels": [], "entities": [{"text": "WSI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6587866544723511}, {"text": "tracking changes of word meaning overtime", "start_pos": 122, "end_pos": 163, "type": "TASK", "confidence": 0.7193528016408285}]}, {"text": "According to the distributional hypothesis, word meaning is reflected in the set of contexts in which a word occurs.", "labels": [], "entities": []}, {"text": "This intuition makes it natural to operationalize the meaning of a word -and of its contexts -using a vector-space representation, where geometric proximity corresponds to similarity of meaning.", "labels": [], "entities": []}, {"text": "A common approach used in several successful WSI systems is to apply this geometric intuition and represent each context of a polysemous word as a vector, look for coherent clusters in the set of context vectors, and let these define the senses of the word.", "labels": [], "entities": [{"text": "WSI", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9563307166099548}]}, {"text": "This approach was pioneered by using second order co-occurrences to construct the context representation.", "labels": [], "entities": []}, {"text": "It is clear that in order to be useful in a WSI system, a geometric representation of context meaning must be designed in away that makes clusters distinct.", "labels": [], "entities": [{"text": "WSI", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9528275728225708}]}, {"text": "Recently, neural embeddings, such as the popular Skip-gram model (), have proven efficient and accurate in the task of embedding words in vector spaces.", "labels": [], "entities": []}, {"text": "As of yet, however, neural embeddings have not been considered for representing contexts in WSI.", "labels": [], "entities": [{"text": "WSI", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.8050070405006409}]}, {"text": "The systems that seem most relevant in this context are those that train multi-prototype embeddings: more than one embedding per word ().", "labels": [], "entities": []}, {"text": "In particular, described a modified Skipgram algorithm that clusters instances on the fly, effectively training several vectors per word.", "labels": [], "entities": []}, {"text": "However, whether this or any other similar approach is useful if considered as a WSI system is still an open question, since they have never been evaluated in that setting.", "labels": [], "entities": []}, {"text": "We make the following contributions: (1) We define the Instance-context embedding (ICE), a novel way for representing word instances and their context.", "labels": [], "entities": []}, {"text": "ICE combines vectors representing context words using a novel weighting schema consisting of a semantic component, and a temporal component, see Section 3.", "labels": [], "entities": [{"text": "ICE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6380646228790283}]}, {"text": "(2) We propose two methods for using our embeddings in word sense induction, see Section 4.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7036203046639761}]}, {"text": "The first adopts a batch clustering scheme, where senses are induced after the word embeddings are computed.", "labels": [], "entities": []}, {"text": "The number of senses is automatically chosen, based on data.", "labels": [], "entities": []}, {"text": "The second extends an existing method for simultaneous embedding and clustering of words ().", "labels": [], "entities": []}, {"text": "We show that our extension substantially improves the model.", "labels": [], "entities": []}, {"text": "(3) We evaluate both proposed methods in the WSI task.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.9356827139854431}]}, {"text": "We show that the two components of our proposed weighting schema both contribute to an increased overall performance.", "labels": [], "entities": []}, {"text": "Further, we compare our method to state-of-the-art methods on Task 13 of SemEval-2013, achieving a 33% relative improvement see, Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our methods for word sense induction on shared task 13 of SemEval-2013, Word Sense Induction for Graded and Non-Graded Senses (Jurgens and).", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8438915212949117}, {"text": "Word Sense Induction", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.5832390785217285}]}, {"text": "Henceforth, we let \"SemEval-2013\" refer to this specific task.", "labels": [], "entities": []}, {"text": "We also investigate the influence of our weighting schema on both methods.", "labels": [], "entities": []}, {"text": "Further, we study qualitative properties of the word instance embeddings produced by our method.", "labels": [], "entities": []}, {"text": "For ICE-kmeans, we train a 300 dimensional Skipgram model on the ukWaC corpus using standard parameter settings.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9940740168094635}]}, {"text": "I.e. context width set to 20 (10 before and 10 after), and 10 negative samples.", "labels": [], "entities": []}, {"text": "We let the model iterate over the training data 9 times to improve the embeddings.", "labels": [], "entities": []}, {"text": "For sense induction, we sample 1800 instances of very target word at random, from the ukWaC corpus.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8415583968162537}, {"text": "ukWaC corpus", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.9953201711177826}]}, {"text": "Using more instances did not improve the results in our experiments, however, for larger datasets this might not be valid.", "labels": [], "entities": []}, {"text": "To remain general, we opted not to use the POS tags available in ukWaC, even though using them might have improved the result.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8663319945335388}]}, {"text": "Also, due to the noisy nature of the corpus, we exclude contexts where more than 30% of the words contain non-alphabetic characters.", "labels": [], "entities": []}, {"text": "We cluster the selected instances using kmeans clustering with the heuristic of for choosing k.", "labels": [], "entities": []}, {"text": "For both ICE-kmeans and ICE-online, when computing the ICE vectors, the context width for was set to 20 when using the full schema, see, and 10 otherwise, as the full schema is less sensitive to irrelevant context words.", "labels": [], "entities": []}, {"text": "For the MSSG part of ICE-online, we use the parameters reported in.", "labels": [], "entities": []}, {"text": "Consider the word \"paper\".", "labels": [], "entities": []}, {"text": "Assigning an instance to one of these senses can be challenging even fora human reader.", "labels": [], "entities": []}, {"text": "The word \"paper\" is one of the 50 lemmas in the SemEval-2013 evaluation data with corresponding instances that cover six of the senses listed in WordNet.", "labels": [], "entities": [{"text": "SemEval-2013 evaluation data", "start_pos": 48, "end_pos": 76, "type": "DATASET", "confidence": 0.7257412870724996}, {"text": "WordNet", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.960034966468811}]}, {"text": "In, we show context embeddings for these instances, plotted using the dimensionality reduction tool t-SNE (Van der Maaten and.", "labels": [], "entities": []}, {"text": "represents context embeddings computed using a uniform average, and Figure 1b plots the instance context embeddings computed with using ICE, as described in Section 3.", "labels": [], "entities": []}, {"text": "The colors and markers correspond to gold-standard WordNet annotations provided by SemEval.", "labels": [], "entities": []}, {"text": "The size of a marker in is proportional to the average ICE weight of words in the context of an instance and is indicative of the confidence in the instance vector.", "labels": [], "entities": []}, {"text": "A low average ICE weight indicates that the context is not predictive of the target word.", "labels": [], "entities": [{"text": "ICE", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8096289038658142}]}, {"text": "For the senses, \"material\", \"scholarly article\", \"newspaper\" and \"essay\", the instances in are noticeably more clustered than in.", "labels": [], "entities": []}, {"text": "This shows that the senses of these words are better represented using ICE weighting for context embeddings than a uniform schema.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for single-sense instances on the  WSI task of SemEval-2013. HM is the harmonic  mean of FBC and FNMI.", "labels": [], "entities": [{"text": "WSI task of SemEval-2013", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.7561294585466385}, {"text": "HM", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9915906190872192}, {"text": "FBC", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.51080322265625}, {"text": "FNMI", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.6438917517662048}]}]}