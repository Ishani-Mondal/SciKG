{"title": [{"text": "Using word embedding for bio-event extraction", "labels": [], "entities": [{"text": "bio-event extraction", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7717427015304565}]}], "abstractContent": [{"text": "Bio-event extraction is an important phase towards the goal of extracting biological networks from the scientific literature.", "labels": [], "entities": [{"text": "Bio-event extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9203118979930878}]}, {"text": "Recent advances in word embedding make computation of word distribution more efficient and possible.", "labels": [], "entities": [{"text": "word distribution", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7223725467920303}]}, {"text": "In this study, we investigate methods bringing distributional characteristics of words in the text into event extraction by using the latest word embedding methods.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.7290219813585281}]}, {"text": "By using bag-of-words (BOW) features as the baseline, the result has been improved by the introduction of word-embedding features, and is comparable to the state-of-the-art solution.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated extraction of bio-events from the scientific literature is an important research stage towards extraction of bio-networks, and is the main focus of bio-text-mining.", "labels": [], "entities": [{"text": "Automated extraction of bio-events", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7886820733547211}]}, {"text": "An event represents a biochemical process, e.g. a protein-protein interaction or chemicalprotein interaction, within a signalling pathway or a metabolic pathway.", "labels": [], "entities": []}, {"text": "An event in text is usually anchored by a word indicating the occurrence of the event, named a trigger, and the other words, which are arguments involved in the reaction.", "labels": [], "entities": []}, {"text": "Solutions of extracting events usually begin with detecting trigger words first, and then assemble other detected argument words to a trigger.", "labels": [], "entities": []}, {"text": "Some solutions consider event extraction as a structured prediction problem and extract triggers with corresponding arguments at once,.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8151282370090485}]}, {"text": "BOW is common features of representing tokens when lexcial information is need for prediction, e.g. trigger prediction.", "labels": [], "entities": [{"text": "BOW", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9729197025299072}, {"text": "trigger prediction", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7400472462177277}]}, {"text": "However, it has drawbacks of being high dimensional, sparse and discrete.", "labels": [], "entities": []}, {"text": "While word embedding is a collective name fora set of language modelling and feature learning techniques, by which words in a vocabulary could by mapped to vectors in a lower dimensional space, which is continuous in and relative to the vocabulary size.", "labels": [], "entities": []}, {"text": "It is capable of representing a words distributional characteristics.", "labels": [], "entities": []}, {"text": "In this way, word embedding model may capture semantic and sequential information of a word in text.", "labels": [], "entities": []}, {"text": "Meanwhile, a word-embedding feature is continuous, since continuous space language models maps integer vector into continuous space via learned parameters.", "labels": [], "entities": []}, {"text": "By training a neural network language model, one obtains not just the model itself, but also the learned word embedding.", "labels": [], "entities": []}, {"text": "Due to the size of a dictionary word embedding might involve, computation of word distribution could be expensive.", "labels": [], "entities": [{"text": "word distribution", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.704958975315094}]}, {"text": "Mikolov et al. proposed two model architectures called CBOW and skip-gram for maing computation of word embedding feasible and efficient.", "labels": [], "entities": []}, {"text": "The skip-gram model tries to maximize classification of a word based on another word in the same sentence.", "labels": [], "entities": []}, {"text": "Each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word (.", "labels": [], "entities": []}, {"text": "Nie et al. utilized word embedding for detecting trigger words.", "labels": [], "entities": []}, {"text": "In this paper, we present the experiments using word embedding as token features to extract complete events including triggers and their arguments.", "labels": [], "entities": []}, {"text": "The skip-gram model is used to obtain word-embedding features and is compared with a baseline model of using BOW features.", "labels": [], "entities": []}, {"text": "The result demonstrates that the introduction of word embedding improves the result, and is comparable to the state-of-the-art solution.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The comparison between the BOW model, the word embedding model and the joint model on  the test set of BioNLP 2013. The results are represented in F-scores.", "labels": [], "entities": [{"text": "BOW", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.7728719115257263}, {"text": "test set of BioNLP 2013", "start_pos": 101, "end_pos": 124, "type": "DATASET", "confidence": 0.727156263589859}, {"text": "F-scores", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9900015592575073}]}, {"text": " Table 2: The detail result on the BioNLP 2013 GENIA test dataset by using the word-embedding model.", "labels": [], "entities": [{"text": "detail", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.988836407661438}, {"text": "BioNLP 2013 GENIA test dataset", "start_pos": 35, "end_pos": 65, "type": "DATASET", "confidence": 0.9215496897697448}]}]}