{"title": [{"text": "Reinforcement Learning of Multi-Issue Negotiation Dialogue Policies", "labels": [], "entities": [{"text": "Reinforcement Learning of Multi-Issue Negotiation Dialogue Policies", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8176154579435077}]}], "abstractContent": [{"text": "We use reinforcement learning (RL) to learn a multi-issue negotiation dialogue policy.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.6723223865032196}]}, {"text": "For training and evaluation, we build a hand-crafted agenda-based policy , which serves as the negotiation partner of the RL policy.", "labels": [], "entities": [{"text": "RL policy", "start_pos": 122, "end_pos": 131, "type": "TASK", "confidence": 0.87308469414711}]}, {"text": "Both the agenda-based and the RL policies are designed to work fora large variety of negotiation settings, and perform well against negotiation partners whose behavior has not been observed before.", "labels": [], "entities": [{"text": "RL", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8336946368217468}]}, {"text": "We evaluate the two models by having them negotiate against each other under various settings.", "labels": [], "entities": []}, {"text": "The learned model consistently out-performs the agenda-based model.", "labels": [], "entities": []}, {"text": "We also ask human raters to rate negotiation transcripts between the RL policy and the agenda-based policy, regarding the ratio-nality of the two negotiators.", "labels": [], "entities": [{"text": "RL", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9225836992263794}]}, {"text": "The RL policy is perceived as more rational than the agenda-based policy.", "labels": [], "entities": [{"text": "RL", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.876060962677002}]}], "introductionContent": [{"text": "Negotiation is a process in which two or more parties participate in order to reach a joint decision.", "labels": [], "entities": []}, {"text": "Negotiators have goals and preferences, and follow a negotiation policy or strategy to accomplish their goals.", "labels": [], "entities": []}, {"text": "There has been a lot of work on building automated agents for negotiation in the communities of autonomous agents and game theory.", "labels": [], "entities": []}, {"text": "present a quite comprehensive survey on automated agents designed to negotiate with humans.", "labels": [], "entities": []}, {"text": "Below we focus only on research that is directly related to our work. and Heeman (2009) applied reinforcement learning (RL) to a furniture layout negotiation task.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 96, "end_pos": 123, "type": "TASK", "confidence": 0.653019231557846}, {"text": "furniture layout negotiation task", "start_pos": 129, "end_pos": 162, "type": "TASK", "confidence": 0.7762719541788101}]}, {"text": "learned argumentation policies against users of different cultural norms in a oneissue negotiation scenario.", "labels": [], "entities": []}, {"text": "Then Georgila learned argumentation policies in a two-issue negotiation scenario.", "labels": [], "entities": []}, {"text": "These policies were trained for some initial conditions, and they could perform well only when they were tested under similar conditions.", "labels": [], "entities": []}, {"text": "More recently, learned negotiation behaviors fora noncooperative trading game (the Settlers of Catan).", "labels": [], "entities": [{"text": "Settlers of Catan)", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.7609181106090546}]}, {"text": "Again, in's work, the initial settings were always the same.", "labels": [], "entities": []}, {"text": "used multi-agent RL to learn negotiation policies in a resource allocation scenario.", "labels": [], "entities": []}, {"text": "They compared single-agent RL vs. multi-agent RL and they did not deal with argumentation, nor did they allow fora variety of initial conditions.", "labels": [], "entities": []}, {"text": "Finally, applied RL to the problem of learning cooperative persuasive policies using framing.", "labels": [], "entities": [{"text": "RL", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9619672894477844}]}, {"text": "Due to the complexity of negotiation tasks, none of the above works dealt with speech recognition or understanding errors.", "labels": [], "entities": [{"text": "negotiation tasks", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8987955749034882}, {"text": "speech recognition", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7698476910591125}]}, {"text": "In this paper, we focus on two-party negotiation, and use RL to learn a multi-issue negotiation policy for an agent aimed for negotiating with humans.", "labels": [], "entities": []}, {"text": "We train our RL policy against a simulated user (SU), which plays the role of the other negotiator.", "labels": [], "entities": [{"text": "RL", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9158623814582825}]}, {"text": "Our SU is a hand-crafted negotiation dialogue policy inspired by the agenda paradigm, previously used for dialogue management) and user modeling) in information providing tasks.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7361925542354584}]}, {"text": "Both the agenda-based and the RL policies are designed to work fora variety of goals, preferences, and negotiation moves, even under conditions that are very different from the conditions that the agents have experienced before.", "labels": [], "entities": []}, {"text": "We vary the goals of the agents, how easy it is for the agents to be persuaded, whether they have enough arguments to accomplish their goals (i.e., shift their partners' preferences), and the importance of each issue for each agent.", "labels": [], "entities": []}, {"text": "We evaluate our two models by having them negotiate against each other under various settings.", "labels": [], "entities": []}, {"text": "We also ask human raters to rate negotiation transcripts between the RL policy and the agenda-based SU, regarding the rationality of the two negotiators.", "labels": [], "entities": [{"text": "RL", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9142730832099915}]}, {"text": "In our negotiation task, both the agenda-based SU and the RL policy have human-like constraints of imperfect information about each other; they do not know each other's goals or preferences, number of available arguments, degree of persuadability, or degree of rationality.", "labels": [], "entities": []}, {"text": "Furthermore, both agents are required to perform well fora variety of negotiation settings, and against opponents whose negotiation behavior has not been observed before and may vary from one interaction to another or even within the same interaction.", "labels": [], "entities": []}, {"text": "Thus our negotiation task is very complex and it is not possible (or at least it is very difficult) to compute an analytical solution to the problem using game theory.", "labels": [], "entities": [{"text": "negotiation task", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.8956400454044342}]}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "First, this is the first time in the literature that the agenda-based paradigm is applied to negotiation.", "labels": [], "entities": []}, {"text": "Second, to our knowledge this is the first time that RL is used to learn so complex multi-issue negotiation and argumentation policies (how to employ arguments to persuade the other party) designed to work fora large variety of negotiation settings, including settings that did not appear during training.", "labels": [], "entities": [{"text": "RL", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.8236286044120789}, {"text": "multi-issue negotiation and argumentation", "start_pos": 84, "end_pos": 125, "type": "TASK", "confidence": 0.7269699573516846}]}], "datasetContent": [{"text": "For our evaluation, we have the RL policy interact with the agenda-based SU for 20000 episodes varying the initial settings for both agents in the same fashion as for training.", "labels": [], "entities": [{"text": "RL", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.8621999621391296}]}, {"text": "Similarly to training, we have 10 runs and report averages (see.", "labels": [], "entities": []}, {"text": "The RL policy outperforms the agendabased SU.", "labels": [], "entities": [{"text": "RL", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.7742987871170044}]}, {"text": "The RL policy learned to exploit tradeoffs that while not being optimal for the SU, they are good enough for the SU to accept (the SU is: Average scores as a function of the number of episodes during training (10 runs).", "labels": [], "entities": [{"text": "RL", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8853468894958496}, {"text": "Average scores", "start_pos": 138, "end_pos": 152, "type": "METRIC", "confidence": 0.957867830991745}]}, {"text": "In the last 20000 episodes the exploration rate is almost 0 (similarly to testing).", "labels": [], "entities": [{"text": "exploration rate", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.8317375779151917}]}, {"text": "designed to accept only trade-offs and offers that lead to reasonable agreements).", "labels": [], "entities": []}, {"text": "Note that some decisions of the SU about what to accept are based on inaccurate estimates of its opponent's persuadability and goals.", "labels": [], "entities": [{"text": "SU", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9392668008804321}]}, {"text": "reports results about the success percentages of the RL policy and the agenda-based SU.", "labels": [], "entities": [{"text": "RL", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9541664719581604}, {"text": "SU", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.8454845547676086}]}, {"text": "We show on average how many times (10 runs) the agents fully succeeded in their goals (score equal to 100), how many times they achieved roughly at least their second best values for all issues (score > 65), and how many times they achieved roughly at least their third best values for all issues (score > 30).", "labels": [], "entities": []}, {"text": "A higher than 65 score can also be achieved when an agent achieves the best possible outcome in some of the issues and the third possible outcome in the rest of the issues.", "labels": [], "entities": []}, {"text": "Likewise for scores greater than 30.", "labels": [], "entities": []}, {"text": "Ina second experiment we asked human raters to rate negotiation transcripts between the agendabased SU and the RL policy.", "labels": [], "entities": [{"text": "RL policy", "start_pos": 111, "end_pos": 120, "type": "TASK", "confidence": 0.6910625994205475}]}, {"text": "The domain was organizing a party.", "labels": [], "entities": []}, {"text": "The negotiators had to agree on 4 issues (food type, drink, music, day of week) and there were 3 possible values per issue.", "labels": [], "entities": []}, {"text": "We replaced the speech acts with full sentences but for arguments we used sentences such as \"here is a strong argument supporting jazz for music\".", "labels": [], "entities": []}, {"text": "We randomly selected 20 negotiations between the RL policy and the agenda-based SU.", "labels": [], "entities": [{"text": "RL", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.8589054346084595}]}, {"text": "In 10 of those the RL policy earned more points, and in the other 10 the agenda-based SU earned more points.", "labels": [], "entities": [{"text": "RL", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9120189547538757}]}, {"text": "This was to ensure that the transcripts were balanced and that we had not picked only transcripts where one of the agents was always better than the other.", "labels": [], "entities": []}, {"text": "We did not tell raters that these were artificial dialogues.", "labels": [], "entities": []}, {"text": "We deliberately included some questions with rather obvious answers (sanity checks) to check how committed the raters were.", "labels": [], "entities": []}, {"text": "We recruited raters from MTurk (www.mturk.com).", "labels": [], "entities": [{"text": "MTurk", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.9710798263549805}]}, {"text": "We asked raters to read 2 transcripts and for each transcript rate the negotiators in terms of how rationally they behaved, on a Likert scale from 1 to 5.", "labels": [], "entities": []}, {"text": "We excluded ratings that were done in less than 3 minutes and that had failed in more than half of our sanity checks.", "labels": [], "entities": []}, {"text": "In total there were 6 sanity checks (3 per negotiation transcript).", "labels": [], "entities": []}, {"text": "Thus we ended up with 89 raters.", "labels": [], "entities": [{"text": "raters", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9272407293319702}]}, {"text": "Results are shown in Table 2.", "labels": [], "entities": []}, {"text": "The RL policy was perceived as more rational, and both agents were rated as reasonably rational.", "labels": [], "entities": [{"text": "RL", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.6850530505180359}]}, {"text": "Interestingly, rationality was perceived differently by different human raters, e.g., revisiting an agreed-upon issue was considered as rational by some and irrational by others.: Human evaluation scores (the p-value is based on the Wilcoxon signed-rank test).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average success percentages (10 runs).  Learned Policy Score  3.43  Agenda-based SU Score 3.02  p-value  0.027", "labels": [], "entities": [{"text": "Average success percentages", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.940967321395874}, {"text": "Learned Policy Score  3.43  Agenda-based SU Score", "start_pos": 50, "end_pos": 99, "type": "DATASET", "confidence": 0.7555409755025592}]}, {"text": " Table 2: Human evaluation scores (the p-value is  based on the Wilcoxon signed-rank test).", "labels": [], "entities": []}]}