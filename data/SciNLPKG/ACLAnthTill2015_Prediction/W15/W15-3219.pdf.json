{"title": [{"text": "SAHSOH@QALB-2015 Shared Task: A Rule-Based Correction Method of Common Arabic Native and Non-Native Speakers' Errors", "labels": [], "entities": [{"text": "SAHSOH@QALB-2015 Shared Task", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.7595249056816101}]}], "abstractContent": [{"text": "This paper describes our participation in the QALB-2015 Automatic Correction of Arabic Text shared task.", "labels": [], "entities": [{"text": "QALB-2015 Automatic Correction of Arabic Text shared task", "start_pos": 46, "end_pos": 103, "type": "TASK", "confidence": 0.530142679810524}]}, {"text": "We employed various tools and external resources to build a rule based correction method.", "labels": [], "entities": [{"text": "rule based correction", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.6399574279785156}]}, {"text": "Hand written linguistic rules were added by using existing lexicons and regular expressions.", "labels": [], "entities": []}, {"text": "We handled specific errors with dedicated rules reserved for non-native speakers.", "labels": [], "entities": []}, {"text": "The system is simple as it does not employ any sophisticated machine learning methods and it does not correct punctuation errors.", "labels": [], "entities": []}, {"text": "The system achieved results comparable to other approaches when the punctuation errors are ignored with an F1 of 66.9% for native speakers' data and an F1 of 31.72% for the non-native speakers' data.", "labels": [], "entities": [{"text": "F1", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9994388222694397}, {"text": "F1", "start_pos": 152, "end_pos": 154, "type": "METRIC", "confidence": 0.9983487129211426}]}], "introductionContent": [{"text": "The Automatic Error Correction (AEC) is an interesting and challenging problem in Natural Language Processing.", "labels": [], "entities": [{"text": "Automatic Error Correction (AEC)", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7332453578710556}]}, {"text": "The existing methods that attempt to solve this problem are generally based on deep linguistic and statistical analysis.", "labels": [], "entities": []}, {"text": "AEC tools can assist in solving multiple natural language processing (NLP) tasks like Machine Translation or Natural Language Generation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8514097929000854}, {"text": "Natural Language Generation", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.637326200803121}]}, {"text": "However, the main application of AEC is the building of automated spell checkers to be used as writing assistant tools (e.g. word-processing) or even for applications such as Mobile autocompletion and auto correction programs, postprocessing optical character recognition tools or with the correction of large content site such as Wikipedia.", "labels": [], "entities": [{"text": "auto correction", "start_pos": 201, "end_pos": 216, "type": "TASK", "confidence": 0.6964330673217773}, {"text": "postprocessing optical character recognition", "start_pos": 227, "end_pos": 271, "type": "TASK", "confidence": 0.6337349116802216}]}, {"text": "Conventional spelling correction tools detect typing errors simply by comparing each token of a text against a dictionary of words that are known to be correctly spelled.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7959683239459991}]}, {"text": "Any token that matches an element of the dictionary, possibly after some minimal morphological analysis, is deemed to be correctly spelled; any token that matches no element is flagged as a possible error, with near-matches displayed as suggested corrections In this paper we describe our participation in the QALB-2015 shared task  which is an extension of the first QALB shared task () that took place last year.", "labels": [], "entities": []}, {"text": "The QALB-2014 shared task was reserved to errors in comments written to Aljazeera articles by native Arabic speakers ().", "labels": [], "entities": []}, {"text": "The 2015 competition includes two tracks.", "labels": [], "entities": []}, {"text": "The first track is dedicated to errors produced by native speakers and the second track includes correction of texts written by learners of Arabic as a foreign language (L2) ().", "labels": [], "entities": [{"text": "correction of texts written by learners of Arabic as a foreign language", "start_pos": 97, "end_pos": 168, "type": "TASK", "confidence": 0.6731982628504435}]}, {"text": "The native track includes texts from QALB-2014.", "labels": [], "entities": [{"text": "QALB-2014", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9378969073295593}]}, {"text": "This data was released for the development of the systems.", "labels": [], "entities": []}, {"text": "The systems were scored on blind test sets Alj-test-2015 and L2-test-2015.", "labels": [], "entities": [{"text": "Alj-test-2015", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9304090738296509}]}, {"text": "Our pipeline approach is based on a combination of pre-existing tools, handwritten contextual rules and lexicons.", "labels": [], "entities": []}, {"text": "Detecting and correcting such complex errors within the scope of a rule based approach require specific rules to be written in order to correctly analyze the dependencies between words in a given sentence.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 describes the related works.", "labels": [], "entities": []}, {"text": "Section 3 presents our approach including the tools and resources used and finally in Section 4 we report the results obtained on the Development set.", "labels": [], "entities": [{"text": "Development set", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.7834120392799377}]}], "datasetContent": [{"text": "In order to evaluate the performance of our system, we used the data set provided in the shared task test).", "labels": [], "entities": []}, {"text": "For this evaluation we have used two autocorrected word lists: -A generic word list generated from Attia wordlist and the JRC corpus, this wordlist is used for general correction purposes.", "labels": [], "entities": [{"text": "Attia wordlist", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9013559818267822}, {"text": "JRC corpus", "start_pos": 122, "end_pos": 132, "type": "DATASET", "confidence": 0.8672856688499451}]}, {"text": "-A customized wordlist based on each dataset L2-dev-2015, L2-test-2015, Alj-dev-2014 and Alj-test-2015 by generating a special word list according to each data set, in order to improve the results and avoid unnecessary replacement.", "labels": [], "entities": []}, {"text": "The customized auto correction word list is builtin the same way as the generic one, by replacing the source dictionary by misspelled words from QALB corpus.", "labels": [], "entities": [{"text": "QALB corpus", "start_pos": 145, "end_pos": 156, "type": "DATASET", "confidence": 0.9116422533988953}]}, {"text": "We submitted only one run for each corpus type and the official results obtained on the Development sets and the Test sets are shown in  The relatively low results obtained were expected since we decided to ignore the punctuation errors and therefore our system is penalized by this decision.", "labels": [], "entities": [{"text": "Development sets", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.7950215339660645}]}, {"text": "We estimate that punctuation errors represent more than 38% of the errors in the QALB data sets (L1 and L2).", "labels": [], "entities": [{"text": "QALB data sets", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.9640753070513407}]}, {"text": "When the punctuation errors were removed from the evaluation, we noticed a significant improvement of the recall and the F1 score for L1 (+13 points) and for L2 (+6.6 points) as seen in", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9983080625534058}, {"text": "F1 score", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9914779961109161}]}], "tableCaptions": [{"text": " Table 5: Results on the Dev and Test sets", "labels": [], "entities": [{"text": "Dev and Test sets", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.8482189923524857}]}, {"text": " Table 6: Official Results on the Dev and Test  sets with with punctuation errors ignored", "labels": [], "entities": [{"text": "Dev and Test  sets", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.8610881865024567}]}]}