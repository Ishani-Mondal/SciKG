{"title": [{"text": "Translating Literary Text between Related Languages using SMT", "labels": [], "entities": [{"text": "Translating Literary Text between Related Languages", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8802844087282816}, {"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9370197057723999}]}], "abstractContent": [{"text": "We explore the feasibility of applying machine translation (MT) to the translation of literary texts.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8398695826530457}, {"text": "translation of literary texts", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.80556420981884}]}, {"text": "To that end, we measure the translata-bility of literary texts by analysing parallel corpora and measuring the degree of freedom of the translations and the narrowness of the domain.", "labels": [], "entities": []}, {"text": "We then explore the use of domain adaptation to translate a novel between two related languages, Spanish and Catalan.", "labels": [], "entities": []}, {"text": "This is the first time that specific MT systems are built to translate novels.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9601446390151978}]}, {"text": "Our best system out-performs a strong baseline by 4.61 absolute points (9.38% relative) in terms of BLEU and is corroborated by other automatic evaluation metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9994545578956604}]}, {"text": "We provide evidence that MT can be useful to assist with the translation of novels between closely-related languages, namely (i) the translations produced by our best system are equal to the ones produced by a professional human translator in almost 20% of cases with an additional 10% requiring at most 5 character edits, and (ii) a complementary human evaluation shows that over 60% of the translations are perceived to be of the same (or even higher) quality by native speakers.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9786643981933594}]}], "introductionContent": [{"text": "The field of Machine Translation (MT) has evolved very rapidly since the emergence of statistical approaches almost three decades ago.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8658476173877716}]}, {"text": "MT is nowadays a growing reality throughout the industry, which continues to adopt this technology as it results in demonstrable improvements in translation productivity, at least for technical domains.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9632300734519958}, {"text": "translation", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.9645646810531616}]}, {"text": "Meanwhile, the performance of MT systems in research continues to improve.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9928921461105347}]}, {"text": "In this regard, a recent study looked at the best-performing systems of the WMT shared task for seven language pairs during the period between 2007 and 2012, and estimated the improvement in translation quality during this period to be around 10% absolute, in terms of both adequacy and fluency ().", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7936332623163859}]}, {"text": "Having reached this level of research maturity and industrial adoption, in this paper we explore the feasibility of applying the current state-of-the-art MT technology to literary texts, what might be considered to be the last bastion of human translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.9584906697273254}]}, {"text": "The perceived wisdom is that MT is of no use for the translation of literature.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9748040437698364}, {"text": "translation of literature", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.852739155292511}]}, {"text": "We challenge that view, despite the fact that -to the best of our knowledge -the applicability of MT to literature has to date been only partially studied from an empirical point of view.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.987999439239502}]}, {"text": "In this paper we aim to measure the translatability of literary text.", "labels": [], "entities": []}, {"text": "Our empirical methodology relies on the fact that the applicability of MT to a given type of text can be assessed by analysing parallel corpora of that particular type and measuring (i) the degree of freedom of the translations (how literal the translations are), and (ii) the narrowness of the domain (how specific or general that text is).", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9732530117034912}]}, {"text": "Hence, we tackle the problem of measuring the translatability of literary text by comparing the degree of freedom of translation and domain narrowness for such texts to documents in two other domains which have been widely studied in the area of MT: technical documentation and news.", "labels": [], "entities": [{"text": "MT", "start_pos": 246, "end_pos": 248, "type": "TASK", "confidence": 0.9892022013664246}]}, {"text": "Furthermore, we assess the usefulness of MT in translating a novel between two closely-related languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9905585050582886}]}, {"text": "We build an MT system using state-of-theart domain-adaptation techniques and evaluate its performance against the professional human translation, using both automatic metrics and manual evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9866337776184082}]}, {"text": "To the best of our knowledge, this is the first time that a specific MT system is built to translate novels.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9637818932533264}, {"text": "translate novels", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.9119893312454224}]}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of the current state-of-theart in applying MT to literary texts.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9918839931488037}]}, {"text": "In Section 3 we measure the translatability of literary texts.", "labels": [], "entities": []}, {"text": "In Section 4 we explore the use of MT to translate a novel between two related languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9793013334274292}]}, {"text": "Finally, in Section 5 we present our conclusions and outline avenues of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to carryout our experiment on the translatability of literary texts, we use monolingual datasets for Spanish and parallel datasets for two language pairs with varying levels of relatedness: SpanishCatalan and Spanish-English.", "labels": [], "entities": []}, {"text": "Regarding the different types of corpora, we consider datasets that fall in the following four groups: novels, news, technical documentation and Europarl (EP).", "labels": [], "entities": [{"text": "Europarl (EP)", "start_pos": 145, "end_pos": 158, "type": "DATASET", "confidence": 0.8956275880336761}]}, {"text": "We use two sources for novels: two novels from Carlos Ruiz Zaf\u00f3n, The Shadow of the Wind (published originally in Spanish in 2001) and The Angel's Game (2008), for Spanish-Catalan and Spanish-English, referred to as novel1; and two novels from Gabriel Garc\u00eda M\u00e1rquez, Hundred Years of and Love in the Time of, for Spanish-English, referred to as novel2.", "labels": [], "entities": []}, {"text": "We use two sources of news data: a corpus made of articles from the newspaper El Peri\u00f3dico 3 (re-ferred to as news1) for Spanish-Catalan, and newscommentary v8 (referred to as news2) for SpanishEnglish.", "labels": [], "entities": [{"text": "SpanishEnglish", "start_pos": 187, "end_pos": 201, "type": "DATASET", "confidence": 0.9279710650444031}]}, {"text": "For technical documentation we use four datasets: DOGC, 5 a corpus from the official journal of the Catalan Goverment, for Spanish-Catalan; EMEA, 6 a corpus from the European Medicines Agency, for Spanish-English; JRC-Acquis (henceforth referred as JRC)), made of legislative text of the European Union, for SpanishEnglish; and KDE4, 7 a corpus of localisation files of the KDE desktop environment, for the two language pairs.", "labels": [], "entities": []}, {"text": "Finally, we consider the Europarl corpus v7 (, given it is widely used in the MT community, for Spanish-English.", "labels": [], "entities": [{"text": "Europarl corpus v7", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.972054680188497}]}, {"text": "All the datasets are pre-processed as follows.", "labels": [], "entities": []}, {"text": "First they are tokenised and truecased with Moses' () scripts.", "labels": [], "entities": []}, {"text": "Truecasing is carried outwith a model trained on the caWaC corpus for Catalan) and News Crawl 2012 8 both for English and Spanish.", "labels": [], "entities": [{"text": "Truecasing", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9398444294929504}, {"text": "caWaC corpus", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9526053369045258}, {"text": "News Crawl 2012 8", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.9538595676422119}]}, {"text": "Parallel datasets not available in a sentence-split format (novel1 and novel2) are sentence-split using Freeling).", "labels": [], "entities": []}, {"text": "All parallel datasets are then sentence aligned.", "labels": [], "entities": []}, {"text": "We use Hunalign () and keep only one-toone alignments.", "labels": [], "entities": [{"text": "Hunalign", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.4937930107116699}]}, {"text": "The dictionaries used for SpanishCatalan and Spanish-English are extracted from Apertium bilingual dictionaries for those language pairs.", "labels": [], "entities": [{"text": "SpanishCatalan", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.9052568674087524}, {"text": "Apertium bilingual dictionaries", "start_pos": 80, "end_pos": 111, "type": "DATASET", "confidence": 0.916410505771637}]}, {"text": "9,10 Only sentence pairs for which the confidence score of the alignment is >= 0.4 are kept.", "labels": [], "entities": []}, {"text": "11 Although most of the parallel datasets are provided in sentence-aligned form, we realign them to ensure that the data used to calculate word alignment perplexity are properly aligned at sentence level. is to avoid having high word alignment perplexities due, not to high degrees of translation freedom, but to the presence of misaligned parallel data.", "labels": [], "entities": [{"text": "word alignment perplexity", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.6801408032576243}]}, {"text": "Our systems are evaluated with a set of state-ofthe-art automatic metrics: BLEU (), TER () and METEOR 1.5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9991530179977417}, {"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.997837483882904}, {"text": "METEOR 1.5", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9712305963039398}]}, {"text": "We decided to compare our system also to widelyused on-line third-party systems, as these are the ones that a translator could easily have access to.", "labels": [], "entities": []}, {"text": "We consider the following three systems: Google Translate, 12 Apertium (Forcada et al., 2011) 13 and 12 https://translate.google.com 13 http://apertium.org/ Lucy.", "labels": [], "entities": [{"text": "Apertium", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9284306764602661}]}, {"text": "14 These three systems follow different approaches; while the first is statistical, the second and the third are rule-based, classified respectively as shallow and deep formalisms.", "labels": [], "entities": []}, {"text": "shows the results of the third-party system and compares their scores with our best domainadapted system in terms of relative improvement (columns diff).", "labels": [], "entities": []}, {"text": "The results of the third-party systems are similar, albeit slighly lower, compared to our baseline (cf..", "labels": [], "entities": []}, {"text": "We conducted statistical significance tests for BLEU between our best domain-adapted system, the baseline and the three third-party systems using paired bootstrap resampling) with 1,000 iterations and p = 0.01.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9923834204673767}]}, {"text": "In all cases the improvement brought by our best system is found out to be significant.", "labels": [], "entities": []}, {"text": "Finally we report on the percentage of translations that are equal in the MT output and in the reference.", "labels": [], "entities": [{"text": "MT output", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.7965873181819916}]}, {"text": "These account for 15.3% of the sentences for the baseline and 19.7% for the best domain-adapted system.", "labels": [], "entities": []}, {"text": "It should be noted though that these tend to be short sentences, so if we consider their percentage in terms of words, they account for 4.97% and 7.15% of the data, respectively.", "labels": [], "entities": []}, {"text": "If we consider also the translations that can reach the reference in at most five character editing steps, then the percentage of equal and near-equal translations pro-duced by our best domain-adapted system reaches 29.5% of the sentences.", "labels": [], "entities": []}, {"text": "To gain further insight on the results, we conducted a manual evaluation.", "labels": [], "entities": []}, {"text": "A common procedure (e.g. conducted in the MT shared task at WMT) consists of ranking MT translations.", "labels": [], "entities": [{"text": "MT shared task", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.897329052289327}, {"text": "WMT", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.6916363835334778}, {"text": "MT translations", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.9116584658622742}]}, {"text": "Given the source and target sides of the reference (human) translations, and two or more outputs from MT systems, these outputs are ranked according to their quality, i.e. how close they are to the reference, e.g. in terms of adequacy and/or fluency.", "labels": [], "entities": []}, {"text": "In our experiment, we are of course not interested in comparing two MT systems, but rather one MT system (the best one according to the automatic metrics) and the human translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.972724974155426}]}, {"text": "Hence, we conduct the rank-based manual evaluation in a slightly modified setting; we do not provide the target of the reference translation as reference but as one of the MT systems to be ranked.", "labels": [], "entities": [{"text": "MT", "start_pos": 172, "end_pos": 174, "type": "TASK", "confidence": 0.9100207686424255}]}, {"text": "The evaluator thus is given the source-side of the reference and two translations, one being the human translation and the other the translation produced by an MT system.", "labels": [], "entities": []}, {"text": "The evaluator of course does not know which is which.", "labels": [], "entities": []}, {"text": "Moreover, in order to avoid any bias with respect to MT, they do not know that one of them has been produced by a human.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.989956259727478}]}, {"text": "Two bilingual speakers in Spanish and Catalan, with a background in linguistics but without indepth knowledge of MT (again, to avoid any bias with respect to MT) ranked a set of 101 translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.585969865322113}]}, {"text": "We carried out this rank-based evaluation with the Appraise tool, using its 3-way ranking task type, whereby given two translations A and B, the evaluator can rank them as A>B (if A is better than B), A<B (if A is worse than B) and A=B (if both are of the same quality).", "labels": [], "entities": []}, {"text": "Considering the aggregated 202 judgements, we have the breakdown shown in.", "labels": [], "entities": []}, {"text": "In most cases (41.58% of the judgements), both the human translation (HT) and the MT are considered to be of equal quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.8857865929603577}]}, {"text": "The HT is considered better than MT in 39.11% of the cases.", "labels": [], "entities": [{"text": "HT", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.7058623433113098}, {"text": "MT", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.5051062703132629}]}, {"text": "Perhaps surprisingly, the evaluators ranked MT higher than HT in almost 20% of their judgements.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.6522645950317383}, {"text": "HT", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.42913269996643066}]}, {"text": "We now delve deeper into the results and show in the breakdown of judgements by evaluator.", "labels": [], "entities": []}, {"text": "For around two thirds of the sentences, both evaluators agreed in their judgement: in 28.71% of the sentences both for HT=MT and for HT>MT, and in 9.9% of the sentences for HT<MT.", "labels": [], "entities": [{"text": "HT<MT", "start_pos": 173, "end_pos": 178, "type": "TASK", "confidence": 0.6488332947095236}]}, {"text": "They disagreed in the remaining one third of the data, the two main disagreements being between HT=MT and HT>MT (13.86%) and between HT=MT and HT<MT (11.88%).", "labels": [], "entities": []}, {"text": "The remaining case of disagreement (between HT>MT and HT<MT) is encountered less frequently (6.93%).", "labels": [], "entities": [{"text": "HT>MT", "start_pos": 44, "end_pos": 49, "type": "TASK", "confidence": 0.43714337547620136}]}], "tableCaptions": [{"text": " Table 1: Datasets used for MT", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.994208574295044}]}, {"text": " Table 2: Automatic evaluation scores for the MT systems built", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9799743890762329}]}, {"text": " Table 3: Automatic evaluation scores for third-party MT systems", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9735972285270691}]}, {"text": " Table 4. In most cases  (41.58% of the judgements), both the human trans- lation (HT) and the MT are considered to be of equal  quality. The HT is considered better than MT in  39.11% of the cases. Perhaps surprisingly, the eval- uators ranked MT higher than HT in almost 20% of  their judgements.", "labels": [], "entities": []}, {"text": " Table 4: Manual Evaluation. Breakdown of ranks (over- all)", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9108970761299133}]}, {"text": " Table 5: Manual Evaluation. Breakdown of ranks (per  evaluator)", "labels": [], "entities": []}, {"text": " Table 7: Manual Evaluation. Examples of translations ranked as HT<MT", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.2564983665943146}]}, {"text": " Table 6: Manual Evaluation. Avg sentence length per  rank", "labels": [], "entities": [{"text": "Avg sentence length", "start_pos": 29, "end_pos": 48, "type": "METRIC", "confidence": 0.8739947477976481}]}]}