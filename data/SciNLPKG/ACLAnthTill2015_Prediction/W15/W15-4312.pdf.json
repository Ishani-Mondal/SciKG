{"title": [{"text": "Bekli: A Simple Approach to Twitter Text Normalization", "labels": [], "entities": [{"text": "Twitter Text Normalization", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6100412408510844}]}], "abstractContent": [{"text": "Every day, Twitter users generate vast quantities of potentially useful information in the form of written language.", "labels": [], "entities": []}, {"text": "Due to Twitter's frequently informal tone, text normalization can be a crucial element for exploiting that information.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7634666562080383}]}, {"text": "This paper outlines our approach to text normalization used in the WNUT shared task.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7823838293552399}, {"text": "WNUT shared task", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.5350208878517151}]}, {"text": "We show that a very simple solution , powered by a modestly sized, partially-curated wordlist-combined with a modest re-ranking scheme-can deliver respectable results .", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter is an immense, living collection of written language from allover the world.", "labels": [], "entities": []}, {"text": "Every day, Twitter publishes a staggering 500 million tweets . The content of Twitter is virtually unlimited, and has proven useful for much research, including epidemiology:; and sentiment analysis:,,, . It would take many readers to keep up with Twitter's output, but, fortunately, we have natural language processing (NLP) methods that can automatically filter, condense, or extract information from text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.8894668519496918}]}, {"text": "However, NLP approaches are typically trained on formal edited text, and struggle with the informal, unedited text of Twitter.", "labels": [], "entities": []}, {"text": "But there is a wellknown way to mitigate this problem: text normalization, i.e. replacing non-standard tokens with their standard equivalents, yielding text that will be more agreeable to NLP.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7401979118585587}]}, {"text": "One flavor of non-standard writing-what I have previously focused on-is what I call \"vernacular orthography\" (VO).", "labels": [], "entities": [{"text": "vernacular orthography\" (VO)", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6669492324193319}]}, {"text": "VO is spelling that indicates itentional non-standard pronunciation, such as when the string \"dat\" stands in for \"that\".", "labels": [], "entities": [{"text": "VO", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8494660258293152}]}, {"text": "While numerous papers offer solutions for text normalization (e.g.,,,,), and a few build models based on phonemic similarity (e.g.,), none to our knowledge have addressed VO in particular.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.781516820192337}, {"text": "VO", "start_pos": 171, "end_pos": 173, "type": "TASK", "confidence": 0.9686762094497681}]}, {"text": "This paper, too, addresses the general normalization problem, but uses lessons learned attempting to normalize VO.", "labels": [], "entities": [{"text": "normalization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9449202418327332}, {"text": "normalize VO", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.6361595541238785}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Team Results for the unconstrained task.", "labels": [], "entities": []}]}