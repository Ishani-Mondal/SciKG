{"title": [{"text": "NCSU_SAS_SAM: Deep Encoding and Reconstruction for Normalization of Noisy Text", "labels": [], "entities": [{"text": "NCSU_SAS_SAM", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7011974692344666}, {"text": "Normalization of Noisy Text", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.8909728080034256}]}], "abstractContent": [{"text": "As a participant in the W-NUT Lexical Normalization for English Tweets challenge , we use deep learning to address the constrained task.", "labels": [], "entities": [{"text": "W-NUT Lexical Normalization for English Tweets challenge", "start_pos": 24, "end_pos": 80, "type": "TASK", "confidence": 0.8066938391753605}]}, {"text": "Specifically, we use a combination of two augmented feed forward neural networks, a flagger that identifies words to be normalized and a normalizer, to take in a single token at a time and output a corrected version of that token.", "labels": [], "entities": []}, {"text": "Despite avoiding off-the-shelf tools trained on external data and being an entirely context-free model, our system still achieved an F1-score of 81.49%, comfortably surpassing the next runner up by 1.5% and trailing the second place model by only 0.26%.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9996991157531738}]}], "introductionContent": [{"text": "The phenomenal growth of social media, web forums, and online reviews has spurred a growing interest in automated analysis of usergenerated text.", "labels": [], "entities": []}, {"text": "User-generated text presents significant computational challenges because it is often highly disfluent.", "labels": [], "entities": []}, {"text": "To address these challenges, we have begun to see a growing demand for tools and techniques to transform noisy usergenerated text into a canonical form, most recently in the Workshop on Noisy User Text at the Association for Computational Linguistics.", "labels": [], "entities": []}, {"text": "This work describes a submission to the Lexical Normalization for English Tweets challenge as part of this workshop ( Motivated by the success of prior deep neural network architectures, particularly denoising autoencoders, we have developed an approach to transform noisy user-generated text into a canonical form with a feed-forward neural network augmented with a projection layer.", "labels": [], "entities": [{"text": "Lexical Normalization for English Tweets challenge", "start_pos": 40, "end_pos": 90, "type": "TASK", "confidence": 0.9212212959925333}]}, {"text": "The model performs a character-level analysis on each word of the input.", "labels": [], "entities": []}, {"text": "The absence of hand-engineered features and the avoidance of direct and indirect external data make this model unique among the three topperforming models in the constrained task.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe each component of our model.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the specific instantiation of our model, and in Section 4 we present and discuss results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The model was implemented in Theano, a Python library for fast evaluation of multidimensional arrays using matrix operations (.", "labels": [], "entities": [{"text": "Theano", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9303135871887207}]}, {"text": "We used Theano's implementation of backpropagation to train our model.", "labels": [], "entities": []}, {"text": "For our window size, we selected 25 characters, which is large enough to completely represent 99.9% of the tokens in the training data while remaining computationally feasible.", "labels": [], "entities": []}, {"text": "There are also a number of hyperparameters: the number and size of hidden layers, the size of character embeddings, and the dropout rate.", "labels": [], "entities": []}, {"text": "We tried various combinations of values between 50 and 6000 for the size and 1 and 4 for the number of hidden layers in both our Normalizer and Flagger.", "labels": [], "entities": [{"text": "Flagger", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.9303596019744873}]}, {"text": "Some combinations we tried can be seen in the results section.", "labels": [], "entities": []}, {"text": "Especially large sizes and numbers of layers proved to require more memory than our GPU could support, and training them on our CPU was exceptionally slow.", "labels": [], "entities": []}, {"text": "We also tried 50% and 75% dropout, meaning that during training we randomly excluded hidden nodes from consideration at each layer.", "labels": [], "entities": []}, {"text": "Dropout has been shown to improve performance by discouraging overfitting on the training data, and 50% and 75% are common dropout rates.", "labels": [], "entities": []}, {"text": "We found the highest F1 score on the validation data for the Normalizer with two hidden layers of size 2000 each and 50% dropout.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9846403002738953}]}, {"text": "This was close to the maximum size our GPU could support without reducing the batch size to be too small to take advantage of the parallelism.", "labels": [], "entities": []}, {"text": "The Flagger's highest score was found at two hidden layers of size 1000 each and 75% dropout.", "labels": [], "entities": []}, {"text": "Attempts to provide hidden layers of different sizes consistently found inferior results.", "labels": [], "entities": []}, {"text": "For the size of each embedding in the character projection layer, 10 had proven effective earlier in a simpler unpublished Twitter part-of-speech task.", "labels": [], "entities": [{"text": "character projection layer", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.8138808012008667}]}, {"text": "We selected 25 for our character embedding size to account for the greater complexity of a normalization task.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.9216776192188263}]}, {"text": "We separated the provided training data into 90% training data, 5% validation data and 5% was held out as test data.", "labels": [], "entities": []}, {"text": "In order to construct a useful model on the small amount of available data, we iterate training over the same data many times.", "labels": [], "entities": []}, {"text": "Our model stopped training after 150 training iterations in which there was no improvement on the validation set.", "labels": [], "entities": []}, {"text": "We chose 150 iterations as the smallest value that did not lead to ending the training at a clearly suboptimal value.", "labels": [], "entities": []}, {"text": "The training also stops at 5,000 iterations but in practice it converged before reaching this value.", "labels": [], "entities": []}, {"text": "Early in development we found that the Normalizer had exceptional trouble reconstructing twitter-specific objects, that is, hash-tags (#goodday), at-mentions (@marysue) and URLs (http://blahblah.com).", "labels": [], "entities": []}, {"text": "Generally its behavior in all three cases was to follow the standard marker characters (@, #, http://) with a string of gibberish unrelated to the word itself.", "labels": [], "entities": []}, {"text": "Because these are protected categories that should not be changed, we removed them from the training data and rely on the Flagger to flag them as not to be corrected.", "labels": [], "entities": [{"text": "Flagger", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.9259374141693115}]}, {"text": "We used layer-wise pre-training, meaning we first trained with zero hidden layers (going directly from the character projection to the softmax layer) to initialize the character embeddings, then we trained with one hidden layer, initializing the character embeddings with their previously trained values.", "labels": [], "entities": []}, {"text": "When we trained the full model using two hidden layers, we initialized both the character projection layer and the weights from the projected input to the first hidden layer with the values learned before.", "labels": [], "entities": []}, {"text": "The model continued to learn all the weights it used.", "labels": [], "entities": []}, {"text": "Pretrained weights continued to be trained in the full model, although \"freezing\" some pretrained weights after pretraining and only training later weights in the full model has shown success when working with large amounts of unsupervised data and maybe worthwhile to consider in future work.", "labels": [], "entities": []}, {"text": "Running on an NVIDIA GeForce GTX 680 GPU with 2 GB of onboard memory, training the Normalizer took about six hours.", "labels": [], "entities": []}, {"text": "We do not include CPU and RAM specifications because they were not heavily utilized in the GPU implementation.", "labels": [], "entities": [{"text": "RAM", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9695886969566345}]}, {"text": "The Flagger was considerably faster to train than the Normalizer, taking only a little over half an hour.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the constrained task", "labels": [], "entities": []}, {"text": " Table 2: Model Scores on Validation and Test Data", "labels": [], "entities": []}, {"text": " Table 3: Flagger scores on Validation and Test Data", "labels": [], "entities": [{"text": "Flagger", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9563515782356262}]}]}