{"title": [], "abstractContent": [{"text": "Due to the short and noisy nature of Twit-ter microposts, detecting named entities is often a cumbersome task.", "labels": [], "entities": [{"text": "Twit-ter microposts", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.8984462022781372}, {"text": "detecting named entities", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8693796197573344}]}, {"text": "As part of the ACL2015 Named Entity Recognition (NER) shared task, we present a semi-supervised system that detects 10 types of named entities.", "labels": [], "entities": [{"text": "ACL2015 Named Entity Recognition (NER) shared task", "start_pos": 15, "end_pos": 65, "type": "TASK", "confidence": 0.749253488249249}]}, {"text": "To that end, we leverage 400 million Twitter microposts to generate powerful word embeddings as input features and use a neural network to execute the classification.", "labels": [], "entities": []}, {"text": "To further boost the performance , we employ dropout to train the network and leaky Rectified Linear Units (ReLUs).", "labels": [], "entities": []}, {"text": "Our system achieved the fourth position in the final ranking, without using any kind of hand-crafted features such as lexical features or gazetteers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Users on Online Social Networks such as Facebook and Twitter have the ability to share microposts with their friends or followers.", "labels": [], "entities": []}, {"text": "These microposts are short and noisy, and are therefore much more difficult to process for existing Natural Language Processing (NLP) pipelines.", "labels": [], "entities": []}, {"text": "Moreover, due to the informal and contemporary nature of these microposts, they often contain Named Entities (NEs) that are not part of any gazetteer.", "labels": [], "entities": []}, {"text": "In this challenge, we tackled Named Entity Recognition (NER) in microposts.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.7661984860897064}]}, {"text": "The goal was to detect named entities and classify them in one of the following 10 categories: company, facility, geolocation, music artist, movie, person, product, sports team, tv show and other entities.", "labels": [], "entities": []}, {"text": "To do so, we only used word embeddings that were automatically inferred from 400 million Twitter microposts as input features.", "labels": [], "entities": []}, {"text": "Next, these word embeddings were used as input to a neural network to classify the words in the microposts.", "labels": [], "entities": []}, {"text": "Finally, a post-processing step was executed to check for inconsistencies, given that we classified on a wordper-word basis and that a named entity can span multiple words.", "labels": [], "entities": []}, {"text": "An overview of the task can be found in.", "labels": [], "entities": []}, {"text": "The challenge consisted of two subtasks.", "labels": [], "entities": []}, {"text": "For the first subtask, the participants only needed to detect NEs without categorizing them.", "labels": [], "entities": []}, {"text": "For the second subtask, the NEs also needed to be categorized into one of the 10 categories listed above.", "labels": [], "entities": [{"text": "NEs", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9707803130149841}]}, {"text": "Throughout the remainder of this paper, only the latter subtask will be considered, given that solving subtask two makes subtask one trivial.", "labels": [], "entities": []}], "datasetContent": [{"text": "The challenge provided us with three different datasets: train, dev and dev_2015.", "labels": [], "entities": []}, {"text": "These datasets have 1795, 599 and 420 microposts, respectively, also containing 1140, 356 and 272 NEs, respectively.", "labels": [], "entities": []}, {"text": "The train and dev datasets came from the same period and therefore have some overlap in NEs.", "labels": [], "entities": []}, {"text": "Moreover, they contained the complete dataset of.", "labels": [], "entities": []}, {"text": "The microposts within dev_2015, however, were sampled more recently and resembled the test set of this challenge.", "labels": [], "entities": [{"text": "dev_2015", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.750187337398529}]}, {"text": "The test set consisted of 1000 microposts, having 661 NEs.", "labels": [], "entities": [{"text": "NEs", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9398655295372009}]}, {"text": "The train and dev dataset will be used as training set throughout the experiments and the dev_2015 dataset will be used as development set.", "labels": [], "entities": [{"text": "dev_2015 dataset", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.691639631986618}]}, {"text": "For inferring the word embeddings, a set of raw Twitter microposts was used, collected during 300 days using the Twitter Streaming API, from microposts using the micropost language classifier of, 400 million raw English Twitter microposts were left.", "labels": [], "entities": []}, {"text": "Our best model realized a F1-score of 58.82% on subtask one (no categories), hereby realizing an error reduction of 17.84% over the baseline (49.88%).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9997057318687439}, {"text": "error reduction", "start_pos": 97, "end_pos": 112, "type": "METRIC", "confidence": 0.9804771840572357}]}, {"text": "On subtask two (10 categories), an F1-score of 43.75% was realized, yielding an error reduction of 17.32% over the baseline (31.97%).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9997484087944031}, {"text": "error reduction", "start_pos": 80, "end_pos": 95, "type": "METRIC", "confidence": 0.9883410632610321}]}, {"text": "A break-down of the results on the different NE categories can be found in.", "labels": [], "entities": []}, {"text": "Our system ranked fourth in both subtasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of the influence of the context  window size of the word embeddings on the ac- curacy of predicting NER tags using a neural net- work with an input window of five words, 500 hid- den Leaky ReLU units and dropout. All word em- beddings are inferred using negative sampling and  a Skip-gram architecture, and have a vector size  of 400. The baseline accuracy is achieved when  tagging all words of a micropost with the O-tag.", "labels": [], "entities": [{"text": "predicting NER tags", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8043177723884583}, {"text": "accuracy", "start_pos": 369, "end_pos": 377, "type": "METRIC", "confidence": 0.9884126782417297}]}, {"text": " Table 3: Evaluation of the influence of the input layer and hidden layer size on the accuracy/error reduc- tion when predicting NE tags. The fixed neural network is trained with dropout, word embeddings of  size 400 and ReLUs. The error reduction values are calculated using the baseline which tags all words  with an O-tag.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9992202520370483}, {"text": "predicting NE tags", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8003607988357544}, {"text": "ReLUs", "start_pos": 221, "end_pos": 226, "type": "METRIC", "confidence": 0.9412705898284912}]}]}