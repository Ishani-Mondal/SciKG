{"title": [{"text": "Evaluation of Coreference Resolution Tools for Polish from the Information Extraction Perspective", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.972801148891449}, {"text": "Information Extraction Perspective", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.6820616722106934}]}], "abstractContent": [{"text": "In this paper we discuss the performance of existing tools for coreference resolution for Polish from the perspective of information extraction tasks.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.9671907722949982}, {"text": "information extraction tasks", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.848671019077301}]}, {"text": "We take into consideration the source of mentions, i.e., gold standard vs mentions recognized automatically.", "labels": [], "entities": []}, {"text": "We evaluate three existing tools, i.e., IKAR, Ruler and Bartek on the KPWr corpus.", "labels": [], "entities": [{"text": "IKAR", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.6586968898773193}, {"text": "Ruler", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8590530753135681}, {"text": "KPWr corpus", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9094143807888031}]}, {"text": "We show that the widely used met-rics for coreference evaluation (B 3 , MUC, CEAF, BLANC) do not reflect the real performance when dealing with the task of semantic relations recognition between named entities.", "labels": [], "entities": [{"text": "coreference evaluation", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9176126420497894}, {"text": "BLANC", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.991377592086792}, {"text": "semantic relations recognition between named entities", "start_pos": 156, "end_pos": 209, "type": "TASK", "confidence": 0.7971112132072449}]}, {"text": "Thus, we propose a supplementary metric called PARENT, which measures the correctness of linking between referential mentions and named entities .", "labels": [], "entities": [{"text": "PARENT", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9967355132102966}]}], "introductionContent": [{"text": "In this paper we approach the problem of coreference resolution and its evaluation metrics.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.9763998687267303}]}, {"text": "We consider this problem from a slightly different perspective-not as a simple clustering problem, but rather as a problem of extracting information from text.", "labels": [], "entities": []}, {"text": "We make an observation that not every mention carries equal amount of information, e.g., when considering a pronoun resolution problem there are usually a few named entities that can be assigned to real world objects and relatively larger amount of pronouns that carry almost no information about the object they are referring to, without resolving the coreference with the named entity.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7493707239627838}]}, {"text": "Thus we do not want to treat named entities and pronouns equally as in the case below.", "labels": [], "entities": []}, {"text": "We can imagine a document with two named entities, for simplicity each with equal count of n pronouns in gold coreferential clusters: {Romeo, he 1 , he 2 , . .", "labels": [], "entities": []}, {"text": ", hen } {Juliet, she 1 , she 2 , . .", "labels": [], "entities": []}, {"text": ", she n } and two possible system responses, one with two pronouns interchanged between coreferential clusters: {Romeo, she 1 , he 2 , . .", "labels": [], "entities": []}, {"text": ", hen } {Juliet, he 1 , she 2 , . .", "labels": [], "entities": []}, {"text": ", she n } and the second with the named entities interchanged: {Juliet, he 1 , he 2 , . .", "labels": [], "entities": []}, {"text": ", hen } {Romeo, she 1 , she 2 , . .", "labels": [], "entities": []}, {"text": ", she n } According to the measures which do not distinguish between types of mentions and are based only on the similarity of clusters, these two responses are scored equally.", "labels": [], "entities": []}, {"text": "However, from information extraction perspective the first answer is almost correct, while the second gives us totally incorrect information about both named entities.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.8077894747257233}]}, {"text": "Thus we propose a supplementary method to score the performance of coreference resolution systems with respect to different types of mentions.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.9262080788612366}]}], "datasetContent": [{"text": "We evaluated the following tools for Polish coreference resolution: IKAR, Bartek and Ruler.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7984082996845245}, {"text": "IKAR", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.6850752234458923}, {"text": "Ruler", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.5959197282791138}]}, {"text": "The results for IKAR were obtained for several different configuration settings.", "labels": [], "entities": [{"text": "IKAR", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.6553822159767151}]}, {"text": "We tested it on the gold standard mentions and on the mentions that were automatically added by simply annotating all the agreed phrases and pronouns, and by using Minos for the detection of zero subject verbs.", "labels": [], "entities": []}, {"text": "Bartek and Ruler were tested on the same corpus but with system mentions annotated by their own system for automatic mention annotation, i.e.,).", "labels": [], "entities": []}, {"text": "For the evaluation we used 10-fold cross validation on the KPWr corpus (see next section: Comparison of different metrics fora sample system response.", "labels": [], "entities": [{"text": "KPWr corpus", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9434439241886139}]}, {"text": "Holen (2013) made some critical observations on the nature of commonly used evaluation metrics, claiming that the loss of information value-an important factor in the perception of coreference resolution-is not addressed good enough in the current evaluation metrics.", "labels": [], "entities": [{"text": "coreference resolution-is", "start_pos": 181, "end_pos": 206, "type": "TASK", "confidence": 0.9355405867099762}]}, {"text": "Some of the issues with different levels of informativeness of mentions were addressed by.", "labels": [], "entities": []}, {"text": "The main idea was to extend the existing metrics with link weights that would reflect the informativeness of certain types of relations.", "labels": [], "entities": []}, {"text": "These enhancements provided a more accurate way of scoring coreference results, however, making them less intuitive and harder to interpret.", "labels": [], "entities": []}, {"text": "presented an approach that considers coreference results as mention chains and scores every mention according to whether it has a correct direct antecedent.", "labels": [], "entities": []}, {"text": "As an extension of this approach he proposed to consider the relations to the closest preceding nouns, e.g., two pronouns are not really useful for higher level applications of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.9621967971324921}]}, {"text": "The final proposition was to determine the so-called anchor mentions for each key coreference chain and to measure the score as the harmonic mean of the score for detection of these anchor mentions and the score for resolving mentions to anchor mentions that were found by the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of different metrics for a sample system response.", "labels": [], "entities": []}]}