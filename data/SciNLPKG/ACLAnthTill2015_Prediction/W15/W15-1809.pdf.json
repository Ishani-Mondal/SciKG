{"title": [], "abstractContent": [{"text": "In this paper we describe a question interpretation module designed as apart of a Question Answering Dialogue System (QADS) which is used for an interactive quiz application.", "labels": [], "entities": [{"text": "question interpretation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8437972366809845}, {"text": "Question Answering Dialogue", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.7861236830552419}]}, {"text": "Question interpretation is achieved in applying a sequence of classification , information extraction, query formalization and query expansion tasks.", "labels": [], "entities": [{"text": "Question interpretation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8488680720329285}, {"text": "information extraction", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.8076246082782745}, {"text": "query expansion", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7216424345970154}]}, {"text": "The process of a question classification is performed based on a domain-specific taxonomy of semantic roles and relations.", "labels": [], "entities": [{"text": "question classification", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7116317451000214}]}, {"text": "Our taxonomy was designed in accordance with the real spoken dialogue data.", "labels": [], "entities": []}, {"text": "The SVM-based classifier is trained to predict the Expected Answer Type (EAT) with the precision of 82%.", "labels": [], "entities": [{"text": "Expected Answer Type (EAT)", "start_pos": 51, "end_pos": 77, "type": "METRIC", "confidence": 0.9109523892402649}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9992910623550415}]}, {"text": "In order to retrieve a correct answer, focus word(-s) are extracted to augment the EAT identified by the system.", "labels": [], "entities": []}, {"text": "Our hybrid algorithm for the extraction of focus words demonstrates the accuracy of 94.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9997772574424744}]}, {"text": "EAT together with focus words are formalized in a query, which is further expanded with the synonyms from WordNet.", "labels": [], "entities": [{"text": "EAT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6332585215568542}, {"text": "WordNet", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9716981649398804}]}, {"text": "The expanded query facilitates the search and retrieval of the information that is necessary to generate the system's responses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Any question answering (QA) system has to be able to give as precise as possible answers to natural language questions.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8405530095100403}]}, {"text": "In order to perform this task with a reasonably high accuracy, an adequate question interpretation is required.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9951756000518799}, {"text": "question interpretation", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7418890595436096}]}, {"text": "In the NLP field, this problem is often defined as the question classification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7149336040019989}]}, {"text": "Due to the ambiguity of natural language utterances the task may become very complicated.", "labels": [], "entities": []}, {"text": "For this reason the question classification phase has proven to be one of the most important parts of many QA system.", "labels": [], "entities": [{"text": "question classification phase", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.882231334845225}, {"text": "QA", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.8824499249458313}]}, {"text": "If a question type is not correctly identified, the system will not be able to find the correct and/or complete answer.", "labels": [], "entities": []}, {"text": "According to, correctly classified questions are answered correctly twice as often as misclassified ones.", "labels": [], "entities": []}, {"text": "The study conducted by set anew modern foundation in the QA task.", "labels": [], "entities": [{"text": "QA task", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.91692054271698}]}, {"text": "An end-to-end open-domain QA system has been In TREC-8 1 , it achieved the highest result by demonstrating the accuracy of 77.7%.", "labels": [], "entities": [{"text": "TREC-8 1", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.7105966806411743}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9995898604393005}]}, {"text": "The designed system performs question processing, including question classification, focus and key words extraction, as well as the specification of an expected answer type.", "labels": [], "entities": [{"text": "question processing", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8317946493625641}, {"text": "question classification", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7984959781169891}, {"text": "focus and key words extraction", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6807094812393188}]}, {"text": "In 2011, IBM Watson QA system won Jeopardy!", "labels": [], "entities": [{"text": "Jeopardy!", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.7740884125232697}]}, {"text": "quiz game, where it was able to beat two highest ranked players.", "labels": [], "entities": []}, {"text": "The system includes a component responsible for the question analysis: the system needs to know what was asked in a question.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7686233222484589}]}, {"text": "Having this knowledge, the system generates candidate answers.", "labels": [], "entities": []}, {"text": "In 2013 IBM made an attempt to adapt Watson QA to the healthcare domain.", "labels": [], "entities": []}, {"text": "The scenario targeted in our application is comparable to the Jeopardy!", "labels": [], "entities": [{"text": "Jeopardy!", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.7855179607868195}]}, {"text": "Our system, however, provides an interactive quiz game meaning that the returned answers are not just extracted information chunks or slot fillers, or database entries, but rather full-fledged dialogue utterances.", "labels": [], "entities": []}, {"text": "The domain, on the other hand, is restricted to bibliographical facts about a famous person, and the player's task is to guess his/her identity by asking ten questions of various types.", "labels": [], "entities": []}, {"text": "For such a close-domain, for the system to understand a question it is possible to narrow down the knowledge available to it.", "labels": [], "entities": []}, {"text": "For example, structured knowledge bases can be used, e.g. Freebase . They are however not complete to achieve sufficient cover-age of factual information required for our game.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9755290150642395}]}, {"text": "Therefore, the content that the system operates on is a bigger collection of unstructured free texts, namely, Wikipedia articles . This impacts search and retrieval tasks.", "labels": [], "entities": []}, {"text": "As a consequence, the output of a question interpretation module should be a rather comprehensive query capturing various semantic information concerning events in question, entities involved in this event and their properties, and type of relations between entities and possibly between events.", "labels": [], "entities": [{"text": "question interpretation module", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.7753271460533142}]}, {"text": "Thus, question interpretation is defined as a sequence of classification, information extraction, query formalization and query expansion tasks.", "labels": [], "entities": [{"text": "question interpretation", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.8314633965492249}, {"text": "information extraction", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7815936207771301}, {"text": "query expansion", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.6776301115751266}]}, {"text": "Given the closeness of the domain, the system can operate on the basis of pre-defined domain-specific taxonomy of various semantic relations between different types of entities in order to compute an Expected Answer Type (EAT).", "labels": [], "entities": []}, {"text": "The EAT is classified using statistical classifiers like Support Vector Machines (SVM) operating on multiple features, such as n-grams, part-of-speech and other syntactic information.", "labels": [], "entities": []}, {"text": "The EAT is further augmented with question focus word(-s) information to determine the main event in question.", "labels": [], "entities": [{"text": "EAT", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5907468199729919}]}, {"text": "Both, the EAT and focus word(-s), are formalized in a query which, on its turn, is expanded to cover as many as possible natural language variations.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work that has been reported in the area of general question answering and in question classification in particular.", "labels": [], "entities": [{"text": "general question answering", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7565900484720866}, {"text": "question classification", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.8166295289993286}]}, {"text": "In Section 3 we outline performed experiments describing the data, tagset, features, algorithms and evaluation metrics that have been used.", "labels": [], "entities": []}, {"text": "Section 4 reports on the experimental results, applying SVM on various feature combinations, to assess the automatic EAT classification.", "labels": [], "entities": [{"text": "EAT classification", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7637052237987518}]}, {"text": "We also assess the semantic relations learnability by partitioning the training set and increasing a number of training instances in each next run.", "labels": [], "entities": []}, {"text": "In Section 5 we describe an algorithm for automatic extraction of focus words.", "labels": [], "entities": [{"text": "automatic extraction of focus words", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.8105784177780151}]}, {"text": "Section 6 explains how the query is generated and expanded.", "labels": [], "entities": []}, {"text": "Section 7 summarizes our findings and outlines plans for the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "It is desirable, that in a quiz game the system provides the player with a correct answer, and rather acknowledge the fact if no answer is not found by generating utterances like \"Sorry, I do not have this information\".", "labels": [], "entities": []}, {"text": "7 In other words, to return the correct answer or acknowledge the fact that no answer is found is more important for the overall system performance than to return a wrong answer.", "labels": [], "entities": []}, {"text": "That is why the precision for both question classification and answer detection tasks was more important than the recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9995624423027039}, {"text": "question classification", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.809596598148346}, {"text": "answer detection tasks", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.8323845863342285}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9989162683486938}]}, {"text": "The precision metrics indicates how relevant the returned answers is to the question asked.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999112069606781}]}, {"text": "Recall, by contrast, indicates how many relevant answers are returned by the classifier, which is not important information for the system to know, therefore disregarded in further evaluations.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9824284911155701}]}, {"text": "We calculated a weighted precision score taking into account the proportion of instances in each class.", "labels": [], "entities": [{"text": "precision score", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9748393595218658}]}, {"text": "The weighted precision is computed by the following formula: 6 http://www.cis.upenn.edu/\u223ctreebank/ 7 To make the game more entertaining, the system can always play with strategies to turn a negative situation in a system's favour.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9362144470214844}]}, {"text": "For example, if no answer is found, the system may ask the player to put another question claiming that the previous one was not eligible for whatever reasons or the answer to it would lead to quick game end, or alike.", "labels": [], "entities": []}, {"text": "where P c -precision fora certain class of questions, W c -weight associated with that class (number of instances in a individual class).", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9919106364250183}]}, {"text": "As a baseline it is common practice to use the majority class tag, but for our data sets such a baseline is not very useful because of the relatively low frequencies of the tags for many classes (see).", "labels": [], "entities": []}, {"text": "Instead, we computed a baseline that is based on a single feature, namely, bag-of-words when training the Naive Bayes classifier.", "labels": [], "entities": []}, {"text": "The baseline classifier achieved the precision of 56%.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9997058510780334}]}, {"text": "It was implemented using Multinomial Naive Bayes algorithm from scikit-learn ().", "labels": [], "entities": []}, {"text": "Naive Bayes has been chosen for several reasons.", "labels": [], "entities": []}, {"text": "Firstly, it is considered to be one of the basic classification algorithms.", "labels": [], "entities": []}, {"text": "Secondly, it can be easily implemented.", "labels": [], "entities": []}, {"text": "Thirdly, Naive Bayes is relatively simple and works quite fast.", "labels": [], "entities": []}, {"text": "In the first experiment we intended to establish how the classifier performs on the following features: surface word forms, POS-tags, lemmas, surface forms + POS-tags, lemmas + POS-tags, focus words and lemmas of focus words.", "labels": [], "entities": []}, {"text": "Second experiment is based on the assumption that the classifier should be able to predict coarse classes with a higher precision, since coarse classes are better represented in our data.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9827795028686523}]}, {"text": "We added coarse class labels as complementary features to the existing ones to predict fine classes.", "labels": [], "entities": []}, {"text": "Labels were taken from the annotated data.", "labels": [], "entities": []}, {"text": "Unfortunately, this is not a realistic setting, since the classifier can hardly predict coarse class labels with the precision of 100%.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9986144304275513}]}, {"text": "In the third experiment, the classifier was trained on the actual predicted coarse-class labels instead of the annotated ones.", "labels": [], "entities": []}, {"text": "We have conducted three experiments, each time using different feature sets.", "labels": [], "entities": []}, {"text": "Our classifier outperformed the baseline (X 2 (1, n = 2403) = 293.181, p<.05).", "labels": [], "entities": []}, {"text": "The highest precision of 82% was achieved by the model which had been trained on unigrams+bigrams of lemmas.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9997304081916809}]}, {"text": "In most cases models based on unigrams+bigrams demonstrated significantly better results than unigram, bigram, or trigram models.", "labels": [], "entities": []}, {"text": "It means that the word order is important for the classifier, but not very crucial.", "labels": [], "entities": []}, {"text": "In we summarize results from all of the experiments.: Precision of the classifier for fine classes (CA -coarse class labels from the annotated data, CP -coarse class labels predicted by the classifier).", "labels": [], "entities": [{"text": "Precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.995613694190979}]}, {"text": "In Experiment 1 models based on unigrams and unigrams+bigrams of surface word forms achieved the precision 81%, while models based on unigrams+bigrams of lemmas -82%.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.999554455280304}]}, {"text": "These are the two best results in the Experiment 1.", "labels": [], "entities": []}, {"text": "However, it is necessary to say that there is no significant difference in performance between these models (X 2 (1, n = 2403) = 0.5745, p>.05).", "labels": [], "entities": []}, {"text": "Deviations in performance between the unigrams and unigrams+bigrams of lemmas turned out to be statistically insignificant (X 2 (1, n = 2403) = 2.2640, p>.05).", "labels": [], "entities": []}, {"text": "However, the model based on unigrams+bigrams is more precise than the one based on bigrams (X 2 (1, n = 2403) = 33.3749, p<.05).", "labels": [], "entities": []}, {"text": "Unfortunately, by adding POS-tags we did not get any improvements.", "labels": [], "entities": []}, {"text": "Words+POS-tags and Lemmas+POS-tags feature sets accounted for the same maximal values: 81% and 82% respectively.", "labels": [], "entities": []}, {"text": "Using exclusively POS-tags as features, the classifier was able to achieve the precision of 46%.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9995962977409363}]}, {"text": "It is a very poor result in comparison to the unigrams+bigrams of lemmas (X 2 (1, n = 2403) = 522.6607, p<.05).", "labels": [], "entities": []}, {"text": "Surface word forms and lemmas of focus words demonstrate similar results (X 2 (1, n = 2403) = 0.4674, p>.05), achieving maximal precision of 75% and 76% respectively.", "labels": [], "entities": [{"text": "maximal", "start_pos": 120, "end_pos": 127, "type": "METRIC", "confidence": 0.9473760724067688}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.8402401208877563}]}, {"text": "These values are significantly lower than the results achieved by ques-tion surface word forms (X 2 (1, n = 2403) = 12.4608, p<.05) or by question lemmas (X 2 (1, n = 2403) = 18.1542, p<.05).", "labels": [], "entities": []}, {"text": "We can see that in Experiment 2 unigrams, unigrams+bigrams of Words+CA and unigrams+bigrams of Lemmas+CA perform almost equally well (X 2 (1, n = 2403) = 0.7440, p>.05), demonstrating the highest precision of 86% and 87% respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9992194175720215}]}, {"text": "There is no significant difference between unigrams and unigrams+bigrams of lemmas (X 2 (1, n = 2403) = 0.7440, p>.05).", "labels": [], "entities": []}, {"text": "The difference is significant for unigrams+bigrams and bigrams of lemmas (X 2 (1, n = 2403) = 24.1152, p>.05), and for unigrams+bigrams and bigrams of surface words forms.", "labels": [], "entities": []}, {"text": "Combinations with POS-tags again were not beneficial for the classification process.", "labels": [], "entities": [{"text": "classification", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.9838067293167114}]}, {"text": "They did not show any improvements.", "labels": [], "entities": []}, {"text": "Comparing the results of Experiment 2 with the results of Experiment 1, we may conclude that by adding coarse class labels as additional features a significantly higher precision was achieved.", "labels": [], "entities": [{"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.998794436454773}]}, {"text": "In Experiment 3 we used coarse class labels predicted by the classifier (on unigrams+bigrams of lemmas) and did not observe any significant difference in comparison to the results from Experiment 1.", "labels": [], "entities": []}, {"text": "Unigrams+bigrams of lemmas, unigrams+bigrams of Words+POS-tags and Lemmas+POS-tags demonstrated absolutely identical results.", "labels": [], "entities": []}, {"text": "As for separate classes, questions of the most prevailing classes were identified with a very high precision: title -85%, creatorOf -81%, name -89%.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9974738955497742}]}, {"text": "The most frequent questions in our corpus are related to the professional activity of a person.", "labels": [], "entities": []}, {"text": "Most of the time this kind of questions belong to the class acitivityOf or to the class title.", "labels": [], "entities": []}, {"text": "They turned out to be very similar: for example, by asking What do you do fora living?, the player expects to get as a potential answer either the description of a particular professional activity or the name of a title (position) the person holds.", "labels": [], "entities": []}, {"text": "Moreover, very often the player does not care which of them will be chosen, both answers will be correct.", "labels": [], "entities": []}, {"text": "As consequence, the classifier can confuse the classes acitivityOf and title with each other.", "labels": [], "entities": []}, {"text": "As we expected, the classifier achieved the best results by using lexical clues, i.e. the presence of absence of certain words is a strong feature to determine to which class or classes a question will be assigned.", "labels": [], "entities": []}, {"text": "Unfortunately when a question contains words shared by questions belonging to different classes, it may cause prediction errors.", "labels": [], "entities": []}, {"text": "For example, the classifier may assign several labels instead of one and vice versa.", "labels": [], "entities": []}, {"text": "Based on the analysis of misclassified instances, we can tell that a question will receive more than one label, if wording representative for two (or more) classes is observed and extracted as features.", "labels": [], "entities": []}, {"text": "The analysis of false predictions indicates that most of them were caused by the imbalanced training set.", "labels": [], "entities": []}, {"text": "There are also no strict borders between some classes.", "labels": [], "entities": []}, {"text": "Questions with multiple labels are under-represented.", "labels": [], "entities": []}, {"text": "According to they comprise only 1.04%.", "labels": [], "entities": []}, {"text": "By applying coarse class labels as additional features we tried to get a higher precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9970558881759644}]}, {"text": "Unfortunately, it worked only when these labels were taken from the annotated corpus.", "labels": [], "entities": []}, {"text": "The classifier was able to predict coarse class labels with the average precision of 90% (see).", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9883393049240112}]}, {"text": "However, it was not enough to make the actual predicted labels useful for the next classifier.: Precision of the classifier for coarse classes (unigrams+bigrams of lemmas).", "labels": [], "entities": [{"text": "Precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9815555214881897}]}, {"text": "The precision for all coarse classes is already relatively high.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996253252029419}]}, {"text": "Questions of the class eatTime, for example, were correctly identified in 97% of cases.", "labels": [], "entities": [{"text": "eatTime", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.88176029920578}, {"text": "correctly", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9807830452919006}]}, {"text": "It maybe very problematic to make further improvements.", "labels": [], "entities": []}, {"text": "To explore learnability of the best performing classification model and to evaluate how the size of the training set affects the classifier's results, we divided the corpus of annotated questions into 20 parts.", "labels": [], "entities": []}, {"text": "All partitions, except for the first one, contained the equal number of questions.", "labels": [], "entities": []}, {"text": "Different question types were equally distributed among the partitions.", "labels": [], "entities": []}, {"text": "We started with the training set consisting of 636 questions and gradually increased its size.", "labels": [], "entities": []}, {"text": "Based the obtained results, the learning curve has been plotted presented in.", "labels": [], "entities": []}, {"text": "As we can observe, the precision rose almost steadily until the size of the training set became bigger than 2000 questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9997444748878479}]}, {"text": "The growth stopped at the precision of around 81%.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9990666508674622}]}, {"text": "It was followed by a decrease of 2%.", "labels": [], "entities": []}, {"text": "After that the precision grew by 3%, and then again dropped about 1%.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9997119307518005}]}, {"text": "These fluctuations, as we believe, were caused by the quality of the data.", "labels": [], "entities": []}, {"text": "The growth slows down gradually, i.e. in the range from about 700 till 1200 questions the precision increased from 65% till 75%, while to get another 5%, the classifier required 800 additional questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9995811581611633}]}, {"text": "Taking these calculations into account, we were able to obtain the formula, which allowed to extrapolate the learning curve: We came to the conclusion that the classifier will need the training set including approximately 3100-3200 questions to achieve the precision of 85%.", "labels": [], "entities": [{"text": "precision", "start_pos": 257, "end_pos": 266, "type": "METRIC", "confidence": 0.9993181228637695}]}, {"text": "Thus, getting more data may potentially improve the performance of the classifier.", "labels": [], "entities": []}, {"text": "However, given the obtained learnability results and since data collection and its annotation is a very time consuming task, the efforts maybe better spent to explore other approaches additional to machine learning, e.g. pattern matching and bootstrapping from collected examples.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 221, "end_pos": 237, "type": "TASK", "confidence": 0.8632582128047943}]}], "tableCaptions": [{"text": " Table 1: Distribution of semantic relation classes  (in terms of relative frequencies in the corpus).", "labels": [], "entities": []}, {"text": " Table 2: Precision of the classifier for fine classes  (CA -coarse class labels from the annotated data,  CP -coarse class labels predicted by the classifier).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9891586303710938}]}, {"text": " Table 3: Precision of the classifier for coarse  classes (unigrams+bigrams of lemmas).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9807104468345642}]}]}