{"title": [{"text": "INVITED TALK: Embedding Probabilistic Logic for Machine Reading", "labels": [], "entities": [{"text": "INVITED", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5070861577987671}, {"text": "TALK", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.7278369665145874}, {"text": "Machine Reading", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.8011554479598999}]}], "abstractContent": [{"text": "We want to build machines that read, and make inferences based on what was read.", "labels": [], "entities": []}, {"text": "A long line of the work in the field has focussed on approaches where language is converted (possibly using machine learning) into a symbolic and relational representation.", "labels": [], "entities": []}, {"text": "A reasoning algorithm (such as a theorem prover) then derives new knowledge from this representation.", "labels": [], "entities": []}, {"text": "This allows for rich knowledge to captured, but generally suffers from two problems: acquiring sufficient symbolic background knowledge and coping with noise and uncertainty in data.", "labels": [], "entities": []}, {"text": "Probabilistic logics (such as Markov Logic) offer a solution, but are known to often scale poorly.", "labels": [], "entities": []}, {"text": "In recent years a third alternative emerged: latent variable models in which entities and relations are embedded in vector spaces (and represented \"distribu-tional\").", "labels": [], "entities": []}, {"text": "Such approaches scale well and are robust to noise, but they raise their own set of questions: What type of inferences do they support?", "labels": [], "entities": []}, {"text": "What is a proof in embeddings?", "labels": [], "entities": []}, {"text": "How can explicit background knowledge be injected into embed-dings?", "labels": [], "entities": []}, {"text": "In this talk I first present our work on latent variable models for machine reading, using ideas from matrix factorisation as well as both closed and open information extraction.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.8228814005851746}, {"text": "open information extraction", "start_pos": 150, "end_pos": 177, "type": "TASK", "confidence": 0.7357417345046997}]}, {"text": "Then I will present recent work we conducted to address the questions of injecting and extracting symbolic knowledge into/from models based on embeddings.", "labels": [], "entities": []}, {"text": "In particular, I will show how one can rapidly build accurate relation extractors through combining logic and embeddings.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6938395500183105}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}