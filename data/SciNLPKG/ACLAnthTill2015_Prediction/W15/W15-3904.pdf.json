{"title": [{"text": "Boosting Named Entity Recognition with Neural Character Embeddings", "labels": [], "entities": [{"text": "Boosting Named Entity Recognition", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5616282746195793}]}], "abstractContent": [{"text": "Most state-of-the-art named entity recognition (NER) systems rely on handcrafted features and on the output of other NLP tasks such as part-of-speech (POS) tagging and text chunking.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.809669146935145}, {"text": "part-of-speech (POS) tagging", "start_pos": 135, "end_pos": 163, "type": "TASK", "confidence": 0.6586188554763794}, {"text": "text chunking", "start_pos": 168, "end_pos": 181, "type": "TASK", "confidence": 0.7463552057743073}]}, {"text": "In this work we propose a language-independent NER system that uses automatically learned features only.", "labels": [], "entities": []}, {"text": "Our approach is based on the CharWNN deep neural network, which uses word-level and character-level representations (embeddings) to perform sequential classification.", "labels": [], "entities": [{"text": "CharWNN deep neural network", "start_pos": 29, "end_pos": 56, "type": "DATASET", "confidence": 0.7527117729187012}, {"text": "sequential classification", "start_pos": 140, "end_pos": 165, "type": "TASK", "confidence": 0.6927536576986313}]}, {"text": "We perform an extensive number of experiments using two annotated corpora in two different languages: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in Span-ish.", "labels": [], "entities": [{"text": "HAREM", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9413383603096008}, {"text": "SPA CoNLL-2002 corpus", "start_pos": 162, "end_pos": 183, "type": "DATASET", "confidence": 0.840307374795278}]}, {"text": "Our experimental results give evidence of the contribution of neural character embeddings for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.958733856678009}]}, {"text": "Moreover, we demonstrate that the same neural network which has been successfully applied to POS tagging can also achieve state-of-the-art results for language-independet NER, using the same hyperparameters, and without any handcrafted features.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.7287752330303192}]}, {"text": "For the HAREM I corpus, CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score for the total scenario (ten NE classes).", "labels": [], "entities": [{"text": "HAREM I corpus", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.656456728776296}, {"text": "CharWNN", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8817487955093384}, {"text": "F1-score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9987531900405884}]}, {"text": "For the SPA CoNLL-2002 corpus , CharWNN outperforms the state-of-the-art system by 0.8 point in the F1.", "labels": [], "entities": [{"text": "SPA CoNLL-2002 corpus", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.8386733929316202}, {"text": "CharWNN", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.8412683606147766}, {"text": "F1", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.8426394462585449}]}], "introductionContent": [{"text": "Named entity recognition is a natural language processing (NLP) task that consists of finding names in a text and classifying them among several predefined categories of interest such as person, organization, location and time.", "labels": [], "entities": [{"text": "Named entity recognition is a natural language processing (NLP) task that consists of finding names in a text and classifying them among several predefined categories of interest such as person, organization, location and time", "start_pos": 0, "end_pos": 226, "type": "Description", "confidence": 0.758132245979811}]}, {"text": "Although machine learning based systems have been the predominant approach to achieve state-of-the-art results for NER, most of these NER systems rely on the use of costly handcrafted features and on the output of other NLP tasks).", "labels": [], "entities": [{"text": "NER", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9289347529411316}]}, {"text": "On the other hand, some recent work on NER have used deep learning strategies which minimize the need of these costly features ().", "labels": [], "entities": [{"text": "NER", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9377824068069458}]}, {"text": "However, as far as we know, there are still no work on deep learning approaches for NER that use character-level embeddings.", "labels": [], "entities": [{"text": "NER", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9313254952430725}]}, {"text": "In this paper we approach languageindependent NER using CharWNN, a recently proposed deep neural network (DNN) architecture that jointly uses word-level and character-level embeddings to perform sequential classification (dos).", "labels": [], "entities": [{"text": "sequential classification (dos)", "start_pos": 195, "end_pos": 226, "type": "TASK", "confidence": 0.807219511270523}]}, {"text": "CharWNN employs a convolutional layer that allows effective character-level feature extraction from words of any size.", "labels": [], "entities": [{"text": "character-level feature extraction from words", "start_pos": 60, "end_pos": 105, "type": "TASK", "confidence": 0.7625110507011413}]}, {"text": "This approach has proven to be very effective for language-independent POS tagging (dos.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.8285808265209198}]}, {"text": "We perform an extensive number of experiments using two annotated corpora: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL-2002, which contains texts in Spanish.", "labels": [], "entities": [{"text": "HAREM", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9572249054908752}, {"text": "SPA CoNLL-2002", "start_pos": 135, "end_pos": 149, "type": "DATASET", "confidence": 0.7726475298404694}]}, {"text": "In our experiments, we compare the performance of the joint and individual use of character-level and word-level embeddings.", "labels": [], "entities": []}, {"text": "We provide information on the impact of unsupervised pre-training of word embeddings in the performance of our proposed NER approach.", "labels": [], "entities": [{"text": "NER", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9623944163322449}]}, {"text": "Our experimental results evidence that CharWNN is effective and robust for Portuguese and Spanish NER.", "labels": [], "entities": [{"text": "CharWNN", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.8221186995506287}, {"text": "Portuguese and Spanish NER", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.49758318066596985}]}, {"text": "Using the same CharWNN configuration used by dos for POS Tagging, we achieve state-of-the-art results for both corpora.", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.6110862195491791}]}, {"text": "For the HAREM I corpus, CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score for the total scenario (ten NE classes), and by 7.2 points in the F1 for the selective scenario (five NE classes).", "labels": [], "entities": [{"text": "HAREM I corpus", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.6717142959435781}, {"text": "CharWNN", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8828203082084656}, {"text": "F1-score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9956035614013672}, {"text": "F1", "start_pos": 168, "end_pos": 170, "type": "METRIC", "confidence": 0.9943716526031494}]}, {"text": "For the SPA CoNLL-2002 corpus, CharWNN outperforms the state-of-the-art system by 0.8 point in the F1.", "labels": [], "entities": [{"text": "SPA CoNLL-2002 corpus", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.8386737108230591}, {"text": "CharWNN", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.8412684202194214}, {"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.8426405191421509}]}, {"text": "This work is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly describe the CharWNN architecture.", "labels": [], "entities": []}, {"text": "Section 3 details our experimental setup and Section 4 discuss our experimental results.", "labels": [], "entities": []}, {"text": "Section 6 presents our final remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Named Entity Recognition Corpora.  Training Data  Test Data  Corpus  Language Sentenc. Tokens Sentenc. Tokens", "labels": [], "entities": [{"text": "Named Entity Recognition Corpora", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6738975271582603}, {"text": "Training Data  Test Data  Corpus  Language Sentenc", "start_pos": 45, "end_pos": 95, "type": "DATASET", "confidence": 0.8233573692185538}]}, {"text": " Table 2: Neural Network Hyperparameters.  Parameter Parameter Name  CharWNN WNN CharNN  d wrd  Word embedding dimensions  100  100  - k wrd  Word context window size  5  5  5  d chr  Char. embedding dimensions  10  - 50  k chr  Char. context window size  5  - 5  cl u  Convolutional units  50  - 200  hl u  Hidden units  300  300  300  \u03bb  Learning rate  0.0075 0.0075  0.0075", "labels": [], "entities": [{"text": "Learning rate  0.0075 0.0075  0.0075", "start_pos": 340, "end_pos": 376, "type": "METRIC", "confidence": 0.8519754052162171}]}, {"text": " Table 3: Comparison of different NNs for the SPA CoNLL-2002 corpus.", "labels": [], "entities": [{"text": "SPA CoNLL-2002 corpus", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.8573174476623535}]}, {"text": " Table 4: Comparison with the state-of-the-art for the SPA CoNLL-2002 corpus.  System  Features  Prec. Rec.  F1  CharWNN  word embeddings, char embeddings  82.21 82.21 82.21  words, ortographic, POS tags, trigger words,  AdaBoost  bag-of-words, gazetteers, word suffixes,  81.38 81.40 81.39  word type patterns, entity length", "labels": [], "entities": [{"text": "SPA CoNLL-2002 corpus", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.8433029452959696}]}, {"text": " Table 5: Comparison of different NNs for the HAREM I corpus.", "labels": [], "entities": [{"text": "HAREM I corpus", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.8370754520098368}]}, {"text": " Table 6: Comparison with the State-of-the-art for the HAREM I corpus.", "labels": [], "entities": [{"text": "HAREM I corpus", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.8827584385871887}]}, {"text": " Table 7: Results by entity type for the HAREM I corpus.", "labels": [], "entities": [{"text": "HAREM I corpus", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.750314990679423}]}, {"text": " Table 8: Impact of unsup. pre-training of word emb. in CharWNN performance.  Corpus  Pre-trained word emb. Precision Recall  F1", "labels": [], "entities": []}]}