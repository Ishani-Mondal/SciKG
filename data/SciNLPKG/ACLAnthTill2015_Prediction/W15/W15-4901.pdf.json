{"title": [{"text": "Exploiting portability to build an RBMT prototype fora new source language", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents the work done to port a deep-transfer rule-based machine translation system to translate from a different source language by maximizing the exploitation of existing resources and by limiting the development work.", "labels": [], "entities": [{"text": "deep-transfer rule-based machine translation", "start_pos": 44, "end_pos": 88, "type": "TASK", "confidence": 0.6148350834846497}]}, {"text": "Specifically, we report the changes and effort required in each of the system's modules to obtain an English-Basque translator, ENEUS, starting from the Spanish-Basque Matxin system.", "labels": [], "entities": [{"text": "ENEUS", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.7084836959838867}]}, {"text": "We run a human pairwise comparison for the new prototype and two statistical systems and see that ENEUS is preferred in over 30% of the test sentences.", "labels": [], "entities": [{"text": "ENEUS", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.909488320350647}]}], "introductionContent": [{"text": "Building a corpus-based system is undeniably quicker than building a rule-based machine translation (RBMT) system, given the availability of large quantities of parallel text.", "labels": [], "entities": [{"text": "rule-based machine translation (RBMT)", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.8176427682240804}]}, {"text": "However, this is often not the case for many language pairs, which makes building a mainstream statistical system suboptimal.", "labels": [], "entities": []}, {"text": "Usually, lesser-resourced languages opt for RBMT systems, where language-specific NLP tools and resources are crafted.", "labels": [], "entities": []}, {"text": "Heavy investment and long development periods have been attributed to RBMT systems but () pointed out that a large part of the systems' code is reusable.", "labels": [], "entities": []}, {"text": "They state that 80% of Systran's code belongs to the analysis module, whereas the remaining 20% is equally divided into transfer and generation.", "labels": [], "entities": []}, {"text": "Transfer is languagepair specific, but analysis and generation are built with information about one language only and they are therefore reusable for systems that use those languages.", "labels": [], "entities": []}, {"text": "Rapid development of new language pairs benefits from existing resources but also from modular, stable infrastructures where new pairs can be developed by modifying the linguistic data.", "labels": [], "entities": []}, {"text": "An example of RBMT portability attempts for lesser-resourced languages is the Apertium project).", "labels": [], "entities": [{"text": "RBMT portability", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.9329428672790527}]}, {"text": "Apertium is a free/opensource shallow-transfer MT platform.", "labels": [], "entities": [{"text": "Apertium", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9323205351829529}, {"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9087918996810913}]}, {"text": "Researchers have been active in porting the system to different language pairs ().", "labels": [], "entities": []}, {"text": "The system specializes in translation between related languages where shallow transfer suffices to produce good quality translations.", "labels": [], "entities": [{"text": "translation between related languages", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.8780444115400314}]}, {"text": "Shallow parsing is sometimes too limited for dissimilar language pairs.", "labels": [], "entities": [{"text": "Shallow parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8309946954250336}]}, {"text": "Unrelated languages often require a richer and more flexible deeper transfer architecture to tackle differing linguistic features.", "labels": [], "entities": []}, {"text": "In this work we present an attempt to port the deep-transfer RBMT Matxin 1 , designed to cope with dissimilar languages.", "labels": [], "entities": [{"text": "RBMT Matxin 1", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.8483530084292094}]}, {"text": "The remaining work is organized as follows: Section 2 gives a brief overview of the architecture of the Matxin system.", "labels": [], "entities": []}, {"text": "Section 3 describes the work done in each of the system's modules.", "labels": [], "entities": []}, {"text": "Section 4 provides the results of the new Matxin ENEUS prototype's evaluation.", "labels": [], "entities": [{"text": "Matxin ENEUS prototype", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.884082575639089}]}, {"text": "Finally, Section 5 presents the conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used human evaluation as the main indicator for the prototype's performance.", "labels": [], "entities": []}, {"text": "Also, we ran automatic metrics to compare their scores against the human evaluation even when it is known that automatic scores tend to favor SMT systems over RBMT systems because they do not consider the correctness of the output but rather compare the difference between the output and the reference translations).", "labels": [], "entities": [{"text": "SMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.985704779624939}]}, {"text": "And the use of a single reference accentuates this.", "labels": [], "entities": []}, {"text": "To get a perspective on the overall performance, we ran the evaluation for two additional systems, an in-house statistical system, SMTs, and Google Translate, as well as Matxin ENEUS.", "labels": [], "entities": [{"text": "SMTs", "start_pos": 131, "end_pos": 135, "type": "TASK", "confidence": 0.7876057028770447}, {"text": "Matxin ENEUS", "start_pos": 170, "end_pos": 182, "type": "DATASET", "confidence": 0.8494734168052673}]}, {"text": "Our SMT system was trained on a parallel corpus of 12 million Basque words and 14 million English words comprising user manuals, academic books and web data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9880200028419495}]}, {"text": "We implemented a phrase-based system using Moses (.", "labels": [], "entities": []}, {"text": "To better deal with the agglutinative nature of Basque, we trained the system on morpheme-level segmented data.", "labels": [], "entities": []}, {"text": "As a result, we need a generation postprocess to obtain real word forms for the decoder.", "labels": [], "entities": []}, {"text": "We incorporated a second language model (LM) based on real word forms to be used after the morphological postprocess.", "labels": [], "entities": []}, {"text": "We implemented the word form-based LM by using an nbest list following.", "labels": [], "entities": []}, {"text": "We first generate a candidate ranking based on the segmented training.", "labels": [], "entities": []}, {"text": "Next, these candidates are postprocessed.", "labels": [], "entities": []}, {"text": "We then recalculate the total cost of each candidate by including the cost assigned by the new word form-based LM in the models used during decoding.", "labels": [], "entities": []}, {"text": "Finally, the candidate list is re-ranked according to the new total cost.", "labels": [], "entities": []}, {"text": "This revises the candidate list to promote those that are more likely to be real word form sequences.", "labels": [], "entities": []}, {"text": "The weight for the word form-based LM was optimized with minimum error rate training together with the weights for the rest of the models.", "labels": [], "entities": []}, {"text": "We used the same evaluation set for both the human evaluation and the automatic metrics.", "labels": [], "entities": []}, {"text": "It is a set of 500 sentences consisting of 250 sentences set aside from the training corpus and 250 out-ofdomain sentences from online news sites and magazines.", "labels": [], "entities": []}, {"text": "All sentences contain at least one verb, are self-contained and have 5 to 20 tokens.", "labels": [], "entities": []}, {"text": "We performed a human evaluation for the three systems mentioned above as part of a wider evaluation campaign.", "labels": [], "entities": []}, {"text": "We carried out a pairwise comparison evaluation with non-expert volunteer participants who accessed an evaluation platform on-line.", "labels": [], "entities": []}, {"text": "They were presented with a source sentence and two machine translations.", "labels": [], "entities": []}, {"text": "They were asked to compare the translations and decide which was better.", "labels": [], "entities": []}, {"text": "They were given the options 1st is better, 2nd is better and they are both of equal quality.", "labels": [], "entities": []}, {"text": "Over 551 participants provided responses in the campaign which allowed us to collect over 7,500 data points for the systems we show here.", "labels": [], "entities": []}, {"text": "We collected at least 5 evaluations per source sentence for each system-pair (2,500 evaluations per pair).", "labels": [], "entities": []}, {"text": "We adopted the following strategy to decide on a winning system for each evaluated sentence in each system-pair comparison: if the difference in votes between two systems is larger than 2, the system with the highest number of votes is the undisputed winner (System X++).", "labels": [], "entities": []}, {"text": "If the difference in votes is 1 or 2, the system scoring higher is the winner (System X+).", "labels": [], "entities": []}, {"text": "If both systems score the same amount of votes, the result is a draw (equal).", "labels": [], "entities": []}, {"text": "From the evaluations collected), we see that the output of Matxin ENEUS is considered better than its competitors 31-34% of the time, a significant proportion given the prototype's rapid development and limited coverage.", "labels": [], "entities": [{"text": "Matxin ENEUS", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.8532548248767853}]}, {"text": "This is particularly interesting for hybridization purposes.", "labels": [], "entities": []}, {"text": "It would be invaluable to pinpoint the specific structures in which this system succeeds and its specific strengths to guide future hybridization attempts.", "labels": [], "entities": []}, {"text": "SMTs and Google are preferred over the prototype.", "labels": [], "entities": [{"text": "SMTs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5306894183158875}]}, {"text": "When compared against each other, the difference in sentences allocated to each system is not significant, with only 8 additional sentences allocated to SMTs (229 vs 221, 50 equal).", "labels": [], "entities": [{"text": "SMTs", "start_pos": 153, "end_pos": 157, "type": "TASK", "confidence": 0.9283453822135925}]}], "tableCaptions": [{"text": " Table 1: Statistics for the preposition dictionary.", "labels": [], "entities": []}, {"text": " Table 2: Verb transfer rules by type.", "labels": [], "entities": [{"text": "Verb transfer", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7909930646419525}]}]}