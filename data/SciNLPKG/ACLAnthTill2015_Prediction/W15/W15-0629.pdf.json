{"title": [{"text": "Automatically Scoring Freshman Writing: A Preliminary Investigation", "labels": [], "entities": [{"text": "Automatically Scoring Freshman Writing", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.735550545156002}]}], "abstractContent": [{"text": "In this work, we explore applications of automatic essay scoring (AES) to a corpus of essays written by college freshmen and discuss the challenges we faced.", "labels": [], "entities": [{"text": "automatic essay scoring (AES)", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6400014162063599}]}, {"text": "While most AES systems evaluate highly constrained writing, we developed a system that handles open-ended, long-form writing.", "labels": [], "entities": []}, {"text": "We present a novel corpus for this task, containing more than 3,000 essays and drafts written fora freshman writing course.", "labels": [], "entities": []}, {"text": "We describe statistical analysis of the corpus and identify problems with automatically scoring this type of data.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate how to overcome grader bias by using a multi-task setup, and predict scores as well as human graders on a different dataset.", "labels": [], "entities": []}, {"text": "Finally, we discuss how AES can help teachers assign more uniform grades.", "labels": [], "entities": [{"text": "AES", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5302954316139221}]}], "introductionContent": [{"text": "Automatic essay scoring (AES) is the task of automatically predicting the scores of written essays.", "labels": [], "entities": [{"text": "Automatic essay scoring (AES)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6707662443319956}, {"text": "predicting the scores of written essays", "start_pos": 59, "end_pos": 98, "type": "TASK", "confidence": 0.6599748830000559}]}, {"text": "AES has primarily focused on high-stakes standardized tests and statewide evaluation exams.", "labels": [], "entities": [{"text": "AES", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.48673906922340393}]}, {"text": "In this paper, we consider a classroom application of AES to evaluate a novel corpus of more than 3,000 essays written fora first-year writing program.", "labels": [], "entities": [{"text": "AES", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.5868090391159058}]}, {"text": "Many colleges have first-year writing programs, which are typically large courses divided into multiple sections taught by different teachers.", "labels": [], "entities": []}, {"text": "These essays are more representative of college writing than assessment-based datasets used for AES, and we wish to examine how AES can help students and teachers in the classroom.", "labels": [], "entities": [{"text": "AES", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.5062435865402222}]}, {"text": "These preliminary experiments could help teachers evaluate students and colleges gain insight into variance across instructors.", "labels": [], "entities": []}, {"text": "This corpus maybe more difficult to model compared to previous datasets because it lacks multiple grades to establish validity and the essays are not constrained by a prompt.", "labels": [], "entities": [{"text": "validity", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9469506740570068}]}, {"text": "reported that prompt-independent scoring generally had 10% lower reliability than prompt-specific scoring.", "labels": [], "entities": [{"text": "reliability", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9947920441627502}]}, {"text": "We address several issues surrounding automatically scoring essays of this nature: 1.", "labels": [], "entities": []}, {"text": "Is it possible to model essays graded by several different teachers with no overlapping grades?", "labels": [], "entities": []}, {"text": "2. ...even when scores given by each teacher have different distributions?", "labels": [], "entities": []}, {"text": "3. Can a single model predict the scores of long essays that are (a) not constrained by an essay prompt and (b) written in different styles?", "labels": [], "entities": []}, {"text": "4. How can AES provide constructive feedback to teachers and administrators?", "labels": [], "entities": []}, {"text": "In this work, we describe how multi-task learning can accommodate the differences in teacher scoring patterns by jointly modeling the scores of individual teachers, while sharing information across all teachers.", "labels": [], "entities": []}, {"text": "Our multi-task model correlates strongly with actual grades.", "labels": [], "entities": []}, {"text": "We also provide an example of how to provide feedback to help teachers grade more uniformly, using the weights learned by a linear model.", "labels": [], "entities": []}, {"text": "Our corpus is described in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our experimental setup and the features used.", "labels": [], "entities": []}, {"text": "Section 5 presents results from our system that achieve human-like levels of correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9305345416069031}]}, {"text": "Section 6 discusses our results and proposes anew way to provide feedback to teachers about their grading.", "labels": [], "entities": []}, {"text": "600-770 A personal narrative that describes an experience and uses that experience to tell readers something important about the writer.", "labels": [], "entities": []}, {"text": "2 600 A bibliographic essay that asks you to understand the conversation surrounding your chosen topic by examining four relevant sources.", "labels": [], "entities": []}, {"text": "Two of these sources must beat least ten years apart so that you can see how interpretations of an event, concept, or person evolve overtime and that textual scholarship is an ongoing conversation.", "labels": [], "entities": []}, {"text": "3 600-800 A reflection that asks you to think carefully about how audience and purpose, as well as medium and genre, affect your choices as composers and reflect carefully on anew dimension of your topic.", "labels": [], "entities": []}, {"text": "4 1000-1200 A polished essay that asserts an arguable thesis that is supported by research and sound reasoning.: Brief description of the assignments in the FWC, as provided by the syllabus.", "labels": [], "entities": [{"text": "FWC", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.933527410030365}]}], "datasetContent": [{"text": "We separated 3,362 essays by draft, Intermediate and Final (1,400 and 1,962 essays, respectively, skipping 31 Intermediate drafts that had no grade assigned).", "labels": [], "entities": []}, {"text": "We randomly selected 100 essays for development and 100 for testing from each draft type and represented all essays with feature vectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Brief description of the assignments in the FWC, as provided by the syllabus.", "labels": [], "entities": [{"text": "Brief", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9774898886680603}, {"text": "FWC", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.8973718881607056}]}, {"text": " Table 4: Average score for each draft by project, includ- ing the average change in score between the Intermediate  and Final drafts. The standard deviation of the Intermedi- ate and Final draft scores are 0.92 and 0.68, respectively.", "labels": [], "entities": [{"text": "Average score", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9768993854522705}]}, {"text": " Table 5: Correlation between predictions and teacher scores, measured by Pearson's r and Kendall's \u03c4 , as well as the  mean squared error (MSE) and exact and adjacent agreements. The baseline is a random balanced sample.", "labels": [], "entities": [{"text": "Pearson's r and Kendall's \u03c4", "start_pos": 74, "end_pos": 101, "type": "METRIC", "confidence": 0.7829889399664742}, {"text": "mean squared error (MSE)", "start_pos": 120, "end_pos": 144, "type": "METRIC", "confidence": 0.9515895048777262}]}, {"text": " Table 6: The correlation (Pearson's r) of actual scores  to predictions made by individual models for each  project/draft pair. P1-4 represents predictions of all  project models.", "labels": [], "entities": [{"text": "Pearson's r) of actual scores", "start_pos": 27, "end_pos": 56, "type": "METRIC", "confidence": 0.9178639565195356}]}, {"text": " Table 8: Correlation between the predicted and actual  change between Intermediate and Final draft scores.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9576391577720642}, {"text": "Intermediate and Final draft scores", "start_pos": 71, "end_pos": 106, "type": "METRIC", "confidence": 0.6926744937896728}]}, {"text": " Table 7: Correlation (Pearson's r) of predicted to actual scores for individual rubric categories.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9842115044593811}, {"text": "Pearson's r)", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9350090324878693}]}]}