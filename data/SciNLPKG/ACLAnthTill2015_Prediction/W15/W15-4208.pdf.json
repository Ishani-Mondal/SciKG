{"title": [{"text": "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models", "labels": [], "entities": [{"text": "EVALution 1.0", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8519521653652191}]}], "abstractContent": [{"text": "In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs).", "labels": [], "entities": []}, {"text": "This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (in-cluding hypernymy, synonymy, antonymy, meronymy).", "labels": [], "entities": []}, {"text": "The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results.", "labels": [], "entities": []}, {"text": "The tuples were extracted from a combination of ConceptNet 5.0 and Word-Net 4.0, and subsequently filtered through automatic methods and crowdsourcing in order to ensure their quality.", "labels": [], "entities": []}, {"text": "The dataset is freely downloadable 1.", "labels": [], "entities": []}, {"text": "An extension in RDF format, including also scripts for data processing, is underdevelopment.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional Semantic Models (DSMs) represent lexical meaning in vector spaces by encoding corpora derived word co-occurrences in vectors).", "labels": [], "entities": []}, {"text": "These models are based on the assumption that meaning can be inferred from the contexts in which terms occur.", "labels": [], "entities": []}, {"text": "Such assumption is The resource is available at http://colinglab.humnet.unipi.it/resources/ and at https://github.com/esantus typically referred to as the distributional hypothesis.", "labels": [], "entities": []}, {"text": "DSMs are broadly used in Natural Language Processing (NLP) because they allow systems to automatically acquire lexical semantic knowledge in a fully unsupervised way and they have been proved to outperform other semantic models in a large number of tasks, such as the measurement of lexical semantic similarity and relatedness.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6820349643627802}, {"text": "measurement of lexical semantic similarity", "start_pos": 268, "end_pos": 310, "type": "TASK", "confidence": 0.776400887966156}]}, {"text": "Their geometric representation of semantic distance) allows its calculation through mathematical measures, such as the vector cosine.", "labels": [], "entities": []}, {"text": "A related but more complex task is the identification of semantic relations.", "labels": [], "entities": [{"text": "identification of semantic relations", "start_pos": 39, "end_pos": 75, "type": "TASK", "confidence": 0.8710875958204269}]}, {"text": "Words, in fact, can be similar in many ways.", "labels": [], "entities": []}, {"text": "Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates).", "labels": [], "entities": []}, {"text": "DSMs do not provide by themselves a principled way to single out the items linked by a specific relation.", "labels": [], "entities": []}, {"text": "Several distributional approaches have tried to overcome such limitation in the last decades.", "labels": [], "entities": []}, {"text": "Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur).", "labels": [], "entities": []}, {"text": "Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (.", "labels": [], "entities": []}, {"text": "Both the abovementioned approaches need to rely on datasets containing semantic relations for training and/or evaluation.", "labels": [], "entities": []}, {"text": "EVALution is a dataset designed to support DSMs on both processes.", "labels": [], "entities": [{"text": "EVALution", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.859677791595459}, {"text": "DSMs", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.9686933159828186}]}, {"text": "This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy).", "labels": [], "entities": []}, {"text": "The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results.", "labels": [], "entities": []}, {"text": "The quality of the pairs is guaranteed by i.) their presence in previous resources, such as ConceptNet 5.0 () and WordNet 4.0, and ii.) a large agreement between native speakers (obtained in crowdsourcing tasks, performed with Crowdflower).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9327765703201294}]}, {"text": "In order to increase the homogeneity of the data and reduce its variability 2 , the dataset only contains word pairs whose terms (henceforth relata) occur in more than one semantic relation.", "labels": [], "entities": []}, {"text": "The additional information is provided for both relata and relations.", "labels": [], "entities": []}, {"text": "Such information is based on both human judgments (e.g. relation domain, term generality, term abstractness, etc.) and on corpus data (e.g. frequency, POS, etc.).", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to further evaluate the dataset, we built a 30K dimensions standard window-based matrix, recording co-occurrences with the nearest 2 content words to the left and the right of the target.", "labels": [], "entities": []}, {"text": "Co-occurrences are extracted from a combination of the freely available ukWaC and WaCkypedia corpora () and weighted with Local Mutual Information (LMI).", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.9890778064727783}, {"text": "Local Mutual Information (LMI)", "start_pos": 122, "end_pos": 152, "type": "METRIC", "confidence": 0.6738164077202479}]}, {"text": "We then calculate the vector cosine values for all the pairs in: The distribution of tags for relations and relata (only tags that were selected at least twice are reported).", "labels": [], "entities": []}, {"text": "Every relation and relatum can have more than one tag.", "labels": [], "entities": []}, {"text": "EVALution and for all those in BLESS (for comparison).", "labels": [], "entities": [{"text": "EVALution", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.5673667788505554}, {"text": "BLESS", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.991595447063446}]}, {"text": "shows the box-plots summarizing their distribution per relation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Relations, number of pairs, number of  relata and sentence templates", "labels": [], "entities": []}, {"text": " Table 2: The distribution of tags for relations and  relata (only tags that were selected at least twice  are reported). Every relation and relatum can have  more than one tag.", "labels": [], "entities": []}]}