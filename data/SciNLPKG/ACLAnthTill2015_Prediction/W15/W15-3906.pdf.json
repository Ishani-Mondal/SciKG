{"title": [{"text": "HAREM and Klue: how to compare two tagsets for named entities annotation", "labels": [], "entities": [{"text": "HAREM", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7966193556785583}]}], "abstractContent": [{"text": "This paper describes an undergoing experiment to compare two tagsets for Named Entities (NE) annotation.", "labels": [], "entities": []}, {"text": "We compared Klue 2 tagset, developed by IBM Research , with HAREM tagset, developed for tagging the Portuguese corpora used in Second HAREM competition.", "labels": [], "entities": [{"text": "HAREM tagset", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.7293323278427124}, {"text": "Second HAREM competition", "start_pos": 127, "end_pos": 151, "type": "DATASET", "confidence": 0.75424857934316}]}, {"text": "From this report, we expected to evaluate our methodology for comparison and to survey the problems that arise from it.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named-entity recognition (NER) is a subtask of many information extraction procedures.", "labels": [], "entities": [{"text": "Named-entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8538174211978913}, {"text": "information extraction", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7561094164848328}]}, {"text": "Its aims is to track and categorize pieces of texts (words, multiwords expressions, etc) into predefined classes such as the names of persons, organizations, etc.", "labels": [], "entities": [{"text": "categorize pieces of texts (words, multiwords expressions, etc) into predefined classes such as the names of persons, organizations, etc", "start_pos": 25, "end_pos": 161, "type": "Description", "confidence": 0.7015821719169617}]}, {"text": "The state-of-the-art systems for English are able to produce near-human performance.", "labels": [], "entities": []}, {"text": "In MUC-7), the best system entering the joint evaluation scored 93.39% of F-measure while human annotators scored 97.6% and 96.95%.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.7797913551330566}, {"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9977703094482422}]}, {"text": "The good results achieved by some systems in MUC-7 don't mean that NER is entirely understood, mainly if we consider languages different from English.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.8771196007728577}, {"text": "NER", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9543707966804504}]}, {"text": "Moreover, to compare NER systems is a hard goal since the definition of what is a named entity itself is getting fuzzier and have passed to included not only proper nouns).", "labels": [], "entities": [{"text": "NER", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.951509952545166}]}, {"text": "The decision to add dates, quantities or events to NE label, for example, makes necessary the retrieve of more information and is harder to keep the same score of recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9983072280883789}, {"text": "precision", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9973994493484497}]}, {"text": "In most cases, NER is done through statistical or machine learning procedures.", "labels": [], "entities": [{"text": "NER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9959399700164795}]}, {"text": "The IBM Statistical Information and Relation Extraction (SIRE) is one of such systems.", "labels": [], "entities": [{"text": "Statistical Information and Relation Extraction (SIRE)", "start_pos": 8, "end_pos": 62, "type": "TASK", "confidence": 0.7947654575109482}]}, {"text": "It can be use to build trainable extractors for different domains.", "labels": [], "entities": []}, {"text": "SIRE provides components for mention detection using Maximum Entropy models) that can be trained from annotated data created by using a highly optimized web-browser annotation tool, called HAT, a trainable co-reference component for grouping detected mentions in a document that correspond to the same entity, and a trainable relation extraction system.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7165401428937912}, {"text": "relation extraction", "start_pos": 326, "end_pos": 345, "type": "TASK", "confidence": 0.7256461828947067}]}, {"text": "The HAT annotation tool can be configure to use different tagset, which is also called type system, depending on the project.", "labels": [], "entities": []}, {"text": "For news domain, a tagset named Klue was created.", "labels": [], "entities": []}, {"text": "The Klue tagset was developed among several projects at IBM Research, mainly focused on annotate English articles with the goal extracted entities and relations between them.", "labels": [], "entities": [{"text": "Klue tagset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8112219572067261}]}, {"text": "Therefore, Klue is a product of successive refinements, now in its third version.", "labels": [], "entities": []}, {"text": "After the introduction of Watson technology in the market, IBM is moving forward to make the systems adapted to work with other languages, not only English.", "labels": [], "entities": []}, {"text": "The SIRE toolkit is part of the Watson ecosystem.", "labels": [], "entities": [{"text": "Watson ecosystem", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.9235269129276276}]}, {"text": "Our project is to help on the improvement of SIRE models for Portuguese.", "labels": [], "entities": [{"text": "SIRE", "start_pos": 45, "end_pos": 49, "type": "TASK", "confidence": 0.7379955649375916}]}, {"text": "Since annotated corpora were necessary to this task, we have developed an initial experiment to use an already available annotated Portuguese corpora to train a extractor model using SIRE.", "labels": [], "entities": []}, {"text": "For this, we decide to use HAREM 1 gold collection, mapping the annotation from HAREM into Klue.", "labels": [], "entities": [{"text": "HAREM 1 gold collection", "start_pos": 27, "end_pos": 50, "type": "DATASET", "confidence": 0.8553681522607803}]}, {"text": "Since SIRE achieves high F1 measures in many languages, this make us to believe that if we use a good annotated corpus in Portuguese, we could also obtain a good extractor using SIRE training module.", "labels": [], "entities": [{"text": "F1 measures", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9822016954421997}]}, {"text": "HAREM was a joint evaluation of NER system for Portuguese promoted by Linguateca, that had two editions so far.", "labels": [], "entities": [{"text": "HAREM", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8970404863357544}, {"text": "NER", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.7986155152320862}, {"text": "Linguateca", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.9337877035140991}]}, {"text": "The tagset used in the gold collection of HAREM was created especially for the Second HAREM competition and it was the result of an agreement between the competitors that shared the combination of the types that their systems were able to recognize.", "labels": [], "entities": [{"text": "gold collection of HAREM", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.7602468729019165}, {"text": "Second HAREM competition", "start_pos": 79, "end_pos": 103, "type": "DATASET", "confidence": 0.8112071553866068}]}, {"text": "In other words, the HAREM tagset was not planned as a tagset with the goal of supporting information extraction in any particular project, instead it was built from the combination of the types that several systems could annotate.", "labels": [], "entities": [{"text": "HAREM tagset", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.8311810195446014}, {"text": "information extraction", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7306929379701614}]}, {"text": "This works aims to describe our attempt to evaluate how adequate a tagset created for annotate named entities occurrences in English texts is to annotate Portuguese texts.", "labels": [], "entities": []}, {"text": "Although Klue tagset is supposed to be a language-independent tagset, the differences we found between Klue and HAREM type systems grew some important questions: (i) Can tagsets really be language-independent?", "labels": [], "entities": []}, {"text": "(ii) Can we believe in a true universal tagset which capture NE from any language?", "labels": [], "entities": []}, {"text": "(iii) Does it make sense to expect that they will be completely interchangeable?", "labels": [], "entities": []}, {"text": "For now, we are still working on these answers and analysing both Klue and HAREM tagsets under these thoughts.", "labels": [], "entities": [{"text": "Klue and HAREM tagsets", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.6884849369525909}]}, {"text": "For sure, tagsets are more useful if they are created to a specific domain and project, in a specific language, to a specific textual genre, but the general attempt to reach an universal tagset is an imposed challenge, since most of the tools for Natural Language Processing (NLP) aim to be universal, i.e., they aim to work with many languages and to be interoperable.", "labels": [], "entities": []}, {"text": "We expect to achieve a straightforward methodology to compare and adapt two different tagsets for NER.", "labels": [], "entities": []}, {"text": "Also, we expected that there problems when using Klue tagset into Portuguese data will arise.", "labels": [], "entities": [{"text": "Klue tagset into Portuguese data", "start_pos": 49, "end_pos": 81, "type": "DATASET", "confidence": 0.8045144319534302}]}, {"text": "We'll attempt to produce an empirical overview of this kind of adaptation, that is common in NLP studies, but it is not so frequently considered.", "labels": [], "entities": []}, {"text": "This short paper is being written while the experiment is still undergoing, but we intend to report our experience so far and share ideas with the NER researchers community.", "labels": [], "entities": [{"text": "NER researchers", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.6244497001171112}]}, {"text": "The work is presented as following: first we'll introduce Klue and HAREM.", "labels": [], "entities": [{"text": "HAREM", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.965655505657196}]}, {"text": "In Section 3.1 we'll describe our proposal for the comparison between them and present the issues we found, focusing on linguistic problems from multilingual perspective, and what we could learn until now from this experiment.", "labels": [], "entities": []}, {"text": "Finally we'll discuss some possible conclusions from it and what we leave as future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}