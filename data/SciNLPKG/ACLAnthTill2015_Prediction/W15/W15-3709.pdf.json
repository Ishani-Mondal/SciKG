{"title": [{"text": "Enriching Interlinear Text using Automatically Constructed Annotators", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we will demonstrate a system that shows great promise for creating Part-of-Speech taggers for languages with little to no curated resources available, and which needs no expert involvement.", "labels": [], "entities": [{"text": "Part-of-Speech taggers", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7610228955745697}]}, {"text": "In-terlinear Glossed Text (IGT) is a resource which is available for over 1,000 languages as part of the Online Database of INterlinear text (ODIN) (Lewis and Xia, 2010).", "labels": [], "entities": [{"text": "In-terlinear Glossed Text (IGT)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.4740615139404933}, {"text": "Online Database of INterlinear text (ODIN) (Lewis and Xia, 2010)", "start_pos": 105, "end_pos": 169, "type": "DATASET", "confidence": 0.7902810215950012}]}, {"text": "Using nothing more than IGT from this database and a classification-based projection approach tailored for IGT, we will show that it is feasible to train reasonably performing annotators of interlinear text using projected annotations for potentially hundreds of world's languages.", "labels": [], "entities": [{"text": "IGT", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.7999932765960693}]}, {"text": "Doing so can facilitate automatic enrichment of interlinear resources to aid the field of linguistics.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we discuss the process by which a highly multilingual linguistic resource (greater than 1,200 languages) can be built and then subsequently automatically enriched.", "labels": [], "entities": []}, {"text": "Although we touch upon tools for building and maintaining such a resource, our focus in this paper is not so much on the process by which we curate the data, but the process by which automatically enrich the data with additional layers of linguistic analysis.", "labels": [], "entities": []}, {"text": "Crucially, we show that the linguistic knowledge encapsulated in all of the data, irrespective of the language, can improve the accuracy of NLP tools that are developed for any specific language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9986749291419983}]}, {"text": "This is particularly true for languages that are otherwise highly under-resourced, and where the development of automated NLP tools, such as taggers, are either not possible or very expensive to develop using traditional methods.", "labels": [], "entities": []}, {"text": "We will focus on the development of Part-ofSpeech (POS) taggers.", "labels": [], "entities": []}, {"text": "POS tagging is generally thought of as a solved task for many languages, with per-token accuracies reaching 97%.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7678866982460022}, {"text": "accuracies", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.8106701374053955}]}, {"text": "While these high accuracies can certainly be achieved for languages with substantial annotated resources, many lowresource languages have little to no annotated data available, making such traditional supervised approaches impossible.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9894005656242371}]}, {"text": "Given the cost in developing such resources, many languages with insufficient economic or strategic interest may never see dedicated tools.", "labels": [], "entities": []}, {"text": "If annotated resources are not available, what methods can be used?", "labels": [], "entities": []}, {"text": "Several approaches have been proposed to solve the problems posed by the shortage of labeled training data.", "labels": [], "entities": []}, {"text": "The first are purely unsupervised techniques.", "labels": [], "entities": []}, {"text": "POS induction techniques, such as class-based n-grams ( or feature-based HMM induce parts-of-speech without the need for labeled data by finding the ways in which words appear to pattern similarly in clusters.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8446548581123352}]}, {"text": "However, as noted, the way to map the induced clusters to meaningful tags is not straightforward.", "labels": [], "entities": []}, {"text": "Other work has looked at solving the issue of alack of data by using two or more closely related languages where one of the languages is resourcerich.", "labels": [], "entities": []}, {"text": "used Czech resources to tag Russian.", "labels": [], "entities": [{"text": "tag Russian", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.818830132484436}]}, {"text": "This, however, requires the languages to be closely related, and not all resourcepoor languages have closely-related resource-rich languages.", "labels": [], "entities": []}, {"text": "Another path of inquiry has been to use one unrelated resource-rich language and alignment tech-nnisaau daxalna makaatibahunna the-women(3.PL.F.)-NOM entered-3.PL.F office(PL.)-ACC-their(F.) their offices.\"", "labels": [], "entities": []}, {"text": "\"The women have entered LANG GLOSS TRANS: An example of Interlinear Glossed Text (IGT) in Arabic from, with an English translation.", "labels": [], "entities": [{"text": "LANG GLOSS TRANS", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7193843324979147}, {"text": "Interlinear Glossed Text (IGT)", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.620512475570043}]}, {"text": "niques to \"project\" information from the resourcerich language to the resource-poor one.; both investigated training POS taggers by projecting labels from one language to another, while looked at projecting dependency parsers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.7967678606510162}]}, {"text": "In this paper, we focus on using a resource known as Interlinear Glossed Text (IGT) as a possible source of linguistic knowledge for the POS tagging task on resource-poor languages, and apply it to the enrichment of a linguistic resource composed of IGT data.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.924296498298645}]}, {"text": "An example of IGT is shown in.", "labels": [], "entities": [{"text": "IGT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.6886146068572998}]}, {"text": "IGT is a format used by linguists forgiving examples of linguistic phenomena, and since linguists study a large number of languages, IGT instances can be found for hundreds of languages.", "labels": [], "entities": []}, {"text": "We will explain the precise structure of the data in depth later, but IGT as a resource is appealing not only for its broad coverage, but also the linguistic knowledge it contains.", "labels": [], "entities": []}, {"text": "Although it does not typically contain POS tags explicitly, these examples often contain enough data to make inferences which can be used to enrich the data, whether with POS tags or with other syntactic information (.", "labels": [], "entities": []}, {"text": "We present a system which takes advantage of the structure of IGT instances in order to perform automatic part-of-speech tagging of the target language, regardless of the language.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.6625567376613617}]}, {"text": "While the tagging performance is not necessarily competitive with state-of-the-art supervised systems, it shows great promise for languages with which such supervised systems are not currently possible, and can increase the value of the entire resource to the linguistic and computational linguistic communities.", "labels": [], "entities": [{"text": "tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9632251858711243}]}, {"text": "POS taggers are intrinsically valuable to computational linguists, since they are building blocks fora number of other NLP tools.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7071316242218018}]}, {"text": "Theoretical and descriptive linguists might question their value to them; however, they only represent a class of possible annotators.", "labels": [], "entities": []}, {"text": "The projection methodology, especially the fact that projection accuracy is boosted by relying on an entire corpus, can be applied to other forms of annotation, such as tags or analyses that maybe of benefit for subsequent analyses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9887879490852356}]}, {"text": "Although such taggers will not be as accurate as human annotators, they could reduce workload by doing first pass analyses automatically.", "labels": [], "entities": []}], "datasetContent": [{"text": "For this work, we wanted to test three overall scenarios: using projection alone ( \u00a76.1), using the classifier trained on ODIN data ( \u00a76.2), and then using the classifier trained on Chintang data ( \u00a76.3).", "labels": [], "entities": [{"text": "ODIN data", "start_pos": 122, "end_pos": 131, "type": "DATASET", "confidence": 0.953070342540741}, {"text": "Chintang data", "start_pos": 182, "end_pos": 195, "type": "DATASET", "confidence": 0.9387522339820862}]}, {"text": "All three scenarios will be evaluated on the dev set of the Chintang corpus.", "labels": [], "entities": [{"text": "Chintang corpus", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9211488366127014}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics for Chintang IGT data  (Bickel et al., 2011).", "labels": [], "entities": [{"text": "Chintang IGT data", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.7882533272107443}]}, {"text": " Table 3: Top 12 gloss tokens labeled \"gm,\" sorted  by decreasing frequency.", "labels": [], "entities": []}, {"text": " Table 4: Results of projection-only approach.", "labels": [], "entities": []}, {"text": " Table 5: Classification results showing different  sets of training data and classifier features.", "labels": [], "entities": []}]}