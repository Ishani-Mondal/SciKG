{"title": [{"text": "Semi-Automatic Construction of a Textual Entailment Dataset: Selecting Candidates with Vector Space Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Recognizing Textual Entailment (RTE) is an NLP task aimed at detecting whether the meaning of a given piece of text entails the meaning of another one.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.855065921942393}, {"text": "detecting whether the meaning of a given piece of text entails", "start_pos": 61, "end_pos": 123, "type": "TASK", "confidence": 0.5935048650611531}]}, {"text": "Despite its relevance to many NLP areas, it has been scarcely explored in Portuguese, mainly due to the lack of labeled data.", "labels": [], "entities": []}, {"text": "A dataset for RTE must contain both positive and negative examples of entailment, and neither should be obvious: negative examples shouldn't be completely unrelated texts and positive examples shouldn't be too similar.", "labels": [], "entities": [{"text": "RTE", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9449089765548706}]}, {"text": "We report here an ongoing work to address this difficulty using Vector Space Models (VSMs) to select candidate pairs from news clusters.", "labels": [], "entities": []}, {"text": "We compare three different VSMs, and show that Latent Dirichlet Allocation achieves promising results, yielding both good positive and negative examples.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.8874306082725525}]}], "introductionContent": [{"text": "Recognizing Textual Entailment (RTE) is a Natural Language Processing (NLP) task aimed at determining when a given piece of text T entails the meaning of a hypothesis H.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8388209144274393}, {"text": "determining when a given piece of text T entails the meaning of a hypothesis H", "start_pos": 90, "end_pos": 168, "type": "TASK", "confidence": 0.6713498413562775}]}, {"text": "It is useful in many NLP applications, such as Question Answering, Automatic Summarization, Information Extraction and others.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8404603004455566}, {"text": "Automatic Summarization", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8002896904945374}, {"text": "Information Extraction", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.8510509431362152}]}, {"text": "For example, (1) entails the meaning of both (2) and (3).", "labels": [], "entities": []}, {"text": "The last two sentences entail each other, forming a paraphrase relationship.", "labels": [], "entities": []}, {"text": "Paraphrases can be seen as a special case of entailment, occurring when both pieces of text have essentially the same content.", "labels": [], "entities": []}, {"text": "(1) For the accession of new contracts, the closing date was kept on the 30th, as previously informed.", "labels": [], "entities": [{"text": "accession of new contracts", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.8908801972866058}, {"text": "closing date", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.951707124710083}]}, {"text": "(2) The deadline for new contracts accession is on the 30th.", "labels": [], "entities": []}, {"text": "(3) New contracts can be accessed until the 30th.", "labels": [], "entities": []}, {"text": "The exact definition of entailment in the NLP research community is rather subjective.", "labels": [], "entities": []}, {"text": "The widely accepted notion is that T entails H when a person reading T would affirm that H is most likely true.", "labels": [], "entities": []}, {"text": "We also follow this view here.", "labels": [], "entities": []}, {"text": "In order to automatically recognize when T entails H (or refute this possibility), the NLP community has come up with many different strategies, with no single one emerging as the best.", "labels": [], "entities": []}, {"text": "Virtually all of them, however, need labeled data in order to calibrate system parameters; moreover, a labeled dataset is necessary to evaluate systems' performance in a standardized benchmark.", "labels": [], "entities": []}, {"text": "The lack of labeled RTE data is a major obstacle to research in this area for Portuguese.", "labels": [], "entities": []}, {"text": "Obtaining an entailment dataset, however, is not a simple task: while in some other areas, such as part-of-speech tagging and named entity recognition, it suffices to pick texts and have a group of annotators tag them, some other points must betaken into account when it comes to RTE.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.726233497262001}, {"text": "named entity recognition", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.610629161198934}]}, {"text": "First, each item considered in RTE is actually a pair of text passages.", "labels": [], "entities": [{"text": "RTE", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.7738675475120544}]}, {"text": "Such pairs will hardly be found in a single text document; instead, they must be collected from different but related sources.", "labels": [], "entities": []}, {"text": "Some possibilities are news articles grouped by subject or different translations of the same original text; since each one in these cases is a description of the same event, a sentence in one text may entail a sentence in another one.", "labels": [], "entities": []}, {"text": "Second, the presence or absence of an entailment relation must not be obvious.", "labels": [], "entities": []}, {"text": "In other words, T and H should be somewhat similar to each other, especially when there is no entailment; conversely, if T entails H, they should bear some differences.", "labels": [], "entities": []}, {"text": "Thus, while (4) is entailed by (1), it would be a bad example in an RTE dataset, not reflecting the difficulty of the task, since the only change was the replacement of a word by a synonym.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.766329824924469}]}], "datasetContent": [{"text": "Too similar Sentences are too similar in syntactic structure and wording to be useful as an RTE example.", "labels": [], "entities": []}, {"text": "Still, they usually display a paraphrase relationship.", "labels": [], "entities": []}, {"text": "Too different Sentences are too different to be useful as an RTE example.", "labels": [], "entities": []}, {"text": "The first three classes are interesting as RTE examples and should be kept in the final dataset, while the last two should be discarded.", "labels": [], "entities": [{"text": "RTE", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.5329478979110718}]}, {"text": "The notion of when a pair is too similar or too different is rather subjective, but it is important to filter out such cases.", "labels": [], "entities": []}, {"text": "An example of a too different pair is shown in and; and show a too similar one.", "labels": [], "entities": []}, {"text": "Senado adia vota\u00e7\u00e3o do novo indexador de d\u00edvidas dos Estados.", "labels": [], "entities": []}, {"text": "Senate postpones voting for the new debt index of the states.", "labels": [], "entities": []}, {"text": "(8) O objetivo \u00e9 fixar em lei as regras para que os estados usem os recursos dos dep\u00f3sitos judiciais.", "labels": [], "entities": []}, {"text": "The objective is to establish in law the rules for the states to use resources from court deposits.", "labels": [], "entities": []}, {"text": "(9) A segunda partida acontece no pr\u00f3ximo domingo, dia 03, na Arena Joinville.", "labels": [], "entities": [{"text": "Arena Joinville", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9658598899841309}]}, {"text": "The return leg takes place next Sunday, the 3rd, in Arena Joinville.", "labels": [], "entities": [{"text": "Arena Joinville", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9634611904621124}]}, {"text": "(10) A partida de volta em Joinville acontece no pr\u00f3ximo domingo (3), \u00e0s 16h.", "labels": [], "entities": []}, {"text": "The return leg in Joinville takes place next Sunday (the 3rd), at 16h.", "labels": [], "entities": [{"text": "Joinville", "start_pos": 18, "end_pos": 27, "type": "DATASET", "confidence": 0.890884280204773}]}, {"text": "The fourth class, however, requires more careful investigation.", "labels": [], "entities": []}, {"text": "One option is to follow a stricter view and regard pairs in this category as not having an entailment relation; another one would be a more permissive interpretation that would consider that T entails H even if H has some piece of information not found in T . An example of overlapping sentences is the pair (11) and (12).", "labels": [], "entities": []}, {"text": "The first one informs that the game took place on a Wednesday, and that the winning team was Internacional.", "labels": [], "entities": [{"text": "Internacional", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9853295683860779}]}, {"text": "The second one doesn't mention the team's name, but tells that the game was on its home.", "labels": [], "entities": []}, {"text": "While in some situations the missing information might make all the difference (e.g., in a query about the day a game was played), we believe that the pros and cons of treating such pairs as positive or negative should be carefully analyzed.", "labels": [], "entities": []}, {"text": "The results of the manual analysis over a sample of 100 pairs produced by each VSM are summarized in.", "labels": [], "entities": [{"text": "VSM", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.772196888923645}]}, {"text": "They suggest that RP tends to select pairs with higher Soccer team From Rio Grande do Sul state in Brazil.", "labels": [], "entities": [{"text": "RP", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9142357707023621}]}, {"text": "Vector Space Models similarity: it has the most cases of pairs discarded for being too similar and the most paraphrases, and was the only method to select more positive entailment cases than negative ones.", "labels": [], "entities": []}, {"text": "LSI, on the other hand, has the highest number of \"too different\" pairs and the lowest number of overlaps.", "labels": [], "entities": [{"text": "LSI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9454256892204285}]}, {"text": "LDA appears to have a good balance between both extremes, with the lowest total amount of pairs that need to be discarded either for excessive similarity or difference.", "labels": [], "entities": []}, {"text": "The number of negative examples extracted by LDA is a very promising result, since our main concern was the lack of methods for extracting good quality negative RTE pairs.", "labels": [], "entities": []}, {"text": "We want to reinforce that a negative pair here means not only the absence of an entailment relation, but also that T and H talk about related subjects.", "labels": [], "entities": []}, {"text": "All methods selected a low number of positive entailment cases 8 . While we expected higher figures, there are still ways to circumvent this problem.", "labels": [], "entities": []}, {"text": "One way would be to extract pairs from some news clusters using LDA, thus yielding more negative examples, and use Random Projections on others, which would give us more positive pairs.", "labels": [], "entities": []}, {"text": "Also, since our final goal is obtaining an RTE dataset in Portuguese, not bound to a specific method, we could experiment with some strategies found in the literature for obtaining positive entailment pairs.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.8277364671230316}]}], "tableCaptions": [{"text": " Table 1. Counts of each class in the data generated by the VSMs", "labels": [], "entities": [{"text": "Counts", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9746375679969788}, {"text": "VSMs", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.6512650847434998}]}]}