{"title": [{"text": "Rule-based Coreference Resolution in German Historic Novels", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.8745070099830627}]}], "abstractContent": [{"text": "Coreference resolution (CR) is a key task in the automated analysis of characters in stories.", "labels": [], "entities": [{"text": "Coreference resolution (CR)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8951971054077148}, {"text": "automated analysis of characters in stories", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.7198562820752462}]}, {"text": "Standard CR systems usually trained on newspaper texts have difficulties with literary texts, even with novels; a comparison with newspaper texts showed that average sentence length is greater in novels and the number of pronouns, as well as the percentage of direct speech is higher.", "labels": [], "entities": []}, {"text": "We report promising evaluation results fora rule-based system similar to [Lee et al.", "labels": [], "entities": []}, {"text": "2011], but tailored to the domain which recognizes coreference chains in novels much better than CR systems like CorZu.", "labels": [], "entities": []}, {"text": "Rule-based systems performed best on the CoNLL 2011 challenge [Pradhan et al. 2011].", "labels": [], "entities": [{"text": "CoNLL 2011 challenge", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.8904930949211121}]}, {"text": "Recent work in machine learning showed similar results as rule-based systems [Durett et al. 2013].", "labels": [], "entities": [{"text": "machine learning", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7460931539535522}]}, {"text": "The latter has the advantage that its explanation component facilitates a fine grained error analysis for incremental refinement of the rules.", "labels": [], "entities": []}], "introductionContent": [{"text": "The overall goal of our research is the identification of characters in German novels from the 19 th century and an analysis of their attributes.", "labels": [], "entities": [{"text": "identification of characters in German novels from the 19 th century", "start_pos": 40, "end_pos": 108, "type": "TASK", "confidence": 0.8287670070474799}]}, {"text": "The main steps are named entity recognition (NER) of the persons, coreference resolution (CR), attribution of persons and character description with focus on sentiment analysis.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.8010345796744028}, {"text": "coreference resolution (CR)", "start_pos": 66, "end_pos": 93, "type": "METRIC", "confidence": 0.7044513881206512}, {"text": "character description", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.7339510023593903}, {"text": "sentiment analysis", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.9435013234615326}]}, {"text": "While NER in novels is discussed in, we report on work in progress on rule-based coreference resolution in novels.", "labels": [], "entities": [{"text": "NER", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9693537354469299}, {"text": "coreference resolution", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.8388670980930328}]}, {"text": "Tests with existing rule-based or machine learning NLP tools on our novels had unsatisfying results.", "labels": [], "entities": []}, {"text": "In contrast to newspaper texts novels not only exhibit different topics and wording, but also show a heavy use of pronouns (in our corpus 70% of all NEs) and relative few large clusters with long coreference chains opposed to many small clusters (see baseline analysis in table 1).", "labels": [], "entities": []}, {"text": "Another important difference is the number and lengths of passages containing direct speech.", "labels": [], "entities": []}, {"text": "We decided on a rule-based approach because: \uf0b7 A key aspect in coreference resolution is feature and constraint detection.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.9547244012355804}, {"text": "feature and constraint detection", "start_pos": 89, "end_pos": 121, "type": "TASK", "confidence": 0.6331485733389854}]}, {"text": "Features and constraints for ruling in or out candidates for coreference with a high precision can be combined to achieve a high recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9948089122772217}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9987887740135193}]}, {"text": "If such features and constraints are represented by rules, the explanation component of rule-based systems is very valuable in understanding the errors and thus enabling rapid rule refinement.", "labels": [], "entities": [{"text": "rule refinement", "start_pos": 176, "end_pos": 191, "type": "TASK", "confidence": 0.7040501236915588}]}, {"text": "\uf0b7 We do not have a large corpus with annotated German novels to learn from.", "labels": [], "entities": []}, {"text": "As mentioned above, there are substantial differences between e.g. newspapers and novels, so that machine learning approaches with domain adaptation (e.g.) are difficult.", "labels": [], "entities": []}, {"text": "\uf0b7 We intend to use rule-based CR to semiautomatically create a large corpus of annotated novels for experimenting with machine learning CR approaches.", "labels": [], "entities": []}, {"text": "We present a state-of-the-art rule-based system tailored for CR in novels.", "labels": [], "entities": [{"text": "CR", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9697070717811584}]}, {"text": "In comparison to CorZu (see section 4) which recognizes CR well in newspapers, we achieve better results in novels (MUC F1: 85.5% vs. 65.9%, B 3 F1: 56.0% vs. 33.6%).", "labels": [], "entities": [{"text": "CorZu", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.8961765766143799}, {"text": "CR", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.8657836318016052}, {"text": "MUC", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.7849634289741516}, {"text": "F1", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.6010320782661438}, {"text": "B 3 F1", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.7000916004180908}]}, {"text": "Our explanation component facilitates a fine grained error analysis for incremental rule refinement.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the coreference resolution algorithm in two experiments.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.9601791203022003}]}, {"text": "The first one uses the test corpus of 48 novel fragments with about 19.000 manually annotated character references in total.", "labels": [], "entities": []}, {"text": "The following common evaluation metrics are used (see]): \uf0b7 The MUC-Score.", "labels": [], "entities": [{"text": "MUC-Score", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.5911310911178589}]}, {"text": "It is based on the idea to count the minimum amount of links which need to be added to derive the true set of entities from the predicted set of entities or vice versa, divided by the amount of links in the spanning tree of the true partition.", "labels": [], "entities": []}, {"text": "The MUC-Score itself is the harmonic mean out of both numbers that you get when you switch the true partition with the gold partition.", "labels": [], "entities": [{"text": "MUC-Score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.6451077461242676}]}, {"text": "\uf0b7 The B 3 -Score.", "labels": [], "entities": [{"text": "B 3 -Score", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9342819601297379}]}, {"text": "The MUC Score cannot measure the influence of singleton clusters, that's why an additional evaluation metric is needed.", "labels": [], "entities": [{"text": "MUC Score", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7820520997047424}]}, {"text": "The B\u00b3-Score scales the overlap of predicted clusters and true clusters, based on how many markables were correctly assigned to a given cluster.", "labels": [], "entities": [{"text": "B\u00b3-Score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9779139161109924}]}, {"text": "The effect of the different evaluation measures on a newspaper corpus with rather short coreference chains and on a novel corpus with long chains is shown in the baseline analysis in table 1.", "labels": [], "entities": [{"text": "newspaper corpus", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.8906869888305664}]}, {"text": "While for newspapers the baseline with n clusters for n NEs is very good, for novels the baseline with just one cluster for all NEs performs well.", "labels": [], "entities": []}, {"text": "This can be explained using the structure of the underlying entities.", "labels": [], "entities": []}, {"text": "While in newspaper texts many different entities with only a few mentions appear, our domain shows relatively few entities that tend to show up frequently.", "labels": [], "entities": []}, {"text": "We compared our system with the free coreference resolution software CorZu, using ParZu 4] as its parser from the university of Zurich, which was developed using a newspaper corpus.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.8325040936470032}]}, {"text": "CorZu was given the same annotated named entities in the same novel fragments, so that the detected chains were comparable.", "labels": [], "entities": []}, {"text": "The effect of the passes (see section 3) in our system evaluated on the novel corpus is given in  We finally evaluated our system on our second test set, comprising 30 completely unseen fragments and achieved an F1-score of 86% MUC-F1 and a B\u00b3-F1 of 55.5%.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9995587468147278}, {"text": "MUC-F1", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.7837339639663696}, {"text": "B\u00b3-F1", "start_pos": 241, "end_pos": 246, "type": "METRIC", "confidence": 0.9614378015200297}]}, {"text": "It is almost identical to the result of the first test set.", "labels": [], "entities": []}, {"text": "Rule-based systems with an explanation component allow a fine-grained error analysis.", "labels": [], "entities": []}, {"text": "shows an error analysis for 5 randomly selected novel fragments from the 30 novels, drawn from the second test set that we used for evaluation: The category \"Wrong g|n|p\" refers to the sum of mistakes the algorithm made that were caused by a wrong assignment of gender, number or person.", "labels": [], "entities": []}, {"text": "The category \"Ds related\", contains all errors related to direct speech, e.g. by assigning a wrong speaker or the wrong detection of the addressed person to a given direct speech annotation.", "labels": [], "entities": []}, {"text": "shows that even though we combined 4 different morphological resources the recognition of wrong number, gender and person still makes up a fraction of about 14% of the total amount of errors in the analyzed documents.", "labels": [], "entities": []}, {"text": "Another part with 14% of the mistakes is the category that describes all errors related to direct speech, e.g. wrong speaker detection, missed detection of \"Sie\" in the role of \"du\" or wrong detection of an addressed person.", "labels": [], "entities": [{"text": "speaker detection", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7142993807792664}]}, {"text": "We intend to find some additional constraints to further reduce the errors made in these categories.", "labels": [], "entities": []}, {"text": "The next category with 35% error contribution, labeled as heuristics, sums up all the errors which happened due to a wrong assumption of salience, parser errors or errors that were induced by former misclassified NEs.", "labels": [], "entities": [{"text": "error contribution", "start_pos": 27, "end_pos": 45, "type": "METRIC", "confidence": 0.9486933946609497}]}, {"text": "Still the biggest share of mistakes (37%) and probably also the ones that are most difficult to fix is the class of semantic errors.", "labels": [], "entities": []}, {"text": "Most of these misclassifications can only be resolved with additional knowledge about the world or the entities in the novel itself (\"a widow is a woman who lost her husband; \"his profession is forester\"; \u2026).", "labels": [], "entities": []}, {"text": "Apart from these, there are other mistakes related to an unmodeled context, such as thoughts, songs or letters that appear throughout the text.", "labels": [], "entities": []}, {"text": "We plan to integrate the work of Brunner] to detect those instances and thereby improve the quality of our system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: evaluation results of our system and CorZu on 48  novel fragments with about 19 000 named entities.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation and comparison of the effects of the  different passes of the rule-based algorithm.", "labels": [], "entities": []}, {"text": " Table 4: Number of named entities, clusters, evaluation met- rics and error types for a sample of 5 novel fragments, drawn  randomly from our second test set comprising 30 fragments.", "labels": [], "entities": []}]}