{"title": [{"text": "Incremental Adaptation Strategies for Neural Network Language Models", "labels": [], "entities": [{"text": "Incremental Adaptation Strategies", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8854197462399801}]}], "abstractContent": [{"text": "It is today acknowledged that neural network language models outperform back-off language models in applications like speech recognition or statistical machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7395909279584885}, {"text": "statistical machine translation", "start_pos": 140, "end_pos": 171, "type": "TASK", "confidence": 0.6958730022112528}]}, {"text": "However, training these models on large amounts of data can take several days.", "labels": [], "entities": []}, {"text": "We present efficient techniques to adapt a neural network language model to new data.", "labels": [], "entities": []}, {"text": "Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers.", "labels": [], "entities": []}, {"text": "We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.993889331817627}]}, {"text": "Both methods are very fast and achieve significant improvements without over-fitting the small adaptation data.", "labels": [], "entities": []}], "introductionContent": [{"text": "A language model (LM) plays an important role in many natural language processing applications, namely speech recognition and statistical machine translation (SMT).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7751296758651733}, {"text": "statistical machine translation (SMT)", "start_pos": 126, "end_pos": 163, "type": "TASK", "confidence": 0.7941791315873464}]}, {"text": "For a very longtime, back-off n-gram models were considered to be the state-ofthe-art, in particular when large amounts of training data are available.", "labels": [], "entities": []}, {"text": "An alternative approach is based on the use of high-dimensional embeddings of the words and the idea to perform the probability estimation in this space.", "labels": [], "entities": []}, {"text": "By these means, meaningful interpolations can be expected.", "labels": [], "entities": []}, {"text": "The projection and probability estimation can be jointly learned by a neural network ().", "labels": [], "entities": []}, {"text": "These models, also called continuous space language models (CSLM), have seen a surge in popularity, and it was confirmed in many studies that they systematically outperform back-off n-gram models by a significant margin in SMT and speech recognition.", "labels": [], "entities": [{"text": "SMT", "start_pos": 223, "end_pos": 226, "type": "TASK", "confidence": 0.9960965514183044}, {"text": "speech recognition", "start_pos": 231, "end_pos": 249, "type": "TASK", "confidence": 0.7915445864200592}]}, {"text": "Many variants of the basic approach were proposed during the last years, e.g. the use of recurrent architectures) or LSTM ().", "labels": [], "entities": []}, {"text": "More recently, neural networks were also used for the translation model in an SMT system (, and first translations systems entirely based on neural networks were proposed.", "labels": [], "entities": [{"text": "translation", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9757177829742432}, {"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9878796935081482}]}, {"text": "However, to the best of our knowledge, all these systems are static, i.e. they are trained once on a large representative corpus and are not changed or adapted to new data or conditions.", "labels": [], "entities": []}, {"text": "The ability to adapt to changing conditions is a very important property of an operational SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.98482346534729}]}, {"text": "The need for adaptation occurs for instance in a system to translate daily news articles in order to account for the changing environment.", "labels": [], "entities": []}, {"text": "Another typical application is the integration of an SMT system in an CAT 1 tool: we want to improve the SMT systems with help of user corrections.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9889647960662842}, {"text": "SMT", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9803407192230225}]}, {"text": "Finally, one may also want to adapt a generic SMT to a particular genre or topic for which we lack large amounts of specific data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9735643267631531}]}, {"text": "Various adaptation schemes were proposed for classical SMT systems, but to the best of our knowledge, there is only very limited works involving neural network models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9936383366584778}]}, {"text": "We are interested in a setting where an LM needs to be adapted to a small amount of data which is representative of a domain change, so that the overall system will perform better on this domain in the future.", "labels": [], "entities": []}, {"text": "Our task, which corresponds to concrete needs in real-world applications, is the translation of a document by an human over several days.", "labels": [], "entities": [{"text": "translation of a document", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.8496979475021362}]}, {"text": "The human translator is assisted by an SMT system which proposes translation hypothesis to speedup his work (post editing).", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9666953086853027}]}, {"text": "After one day of work, we adapt the CSLM to the transla-1 Computer Assisted Translation tions already performed by the human translator, and show that the SMT system performs better on the remaining part of the document.", "labels": [], "entities": [{"text": "transla-1 Computer Assisted Translation tions", "start_pos": 48, "end_pos": 93, "type": "TASK", "confidence": 0.5972237408161163}, {"text": "SMT", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.9659115076065063}]}, {"text": "In this paper, we use the open-source MateCat tool 2 and a closely integrated SMT system 3 which is already adapted to the task (translation of legal documents).", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9819177985191345}, {"text": "translation of legal documents", "start_pos": 129, "end_pos": 159, "type": "TASK", "confidence": 0.8983428180217743}]}, {"text": "For each source sentence, the system proposes an eventual match in the translation memory and a translation by the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9420784711837769}]}, {"text": "The human translator can decide to either post-edit them, or to perform anew translation from scratch.", "labels": [], "entities": []}, {"text": "After one day of work, we want to use all the postedited sentences to adapt the SMT systems, so that the translation quality is improved for the next day.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9589948654174805}]}, {"text": "This means that the SMT system will be adapted to the specific translation project.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9897429347038269}]}, {"text": "One important particularity of the task is that we have a very small amount of adaptation data, usually around three thousand words per day.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next two sections, we summarize basic notions of statistical machine translation and continuous space language models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.7208051482836405}]}, {"text": "We then present our tasks and results.", "labels": [], "entities": []}, {"text": "The paper concludes with a discussion and directions of future research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparative BLEU scores for the English/German systems. Italic values in parenthesis are for  information only. They are biased since the reference translations are used in training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9840819835662842}]}, {"text": " Table 3: English/German system: number of examples (28-grams) seen by the CSLM at each epoch.  For the domain adapted system, we randomly resample about 42% of the examples at each epoch. For  the project-adapted system, we experimented with various mixtures between generic and project specific  data (Day 1). We don't want to train on Day 1 data only since this would result in strong over-fitting.", "labels": [], "entities": []}, {"text": " Table 6: English/French task: proportion of each  day in the adaptation data set, e.g. at the end of  Day 2, we create an adaptation corpus which con- sists of 27.9% and 29.6% of data from Day 1 and  Day 2 respectively, the remaining portions are ran- domly resampled in the training data.", "labels": [], "entities": []}, {"text": " Table 7: English/French task: perplexities of base- line and adapted CSLM (on all preceding days),  e.g. the CSLM tested on Day 4 is the baseline  CSLM that had been adapted with Days 1-3.", "labels": [], "entities": []}, {"text": " Table 5: BLEU scores obtained by a baseline SMT (without and with an CSLM) and a project-adapted  SMT with baseline (unadapted) CSLM and adapted CSLM. The first value in every cell is the BLEU  score obtained with respect to the reference translation of the human translator; the second one is cal- culated with respect to all the 3 references created by the professional translators (i.e. obtained by  post-edition) and an independent reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985454082489014}, {"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.953019380569458}, {"text": "BLEU  score obtained", "start_pos": 189, "end_pos": 209, "type": "METRIC", "confidence": 0.9660943150520325}]}]}