{"title": [{"text": "Domain Adaptation for Dependency Parsing via Self-training", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.71024589240551}, {"text": "Dependency Parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.788262665271759}]}], "abstractContent": [{"text": "This paper presents a successful approach for domain adaptation of a dependency parser via self-training.", "labels": [], "entities": [{"text": "domain adaptation of a dependency parser", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.7466000020503998}]}, {"text": "We improve parsing accuracy for out-of-domain texts with a self-training approach that uses confidence-based methods to select additional training samples.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9693348407745361}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9338051676750183}]}, {"text": "We compare two confidence-based methods: The first method uses the parse score of the employed parser to measure the confidence into a parse tree.", "labels": [], "entities": []}, {"text": "The second method calculates the score differences between the best tree and alternative trees.", "labels": [], "entities": []}, {"text": "With these methods, we were able to improve the labeled accuracy score by 1.6 percentage points on texts from a chemical domain and by 0.6 on average on texts of three web domains.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 56, "end_pos": 70, "type": "METRIC", "confidence": 0.9485700130462646}]}, {"text": "Our improvements on the chemical texts of 1.5% UAS is substantially higher than improvements reported in previous work of 0.5% UAS.", "labels": [], "entities": [{"text": "UAS", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.590704083442688}, {"text": "UAS", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.6420786380767822}]}, {"text": "For the three web domains, no positive results for self-training have been reported before.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semi-supervised techniques gain popularity since they allow the exploitation of unlabeled data and avoid the high costs for labeling new data, cf.).", "labels": [], "entities": []}, {"text": "For domain adaptation, semi-supervised techniques have been applied successfully, cf. ().", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.83011794090271}]}, {"text": "Self-training is one of these appealing techniques which improves parsing accuracy by using a parser's own annotations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9681315422058105}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8994113206863403}]}, {"text": "Ina self-training iteration, abase model is first trained on annotated corpora, the base model is then used to annotate unlabeled data, finally a self-trained model is trained on both manually and automatically annotated data.", "labels": [], "entities": []}, {"text": "This procedure might be repeated several times.", "labels": [], "entities": []}, {"text": "Self-training has been successfully used for instance in constituency parsing for in-domain and out-of-domain parsing (.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.8470748960971832}]}, {"text": "used self-training for constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.93927863240242}]}, {"text": "In their approaches, self-training was most effective when the parser is retrained on the combination of the initial training set and the large unlabeled dataset generated by both the generative parser and the reranker.", "labels": [], "entities": []}, {"text": "This leads to many subsequent applications on domain adaptation via self-training for constituency parsing, while for dependency parsing, self-training was only effective in few cases.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7837704420089722}, {"text": "constituency parsing", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.8786568343639374}, {"text": "dependency parsing", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8714004755020142}]}, {"text": "The question why it does notwork equally well for dependency parsing is still a question that has not been satisfactorily answered.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8829932510852814}]}, {"text": "The paper tries to shed some light on the question under which circumstances and why self-training is applicable.", "labels": [], "entities": []}, {"text": "More precisely, this paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We present an effective confidence-based self-training approach.", "labels": [], "entities": []}, {"text": "2. We compare two confidence-based methods to select training sentences for self-training.", "labels": [], "entities": []}, {"text": "3. We apply our approaches on three web domains as well as on a chemical domain and we successfully improved the parsing performances for all tested domains.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: In Section 2, we give an overview of related work.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce two approaches 1 to self-training and apply those on parsing of outof-domain data.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the data and the experimental set-up.", "labels": [], "entities": []}, {"text": "In Section 5, we present and discuss the results.", "labels": [], "entities": []}, {"text": "Section 6 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply the approaches on three web domains and chemical texts.", "labels": [], "entities": []}, {"text": "Section 4.1 describes the datasets that we use in our experiments.", "labels": [], "entities": []}, {"text": "Section 4.2 explains the parser and Section 4.3 the evaluation methods.  of the weblogs section of the OntoNotes v5.0 corpus.", "labels": [], "entities": [{"text": "OntoNotes v5.0 corpus", "start_pos": 103, "end_pos": 124, "type": "DATASET", "confidence": 0.7620541254679362}]}, {"text": "Exact 50% of the selected sentences is used with SANCL Newsgroups and Reviews test data as test sets while the second half is used as a development set.", "labels": [], "entities": [{"text": "SANCL Newsgroups and Reviews test data", "start_pos": 49, "end_pos": 87, "type": "DATASET", "confidence": 0.9360522131125132}]}, {"text": "We converted the datasets with the LTH constituent-to-dependency conversion tool, cf. (. For the source domain training data, we use the CoNLL 2009 training dataset, cf. (. shows the details for the training, development and test set.", "labels": [], "entities": [{"text": "LTH constituent-to-dependency conversion", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.5426222781340281}, {"text": "CoNLL 2009 training dataset", "start_pos": 137, "end_pos": 164, "type": "DATASET", "confidence": 0.9700812995433807}]}, {"text": "We use 500k of the SANCL unlabeled data for each domain after we pre-processed them by removing sentences that are longer than 500 tokens or containing non-English words which reduced the size of the datasets by 2%.", "labels": [], "entities": [{"text": "SANCL unlabeled data", "start_pos": 19, "end_pos": 39, "type": "DATASET", "confidence": 0.7739055554072062}]}, {"text": "shows details about the amount of unlabeled texts.", "labels": [], "entities": []}, {"text": "To compare with previous work, we apply the approach on texts from the chemical domain that were prepared for the domain adaptation track of the CoNLL 2007 shared task, cf. ( . shows the details about the amount of available sentences for training, development and test set.", "labels": [], "entities": [{"text": "CoNLL 2007 shared task", "start_pos": 145, "end_pos": 167, "type": "DATASET", "confidence": 0.8837678879499435}]}, {"text": "The source data sets of the chemical domain are smaller than the ones for web domains.", "labels": [], "entities": []}, {"text": "The training set has about half of the size.", "labels": [], "entities": []}, {"text": "Thus we use only 250k unlabeled sentences from the chemical domain which share the same ratio of training set size to unlabeled data set size compared to the web domain data sets.", "labels": [], "entities": []}, {"text": "To keep the same scale for training and unlabeled sets allows us easily adapt the best setting from web domain experiments.", "labels": [], "entities": []}, {"text": "For the evaluation of the parser's accuracy, we report labeled attachment scores (LAS).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9985975623130798}, {"text": "labeled attachment scores (LAS)", "start_pos": 55, "end_pos": 86, "type": "METRIC", "confidence": 0.8388485213120779}]}, {"text": "We included all punctuation marks in the evaluation.", "labels": [], "entities": []}, {"text": "For significance testing, we use the script provided by the CoNLL 2007 shared task which is Dan Bikel's randomized parsing evaluation comparator with the default settings of 10,000 iterations.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9662895500659943}, {"text": "CoNLL 2007 shared task", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.9294051229953766}]}, {"text": "The statistically significant results are marked due to their p-values, (*) p-value<0.05, (**) p-value<0.01.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The size of the source domain training set and target domain test datasets for web domain  evaluation.", "labels": [], "entities": [{"text": "web domain  evaluation", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.6100885073343912}]}, {"text": " Table 2: The size of unlabeled datasets for web domain evaluation.", "labels": [], "entities": [{"text": "web domain evaluation", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.629914273818334}]}, {"text": " Table 3: The size of datasets for chemical domain  evaluation.", "labels": [], "entities": [{"text": "chemical domain  evaluation", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.640382985273997}]}, {"text": " Table 5: The results of the adjusted parse score-based and the Delta-based self-training approaches  on the chemical test set compared with the best-reported self-training gain", "labels": [], "entities": [{"text": "parse score-based", "start_pos": 38, "end_pos": 55, "type": "METRIC", "confidence": 0.952738344669342}, {"text": "chemical test set", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.7359631260236105}]}, {"text": " Table 4: The effect of the adjusted parse score- based (PS) and the Delta-based self-training ap- proaches on weblogs, newsgroups and reviews test  sets.", "labels": [], "entities": [{"text": "adjusted parse score- based (PS)", "start_pos": 28, "end_pos": 60, "type": "METRIC", "confidence": 0.8773684799671173}]}]}