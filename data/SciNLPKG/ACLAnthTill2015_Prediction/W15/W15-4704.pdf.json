{"title": [{"text": "Inducing Clause-Combining Rules: A Case Study with the SPaRKy Restaurant Corpus", "labels": [], "entities": [{"text": "SPaRKy Restaurant", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.8622202277183533}]}], "abstractContent": [{"text": "We describe an algorithm for inducing clause-combining rules for use in a traditional natural language generation architecture.", "labels": [], "entities": []}, {"text": "An experiment pairing lexical-ized text plans from the SPaRKy Restaurant Corpus with logical forms obtained by parsing the corresponding sentences demonstrates that the approach is able to learn clause-combining operations which have essentially the same coverage as those used in the SPaRKy Restaurant Corpus.", "labels": [], "entities": [{"text": "SPaRKy Restaurant Corpus", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.881111224492391}, {"text": "SPaRKy Restaurant Corpus", "start_pos": 285, "end_pos": 309, "type": "DATASET", "confidence": 0.8118122617403666}]}, {"text": "This paper fills a gap in the literature , showing that it is possible to learn mi-croplanning rules for both aggregation and discourse connective insertion, an important step towards ameliorating the knowledge acquisition bottleneck for NLG systems that produce texts with rich discourse structures using traditional architectures.", "labels": [], "entities": [{"text": "discourse connective insertion", "start_pos": 126, "end_pos": 156, "type": "TASK", "confidence": 0.6761907835801443}]}], "introductionContent": [{"text": "Ina traditional natural language generation (NLG) system), a pipeline of hand-crafted components is used to generate high quality text, albeit at considerable knowledgeengineering expense.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.8146303792794546}]}, {"text": "While there has been progress on using machine learning to ameliorate this issue in content planning) and broad coverage surface realization, the central stage of sentence planning (or microplanning) has proved more difficult to automate.", "labels": [], "entities": [{"text": "content planning", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7451603710651398}, {"text": "broad coverage surface realization", "start_pos": 106, "end_pos": 140, "type": "TASK", "confidence": 0.6574647203087807}, {"text": "sentence planning", "start_pos": 163, "end_pos": 180, "type": "TASK", "confidence": 0.7228507548570633}]}, {"text": "More recently, and, inter alia, have developed end-to-end learning methods for NLG systems; however, as discussed further in the next section, these systems assume quite limited discourse structures in comparison to those with more traditional architectures.", "labels": [], "entities": []}, {"text": "In this paper, we describe a method of inducing clause-combining rules of the kind used in traditional sentence planners.", "labels": [], "entities": []}, {"text": "In particular, we base our approach on the architecture used in the SPaRKy restaurant recommendation system (, where a sentence plan generator is used to map a text plan to a range of possible sentence plans, from which one is selected for output by a sentence plan ranker.", "labels": [], "entities": [{"text": "SPaRKy restaurant recommendation", "start_pos": 68, "end_pos": 100, "type": "TASK", "confidence": 0.7747661670049032}]}, {"text": "To demonstrate the viability of our method, we present an experiment demonstrating that rules corresponding to all of the hand-crafted operators for aggregation and discourse connective insertion used in the SPaRKy Restaurant Corpus can be effectively learned from examples of their use.", "labels": [], "entities": [{"text": "discourse connective insertion", "start_pos": 165, "end_pos": 195, "type": "TASK", "confidence": 0.7082130511601766}, {"text": "SPaRKy Restaurant Corpus", "start_pos": 208, "end_pos": 232, "type": "DATASET", "confidence": 0.6629995206991831}]}, {"text": "To our knowledge, these induced rules for the first time incorporate the constraints necessary to be functionally equivalent to the hand-crafted clause-combining operators; in particular, our method goes beyond the one Stent and Molina (2009) develop for learning clause-combining rules, which focuses on learning domain-independent rules for discourse connective insertion, ignoring aggregation rules and any potentially domain-dependent aspects of the rules.", "labels": [], "entities": [{"text": "discourse connective insertion", "start_pos": 343, "end_pos": 373, "type": "TASK", "confidence": 0.6747153997421265}]}, {"text": "As such, our approach promises to be of immediate benefit to NLG system developers, while also taking an important step towards reducing the knowledge acquisition bottleneck for developing NLG systems requiring rich discourse structures in their outputs.", "labels": [], "entities": []}, {"text": "present an end-to-end trainable NLG system that generates by selecting a The sentence plan ranker uses machine learning to rank sentence plans based on features derived from the sentence plan and its realization, together with accompanying human ratings for the realizations in the training data.", "labels": [], "entities": []}, {"text": "As such, the SPaRKy architecture differs from traditional ones in using machine learning to rank potential outputs, but it follows the traditional architecture in making use of lexicalization, aggregation and referring expression rules in a distinct sentence planning stage.", "labels": [], "entities": []}, {"text": "sequence of database records to describe, a sequence of fields on those records to mention, and finally a sequence of words for expressing the values of those fields.", "labels": [], "entities": []}, {"text": "Though generalize Angeli et al.'s approach, they acknowledge that handling discourse-level document structure remains for future work.", "labels": [], "entities": []}, {"text": "Given this limitation, under their approach there is no need to explicitly perform aggregation: instead, it suffices to \"pre-aggregate\" propositions about the same entity onto the same record.", "labels": [], "entities": []}, {"text": "However, in the general case aggregation should be subject to discourse structure; for example, when contrasting the positive and negative attributes of an entity according to a given user model, it makes sense to aggregate the positive and negative attributes separately, rather than lumping them together ( . Consequently, we aim to learn aggregation rules that are sensitive to discourse structure, as with the SPaRKy architecture.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using a single-stage version of the algorithm, we also examined its capabilities with respect to learning clause-combining operations not present in the SRC.", "labels": [], "entities": []}, {"text": "To create training examples, we created 167 input pairs based on TPLFS from the SRC", "labels": [], "entities": [{"text": "SRC", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.6735294461250305}]}], "tableCaptions": []}