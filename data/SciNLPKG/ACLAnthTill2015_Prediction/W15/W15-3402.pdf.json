{"title": [{"text": "A Factory of Comparable Corpora from Wikipedia", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.8043016195297241}]}], "abstractContent": [{"text": "Multiple approaches to grab comparable data from the Web have been developed up to date.", "labels": [], "entities": []}, {"text": "Nevertheless, coming outwith a high-quality comparable corpus of a specific topic is not straightforward.", "labels": [], "entities": []}, {"text": "We present a model for the automatic extraction of comparable texts in multiple languages and on specific topics from Wikipedia.", "labels": [], "entities": [{"text": "automatic extraction of comparable texts in multiple languages", "start_pos": 27, "end_pos": 89, "type": "TASK", "confidence": 0.8096364289522171}]}, {"text": "In order to prove the value of the model, we automatically extract parallel sentences from the comparable collections and use them to train statistical machine translation engines for specific domains.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 140, "end_pos": 171, "type": "TASK", "confidence": 0.6208444039026896}]}, {"text": "Our experiments on the English-Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles.", "labels": [], "entities": [{"text": "translating in-domain Wikipedia articles", "start_pos": 189, "end_pos": 229, "type": "TASK", "confidence": 0.8183930367231369}]}, {"text": "Moreover, we show that these corpora can help when translating out-of-domain texts.", "labels": [], "entities": [{"text": "translating out-of-domain texts", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.872045079867045}]}], "introductionContent": [{"text": "Multilingual corpora with different levels of comparability are useful fora range of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Comparable corpora were first used for extracting parallel lexicons.", "labels": [], "entities": [{"text": "extracting parallel lexicons", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.8574048280715942}]}, {"text": "Later they were used for feeding statistical machine translation (SMT) systems and in multilingual retrieval models (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.7722373803456625}]}, {"text": "SMT systems estimate the statistical models from bilingual texts.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9798451066017151}]}, {"text": "Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage.", "labels": [], "entities": []}, {"text": "However, it is evident that no large collections of parallel texts for all domains and language pairs exist.", "labels": [], "entities": []}, {"text": "In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all.", "labels": [], "entities": []}, {"text": "One of the main sources of parallel data is the Web: websites in multiple languages are crawled and contents retrieved to obtain multilingual data.", "labels": [], "entities": []}, {"text": "Wikipedia, an on-line community-curated encyclopaedia with editions in multiple languages, has been used as a source of data for these purposesfor instance, (Adafre and de.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9013960361480713}]}, {"text": "Due to its encyclopaedic nature, editors aim at organising its content within a dense taxonomy of categories.", "labels": [], "entities": []}, {"text": "Such a taxonomy can be exploited to extract comparable and parallel corpora on specific topics and knowledge domains.", "labels": [], "entities": []}, {"text": "This allows to study how different topics are analysed in different languages, extract multilingual lexicons, or train specialised machine translation systems, just to mention some instances.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7547207772731781}]}, {"text": "Nevertheless, the process is not straightforward.", "labels": [], "entities": []}, {"text": "The community-generated nature of the Wikipedia has produced a reasonably good -yet chaotic-taxonomy in which categories are linked to each other at will, even if sometimes no relationship among them exists, and the borders dividing different areas are far from being clearly defined.", "labels": [], "entities": []}, {"text": "The rest of the paper is distributed as follows.", "labels": [], "entities": []}, {"text": "We briefly overview the definition of comparability levels in the literature and show the difficulties inherent to extracting comparable corpora from Wikipedia (Section 2).", "labels": [], "entities": []}, {"text": "We propose a simple and effective platform for the extraction of comparable corpora from Wikipedia (Section 3).", "labels": [], "entities": []}, {"text": "We describe a simple model for the extraction of parallel sentences from comparable corpora (Section 4).", "labels": [], "entities": [{"text": "extraction of parallel sentences from comparable corpora", "start_pos": 35, "end_pos": 91, "type": "TASK", "confidence": 0.8257651158741542}]}, {"text": "Experimental results are reported on each of these sub-tasks for three domains using the English and Spanish Wikipedia editions.", "labels": [], "entities": []}, {"text": "We present an application-oriented evaluation of the comparable corpora by studying the impact of the extracted parallel sentences on a statistical machine translation system (Section 5).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 136, "end_pos": 167, "type": "TASK", "confidence": 0.610061893860499}]}, {"text": "Finally, we draw conclusions and outline ongoing work (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we validate the quality of the obtained corpora by studying its impact on statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 90, "end_pos": 121, "type": "TASK", "confidence": 0.6770017941792806}]}, {"text": "There are several parallel corpora for the English-Spanish language pair.", "labels": [], "entities": []}, {"text": "We select as a general-purpose corpus Europarl v7 (, with 1.97M parallel sentences.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9312661290168762}]}, {"text": "The order of magnitude is similar to the largest corpus we have extracted from Wikipedia, so we can compare the results in a size-independent way.", "labels": [], "entities": []}, {"text": "If our corpus extracted from Wikipedia was made up with parallel fragments of the desired domain, it should be the most adequate to translate these domains.", "labels": [], "entities": []}, {"text": "If the quality of the parallel fragments was acceptable, it should also help when translating out-of-domain texts.", "labels": [], "entities": [{"text": "translating out-of-domain texts", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.8883517384529114}]}, {"text": "In order to test these hypotheses we analyse three settings: (i) train SMT systems only with Wikipedia (WP) or Europarl (EP) to translate domain-specific texts, (ii) train SMT systems with Wikipedia and Europarl to translate domain-specific texts, and (iii) train SMT systems with Wikipedia and Europarl to translate out-of-domain texts (news).", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9922438263893127}, {"text": "SMT", "start_pos": 172, "end_pos": 175, "type": "TASK", "confidence": 0.970376193523407}, {"text": "SMT", "start_pos": 264, "end_pos": 267, "type": "TASK", "confidence": 0.9908317923545837}]}, {"text": "For the out-of-domain evaluation we use the News Commentaries 2011 test set and the News Commentaries 2009 for development.", "labels": [], "entities": [{"text": "News Commentaries 2011 test set", "start_pos": 44, "end_pos": 75, "type": "DATASET", "confidence": 0.9596718788146973}, {"text": "News Commentaries 2009", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.9614440600077311}]}, {"text": "For the in-domain evaluation we build the test and development sets in a semiautomatic way.", "labels": [], "entities": []}, {"text": "We depart from the parallel corpora gathered in Section 4 from which sentences with more than four tokens and beginning with a letter are selected.", "labels": [], "entities": []}, {"text": "We estimate its perplexity with respect to a language model obtained with Europarl in order to select the most fluent sentences and then we rank the parallel sentences according to their similarity and perplexity.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9802256226539612}]}, {"text": "The top-n fragments were manually revised and extracted to build the Wikipedia test (WPtest) and development (WPdev) sets.", "labels": [], "entities": [{"text": "Wikipedia test (WPtest)", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.8921307444572448}]}, {"text": "We repeated the process for the three studied domains and drew 300 parallel fragments for development for every domain and 500 for test.", "labels": [], "entities": []}, {"text": "We removed these sentences from the corresponding training corpora.", "labels": [], "entities": []}, {"text": "For one of the domains, CS, we also gathered a test set from a parallel corpus of GNOME localisation files (Tiedemann, 2012).", "labels": [], "entities": []}, {"text": "shows the size in number of sentences of these test sets and of the 20 Wikipedia training sets used for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.9640445709228516}]}, {"text": "Only one measure, that with the highest F 1 score, is selected from each family: c3g, cog, mono en and \u00af S\u00b7len (cf).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9779519438743591}]}, {"text": "We also compile the corpus that results from the union of the previous four.", "labels": [], "entities": []}, {"text": "Notice that, although we eliminate duplicates from this corpus, the size of the union is close to the sum of the individual corpora.", "labels": [], "entities": []}, {"text": "This indicates that every similarity measure selects a different set of parallel fragments.", "labels": [], "entities": []}, {"text": "Beside the specialised corpus for each domain, we build a larger corpus with all the data (Un).", "labels": [], "entities": []}, {"text": "Again, duplicate fragments coming from articles belonging to more than one domain are removed.", "labels": [], "entities": []}, {"text": "SMT systems are trained using standard freely available software.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9866470098495483}]}, {"text": "We estimate a 5-gram language model using interpolated Kneser-Ney discounting with SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9053062200546265}]}, {"text": "Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with Moses (   against the BLEU evaluation metric ().", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7153168618679047}, {"text": "phrase extraction", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8009040951728821}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9736692905426025}]}, {"text": "Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a lexicalised reordering.", "labels": [], "entities": []}, {"text": "(i) Training systems with Wikipedia or Europarl for domain-specific translation.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8918966054916382}, {"text": "domain-specific translation", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6179269254207611}]}, {"text": "shows the evaluation results on WPtest.", "labels": [], "entities": [{"text": "WPtest", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.8818812370300293}]}, {"text": "All the specialised systems obtain significant improvements with respect to the Europarl system, regardless of their size.", "labels": [], "entities": [{"text": "Europarl system", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9714140594005585}]}, {"text": "For instance, the worst specialised system (c3g with only 95,715 sentences for CS) outperforms by more than 10 points of BLEU the general Europarl translator.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9671267867088318}, {"text": "Europarl translator", "start_pos": 138, "end_pos": 157, "type": "DATASET", "confidence": 0.9414256513118744}]}, {"text": "The most complete system (the union of the four representatives) doubles the BLEU score for all the domains with an impressive improvement of 30 points.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9836761951446533}]}, {"text": "This is of course possible due to the nature of the test set that has been extracted from the same collection as the training data and therefore shares its structure and vocabulary.: BLEU scores obtained on the GNOME test set for systems trained only with Wikipedia.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9994850158691406}, {"text": "GNOME test set", "start_pos": 211, "end_pos": 225, "type": "DATASET", "confidence": 0.9674098690350851}]}, {"text": "A system with Europarl achieves a score of 18.15.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9608744382858276}]}, {"text": "Except for c3g, the Wikipedia translators always outperform the baseline with EP; the union system improves it by 4 BLEU points (22.41 compared to 18.15) with a four times smaller corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9991763234138489}]}, {"text": "This confirms that a corpus automatically extracted with an F 1 smaller than 0.5 is still useful for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9961315393447876}]}, {"text": "Notice also that using only the in-domain data (CS) is always better than using the whole WP corpus (Un) even if the former is in general ten times smaller (cf.).", "labels": [], "entities": [{"text": "WP corpus (Un)", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.8795474529266357}]}, {"text": "According to this indirect evaluation of the similarity measures, character n-grams (c3g) represent the worst alternative.", "labels": [], "entities": []}, {"text": "These results contradict the direct evaluation, where c3g and mono en had the highest F 1 scores on the development set among the individual similarity measures.", "labels": [], "entities": [{"text": "F 1", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9929700493812561}]}, {"text": "The size of the corpus is not relevant here: when we train all the systems with the same amount of data, the ranking in the quality of the measures remains the same.", "labels": [], "entities": []}, {"text": "To see this, we trained four additional systems with the top m number of parallel fragments, where m is the size of the smallest corpus for the union of domains: Un-c3g.", "labels": [], "entities": []}, {"text": "This new comparison is reported in columns \"Comp.\" in Tables 8 and 9.", "labels": [], "entities": [{"text": "Comp.", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9693724513053894}]}, {"text": "In this fair comparison c3g is still the worst measure and \u00af S\u00b7len the best one.", "labels": [], "entities": []}, {"text": "The translator built from its associated corpus outperforms with less than half of the data used for training the general one (883,366 vs. 1,965,734 parallel fragments) both in WPtest (56.78 vs. 30.63) and GNOME (19.76 vs. 18.15).", "labels": [], "entities": [{"text": "WPtest", "start_pos": 177, "end_pos": 183, "type": "DATASET", "confidence": 0.9307004809379578}, {"text": "GNOME", "start_pos": 206, "end_pos": 211, "type": "DATASET", "confidence": 0.9153364300727844}]}, {"text": "(ii) Training systems on Wikipedia and Europarl for domain-specific translation.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.832444429397583}, {"text": "domain-specific translation", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6319352984428406}]}, {"text": "Now we enrich the general translator with Wikipedia data or, equivalently, complement the Wikipedia translator with out-of-domain data.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9179040491580963}, {"text": "Wikipedia translator", "start_pos": 90, "end_pos": 110, "type": "DATASET", "confidence": 0.9272534549236298}]}, {"text": "when using all the union data.", "labels": [], "entities": []}, {"text": "System c3g benefits the most of the inclusion of the Europarl data.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.99532151222229}]}, {"text": "The reason is that it is the individual system with less corpus available and the one obtaining the worst results.", "labels": [], "entities": []}, {"text": "In fact, the better the Wikipedia system, the less important the contribution from Europarl is.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.9841168522834778}]}, {"text": "For the independent test set GNOME, shows that the union corpus on CS is better than any combination of Wikipedia and Europarl.", "labels": [], "entities": [{"text": "GNOME", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.818323016166687}, {"text": "Europarl", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.9444403648376465}]}, {"text": "Still, as aforementioned, the best performance on this test set is obtained with a pure in-domain system (cf.).", "labels": [], "entities": []}, {"text": "(iii) Training systems on Wikipedia and Europarl for out-of-domain translation.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.9453724026679993}, {"text": "Europarl", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.7634953856468201}, {"text": "out-of-domain translation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.6934516131877899}]}, {"text": "Now we check the performance of the Wikipedia translators on the out-of-domain news test.  are controlled by the Europarl baseline.", "labels": [], "entities": [{"text": "Europarl baseline", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.9919216334819794}]}, {"text": "In general, systems in which we include only texts from an unrelated domain do not improve the performance of the Europarl system alone, results of the combined system are better when we use Wikipedia texts from all the domains together (column Un) for training.", "labels": [], "entities": [{"text": "Europarl system", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.9639653563499451}]}, {"text": "This suggests that, as expected, a general Wikipedia corpus is necessary to build a general translator.", "labels": [], "entities": []}, {"text": "This is a different problem to deal with.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Amount of articles and categories in  the Wikipedia editions and in the intersection  (i.e., pages linked across languages).", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9653081297874451}]}, {"text": " Table 2: Number of articles in the root categories  and size of the resulting domain vocabulary.", "labels": [], "entities": []}, {"text": " Table 3: Number of article pairs according to the  percentage of positive categories used to select the  levels of the graph and distance from the root at  which the percentage is smaller to the desired one.", "labels": [], "entities": []}, {"text": " Table 4: Best thresholds and their associated Precision (P), recall (R) and F 1 .", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.9637711644172668}, {"text": "recall (R)", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.965170294046402}, {"text": "F 1", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9794533848762512}]}, {"text": " Table 5: Precision, recall, and F 1 for the average  of the similarities weighted by length model (len)  and/or their F 1 .", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9986211061477661}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993991851806641}, {"text": "F 1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9936557710170746}, {"text": "length model (len)", "start_pos": 86, "end_pos": 104, "type": "METRIC", "confidence": 0.9255542755126953}, {"text": "F 1", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9804879128932953}]}, {"text": " Table 6: Size of the parallel corpora extracted with  each similarity measure.", "labels": [], "entities": []}, {"text": " Table 7: Number of sentences of the Wikipedia  parallel corpora used to train the SMT systems  (top rows) and of the sets used for development  and test.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9899424314498901}]}, {"text": " Table 8: BLEU scores obtained on the Wikipedia  test sets for the 20 specialised systems described in  Section 5. A comparison column (Comp.) where  all the systems are trained with corpora of the  same size is also included (see text).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989252686500549}, {"text": "Wikipedia  test sets", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8942564924558004}]}, {"text": " Table 9: BLEU scores obtained on the GNOME  test set for systems trained only with Wikipedia.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992548823356628}, {"text": "GNOME  test set", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.96527232726415}]}, {"text": " Table 10: BLEU scores obtained on the Wikipedia  test set for the 20 systems trained with the com- bination of the Europarl (EP) and the Wikipedia  corpora. The results with a Europarl system and  the best one from Table 8 (union) shown for com- parison.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9994766116142273}, {"text": "Wikipedia  test set", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.945061445236206}, {"text": "Europarl (EP)", "start_pos": 116, "end_pos": 129, "type": "DATASET", "confidence": 0.891913890838623}]}, {"text": " Table 11: BLEU scores obtained on the GNOME  test set for systems trained with Europarl and  Wikipedia. A system with Europarl achieves a  score of 18.15.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9995276927947998}, {"text": "GNOME  test set", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9584402441978455}, {"text": "Europarl", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9514638781547546}, {"text": "Europarl", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.963708221912384}]}, {"text": " Table 12: BLEU scores for the out-of-domain  evaluation on the News Commentaries 2011 test  set. We show in boldface all the systems that im- prove the Europarl translator, which achieves a  score of 27.02.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9987972974777222}, {"text": "News Commentaries 2011 test  set", "start_pos": 64, "end_pos": 96, "type": "DATASET", "confidence": 0.9228072166442871}, {"text": "Europarl translator", "start_pos": 153, "end_pos": 172, "type": "DATASET", "confidence": 0.9591577649116516}]}]}