{"title": [{"text": "UAlacant word-level machine translation quality estimation system at WMT 2015", "labels": [], "entities": [{"text": "UAlacant word-level machine translation quality estimation", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7955080171426138}, {"text": "WMT", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8799507021903992}]}], "abstractContent": [{"text": "This paper describes the Universitat d'Alacant submissions (labelled as UAla-cant) for the machine translation quality estimation (MTQE) shared task in WMT 2015, where we participated in the word-level MTQE sub-task.", "labels": [], "entities": [{"text": "machine translation quality estimation (MTQE) shared task in WMT 2015", "start_pos": 91, "end_pos": 160, "type": "TASK", "confidence": 0.7957679480314255}]}, {"text": "The method we used to produce our submissions uses external sources of bilingual information as a black box to spot sub-segment correspondences between a source segment Sand the translation hypothesis T produced by a machine translation system.", "labels": [], "entities": []}, {"text": "This is done by segmenting both Sand T into overlapping sub-segments of variable length and translating them in both translation directions, using the available sources of bilingual information on the fly.", "labels": [], "entities": []}, {"text": "For our submissions, two sources of bilingual information were used: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.6964070945978165}]}, {"text": "After obtaining the sub-segment correspondences, a collection of features is extracted from them, which are then used by a binary classifer to obtain the final \"GOOD\" or \"BAD\" word-level quality labels.", "labels": [], "entities": [{"text": "GOOD", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.844081699848175}, {"text": "BAD", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.7496575117111206}]}, {"text": "We prepared two submissions for this year's edition of WMT 2015: one using the features produced by our system, and one combining them with the baseline features published by the organisers of the task, which were ranked third and first for the sub-task, respectively.", "labels": [], "entities": [{"text": "WMT 2015", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.6876386106014252}]}], "introductionContent": [{"text": "Machine translation (MT) post-editing is nowadays an indispensable step that allows to use machine translation for dissemination.", "labels": [], "entities": [{"text": "Machine translation (MT) post-editing", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8771040141582489}]}, {"text": "Consequently, MT quality estimation (MTQE) () has emerged as a mean to minimise the post-editing effort by developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system.", "labels": [], "entities": [{"text": "MT quality estimation (MTQE)", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.8002644777297974}]}, {"text": "In order to boost the scientific efforts on this problem, the WMT 2015 MTQE shared task proposes three tasks that allow to compare different approaches at three different levels: segment-level (sub-task 1), word-level (sub-task 2), and document-level (sub-task 3).", "labels": [], "entities": [{"text": "WMT 2015 MTQE shared task", "start_pos": 62, "end_pos": 87, "type": "DATASET", "confidence": 0.8231231689453125}]}, {"text": "Our submissions tackle the word-level MTQE sub-task, which proposes a framework for evaluating and comparing different approaches.", "labels": [], "entities": []}, {"text": "This year, the sub-task used a dataset obtained by translating segments in English into Spanish using MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "DATASET", "confidence": 0.6822183132171631}]}, {"text": "The task consists in identifying which words in the translation hypothesis had to be post-edited and which of them had to be kept unedited by applying the labels \"BAD\" and \"GOOD\", respectively.", "labels": [], "entities": [{"text": "BAD", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9945447444915771}, {"text": "GOOD", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.8955336809158325}]}, {"text": "In this paper we describe the approach behind the two submissions of the Universitat d'Alacant team to this sub-task.", "labels": [], "entities": []}, {"text": "For our submissions we applied the approach proposed byEsp\u00ec a-, who use black-box bilingual resources from the Internet for word-level MTQE.", "labels": [], "entities": [{"text": "MTQE", "start_pos": 135, "end_pos": 139, "type": "TASK", "confidence": 0.7981570959091187}]}, {"text": "In particular, we combined two on-line MT systems, Apertium 1 and Google Translate, 2 and the bilingual concordancer Reverso Context to spot sub-segment correspondences between a sentence S in the source language (SL) and a given translation hypothesis T in the target language (TL).", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9691904783248901}]}, {"text": "To do so, both Sand T are segmented into all possible overlapping sub-segments up to a certain length and translated into the TL and the SL, respectively, by means of the sources of bilingual information mentioned above.", "labels": [], "entities": []}, {"text": "These sub-segment correspondences are used to extract a collection of features that is then used by a binary classifier to determine the final word-level MTQE labels.", "labels": [], "entities": []}, {"text": "One of the novelties of the task this year is that the organisation provided a collection of baseline features for the dataset published.", "labels": [], "entities": []}, {"text": "Therefore, we submitted two systems: one using only the features defined byEsp\u00ec a-, and another combining them with the baseline features published by the organisers of the shared task.", "labels": [], "entities": []}, {"text": "The results obtained by our submissions were ranked third and first, respectively.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the approach used to produce our submissions.", "labels": [], "entities": []}, {"text": "Section 3 describes the experimental setting and the results obtained.", "labels": [], "entities": []}, {"text": "The paper ends with some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the dataset provided for the word-level MTQE sub-task and the results obtained by our method on these datasest.", "labels": [], "entities": []}, {"text": "This year, the task consisted in measuring the word-level MTQE on a collection of segments in Spanish that had been obtained through machine translation from English.", "labels": [], "entities": [{"text": "MTQE", "start_pos": 58, "end_pos": 62, "type": "TASK", "confidence": 0.7463707327842712}]}, {"text": "As already mentioned, two configurations of our system were submitted: one using only the features defined in Section 2, and one combining them with the baseline features.", "labels": [], "entities": []}, {"text": "In order to obtain our features we used two sources of bilingual information, as already mentioned: MT and a bilingual concordancer.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "DATASET", "confidence": 0.4725216031074524}]}, {"text": "As explained above, for our experiments we used two MT systems which are freely available on the Internet: Apertium and Google Translate.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9521669745445251}]}, {"text": "The bilingual concordancer Reverso Context was also used for translating sub-segments.", "labels": [], "entities": [{"text": "translating sub-segments", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.8815656006336212}]}, {"text": "Actually, only the sub-sentential translation memory of this system was used, which provides the collection of TL translation alternatives fora given SL subsegment, together with the number of occurrences of the sub-segments pair in the translation memory.", "labels": [], "entities": []}, {"text": "shows the results obtained by our system, both on the development set during the training phase and on the test set.", "labels": [], "entities": []}, {"text": "The table also includes the results for the baseline system as published by the organisers of the shared task, which uses the baseline features provided by them and a standard logistic regression binary classifier.", "labels": [], "entities": []}, {"text": "As can be seen in, the results obtained on the development set and the test set are quite similar and coherent, which highlights the robustness of the approach.", "labels": [], "entities": []}, {"text": "The results obtained clearly outperform the baseline on the main evaluation metric (: Results of the two systems submitted to the WMT 2015 sub-task on word-level MTQE: the one using only sources of bilingual information (SBI) and the one combining these sources of information with the baseline features (SBI+baseline).", "labels": [], "entities": [{"text": "WMT 2015 sub-task", "start_pos": 130, "end_pos": 147, "type": "DATASET", "confidence": 0.8121604522069296}, {"text": "MTQE", "start_pos": 162, "end_pos": 166, "type": "TASK", "confidence": 0.575398862361908}]}, {"text": "The table also includes the results of the baseline system proposed by the organisation; in this case only the F1 scores are provided because, at the time of writing this paper, the rest of metrics remain unpublished.", "labels": [], "entities": [{"text": "F1", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9993870258331299}]}], "tableCaptions": [{"text": " Table 1: Results of the two systems submitted to the WMT 2015 sub-task on word-level MTQE: the one using only sources of  bilingual information (SBI) and the one combining these sources of information with the baseline features (SBI+baseline). The  table also includes the results of the baseline system proposed by the organisation; in this case only the F1 scores are provided  because, at the time of writing this paper, the rest of metrics remain unpublished.", "labels": [], "entities": [{"text": "WMT 2015 sub-task", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.7832242647806803}, {"text": "MTQE", "start_pos": 86, "end_pos": 90, "type": "TASK", "confidence": 0.596031129360199}, {"text": "F1", "start_pos": 357, "end_pos": 359, "type": "METRIC", "confidence": 0.9986622333526611}]}]}