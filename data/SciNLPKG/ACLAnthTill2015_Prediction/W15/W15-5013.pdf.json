{"title": [{"text": "Source Phrase Segmentation and Translation for Japanese-English Translation Using Dependency Structure", "labels": [], "entities": [{"text": "Phrase Segmentation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7507197260856628}, {"text": "Japanese-English Translation", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6469962745904922}]}], "abstractContent": [{"text": "There are various approaches to statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.8459471066792806}]}, {"text": "In particular, phrase-based SMT (PBSMT) is used as a de facto standard for many language pairs because it works robustly across languages and it is easy to implement.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.5372481793165207}]}, {"text": "However, the results of PBSMT can include ungram-matical sentences, since it typically does not take syntactic structure into account.", "labels": [], "entities": []}, {"text": "To overcome this problem, we propose a linguistically motivated approach based on segmenting a source phrase using a dependency structure and translating each phrase with PBSMT.", "labels": [], "entities": []}, {"text": "This paper presents the results of our method on Japanese-English translation and discusses potential improvements.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6555986106395721}]}], "introductionContent": [{"text": "It is difficult for statistical machine translation (SMT) to perform translation between languages such as Japanese and English, which have a systematic difference in their word orders: typically, Japanese is a subject-object-verb (SOV) language, whereas English is a subject-object-verb (SVO) language.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.7810681561628977}]}, {"text": "Although PBSMT is used as a de facto standard for many language pairs because it works robustly across languages and it is easy to implement; it typically does not take syntactic structure into account.", "labels": [], "entities": []}, {"text": "It is difficult to recognize syntactic information for phrase-based SMT therefore it cannot handle long-distance reordering that frequently occurs in these language pairs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.7466561198234558}]}, {"text": "To incorporate syntactic information into the PBSMT framework, we attempt to identify the SOV of the source language (Japanese) and then correctly produce the SVO of the target language (English).", "labels": [], "entities": []}, {"text": "Concretely, we devise a dependencybased method that extracts a sentence's frame (hareafter \"basic frame\"), consisting of the predicate and its direct children (hereafter \"anchor words\"), and its dependent phrases consisting of the anchor words and their all descendants.", "labels": [], "entities": []}, {"text": "After extracting these words and phrases, our method translates them separately and then combines their translation.", "labels": [], "entities": []}, {"text": "We conducted an experiment with the proposed method on a Japanese-to-English task at the Second Workshop on Asian Translation (.", "labels": [], "entities": [{"text": "Second Workshop on Asian Translation", "start_pos": 89, "end_pos": 125, "type": "TASK", "confidence": 0.5776670694351196}]}, {"text": "Although the results of our method are not positive, we discuss potential improvements.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we discuss related work.", "labels": [], "entities": []}, {"text": "In Section 3, the details of our method are explained.", "labels": [], "entities": []}, {"text": "Then, we describe our experiments and analyze the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use three million parallel sentences from the Asian Scientific Paper Excerpt Corpus.", "labels": [], "entities": [{"text": "Asian Scientific Paper Excerpt Corpus", "start_pos": 49, "end_pos": 86, "type": "DATASET", "confidence": 0.9368101954460144}]}, {"text": "We   use JUMAN (version 7.0) for segmentation, and GIZA++ (version 1.0.7) for alignment.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9834098815917969}, {"text": "alignment", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.9704524278640747}]}, {"text": "We use Moses (version 2.1.1)'s default configurations: monotone, swap, and discontinuous.", "labels": [], "entities": []}, {"text": "The language and translation models of Moses are trained with the ASPEC.", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.7950162291526794}]}, {"text": "In translating the basic frame and dependent phrases, we use the same language and translation models.", "labels": [], "entities": []}, {"text": "MERT is performed on the full dev-set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.49728381633758545}]}, {"text": "We follow the split of dev-set and test-set provided by the organizer of the workshop.", "labels": [], "entities": []}, {"text": "To preprocess the input sentences, basic frames and dependent phrases are extracted by a dependency parser CaboCha (version 0.68).", "labels": [], "entities": []}, {"text": "We use Moses as a baseline.", "labels": [], "entities": []}, {"text": "Moses's settings are the same as the above settings.", "labels": [], "entities": []}, {"text": "Our method is evaluated using Bilingual Evaluation Understudy (BLEU) and Rank-based Intuitive Bilingual Evaluation Score (RIBES).", "labels": [], "entities": [{"text": "Bilingual Evaluation Understudy (BLEU)", "start_pos": 30, "end_pos": 68, "type": "METRIC", "confidence": 0.7115094115336736}, {"text": "Rank-based Intuitive Bilingual Evaluation Score (RIBES)", "start_pos": 73, "end_pos": 128, "type": "METRIC", "confidence": 0.8232347406446934}]}, {"text": "reports our official evaluation results for the WAT 2015 and an additional experiment after the official evaluation campaign.", "labels": [], "entities": [{"text": "WAT 2015", "start_pos": 48, "end_pos": 56, "type": "TASK", "confidence": 0.5620818734169006}]}, {"text": "Both BLEU and RIBES deviated from the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.997767448425293}, {"text": "RIBES", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9875562787055969}]}], "tableCaptions": [{"text": " Table 2: BLEU and RIBES of the baseline and  our methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994316697120667}, {"text": "RIBES", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9969282746315002}]}, {"text": " Table 3: Types of errors in the first 100 sentences  of the test-set.", "labels": [], "entities": []}]}