{"title": [{"text": "Efficiency in Ambiguity: Two Models of Probabilistic Semantics for Natural Language", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper explores theoretical issues in constructing an adequate probabilistic semantics for natural language.", "labels": [], "entities": []}, {"text": "The first extends Montague Semantics with a probability distribution over models.", "labels": [], "entities": []}, {"text": "It has nice theoretical properties, but does not account for the ubiquitous nature of ambiguity; moreover inference is NP-hard.", "labels": [], "entities": []}, {"text": "An alternative approach is described in which a sequence of pairs of sentences and truth values is generated randomly.", "labels": [], "entities": []}, {"text": "By sacrificing some of the nice theoretical properties of the first approach it is possible to model ambiguity naturally; moreover inference now has polynomial time complexity.", "labels": [], "entities": []}, {"text": "Both approaches provide a compositional semantics and account for the gradience of semantic judgements of belief and inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper explores theoretical issues in developing an expressive and computationally tractable, probabilistic semantics for natural language.", "labels": [], "entities": []}, {"text": "Our general approach is situated within the formal, compositional semantics developed by Richard Montague, which is augmented to allow for probabilistic judgements about truth and inference.", "labels": [], "entities": []}, {"text": "The present work takes as a point of departure a number of key assumptions.", "labels": [], "entities": []}, {"text": "First, an adequate semantics should provide an account of both lexical and phrasal (i.e. compositional) meaning.", "labels": [], "entities": []}, {"text": "Second, it should provide for judgements about degrees of belief.", "labels": [], "entities": []}, {"text": "That is, a semantics should account for beliefs that statements are more or less likely to be true, or that one statement may entail another to a certain degree.", "labels": [], "entities": []}, {"text": "Third, an adequate computational semantics should support effective procedures for learning semantic representations and for inference.", "labels": [], "entities": []}, {"text": "Vector space models of meaning have become a popular approach to computational semantics.", "labels": [], "entities": []}, {"text": "Distributional models represent word meanings as vectors of corpus-based distributional contexts and have been successfully applied to a wide variety of tasks, including, inter alia, word sense induction and disambiguation (, textual entailment (), coreference resolution () and taxonomy induction.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 183, "end_pos": 203, "type": "TASK", "confidence": 0.7017371853192648}, {"text": "coreference resolution", "start_pos": 249, "end_pos": 271, "type": "TASK", "confidence": 0.9404236674308777}, {"text": "taxonomy induction", "start_pos": 279, "end_pos": 297, "type": "TASK", "confidence": 0.8790608644485474}]}, {"text": "The success of vector-space models is due to several factors.", "labels": [], "entities": []}, {"text": "They support fine-grained judgements of similarity, allowing us to account for semantic gradience, for example, that the lexeme pear is more similar to banana than to, say, cat.", "labels": [], "entities": []}, {"text": "Moreover, distributional vectors can be learnt in an unsupervised fashion from corpus data, either by counting occurrences of distributional contexts fora word or phrase, or by performing more sophisticated analysis on the data ().", "labels": [], "entities": []}, {"text": "Vector-based approaches differ in many regards from compositional, model-theoretic treatments of meaning such as Montague semantics or Discourse Representation Theory (.", "labels": [], "entities": []}, {"text": "It has proved challenging to extend vector space models to account for the way in which meanings maybe composed and to support inference.", "labels": [], "entities": []}, {"text": "The problem of developing a fully compositional, distributional semantics has recently become a very active area of research and efforts made to find a theoretical foundation).", "labels": [], "entities": []}, {"text": "Researchers have also begun to address the problem of combining the strengths of both the distributional and model-theoretic approaches.", "labels": [], "entities": []}, {"text": "This paper considers an alternative strategy for developing a computational semantics.", "labels": [], "entities": []}, {"text": "Starting with a compositional, model-theoretic semantics, this is augmented with ideas drawn from probabilistic semantics.", "labels": [], "entities": []}, {"text": "Probabilistic semantics provides a rich seam of ideas that can be applied to the construction of a compositional semantics with desirable properties such as gradience and learnability.", "labels": [], "entities": []}, {"text": "Below, we explore two different ways in which this might be achieved.", "labels": [], "entities": []}, {"text": "Our objective is to map out some of the territory, to consider issues of representational adequacy and computational complexity and to provide useful guidance for others venturing into this landscape.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Four possible models describing rela- tionships between John and Mary.", "labels": [], "entities": []}, {"text": " Table 4: Learnt probabilities ob- tained using the Stochastic Se- mantics implementation.", "labels": [], "entities": []}]}