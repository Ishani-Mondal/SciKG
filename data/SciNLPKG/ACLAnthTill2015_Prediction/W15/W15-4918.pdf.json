{"title": [{"text": "Evaluating machine translation for assimilation via a gap-filling task", "labels": [], "entities": [{"text": "Evaluating machine translation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8282881379127502}]}], "abstractContent": [{"text": "This paper provides additional observations on the viability of a strategy independently proposed in 2012 and 2013 for evaluation of machine translation (MT) for assimilation purposes.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.8416356801986694}]}, {"text": "The strategy involves human evaluators, who are asked to restore keywords (to fill gaps) in reference translations.", "labels": [], "entities": []}, {"text": "The evaluation method is applied to two language pairs, Basque-Spanish and Tatar-Russian.", "labels": [], "entities": []}, {"text": "To reduce the amount of time required to prepare tasks and analyse results, an open-source task management system is introduced.", "labels": [], "entities": []}, {"text": "The evaluation results show that the gap-filling task maybe suitable for measuring MT quality for assimilation purposes.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9846850633621216}]}], "introductionContent": [{"text": "As suggested by, modern machine translation (MT) systems maybe divided into two broad categories according to their purpose: post-editing and assimilation systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.8638556003570557}]}, {"text": "The output of the former is intended to be transformed into text comparable to human translation; the latter systems' goal is to enhance user's comprehension of text.", "labels": [], "entities": []}, {"text": "Both kinds maybe evaluated, either to control for quality in the development processor to compare the systems.", "labels": [], "entities": []}, {"text": "Importantly, according to, the evaluation methods must closely consider the system's primary purpose.", "labels": [], "entities": []}, {"text": "Despite the fact that, as a result of widespread usage of online MT, assimilation (or gisting) is currently the most frequent application of MT (in 2012, daily output of Google Translate matched the yearly output of human translations 1 ), few methodologies are established for assimilation evaluation of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9132989645004272}, {"text": "MT", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.9284049868583679}, {"text": "MT", "start_pos": 305, "end_pos": 307, "type": "TASK", "confidence": 0.9444060325622559}]}, {"text": "The methods include post-editing and comparison by bilingual experts, and multiple choice tests ().", "labels": [], "entities": []}, {"text": "These approaches are often costly and prone to subjectivity: seethe discussion by O'.", "labels": [], "entities": [{"text": "O'", "start_pos": 82, "end_pos": 84, "type": "DATASET", "confidence": 0.8977101445198059}]}, {"text": "As an alternative, the modification of cloze testing was introduced for assimilation evaluation, first by as a supplementary technique, and then by O' as a stand-alone method.", "labels": [], "entities": [{"text": "assimilation evaluation", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.9055896103382111}]}, {"text": "Prior to this, cloze tests have been used to evaluate raw MT quality;).", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9738203287124634}]}, {"text": "While these authors ask informants to fill gaps in MT output, and O' ask informants to fill gaps in the reference (human) translation.", "labels": [], "entities": [{"text": "MT output", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.8876552581787109}, {"text": "O", "start_pos": 66, "end_pos": 67, "type": "METRIC", "confidence": 0.9335325956344604}]}, {"text": "A designated number of keywords is removed from the human-translated sentences.", "labels": [], "entities": []}, {"text": "The evaluators are then asked to fill the gaps with suitable words with and without the help of MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9359400868415833}]}, {"text": "The gap-filling task models how well users comprehend the key points of the text, as it is roughly equivalent with answering questions.", "labels": [], "entities": []}, {"text": "Thus, the method does not directly evaluate the quality of machine-produced text, but rather its usefulness in understanding the meaning of the original text.", "labels": [], "entities": []}, {"text": "The gap-filling method has been successfully used to evaluate the Basque-English Apertium language pair.", "labels": [], "entities": [{"text": "Basque-English Apertium language pair", "start_pos": 66, "end_pos": 103, "type": "DATASET", "confidence": 0.8278923332691193}]}, {"text": "In this work we extend the evalua-tion to two more language pairs: Basque-Spanish and Tatar-Russian.", "labels": [], "entities": []}, {"text": "The former pair, while not producing output suitable for post-editing, is a good example of an assimilation MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.79367995262146}]}, {"text": "In addition, Basque and Spanish are not mutually understandable, and therefore constitute a good pair for evaluation.", "labels": [], "entities": []}, {"text": "For the latter pair, the evaluation served as a quality check in the period of active development during the Google Summer of Code 2014 programme.", "labels": [], "entities": [{"text": "Google Summer of Code 2014 programme", "start_pos": 109, "end_pos": 145, "type": "DATASET", "confidence": 0.7499845325946808}]}, {"text": "In addition to evaluating, we explore the previously unconsidered aspects of the experiment: the correlation between evaluators' scores, and the effects of the linguistic domain of texts and the percentage of gaps in a sentence.", "labels": [], "entities": []}, {"text": "To facilitate the evaluation, we introduce an automated system which creates task sets from parallel corpora given a range of parameters (number of gaps in a sentence, hint type, gap filler, etc.), checks evaluators' answers, and calculates and reports generalized results.", "labels": [], "entities": []}, {"text": "This system is integrated into the Appraise MT evaluation platform; the code is open-source and is available on GitHub.", "labels": [], "entities": [{"text": "Appraise MT evaluation", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.5655924280484518}]}, {"text": "We anticipate that the assessed MT systems will contribute to the users' understanding of text, that is, the users will show better results in gap-filling tasks when assisted with MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9823029637336731}, {"text": "MT", "start_pos": 180, "end_pos": 182, "type": "TASK", "confidence": 0.9847334027290344}]}, {"text": "We also expect to see different results depending on text domain and the relative number of gaps in a sentence.", "labels": [], "entities": []}, {"text": "The paper is organised as follows: in section 2 we describe the gap-filling method for assimilation evaluation: the task layout, the choice of words, and how the tasks are generated.", "labels": [], "entities": [{"text": "assimilation evaluation", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.9005282819271088}]}, {"text": "Section 3 introduces the experimental material, the evaluators, the distribution of tasks and the evaluation procedure.", "labels": [], "entities": []}, {"text": "In section 4 we describe and discuss the experiment results.", "labels": [], "entities": []}, {"text": "Finally, section 5 draws some conclusions.", "labels": [], "entities": []}, {"text": "This paper is concerned primarily with assimilation evaluation; fora deeper discussion on evaluation see e.g.).", "labels": [], "entities": [{"text": "assimilation evaluation", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.9545843303203583}]}], "datasetContent": [{"text": "In this section we will discuss the evaluators, the evaluation procedure, and the tasks in more detail.", "labels": [], "entities": []}, {"text": "For each experiment we called for native speakers of target language of the language pair (i.e. Spanish and Russian) who had no command of source language of the pair (Basque and Tatar, respectively).", "labels": [], "entities": []}, {"text": "The knowledge was self-reported, and the participants were not asked about any other languages they may know.", "labels": [], "entities": []}, {"text": "Eleven evaluators participated in the Basque-Spanish experiment, and 28 in Tatar-Russian (although not everyone completed the task in full, see discussion).", "labels": [], "entities": []}, {"text": "The majority of Russian participants were aged 20-25, with university degrees or in the process of obtaining them.", "labels": [], "entities": []}, {"text": "Although we have not asked the participants about their knowledge of languages other than Tatar and Russian, it is reasonable to assume that most Russian participants knew English to some extent.", "labels": [], "entities": []}, {"text": "The Spanish participants were university staff with background in computer science.", "labels": [], "entities": []}, {"text": "By design, our gap-filling tasks require a human translation (reference) of source sentences.", "labels": [], "entities": []}, {"text": "Calling fora human translator, however, would significantly increase the resources needed for evaluation.", "labels": [], "entities": []}, {"text": "We therefore use parallel text sources, which provide the same sentence in two languages simultaneously: 1.", "labels": [], "entities": []}, {"text": "For Basque-Spanish, from the corpus of legal texts \"Memorias de traducci\u00f3n del Servicio Oficial de Traductores del IVAP\"; 3 2.", "labels": [], "entities": []}, {"text": "For Tatar-Russian, from the following sources on three different topics: (a) Casual conversations, from a textbook 4 of spoken Tatar; (b) Legal texts, from the Constitution and laws 5 of Tatarstan; (c) News, from the President of Tatarstan website 6 . Each set features 36 pairs of sentences.", "labels": [], "entities": []}, {"text": "For the Basque-Spanish experiment the pairs were drawn randomly from the corpora; for Tatar-Russian, compiled by hand by the developer of the language pair in Apertium.", "labels": [], "entities": [{"text": "Apertium", "start_pos": 159, "end_pos": 167, "type": "DATASET", "confidence": 0.9071387648582458}]}, {"text": "The Basque-Spanish experiment featured 94, 181 and 272 gaps in the 10, 20 and 30 % tasks, respectively.", "labels": [], "entities": []}, {"text": "For Tatar-Russian these numbers are 272, 396 and 724, due to longer sentences used in task creation.", "labels": [], "entities": [{"text": "task creation", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.7063655108213425}]}], "tableCaptions": [{"text": " Table 4: Krippendorff Alpha measure of annotator agreement, for each language pair in all four task modes.", "labels": [], "entities": [{"text": "Alpha measure of annotator agreement", "start_pos": 23, "end_pos": 59, "type": "METRIC", "confidence": 0.897756814956665}]}, {"text": " Table 5: Tatar-Russian Average number of gaps successfully filled (%), using a synonym list, for three different domains, in  all four task modes.", "labels": [], "entities": [{"text": "Average number of gaps successfully", "start_pos": 24, "end_pos": 59, "type": "METRIC", "confidence": 0.8589699506759644}]}]}