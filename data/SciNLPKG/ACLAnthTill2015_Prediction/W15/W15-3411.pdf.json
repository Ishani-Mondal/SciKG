{"title": [{"text": "BUCC Shared Task: Cross-Language Document Similarity", "labels": [], "entities": [{"text": "BUCC Shared Task", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5565381447474161}, {"text": "Cross-Language Document Similarity", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.8108596801757812}]}], "abstractContent": [{"text": "We summarise the organisation and results of the first shared task aimed at detecting the most similar texts in a large multilingual collection.", "labels": [], "entities": []}, {"text": "The dataset of the shared was based on Wikipedia dumps with inter-language links with further filtering to ensure comparability of the paired articles.", "labels": [], "entities": []}, {"text": "The eleven system runs we received have been evaluated using the TREC evaluation metrics.", "labels": [], "entities": [{"text": "TREC evaluation metrics", "start_pos": 65, "end_pos": 88, "type": "DATASET", "confidence": 0.6250287691752116}]}], "introductionContent": [], "datasetContent": [{"text": "Evaluation has been done using standard TREC evaluation measures, modeling the task as the retrieval of a ranked list of links from a source page.", "labels": [], "entities": []}, {"text": "Extrinsic evaluation setups, for example, via terminology extraction, would possibly provide more interesting measures, but this would require a baseline system which works with all the languages in question.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8262502253055573}]}], "tableCaptions": [{"text": " Table 1: Ratios of lengths of aligned articles to English", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results for French and Chinese. lina.p corresponds to Pigeonhole, lina.cl to  Cross-lingual in the authors' paper.", "labels": [], "entities": [{"text": "Pigeonhole", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9460024833679199}]}]}