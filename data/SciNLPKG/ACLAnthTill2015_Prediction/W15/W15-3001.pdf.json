{"title": [{"text": "Findings of the 2015 Workshop on Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7542034784952799}]}], "abstractContent": [{"text": "This paper presents the results of the WMT15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for run-time estimation of machine translation quality, and an automatic post-editing task.", "labels": [], "entities": [{"text": "WMT15 shared tasks", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.5228057603041331}, {"text": "news translation task", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.733079344034195}, {"text": "run-time estimation of machine translation quality", "start_pos": 150, "end_pos": 200, "type": "TASK", "confidence": 0.6054455240567526}]}, {"text": "This year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7157085984945297}, {"text": "standard translation task", "start_pos": 119, "end_pos": 144, "type": "TASK", "confidence": 0.6930334866046906}]}, {"text": "An additional 7 anonymized systems were included, and were then evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries.", "labels": [], "entities": [{"text": "quality estimation task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7190835575262705}]}, {"text": "The pilot automatic post-editing task had a total of 4 teams, submitting 7 entries.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT) held at EMNLP 2015", "start_pos": 62, "end_pos": 118, "type": "TASK", "confidence": 0.8085473477840424}]}, {"text": "This workshop builds on eight previous WMT workshops ().", "labels": [], "entities": [{"text": "WMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.70322585105896}]}, {"text": "This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task 1 , and a automatic postediting task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.9180617332458496}]}, {"text": "In the translation task ( \u00a72), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data.", "labels": [], "entities": [{"text": "translation task", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.9291570782661438}]}, {"text": "We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8933379650115967}]}, {"text": "The Finnish translation tasks were new this year, providing a lesser resourced data condition on a challenging language pair.", "labels": [], "entities": [{"text": "Finnish translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7301350235939026}]}, {"text": "The system outputs for each task were evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "The human evaluation ( \u00a73) involves asking human judges to rank sentences output by anonymized systems.", "labels": [], "entities": []}, {"text": "We obtained large numbers of rankings from researchers who contributed evaluations proportional to the number of tasks they entered.", "labels": [], "entities": []}, {"text": "We made data collection more efficient and used TrueSkill as ranking method.", "labels": [], "entities": [{"text": "data collection", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7625780999660492}]}, {"text": "The quality estimation task ( \u00a74) this year included three subtasks: sentence-level prediction of post-editing effort scores, word-level prediction of good/bad labels, and document-level prediction of Meteor scores.", "labels": [], "entities": [{"text": "sentence-level prediction of post-editing effort", "start_pos": 69, "end_pos": 117, "type": "TASK", "confidence": 0.7248835504055023}, {"text": "word-level prediction of good/bad labels", "start_pos": 126, "end_pos": 166, "type": "TASK", "confidence": 0.8338951383318219}, {"text": "document-level prediction of Meteor", "start_pos": 172, "end_pos": 207, "type": "TASK", "confidence": 0.6845006793737411}]}, {"text": "Datasets were released with English\u2192Spanish news translations for sentence and word level, English\u2194German news translations for document level.", "labels": [], "entities": []}, {"text": "The first round of the automatic post-editing task ( \u00a75) examined automatic methods for correcting errors produced by an unknown machine translation system.", "labels": [], "entities": [{"text": "correcting errors produced by an unknown machine translation", "start_pos": 88, "end_pos": 148, "type": "TASK", "confidence": 0.7334135249257088}]}, {"text": "Participants were provided with training triples containing source, target and human post-editions, and were asked to return automatic post-editions for unseen (source, target) pairs.", "labels": [], "entities": []}, {"text": "This year we focused on correcting English\u2192Spanish news translations.", "labels": [], "entities": [{"text": "correcting English\u2192Spanish news translations", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.9142941832542419}]}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation and estimation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8719261288642883}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8308209776878357}, {"text": "machine translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.8069436550140381}]}, {"text": "As before, all of the data, translations, and collected human judgments are publicly available.", "labels": [], "entities": []}, {"text": "We hope these datasets serve as a valuable resource for research into statistical 2 http://statmt.org/wmt15/results.html 1 machine translation and automatic evaluation or prediction of translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7742563784122467}]}], "datasetContent": [{"text": "Following what we had done for previous workshops, we again conduct a human evaluation campaign to assess translation quality and determine the final ranking of candidate systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9665855169296265}]}, {"text": "This section describes how we prepared the evaluation data, collected human assessments, and computed the final results.", "labels": [], "entities": []}, {"text": "This year's evaluation campaign differed from last year in several ways: \u2022 In previous years each ranking task compared five different candidate systems which were selected without any pruning or redundancy cleanup.", "labels": [], "entities": []}, {"text": "This had resulted in a noticeable amount of near-identical ranking candidates in WMT14, making the evaluation process unnecessarily tedious as annotators ran into a fair amount of ranking tasks containing very similar segments which are hard to inspect.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 81, "end_pos": 86, "type": "DATASET", "confidence": 0.8648055791854858}]}, {"text": "For WMT15, we perform redundancy cleanup as an initial preprocessing step and This is perfectly illustrated by the UKIP numbties banning people with HIV.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6127691268920898}, {"text": "UKIP numbties", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.7586546838283539}]}, {"text": "You mean Nigel Farage saying the NHS should not be used to pay for people coming to the UK as health tourists, and saying yes when the interviewer specifically asked if, with the aforementioned in mind, people with HIV were included in not being welcome.", "labels": [], "entities": []}, {"text": "You raise a straw man and then knock it down with thinly veiled homophobia.", "labels": [], "entities": []}, {"text": "Every time I or my family need to use the NHS we have to queue up behind bigots with a sense of entitlement and chronic hypochondria.", "labels": [], "entities": []}, {"text": "I think the straw man is yours.", "labels": [], "entities": []}, {"text": "Health tourism as defined by the right wing loonies is virtually none existent.", "labels": [], "entities": [{"text": "Health tourism", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6885688155889511}]}, {"text": "I think it's called democracy.", "labels": [], "entities": []}, {"text": "So no one would be affected by UKIP's policies against health tourism so no problem.", "labels": [], "entities": [{"text": "UKIP", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.9493377804756165}]}, {"text": "Only in UKIP La La Land could Carswell be described as revolutionary.", "labels": [], "entities": [{"text": "UKIP La La Land", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9762158542871475}]}, {"text": "Quoting the bollox The Daily Muck spew out is not evidence.", "labels": [], "entities": [{"text": "bollox The Daily Muck spew out", "start_pos": 12, "end_pos": 42, "type": "DATASET", "confidence": 0.9250699778397878}]}, {"text": "Ah, shoot the messenger.", "labels": [], "entities": []}, {"text": "The Mail didn't write the report, it merely commented on it.", "labels": [], "entities": [{"text": "The", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.988853931427002}, {"text": "Mail", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.5217640399932861}]}, {"text": "Whoever controls most of the media in this country should undead be shot for spouting populist propaganda as fact.", "labels": [], "entities": []}, {"text": "I don't think you know what a straw man is.", "labels": [], "entities": []}, {"text": "You also don't know anything about my personal circumstances or identity so I would be very careful about trying to eradicate a debate with accusations of homophobia.", "labels": [], "entities": []}, {"text": "Farage's comment came as quite a shock, but only because it is so rarely addressed.", "labels": [], "entities": []}, {"text": "He did not express any homophobic beliefs whatsoever.", "labels": [], "entities": []}, {"text": "You will just have to find away of getting over it.", "labels": [], "entities": []}, {"text": "I'm not entirely sure what you're trying to say, but my guess is that you dislike the media reporting things you disagree with.", "labels": [], "entities": []}, {"text": "It is so rarely addressed because unlike Fararge and his Thatcherite loony disciples who think aids and floods area signal from the divine and not a reflection on their own ignorance in understanding the complexities of humanity as something to celebrate,then no.", "labels": [], "entities": []}, {"text": "Language Sources (Number of Documents) Czech aktu\u00e1ln\u011b.cz (4), blesk.cz (1), blisty.cz (1), ctk.cz (1), den\u00edk.cz (1), e15.cz (1), iDNES.cz, ihned.cz (3), lidovky.cz (6), Novinky.cz (2), tyden.cz (1).", "labels": [], "entities": []}, {"text": "WMT15 featured the largest evaluation campaign to date.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.964733898639679}]}, {"text": "Similar to last year, we decided to collect researcher-based judgments only.", "labels": [], "entities": []}, {"text": "A total of 137 individual annotator accounts have been actively involved.", "labels": [], "entities": []}, {"text": "Users came from 24 different research groups and contributed judgments on 9,669 HITs.", "labels": [], "entities": []}, {"text": "Overall, these correspond to 29,007 individual ranking tasks (plus some more from incomplete HITs), each of which would have spawned exactly 10 individual \"A > B\" judgments last year, so we expected at least >290,070 binary data points.", "labels": [], "entities": []}, {"text": "Due to our redundancy cleanup, we are able to get a lot more, namely 542,732.", "labels": [], "entities": []}, {"text": "We report our inter/intra-annotator agreement scores based on the actual work done (otherwise, we'd artificially boost scores based on inferred rankings) and use the full set of data to compute clusters (where the inferred rankings contribute meaningful data).", "labels": [], "entities": []}, {"text": "Human annotation effort was exceptional and we are grateful to all participating individuals and teams.", "labels": [], "entities": [{"text": "Human annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6518204212188721}]}, {"text": "We believe that human rankings provide the best decision basis for machine translation evaluation and it is great to see contributions on this large a scale.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.8939634362856547}]}, {"text": "In total, our human annotators spent 32 days and 20 hours working in Appraise.", "labels": [], "entities": [{"text": "Appraise", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.7032709717750549}]}, {"text": "The average annotation time per HIT amounts to 4 minutes 53 seconds.", "labels": [], "entities": [{"text": "HIT", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.6106225252151489}]}, {"text": "Several annotators passed the mark of 100 HITs annotated, some worked for more than 24 hours.", "labels": [], "entities": []}, {"text": "We don't take this enormous amount of effort for granted and will make sure to improve the evaluation platform and overall process for upcoming workshops.", "labels": [], "entities": []}, {"text": "The evaluation of the paragraphlevel task was the same as that for the sentencelevel task.", "labels": [], "entities": []}, {"text": "MAE and RMSE are reported as evaluation metrics for the scoring task, with MAE as official metric for systems ranking.", "labels": [], "entities": [{"text": "MAE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9456396698951721}, {"text": "RMSE", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.8861786723136902}, {"text": "MAE", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.8915322422981262}]}, {"text": "For the ranking task, DeltaAvg and Spearman's \u03c1 correlation are reported, with DeltaAvg as official metric for systems ranking.", "labels": [], "entities": [{"text": "DeltaAvg", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9317591190338135}, {"text": "Spearman's \u03c1 correlation", "start_pos": 35, "end_pos": 59, "type": "METRIC", "confidence": 0.5817671120166779}]}, {"text": "To evaluate the significance of the results, bootstrap resampling (1K times) with 95% confidence intervals was used.", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 45, "end_pos": 65, "type": "METRIC", "confidence": 0.779761016368866}]}, {"text": "Pearson's r correlation scores with the Williams significance test are also reported.", "labels": [], "entities": [{"text": "Pearson's r correlation", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.6292552798986435}, {"text": "Williams significance test", "start_pos": 40, "end_pos": 66, "type": "METRIC", "confidence": 0.6810266971588135}]}, {"text": "summarises the results of the ranking variant of Task 3. 21 They are sorted from best to worst using the DeltaAvg metric scores as primary key and the Spearman's \u03c1 rank correlation scores as secondary key.", "labels": [], "entities": [{"text": "DeltaAvg metric scores", "start_pos": 105, "end_pos": 127, "type": "METRIC", "confidence": 0.7760796348253886}, {"text": "Spearman's \u03c1 rank correlation scores", "start_pos": 151, "end_pos": 187, "type": "METRIC", "confidence": 0.6253553330898285}]}, {"text": "RTM-DCU submissions achieved the best scores: RTM-SVR was the winner for EN-DE, and RTM-FS-SVR for DE-EN.", "labels": [], "entities": [{"text": "RTM-SVR", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.46918272972106934}, {"text": "RTM-FS-SVR", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.7419416904449463}]}, {"text": "For EN-DE, the HIDDEN system did not show significant difference against the baseline.", "labels": [], "entities": [{"text": "EN-DE", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7996159791946411}, {"text": "HIDDEN", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9107431173324585}]}, {"text": "For DE-EN, USHEF/QUEST-DISC-BO, USAAR-USHEF/BFF and HIDDEN were not significantly different from the baseline.", "labels": [], "entities": [{"text": "DE-EN", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.628563642501831}, {"text": "USHEF", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.7773478031158447}, {"text": "USAAR-USHEF", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.7198296785354614}, {"text": "BFF", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.6149397492408752}, {"text": "HIDDEN", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9330689311027527}]}, {"text": "System performance is evaluated by computing the distance between automatic and human post-editions of the machine-translated sentences present in the test set (i.e. for each of the 1,817 target test sentences).", "labels": [], "entities": []}, {"text": "This distance is measured in terms of Translation Error Rate (TER)), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.", "labels": [], "entities": [{"text": "Translation Error Rate (TER))", "start_pos": 38, "end_pos": 67, "type": "METRIC", "confidence": 0.8872048358122507}, {"text": "MT-related tasks", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.9377003312110901}, {"text": "quality estimation)", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7101753056049347}]}, {"text": "Systems are ranked based on the average TER calculated on the test set by using the TERcom 24 software: lower average TER scores correspond to higher ranks.", "labels": [], "entities": [{"text": "TER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9956302642822266}, {"text": "TERcom 24 software", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.8352973461151123}, {"text": "TER", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9922349452972412}]}, {"text": "Each run is evaluated in two modes, namely: i) case insensitive and ii) case sensitive.", "labels": [], "entities": []}, {"text": "Evaluation scripts to compute TER scores in both modalities have been made available to participants through the APE task web page.", "labels": [], "entities": [{"text": "TER", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.8527766466140747}, {"text": "APE task web page", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.752975657582283}]}], "tableCaptions": [{"text": " Table 4: \u03ba scores measuring inter-annotator agreement for WMT15. See Table 5 for corresponding intra-annotator agreement  scores. WMT13r and WMTm refer to researchers' judgments and crowd-sourced judgments obtained using Mechanical Turk,  respectively. WMT14 and WMT15 results are based on researchers' judgments only (hence, comparable to WMT13r).", "labels": [], "entities": [{"text": "WMT15", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.818291187286377}, {"text": "WMT13r", "start_pos": 131, "end_pos": 137, "type": "DATASET", "confidence": 0.8963000774383545}, {"text": "WMTm", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.7944995760917664}, {"text": "WMT14", "start_pos": 254, "end_pos": 259, "type": "DATASET", "confidence": 0.9137970805168152}, {"text": "WMT15", "start_pos": 264, "end_pos": 269, "type": "DATASET", "confidence": 0.778405487537384}, {"text": "WMT13r", "start_pos": 341, "end_pos": 347, "type": "DATASET", "confidence": 0.9425466656684875}]}, {"text": " Table 5: \u03ba scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the  human evaluation campaign. Scores are much higher for WMT15 which makes sense as we enforce annotation consistency  through our initial preprocessing which joins near-identical translation candidates into multi-system entries. It seems that the  focus on actual differences in our annotation tasks as well as the possibility of having \"easier\" ranking scenarios for n < 5  candidate systems results in a higher annotator agreement, both for inter-and intra-annotator agreement scores.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 182, "end_pos": 187, "type": "DATASET", "confidence": 0.7967837452888489}]}, {"text": " Table 6: Official results for the WMT15 translation task. Systems are ordered by their inferred system means, though systems  within a cluster are considered tied. Lines between systems indicate clusters according to bootstrap resampling at p-level  p \u2264 .05. Systems with grey background indicate use of resources that fall outside the constraints provided for the shared task.", "labels": [], "entities": [{"text": "WMT15 translation task", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8330232501029968}]}, {"text": " Table 8: Official results for the ranking variant of the WMT15 quality estimation Task 1. The winning submissions are  indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to pairwise bootstrap  resampling (1K times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system  at a statistically significant level according to the same test.", "labels": [], "entities": [{"text": "WMT15 quality estimation Task", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.7642923444509506}]}, {"text": " Table 9: Official results for the scoring variant of the WMT15 quality estimation Task 1. The winning submissions are indicated  by a \u2022. These are the top-scoring submission and those that are not significantly worse according to bootstrap resampling (1K  times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically  significant level according to the same test.", "labels": [], "entities": [{"text": "WMT15 quality estimation Task", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.7469313591718674}]}, {"text": " Table 10: Alternative results for the scoring variant of the WMT15 quality estimation Task 1. The winning submissions are  indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to Williams test with  95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically significant  level according to the same test.", "labels": [], "entities": [{"text": "WMT15 quality estimation Task", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.7580285519361496}, {"text": "Williams test", "start_pos": 235, "end_pos": 248, "type": "DATASET", "confidence": 0.8877496123313904}]}, {"text": " Table 11: Datasets for Task 2.", "labels": [], "entities": []}, {"text": " Table 12: Official results for the WMT15 quality estimation Task 2. The winning submissions are indicated by a \u2022. These are  the top-scoring submission and those that are not significantly worse according to approximate randomization tests with 95%  confidence intervals. Submissions whose results are statistically different from others according to the same test are grouped  by a horizontal line.", "labels": [], "entities": [{"text": "WMT15 quality estimation Task", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7712059617042542}]}, {"text": " Table 13: Alternative results for the WMT15 quality estimation Task 2 according to the sequence correlation metric. The win- ning submissions are indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according  to approximate randomization tests with 95% confidence intervals. Submissions whose results are statistically different from  others according to the same test are grouped by a horizontal line.", "labels": [], "entities": [{"text": "WMT15 quality estimation Task", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.7696659415960312}]}, {"text": " Table 14: Official results for the ranking variant of the WMT15 quality estimation Task 3. The winning submissions are  indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to bootstrap  resampling (1K times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system  at a statistically significant level according to the same test.", "labels": [], "entities": [{"text": "WMT15 quality estimation Task", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.7549902647733688}]}, {"text": " Table 17: Average metric scores for automatic metrics in the  corpus for Task 3.", "labels": [], "entities": []}, {"text": " Table 20: Official results for the WMT15 Automatic  Post-editing task -average TER (\u2193) case sensitive.", "labels": [], "entities": [{"text": "WMT15 Automatic  Post-editing task", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.5270530581474304}, {"text": "TER (\u2193) case sensitive", "start_pos": 80, "end_pos": 102, "type": "METRIC", "confidence": 0.8772637844085693}]}, {"text": " Table 21: Official results for the WMT15 Automatic  Post-editing task -average TER (\u2193) case insensitive.", "labels": [], "entities": [{"text": "WMT15 Automatic  Post-editing task", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.5289974734187126}, {"text": "TER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.919003427028656}]}, {"text": " Table 22: WMT APe Task and Autodesk training data statis- tics.", "labels": [], "entities": [{"text": "WMT APe Task", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.6173998912175497}]}, {"text": " Table 23: Phrase pair count distribution in two phrase tables  built using the APE 2015 training and the Autodesk dataset.", "labels": [], "entities": [{"text": "Phrase pair count distribution", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.908534437417984}, {"text": "APE 2015 training", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9514091412226359}, {"text": "Autodesk dataset", "start_pos": 106, "end_pos": 122, "type": "DATASET", "confidence": 0.9605384469032288}]}, {"text": " Table 24: Number of test sentences modified, improved and deteriorated by each submitted run, together with the correspond- ing percentage of insertions, deletions, substitutions and shifts (case sensitive).", "labels": [], "entities": [{"text": "correspond- ing percentage of insertions", "start_pos": 113, "end_pos": 153, "type": "METRIC", "confidence": 0.9058018922805786}]}, {"text": " Table 25: Head to head comparison, ignoring ties, for Czech-English systems", "labels": [], "entities": []}, {"text": " Table 26: Head to head comparison, ignoring ties, for English-Czech systems", "labels": [], "entities": []}, {"text": " Table 27: Head to head comparison, ignoring ties, for German-English systems", "labels": [], "entities": []}, {"text": " Table 28: Head to head comparison, ignoring ties, for English-German systems", "labels": [], "entities": []}, {"text": " Table 29: Head to head comparison, ignoring ties, for French-English systems", "labels": [], "entities": []}, {"text": " Table 30: Head to head comparison, ignoring ties, for English-French systems", "labels": [], "entities": []}]}