{"title": [{"text": "Recognition of Distress Calls in Distant Speech Setting: a Preliminary Experiment in a Smart Home", "labels": [], "entities": [{"text": "Recognition of Distress Calls in Distant Speech Setting", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.7677375562489033}]}], "abstractContent": [{"text": "This paper presents a system to recognize distress speech in the home of seniors to provide reassurance and assistance.", "labels": [], "entities": []}, {"text": "The system is aiming at being integrated into a larger system for Ambient Assisted Living (AAL) using only one microphone with a fix position in a non-intimate room.", "labels": [], "entities": [{"text": "Ambient Assisted Living (AAL)", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.621026337146759}]}, {"text": "The paper presents the details of the automatic speech recognition system which must work under distant speech condition and with expressive speech.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7111635208129883}]}, {"text": "Moreover, privacy is ensured by running the decoding on-site and not on a remote server.", "labels": [], "entities": [{"text": "privacy", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9399099349975586}]}, {"text": "Furthermore the system was biased to recognize only set of sentences defined after a user study.", "labels": [], "entities": []}, {"text": "The system has been evaluated in a smart space reproducing atypical living room where 17 participants played scenarios including falls during which they uttered distress calls.", "labels": [], "entities": []}, {"text": "The results showed a promising error rate of 29% while emphasizing the challenges of the task.", "labels": [], "entities": [{"text": "error rate", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9927029013633728}]}], "introductionContent": [{"text": "Life expectancy has increased in all countries of the European Union in the last decade.", "labels": [], "entities": []}, {"text": "Therefore the part of the people who are at least 75 years old has strongly increased and solutions are needed to satisfy the wishes of elderly people to live as long as possible in their own homes.", "labels": [], "entities": []}, {"text": "Ageing can cause functional limitations that -if not compensated by technical assistance or environmental management-lead to activity restriction.", "labels": [], "entities": [{"text": "activity restriction", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.675897479057312}]}, {"text": "Smart homes area promising way to help elderly people to live independently at their own home, they are housings equipped with sensors and actuators[4][1].", "labels": [], "entities": []}, {"text": "Another aspect is the increasing risk of distress, among which falling is one of the main fear and lethal risk, but also blocking hip or fainting.", "labels": [], "entities": [{"text": "hip", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9754239320755005}]}, {"text": "The most common solution is the use of kinematic sensors worn by the person but this imposes some constraints in the everyday life and worn sensors are not always a good solution because some persons can forget or refuse to wear it.", "labels": [], "entities": []}, {"text": "Nowadays, one of the best suited interfaces is the voice-user interface (VUI), whose technology has reached maturity and is avoiding the use of worn sensors thanks to microphones setup in the home and allowing hands-free and distant interaction.", "labels": [], "entities": []}, {"text": "It was demonstrated that VUI is useful for system integrating speech commands.", "labels": [], "entities": [{"text": "VUI", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.7013735771179199}]}, {"text": "The use of speech technologies in home environment requires to address particular challenges due to this specific environment.", "labels": [], "entities": []}, {"text": "There is a rising number of smart home projects considering speech processing in their design.", "labels": [], "entities": []}, {"text": "They are related to wheelchair command, vocal command for people with dysarthria, companion robot, vocal control of appliances and devices.", "labels": [], "entities": []}, {"text": "Due to the experimental constraints, few systems were validated with real users in realistic situation condition like in the SWEET-HOME project during which a dedicated voice based home automation system was able to drive a smart home thanks to vocal commands with typical people and with elderly and visually impaired people.", "labels": [], "entities": []}, {"text": "In this paper we present an approach to provide assistance in a smart home for seniors in case of distress situation in which they can't move but can talk.", "labels": [], "entities": []}, {"text": "The challenge is due to expressive speech which is different from standard speech: is it possible to use state of the art ASR techniques to recognize expressive speech?", "labels": [], "entities": [{"text": "ASR", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.979181170463562}]}, {"text": "In our approach, we address the problem by using the microphone of a home automation and social system placed in the living room with ASR decoding and voice call matching.", "labels": [], "entities": []}, {"text": "In this way, the user must be able to command the environment without having to wear a specific device for fall detection or for physical interaction (e.g., a remote control too far from the user when needed).", "labels": [], "entities": [{"text": "fall detection", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.7172262966632843}]}, {"text": "Though microphones in a home is areal breach of privacy, by contrast to current smart-phones, we address the problem using an in-home ASR engine rather than a cloud based one (private conversations do not go outside the home).", "labels": [], "entities": []}, {"text": "Moreover, the limited vocabulary ensures that only speech relevant to the command of the home is correctly decoded.", "labels": [], "entities": []}, {"text": "Finally, another strength of the approach is to have been evaluated in realistic conditions.", "labels": [], "entities": []}, {"text": "The paper is organised as follow.", "labels": [], "entities": []}, {"text": "Section 2 presents the method for speech acquisition and recognition in the home.", "labels": [], "entities": [{"text": "speech acquisition and recognition", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.7583342343568802}]}, {"text": "Section 3, presents the experimentation and the results which are discussed in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "An experiment was run in the experimental platform of the LIG laboratory in a room whose setting corresponds to and equipped with a sofa, a carpet, 2 chairs, a table and e-lio.", "labels": [], "entities": [{"text": "LIG laboratory", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.8648825585842133}]}, {"text": "A Sennheiser SKM 300 G2 ME2 omnidirectional microphone was set on the cupboard.", "labels": [], "entities": []}, {"text": "In these conditions, the microphone was at a distance of above 2 meters from the speaker (Distant speech conditions).", "labels": [], "entities": []}, {"text": "The audio analysis system consisted in the CIR-DOX software presented in Section 2 which was continuously recording and analysing the audio streams to detect the calls.", "labels": [], "entities": []}, {"text": "The scenarios were elaborated after field studies made by the GRePS laboratory.", "labels": [], "entities": [{"text": "GRePS laboratory", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9470408856868744}]}, {"text": "These studies allowed to specify the fall context, the movements during the fall as well as the person's reaction once on the floor.", "labels": [], "entities": []}, {"text": "Phrases uttered during and after the fall were also identified \"Blast!", "labels": [], "entities": [{"text": "Blast", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9936527013778687}]}, {"text": "What's happening to me?", "labels": [], "entities": []}, {"text": "Oh shit, shit!\".", "labels": [], "entities": []}, {"text": "The protocol was as follows.", "labels": [], "entities": []}, {"text": "Each participant was introduced to the context of the research and was invited to sign a consent form.", "labels": [], "entities": []}, {"text": "The participants played four scenarios of fall, one blocked hip scenario and two other scenarios called \"true-false\" added to challenge the automatic detection of falls by the video analysis system.", "labels": [], "entities": []}, {"text": "If the participant's age was under 60, he wore a simulator which hampered his mobility and reduced his vision and hearing to simulate aged physical conditions.", "labels": [], "entities": []}, {"text": "shows a young participant wearing the simulator at the end of a fall scenario.", "labels": [], "entities": []}, {"text": "The average experiment duration of an experiment was 2h 30min per person.", "labels": [], "entities": []}, {"text": "This experiment was very tiring for the participants and it was necessary to include rehearsals before starting the recordings so that the participant felt comfortable and was able to fall securely.", "labels": [], "entities": []}, {"text": "The methods presented in Section 2 were run on the Cirdo-set corpus presented in Section 3.1.3.", "labels": [], "entities": [{"text": "Cirdo-set corpus", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.9116585552692413}]}, {"text": "The SGMM model presented in Section 2.2 was used as acoutic model.", "labels": [], "entities": []}, {"text": "The generic language model (LM) was estimated from French newswire collected in the Gigaword corpus.", "labels": [], "entities": [{"text": "French newswire collected", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.9031065901120504}, {"text": "Gigaword corpus", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.828747034072876}]}, {"text": "It was 1-gram with 13,304 words.", "labels": [], "entities": []}, {"text": "Moreover, to reduce the linguistic variability, a 3-gram domain language model, the specialized language model was learnt from the sentences used during the corpus collection described in Section 3.", "labels": [], "entities": []}, {"text": "guage model was a 3-gram-type which resulted from the combination of the generic LM (with a 10% weight) and the specialized LM (with 90% weight).", "labels": [], "entities": []}, {"text": "This combination has been shown as leading to the best WER for domain specific application.", "labels": [], "entities": [{"text": "WER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.8067677617073059}]}, {"text": "The interest of such combination is to bias the recognition towards the domain LM but when the speaker deviates from the domain, the general LM makes it possible to avoid the recognition of sentences leading to \"false-positive\" detection.", "labels": [], "entities": []}, {"text": "Results on manually annotated data are given.", "labels": [], "entities": []}, {"text": "The most important performance measures are the Word Error Rate (WER) of the overall decoded speech and those of the specific distress calls as well as the Call Error Rate (CER: c.f. equation 2).", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 48, "end_pos": 69, "type": "METRIC", "confidence": 0.9189107120037079}, {"text": "Call Error Rate (CER", "start_pos": 156, "end_pos": 176, "type": "METRIC", "confidence": 0.7550433874130249}]}, {"text": "Considering distress calls only, the average WER is 34.0% whereas it a 39.3% when all interjections and sentences are taken into account.", "labels": [], "entities": [{"text": "WER", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9948782920837402}]}, {"text": "Unfortunately and as mentionned above, the used corpus doesn't allow the d\u00e9termine a False Alarm Rate.", "labels": [], "entities": [{"text": "False Alarm Rate", "start_pos": 85, "end_pos": 101, "type": "METRIC", "confidence": 0.9216928879419962}]}, {"text": "Previous studies based on the AD80 corpus showed recall, precision and Fmeasure equal to 88.4%, 86.9% and 87.2%.", "labels": [], "entities": [{"text": "AD80 corpus", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9733285307884216}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9996067881584167}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.999743640422821}, {"text": "Fmeasure", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9995905756950378}]}, {"text": "Nevertheless, this corpus was recorded in very different conditions, text reading in a studio, in contrary of those of Cirdo-set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of sentences of the AD80 corpus (\u22c6 denotes a sentence identified during the sociological study)", "labels": [], "entities": [{"text": "AD80 corpus", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9578383564949036}]}, {"text": " Table 3: Word and Call Error Rate for each participant", "labels": [], "entities": [{"text": "Word and Call Error Rate", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6143827199935913}]}]}