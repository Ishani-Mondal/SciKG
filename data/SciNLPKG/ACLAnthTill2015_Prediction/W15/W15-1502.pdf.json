{"title": [{"text": "Unsupervised Text Normalization Using Distributed Representations of Words and Phrases", "labels": [], "entities": [{"text": "Unsupervised Text Normalization", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6587932705879211}]}], "abstractContent": [{"text": "Text normalization techniques that use rule-based normalization or string similarity based on static dictionaries are typically unable to capture domain-specific abbreviations (custy, cx \u2192 customer) and shorthands (5ever, 7ever \u2192 forever) used in informal texts.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7113382816314697}]}, {"text": "In this work, we exploit the property that noisy and canoni-cal forms of a particular word share similar context in a large noisy text collection (millions or billions of social media feeds from Twitter, Facebook, etc.).", "labels": [], "entities": []}, {"text": "We learn distributed representations of words to capture the notion of contextual similarity and subsequently learn normalization lexicons from these representations in a completely unsupervised manner.", "labels": [], "entities": []}, {"text": "We experiment with linear and non-linear distributed representations obtained from log-linear models and neural networks, respectively.", "labels": [], "entities": []}, {"text": "We apply our framework for normalizing customer care notes and Twitter.", "labels": [], "entities": [{"text": "normalizing customer care notes", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.8408165276050568}]}, {"text": "We also extend our approach to learn phrase nor-malization lexicons (g2g \u2192 got to go) by training distributed representations over compound words.", "labels": [], "entities": []}, {"text": "Our approach outper-forms Microsoft Word, Aspell and a manually compiled urban dictionary from the Web and achieves state-of-the-art results on a publicly available Twitter dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text normalization is a prerequisite fora variety of tasks involving speech and language.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7987786531448364}]}, {"text": "Most natural language processing (NLP) tasks require a tight and compact vocabulary to reduce the model complexity in terms of feature size.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.75766854484876}]}, {"text": "As a consequence, applications such as syntactic tagging and parsing, semantic tagging, named entity extraction, information extraction, machine translation, language * The author is currently with Apple, Inc., and can be contacted at vrangarajansridh@apple.com.", "labels": [], "entities": [{"text": "syntactic tagging and parsing", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.724375456571579}, {"text": "semantic tagging", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7214661985635757}, {"text": "named entity extraction", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.621470163265864}, {"text": "information extraction", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.8264647126197815}, {"text": "machine translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7837813198566437}]}, {"text": "models for speech recognition, etc., are trained on clean data that is normalized and restricted to some user defined vocabulary.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8321285843849182}]}, {"text": "Conventionally, most NLP researchers perform such normalization through rule-based mapping that can get unwieldily and cumbersome for extremely noisy texts as in SMS, chat or social media.", "labels": [], "entities": []}, {"text": "Unnormalized text, as witnessed in social media forums such as Facebook, Twitter and message boards, or short messaging service (SMS), have a variety of issues with spelling that include repeating letters, eliminating vowels, using phonetic spellings, substituting letters (typically syllables) with numbers, using shorthands and user created abbreviations for phrases.", "labels": [], "entities": []}, {"text": "The remarkable property of such texts is that new variants of canonical words and phrases evolve constantly (e.g., jghome \u2192 just got home).", "labels": [], "entities": []}, {"text": "Hence, it is important to design a framework that can learn the mapping between unnormalized and canonical forms of such words and phrases in an unsupervised and extensible manner.", "labels": [], "entities": []}, {"text": "Conventional edit distance) based approaches are not accurate for predicting spelling correction for large number of edits in abbreviations and shorthands found in informal texts.", "labels": [], "entities": [{"text": "predicting spelling correction", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.8896547555923462}]}, {"text": "In this work, we exploit the property that noisy and canonical forms of a particular word share similar context in a large noisy text collection (millions or billions of social media feeds from Twitter, Facebook, etc.).", "labels": [], "entities": []}, {"text": "We represent the words in a vector space using distributed representations to capture the notion contextual similarity and subsequently learn normalization lexicons from these representations.", "labels": [], "entities": []}, {"text": "The distributed representations are induced either through neural networks (non-linear embeddings) or log-linear models (linear embeddings).", "labels": [], "entities": []}, {"text": "The proposed approach uses the property of contextual similarity between canonical and noisy versions of a particular word to cluster them in RD , where Dis the dimension of the distributed representation.", "labels": [], "entities": []}, {"text": "We also extend our framework to learn one-to-many mappings (e.g., ily \u2192 i love you, nbd \u2192 no big deal by learning distributed representations over words and phrases.", "labels": [], "entities": []}, {"text": "We demonstrate the fidelity of our approach on customer care domain and Twitter.", "labels": [], "entities": []}, {"text": "We also compare our approach with Microsoft Word, Aspell, custom dictionaries compiled from the Web as well as state-of-the-art techniques for unsupervised normalization.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we evaluated our approach on customer care data.", "labels": [], "entities": []}, {"text": "A set of 300 sentences from the customer care data was randomly selected and the reference sentences were created manually by a professional transcriber.", "labels": [], "entities": []}, {"text": "A total of 2387 tokens were normalized by the transcribers.", "labels": [], "entities": []}, {"text": "The distributed representation was trained on the remaining customer care data through neural network learning approach) over a window of 11 words with a vector dimension of 100.", "labels": [], "entities": []}, {"text": "We compare our approach with Microsoft Word and Aspell, where the best option was manually chosen (oracle) from the suggestion list.", "labels": [], "entities": []}, {"text": "If no option was appropriate, the word was left in it's original form.", "labels": [], "entities": []}, {"text": "We measure the fidelity of normalization using precision and recall.", "labels": [], "entities": [{"text": "normalization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9522917866706848}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9994482398033142}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9988436698913574}]}, {"text": "The results are presented in: Sentence level word normalization on English Twitter data to note that while our approach is customized to the domain, the baseline comparisons are not.", "labels": [], "entities": [{"text": "Sentence level word normalization", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.5085527375340462}, {"text": "English Twitter data", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.8628608187039694}]}, {"text": "The performance for noisy words that differ in edit distance by more than 2 from the canonical word is also shown in.", "labels": [], "entities": []}, {"text": "Our framework achieves significantly better performance for abbreviations that typically have edit distance > 2.", "labels": [], "entities": []}, {"text": "Since our approach combines the strength of distributional and lexical similarity as opposed to most approaches that rely only on string similarity, we are also able to correctly normalize domain specific abbreviations, e.g., custy \u2192 customer, cx \u2192 customer, lqd \u2192 liquid, bal \u2192 balance, exp \u2192 expectations, etc.", "labels": [], "entities": []}, {"text": "The use of a language model significantly improves the normalization accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9626651406288147}]}, {"text": "We also performed sentence (tweet) level normalization on Twitter data.", "labels": [], "entities": [{"text": "sentence (tweet) level normalization", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.6400347004334132}]}, {"text": "We manually annotated (expanded abbreviations, shorthands and spelling errors) 1000 tweets and performed normalization using our approach.", "labels": [], "entities": [{"text": "normalization", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.9618926644325256}]}, {"text": "The annotation was performed serially by two professional transcribers.", "labels": [], "entities": []}, {"text": "We compare our approach with Microsoft Word, Aspell and a dictionary compiled from several websites.", "labels": [], "entities": []}, {"text": "We use a log-linear model (continuous-bag-of-words) as well as a neural network (see Section 5) to automatically learn normalization lexicons.", "labels": [], "entities": []}, {"text": "For each model, we experimented with window length (wlen) of 11 and 15 while the dimension of distributed representation was either 100 or 200.", "labels": [], "entities": [{"text": "window length (wlen)", "start_pos": 37, "end_pos": 57, "type": "METRIC", "confidence": 0.8854283571243287}]}, {"text": "The results in indicate that using Algorithm 1 we achieve impressive performance with both models in comparison with the other schemes.", "labels": [], "entities": []}, {"text": "The log-linear model works just as well as the non-linear model and is much quicker to train.", "labels": [], "entities": []}, {"text": "One should note that the results from Microsoft Word and Aspell overestimate the fidelity of normalization since the task was performed manually, i.e., we picked the best option from the suggestion list.", "labels": [], "entities": []}, {"text": "In case of no correct suggestion, we left the original form as is.", "labels": [], "entities": []}, {"text": "Hence, the results are skewed towards achieving high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9983115196228027}]}, {"text": "In contrast, our approach is completely unsupervised in design and evaluation.", "labels": [], "entities": []}, {"text": "We also compared our approach with a Twitter and SMS dictionary compiled from several websites.", "labels": [], "entities": []}, {"text": "The dictionary contained entries for 3864 words and 3536 phrases.", "labels": [], "entities": []}, {"text": "The dictionary was compiled into a FST and the procedure in Section 6.2 was used for evaluation.", "labels": [], "entities": [{"text": "FST", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.5789967179298401}]}, {"text": "Since, the dictionary entries do not have an associated score, the FST lexicon N is unweighted.", "labels": [], "entities": [{"text": "FST lexicon N", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.6648556292057037}]}, {"text": "Our results clearly indicate that for construction of the normalization lexicon all we need is a reliable distributed representation trained on large amount of noisy text.", "labels": [], "entities": []}, {"text": "The non-linearity with the neural network does not help significantly for this task.", "labels": [], "entities": []}, {"text": "We also tested our approach on a publicly available Twitter test set () comprising of 548 sentences to compare our framework with other state-of-the-art approaches.", "labels": [], "entities": [{"text": "Twitter test set", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.8159908850987753}]}, {"text": "The training data and approach for each of these schemes is different and we did not optimize our model in anyway on the test domain or data.", "labels": [], "entities": []}, {"text": "The normalization was performed at the sentence level and we used the language model described in Section 6.2.", "labels": [], "entities": []}, {"text": "The results in demonstrates that our framework performs favorably in comparison with other techniques.", "labels": [], "entities": []}, {"text": "A major drawback of inducing normalization lexicons using most approaches described in Section 2 is that they are restricted to learning oneto-one word mappings.", "labels": [], "entities": []}, {"text": "However, social media text is strewn with abbreviations that span multiple words, e.g., ily2 refers to i love you too.", "labels": [], "entities": []}, {"text": "With our framework, one can obtain 1-to-many (or vice versa) mappings if the training data is modified to contain compound words, i.e., i love you too is replaced with i love you too and treated as a single token.", "labels": [], "entities": []}, {"text": "The biggest obstacle is to get a reliable list of such phrases since they keep changing and growing.", "labels": [], "entities": []}, {"text": "Unsupervised phrase induction using likelihood ratio test, point-wise mutual information, etc., maybe used for such a task but they typically do not capture phrases formed from high frequency function words.", "labels": [], "entities": [{"text": "phrase induction", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7616494596004486}, {"text": "likelihood ratio test", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.8975954055786133}]}, {"text": "We used a dataset of speech-based SMS message transcriptions for compiling a list of common phrases.", "labels": [], "entities": []}, {"text": "The SMS messages were collected through a smartphone application and a majority of them were collected while the users used the application in their cars.", "labels": [], "entities": []}, {"text": "We had access to a total of 41.3 million English messages.", "labels": [], "entities": []}, {"text": "The speech transcripts were mostly automatic and only a subset of around 400K utterances were manually transcribed.", "labels": [], "entities": []}, {"text": "To avoid the use of erroneous transcripts, we sorted the messages by frequency and picked phrases between length 2 and 4 that resulted in 27356 English phrases.", "labels": [], "entities": []}, {"text": "The training data was then phrasified (words were compounded) with the above phrase lists and the experiments to learn distributed representations was repeated.", "labels": [], "entities": []}, {"text": "We performed this experiment only on Twitter data.", "labels": [], "entities": []}, {"text": "Once the representations were learned, we computed the K-nearest neighbors using the cosine: Phrase normalizations learned through our framework similarity metric for each phrase.", "labels": [], "entities": []}, {"text": "The lexical similarity cost was computed differently for the phrases.", "labels": [], "entities": []}, {"text": "The first character of each word in the phrase was picked to form anew string (e.g., i love you too would be converted into ilyt) and a similar technique was used on the nearest neighbors; singleton numbers were expanded into strings (e.g., ily2 would be converted into ilyt).", "labels": [], "entities": []}, {"text": "The lexical similarity metric in Equation (3) was then used to compute the distance between the two strings.", "labels": [], "entities": []}, {"text": "The normalization table was subsequently inverted and compiled into a FST.", "labels": [], "entities": [{"text": "normalization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9663413166999817}, {"text": "FST", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.6636647582054138}]}, {"text": "shows some of the phrase normalizations learnt by our framework.", "labels": [], "entities": [{"text": "phrase normalizations", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.6906023323535919}]}, {"text": "The phrase normalization results for English are presented in.", "labels": [], "entities": [{"text": "phrase normalization", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6257235109806061}]}, {"text": "Our framework learns phrase normalizations quite well.", "labels": [], "entities": [{"text": "phrase normalizations", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.745081216096878}]}, {"text": "We achieve precision and recall of 92.6% and 72.0%, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9998464584350586}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.999842643737793}]}, {"text": "Even the mistakes committed are not very different from the ground truth, e.g., idk \u2192 i do not know while the reference is i don't know, wtf \u2192 what the hell instead of what the f*** and omfg \u2192 oh my god in place of oh my f**** god.", "labels": [], "entities": []}, {"text": "Our framework does not capture expansions of geographic locations present in the reference data such as nz \u2192 new zealand, la \u2192 los angeles and some highly context dependent expansions such as dw \u2192 doctor who, dm \u2192 direct message, etc., since the compiled phrase list did not contain these entries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Sentence level normalization on customer  care notes", "labels": [], "entities": [{"text": "Sentence level normalization", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8634801705678304}]}, {"text": " Table 3: Sentence level word normalization on English Twitter data", "labels": [], "entities": [{"text": "Sentence level word normalization", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8845729976892471}, {"text": "English Twitter data", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.7562716603279114}]}, {"text": " Table 4: Sentence level normalization on Twitter  test set from (Han et al., 2012b)", "labels": [], "entities": [{"text": "Sentence level normalization", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7853079040845236}, {"text": "Twitter  test set", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.8247247735659281}]}, {"text": " Table 5: Sentence level phrase normalization on English Twitter data", "labels": [], "entities": [{"text": "Sentence level phrase normalization", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.8616891652345657}, {"text": "English Twitter data", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.7569120327631632}]}]}