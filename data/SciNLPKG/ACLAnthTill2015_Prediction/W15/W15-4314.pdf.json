{"title": [{"text": "DCU-ADAPT: Learning Edit Operations for Microblog Normalisation with the Generalised Perceptron", "labels": [], "entities": [{"text": "DCU-ADAPT", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9508332014083862}, {"text": "Microblog Normalisation", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7554511725902557}]}], "abstractContent": [{"text": "We describe the work carried out by the DCU-ADAPT team on the Lexical Nor-malisation shared task at W-NUT 2015.", "labels": [], "entities": [{"text": "DCU-ADAPT", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9600200057029724}, {"text": "Lexical Nor-malisation shared task at W-NUT 2015", "start_pos": 62, "end_pos": 110, "type": "TASK", "confidence": 0.6061368840081351}]}, {"text": "We train a generalised perceptron to annotate noisy text with edit operations that normalise the text when executed.", "labels": [], "entities": []}, {"text": "Features are character n-grams, recurrent neu-ral network language model hidden layer activations, character class and eligibility for editing according to the task rules.", "labels": [], "entities": [{"text": "eligibility", "start_pos": 119, "end_pos": 130, "type": "METRIC", "confidence": 0.9750358462333679}]}, {"text": "We combine predictions from 25 models trained on subsets of the training data by selecting the most-likely normalisation according to a character language model.", "labels": [], "entities": []}, {"text": "We compare the use of a generalised percep-tron to the use of conditional random fields restricted to smaller amounts of training data due to memory constraints.", "labels": [], "entities": []}, {"text": "Furthermore , we make a first attempt to verify Chrupa\u0142a (2014)'s hypothesis that the noisy channel model would not be useful due to the limited amount of training data for the source language model, i.e. the language model on normalised text.", "labels": [], "entities": []}], "introductionContent": [{"text": "The W-NUT Lexical Normalisation for English Tweets shared task is to normalise spelling and to expand contractions in English microblog messages (.", "labels": [], "entities": [{"text": "Lexical Normalisation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6659160107374191}, {"text": "normalise spelling", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.833798885345459}]}, {"text": "This includes one-tomany and many-to-one replacements as in \"we're\" and \"l o v e\".", "labels": [], "entities": []}, {"text": "Tokens containing characters other than alphanumeric characters and the apostrophe are excluded from the task, as well as proper nouns and acronyms that would be acceptable in welledited text.", "labels": [], "entities": []}, {"text": "(The input, however, does not identify such tokens and unnecessarily modifying them is penalised in the evaluation.)", "labels": [], "entities": []}, {"text": "To make evaluation easier, participants are further required to align output tokens to input tokens, e.g. when the four tokens \"l\", \"o\", \"v\" and \"e\" are amalgamated to the single token \"love\", three empty tokens must follow in the output.", "labels": [], "entities": []}, {"text": "This is easy for approaches that process the input token by token but may require extra work if the input string is processed differently.", "labels": [], "entities": []}, {"text": "We participate in the constrained mode that allows off-the-shelf tools but no normalisation lexicons and additional data to be used.", "labels": [], "entities": []}, {"text": "Furthermore, we do not use any lexicon of canonical English but learn our normalisation model purely from the provided training data.", "labels": [], "entities": []}, {"text": "Our approach follows previous work by in that we train a sequence labeller to annotate edit operations that are intended to normalise the text when applied to the input text.", "labels": [], "entities": []}, {"text": "However, while Chrupa\u0142a uses conditional random fields for sequence labelling, we further experiment with using a generalised Perceptron and with using a simple noisy channel model with character n-gram language models trained on the normalised side of the training data to select the final normalisation from a set of candidate normalisation generated from an ensemble of sequence labellers and from selectively ignoring some of the proposed edit operations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our best systems using the evalution script provided by the shared task organisers.", "labels": [], "entities": []}, {"text": "It counts: \u2022 The number of correctly modified tokens, i.e. tokens that need to be replaced by anew nonempty token and the system correctly predicts this token.", "labels": [], "entities": []}, {"text": "\u2022 Number of tokens needing normalisation, i.e. tokens that are modified in the gold output.", "labels": [], "entities": []}, {"text": "However, again, tokens that are to be deleted are ignored, e.g. \"l o v e\" to \"love\" counts as one event only despite the replacement of three tokens with empty tokens.", "labels": [], "entities": []}, {"text": "\u2022 The number of tokens modified by system, i.e. tokens for which a substitution with a non-empty token is proposed by the system.", "labels": [], "entities": []}, {"text": "Based on these numbers, precision, recall and F1-score are calculated and we select the system and configuration to be used on the test set based on highest average F1-score over the 5 crossvalidation runs.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.999729573726654}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9997053742408752}, {"text": "F1-score", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9993374943733215}, {"text": "F1-score", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9919822812080383}]}], "tableCaptions": [{"text": " Table 1: Average language model perplexity over  the five cross-validation runs for n-gram sizes n =  2, ..., 6 and smoothing methods WB = Witten- Bell, KN = Keyser-Ney and GT = Good-Turing.  Standard deviation \u03c3 \u2264 0.23 for all configura- tions.", "labels": [], "entities": [{"text": "Standard deviation \u03c3", "start_pos": 193, "end_pos": 213, "type": "METRIC", "confidence": 0.869523803393046}]}]}