{"title": [{"text": "Re-assessing the WMT2013 Human Evaluation with Professional Translators Trainees", "labels": [], "entities": [{"text": "WMT2013 Human Evaluation", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.7336655855178833}]}], "abstractContent": [{"text": "This paper presents experiments on the human ranking task performed during WMT2013.", "labels": [], "entities": [{"text": "WMT2013", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.5151406526565552}]}, {"text": "The goal of these experiments is to rerun the human evaluation task with translation studies students and to compare the results with the human rankings performed by the WMT development teams during WMT2013.", "labels": [], "entities": [{"text": "WMT development teams during WMT2013", "start_pos": 170, "end_pos": 206, "type": "DATASET", "confidence": 0.7839380502700806}]}, {"text": "More specifically, we test whether we can reproduce, and if yes to what extent, the WMT2013 ranking task and whether specialised knowledge from translation studies influences the results in terms of intra-and inter-annotator agreement as well as in terms of system ranking.", "labels": [], "entities": [{"text": "WMT2013 ranking task", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.5070212682088217}]}, {"text": "We present two experiments on the English-German WMT2013 machine translation output.", "labels": [], "entities": [{"text": "WMT2013 machine translation", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7409066756566366}]}, {"text": "Analysis of the data follows the methods described in the official WMT2013 report.", "labels": [], "entities": [{"text": "WMT2013 report", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.9215726256370544}]}, {"text": "The results indicate a higher inter-and intra-annotator agreement, less ties and slight differences in ranking for the translation studies students as compared to the WMT development teams.", "labels": [], "entities": [{"text": "translation studies", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.9068318009376526}, {"text": "WMT development", "start_pos": 167, "end_pos": 182, "type": "TASK", "confidence": 0.5617112219333649}]}], "introductionContent": [{"text": "Machine translation evaluation is an important element in the process of building MT systems.", "labels": [], "entities": [{"text": "Machine translation evaluation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8964612285296122}, {"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9919712543487549}]}, {"text": "The Workshop for Statistical Machine Translation (WMT) compares new techniques for MT through human and automatic MT evaluation and provides also tracks for evaluation metrics, quality estimation of MT as well as post-editing of MT.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.8111858069896698}, {"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9943826198577881}, {"text": "MT evaluation", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9283069670200348}, {"text": "MT", "start_pos": 229, "end_pos": 231, "type": "TASK", "confidence": 0.9088788032531738}]}, {"text": "To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c \ufffd 2015 The authors.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9344075620174408}, {"text": "hypothesis translation", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7506290972232819}, {"text": "IBM c \ufffd 2015", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.9147640019655228}]}, {"text": "This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND.", "labels": [], "entities": []}, {"text": "BLEU (), NIST),), WER, position-independent error rate metric PER () and the translation edit rate metric TER () and TERp (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9813565015792847}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.7665477395057678}, {"text": "WER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9958410859107971}, {"text": "position-independent error rate metric PER", "start_pos": 23, "end_pos": 65, "type": "METRIC", "confidence": 0.7884479641914368}, {"text": "translation edit rate metric TER", "start_pos": 77, "end_pos": 109, "type": "METRIC", "confidence": 0.5793848693370819}, {"text": "TERp", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9587172269821167}]}, {"text": "as well as introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.8735990722974142}]}, {"text": "Human machine translation evaluation can be performed with different methods.", "labels": [], "entities": [{"text": "Human machine translation evaluation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6958247199654579}]}, {"text": "propose HMEANT, a metric based on MEANT () that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role fillers.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.7225910425186157}, {"text": "MEANT", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9594494700431824}, {"text": "meaning preservation between hypothesis and reference translation", "start_pos": 57, "end_pos": 122, "type": "TASK", "confidence": 0.7916548464979444}]}, {"text": "Another method is HTER () which produces targeted reference translations by post-editing MT output.", "labels": [], "entities": [{"text": "MT output", "start_pos": 89, "end_pos": 98, "type": "TASK", "confidence": 0.8367847204208374}]}, {"text": "Another method is HTER () which produces targeted reference translations by postediting MT output.", "labels": [], "entities": []}, {"text": "Human evaluation can also be performed by measuring post-editing time, or by asking evaluators to assess the fluency and adequacy of a hypothesis translation on a Likert scale.", "labels": [], "entities": []}, {"text": "Another popular human evaluation method is ranking: ordering a set of translation hypotheses according to their quality.", "labels": [], "entities": []}, {"text": "This is also the method applied during the recent WMTs, where humans are asked to rank machine translation output by using APPRAISE, a software tool that integrates facilities for such a ranking task.", "labels": [], "entities": [{"text": "WMTs", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.9184550046920776}]}, {"text": "In WMT, human MT evaluation is carried out by the MT development teams, usually computer scientists or computational linguists, sometimes involving crowd-sourcing based on Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "WMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9623867869377136}, {"text": "MT evaluation", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.9539210498332977}, {"text": "MT development", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9026838541030884}]}, {"text": "Being aware of the two communities, machine translation and translation studies, we took the available online data from the WMT2013 1 and tried to reproduce the ranking task with translation studies students for the English to German translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7819474339485168}, {"text": "translation studies", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8996122181415558}, {"text": "WMT2013 1", "start_pos": 124, "end_pos": 133, "type": "DATASET", "confidence": 0.8683746457099915}]}, {"text": "The three questions we want to answer are: \u2022 Can we reproduce at all the WMT2013 results for the language pair English-German?", "labels": [], "entities": [{"text": "WMT2013 results", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.8477516770362854}]}, {"text": "\u2022 Are translation studies students (future translators) evaluating different from the WMT development teams, or in other words does specialised knowledge from translation studies influence the outcome of the ranking task?", "labels": [], "entities": [{"text": "translation studies students (future translators)", "start_pos": 6, "end_pos": 55, "type": "TASK", "confidence": 0.8052669082369123}, {"text": "WMT development", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.6063793301582336}]}, {"text": "\u2022 Are translation studies students more consistent as a group and with themselves in terms of intra-and inter-agreement?", "labels": [], "entities": [{"text": "translation studies", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.9214459359645844}]}, {"text": "We concentrate on English-German data since the majority of our evaluators were native speakers of German and since, from a translation studies point of view, professional translation should be performed only into the mother tongue.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted the experiments as similar as possible to the manual ranking task in WMT2013.", "labels": [], "entities": [{"text": "WMT2013", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9316696524620056}]}, {"text": "Like in WMT2013, evaluators were presented with a source sentence, a reference translation and five outputs produced by five anonymised and randomised machine translations systems.", "labels": [], "entities": [{"text": "WMT2013", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.7790938019752502}]}, {"text": "The instructions for the evaluators remained the same as in WMT2013: You are shown a source sentence followed by several candidate translations.", "labels": [], "entities": [{"text": "WMT2013", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8320733308792114}]}, {"text": "Your task is to rank the translations from best to worst (ties are allowed) For performing the ranking task we implemented the Java-based ranking tool depicted in.", "labels": [], "entities": []}, {"text": "Similar to APPRAISE) the ranking can be performed on a scale from 1 to 5, with 1 being the best translation and 5 being the worst translation.", "labels": [], "entities": []}, {"text": "For a given source sentence, each ranking of the five MT outputs has the potential to produce 10 ranking pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9439096450805664}]}, {"text": "Before applying the corresponding formulas on the data, the ranking pairs from all evaluators and for all systems are collected in a matrix like the one in.", "labels": [], "entities": []}, {"text": "The matrix records the number of times system Si was ranked better than S j and vice-versa.", "labels": [], "entities": []}, {"text": "For example, if we look at the two systems S 1 and S 3 in the matrix, we can see that S 3 was ranked 2 times higher (from the left triangle) and 4 times lower (from the right triangle) than system S 1 . From the matrix, the final score for each system -as defined by and applied in WMT2013 -can be computed.", "labels": [], "entities": [{"text": "WMT2013", "start_pos": 282, "end_pos": 289, "type": "DATASET", "confidence": 0.9288444519042969}]}, {"text": "From the matrix in the score for system S 1 is computed by counting for each pair of systems (S 1 , S 2 ), (S 1 , S 3 ), (S 1 , S 4 ), (S 1 , S 5 ) the number of times S 1 was ranked higher than the other system divided by the total number of rankings for each pair.", "labels": [], "entities": []}, {"text": "The results for each pair of systems including S 1 are then The implementation of anew tool was motivated by the accessibility of a server for the evaluators.", "labels": [], "entities": []}, {"text": "This way each evaluator had his own evaluation set containing both the tool and the data set.", "labels": [], "entities": []}, {"text": "summed and divided by the number of systems, this being the final score for S 1 . Considering having a system Si from a set of systems S of size k and a set of rankings for each system pair (S i , S j ), where j = 1 . .", "labels": [], "entities": []}, {"text": "k, S j \u2208 Sand i \ufffd = j the score for Si is defined as follows: Based on Koehn's (2012) formula each system gets a score and a ranking among the set of systems.", "labels": [], "entities": []}, {"text": "After performing the ranking the systems are clustered by using bootstrap resampling, thus returning the final score and the cluster for each system.", "labels": [], "entities": []}, {"text": "Different from WMT2013 we run two evaluation rounds for the ranking task.", "labels": [], "entities": [{"text": "WMT2013", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.7049514055252075}]}, {"text": "The first round was a pilot study on which all evaluators had to evaluate the same set of randomised and anonymised sentences selected from the published WMT2013 ranking task data set.", "labels": [], "entities": [{"text": "WMT2013 ranking task data set", "start_pos": 154, "end_pos": 183, "type": "DATASET", "confidence": 0.8337193965911865}]}, {"text": "The set contained 200 source sentences and five anonymised and randomised MT outputs for each source sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9382386803627014}]}, {"text": "In the pilot study we selected, as in WMT2013, only the above mentioned 14 machine translation systems for evaluation, disregarding the remaining anonymised commercial and online systems.", "labels": [], "entities": [{"text": "WMT2013", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9441394209861755}, {"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7627302408218384}]}, {"text": "Regarding the sampling of the data, the second evaluation round followed the ranking task performed in WMT2013: each evaluator ranked a different randomised and anonymised sample consisting of 200 source sentences and five anonymised and randomised MT outputs for each source sentence.", "labels": [], "entities": [{"text": "WMT2013", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9327242970466614}]}, {"text": "The individual samples were built out of all 21 machine translations outputs of the 3000 source sentences provided for the translation task.: The Java-based ranking tool.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Representation of the ranking pairs as a  matrix", "labels": [], "entities": [{"text": "Representation", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9900778532028198}]}, {"text": " Table 2: Intra-annotator agreement for the pilot  study.", "labels": [], "entities": [{"text": "Intra-annotator", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.95423823595047}]}, {"text": " Table 3: System ranking in the pilot study without  bootstrap resampling", "labels": [], "entities": []}, {"text": " Table 4: System ranking in the main study without  bootstrap resampling", "labels": [], "entities": []}, {"text": " Table 5: System ranking with bootstrap resampling in WMT2013 and in the main study", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.6416848301887512}, {"text": "WMT2013", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.8738492131233215}]}, {"text": " Table 6: Overview over collected data and Cohen's \u03ba for the language pair English-German", "labels": [], "entities": []}]}