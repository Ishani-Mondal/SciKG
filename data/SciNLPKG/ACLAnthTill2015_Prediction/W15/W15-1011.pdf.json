{"title": [{"text": "What Matters Most in Morphologically Segmented SMT Models?", "labels": [], "entities": [{"text": "Morphologically Segmented SMT Models", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.5983276441693306}]}], "abstractContent": [{"text": "Morphological segmentation is an effective strategy for addressing difficulties caused by morphological complexity.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9377208650112152}]}, {"text": "In this study, we use an English-to-Arabic test bed to determine what steps and components of a phrase-based statistical machine translation pipeline benefit the most from segmenting the target language.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation pipeline", "start_pos": 96, "end_pos": 149, "type": "TASK", "confidence": 0.6539990067481994}]}, {"text": "We test several scenarios that differ primarily in when desegmentation is applied, showing that the most important criterion for success in segmentation is to allow the system to build target words from morphemes that span phrase boundaries.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 140, "end_pos": 152, "type": "TASK", "confidence": 0.9620825052261353}]}, {"text": "We also investigate the impact of segmented and unsegmented target language models (LMs) on translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8526498079299927}]}, {"text": "We show that an unsegmented LM is helpful according to BLEU score, but also leads to a drop in the overall usage of composi-tional morphology, bringing it to well below the amount observed inhuman references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9990413784980774}]}], "introductionContent": [{"text": "It is well known that morphological segmentation can improve statistical machine translation (SMT).", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.7641530930995941}, {"text": "statistical machine translation (SMT)", "start_pos": 61, "end_pos": 98, "type": "TASK", "confidence": 0.8017772237459818}]}, {"text": "By splitting relevant morphological affixes into independent tokens, segmentation has repeatedly been shown to improve translation into or out of morphologically complex languages.", "labels": [], "entities": []}, {"text": "Segmentation as a preprocessing step brings several benefits to translation: \u2022 Correspondence with morphologically simple languages, such as English is improved.", "labels": [], "entities": [{"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9822256565093994}]}, {"text": "In, segmenting bsyArth allows one-to-one links for \"with\", \"his\" and \"car\".", "labels": [], "entities": [{"text": "segmenting bsyArth", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7831827402114868}]}, {"text": "\u2022 By building models over morphemes, rather than words, data sparsity is reduced.", "labels": [], "entities": []}, {"text": "\u2022 By allowing morphemes with clear syntactic roles to be translated independently, we increase our expressive power by creating new lexical translations.", "labels": [], "entities": []}, {"text": "For example, using the two phrase-pairs in results in anew word after desegmentation (b+ syArp +h \u21d2 bsyArth), which might not have existed in the training data.", "labels": [], "entities": []}, {"text": "However, there is also a price to be paid.", "labels": [], "entities": []}, {"text": "While morpheme-level models are more resistant to data sparsity, they account for less context than wordlevel models, make stronger independence assumptions, and they are less efficient statistically, in that they devote probability mass to sequences containing illegal words.", "labels": [], "entities": []}, {"text": "Furthermore, when segmentation is applied to the target language, the process must be reversed at the end of the pipeline to present the output in a readable format.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9620038270950317}]}, {"text": "This desegmentation step complicates our pipeline, and can introduce errors.", "labels": [], "entities": []}, {"text": "Our work is inspired by two recent contributions that attempt to combine the advantages of word-and morpheme-based models.", "labels": [], "entities": []}, {"text": "combine word and morpheme views in a desegmented phrase table, allowing morphemes to reduce sparsity while words expand context, and eliminating the need fora separate desegmentation step.", "labels": [], "entities": []}, {"text": "Their word-boundary-aware morpheme-level phrase extraction technique restricts phrase boundaries so that no target phrase can begin with a suffix or end with a prefix.", "labels": [], "entities": [{"text": "word-boundary-aware morpheme-level phrase extraction", "start_pos": 6, "end_pos": 58, "type": "TASK", "confidence": 0.621712900698185}]}, {"text": "This allows them to desegment each target phrase independently, enabling the use of both word-and morpheme-level language models during decoding.", "labels": [], "entities": []}, {"text": "However, this phrase-table desegmentation approach lacks the expressive power that comes from translating morphemes independently.", "labels": [], "entities": [{"text": "phrase-table desegmentation", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.7408000528812408}]}, {"text": "More recently, propose a lattice desegmentation approach, which comes close to combining all the advantages of word and morpheme views.", "labels": [], "entities": []}, {"text": "By desegmenting a lattice that compactly represents many translation options, and rescoring it with a word-level language model, they avoid restricting the phrase table.", "labels": [], "entities": []}, {"text": "However, by delaying desegmentation until rescoring, the approach loses's advantage of full decoder integration.", "labels": [], "entities": []}, {"text": "In this paper, we present an experimental study of English-to-Arabic translation that is designed to better understand the impact of various trade-offs when translating into a morphologically segmented target language, and to identify what aspects of segmentation are most beneficial to translation.", "labels": [], "entities": [{"text": "English-to-Arabic translation", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.7534896731376648}]}, {"text": "The benefits of segmentation can impact several components in the SMT pipeline: the alignment model, the translation table, and the various language and translation models.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9655119180679321}, {"text": "SMT pipeline", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.934310644865036}]}, {"text": "Throughout this study, we investigate the effect of varying the point in the SMT pipeline where the segmentation is reversed.", "labels": [], "entities": [{"text": "SMT pipeline", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.906961977481842}]}, {"text": "In addition, we attempt to combine word-and morpheme-level models within the decoder as much as possible.", "labels": [], "entities": []}, {"text": "Our experimental study provides three novel insights.", "labels": [], "entities": []}, {"text": "First, we present strong evidence indicating that the ability to build target words across phrase boundaries is the most important property of target language segmentation.", "labels": [], "entities": [{"text": "target language segmentation", "start_pos": 143, "end_pos": 171, "type": "TASK", "confidence": 0.6473254064718882}]}, {"text": "This implies that phrase table desegmentation, the only published desegmentation technique that has been fully integrated into decoding, gives up segmentation's primary advantage.", "labels": [], "entities": [{"text": "phrase table desegmentation", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.7014476458231608}]}, {"text": "Second, we draw a previously unobserved connection between the use of an unsegmented LM and the decoder's overall use of compositional morphology; we show that although unsegmented LMs tend to increase BLEU score, they also reduce the system's use of morphological affixes to well below that of a human.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 202, "end_pos": 212, "type": "METRIC", "confidence": 0.9839259088039398}]}, {"text": "Finally, we present the first direct comparison between phrase table desegmentation () and lattice desegmentation ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our English-to-Arabic system using 1.49 million sentence pairs drawn from the NIST 2012 training set, excluding the UN data.", "labels": [], "entities": [{"text": "NIST 2012 training set", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.9718134552240372}, {"text": "UN data", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.7391133308410645}]}, {"text": "This training set contains about 40 million Arabic tokens before segmentation, and 47 million after segmentation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.9629115462303162}]}, {"text": "We tune on the NIST 2004 evaluation set (1353 sentences) and evaluate on NIST 2005 (1056 sentences).", "labels": [], "entities": [{"text": "NIST 2004 evaluation set", "start_pos": 15, "end_pos": 39, "type": "DATASET", "confidence": 0.9740340858697891}, {"text": "NIST 2005", "start_pos": 73, "end_pos": 82, "type": "DATASET", "confidence": 0.9793759286403656}]}, {"text": "We also report a second test, which tunes on the NIST 2006 evaluation set (1664 sentences) and evaluates on NIST 2008 (1360 sentences) and 2009 (1313 sentences).", "labels": [], "entities": [{"text": "NIST 2006 evaluation set", "start_pos": 49, "end_pos": 73, "type": "DATASET", "confidence": 0.9802030175924301}, {"text": "NIST 2008", "start_pos": 108, "end_pos": 117, "type": "DATASET", "confidence": 0.9583469331264496}]}, {"text": "NIST 2004 and 2005 datasets have sentences from newswire, while NIST 2006/2008/2009 have sentences drawn from newswire and the web.", "labels": [], "entities": [{"text": "NIST 2004 and 2005 datasets", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.936982023715973}, {"text": "NIST 2006/2008/2009", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.9669880370299021}]}, {"text": "These evaluation sets are intended for Arabic-to-English translation, and therefore have multiple English references.", "labels": [], "entities": [{"text": "Arabic-to-English translation", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.6535735428333282}]}, {"text": "As we are translating into Arabic, we select the first English reference to use as our source text, and use the Arabic source as our single reference translation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU scores on on each of the methods described in section 3 . MT05 results are tuned using NIST MT04.  Results on NIST MT08 and MT09 datasets are tuned on MT06 dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993171691894531}, {"text": "MT05", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.5524193048477173}, {"text": "NIST", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.9822942018508911}, {"text": "MT04", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.49097493290901184}, {"text": "NIST MT08", "start_pos": 125, "end_pos": 134, "type": "DATASET", "confidence": 0.7757181525230408}, {"text": "MT09 datasets", "start_pos": 139, "end_pos": 152, "type": "DATASET", "confidence": 0.8440414369106293}, {"text": "MT06 dataset", "start_pos": 166, "end_pos": 178, "type": "DATASET", "confidence": 0.9861101806163788}]}, {"text": " Table 3: Percentage of words in the SMT output that have non-identity morphological segmentations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9882904291152954}]}]}