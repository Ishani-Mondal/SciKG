{"title": [{"text": "Dependency Link Embeddings: Continuous Representations of Syntactic Substructures", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple method to learn continuous representations of dependency substruc-tures (links), with the motivation of directly working with higher-order, structured embed-dings and their hidden relationships, and also to avoid the millions of sparse, template-based word-cluster features in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 297, "end_pos": 315, "type": "TASK", "confidence": 0.778489887714386}]}, {"text": "These link embeddings allow a significantly smaller and simpler set of unary features for dependency parsing, while maintaining improvements similar to state-of-the-art, n-ary word-cluster features, and also stacking over them.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8677635490894318}]}, {"text": "Moreover, these link vectors (made publicly available) are directly portable as off-the-shelf, dense, syntactic features in various NLP tasks.", "labels": [], "entities": []}, {"text": "As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manually-defined features, and also stacks over them.", "labels": [], "entities": [{"text": "constituent parse reranking", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6590118010838827}]}], "introductionContent": [{"text": "Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (.", "labels": [], "entities": []}, {"text": "While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 266, "end_pos": 284, "type": "TASK", "confidence": 0.77619668841362}]}, {"text": "Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings ().", "labels": [], "entities": [{"text": "statistical dependency parsers", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.6377438306808472}]}, {"text": "Some recent work has addressed this issue, via low-rank tensor mappings (), feature embeddings ), or neural network parsers.", "labels": [], "entities": []}, {"text": "Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships.", "labels": [], "entities": []}, {"text": "In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (), while still maintaining their strong accuracies.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 308, "end_pos": 326, "type": "TASK", "confidence": 0.7807299792766571}]}, {"text": "Trained using appropriate dependency-based context in word2vec, the fast neural language model of, these link vectors allow a substantially smaller set of unary link features (as opposed to n-ary, conjoined features) which provide savings in parsing time and memory.", "labels": [], "entities": [{"text": "parsing", "start_pos": 242, "end_pos": 249, "type": "TASK", "confidence": 0.9658786654472351}]}, {"text": "Moreover, unlike conjoined features, link embeddings allow a tractable set of accurate per-dimension features, making the feature set even smaller and the featuregeneration process orders of magnitude faster (than hierarchical clustering features).", "labels": [], "entities": []}, {"text": "At the same time, these link embedding features maintain dependency parsing improvements similar to the complex, template-based features on word clusters and embeddings by previous work) (up to 9% relative error reduction), and also stack statistically significantly over them (up to an additional 5% relative error reduction).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6772054880857468}, {"text": "relative error reduction", "start_pos": 301, "end_pos": 325, "type": "METRIC", "confidence": 0.6951364676157633}]}, {"text": "Another advantage of this approach (versus previous work on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful . As an example, we incorporate them into a constituent parse reranker and see improvements that again match state-of-the-art, manually-defined, non-local reranking features and stack over them statistically significantly.", "labels": [], "entities": [{"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9735279679298401}]}, {"text": "We make our link embeddings publicly available 1 and hope that they will prove useful in various other NLP tasks in future work, e.g., as dense, syntactic features in sentence classification or as linguistically-intuitive, initial units in vectorspace composition.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 167, "end_pos": 190, "type": "TASK", "confidence": 0.7201360613107681}]}], "datasetContent": [{"text": "In this section, we will first discuss how we use the link embeddings as features in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8905932903289795}]}, {"text": "Next, we will present empirical results on feature space reduction and on parsing performance on both in-domain and out-of-domain datasets.", "labels": [], "entities": [{"text": "feature space reduction", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6305414934953054}]}], "tableCaptions": [{"text": " Table 2: Number of features.", "labels": [], "entities": []}, {"text": " Table 3: UAS results on WSJ.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.5930595397949219}, {"text": "WSJ", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.7772414088249207}]}, {"text": " Table 4: UAS results on Web treebanks.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.645561158657074}, {"text": "Web treebanks", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8161780834197998}]}, {"text": " Table 5: F1 results of constituent reranker on WSJ.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9985798597335815}, {"text": "WSJ", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9786770343780518}]}]}