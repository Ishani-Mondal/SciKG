{"title": [{"text": "Machine Translation Evaluation using Recurrent Neural Networks", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8523708581924438}]}], "abstractContent": [{"text": "This paper presents our metric (UoW-LSTM) submitted in the WMT-15 met-rics task.", "labels": [], "entities": [{"text": "WMT-15 met-rics task", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.5563134948412577}]}, {"text": "Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve the best results.", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.881872167189916}]}, {"text": "We use a metric based on dense vector spaces and Long Short Term Memory (LSTM) networks, which are types of Recurrent Neural Networks (RNNs).", "labels": [], "entities": []}, {"text": "For WMT-15 our new metric is the best performing metric overall according to Spearman and Pearson (Pre-TrueSkill) and second best according to Pearson (TrueSkill) system level correlation.", "labels": [], "entities": [{"text": "WMT-15", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8532037734985352}, {"text": "Pearson (TrueSkill) system level correlation", "start_pos": 143, "end_pos": 187, "type": "METRIC", "confidence": 0.6990650168487004}]}], "introductionContent": [{"text": "Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (), sentiment analysis (), parsing) and machine translation ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.9467488825321198}, {"text": "parsing", "start_pos": 131, "end_pos": 138, "type": "TASK", "confidence": 0.9760253429412842}, {"text": "machine translation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.8275846540927887}]}, {"text": "While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or Recurrent Neural Networks (RNNs) are able to capture semantic similarity for words), segments) and documents () naturally, traditional measures can only achieve this using resources like WordNet and paraphrase databases.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 290, "end_pos": 297, "type": "DATASET", "confidence": 0.9391885995864868}]}, {"text": "This paper presents a novel, efficient and compact MT evaluation measure based on RNNs.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9478976726531982}]}, {"text": "Our metric () is simple in the sense that it does not require much machinery and resources apart from the dense word vectors.", "labels": [], "entities": []}, {"text": "This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9513408839702606}]}, {"text": "Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks.", "labels": [], "entities": []}, {"text": "LSTM is a sequence learning technique which uses a memory cell to preserve a state over along period of time.", "labels": [], "entities": []}, {"text": "This enables distributed representations of sentences using distributed representations of words.", "labels": [], "entities": []}, {"text": "Tree-LSTM () is a recent approach, which is an extension of the simple LSTM framework).", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the ReVal (Gupta et al., 2015) metric for this task.", "labels": [], "entities": [{"text": "ReVal (Gupta et al., 2015) metric", "start_pos": 12, "end_pos": 45, "type": "DATASET", "confidence": 0.8025960458649529}]}, {"text": "This metric represents both the reference (h ref ) and the translation (h tra ) using a dependency Tree-LSTM () and predicts the similarity scor\u00ea y based on a neural network which considers both distance and angle between h ref and h tra : where, \u03c3 is a sigmoid function, \u02c6 p \u03b8 is the estimated probability distribution vector and r T = [1 2...K].", "labels": [], "entities": []}, {"text": "The cost function J(\u03b8) is defined over probability distributions p and\u02c6pand\u02c6 and\u02c6p \u03b8 using regularised KullbackLeibler (KL) divergence.", "labels": [], "entities": [{"text": "J", "start_pos": 18, "end_pos": 19, "type": "METRIC", "confidence": 0.6519066095352173}]}, {"text": "In Equation 3, i represents the index of each training pair, n is the number of training pairs and p is the sparse target distribution such that y = r T p is defined as follows: is the similarity score of a training pair.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 185, "end_pos": 201, "type": "METRIC", "confidence": 0.9494177401065826}]}, {"text": "For example, for y = 2.7, p T = [0 0.3 0.7 0 0].", "labels": [], "entities": []}, {"text": "In our case, the similarity score y is a value between 1 and 5.", "labels": [], "entities": [{"text": "similarity score y", "start_pos": 17, "end_pos": 35, "type": "METRIC", "confidence": 0.9796153903007507}]}, {"text": "To compute our training data we automatically convert the human rankings of the WMT-13 evaluation data into similarity scores between the reference and the translation.", "labels": [], "entities": [{"text": "WMT-13 evaluation data", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.8702470660209656}]}, {"text": "These translationreference pairs labelled with similarity scores are used for training.", "labels": [], "entities": []}, {"text": "We also augment the WMT-13 data with 4500 pairs from the SICK training set (), resulting in a training dataset of 14059 pairs in total.", "labels": [], "entities": [{"text": "WMT-13 data", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.8904480040073395}, {"text": "SICK training set", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9108138084411621}]}, {"text": "The metric uses Glove word vectors) and the simple LSTM, the dependency Tree-LSTM and neural network implementations by.", "labels": [], "entities": []}, {"text": "Training is performed using a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.9766018092632294}, {"text": "regularization strength", "start_pos": 80, "end_pos": 103, "type": "METRIC", "confidence": 0.9760912954807281}]}, {"text": "The memory dimension is 300, hidden dimension is 100 and compositional parameters are 541,800.", "labels": [], "entities": []}, {"text": "Training is performed for 10 epochs.", "labels": [], "entities": []}, {"text": "System level scores are computed by aggregating and normalising the segment level scores.", "labels": [], "entities": []}, {"text": "Full details can be found in ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results WMT-15 Evaluation: System-Level Correlations", "labels": [], "entities": [{"text": "WMT-15", "start_pos": 18, "end_pos": 24, "type": "TASK", "confidence": 0.6351848840713501}]}]}