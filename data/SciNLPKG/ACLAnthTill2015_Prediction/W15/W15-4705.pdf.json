{"title": [{"text": "Reading Times Predict the Quality of Generated Text Above and Beyond Human Ratings", "labels": [], "entities": []}], "abstractContent": [{"text": "Typically, human evaluation of NLG output is based on user ratings.", "labels": [], "entities": []}, {"text": "We collected ratings and reading time data in a simple, low-cost experimental paradigm for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.8562988042831421}]}, {"text": "Participants were presented corpus texts, automatically linearised texts, and texts containing predicted referring expressions and automatic lin-earisation.", "labels": [], "entities": []}, {"text": "We demonstrate that the reading time metrics outperform the ratings in classifying texts according to their quality.", "labels": [], "entities": []}, {"text": "Regression analyses showed that self-reported ratings discriminated poorly between the kinds of manipulation, especially between defects in word order and text coherence.", "labels": [], "entities": []}, {"text": "In contrast, a combination of objective measures from the low-cost mouse contingent reading paradigm provided very high classification accuracy and thus, greater insight into the actual quality of an automatically generated text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9130737781524658}]}], "introductionContent": [{"text": "Evaluating and comparing systems that produce natural language text as output, such as natural language generation (NLG) systems, is notoriously difficult.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 87, "end_pos": 120, "type": "TASK", "confidence": 0.7952662607034048}]}, {"text": "Many aspects of linguistic well-formedness and naturalness play a role for assessing the quality of an automatically generated text.", "labels": [], "entities": []}, {"text": "On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness.", "labels": [], "entities": []}, {"text": "On the text-level, further criteria related to coherence, text structure, and content should be considered.", "labels": [], "entities": []}, {"text": "One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.8809615075588226}]}, {"text": "Human ratings have been used for system comparison in a number of NLG shared tasks (), for validating other automatic evaluation methods in NLG, and for training statistical components of NLG systems.", "labels": [], "entities": []}, {"text": "When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as 'A: how fluent is the text?' and 'B: how clear and understandable is the text?'", "labels": [], "entities": []}, {"text": "This is a hard and unnatural task for most naive users, and can be nontrivial even for experts: raters have to reflect on and differentiate between detailed, linguistic aspects of text quality, and assign scores precisely and systematically across a set of generated outputs that potentially contain various types of linguistic defects.", "labels": [], "entities": []}, {"text": "The rating task turns increasingly difficult if they have to compare texts with multiple sentences and multiple types of linguistic defects, e.g. fluency on the sentence level, clarity and coherence on the text level.", "labels": [], "entities": [{"text": "clarity", "start_pos": 177, "end_pos": 184, "type": "METRIC", "confidence": 0.987846314907074}]}, {"text": "Consequently, low agreement between raters, and even inconsistencies between ratings of the same human judge have been found in previous studies).", "labels": [], "entities": [{"text": "agreement", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.981261134147644}]}, {"text": "Standard evaluation methods for, e.g. text summarisation tend to avoid possible interactions between local sentence-level and global text-level defects.", "labels": [], "entities": [{"text": "text summarisation", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7415074706077576}]}, {"text": "Instead, they focus on coherence and content).", "labels": [], "entities": []}, {"text": "In particular, this is due to the fact that independently rating coherence and clarity locally for each sentence and globally for an entire text is tedious, unnatural, tiring and hardly achievable for human judges.", "labels": [], "entities": [{"text": "clarity", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9727884531021118}]}, {"text": "In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading.", "labels": [], "entities": [{"text": "assess human text reading", "start_pos": 153, "end_pos": 178, "type": "TASK", "confidence": 0.5860039815306664}]}, {"text": "In particular, psycholinguistic approaches typically use objective measures such as reading times and eye movements to quantify how well human readers can process a sentence.", "labels": [], "entities": []}, {"text": "The advantage of these measures is that humans typically focus on reading the text.", "labels": [], "entities": []}, {"text": "Importantly, they do not consciously control their eye movements.", "labels": [], "entities": []}, {"text": "Longer reading times or certain patterns of eye movements have been well associated with difficulties that humans encounter when reading text, e.g. apparent inconsistencies as garden path sentences), and complex grammatical constructs ().", "labels": [], "entities": []}, {"text": "This paper investigates whether more objective reading measures can be exploited for evaluating NLG systems and systematically measuring text quality.", "labels": [], "entities": []}, {"text": "However, using eye tracking for evaluation purposes is more costly than relying on ratings.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.736797571182251}]}, {"text": "Furthermore, most eye tracking studies used carefully designed stimuli to test a specific effect at a particular known position in a sentence.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.7426303327083588}]}, {"text": "In sum, eye tracking is highly sensitive to pro-cessing difficulties.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.8170889317989349}]}, {"text": "But due the costly devices and experiments, it was -to the best of our knowledge -not applied for evaluating comparably uncontrolled texts that are typical in NLG.", "labels": [], "entities": []}, {"text": "Thus, we have developed and tested mouse contingent reading (MCR) for evaluating generated texts.", "labels": [], "entities": []}, {"text": "This method combines the sensitivity of eye tracking with the cost effectiveness of a rating study.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.7892639338970184}]}, {"text": "The automatically generated texts are presented to human raters in a sentence-by-sentence, mouse-contingent way such that a number of parameters of the reading process are recorded, e.g. the time that people spent looking at single sentences and an entire text.", "labels": [], "entities": []}, {"text": "We hypothesized that these parameters are more informative for the quality of a text than the user ratings of clarity and fluency.", "labels": [], "entities": [{"text": "clarity", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9978972673416138}]}, {"text": "As objective criteria for text quality are hardly available in NLG (), we did not compare reading times and ratings on manual, potentially flawed annotations of text quality.", "labels": [], "entities": []}, {"text": "Instead, we selected experimental material from a corpus-based generation framework that combines sentence-level linearisation and text-level referring expression generation (.", "labels": [], "entities": [{"text": "text-level referring expression generation", "start_pos": 131, "end_pos": 173, "type": "TASK", "confidence": 0.5973688215017319}]}, {"text": "We based our study on a set of texts that were available in 3 versions: (i) the \"gold standard\" corpus text, (ii) automatically linearised texts where word order deviated from the original corpus and contained potential fluency-related defects, (iii) texts with potential defects in referring expressions and linearisation which are likely to deteriorate clarity or coherence on the discourse level.", "labels": [], "entities": []}, {"text": "We controlled the broad type of linguistic defects but not the details of each sentence or text.", "labels": [], "entities": []}, {"text": "We argue that an objective evaluation method for NLG should clearly distinguish coherence and surfacerelated aspects of text quality.", "labels": [], "entities": []}, {"text": "In our data, there is a single human-authored version of each text which is free of errors.", "labels": [], "entities": []}, {"text": "We do not know whether a deviation of the other versions is an error or an acceptable alternative realisation.", "labels": [], "entities": []}, {"text": "Thus, in contrast to typical eye tracking studies we do not aim at detecting the effect of a particular type of error.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.717945784330368}]}, {"text": "Our assumption is more conservative: we expect that a set of automatically generated texts that deviates significantly from a set of corpus texts on several levels of linguistic realisation (referring expressions and linearisation) has lower quality than texts that only deviate from the corpus on a single level (linearisation).", "labels": [], "entities": []}, {"text": "To further accommodate for the fact that we do not control the exact degree of acceptability of the potential defects, we add a set of filler texts that we manually manipulated to contain severe errors in coherence.", "labels": [], "entities": []}, {"text": "Based on the human ratings and MCR data collected fora set of automatically generated texts, we investigated whether a regression model can predict which types of linguistic defects were present in the text read by the participant, i.e. which generation components were used to generate it.", "labels": [], "entities": []}, {"text": "We find that it is possible to achieve a good prediction accuracy for text quality, despite the fact that there is uncertainty with respect to the exact number and types of errors in the texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9718174934387207}]}, {"text": "However, the accuracy of the regression models varies considerably according to the type of predictors: Human ratings can hardly discriminate incoherent automatically generated texts from original corpus texts and texts containing defects in word order.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.99592125415802}]}, {"text": "A regression model based on reading time predictors achieved a very good fit and largely outperformed the rating model in separating different levels of quality in NLG output.", "labels": [], "entities": []}, {"text": "This suggests that some effects were not reliably reflected in the subjective ratings that are consciously controlled and calculated by the participants.", "labels": [], "entities": []}, {"text": "However, these effects were accounted for by the objective reading measures that are (mostly) outside of conscious control.", "labels": [], "entities": []}, {"text": "Section 2 provides background on research in NLG evaluation.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.9067556262016296}]}, {"text": "Section 3 introduces our MCR paradigm.", "labels": [], "entities": []}, {"text": "The generation framework we used to collect our experimental material is presented in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 describes the experimental design.", "labels": [], "entities": []}, {"text": "The models are discussed in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In recent years, the NLG community has become increasingly interested in comparative evaluation between NLG systems ().", "labels": [], "entities": []}, {"text": "Generally, evaluation methods for assessing NLG systems fall into three main categories: 1) automatic evaluation methods that compare system output against one or multiple reference texts, 2) human evaluation methods where human readers are asked to judge a text, typically with respect to several criteria.", "labels": [], "entities": []}, {"text": "If the NLG component is embedded in an end-to-end system, such as a dialogue system, 3) extrinsic factors of task success and usefulness of the NLG output can be measured.", "labels": [], "entities": []}, {"text": "For corpus-based NLG components such as surface realisers or referring expression generators, extrinsic factors cannot be assessed, but in this case, reference or gold text outputs are often available.", "labels": [], "entities": []}, {"text": "first suggested to use automatic evaluation measures inspired from methods in machine translation, such as BLEU () or NIST, that measure the n-gram overlap between the system and some reference text, sentence or phrase.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7378999888896942}, {"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9807522892951965}]}, {"text": "The advantage of such automatic and cheap evaluation methods can be enormous.", "labels": [], "entities": []}, {"text": "If tightly integrated in the development cycle of an NLG system, they allow fast and empirically optimised implementation decisions.", "labels": [], "entities": []}, {"text": "In turn, a lot of research on NLG evaluation focussed on defining and validating automatic evaluation measures.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.909895271062851}]}, {"text": "Such a metric is typically considered valid if it correlates well with human judgements of text quality).", "labels": [], "entities": []}, {"text": "However, automatic evaluation measures in NLG still have a range of known conceptual deficits, i.e. they do not reflect appropriateness of content, or meaning ().", "labels": [], "entities": []}, {"text": "Thus, many studies and evaluation challenges in NLG additionally collect human ratings to assess the quality.", "labels": [], "entities": []}, {"text": "Compared to the large body of work on automatic evaluation measures, there has been little research that assessed the validity of human evaluation methods.", "labels": [], "entities": []}, {"text": "provided an extensive discussion of human and automatic evaluation for text quality.", "labels": [], "entities": []}, {"text": "They proposed a Turing-style test where participants are asked to judge whether a text was generated by a computer or written by a human.", "labels": [], "entities": []}, {"text": "showed that higher agreement between human raters can be obtained if they compare two automatically generated texts, instead of assigning scores to texts in isolation.", "labels": [], "entities": [{"text": "agreement", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.974219799041748}]}, {"text": "found that human judges preferred to use continuous rating scales over discrete rating scales.", "labels": [], "entities": []}, {"text": "Siddharthan and Katsos (2012) investigated two offline measures inspired from psycholinguistic studies of sentence processing for assessing text readability, namely magnitude estimation and sentence recall.", "labels": [], "entities": [{"text": "magnitude estimation", "start_pos": 165, "end_pos": 185, "type": "TASK", "confidence": 0.7357017397880554}, {"text": "recall", "start_pos": 199, "end_pos": 205, "type": "METRIC", "confidence": 0.8044837117195129}]}, {"text": "They demonstrate that the sentence recall method did not discriminate well between sentences of differing fluency if sentences were short.", "labels": [], "entities": [{"text": "sentence recall", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.5299522578716278}]}, {"text": "On the other hand, human judgements, did not discriminate well between surface level disfluencies and breakdowns in comprehension.", "labels": [], "entities": []}, {"text": "In mouse contingent reading (MCR), the reader is presented a text on a computer screen.", "labels": [], "entities": [{"text": "mouse contingent reading (MCR)", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.7923992325862249}]}, {"text": "The entire text is covered by a mask or masking pattern.", "labels": [], "entities": []}, {"text": "Only if the reader moves the mouse cursor over a particular section of text, the mask is removed and the text is shown in clear font (see).", "labels": [], "entities": []}, {"text": "This paradigm is equivalent to gaze contingent reading and self-guided reading) but it does not require an eye tracking device or touch sensitive device.", "labels": [], "entities": [{"text": "gaze contingent reading", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6250037749608358}]}, {"text": "However, the same metrics can be collected, i.e. the time spent on each area of interest and the scan path.", "labels": [], "entities": []}, {"text": "shows an example of how the reader transits forth and back between areas of interest and how much time is spent on each area.", "labels": [], "entities": []}, {"text": "This study tested human evaluation methods for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8670110404491425}]}, {"text": "We focussed on the problem of evaluating NLG output formed of multiple sentences and detecting whether the user experienced difficulties in reading and understanding the text.", "labels": [], "entities": []}, {"text": "Participants Thirty-three participants were recruited from the department's participant pool (including students and staff).", "labels": [], "entities": []}, {"text": "They received e5 as well as candy sweets in exchange for their time and effort.", "labels": [], "entities": []}, {"text": "Apparatus The participants were seated in front of atypical office computer screen.", "labels": [], "entities": []}, {"text": "A dedicated Python programme controlled the presentation of the stimuli, recorded the reading times 1 and mouse transitions, and collected the ratings.", "labels": [], "entities": []}, {"text": "The participants interacted with the programme through a standard mouse and keyboard.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Means and SD for ratings, text-based and sentence-based reading times. SD is computed on mean values  per participant and indicates agreement/consistency between participants.", "labels": [], "entities": [{"text": "Means", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9641578197479248}]}, {"text": " Table 3: Hierarchical binary regression for text qual- ity conditions, using different sets of predictors ('RTs'  stands for reading times, 'All RTs' include text and sen- tence reading measures).", "labels": [], "entities": []}, {"text": " Table 4: Sentence-level, text-level, rating-based pre- dictors and their coefficients in final filler, sem and lin  models from", "labels": [], "entities": []}]}