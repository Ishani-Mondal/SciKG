{"title": [{"text": "On the Proper Treatment of Quantifiers in Probabilistic Logic Semantics", "labels": [], "entities": [{"text": "Proper Treatment of Quantifiers in Probabilistic Logic Semantics", "start_pos": 7, "end_pos": 71, "type": "TASK", "confidence": 0.7942558377981186}]}], "abstractContent": [{"text": "As a format for describing the meaning of natural language sentences, probabilistic logic combines the expressivity of first-order logic with the ability to handle graded information in a principled fashion.", "labels": [], "entities": [{"text": "describing the meaning of natural language sentences", "start_pos": 16, "end_pos": 68, "type": "TASK", "confidence": 0.7267916968890599}]}, {"text": "But practical probabilistic logic frameworks usually assume a finite domain in which each entity corresponds to a constant in the logic (domain closure assumption).", "labels": [], "entities": []}, {"text": "They also assume a closed world where everything has a very low prior probability.", "labels": [], "entities": []}, {"text": "These assumptions lead to some problems in the inferences that these systems make.", "labels": [], "entities": []}, {"text": "In this paper, we show how to formulate Textual Entail-ment (RTE) inference problems in probabilistic logic in away that takes the domain closure and closed-world assumptions into account.", "labels": [], "entities": [{"text": "formulate Textual Entail-ment (RTE) inference", "start_pos": 30, "end_pos": 75, "type": "TASK", "confidence": 0.741669454744884}]}, {"text": "We evaluate our proposed technique on three RTE datasets, on a synthetic dataset with a focus on complex forms of quantification, on FraCas and on one more natural dataset.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.8385913372039795}, {"text": "FraCas", "start_pos": 133, "end_pos": 139, "type": "DATASET", "confidence": 0.9372873306274414}]}, {"text": "We show that our technique leads to improvements on the more natural dataset, and achieves 100% accuracy on the synthetic dataset and on the relevant part of FraCas.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9995887875556946}, {"text": "FraCas", "start_pos": 158, "end_pos": 164, "type": "DATASET", "confidence": 0.9730036854743958}]}], "introductionContent": [{"text": "Tasks in natural language semantics are becoming more fine-grained, like Textual Entailment (), Semantic Parsing (, or fine-grained opinion analysis.", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.8281257748603821}, {"text": "opinion analysis", "start_pos": 132, "end_pos": 148, "type": "TASK", "confidence": 0.7045765370130539}]}, {"text": "With the more complex tasks, there has been a renewed interest in phenomena like negation ( or the factivity of embedded clauses -phenomena that used to be standardly handled by logic-based semantics.", "labels": [], "entities": []}, {"text": "identifies the use of broad-coverage lexical resources as one aspect that is crucial to the success of logic-based approaches.", "labels": [], "entities": []}, {"text": "Another crucial aspect is the ability to reason with uncertain, probabilistic information (.", "labels": [], "entities": []}, {"text": "Lexical information typically comes with weights, be it weights of paraphrase rules, confidence ratings of word sense disambiguation systems, or distributional similarity values, and reasoning with such information and finding the overall best interpretation requires the ability to handle weights.", "labels": [], "entities": []}, {"text": "This is possible in the framework of probabilistic logic.", "labels": [], "entities": []}, {"text": "In this paper we do not talk about why one should use probabilistic logic for natural language semantics (we argue for the need for probabilistic logic in previous work) which is summarized in section 2.4), we focus on the how, as it turns out that some practical design properties of probabilistic reasoning systems make it necessary to make changes to the meaning representations.", "labels": [], "entities": [{"text": "natural language semantics", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.6772242983182272}]}, {"text": "One characteristic of practical probabilistic logic frameworks such as Markov Logic () is that they assume a finite domain, in particular they assume that the entities in the domain correspond to the constants mentioned in the set of formulas at hand.", "labels": [], "entities": []}, {"text": "This is the domain closure assumption (DCA), a strong assumption that reduces any inference problem to the propositional case.", "labels": [], "entities": [{"text": "domain closure assumption (DCA)", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.5656547397375107}]}, {"text": "It also has far-reaching consequences on the behavior of a system.", "labels": [], "entities": []}, {"text": "For example, suppose we know that Tweety is a bird that flies: bird(T ) \u2227 f ly(E) \u2227 agent(T, E) (There is a flying event of which Tweety is the agent.", "labels": [], "entities": []}, {"text": "We use this Neo-Davidsonian representation throughout the paper, as it is also produced by the wide-coverage semantic analysis system we use).", "labels": [], "entities": []}, {"text": "Then we can conclude that every bird flies, because by the DCA we are only considering models with a domain of size one.", "labels": [], "entities": [{"text": "DCA", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9634277820587158}]}, {"text": "Of that single entity, we know both that it is a bird and that it flies.", "labels": [], "entities": []}, {"text": "Ina natural language inference setting, such as Textual Entailment, this is not the conclusion we would like to draw.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.6154086589813232}]}, {"text": "So we need to use a nonstandard encoding to ensure that existential and universal quantifiers behave in the way they should.", "labels": [], "entities": []}, {"text": "Another issue that we face is that practical probabilistic logic frameworks usually have to construct all groundings of the given formulas before doing inference.", "labels": [], "entities": []}, {"text": "The closed-world assumption (CWA) -the assumption that nothing is the case unless stated otherwise -helps to keep memory use for this grounding step in check).", "labels": [], "entities": []}, {"text": "However, the CWA comes with inference problems of its own, for example it would let us infer from The sky is blue that No girl is dancing because by the CWA we assume that no entity is dancing unless we were told otherwise.", "labels": [], "entities": []}, {"text": "In this paper, we concentrate on entailment, one of the fundamental criteria of language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7133357226848602}]}, {"text": "We show how to formulate probabilistic logic inference problems for the task of Recognizing Textual Entailment (RTE,) in away that takes the domain closure assumption as well as the closed-world assumption into account.", "labels": [], "entities": [{"text": "formulate probabilistic logic inference", "start_pos": 15, "end_pos": 54, "type": "TASK", "confidence": 0.7800046056509018}, {"text": "Recognizing Textual Entailment (RTE", "start_pos": 80, "end_pos": 115, "type": "TASK", "confidence": 0.8203072309494018}]}, {"text": "We evaluate our approach on three RTE datasets.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8697215914726257}]}, {"text": "The first is a synthetic dataset that exhaustively tests inference performance on sentences with two quantifiers.", "labels": [], "entities": []}, {"text": "We get 100% accuracy on this dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996987581253052}]}, {"text": "We also evaluate on the first section of the FraCas dataset (, a collection of Textual Entailment problems tailored focusing on particular semantic phenomena.", "labels": [], "entities": [{"text": "FraCas dataset", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9783248901367188}]}, {"text": "We restrict our analysis to sentences with determiners that our current system handles (excluding \"few\", \"most\", \"many\" and \"at least\"), and we get 100% accuracy on them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9994596838951111}]}, {"text": "Also, we evaluate on the RTE part of the SICK dataset () and show that our approach leads to improvements.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.8163958489894867}]}], "datasetContent": [{"text": "We evaluate on three RTE datasets.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.8372964560985565}]}, {"text": "The first is a synthetic dataset that exhaustively tests inference performance on sentences with two quantifiers.", "labels": [], "entities": []}, {"text": "The second is the RTE part of the SICK dataset ().", "labels": [], "entities": [{"text": "RTE", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.43688687682151794}, {"text": "SICK dataset", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8266749978065491}]}, {"text": "The third is FraCas).", "labels": [], "entities": [{"text": "FraCas", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9603425860404968}]}, {"text": "We automatically generate an RTE dataset that exhaustively test inferences on sentences with two quantifiers.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.7516883909702301}]}, {"text": "Each RTE pair (T , H) is generated following this format: \u2022 L t2 , L h2 \u2208 {food, delicious food} and L t2 = L h2 \u2022 R t2 , R h2 = eat Informally, the dataset has all possible combinations of sentences with two quantifiers.", "labels": [], "entities": []}, {"text": "Also it has all possible combinations of monotonicity directions -upward and downward -between L t1 and L h1 and between L t2 and L h2 . The dataset size is 1,024 RTE pairs.", "labels": [], "entities": []}, {"text": "Here is an example of a generated RTE pair: T: No man eats all food H: Some hungry men eat not all delicious food The dataset is automatically annotated for entailment decisions by normalizing the logical forms of the sentences and then using standard monotonicy rules on the bodies and restrictors of the quantifiers.", "labels": [], "entities": []}, {"text": "72 pairs out of the 1,024 are entailing pairs, and the rest are non-entailing.", "labels": [], "entities": []}, {"text": "Our system computes P (H|T ).", "labels": [], "entities": []}, {"text": "The resulting probability between 0 and 1 needs to be mapped to an Entail/Non-entail decision.", "labels": [], "entities": []}, {"text": "In this dataset, and because we do not have weighted inference rules, all output probabilities close to 1 denote Entail and probabilities close to 0 denote Non-entail.: Results of all datasets on different configurations of the system.", "labels": [], "entities": [{"text": "Entail", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9589920043945312}]}, {"text": "The most common class baseline is Non-entail for the synthetic dataset, Neutral for SICK and Entail for FraCas.", "labels": [], "entities": [{"text": "FraCas", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.9329490661621094}]}, {"text": "False positives (FP) and False negatives (FN) statistic are reported only for the synthetic dataset because it is a binary classification task.", "labels": [], "entities": [{"text": "False positives (FP)", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.955417799949646}, {"text": "False negatives (FN) statistic", "start_pos": 25, "end_pos": 55, "type": "METRIC", "confidence": 0.9635347425937653}]}, {"text": "FP/FN results are counts out of 1,024.", "labels": [], "entities": [{"text": "FP/FN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.49552589654922485}]}, {"text": "Results The leftmost part of summarizes the results on the synthetic dataset in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9994062185287476}]}, {"text": "The baseline always judges non-entailment.", "labels": [], "entities": []}, {"text": "Ablation tests are as follows.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9900496006011963}]}, {"text": "Skolem is a system that applies Skolemization to existentially quantified variables in the Text T (Sec. 3.1) but none of the other adaptations of Section 3.", "labels": [], "entities": []}, {"text": "Existence is a system that makes the existence assumption for universal quantifiers in T (Sec. 3.2).", "labels": [], "entities": []}, {"text": "(\u2200 in H) is constant introduction for universal quantifiers in the Hypothesis (Sec. 3.3).", "labels": [], "entities": []}, {"text": "Finally, CWA is a system that handles negation in the Hypothesis H in away that takes the closed-world assumption into account (Sec. 3.4).", "labels": [], "entities": []}, {"text": "The results in show the importance of each part of the proposed system.", "labels": [], "entities": []}, {"text": "Skolemization and the Existence assumption eliminate some false negatives from missing constants.", "labels": [], "entities": []}, {"text": "All false positives are eliminated when constants are introduced for universal quantifiers in the Hypothesis (\u2200 in H) and when the effects of the closed-world assumption are counteracted (CWA).", "labels": [], "entities": []}, {"text": "The full system achieves 100% accuracy, showing that our formulation is perfectly handling the DCA and CWA on these complex quantified sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9995098114013672}, {"text": "DCA", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.7117545008659363}]}, {"text": "Note that a pure logic system such as Boxer ( can also achieve 100% on this dataset.", "labels": [], "entities": [{"text": "Boxer", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.8658686280250549}]}, {"text": "But again, the aim of this paper is not to argue that probabilistic logic is preferable to standard first order logic.", "labels": [], "entities": []}, {"text": "This can only be done by showing that weighted, uncertain knowledge leads to better inferences.", "labels": [], "entities": []}, {"text": "Rather, this paper constitutes a necessary prerequisite, in that it is necessary first to determine how to achieve the correct (hard) inferences with probabilistic logic before we measure the effect of weighting.", "labels": [], "entities": []}, {"text": "Sentences Involving Compositional Knowledge (SICK,) is an RTE dataset collected for the SemEval 2014 competition.", "labels": [], "entities": [{"text": "Sentences Involving Compositional Knowledge (SICK", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7756768961747488}, {"text": "RTE dataset collected", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.8880873719851176}, {"text": "SemEval 2014 competition", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.641520102818807}]}, {"text": "It consists of 5,000 T/H pairs for training and 5,000 for testing.", "labels": [], "entities": []}, {"text": "Pairs are annotated for both RTE and STS (sentence similarity).", "labels": [], "entities": [{"text": "RTE", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9444375038146973}, {"text": "STS", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9907388687133789}]}, {"text": "For the purpose of this paper, we only use the RTE annotation.", "labels": [], "entities": [{"text": "RTE", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7011671662330627}]}, {"text": "Pairs in SICK (as well as FraCas in the next section) are classified into three classes, Entailment, Contradiction, and Neutral.", "labels": [], "entities": [{"text": "SICK", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.6997637748718262}, {"text": "FraCas", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.8944616913795471}]}, {"text": "This means that computing the probability P (H|T ) alone is not enough for this threeway classification.", "labels": [], "entities": [{"text": "threeway classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.5942641645669937}]}, {"text": "We additionally compute P (\u00acT |H).", "labels": [], "entities": []}, {"text": "Entailing pairs have P (H|T ) \u2248 1, P (\u00acT |H) \u2248 0, Contradicting pairs have P (H|T ) \u2248 0, P (\u00acT |H) \u2248 1, and Neutral pairs have P (H|T ) \u2248 P (\u00acT |H).", "labels": [], "entities": []}, {"text": "We use these two conditional probabilities as input features to an SVM classifier trained on the training set to mapped them to an Entail/Neutral/Contradict decision.", "labels": [], "entities": []}, {"text": "Results The middle panel of reports results on the SICK dataset, again in terms of accuracy.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.825279951095581}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9993056058883667}]}, {"text": "Almost all sentences in the SICK dataset are simple existentially quantified sentences except fora few sentences with an outer negation.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.7417280375957489}]}, {"text": "Accordingly, the system with Skolemization basically achieves the same accuracy as when Existence and (\u2200 in H) are added.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9993763566017151}]}, {"text": "Handling negation in H effectively improves the accuracy of our system by reducing the number of false positives resulting from the CWA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.999045193195343}]}, {"text": "FraCas ( 2 is a dataset of hand-built entailments pairs.", "labels": [], "entities": [{"text": "FraCas", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9262183904647827}]}, {"text": "The dataset consists of 9 sections, each of which is testing a different set of phenomena.", "labels": [], "entities": []}, {"text": "For this paper, we use sentences from the first section, which tests quantification and monotonicity.", "labels": [], "entities": []}, {"text": "However, we exclude pairs containing the determiners \"few\", \"most\", \"many\" and \"at least\" because our system does not currently have a representation for them.", "labels": [], "entities": []}, {"text": "We evaluate on 46 pairs out of 74.", "labels": [], "entities": []}, {"text": "Because of that, we cannot compare with previous systems that evaluate on the whole section.", "labels": [], "entities": []}, {"text": "To map sentences to logical form, we use Boxer as discussed in section 2.4.", "labels": [], "entities": []}, {"text": "By default, Boxer relies on C&C () to get the CCG parses of the sentences.", "labels": [], "entities": [{"text": "CCG parses", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.6913924813270569}]}, {"text": "Instead, we run Boxer on CCG parses produced by EasyCCG () because it is more accurate on FraCas.", "labels": [], "entities": [{"text": "Boxer", "start_pos": 16, "end_pos": 21, "type": "TASK", "confidence": 0.7168474793434143}, {"text": "EasyCCG", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9403228759765625}, {"text": "FraCas", "start_pos": 90, "end_pos": 96, "type": "DATASET", "confidence": 0.9686841368675232}]}, {"text": "Like we additionally test on gold-standard parses to be able to evaluate our technique of handling quantifiers in the absence of parser errors.", "labels": [], "entities": []}, {"text": "Also, as we do with the SICK dataset, we add the inference P (\u00acT |H) for the detection of contradictions.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.7806056141853333}]}, {"text": "For multi-sentence examples, we add a simple co-reference resolution step that connects definite NPs across sentences.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7492799162864685}]}, {"text": "Results The rightmost panel in summarizes the results of our system for gold parses and system parses.", "labels": [], "entities": []}, {"text": "We see that the Existence assumption is not needed in this dataset because it is constructed to test semantics and not presupposition.", "labels": [], "entities": []}, {"text": "Results with Existence are lower because without Existence, three cases (the previous example is one of them) are correctly classified as Entail, but with Existence they are classified as Neutral.", "labels": [], "entities": []}, {"text": "Without Existence the domain is empty, and P (\u00acT |H) = 0 because \u00acT , which is existentially quantified, is trivially false.", "labels": [], "entities": []}, {"text": "With Existence added, P (\u00acT |H) = 1 because the domain is not empty, and the CWA is not handled.", "labels": [], "entities": []}, {"text": "Also we see that, as in the two previous experiments, adding the approach that handles the CWA has the biggest impact.", "labels": [], "entities": []}, {"text": "With all components of the system added, and with gold parses, we get 100% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9991819262504578}]}, {"text": "With system parses, all results are lower, but the relative scores for the different subsystems are comparable to the gold parse case.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of all datasets on different configurations of the system. The most common class  baseline is Non-entail for the synthetic dataset, Neutral for SICK and Entail for FraCas. False positives  (FP) and False negatives (FN) statistic are reported only for the synthetic dataset because it is a binary  classification task. FP/FN results are counts out of 1,024.", "labels": [], "entities": [{"text": "FraCas", "start_pos": 182, "end_pos": 188, "type": "DATASET", "confidence": 0.957061231136322}, {"text": "False positives  (FP)", "start_pos": 190, "end_pos": 211, "type": "METRIC", "confidence": 0.9274337530136109}, {"text": "False negatives (FN) statistic", "start_pos": 216, "end_pos": 246, "type": "METRIC", "confidence": 0.9480468332767487}]}]}