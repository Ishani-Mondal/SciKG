{"title": [{"text": "Determining an Optimal Set of Flesh Points on Tongue, Lips, and Jaw for Continuous Silent Speech Recognition", "labels": [], "entities": [{"text": "Continuous Silent Speech Recognition", "start_pos": 72, "end_pos": 108, "type": "TASK", "confidence": 0.6571830585598946}]}], "abstractContent": [{"text": "Articulatory data have gained increasing interest in speech recognition with or without acoustic data.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8678478896617889}]}, {"text": "Electromagnetic ar-ticulograph (EMA) is one of the affordable, currently used techniques for tracking the movement of flesh points on articula-tors (e.g., tongue) during speech.", "labels": [], "entities": []}, {"text": "Determining an optimal set of sensors is important for optimizing the clinical applications of EMA data, due to the inconvenience of attaching sensors on tongue and other intraoral articulators, particularly for patients with neurological diseases.", "labels": [], "entities": []}, {"text": "A recent study found an optimal set (tongue tip and body back, upper and lower lips) on tongue and lips for isolated phoneme, word, or short phrase classification from articulatory movement data.", "labels": [], "entities": [{"text": "isolated phoneme, word, or short phrase classification", "start_pos": 108, "end_pos": 162, "type": "TASK", "confidence": 0.5463806324534946}]}, {"text": "This four-sensor set, however , has not been verified in continuous silent speech recognition.", "labels": [], "entities": [{"text": "continuous silent speech recognition", "start_pos": 57, "end_pos": 93, "type": "TASK", "confidence": 0.5978316515684128}]}, {"text": "In this paper, we investigated the use of data from sensor combinations in continuous speech recognition to verify the finding using a publicly available data set MOCHA-TIMIT.", "labels": [], "entities": [{"text": "continuous speech recognition", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.6391550799210867}, {"text": "MOCHA-TIMIT", "start_pos": 163, "end_pos": 174, "type": "DATASET", "confidence": 0.7460153698921204}]}, {"text": "The long-standing speech recognition approach Gaussian mixture model (GMM)-hidden Markov model (HMM) and a recently available approach deep neural network (DNN)-HMM were used as the recognizers.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7117452025413513}]}, {"text": "Experimental results confirmed that the four-sensor set is optimal out of the full set of sensors on tongue, lips, and jaw.", "labels": [], "entities": []}, {"text": "Adding upper incisor and/or velum data further improved the recognition performance slightly.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the availability of affordable devices for tongue movement data collection, articulatory data have obtained interest not only in speech science but also in speech technology (i.e., automatic speech recognition).", "labels": [], "entities": [{"text": "tongue movement data collection", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.645055741071701}, {"text": "automatic speech recognition)", "start_pos": 186, "end_pos": 215, "type": "TASK", "confidence": 0.7832950353622437}]}, {"text": "First, articulatory data have been successfully used to improve the speech recognition accuracy.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8290211856365204}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9413214921951294}]}, {"text": "Articulatory data are particularly useful when speech signals are noisy or low quality for recognizing dysarthric speech.", "labels": [], "entities": []}, {"text": "Second, when acoustic data is not available, a silent speech interface (SSI) based on articulatory data has potential clinical applications.", "labels": [], "entities": []}, {"text": "An SSI recognizes speech from articulatory data only (without using audio data) and then drives a text-to-speech synthesizer for sound playback.", "labels": [], "entities": []}, {"text": "For example, SSIs can be used to assist the oral communication for patients with severe voice disorders or without the ability to produce speech sounds (e.g., due to laryngectomy, a surgical removal of larynx due to treatment of laryngeal cancer).", "labels": [], "entities": []}, {"text": "There are currently limited options to assist speech communication for those individuals (e.g., esophageal speech, tracheo-esophageal speech or tracheo-esophageal puncture (TEP) speech, and electrolarynx).", "labels": [], "entities": [{"text": "speech communication", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7612183094024658}]}, {"text": "These approaches, however, produce an abnormal sounding voice, which impacts the quality of life of laryngectomees.", "labels": [], "entities": []}, {"text": "Current text-to-speech technologies have been able to produce speech with natural sounding voice for SSIs.", "labels": [], "entities": [{"text": "SSIs", "start_pos": 101, "end_pos": 105, "type": "TASK", "confidence": 0.95086669921875}]}, {"text": "One of the current challenges of SSI development is silent speech recognition algorithms (without using audio data) or mapping articulatory information to speech.", "labels": [], "entities": [{"text": "SSI development", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.9396399259567261}, {"text": "silent speech recognition", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6438074807325999}]}, {"text": "Electromagnetic motion tracking is one of the affordable, currently used technologies for tracking tongue movement during speech.", "labels": [], "entities": [{"text": "Electromagnetic motion tracking", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6092124283313751}]}, {"text": "There are currently two commercially available devices, EMA AG series (by Carstens) and Wave system (by NDI, Inc.).", "labels": [], "entities": []}, {"text": "Tongue tracking using electromagnetic devices is accomplished through attaching small sensors on the surface of tongue and other articulators.", "labels": [], "entities": [{"text": "Tongue tracking", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8490987420082092}]}, {"text": "In prior work, the number of tongue sensors and their locations have been justified based on long-standing assumptions about tongue movement patterns in classic phonetics, or the specific purpose of the study.", "labels": [], "entities": []}, {"text": "Other techniques that have been used to record non-audio articulatory information include ultrasound, and surface electromyography (EMG).", "labels": [], "entities": []}, {"text": "Determining an optimal set of tongue sensors for speech production is significant for both science and technology.", "labels": [], "entities": [{"text": "speech production", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7398721575737}]}, {"text": "Scientifically, determining an optimal set of sensors can improve the understanding of the coordination of articulators for speech production.", "labels": [], "entities": [{"text": "speech production", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7185748666524887}]}, {"text": "Technologically, it can be helpful for clinical applications including (1) silent speech interfaces, (2) speech recognition with articulatory information, and (3) speech training using real-time visual feedback of tongue movements.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7764421105384827}]}, {"text": "In literature, three or four EMA sensors on the tongue have been commonly used (e.g.,).", "labels": [], "entities": []}, {"text": "The use of more sensors than necessary comes at a cost for both researchers and subjects; the procedure for attaching sensors to the tongue is time intensive and can cause discomfort and therefore may limit the scope of EMA for practical use, particularly for persons with neurological diseases (e.g., Parkinson's disease and amyotrophic lateral sclerosis).", "labels": [], "entities": []}, {"text": "Here, optimal set means a sensor set that contains the least number of sensors that performs no worse than other sets with more sensors.", "labels": [], "entities": []}, {"text": "There maybe more than one optimal set with the same number of sensors.", "labels": [], "entities": []}, {"text": "Until recently, a study found two tongue sensors (Tongue Tip and Tongue Body Back) and two lip sensors (Upper Lip and Lower Lip) are optimal for isolated phoneme (vowels and consonants), word, and short phrase classification.", "labels": [], "entities": [{"text": "short phrase classification", "start_pos": 197, "end_pos": 224, "type": "TASK", "confidence": 0.5916314721107483}]}, {"text": "The classification results based on data using the optimal set were not significantly different from these based on data from the full set with four tongue sensors (Tongue Tip, Tongue Blade, Tongue Body Front, and Tongue Body Back) plus the two lip sensors.", "labels": [], "entities": []}, {"text": "However, this set has not been verified in continuous silent speech recognition or speech recognition from both acoustic and articulatory data.", "labels": [], "entities": [{"text": "continuous silent speech recognition", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.6068584620952606}, {"text": "speech recognition", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7520220577716827}]}, {"text": "If the two-tongue-sensor set can be confirmed for continuous speech recognition, it would be beneficial for future collection of a larger articulatory data set.", "labels": [], "entities": [{"text": "continuous speech recognition", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6401160061359406}]}, {"text": "Other studies compared the whole tongue and lips (e.g., using ultrasound and optical data), but not on flesh points.", "labels": [], "entities": []}, {"text": "In this paper, we investigated the optimal set of tongue sensors for speaker-dependent continuous silent speech recognition (using articulatory data only) and speech recognition (using combined acoustic and articulatory data).", "labels": [], "entities": [{"text": "speaker-dependent continuous silent speech recognition", "start_pos": 69, "end_pos": 123, "type": "TASK", "confidence": 0.6225199818611145}, {"text": "speech recognition", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.8138033747673035}]}, {"text": "The goals were (1) to confirm if more than two tongue sensors are unnecessary for continuous silent speech recognition and speech recognition using both acoustic and articulatory data when only tongue and lips are used, and (2) to provide a reference for choosing the number of sensors and their locations on the tongue, lips, jaw and other articulators for future studies.", "labels": [], "entities": [{"text": "continuous silent speech recognition", "start_pos": 82, "end_pos": 118, "type": "TASK", "confidence": 0.6402327790856361}, {"text": "speech recognition", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.765540212392807}]}, {"text": "However, due to the space limitation, this paper did not verify if the hypothesized optimal four-sensor set is unique.", "labels": [], "entities": []}, {"text": "The articulatory and acoustic data in the MOCHA-TIMIT data set were used in this experiment.", "labels": [], "entities": [{"text": "MOCHA-TIMIT data set", "start_pos": 42, "end_pos": 62, "type": "DATASET", "confidence": 0.964718242486318}]}, {"text": "The MOCHA-TIMIT data set is appropriate for this study because it contains data collected from sensors attached on multiple articulators, including three sensors on the tongue, two on the lips, two on the incisors, and one on the velum.", "labels": [], "entities": [{"text": "MOCHA-TIMIT data set", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8497766455014547}]}, {"text": "In addition, both MOCHA-TIMIT and the data set in have tongue tip and body back (or dorsum).", "labels": [], "entities": [{"text": "MOCHA-TIMIT", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.5697905421257019}]}, {"text": "Thus the first goal of this paper became to verify if the tongue blade sensor is unnecessary in addition to the hypothesized optimal set.", "labels": [], "entities": []}, {"text": "The traditional speech recognition approach Gaussian mixture model (GMM)-hidden Markov model (HMM) and a recently available and promising approach deep neural network (DNN)-HMM were used.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7294287383556366}]}], "datasetContent": [{"text": "Data from individual sensors or combinations of sensors were used in speech recognition experiments (from articulatory data only or from combined acoustic and articulatory data).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7862146496772766}]}, {"text": "The recognition performances obtained from individual sensors or their combinations were compared to determine (1) if Tongue Blade was unnecessary in addition to the other two tongue sensors and lips (Tongue Tip, Tongue Dorsum, Upper Lip, and Lower Lip), and (2) if the performance improved when more sensor's data (e.g., upper incisor and velum) were added.", "labels": [], "entities": []}, {"text": "In each experiment, a 5-fold cross validation strategy with a jackknife procedure was performed to set training and test sets in the experiment.", "labels": [], "entities": []}, {"text": "In each of the five executions, a group of 92 sentences were selected for test with the remaining 368 sentences for training.", "labels": [], "entities": []}, {"text": "Due to the high degree of variation in the articulation across speakers and there were only two speakers in MOCHA-TIMIT, speaker-dependent recognition was conducted.", "labels": [], "entities": [{"text": "MOCHA-TIMIT", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.8035033345222473}, {"text": "speaker-dependent recognition", "start_pos": 121, "end_pos": 150, "type": "TASK", "confidence": 0.7109479010105133}]}, {"text": "The average training data length for each cross validation became 21.3 mins (368 sentences) for the female speaker and 20.6 mins (368 sentences) for the male speaker.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.6083660125732422}]}, {"text": "The average test data length along 5 cross validations was 5.3 mins (92 sentences) for the female speaker and 5.2 mins (92 sentences) for the male speaker, respectively.", "labels": [], "entities": []}, {"text": "Articulatory features were extracted from the corpus using EMAtools.", "labels": [], "entities": []}, {"text": "The original articulatory features and their first and second derivatives were concatenated to build various dimensional feature vectors for each set of sensors.", "labels": [], "entities": []}, {"text": "The \"breath\" segments were merged with \"silence\" for both training and testing.", "labels": [], "entities": []}, {"text": "The input features in DNN were a concatenation of articulatory feature vectors (number of sensors \u00d7 2-dimension articulatory movement data + \u2206 + \u2206\u2206) with 9.", "labels": [], "entities": []}, {"text": "Mel-frequency cepstral coefficients (MFCCs) were extracted from the acoustic data and used as the acoustic features in the recognition experiments.", "labels": [], "entities": []}, {"text": "The GMM-HMM system was trained using maximum likelihood estimation (MLE) without using segment information provided in MOCHA-TIMIT corpus (flat initialization).", "labels": [], "entities": [{"text": "MOCHA-TIMIT corpus", "start_pos": 119, "end_pos": 137, "type": "DATASET", "confidence": 0.832369476556778}]}, {"text": "The DNN-HMM system was pre-trained using contrastivedivergence algorithm on RBMs and fine-tuned using backpropagation algorithm.", "labels": [], "entities": []}, {"text": "A bi-gram phoneme language model was trained using all 44 phonemes provided in label files of the corpus.", "labels": [], "entities": []}, {"text": "lists the details of the experimental setup and major parameters in GMM-HMM and DNN-HMM.", "labels": [], "entities": [{"text": "GMM-HMM", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9449117183685303}, {"text": "DNN-HMM", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.8852357268333435}]}, {"text": "The training and decoding were performed using the Kaldi speech recognition toolkit.", "labels": [], "entities": [{"text": "Kaldi speech recognition", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.5921235680580139}]}, {"text": "A phoneme error rate (PER) was used as a performance measure, which is the ratio of the sum of the number of errors over the total number of phonemes.", "labels": [], "entities": [{"text": "phoneme error rate (PER)", "start_pos": 2, "end_pos": 26, "type": "METRIC", "confidence": 0.8251827955245972}]}, {"text": "The PER is represented by where S represents the number of substitution errors, Dis the number of deletion errors, I stands for the number of insertion errors, and N is the total number of phonemes in the test set.", "labels": [], "entities": [{"text": "PER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9643439054489136}, {"text": "I", "start_pos": 115, "end_pos": 116, "type": "METRIC", "confidence": 0.9535307884216309}]}, {"text": "For DNN, we conducted experiments using 1 to 6 hidden layers and the best performance was reported.", "labels": [], "entities": []}, {"text": "Finally, the PERs from each test group in the 5-fold cross validation were averaged as the overall PER.", "labels": [], "entities": [{"text": "PERs", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9901109337806702}, {"text": "PER", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9917334318161011}]}], "tableCaptions": [{"text": " Table 2: Phoneme Error Rates (PER; %) obtained from sensor combination {TT, TD, UL, LL} and {TT, TB, TD, UL, LL}.", "labels": [], "entities": [{"text": "Phoneme Error Rates (PER", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6871741473674774}]}]}