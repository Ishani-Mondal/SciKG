{"title": [{"text": "Argument Mining: Extracting Arguments from Online Dialogue", "labels": [], "entities": [{"text": "Argument Mining: Extracting Arguments from Online Dialogue", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7033016979694366}]}], "abstractContent": [{"text": "Online forums are now one of the primary venues for public dialogue on current social and political issues.", "labels": [], "entities": []}, {"text": "The related corpora are often huge, covering any topic imaginable.", "labels": [], "entities": []}, {"text": "Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic.", "labels": [], "entities": []}, {"text": "We frame this goal as consisting of two tasks: argument extraction and argument facet similarity.", "labels": [], "entities": [{"text": "argument extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7329211831092834}]}, {"text": "We focus hereon the argument extraction task, and show that we can train regressors to predict the quality of extracted arguments with RRSE values as low as .73 for some topics.", "labels": [], "entities": [{"text": "argument extraction", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7790861427783966}, {"text": "RRSE", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9955891370773315}]}, {"text": "A secondary goal is to develop re-gressors that are topic independent: we report results of cross-domain training and domain-adaptation with RRSE values for several topics as low as .72, when trained on topic independent features.", "labels": [], "entities": [{"text": "RRSE", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9943222999572754}]}], "introductionContent": [{"text": "Online forums are now one of the primary venues for public dialogue on current social and political issues.", "labels": [], "entities": []}, {"text": "The related corpora are often huge, covering any topic imaginable, thus providing novel opportunities to address a number of open questions about the structure of dialogue.", "labels": [], "entities": []}, {"text": "Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic.", "labels": [], "entities": []}, {"text": "We build anew dataset of 109,074 posts on the topics gay marriage, gun control, death penalty and evolution.", "labels": [], "entities": [{"text": "gun control", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.7633635103702545}]}, {"text": "We frame our problem as consisting of two separate tasks: \u2022 Argument Extraction: How can we extract argument segments in dialogue that clearly express a particular argument facet?", "labels": [], "entities": [{"text": "Argument Extraction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8954080045223236}]}, {"text": "\u2022 Argument Facet Similarity: How can we recognize that two argument segments are semantically similar, i.e. about the same facet of the argument?", "labels": [], "entities": [{"text": "Argument Facet Similarity", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.7011145651340485}]}, {"text": "Parent Post P, Response R P1: A person should be executed for kicking a dog?", "labels": [], "entities": [{"text": "Parent Post P", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8807880083719889}]}, {"text": "Your neurologically imbalanced attitude is not only worrying, it is psychopathic.", "labels": [], "entities": []}, {"text": "How would you prove guilt on somebody who 'kicked a dog'?", "labels": [], "entities": []}, {"text": "And, in what way, is kicking a dog so morally abhorrant as to warrant a death sentence for the given act?", "labels": [], "entities": []}, {"text": "R1: Obviously you have issues.", "labels": [], "entities": []}, {"text": "Any person who displays such a weakness of character cannot be allowed to contaminate the gene pool any further.", "labels": [], "entities": []}, {"text": "Therefore, they must be put down.", "labels": [], "entities": []}, {"text": "If a dog bit a human, they would be put down, so why not do the same to a human?", "labels": [], "entities": []}, {"text": "P2: So then you will agree that evolution is useless in getting at possible answers on what really matters, how we got here?", "labels": [], "entities": []}, {"text": "If you concede that then I'm happy to end this discussion.", "labels": [], "entities": []}, {"text": "I recall, however, visiting the Smithsonian and seeing a detailed description of how amino acids combined to form the building blocks of life.", "labels": [], "entities": []}, {"text": "Evolutionary theory does address origins and its explanations are unsuppported by evidence.", "labels": [], "entities": [{"text": "Evolutionary theory", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7106323838233948}]}, {"text": "R2: No, and no.", "labels": [], "entities": []}, {"text": "First, evolution provides the only scientific answers for how humans got here: we evolved from non-human ancestors.", "labels": [], "entities": []}, {"text": "That record is written in both the genes and the fossils.", "labels": [], "entities": []}, {"text": "Science might even be able eventually to tell you what the forces of selection were that propelled this evolution.", "labels": [], "entities": []}, {"text": "P3: Do you have any idea how little violent crime involves guns?", "labels": [], "entities": []}, {"text": "the US has violance problems, how about trying to controle the violance, not the tools.", "labels": [], "entities": [{"text": "violance", "start_pos": 11, "end_pos": 19, "type": "TASK", "confidence": 0.9851009249687195}]}, {"text": "R3: But most murders are committed with guns.", "labels": [], "entities": [{"text": "R3", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6016821265220642}]}, {"text": "So if you think it's important to reduce the murder rate, I don't think that guns can be ignored.", "labels": [], "entities": [{"text": "murder rate", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.7119099348783493}]}, {"text": "P4: Another lie used by people that want to ban guns.", "labels": [], "entities": [{"text": "P4", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8655338883399963}]}, {"text": "Guns as cars were invented to do what the owner uses them for!", "labels": [], "entities": []}, {"text": "There is no difference in them.", "labels": [], "entities": []}, {"text": "It takes a person to make them dangerous.", "labels": [], "entities": []}, {"text": "R4: But guns were made specifically to kill people.", "labels": [], "entities": [{"text": "R4", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7196788191795349}]}, {"text": "Cars were made to get a person from point A to B.", "labels": [], "entities": []}, {"text": "When someone kills a person with a car, it's an accident.", "labels": [], "entities": []}, {"text": "When someone kills a person with a gun, it's on purpose.", "labels": [], "entities": []}, {"text": "Consider for example the sample posts and responses in.", "labels": [], "entities": []}, {"text": "Argument segments that are good targets for argument extraction are indicated, in their dialogic context, in bold.", "labels": [], "entities": [{"text": "argument extraction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.806612104177475}]}, {"text": "Given extracted segments, the argument facet similarity module should recognize that R3 and R4 paraphrase the same argument facet, namely that there is a strong relationship between the availability of guns and the murder rate.", "labels": [], "entities": []}, {"text": "This paper addresses only the argument extraction task, as an important first step towards producing argument summaries that reflect the range and type of arguments being made, on a topic, overtime, by citizens in public forums.", "labels": [], "entities": [{"text": "argument extraction task", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.783164362112681}]}, {"text": "Our approach to the argument extraction task is driven by a novel hypothesis, the IMPLICIT MARKUP hypothesis.", "labels": [], "entities": [{"text": "argument extraction task", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.8664328853289286}, {"text": "IMPLICIT MARKUP hypothesis", "start_pos": 82, "end_pos": 108, "type": "METRIC", "confidence": 0.8101836840311686}]}, {"text": "We posit that the arguments that are good candidates for extraction will be marked by cues (implicit markups) provided by the dialog conversants themselves, i.e. their choices about the surface realization of their arguments.", "labels": [], "entities": []}, {"text": "We examine a number of theoretically motivated cues for extraction, that we expect to be domain-independent.", "labels": [], "entities": [{"text": "extraction", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.9597185254096985}]}, {"text": "We describe how we use these cues to sample from the corpus in away that lets us test the impact of the hypothesized cues.", "labels": [], "entities": []}, {"text": "Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing.", "labels": [], "entities": [{"text": "argument extraction", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7486234307289124}]}, {"text": "Argument extraction resembles the sentence extraction phase of multi-document summarization.", "labels": [], "entities": [{"text": "Argument extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7491406798362732}, {"text": "sentence extraction", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7163870632648468}, {"text": "multi-document summarization", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.5857224762439728}]}, {"text": "Facet similarity resembles semantic textual similarity and paraphrase recognition (.", "labels": [], "entities": [{"text": "Facet similarity", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.810001939535141}, {"text": "paraphrase recognition", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7876770496368408}]}, {"text": "Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.8282584547996521}]}, {"text": "2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments.", "labels": [], "entities": []}, {"text": "We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY.", "labels": [], "entities": [{"text": "ARGUMENT", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.8752179741859436}, {"text": "QUALITY", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.486747145652771}]}, {"text": "3.2 describes experiments to test whether: (1) we can predict argument quality; (2) our hypothesized cues are good indicators of argument quality; and (3) an argument quality predictor trained on one topic or a set of topics can be used on unseen topics.", "labels": [], "entities": []}, {"text": "show that we can predict argument quality with RRSE values as low as .73 for some topics.", "labels": [], "entities": [{"text": "RRSE", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9954997897148132}]}, {"text": "Cross-domain training combined with domainadaptation yields RRSE values for several topics as low as .72, when trained on topic independent features, however some topics are much more difficult.", "labels": [], "entities": [{"text": "RRSE", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9906929135322571}]}, {"text": "We provide a comparison of our work to previous research and sum up in Sec.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Overview of the corpus and Argument Quality (AQ) annotation results.", "labels": [], "entities": [{"text": "Argument Quality (AQ) annotation", "start_pos": 37, "end_pos": 69, "type": "METRIC", "confidence": 0.9382064541180929}]}, {"text": " Table 4: The performance of in domain training  for three regression algorithms.", "labels": [], "entities": []}, {"text": " Table 5: The RRSE for in-domain training on each of the feature sets. Darker values denote better  scores. SEL=Feature Selection, LEX=Lexical, LNG=Lexical N-Grams, !LNG=Everything but LNG,  SPTL=Speciteller, SLEN=Sentence Length, WLEN=Word Length, SYN=Syntactic, DIS=Discourse,  PNG=Part-Of-Speech N-Grams, SPFC=Specific, AGG=Aggregate. XX ALL indicates training on data  from all topics and testing on the XX topic.", "labels": [], "entities": [{"text": "RRSE", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9767431616783142}, {"text": "SEL", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9927895665168762}, {"text": "ALL", "start_pos": 341, "end_pos": 344, "type": "METRIC", "confidence": 0.6982718110084534}]}, {"text": " Table 6: The RMSE for the best performing model  in each domain given instances whose predicted  quality value is in the given percentile.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.49612560868263245}]}]}