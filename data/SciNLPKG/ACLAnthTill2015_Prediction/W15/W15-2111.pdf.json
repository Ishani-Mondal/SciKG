{"title": [{"text": "Using Parallel Texts and Lexicons for Verbal Word Sense Disambiguation", "labels": [], "entities": [{"text": "Verbal Word Sense Disambiguation", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6539355516433716}]}], "abstractContent": [{"text": "We present a system for verbal Word Sense Disambiguation (WSD) that is able to exploit additional information from parallel texts and lexicons.", "labels": [], "entities": [{"text": "verbal Word Sense Disambiguation (WSD)", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.7689964984144483}]}, {"text": "It is an extension of our previous WSD method (Du\u0161ek et al., 2014), which gave promising results but used only monolingual features.", "labels": [], "entities": [{"text": "WSD", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.913001537322998}]}, {"text": "In the follow-up work described here, we have explored two additional ideas: using English-Czech bilingual resources (as features only-the task itself remains a mono-lingual WSD task), and using a \"hybrid\" approach, adding features extracted both from a parallel corpus and from manually aligned bilingual valency lexicon entries, which contain subcategorization information.", "labels": [], "entities": []}, {"text": "Albeit not all types of features proved useful, both ideas and additions have led to significant improvements for both languages explored.", "labels": [], "entities": []}], "introductionContent": [{"text": "Using parallel data for Word Sense Disambiguation (WSD) is as old as Statistical Machine Translation (SMT): analyze texts in both languages before the IBM SMT models are trained and used, including WSD driven purely by translation equivalents.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.77615158756574}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.7829395035902659}]}, {"text": "1 A combination of parallel texts and lexicons also proved useful for SMT at the time ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9963842630386353}]}, {"text": "In our previous experiments (), we have shown that WSD based on a manually created valency lexicon (for verbs) can achieve encouraging results.", "labels": [], "entities": [{"text": "WSD", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9816951751708984}]}, {"text": "Combining the above ideas and previous findings with parallel data and a manually created bilingual valency lexicon, we have moved to add bilingual features to improve on the previous results on the verbal WSD task.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 206, "end_pos": 214, "type": "TASK", "confidence": 0.862881600856781}]}, {"text": "In addition, we have opted fora new machine learning system, the Vowpal Wabbit toolkit ().", "labels": [], "entities": [{"text": "Vowpal Wabbit toolkit", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.9070962071418762}]}, {"text": "In Section 2, we present the annotation framework and the lexicons used throughout this paper.", "labels": [], "entities": []}, {"text": "Section 3 describes our experiments, Section 4 summarizes relevant previous works and Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are focusing hereon measuring the influence of parallel features on the WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9843302369117737}]}, {"text": "In order to compare our results to our previous work, we use the same training/testing data split, i.e., PCEDT 2.0 Sections 02-21 as training data, Section 24 as development data, and Section 23 as evaluation data, and start from the same set of monolingual features.", "labels": [], "entities": [{"text": "PCEDT 2.0 Sections 02-21", "start_pos": 105, "end_pos": 129, "type": "DATASET", "confidence": 0.8384631127119064}]}, {"text": "We also include Czech monolingual results on PDT 2.5 (default data split) for comparison.", "labels": [], "entities": [{"text": "PDT 2.5", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9184111654758453}]}, {"text": "Unlike our previous work using LibLINEAR logistic regression, we apply Vowpal Wabbit ( for classification.", "labels": [], "entities": [{"text": "Vowpal Wabbit", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.6097497344017029}]}, {"text": "Note that the input to our WSD system is plain text without any annotation, and we only use the gold verb senses from PCEDT/PDT to train the system.", "labels": [], "entities": [{"text": "PCEDT/PDT", "start_pos": 118, "end_pos": 127, "type": "DATASET", "confidence": 0.8455550670623779}]}, {"text": "All required annotation for features as well as word alignment for parallel texts is performed automatically.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7687090337276459}]}, {"text": "We applied the one-against-all cost-sensitive setting of the Vowpal Wabbit linear classifier with label-dependent features.", "labels": [], "entities": [{"text": "Vowpal Wabbit linear classifier", "start_pos": 61, "end_pos": 92, "type": "DATASET", "confidence": 0.9159654229879379}]}, {"text": "8 Feature values are combined with a candidate sense label from the valency lexicon.", "labels": [], "entities": []}, {"text": "If a verb was unseen in the training data or is sense-unambiguous, we used the first or only sense from the lexicon instead of the classifier.", "labels": [], "entities": []}, {"text": "The training data were automatically analyzed from plain word forms up to the PDT/PCEDTstyle deep layer using analysis pipelines implemented in the Treex NLP framework.", "labels": [], "entities": [{"text": "Treex NLP framework", "start_pos": 148, "end_pos": 167, "type": "DATASET", "confidence": 0.9417599439620972}]}, {"text": "The gold-standard sense labels were then projected onto the automatic annotation.", "labels": [], "entities": []}, {"text": "This emulates the real-world scenario where no gold-standard annotation is available.", "labels": [], "entities": []}, {"text": "The monolingual feature set of Du\u0161ek et al.", "labels": [], "entities": []}, {"text": "Based on preliminary experiments on development data sets, we used the following options for training: --passes=4 -b 20 --loss_function=hinge --csoaa_ldf=mc, i.e., 4 passes over the training data, a feature space size of 2 20 , the hinge loss function and cost-sensitive one-against-all multiclass reduction with label-dependent features.", "labels": [], "entities": []}, {"text": "Cf. total accuracy vs. classifier accuracy in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.922242283821106}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9448572397232056}]}, {"text": "The automatic deep analysis pipelines for both languages are shown on the Treex demo website at https://lindat.", "labels": [], "entities": [{"text": "Treex demo website", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.947302003701528}]}, {"text": "mff.cuni.cz/services/treex-web/run.", "labels": [], "entities": []}, {"text": "They include part-of-speech taggers () and a dependency parser (), plus a rule-based conversion of the resulting dependency trees to the deep layer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results for English", "labels": [], "entities": []}, {"text": " Table 2: Experimental results for Czech", "labels": [], "entities": []}]}