{"title": [{"text": "Building hybrid machine translation systems by using an EBMT preprocessor to create partial translations", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7235918194055557}]}], "abstractContent": [{"text": "This paper presents a hybrid machine translation framework based on a pre-processor that translates fragments of the input text by using example-based machine translation techniques.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7172841131687164}]}, {"text": "The pre-processor resembles a translation memory with named-entity and chunk generalization , and generates a high quality partial translation that is then completed by the main translation engine, which can be either rule-based (RBMT) or statistical (SMT).", "labels": [], "entities": []}, {"text": "Results are reported for both RBMT and SMT hybridization as well as the preprocessor on its own, showing the effectiveness of our approach.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.7933692932128906}, {"text": "SMT hybridization", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.9459048807621002}]}], "introductionContent": [{"text": "The traditional approach to Machine Translation (MT) has been rule-based (RBMT), but it has been progressively replaced by Statistical Machine Translation (SMT) since the 1990s.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8862919449806214}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 123, "end_pos": 160, "type": "TASK", "confidence": 0.7917693058649699}]}, {"text": "Example-Based Machine Translation (EBMT), the other main MT paradigm, has never attracted that much attention: even though it gives excellent results with repetitive text for which accurate matches are found in the parallel corpus, its quality quickly degrades as more generalization is needed.", "labels": [], "entities": [{"text": "Example-Based Machine Translation (EBMT)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7960526446501414}, {"text": "MT paradigm", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9215249419212341}]}, {"text": "Nevertheless, it has been argued that, along with the raise of hybrid systems that try to combine multiple paradigms, EBMT can help to overcome some of the weaknesses of the other approaches . c \ufffd 2015 The authors.", "labels": [], "entities": []}, {"text": "This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND.", "labels": [], "entities": []}, {"text": "This paper refers to as hybridization to any combination of MT paradigms, no matter if they are integrated in a single In this paper, we propose one such system based on a multi-pass system combination: an EBMT preprocessor translates those fragments of the input text for which accurate matches are found in the parallel corpus, generating a high-quality partial translation that is then completed by the main translator, which can be either rule-based or statistical.", "labels": [], "entities": [{"text": "MT paradigms", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.9336804747581482}]}, {"text": "The function of the EBMT preprocessor is therefore similar to that of Translation Memories (TM), with the difference that previously made translations are not reused to aid human translators but a MT engine.", "labels": [], "entities": [{"text": "Translation Memories (TM)", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.851841950416565}]}, {"text": "Needless to say, if the EBMT preprocessor was only able to reuse full sentences as traditional TM systems do at the most basic level, the quality of its partial translations would match that of humans, but its contribution would be negligible inmost situations.", "labels": [], "entities": []}, {"text": "At the same time, trying to increase the coverage by generalizing too much at the expense of translation quality, as traditional EBMT systems do, would make the whole system pointless if the preprocessor is notable to outperform the main MT engine for the fragments it translates.", "labels": [], "entities": []}, {"text": "This way, for our approach to work as intended, it is necessary to find a trade-off between coverage and translation quality.", "labels": [], "entities": []}, {"text": "In this work, we take a preprocessor that reuses full sentences as our starting point and explore two generalization techniques similar to those used by second and third generation TM systems (): \u2022 Named-entity (NE) generalization, giving the option to replace NEs like proper names and numerals in the parallel corpus with any other found in the text to translate.", "labels": [], "entities": []}, {"text": "However, some authors distinguish between hybridization for systems that meet this requirement and combination for systems that do not.", "labels": [], "entities": []}, {"text": "\u2022 Chunk generalization, giving the option to reuse examples in a subsentential level.", "labels": [], "entities": []}, {"text": "Several other methods that combine EBMT and TM with other MT paradigms have been proposed in the literature.", "labels": [], "entities": [{"text": "TM", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.8911479115486145}, {"text": "MT paradigms", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.9143345057964325}]}, {"text": "use an SMT system to fill the mismatched parts from a fuzzy search in a TM.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9671371579170227}]}, {"text": "Similarly, use a RBMT engine to complete the mismatched fragments from an EBMT system and smooth the resulting output using linguistic rules.", "labels": [], "entities": []}, {"text": "On the other hand, integrate SMT phrase tables into an EBMT framework.", "labels": [], "entities": [{"text": "SMT phrase tables", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8896158138910929}]}, {"text": "Following the opposite approach, feed an SMT system with alignments obtained using EBMT techniques.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9933397173881531}]}, {"text": "use EBMT techniques to obtain bilingual chunks that are then integrated into a RBMT system.", "labels": [], "entities": []}, {"text": "Lastly, propose a multi-engine system that selects the best translation created by a RBMT, an SMT and an EBMT engine.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.8362941145896912}]}, {"text": "However, to the best of our knowledge, the use of a generic multi-pass hybridization method for EBMT that works with both SMT and RBMT has never been reported so far.", "labels": [], "entities": [{"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9325199127197266}]}, {"text": "The remaining of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The proposed method is presented in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 explains the experimental settings under which the system was tested, and the results obtained are then discussed in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "As discussed in Section 1, it is expected that the performance of our method will greatly depend on the similarity between the input text and the examples given in the training corpus.", "labels": [], "entities": []}, {"text": "Taking that into account, we decided to train our system in two different domains: the particularly repetitive domain of collective bargaining agreements, and the more common domain of parliamentary proceedings.", "labels": [], "entities": []}, {"text": "For the former, we used the Spanish-Basque IVAP corpus, consisting of a total of 81 collective bargaining agreements to which we added the larger Elhuyar's administrative corpus to aid wordalignment.", "labels": [], "entities": [{"text": "Spanish-Basque IVAP corpus", "start_pos": 28, "end_pos": 54, "type": "DATASET", "confidence": 0.5996034840742747}]}, {"text": "For the latter, we used the SpanishEnglish Europarl corpus as given in the shared task of the ACL 2007 workshop on statistical machine translation, consisting of proceedings of the European Parliament.", "labels": [], "entities": [{"text": "SpanishEnglish Europarl corpus", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.9317301909128824}, {"text": "statistical machine translation", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.6844906012217203}]}, {"text": "As for the testing data, we used an in-domain test set for each corpus as well as an out-of-domain one for Europarl as shown in.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.9860798716545105}]}, {"text": "In order to evaluate the performance of our method we carried out the following two experiments: \u2022 A manual evaluation of the EBMT preprocessor to measure both the coverage and the quality of its partial translations.", "labels": [], "entities": [{"text": "EBMT preprocessor", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.8342325091362}, {"text": "coverage", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9480660557746887}]}, {"text": "For this purpose, we randomly selected 100 sentences for each in-domain test set and asked 5 volunteers to score the quality of each translated fragment in its context in a scale between 1 (incorrect translation) and 4 (correct translation).", "labels": [], "entities": []}, {"text": "\u2022 An automatic evaluation of the whole system using the Bilingual Evaluation Under-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results of the manual evaluation in IVAP (es-eu)", "labels": [], "entities": []}, {"text": " Table 5: Results of the manual evaluation in Europarl (es-en)", "labels": [], "entities": [{"text": "Europarl (es-en)", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9050764590501785}]}, {"text": " Table 6: BLEU scores with RBMT hybridization", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987234473228455}, {"text": "RBMT hybridization", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7069620490074158}]}, {"text": " Table 8: BLEU scores with SMT hybridization", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988328814506531}, {"text": "SMT hybridization", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.9783432185649872}]}, {"text": " Table 9: An example of SMT hybridization in Europarl", "labels": [], "entities": [{"text": "SMT hybridization", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.9639748632907867}, {"text": "Europarl", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9844679236412048}]}, {"text": " Table 10: Average length of the fragments translated by the EBMT preprocessor", "labels": [], "entities": [{"text": "Average length", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9397148191928864}, {"text": "EBMT preprocessor", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.9093389213085175}]}]}