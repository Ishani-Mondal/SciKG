{"title": [{"text": "How do Humans Evaluate Machine Translation", "labels": [], "entities": [{"text": "Evaluate Machine Translation", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.704860101143519}]}], "abstractContent": [{"text": "In this paper, we take a closer look at the MT evaluation process from a glass-box perspective using eye-tracking.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9410562217235565}]}, {"text": "We analyze two aspects of the evaluation task-the background of evaluators (monolin-gual or bilingual) and the sources of information available, and we evaluate them using time and consistency as criteria.", "labels": [], "entities": []}, {"text": "Our findings show that monolinguals are slower but more consistent than bilinguals, especially when only target language information is available.", "labels": [], "entities": []}, {"text": "When exposed to various sources of information, evaluators in general take more time and in the case of monolinguals, there is a drop in consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 137, "end_pos": 148, "type": "METRIC", "confidence": 0.9933145642280579}]}, {"text": "Our findings suggest that to have consistent and cost effective MT evaluations , it is better to use monolinguals with only target language information.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9925385117530823}]}], "introductionContent": [{"text": "Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.8594267010688782}]}, {"text": "Ina typical scenario human judges evaluate a system's output (or hypothesis) by comparing it to a source sentence and/or to a reference translation.", "labels": [], "entities": []}, {"text": "Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (; or rank a set of hypotheses in order of preference (.", "labels": [], "entities": []}, {"text": "Evaluating MT output can be a challenging task fora number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations.", "labels": [], "entities": [{"text": "MT output", "start_pos": 11, "end_pos": 20, "type": "TASK", "confidence": 0.7526448369026184}]}, {"text": "As a result, evaluations suffer from low inter-and intra-annotator agreements ().", "labels": [], "entities": []}, {"text": "Yet, as argue, using human judgments is essential to the progress of MT because: (i) automatic translations are produced fora human audience; and (ii) human understanding of the real world allows to assess the importance of the errors made by MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.989254355430603}, {"text": "MT", "start_pos": 243, "end_pos": 245, "type": "TASK", "confidence": 0.965864896774292}]}, {"text": "Most of the research inhuman evaluation has focused on analyzing the criteria to use for evaluation, and has regarded the evaluation process as a black-box, where the inputs are different sources of information (i.e source text, reference translation, and translation hypotheses), and the output is a score (or preference ranking).", "labels": [], "entities": []}, {"text": "In this paper, we focus on analyzing evaluation from a different perspective.", "labels": [], "entities": []}, {"text": "First, we regard the process as a glass-box and use eye-tracking to monitor the times evaluators spend digesting different sources of information (scenarios) before making a judgment.", "labels": [], "entities": []}, {"text": "Secondly, we contrast how the availability of such sources can affect the outcome of the evaluation.", "labels": [], "entities": []}, {"text": "Finally, we analyze how the background of the evaluators (in this case whether they are monolingual or bilingual) has an effect on the consistency and speed in which translations are evaluated.", "labels": [], "entities": [{"text": "consistency", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.9912152290344238}]}, {"text": "Our main research questions are: \u2022 Given different scenarios, what source of information do evaluators use to evaluate a translation?", "labels": [], "entities": []}, {"text": "Do they use the source text, the target text, or both?", "labels": [], "entities": []}, {"text": "Does the availability of specific information changes the consistency of the evaluation?", "labels": [], "entities": [{"text": "consistency", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9918389916419983}]}, {"text": "\u2022 Are there differences of behavior between bilinguals (i.e. evaluators fluent in both source and target languages) and monolinguals (i.e. evaluators fluent only in the target language)?", "labels": [], "entities": []}, {"text": "Which group is more consistent?", "labels": [], "entities": []}, {"text": "Our goal is to provide actionable insights that can help to improve the process of evaluation, especially in large-scale shared-tasks such as WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.5359464883804321}]}, {"text": "In the next sections we summarize related work, provide details of our experimental setup, and analyze and discuss the results of our experiment.", "labels": [], "entities": []}], "datasetContent": [{"text": "We planned our experiment to collect 1200 evaluations, 60 from each of the 20 participants.", "labels": [], "entities": []}, {"text": "To do so, we designed an experimental matrix in which we considered the following variables: (i) evaluator type: monolingual, bilingual; (ii) length of reference: short, medium, long; (iii) scenario: source-only, source+target, targetonly; and (iv) type of translation: worst, best.", "labels": [], "entities": []}, {"text": "In our experimental matrix, each participant evaluated 60 translations evenly divided into: 20 translations in each of the scenario; 20 translations from each length type; 30 translations of each quality type.", "labels": [], "entities": []}, {"text": "On the other hand, each translation was evaluated by four different participants, two bilingual and two monolingual.", "labels": [], "entities": []}, {"text": "To avoid any bias, we made sure that each evaluator saw each source sentence only once.", "labels": [], "entities": []}, {"text": "The process of evaluation can be cumbersome, especially if the evaluation sessions last for long; hence we used feedback to boost the engagement of participants throughout the evaluation process.", "labels": [], "entities": []}, {"text": "This is a double-edged sword, as the feedback has the potential to bias the evaluators and influence their decision.", "labels": [], "entities": []}, {"text": "To rule-out any potential bias from the feedback, we investigated the effects that the progression in which the tasks were performed might have on the differences between the evaluator scores and the feedback scores.", "labels": [], "entities": []}, {"text": "If the evaluators learned to reproduce the feedback scores, we would expect that the feedback error (\u03c4 c ) would decrease as a function of time.", "labels": [], "entities": [{"text": "feedback error (\u03c4 c )", "start_pos": 85, "end_pos": 106, "type": "METRIC", "confidence": 0.848129024108251}]}, {"text": "We calculated the feedback error as follows: where f i is the feedback score for translation i, and other variables are the same as in eq.", "labels": [], "entities": []}, {"text": "1. We fitted a linear model to the data, using the scenario, the evaluator type and the progression (time) as predictors; and the feedback error as a response.", "labels": [], "entities": []}, {"text": "We did not find that the progression had any significant effect (p = 0.2856) on the feedback error.", "labels": [], "entities": []}, {"text": "This means that the feedback did not bias the scoring behavior of the evaluators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average task duration time (in seconds)  according to type of setup, type of evaluator and  source sentence length.", "labels": [], "entities": [{"text": "Average task duration time", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.7141785621643066}]}, {"text": " Table 2: Proportional time spent by evaluators  while focusing in different regions of the screen:  translation (trans), reference and its context (ref),  source and its context (src), and the aggregate of  src and ref.", "labels": [], "entities": []}, {"text": " Table 3: Consistency scores: standard deviation  with respect to the class average (\u03c3 c ) for the scores  produced by different types of evaluators across  different conditions. Lower scores means higher  consistency. Each measure is calculated over N =  200 points.", "labels": [], "entities": [{"text": "class average (\u03c3 c )", "start_pos": 70, "end_pos": 90, "type": "METRIC", "confidence": 0.7074383944272995}, {"text": "consistency", "start_pos": 206, "end_pos": 217, "type": "METRIC", "confidence": 0.9944217801094055}]}]}