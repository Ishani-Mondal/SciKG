{"title": [{"text": "Lexicon-assisted tagging and lemmatization in Latin: A comparison of six taggers and two lemmatization methods", "labels": [], "entities": [{"text": "Lexicon-assisted tagging", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6277530491352081}]}], "abstractContent": [{"text": "We present a survey of tagging accuracies concerning part-of-speech and full morphological tagging-for several tag-gers based on a corpus for medieval church Latin (see www.comphistsem.org).", "labels": [], "entities": []}, {"text": "The best tagger in our sample, Lapos, has a PoS tagging accuracy of close to 96% and an overall tagging accuracy (includ-ing full morphological tagging) of about 85%.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.6722274720668793}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9123334884643555}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.8740136027336121}]}, {"text": "When we 'intersect' the taggers with our lexicon, the latter score increases to almost 91% for Lapos.", "labels": [], "entities": []}, {"text": "A conservative assessment of lemmatization accuracy on our data estimates a score of 93-94% fora lexicon-based lemmatization strategy and a score of 94-95% for lemmatizing via trained lemmatizers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.984818160533905}]}], "introductionContent": [{"text": "Part-of-speech (PoS) tagging is a standard task in natural language processing (NLP) in which the goal is to assign each word in a sentence its (possibly complex) part-of-speech label.", "labels": [], "entities": [{"text": "Part-of-speech (PoS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6297194719314575}, {"text": "natural language processing (NLP)", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.7564090092976888}]}, {"text": "While partof-speech tagging for English is well-researched, morphologically rich languages like some Slavic languages or classical languages such as ancient Greek or Latin have received considerably less attention.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.8767282962799072}]}, {"text": "Often-cited problems for the latter class of languages include relatively free word-order and a high degree of inflectional variability, leading to data sparseness problems.", "labels": [], "entities": []}, {"text": "In this work, we survey tagging accuracies (part-of-speech as well as full morphological tagging) for several part-of-speech taggers based on a corpus of Latin texts.", "labels": [], "entities": []}, {"text": "The corpus, which was built as part of the Computational Historical Semantics (CompHistSem) project , comprises about 15 500 sentences as ex-1 www.comphistsem.org emplified in.", "labels": [], "entities": []}, {"text": "The aim of CompHistSem is to develop an historical semantics based on medieval Latin texts that allows for fine-grained analyses of word meanings starting from richly annotated corpora.", "labels": [], "entities": []}, {"text": "The application scenario of the current study is to meet this annotation requirement by means of open access tools.", "labels": [], "entities": []}, {"text": "Our corpus is based on the capitularies, the amalarius corpus as partly available via the Patrologia Latina and three further texts from the MGH 3 corpus (Visio Baronti, Vita Adelphii, Vita Amandi).", "labels": [], "entities": [{"text": "amalarius corpus", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.6445002406835556}, {"text": "MGH 3 corpus", "start_pos": 141, "end_pos": 153, "type": "DATASET", "confidence": 0.9554776748021444}]}, {"text": "Each token of the corpus has been manually annotated with a reference to an associated lexicon entry as described below (cf.).", "labels": [], "entities": []}, {"text": "In this way, full morphological features are available for all tokens.", "labels": [], "entities": []}, {"text": "Our lexicon has been compiled from several sources such as LemLat and from rule-based lexical expanders.", "labels": [], "entities": []}, {"text": "We describe its composition in more depth in Section 2.", "labels": [], "entities": []}, {"text": "The taggers we survey include three relatively new taggers (Lapos, Mate, and the Stanford tagger) as well as two taggers originating in an earlier tagging tradition.", "labels": [], "entities": []}, {"text": "In addition, we report results for two tagger variants available in the OpenNLP package.", "labels": [], "entities": [{"text": "OpenNLP package", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9521963596343994}]}, {"text": "All taggers are trained on our corpus.", "labels": [], "entities": []}, {"text": "In accordance with Moore's law describing scientific/technological progress overtime, we find that more recent tagger classes substantially outperform their predecessor generation.", "labels": [], "entities": []}, {"text": "The best tagger in our sample, Lapos, has a PoS tagging accuracy of close to 96% and an overall tagging accuracy (including full morphological tagging) of about 85%.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.6579765677452087}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9161455631256104}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.8950439691543579}]}, {"text": "When we 'intersect' the taggers with our lexicon, the latter score increases to almost 91% for Lapos.", "labels": [], "entities": []}, {"text": "Concerning lemmatization, we lemmatize words on the basis of the taggers' outputs.", "labels": [], "entities": []}, {"text": "We employ two dif-: Sample sentence ('from the fruits of the earth our body is sustained') in our corpus and its annotation.", "labels": [], "entities": []}, {"text": "ferent lemmatization strategies: we either lookup the current lemma in the lexicon given the word form as well as the predicted tag information (lexicon-based lemmatization) or we lemmatize on the basis of statistical lemmatizers/string transducers trained on our corpus.", "labels": [], "entities": []}, {"text": "A conservative assessment of lemmatization accuracy estimates a score of 93-94% for the lexicon-based strategy and a score of 94-95% for the trained lemmatizers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9834825992584229}]}, {"text": "This work is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our lexicon.", "labels": [], "entities": []}, {"text": "Section 3 outlines related work, on part-of-speech tagging and resources for Latin.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7159053236246109}]}, {"text": "Section 4 describes our lemmatization module and Section 5 the tagging systems we survey.", "labels": [], "entities": []}, {"text": "In Section 6, we outline results and we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Distribution of the lexicon entries over the different parts of speech.", "labels": [], "entities": []}, {"text": " Table 3: Tag accuracies in % for different systems and different categories.", "labels": [], "entities": [{"text": "Tag", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9765651822090149}, {"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.5436363816261292}]}, {"text": " Table 6: Precision, recall and F 1 measure across  the possible PoS tags in our corpus. PoS ordered  by corpus frequency.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9952676296234131}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9989928603172302}, {"text": "F 1", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.99013751745224}]}, {"text": " Table 4: Lemma accuracy in % for 5 selected taggers based on either lexicon-based lemmatization or  using the learned LemmaGen transducer.", "labels": [], "entities": [{"text": "Lemma", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.974465012550354}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.737643301486969}]}, {"text": " Table 7: Agreement of different taggers in %.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9768338799476624}]}]}