{"title": [{"text": "Idiom Paraphrases: Seventh Heaven vs Cloud Nine", "labels": [], "entities": [{"text": "Idiom Paraphrases", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8306829035282135}]}], "abstractContent": [{"text": "The goal of paraphrase identification is to decide whether two given text fragments have the same meaning.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.9838710129261017}]}, {"text": "Of particular interest in this area is the identification of paraphrases among short texts, such as SMS and Twitter.", "labels": [], "entities": [{"text": "identification of paraphrases among short texts", "start_pos": 43, "end_pos": 90, "type": "TASK", "confidence": 0.851339598496755}]}, {"text": "In this paper, we present idiomatic expressions as anew domain for short-text paraphrase identification.", "labels": [], "entities": [{"text": "short-text paraphrase identification", "start_pos": 67, "end_pos": 103, "type": "TASK", "confidence": 0.6568662623564402}]}, {"text": "We propose a technique, utilizing idiom definitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community .", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of paraphrase identification, i.e. finding alternative linguistic expressions of the same or similar meaning, attracted a great deal of attention in the research community in recent years.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.965665876865387}]}, {"text": "This task was extensively studied in Twitter data, where millions of user-generated tweets talk about the same topics and thus present a natural challenge to resolve redundancy in tweets for many applications, such as textual entailment (), text summarization (, first story detection, search), question answering, etc.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.6929227262735367}, {"text": "text summarization", "start_pos": 241, "end_pos": 259, "type": "TASK", "confidence": 0.7525808811187744}, {"text": "first story detection", "start_pos": 263, "end_pos": 284, "type": "TASK", "confidence": 0.6897570689519247}, {"text": "question answering", "start_pos": 295, "end_pos": 313, "type": "TASK", "confidence": 0.8634555041790009}]}, {"text": "In this paper we explore anew domain for the task of paraphrase identification -idiomatic expressions, in which the goal is to determine whether two idioms convey the same idea.", "labels": [], "entities": [{"text": "paraphrase identification -idiomatic expressions", "start_pos": 53, "end_pos": 101, "type": "TASK", "confidence": 0.8683851718902588}]}, {"text": "This task is related to previous short-text paraphrase tasks, but it does not have access to many of the information sources that can be exploited in Twitter/short text paraphrasing: unlike tweets, idioms do not have hashtags, which are very strong topic indictors; unlike SMS, idioms do not have timestamp or geographical metadata; and unlike news headlines, there are no real world events that can serve as anchors to cluster similar expressions.", "labels": [], "entities": []}, {"text": "In addition, an idea, or amoral of the idiom is often expressed in an indirect way, e.g. the idioms (1) make a mountain out of a molehill (2) tempest in a teapot convey similar ideas 1 : (1) If somebody makes a mountain out of a molehill they exaggerate the importance or seriousness of a problem.", "labels": [], "entities": []}, {"text": "(2) If people exaggerate the seriousness of a situation or problem they are making a tempest in a teapot.", "labels": [], "entities": []}, {"text": "There is a line of research focused on extracting idioms from the text or identifying whether a particular expression is idiomatic (or a noncompositional multi-word expression)).", "labels": [], "entities": []}, {"text": "Without linguistic sources such as Wiktionary, usingenglish.com, etc, it is often hard to understand what the meaning of a particular idiom is.", "labels": [], "entities": []}, {"text": "It is even harder to determine whether two idioms convey the same idea or find alternative idiomatic expressions.", "labels": [], "entities": []}, {"text": "Using idiom definitions, given by linguistic resources, one can view this problem as identifying paraphrases between definitions and thus deciding on paraphrases between corresponding idioms.", "labels": [], "entities": []}, {"text": "Efficient techniques for identifying idiom paraphrases would complement any paraphrase identification system, and thus improve the downstream applications, such as question answering, summariza-tion, opinion mining, information extraction, and machine translation.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.729385644197464}, {"text": "question answering", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.8950274586677551}, {"text": "summariza-tion", "start_pos": 184, "end_pos": 198, "type": "TASK", "confidence": 0.9594207406044006}, {"text": "opinion mining", "start_pos": 200, "end_pos": 214, "type": "TASK", "confidence": 0.8480777442455292}, {"text": "information extraction", "start_pos": 216, "end_pos": 238, "type": "TASK", "confidence": 0.8635143935680389}, {"text": "machine translation", "start_pos": 244, "end_pos": 263, "type": "TASK", "confidence": 0.8152889013290405}]}, {"text": "To the best of our knowledge we are the first to address the problem of determining whether two idioms convey the same idea, and to propose anew scheme that utilizes idiom definitions and continuous space word representation (word embedding) to solve it.", "labels": [], "entities": []}, {"text": "By linking word-and sentence-level semantics our technique outperforms state-of-theart paraphrasing approaches on a dataset of 1.4K annotated idiom pairs that we make publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected 2,432 idioms from http://www.usingenglish.com, a site for English learners, where every idiom has a unique description giving a clear explanation of the idiom's meaning.", "labels": [], "entities": []}, {"text": "As opposed to tweets there are no hashtags, no topics or trends, no timestamps, or any other default evidence, that two idioms may convey similar ideas.", "labels": [], "entities": []}, {"text": "Thus it becomes a challenging   task itself to construct a dataset of pairs that is guaranteed to have a certain fraction of true paraphrases.", "labels": [], "entities": []}, {"text": "We used a simple cosine similarity between all possible idiom definitions pairs to have a ranked list and labeled the top 1.5K pairs.", "labels": [], "entities": []}, {"text": "Three annotators were asked to label each pair of idiom definitions as \"similar\" (score 2), \"have something in common\" (score 1), \"not similar\" (score 0).", "labels": [], "entities": []}, {"text": "0.1K pairs received a total score of 4 (either 2+2+0, or 2+1+1), and were further removed as debatable.", "labels": [], "entities": []}, {"text": "The rest of the labeled pairs were randomly split into 1K for test data and 0.4K for development.", "labels": [], "entities": []}, {"text": "Only pairs that received a total score of 5 or higher were considered as positive examples.", "labels": [], "entities": []}, {"text": "There are 364 and 96 true paraphrases in our test and development sets respectively.", "labels": [], "entities": []}, {"text": "Our baselines are simple and tf-idf weighted cosine similarity between idiom description sentences: CosSim and LexicalSim.", "labels": [], "entities": []}, {"text": "Keller, 2015) that was ranked first among 19 teams in the Paraphrase in Twitter (PIT) track on the SemEval 2015 shared task (.", "labels": [], "entities": [{"text": "Paraphrase in Twitter (PIT)", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.7057212591171265}, {"text": "SemEval 2015 shared task", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.7526941448450089}]}, {"text": "This model extracts eight simple and elegant character and word features from two sentences to train an SVM with linear kernel.", "labels": [], "entities": []}, {"text": "It achieves an F-score of 55.1% on our test set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9997585415840149}]}, {"text": "We also compare our method with the stateof-the-art Weighted Textual Matrix Factorization model (WTMF) (, 5 which is specifically developed for short sentences by modeling the semantic space of words, that can be either present or absent from the sentences.", "labels": [], "entities": []}, {"text": "This model achieves a maximal Fscore of 61.4% on the test set.", "labels": [], "entities": [{"text": "maximal", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9925985336303711}, {"text": "Fscore", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9353562593460083}]}, {"text": "The state-of-the-art model for lexically divergent paraphrases on Twitter () is tailored for tweets and requires topic and anchor words to be present in the sentence, which is not applicable to idiom definitions.", "labels": [], "entities": []}, {"text": "To evaluate models we plot precision-recall curves for CosSim, WTMF, LexicalSim, and IdiomSim (for clarity we omit curves for other models).", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 27, "end_pos": 43, "type": "METRIC", "confidence": 0.9922977089881897}, {"text": "WTMF", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8562349081039429}]}, {"text": "We also compare maximal F-score for all models.", "labels": [], "entities": [{"text": "maximal", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9532070159912109}, {"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.7804543972015381}]}, {"text": "We observe that simple cosine similarity (CosSim) achieves a maximal Fscore of 53.7%, LexicalSim is a high baseline and achieves an F-score of 63.75%.", "labels": [], "entities": [{"text": "simple cosine similarity (CosSim)", "start_pos": 16, "end_pos": 49, "type": "METRIC", "confidence": 0.7389531234900156}, {"text": "maximal", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9252952337265015}, {"text": "Fscore", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8747599720954895}, {"text": "F-score", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.9986318945884705}]}, {"text": "When we add averaged word embeddings the maximal F-score is 64.4% (IdiomSim ave ).", "labels": [], "entities": [{"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9145829677581787}, {"text": "IdiomSim", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9602019190788269}]}, {"text": "With tfidf weighted word embeddings we achieve F-score of 65.9%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9995562434196472}]}, {"text": "By filtering out uninformative words such as \"a\", \"the\", etc (12 words total) we improve the Fscore to 66.6% (IdiomSim+), outperforming stateof-the-art paraphrase models by more than 5% absolute.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9937578439712524}, {"text": "IdiomSim", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.922554075717926}]}, {"text": "Both IdiomSim and IdiomSim+ outperform WTMF significantly according to a paired t-test with p less than 0.05.", "labels": [], "entities": [{"text": "WTMF", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8794287443161011}]}, {"text": "We use threshold, corresponding to a maximal F-score obtained on the development dataset, and explore paraphrases from test dataset scored higher and lower than this threshold.", "labels": [], "entities": [{"text": "F-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9881725907325745}]}, {"text": "Examples of extracted idiom paraphrases are in.", "labels": [], "entities": []}, {"text": "Examples of false positives and false negatives are in.", "labels": [], "entities": []}, {"text": "Since words are main units in the computation (5) our metric is biased towards lexical similarity.", "labels": [], "entities": []}, {"text": "Thus we get a false positive paraphrase between \"healthy as a horse\" and \"an apple a day\".", "labels": [], "entities": []}, {"text": "The first one is rather a statement about someone's health while the second one is an advice on how to be healthy.", "labels": [], "entities": []}, {"text": "Moreover, idioms \"heart of steel\" vs \"heart of glass\" convey opposite ideas of being \"not affected emotionally\" vs being \"easily affected emotionally\".", "labels": [], "entities": []}, {"text": "Having \"heart\" and \"affected emotionally\" in both idiom descriptions leads to a high cosine similarity between them and results in a false positive decision.", "labels": [], "entities": []}, {"text": "For the same reason lexically divergent idiom descriptions get a lower rank while convey similar ideas, e.g. \"hopping mad\" vs \"off on one\".", "labels": [], "entities": []}, {"text": "Combining lexical and sentence similarity via (5) performs better than lexical similarity alone) but still does not capture all aspects of a true paraphrase.", "labels": [], "entities": []}], "tableCaptions": []}