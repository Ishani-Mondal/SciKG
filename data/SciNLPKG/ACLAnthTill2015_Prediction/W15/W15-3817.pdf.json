{"title": [{"text": "Automatic Detection of Answers to Research Questions from Medline Abstracts", "labels": [], "entities": [{"text": "Automatic Detection of Answers to Research Questions", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8702013620308467}, {"text": "Medline Abstracts", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.7747071385383606}]}], "abstractContent": [{"text": "Given a set of abstracts retrieved from a search engine such as Pubmed, we aim to automatically identify the claim zone in each abstract and then select the best sen-tence(s) from that zone that can serve as an answer to a given query.", "labels": [], "entities": [{"text": "Pubmed", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9474551677703857}]}, {"text": "The system can provide a fast access mechanism to the most informative sentence(s) in abstracts with respect to the given query.", "labels": [], "entities": []}], "introductionContent": [{"text": "The large amount of medical literature hinders professionals from analyzing all the relevant knowledge to particular medical questions.", "labels": [], "entities": []}, {"text": "Search engines are increasingly used to access such information.", "labels": [], "entities": []}, {"text": "However, such systems retrieve documents based on the appearance of the query terms in the text despite the fact that they may describe another problem.", "labels": [], "entities": []}, {"text": "The search engine Pubmed R for example is a well known IR system to access more than 24 million abstracts for the biomedical literature including Medline R (.", "labels": [], "entities": [{"text": "Pubmed R", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.8900436758995056}, {"text": "Medline R", "start_pos": 146, "end_pos": 155, "type": "DATASET", "confidence": 0.9176906943321228}]}, {"text": "The engine takes a query from user and returns a list of abstracts that can be relevant or partially irrelevant to the query, which requires from the user to go through each abstract for further analysis and evaluation.", "labels": [], "entities": []}, {"text": "Researchers who conduct a systematic review () tend to use the same approach to collect the studies of interest; however, they are found to spend significant effort identifying the studies that are relevant to the research question.", "labels": [], "entities": []}, {"text": "Relevancy is usually measured by scanning the result and conclusion sections to identify authors claim and then comparing the claim with the review question; where a claim can be defined as the summary of the main points presented in a research argument.", "labels": [], "entities": [{"text": "Relevancy", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9622781872749329}]}, {"text": "Incorporating a middle tier system between the search engine and the user will be useful to minimize the effort required to filter the results.", "labels": [], "entities": []}, {"text": "This research presents a system that aids those searching for studies that discuss a particular research question.", "labels": [], "entities": []}, {"text": "The system acts as a mediator between the search engine and the user.", "labels": [], "entities": []}, {"text": "It interprets the search engine results and returns the most informative sentence(s) from the claim zone of each abstract that are potential answers to the research question.", "labels": [], "entities": []}, {"text": "The system reduces the cognitive loads on the user by assisting their identification of relevant claims within abstracts The system comprises two components.", "labels": [], "entities": []}, {"text": "The first component identifies the claim zone in each abstract using the rhetorical moves principle (, and the second component uses the sentences in the claim zone to predict the most informative sentence(s) from each abstract to the given query.", "labels": [], "entities": []}, {"text": "This paper makes three contributions: presenting anew set of features to build a classifier to identify the structure role of sentences in an abstract that is at least shows similar performance to the current systems; building a classifier to detect the best sentence(s) (lexically) that can bean answer to a given query; and introducing anew feature (Z-score) for this task.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table (1) describes the performance of the claim  zoning component using corpus-1. Table (2) de- scribes the performance of Hirohata et al. (2008)  system using the same corpus. Although, the dif- ference was not significant, our system showed an  alternative set of features that can achieve at least  similar performance to the state of the art systems.", "labels": [], "entities": []}, {"text": " Table 2: Hirohata et al. (2008) system perfor- mance.", "labels": [], "entities": []}, {"text": " Table 3: The performance of the answer detection  component using different combinations of fea- tures", "labels": [], "entities": [{"text": "answer detection", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.9124114811420441}]}, {"text": " Table 4: Answer detection performance using both  components", "labels": [], "entities": [{"text": "Answer detection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9848235845565796}]}]}