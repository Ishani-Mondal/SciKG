{"title": [{"text": "Leveraging a Semantically Annotated Corpus to Disambiguate Prepositional Phrase Attachment", "labels": [], "entities": [{"text": "Disambiguate Prepositional Phrase Attachment", "start_pos": 46, "end_pos": 90, "type": "TASK", "confidence": 0.5783844590187073}]}], "abstractContent": [{"text": "Accurate parse ranking requires semantic information, since a sentence may have many candidate parses involving common syntactic constructions.", "labels": [], "entities": [{"text": "parse ranking", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.7698805630207062}]}, {"text": "In this paper, we propose a probabilistic framework for incorporating distributional semantic information into a maximum entropy parser.", "labels": [], "entities": []}, {"text": "Furthermore , to better deal with sparse data, we use a modified version of Latent Dirichlet Allocation to smooth the probability estimates.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation", "start_pos": 76, "end_pos": 103, "type": "METRIC", "confidence": 0.817523737748464}]}, {"text": "This LDA model generates pairs of lemmas, representing the two arguments of a semantic relation, and can be trained, in an unsupervised manner, on a corpus annotated with semantic dependencies.", "labels": [], "entities": []}, {"text": "To evaluate our framework in isolation from the rest of a parser, we consider the special case of prepositional phrase attachment ambiguity.", "labels": [], "entities": [{"text": "prepositional phrase attachment ambiguity", "start_pos": 98, "end_pos": 139, "type": "TASK", "confidence": 0.7084906101226807}]}, {"text": "The results show that our semantically-motivated feature is effective in this case, and moreover, the LDA smoothing both produces semantically interpretable topics, and also improves performance over raw co-occurrence frequencies, demonstrating that it can successfully generalise patterns in the training data.", "labels": [], "entities": [{"text": "LDA smoothing", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.7825812697410583}]}], "introductionContent": [{"text": "Ambiguity is a ubiquitous feature of natural language, and presents a serious challenge for parsing.", "labels": [], "entities": [{"text": "Ambiguity", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7646721601486206}, {"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9624642133712769}]}, {"text": "For people, however, it does not present a problem inmost situations, because only one interpretation will be sensible.", "labels": [], "entities": []}, {"text": "In examples (1) and (2), fluent speakers will not consciously consider a gun-wielding dog or a moustache used as a biting tool.", "labels": [], "entities": []}, {"text": "Both of these examples demonstrate syntactic ambiguity (the final prepositional phrase (PP) could modify the preceding noun, or the main verb), rather than lexical ambiguity (homophony or polysemy).", "labels": [], "entities": []}, {"text": "(1) The sheriff shot a dog with a rifle.", "labels": [], "entities": []}, {"text": "(2) The dog bit a sheriff with a moustache.", "labels": [], "entities": []}, {"text": "In many cases, parse ranking can be achieved by comparing syntactic structures, since some constructions are more common.", "labels": [], "entities": [{"text": "parse ranking", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9403709173202515}]}, {"text": "In the above examples, however, the same set of structures are available, but the best parse differs: the PP should modify the verb \"shot\" in (1), but the noun \"sheriff\" in.", "labels": [], "entities": []}, {"text": "Dealing with such cases requires semantic information.", "labels": [], "entities": []}, {"text": "A promising approach to represent lexical semantics assumes the distributional hypothesis, which was succinctly stated by: \"words that occur in similar contexts tend to have similar meanings\".", "labels": [], "entities": []}, {"text": "Our method uses corpus data to estimate the plausibility of semantic relations, which could then be exploited as features in a maximum entropy parser.", "labels": [], "entities": []}, {"text": "In section 3, we first describe the general framework, then explain how it can be specialised to tackle PP-attachment.", "labels": [], "entities": []}, {"text": "To overcome data sparsity, we introduce a generative model based on\u00b4Oon\u00b4 on\u00b4O S\u00e9aghdha (2010)'s modified version of Latent Dirichlet Allocation (LDA), where two lemmas are generated at a time, which we use to represent the two arguments of a binary semantic relation.", "labels": [], "entities": []}, {"text": "The probabilities produced by the LDA model can then be incorporated into a discriminative parse selection model, using our general framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two datasets were used in evaluation.", "labels": [], "entities": []}, {"text": "We produced the first from WeScience, the manually treebanked portion of the Wikipedia data used to produce WikiWoods.", "labels": [], "entities": [{"text": "WeScience", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.9352065324783325}, {"text": "Wikipedia data", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.8740438222885132}]}, {"text": "This dataset allows evaluation in the same domain and with the same annotation conventions as the training data.", "labels": [], "entities": []}, {"text": "We extracted all potentially ambiguous PPs from the DMRS structures: for PPs attached to a noun, the noun must be the object of a verb, and for PPs attached to a verb, the verb must have an object.", "labels": [], "entities": []}, {"text": "Duplicates were removed, since this would unfairly weight those examples: some repeated cases, such as (store metadata in format), are limited in their domain.", "labels": [], "entities": [{"text": "Duplicates", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9419769644737244}]}, {"text": "If the same tuple occurred with different attachment sites, the most common site was used, which happened twice, or if neither was more common, it was discarded, which happened four times.", "labels": [], "entities": []}, {"text": "This produced 3485 unique sequences, of which 2157 contained one of the nine prepositions under consideration.", "labels": [], "entities": []}, {"text": "The data is available on https://github.com/guyemerson/WeSciencePP.", "labels": [], "entities": [{"text": "WeSciencePP", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9055988192558289}]}, {"text": "The second data set was extracted from the Penn Treebank by.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.994768887758255}]}, {"text": "This dataset has been widely used, allowing a comparison with other approaches.", "labels": [], "entities": []}, {"text": "We extracted tuples with one of the relevant prepositions, lemmatised all words, and removed out of vocabulary items.", "labels": [], "entities": []}, {"text": "This gave 1240 instances from the evaluation section of the corpus.", "labels": [], "entities": []}, {"text": "We note that the data is noisy: it contains 'nouns' such as the (98 times), all (10 times), and 's (10 times), which are impossible under the annotation conventions of WikiWoods.", "labels": [], "entities": []}, {"text": "We discuss limitations of evaluating against this dataset in section 6.1.", "labels": [], "entities": []}, {"text": "Overall accuracy in choosing the correct attachment site is given in table 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9996494054794312}]}, {"text": "The large gap between the high and low baselines shows the importance of lexical information.", "labels": [], "entities": []}, {"text": "The high baseline and the 1-topic model (i.e. backing off to bigrams) show similar performance.", "labels": [], "entities": []}, {"text": "The best performing LDA models achieve 3 and 7 percentage point increases for WeScience and the Penn Treebank, demonstrating the effectiveness of this smoothing method.", "labels": [], "entities": [{"text": "WeScience", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.9410002827644348}, {"text": "Penn Treebank", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9938903152942657}]}, {"text": "The higher gain for the Penn Treebank suggests that smoothing is more important when evaluating across domains.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9945071339607239}]}, {"text": "The choices of hyperparameters suggested by the log-likelihood and AIC closely agree with the best performing model.", "labels": [], "entities": [{"text": "AIC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.522171139717102}]}, {"text": "The results also suggest that the LDA smoothing is robust to choosing too high a value for T . As we can see in table 2, there is only a small drop in performance with larger values of T . This result agrees with, who show that LDA, as applied to topic modelling, is reasonably robust to large choices of T , and that it is generally better to set T too high than too low.", "labels": [], "entities": [{"text": "LDA smoothing", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7194294631481171}]}, {"text": "Surprisingly, hyperparameter optimisation (allowing \u03b1 to be asymmetric) did not provide a significant change in performance, even though we might expect some topics to be more common.", "labels": [], "entities": []}, {"text": "Since the model is probabilistic, we can interpret it conservatively, and only predict attachment if the log-odds are above a threshold.", "labels": [], "entities": []}, {"text": "This reduces coverage, but could increase precision.", "labels": [], "entities": [{"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9732166528701782}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.996993899345398}]}, {"text": "We can be more confident when Gibbs samples produce similar probabilities, so we make the threshold a function of the estimated error, as in (15).", "labels": [], "entities": []}, {"text": "Here, \u03b5 N and \u03b5 V denote the standard error in log-probability for the nominal and verbal models -for k samples with standard deviation s, the standard error in the mean is \u03b5 = 1 \u221a k s.", "labels": [], "entities": []}, {"text": "When summing independent errors, the total error is the square root of the sum of their squares.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of training instances, with proportion of nominal attachment", "labels": [], "entities": []}, {"text": " Table 2: Performance of our model, varying number of topics T , number of Gibbs samples, and hyper- parameter optimisation. The highest scores for each dataset are shown in bold.", "labels": [], "entities": []}]}