{"title": [{"text": "Improving the Cross-Lingual Projection of Syntactic Dependencies", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9775834083557129}, {"text": "Cross-Lingual Projection of Syntactic Dependencies", "start_pos": 14, "end_pos": 64, "type": "TASK", "confidence": 0.8210734128952026}]}], "abstractContent": [{"text": "This paper presents several modifications of the standard annotation projection algorithm for syntactic structures in cross-lingual dependency parsing.", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 118, "end_pos": 150, "type": "TASK", "confidence": 0.638339767853419}]}, {"text": "Our approach reduces projection noise and includes efficient data subset selection techniques that have a substantial impact on parser performance in terms of labeled attachment scores.", "labels": [], "entities": []}, {"text": "We test our techniques on data from the Universal Dependency Treebank and demonstrate the improvements on a number of language pairs.", "labels": [], "entities": [{"text": "Universal Dependency Treebank", "start_pos": 40, "end_pos": 69, "type": "DATASET", "confidence": 0.7378511230150858}]}, {"text": "We also look at treebank translation including syntax-based models and data combination techniques that push the performance even further.", "labels": [], "entities": [{"text": "treebank translation", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.6965111047029495}]}, {"text": "We achieve absolute improvements of up to over seven points in labeled attachment scores pushing the state-of-the art in cross-lingual dependency parsing for all language pairs tested in our experiments.", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 121, "end_pos": 153, "type": "TASK", "confidence": 0.6687219937642416}]}], "introductionContent": [{"text": "State-of-the art dependency parsing is mainly based on annotated data and supervised learning techniques.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.6784880459308624}]}, {"text": "This, however, restricts the use of parsing technology to a few languages for which sufficient amounts of training data is available.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9781025052070618}]}, {"text": "Fully unsupervised techniques still fall far behind in their performance and cannot produce labels that are necessary for many downstream applications.", "labels": [], "entities": []}, {"text": "Cross-lingual learning techniques have, therefore, been proposed as a quick solution to bootstrap tools for otherwise unsupported languages.", "labels": [], "entities": []}, {"text": "There are basically two strategies that can be found in the literature: annotation projection and model transfer.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.8161559700965881}, {"text": "model transfer", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.7360855937004089}]}, {"text": "Model transfer has attracted a lot of interest recently due to the availability of cross-lingually harmonized annotation () that makes it possible to use universal features across languages.", "labels": [], "entities": [{"text": "Model transfer", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.823896199464798}]}, {"text": "The most straightforward technique is to train delexicalized parsers that heavily rely on universal POS tags.", "labels": [], "entities": []}, {"text": "This simple technique has shown some success for closely related languages (.", "labels": [], "entities": []}, {"text": "Several improvements can be achieved by using multiple source languages) and additional cross-lingual features that can be used to transfer models to anew language such as cross-lingual word clusters) or word-typology information.", "labels": [], "entities": []}, {"text": "Annotation projection has already along tradition in NLP.", "labels": [], "entities": [{"text": "Annotation projection", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9358938336372375}]}, {"text": "Initially proposed for tasks like POS tagging (), the seminal work for annotation projection in dependency parsing is presented by.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8104221224784851}, {"text": "annotation projection", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.7010373920202255}, {"text": "dependency parsing", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7682563066482544}]}, {"text": "The general idea is to make use of parallel corpora and automatic word alignment to transfer information from the source language to a target language translation that can then be used for training parsers.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7540481686592102}]}, {"text": "In most cases, treebanks are not taken from parallel corpora and, therefore, one has to rely on automatic annotation of the source language part of another (usually unrelated) bitext.", "labels": [], "entities": []}, {"text": "Together with the noise in automatic word alignment, these steps are bottlenecks in the projection strategy.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7090863883495331}]}, {"text": "propose the basic projection heuristics (which they call the direct correspondence assumption algorithm or DCA for short) that can handle various types of word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 155, "end_pos": 170, "type": "TASK", "confidence": 0.7197081297636032}]}, {"text": "In this paper we revisit this algorithm and include a systematic comparison of projection heuristics together with various modifications and data-set selection techniques.", "labels": [], "entities": []}, {"text": "We can show that these methods lead to significant improvements for all languages tested in our experiments.", "labels": [], "entities": []}, {"text": "Finally, we also look at the recently proposed treebank translation approach ( , which can be used as an alternative to annotation projection on existing parallel data sets.", "labels": [], "entities": [{"text": "treebank translation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.6914741098880768}]}, {"text": "Automatic translation has the advantage that we can use the manually verified annotation of the source language treebank instead of noisy machineannotated parallel data and also the given word alignment, which is an integral part of the translation model.", "labels": [], "entities": []}, {"text": "We present additional improvements when using our modifications of the projection algorithm and also show a positive effect when combining projected data from parallel corpora and machine translated treebanks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Baseline performance in LAS of a DCA- based annotation projection with 40,000 sentences  (models trained on the original treebanks in grey).", "labels": [], "entities": [{"text": "Baseline", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9477658271789551}]}, {"text": " Table 2: Collapsing arcs over unary dummy nodes  and removing dummy leaves (difference to baseline  in superscript).", "labels": [], "entities": []}, {"text": " Table 3: Using the intersection of word align- ments to resolve one-to-many links without cre- ating dummy head nodes. Bold numbers are also  better than", "labels": [], "entities": []}, {"text": " Table 5: Successfully projected trees out of 40,000  sentences when discarding trees with dummy  nodes.", "labels": [], "entities": []}, {"text": " Table 7. For some reason, French as  a target language was not very successful with this  strategy but in most other cases we can see consid- erable improvements over the previously noted top  scores.", "labels": [], "entities": []}, {"text": " Table 7: Discarding all trees that include dummy  nodes or dummy labels on any dependency rela- tions but still projecting 40,000 sentences (bold  numbers are higher than any previous setting).", "labels": [], "entities": []}, {"text": " Table 8: Treebank translation with DCA-based pro- jection (compared to the projection of parallel data  from Table 1).", "labels": [], "entities": []}, {"text": " Table 9: Collapsing relations over unary dummy  nodes and removing dummy leave nodes (same  approach as in Section 2.2; improvements over  Table 8 in superscript)", "labels": [], "entities": []}, {"text": " Table 10: Annotation projection using tree-to- string models for translating treebanks (differences  in LAS scores to the projection baseline are in  superscript numbers). Results in bold are better  than the phrase-based translation", "labels": [], "entities": [{"text": "Annotation projection", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.9267357289791107}]}, {"text": " Table 12: Combining projected data of all source  languages to train target language parsing mod- els. Additionally to LAS we also includes unla- beled attachment scores (UAS) and label accuracy  (LACC).", "labels": [], "entities": [{"text": "unla- beled attachment scores (UAS)", "start_pos": 141, "end_pos": 176, "type": "METRIC", "confidence": 0.8136434853076935}, {"text": "label accuracy  (LACC)", "start_pos": 181, "end_pos": 203, "type": "METRIC", "confidence": 0.832408094406128}]}]}