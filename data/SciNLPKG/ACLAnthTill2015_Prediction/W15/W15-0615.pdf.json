{"title": [{"text": "Reducing Annotation Efforts in Supervised Short Answer Scoring", "labels": [], "entities": [{"text": "Supervised Short Answer Scoring", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.5264624208211899}]}], "abstractContent": [{"text": "Automated short answer scoring is increasingly used to give students timely feedback about their learning progress.", "labels": [], "entities": [{"text": "Automated short answer scoring", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5671182870864868}]}, {"text": "Building scoring models comes with high costs, as state-of-the-art methods using supervised learning require large amounts of hand-annotated data.", "labels": [], "entities": []}, {"text": "We analyze the potential of recently proposed methods for semi-supervised learning based on clustering.", "labels": [], "entities": []}, {"text": "We find that all examined methods (centroids, all clusters, selected pure clusters) are mainly effective for very short answers and do not generalize well to several-sentence responses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated short answer scoring is getting more and more important, e.g. in the context of large-scale assessment in MOOCs or PISA.", "labels": [], "entities": [{"text": "Automated short answer scoring", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5544726252555847}]}, {"text": "The state of the art is currently to use supervised systems that are trained fora certain assessment item using manually annotated student responses.", "labels": [], "entities": []}, {"text": "For highstakes assessments like PISA, the effort that goes into manually scoring a large number of responses in order to train a good model might be justified, but it becomes a large obstacle in settings where new items need to be generated more frequently, like in MOOCs.", "labels": [], "entities": []}, {"text": "Thus, in this paper we explore ways to reduce the number of annotated training instances required to train a model fora new item.", "labels": [], "entities": []}, {"text": "In the traditional setting, human annotators score responses until a certain total or score distribution is reached that is deemed sufficient to train the model.", "labels": [], "entities": []}, {"text": "* Michael Heilman is now a Data Scientist at Civis Analytics.", "labels": [], "entities": []}, {"text": "As long as responses are randomly chosen for manual scoring, it is inevitable that annotators will see a lot of similar answers that will not add much new knowledge to the trained model.", "labels": [], "entities": []}, {"text": "Another drawback is that the class distribution in the data is often highly skewed (e.g. because there are only very few excellent answers).", "labels": [], "entities": []}, {"text": "Thus, the number of responses that need to be manually scored is much higher than it perhaps needs to be.", "labels": [], "entities": []}, {"text": "It should be possible to replace the random selection of responses to be annotated with a more informed approach.", "labels": [], "entities": []}, {"text": "In this paper, we explore two approaches: (i) annotating single selected instances, and (ii) annotating whole answer clusters.", "labels": [], "entities": []}, {"text": "The difference between the two approaches is visualized in.", "labels": [], "entities": []}, {"text": "In the first approach, we try to maximize lexical diversity based on the assumption that the classifier is best informed by responses that are as different as possible (i.e. in the words used).", "labels": [], "entities": []}, {"text": "In the second approach, we simulate letting annotators score whole clusters with a label that is used for all instances in this cluster.", "labels": [], "entities": []}, {"text": "The main advantage of this method is that it yields multiple training instances with just one decision from the annotator.", "labels": [], "entities": []}, {"text": "At the same time, judging whole clusters -especially if they are large -is more difficult than judging a single response, so we need to take this into consideration when comparing the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the datasets used for evaluation as well as the principal setup of our supervised scoring system.", "labels": [], "entities": []}, {"text": "We use two publicly available datasets.", "labels": [], "entities": []}, {"text": "gives an overview of their properties.", "labels": [], "entities": []}, {"text": "Powergrading (PG) The dataset was created by and contains about 5,000 crowdsourced responses to 10 questions from the US citizenship test.", "labels": [], "entities": [{"text": "US citizenship test", "start_pos": 118, "end_pos": 137, "type": "DATASET", "confidence": 0.7057695786158243}]}, {"text": "1 As can be quickly seen from, the responses in this dataset are rather short with on average 4 tokens.", "labels": [], "entities": []}, {"text": "Looking into the data, it quickly becomes clear that there is relatively little variance in the answers.", "labels": [], "entities": []}, {"text": "We thus expect clustering to work rather well on this dataset.", "labels": [], "entities": []}, {"text": "We are not aware of any supervised systems using the PG dataset before.", "labels": [], "entities": [{"text": "PG dataset", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.8758711516857147}]}, {"text": "In order to have a point of reference for the performance of the automatic scoring, we computed an average pairwise inter-annotatoragreement of .86 (quadratic weighted kappa) for the three human annotators.", "labels": [], "entities": []}, {"text": "We use the evaluation metric that was also used in the ASAP challenge: quadratic weighted kappa \u03ba.", "labels": [], "entities": [{"text": "ASAP challenge", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7191253900527954}]}, {"text": "We follow the ASAP challenge procedure by applying Fisher-Z transformation when averaging kappas.", "labels": [], "entities": [{"text": "ASAP challenge", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.6821569204330444}]}, {"text": "According to, quadratic weighted kappa is very similar to Pearson correlation r in such a setting.", "labels": [], "entities": [{"text": "Pearson correlation r", "start_pos": 58, "end_pos": 79, "type": "METRIC", "confidence": 0.9019263982772827}]}], "tableCaptions": [{"text": " Table 1: Overview of datasets", "labels": [], "entities": [{"text": "Overview of datasets", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.7003299395243326}]}]}