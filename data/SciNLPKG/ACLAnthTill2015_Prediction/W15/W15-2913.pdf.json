{"title": [{"text": "Personality Traits on Twitter -or- How to Get 1,500 Personality Tests in a Week", "labels": [], "entities": [{"text": "Personality Traits", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6393019556999207}]}], "abstractContent": [{"text": "Psychology research suggests that certain personality traits correlate with linguistic behavior.", "labels": [], "entities": []}, {"text": "This correlation can be effectively modeled with statistical natural language processing techniques.", "labels": [], "entities": []}, {"text": "Prediction accuracy generally improves with larger data samples, which also allows for more lexical features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.8768963813781738}]}, {"text": "Most existing work on personality prediction, however, focuses on small samples and closed-vocabulary investigations.", "labels": [], "entities": [{"text": "personality prediction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.8878333866596222}]}, {"text": "Both factors limit the generality and statistical power of the results.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of social media as a resource for large-scale, open-vocabulary personality detection.", "labels": [], "entities": [{"text": "open-vocabulary personality detection", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.6314599712689718}]}, {"text": "We analyze which features are predictive of which personality traits, and present a novel corpus of 1.2M English tweets annotated with Myers-Briggs personality type and gender.", "labels": [], "entities": []}, {"text": "Our experiments show that social media data can provide sufficient linguistic evidence to reliably predict two of four personality dimensions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Individual author attributes play an important role in customer modeling, as well as in business intelligence.", "labels": [], "entities": [{"text": "customer modeling", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7955992519855499}]}, {"text": "In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input.", "labels": [], "entities": []}, {"text": "Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (.", "labels": [], "entities": []}, {"text": "A whole body of work in NLP is concerned with attribute prediction from linguistic features (.", "labels": [], "entities": [{"text": "attribute prediction from linguistic features", "start_pos": 46, "end_pos": 91, "type": "TASK", "confidence": 0.8135438024997711}]}, {"text": "Apart from demographic features, such as age or gender, there is also a growing interest in personality types.", "labels": [], "entities": []}, {"text": "Predicting personality is not only of interest for commercial applications and psychology, but also for healthcare.", "labels": [], "entities": [{"text": "Predicting personality", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8295078575611115}]}, {"text": "Recent work by investigated the link between personality types, social media behavior, and psychological disorders, such as depression and posttraumatic stress disorder.", "labels": [], "entities": []}, {"text": "They found that certain personality traits are predictive of mental illness.", "labels": [], "entities": []}, {"text": "Similarly, show that linguistic traits are predictive of schizophrenia.", "labels": [], "entities": []}, {"text": "However, as pointed out by, computational personality recognition is limited by the availability of labeled data, which is expensive to annotate and often hard to obtain.", "labels": [], "entities": [{"text": "computational personality recognition", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.6646655897299448}]}, {"text": "Given the wide array of possible personality types, limited data size is a problem, since lowprobability types and combinations will not occur in statistically significant numbers.", "labels": [], "entities": []}, {"text": "In addition, many existing data sets are comprised of written essays, which usually contain highly canonical language, often of a specific topic.", "labels": [], "entities": []}, {"text": "Such controlled settings inhibit the expression of individual traits much more than spontaneous language.", "labels": [], "entities": []}, {"text": "In this work, we take a data-driven approach to personality identification, to avoid both the limitation of small data samples and a limited vocabulary.", "labels": [], "entities": [{"text": "personality identification", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.8602600693702698}]}, {"text": "We use the large amounts of personalized data voluntarily produced on social media (e.g., Twitter) to collect sufficient amounts of data.", "labels": [], "entities": []}, {"text": "Twitter is highly non-canonical, and famous for an almost unlimited vocabulary size).", "labels": [], "entities": []}, {"text": "In order to enable data-driven personality research, we combine this data source with self-assessed Myers-Briggs Type Indicators, denoted MBTIs.", "labels": [], "entities": []}, {"text": "Myers-Briggs uses four binary dimensions to classify users (INTROVERT-EXTROVERT, 92 INTUITIVE-SENSING, THINKING-FEELING, JUDGING-PERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types.", "labels": [], "entities": [{"text": "INTROVERT-EXTROVERT", "start_pos": 60, "end_pos": 79, "type": "METRIC", "confidence": 0.9878258109092712}, {"text": "INTUITIVE-SENSING", "start_pos": 84, "end_pos": 101, "type": "METRIC", "confidence": 0.8000864386558533}, {"text": "THINKING-FEELING", "start_pos": 103, "end_pos": 119, "type": "METRIC", "confidence": 0.9419695138931274}, {"text": "JUDGING-PERCEIVING", "start_pos": 121, "end_pos": 139, "type": "METRIC", "confidence": 0.8864753246307373}, {"text": "INTJ", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.5962061285972595}]}, {"text": "MBTIs have the distinct advantage of being readily available in large quantities on social media.", "labels": [], "entities": [{"text": "MBTIs", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.49914273619651794}]}, {"text": "We are aware of the ongoing discussion in the psychological literature about the limited expressiveness of MBTI, and a preference for Big Five).", "labels": [], "entities": [{"text": "MBTI", "start_pos": 107, "end_pos": 111, "type": "TASK", "confidence": 0.6895425915718079}]}, {"text": "We are, however, to some extent agnostic to the theoretical differences.", "labels": [], "entities": []}, {"text": "MBTI does presumably still capture aspects of the users' personality.", "labels": [], "entities": [{"text": "MBTI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8861543536186218}]}, {"text": "In fact, several dimensions are correlated to the Big Five.", "labels": [], "entities": []}, {"text": "Over a time frame of one week, we collect a corpus of 1.2M tweets from 1,500 users that selfidentify with an MBTI.", "labels": [], "entities": [{"text": "MBTI", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.6839595437049866}]}, {"text": "We provide an analysis of the type distribution and compare it to existing statistics for the general population.", "labels": [], "entities": []}, {"text": "We train predictive models and report performance for the individual dimensions.", "labels": [], "entities": []}, {"text": "In addition, we select the most relevant features via stability selection and find that-apart from linguistic features-gender and count statistics of the user are some of the most predictive features for several dimensions, even when controlling for gender.", "labels": [], "entities": []}, {"text": "Our results indicate that certain personality distinctions, namely INTROVERT-EXTROVERT (I-E) and THINKING-FEELING (T-F), can be predicted from social media data with high reliability, while others are very hard to model with our features.", "labels": [], "entities": [{"text": "INTROVERT-EXTROVERT (I-E)", "start_pos": 67, "end_pos": 92, "type": "METRIC", "confidence": 0.893153727054596}, {"text": "THINKING-FEELING (T-F)", "start_pos": 97, "end_pos": 119, "type": "METRIC", "confidence": 0.9342019110918045}]}, {"text": "Our open-vocabulary approach improves considerably as the amount of available data increases.", "labels": [], "entities": []}], "datasetContent": [{"text": "Model In order to predict each of the four dimensions from data, we train a logistic regression classifier.", "labels": [], "entities": []}, {"text": "As features, we use binary word ngrams (n \u2208 {1, 2, 3}), gender, and several discretized count-based meta-features, i.e., counts of tweets, followers, statuses (total of tweets and retweets), favorites (number of favorited tweets) and listed counts (number of lists on which the Twitter user appears).", "labels": [], "entities": []}, {"text": "Preliminary experiments showed that removing stopwords (and thus, removing personal pronouns) harms performance.", "labels": [], "entities": []}, {"text": "The data is pre-processed, i.e., tokenized, 4 hashtags, URLs and usernames are replaced with unique tokens.", "labels": [], "entities": []}, {"text": "We also remove any tweets containing a mention of one of the 16 MBTIs.", "labels": [], "entities": []}, {"text": "Feature selection In addition to type prediction, we perform feature selection to obtain insights into the classes.", "labels": [], "entities": [{"text": "type prediction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.9134403467178345}]}, {"text": "We use stability selection to select the most discriminative features.", "labels": [], "entities": []}, {"text": "We do not use the results of this selection in the predictive models.", "labels": [], "entities": []}, {"text": "We want to find the features that carry a high weight, irrespective of the conditions, in the entire data set.", "labels": [], "entities": []}, {"text": "The conditions in this case are the data composition and regularization.", "labels": [], "entities": []}, {"text": "In order to simulate different data compositions, we sample 100 times from the data.", "labels": [], "entities": []}, {"text": "We use a sample size of 75% with replacement.", "labels": [], "entities": []}, {"text": "For each sample, we fit a logistic regression model with a randomly set L 1 regularization constant, which encourages sparse feature weights.", "labels": [], "entities": []}, {"text": "We average the weight vectors of all 100 induced models and select the features with the highest positive weight, representing the probability of being selected in each sample.", "labels": [], "entities": []}, {"text": "Tokenizer from: http://wwbp.org/ 94 shows the prediction accuracy fora majority-class baseline and our models on the full data set (10-fold cross-validation).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.5750293731689453}]}, {"text": "While the model clearly improves on the I-E and F-T distinctions, we see no improvements over the baseline for S-N, and even a slight drop for P-J.", "labels": [], "entities": []}, {"text": "This indicates that for the latter two dimensions, we either do not have the right features, or there is not linguistic evidence for them, given that they are more related to perception.", "labels": [], "entities": []}, {"text": "The results from on Dutch essays also suggest that P-J is difficult to learn.", "labels": [], "entities": [{"text": "Dutch essays", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.8689349293708801}]}, {"text": "Given the heavy gender-skew of our data, we run additional experiments in which we control for gender.", "labels": [], "entities": []}, {"text": "The gender-controlled dataset contains 1070 authors.", "labels": [], "entities": []}, {"text": "The results in show the same tendency as in the previous setup.", "labels": [], "entities": []}, {"text": "shows the effect of increased data size on prediction accuracy for the two best dimensions.", "labels": [], "entities": [{"text": "prediction", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.9425542950630188}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.935604989528656}]}, {"text": "Already from as little as 100 tweets, our model outperforms the baseline and is comparable to other studies.", "labels": [], "entities": []}, {"text": "More data leads to better prediction accuracy.", "labels": [], "entities": [{"text": "prediction", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.9567058086395264}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.91341233253479}]}, {"text": "For I-E, there seems to be more headroom, while the accuracy of T-F plateaus after 500 tweets in the original dataset and slightly decreases in the gender-controlled setup.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9995077848434448}]}, {"text": "The trend on I-E also holds when controlling for gender as a confounding factor, while for T-F the highest performance is obtained with 500 tweets/user.", "labels": [], "entities": []}, {"text": "In general, though, the results emphasize the benefits of large-scale analysis, especially for distinguishing the I-E dimension.", "labels": [], "entities": []}, {"text": ": Learning curves and majority baselines for I-E and T-F on whole data set (top) and gender-balanced (bottom).", "labels": [], "entities": []}, {"text": "x-axis = #tweets/user, y-axis = classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9632779955863953}]}, {"text": "shows the top 10 features for I-E and F-T found by stability selection.", "labels": [], "entities": []}, {"text": "Our results show that linguistic features are by far the most predictive features for personality.", "labels": [], "entities": []}, {"text": "However, meta-features of the user account can also provide strong cues.", "labels": [], "entities": []}, {"text": "More followers seem to indicate extroverts: a follower count of 100-500 users is a moderately strong indicator for extroverts (0.37).", "labels": [], "entities": []}, {"text": "Interestingly, a status count of 1000-5000 tweets is a strong feature for introvert prediction (0.77), while less than 500 statuses correlate with extroverts (0.43).", "labels": [], "entities": [{"text": "introvert prediction", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.8647487759590149}]}, {"text": "Similarly, if a user is member of 5-50 lists, it is indicative of introverts (0.64), while being in less than 5 lists is predictive of extroverts (0.55).", "labels": [], "entities": []}, {"text": "These results support the finding that introverts prefer online media for communication.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy for four discrimination tasks  with 2000 tweets/user.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980440139770508}]}, {"text": " Table 4: Prediction performance for four discrim- ination tasks with 2000 tweets/user controlled for  gender.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9504848718643188}]}]}