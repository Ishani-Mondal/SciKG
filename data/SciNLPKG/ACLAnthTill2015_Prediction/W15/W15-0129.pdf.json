{"title": [], "abstractContent": [{"text": "Computationally detecting the accepting/rejecting force of an utterance in dialogue is often a complex process.", "labels": [], "entities": [{"text": "Computationally detecting the accepting/rejecting force of an utterance in dialogue", "start_pos": 0, "end_pos": 83, "type": "TASK", "confidence": 0.8077859381834666}]}, {"text": "In this paper we focus on a class of utterances we call pragmatic rejections, whose rejection force arises only by pragmatic means.", "labels": [], "entities": []}, {"text": "We define the class of pragmatic rejections, present a novel corpus of such utterances, and introduce a formal model to compute what we call rejections-by-implicature.", "labels": [], "entities": []}, {"text": "To investigate the perceived rejection force of pragmatic rejections, we conduct a crowdsourcing experiment and compare the experimental results to a computational simulation of our model.", "labels": [], "entities": []}, {"text": "Our results indicate that models of rejection should capture partial rejection force.", "labels": [], "entities": []}], "introductionContent": [{"text": "Analysing meaning in dialogue faces many particular challenges.", "labels": [], "entities": [{"text": "Analysing meaning in dialogue", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9225036054849625}]}, {"text": "A fundamental one is to keep track of the information the conversing interlocutors mutually take for granted, their common ground.", "labels": [], "entities": []}, {"text": "Knowledge of what is-and what is not-common ground can be necessary to interpret elliptical, anaphoric, fragmented and otherwise non-sentential expressions.", "labels": [], "entities": []}, {"text": "Establishing and maintaining common ground is a complicated process, even for human interlocutors.", "labels": [], "entities": [{"text": "Establishing and maintaining common ground", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7441574573516846}]}, {"text": "A basic issue is to determine which proposals in the dialogue have been accepted and which have been rejected: Accepted proposals are committed to common ground; rejected ones are not.", "labels": [], "entities": []}, {"text": "An important area of application is the automated summarisation of meeting transcripts, where it is vital to retrieve only mutually agreed propositions ().", "labels": [], "entities": [{"text": "summarisation of meeting transcripts", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.8421250879764557}]}, {"text": "Determining the acceptance or rejection function of an utterance can be a highly nontrivial matter as the utterance's surface form alone is oftentimes not explicit enough).", "labels": [], "entities": [{"text": "Determining the acceptance or rejection function of an utterance", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.726110577583313}]}, {"text": "Acceptance may merely be inferable from a relevant next contribution, and some rejections require substantial contextual awareness and inference capabilities to be detected-for example, when the intuitive meaning of 'yes' and 'no' is reversed, as in (1), or when the rejection requires some pragmatic enrichment, such as computing presuppositions in (2): (1) A: TVs aren't capable of sending.", "labels": [], "entities": [{"text": "Acceptance", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8227712512016296}]}, {"text": "B: Yes they are.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9492812156677246}]}, {"text": "rejection Our main concern in this paper are rejections like (2) whose rejection force can only be detected by pragmatic means.", "labels": [], "entities": []}, {"text": "Aside from presupposition failures, we are particularly concerned with rejections related to implicatures: either rejections-of-implicatures or rejections-by-implicature as in the following examples of scalar implicatures: 2 In both examples, B's utterances do not seem to (fully) agree with their antecedent: In (3) B can betaken to implicate 'good not brilliant', thereby disagreeing with A's assertion; in (4), B can betaken to reject the same implicature attributed to A.", "labels": [], "entities": []}, {"text": "We consider both examples to be what we call pragmatic rejections: utterances whose rejection force is indeterminable by purely semantic means.", "labels": [], "entities": []}, {"text": "A particular feature of such rejections is that they are prima facie not in logical contradiction with their antecedent.", "labels": [], "entities": []}, {"text": "Yet, as pointed out by, a widespread consent identifies rejection force with contradicting content.", "labels": [], "entities": []}, {"text": "We proceed as follows: In the next section, we give a more comprehensive account of what we introduced in the previous paragraph, offer a precise definition of the term pragmatic rejection, and discuss some associated problems.", "labels": [], "entities": [{"text": "pragmatic rejection", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7383026480674744}]}, {"text": "Afterwards, we review related literature, both on the topic of rejection computing and on the pragmatics of positive and negative answers.", "labels": [], "entities": [{"text": "rejection computing", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8799501657485962}]}, {"text": "The main contributions of our work area novel corpus of pragmatic rejections (Section 4), a formal model to compute rejections-by-implicature (Section 5), and a crowdsourcing experiment to gather agreement/disagreement judgements.", "labels": [], "entities": []}, {"text": "In Section 6, we present the results of this experiment and compare them to a computational simulation of our model.", "labels": [], "entities": []}, {"text": "We summarise our findings and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to investigate the perceived rejection force of pragmatic rejections, we conducted an online annotation experiment using the corpus described in Section 4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average percentage of 'probably/definitely disagreeing' judgements by category.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9734424352645874}]}]}