{"title": [{"text": "Image Representations and New Domains in Neural Image Captioning", "labels": [], "entities": [{"text": "Image Representations", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7144530117511749}, {"text": "Neural Image Captioning", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.6490463515122732}]}], "abstractContent": [{"text": "We examine the possibility that recent promising results in automatic caption generation are due primarily to language models.", "labels": [], "entities": [{"text": "automatic caption generation", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.8624898592631022}]}, {"text": "By varying image representation quality produced by a convolutional neu-ral network, we find that a state-of-the-art neural captioning algorithm is able to produce quality captions even when provided with surprisingly poor image representations.", "labels": [], "entities": []}, {"text": "We replicate this result in anew, fine-grained, transfer learned cap-tioning domain, consisting of 66K recipe image/title pairs.", "labels": [], "entities": []}, {"text": "We also provide some experiments regarding the appropriateness of datasets for automatic captioning, and find that having multiple captions per image is beneficial, but not an absolute requirement .", "labels": [], "entities": [{"text": "automatic captioning", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.6021707653999329}]}], "introductionContent": [{"text": "Describing the content of an image is an easy task for humans, but, until recently, had been difficult or impossible for computers.", "labels": [], "entities": [{"text": "Describing the content of an image", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.9051292637983958}]}, {"text": "Recent work in computer vision has addressed this task of automatically generating the caption of an input image with promising results).", "labels": [], "entities": [{"text": "automatically generating the caption of an input image", "start_pos": 58, "end_pos": 112, "type": "TASK", "confidence": 0.7074841484427452}]}, {"text": "Several state-of-the-art approaches couple a pre-trained deep convolutional neural network (CNN) for image representation with a recurrent neural network (RNN) to generate captions that describe image content.", "labels": [], "entities": [{"text": "image representation", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7324761152267456}]}, {"text": "We consider the possibility that the generation of these captions, however, is not heavily reliant upon the image representation input.", "labels": [], "entities": []}, {"text": "For instance, if one was to train a RNN directly on image captions, one could learn a fair amount about the general language of image captions.", "labels": [], "entities": []}, {"text": "demonstrate that RNNs are capable of producing diverse and surprisingly readable sentences, given a short starting sequence of seed words.", "labels": [], "entities": []}, {"text": "Furthermore, non-neural memoization techniques like those proposed by and are capable of producing very convincing language models for particular domains.", "labels": [], "entities": []}, {"text": "While it is clear that existing algorithms do discriminate based on image inputs, it is still unclear if the apparently highly specific generated captions are primarily a result of language modeling rather than image modeling.", "labels": [], "entities": []}, {"text": "If it could be determined that either image modeling or language modeling is acting as the bottleneck in this multimodal setting, research efforts could be directed appropriately.", "labels": [], "entities": [{"text": "image modeling or language modeling", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.7074418306350708}]}, {"text": "To examine the relative multimodal modeling capacities of existing neural captioning algorithms, we execute a series of experiments where we vary image representation quality produced from a fixed CNN, and examine how the output captions are affected.", "labels": [], "entities": []}, {"text": "For two existing datasets and anew domain we analyze here, our results suggest that caption quality does not scale well with increased classification accuracy of a fixed CNN.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9306210875511169}]}, {"text": "In fact, as the testing/validation accuracy of a CNN with fixed architecture increases, all seven caption evaluation metrics we consider appear to saturate at surprisingly low classification accuracies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8940504193305969}]}, {"text": "While this does not prove that better image modeling algorithms could not produce better captions, it appears that many apparently fine-grained aspects of generated natural language are the result of surprisingly coarse grained visual distinctions.", "labels": [], "entities": [{"text": "image modeling", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.7347258627414703}]}, {"text": "For a fixed vision model, our results indicate that there is likely little room for caption improvement via gathering more training images alone.", "labels": [], "entities": [{"text": "caption", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9518252015113831}]}, {"text": "We further postulate that progress could be made most quickly through the development of language modeling techniques that take better advantage of existing image representations.", "labels": [], "entities": []}, {"text": "In particular, coupling our results with independent but consistent observations made by and regarding model modifications that lead to overfitting, it's very likely that overfitting language models to image features is still a big problem for many caption generation algorithms.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 249, "end_pos": 267, "type": "TASK", "confidence": 0.896432489156723}]}, {"text": "Our analysis highlights what we believe to bean important question for these types of algorithms going forward: if better image representations contain useful, fine-grained information, is it possible to take advantage of that information without overfitting?", "labels": [], "entities": []}, {"text": "To supplement our analysis of image representations, we consider anew caption generating task: generating recipe titles based on images of food.", "labels": [], "entities": [{"text": "caption generating", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.908102959394455}]}, {"text": "The motivation for this new task results from the intuition that image representations might matter more in visually fine-grained domains, where algorithms must be able to discriminate between minute changes in the input images.", "labels": [], "entities": []}, {"text": "We collect a dataset consisting of images of food coupled with recipe titles (e.g. \"thai chicken curry\") from Yummly.com for this purpose.", "labels": [], "entities": [{"text": "Yummly.com", "start_pos": 110, "end_pos": 120, "type": "DATASET", "confidence": 0.9304977059364319}]}, {"text": "When compared to captioning the coarse-grained ImageNet domain, the specificity of our food dataset calls for more subtle visual discrimination.", "labels": [], "entities": []}, {"text": "Instead of learning a food image representing CNN from scratch to derive representations, we apply transfer learning on a dataset of 101K food images.", "labels": [], "entities": []}, {"text": "Using this approach, we significantly surpass current state-of-the-art performance fora classification task on this dataset, despite using a somewhat outdated deep architecture.", "labels": [], "entities": []}, {"text": "We further demonstrate that this transfer learning process does indeed improve food captioning, though we observe a similar \"flattening\" of all linguistic evaluation metrics, after a point.", "labels": [], "entities": [{"text": "food captioning", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.7172614187002182}]}], "datasetContent": [{"text": "To evaluate captions, we use BLEU-{1,2,3,4}) METEOR) and CIDEr/CIDEr-D).", "labels": [], "entities": [{"text": "BLEU-{1,2,3,4})", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.9423378258943558}, {"text": "METEOR", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.5800937414169312}]}, {"text": "BLEU-n is a precision measure over n-grams, whereas METEOR is a more sophisticated metric that involves the computation of an alignment between candidate and reference captions; both were originally conceived in the context of machine translation.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9897103905677795}, {"text": "precision measure", "start_pos": 12, "end_pos": 29, "type": "METRIC", "confidence": 0.9794640243053436}, {"text": "METEOR", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9753595590591431}, {"text": "machine translation", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.7825285494327545}]}, {"text": "CIDEr/CIDEr-D was created to evaluate captions of images and focuses on consensus, particularly in cases where there are multiple reference captions.", "labels": [], "entities": []}, {"text": "After establishing that a CNN could be transfer learned to classify images of dishes at state-of-theart performance, we were able to shift our focus to caption generation in a food domain.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.9493975341320038}]}, {"text": "The food dataset we collect contains roughly 66K recipes, each consisting of a single image-: Examples of the captioning system output on several images.", "labels": [], "entities": []}, {"text": "The first row of images represents images that are well captioned.", "labels": [], "entities": []}, {"text": "The second row represents different types of images the system believes to be sandwiches.", "labels": [], "entities": []}, {"text": "The third row represents images that the system has captioned incorrectly.", "labels": [], "entities": []}, {"text": "This data was taken from Yummly.com, a website that aggregates and performs analysis of millions of recipes.", "labels": [], "entities": [{"text": "Yummly.com", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8492070436477661}]}, {"text": "Out of the 66K recipes, 6K are reserved for testing, 6K are designated as a validation set, and the remaining 54K are used for model training.", "labels": [], "entities": []}, {"text": "This dataset differs from the Flickr datasets and MSCOCO both in terms of vocabulary and in terms of image content.", "labels": [], "entities": [{"text": "Flickr datasets", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.970807284116745}, {"text": "MSCOCO", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8870276212692261}]}, {"text": "The vocabulary size per image is smaller than any of the other datasets by a wide margin (see.", "labels": [], "entities": []}, {"text": "While it's clear the vision task requires more subtle distinction when compared to ImageNet, because the average caption length is shorter, it's ambiguous as to whether or not the Yummly language generation task is particularly \"fine-grained.\" presents some baseline results using the algorithms listed.", "labels": [], "entities": [{"text": "Yummly language generation task", "start_pos": 180, "end_pos": 211, "type": "TASK", "confidence": 0.6945145651698112}]}, {"text": "Common-3 predicts a reasonable ordering of the three most common words (\"with chicken and\") for all captions.", "labels": [], "entities": []}, {"text": "Nearest neighbor predicts the caption of nearest neighbor in the transfer-learned 4096-dimensional embedding space.", "labels": [], "entities": []}, {"text": "Common-Tri/Bi predict the most common tri/bigram in our dataset (\"macaroni and cheese\"/\"ice cream\") for all images.", "labels": [], "entities": []}, {"text": "We vary image representation quality as follows: for the Flickr8k and Flickr30k datasets, we compute the representations given by snapshots of AlexNet taken mid-training on the ILSVRC2012 (Russakovsky et al., 2015) task.", "labels": [], "entities": [{"text": "Flickr8k", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9131107926368713}, {"text": "Flickr30k datasets", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.9353964328765869}, {"text": "ILSVRC2012 (Russakovsky et al., 2015) task", "start_pos": 177, "end_pos": 219, "type": "DATASET", "confidence": 0.8286097248395284}]}, {"text": "We use snapshots taken at intervals of 10k from 0k (random initialization) to 100k iterations.", "labels": [], "entities": []}, {"text": "While this range of iterations is before the model has entirely converged, the rank-1 classification accuracy of the trained CNN over the ImageNet validation set increases from roughly 0% to over 40% during this time (after the model converges at 450k iterations, the rank-1 validation accuracy is 57%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.879459023475647}, {"text": "ImageNet validation set", "start_pos": 138, "end_pos": 161, "type": "DATASET", "confidence": 0.8938360015551249}]}, {"text": "From the standpoint of examining representation quality, this set of snapshots is important because this is likely where the network is learning most of its layer-by-layer abstractions, and the behavior of the network after 100k iterations can be extrapolated based on the data we analyze here.", "labels": [], "entities": []}, {"text": "Ina similar fashion, for Yummly we compute representations generated by snapshots of the transfer learned network at intervals of 10k from 0k to 90k, though our starting point is a fullyconverged CNN that produces 57% rank-1 accuracy on ImageNet's validation set.", "labels": [], "entities": [{"text": "Yummly", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.7610278725624084}, {"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9885108470916748}, {"text": "ImageNet's validation set", "start_pos": 237, "end_pos": 262, "type": "DATASET", "confidence": 0.8288032412528992}]}, {"text": "We train 5 NIC models from a random initialization per CNN for Flickr8k and Yummly, and 2-4 NIC models per CNN for Flickr30k.", "labels": [], "entities": [{"text": "Flickr8k", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9657512307167053}, {"text": "Yummly", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.6082417964935303}, {"text": "Flickr30k", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.9751445651054382}]}, {"text": "Every data point described in the following section is the result of up to six days of parallel computation using a modern 4/8-core machine.", "labels": [], "entities": []}, {"text": "It should be noted that test/validation accuracy of these CNNs is not monotonically increasing with snapshot number.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.98439621925354}]}, {"text": "While the trend is that training CNNs for more iterations results in higher accuracy, there is some noise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9990853071212769}]}, {"text": "For instance, for the Food-101 transfer learned CNN, rank-1 test accuracy drops from 61% to 60% over the snapshots extracted at 10k and 20k iterations respectively, before abruptly jumping to 66% testing accuracy in the next 10k iterations.", "labels": [], "entities": [{"text": "Food-101 transfer learned CNN", "start_pos": 22, "end_pos": 51, "type": "DATASET", "confidence": 0.9548023343086243}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9216018319129944}, {"text": "accuracy", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.8757067918777466}]}], "tableCaptions": [{"text": " Table 1: Yummly baseline BLEU-{1,2,3,4} scores  for several baselines and two high performing lan- guage generation algorithms.", "labels": [], "entities": [{"text": "Yummly baseline BLEU-{1,2,3,4} scores", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.8768527933529445}]}, {"text": " Table 2: Effect on caption quality when using  the fully converged AlexNet and VGGNet on  Flickr8k. Significance for all 3 statistical tests that  there was a true difference between the subsetting  techniques: ***p < .001, **p < .01, *p < .05", "labels": [], "entities": [{"text": "AlexNet", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9409852623939514}, {"text": "VGGNet", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.9036225080490112}, {"text": "Flickr8k", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.8868019580841064}]}]}