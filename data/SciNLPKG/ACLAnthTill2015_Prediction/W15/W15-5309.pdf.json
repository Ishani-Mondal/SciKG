{"title": [{"text": "Experiments on Active Learning for Croatian Word Sense Disambiguation", "labels": [], "entities": [{"text": "Croatian Word Sense Disambiguation", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7226396352052689}]}], "abstractContent": [{"text": "Supervised word sense disambiguation (WSD) has been shown to achieve state-of-the-art results but at high annotation costs.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.775292749206225}]}, {"text": "Active learning can ameliorate that problem by allowing the model to dynamically choose the most informative word contexts for manual labeling.", "labels": [], "entities": []}, {"text": "In this paper we investigate the use of active learning for Croa-tian WSD.", "labels": [], "entities": [{"text": "Croa-tian WSD", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.5427843779325485}]}, {"text": "We adopt a lexical sample approach and compile a corresponding sense-annotated dataset on which we evaluate our models.", "labels": [], "entities": []}, {"text": "We carryout a detailed investigation of the different active learning setups, and show that labeling as few as 100 instances suffices to reach near-optimal performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is the task of computationally determining the meaning of a word in its context.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8400265822807947}]}, {"text": "WSD is considered one of the central tasks of natural language processing (NLP).", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8627339601516724}, {"text": "natural language processing (NLP)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7990911503632864}]}, {"text": "A number of NLP applications can benefit from WSD, most notably machine translation, information retrieval (, and information extraction;).", "labels": [], "entities": [{"text": "WSD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9610282778739929}, {"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8259147107601166}, {"text": "information retrieval", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8376155495643616}, {"text": "information extraction", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.8225657641887665}]}, {"text": "At the same time, WSD is also considered a very difficult task; the difficulty arises from the fact that WSD relies on human knowledge and that it lends itself to different formalizations (e.g., the choice of a sense inventory).", "labels": [], "entities": [{"text": "WSD", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9911177158355713}, {"text": "WSD", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9647595286369324}]}, {"text": "The two main approaches to WSD are knowledge-based and supervised.", "labels": [], "entities": [{"text": "WSD", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9866707921028137}]}, {"text": "Knowledgebased approaches rely on lexical knowledge bases such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9718762636184692}]}, {"text": "The drawback of knowledgebased approaches is that the construction of largescale lexical resources requires a tremendous effort, rendering such approaches particularly costineffective for smaller languages.", "labels": [], "entities": []}, {"text": "On the other hand, supervised approaches do not rely on lexical resources and generally outperform knowledgebased approaches.", "labels": [], "entities": []}, {"text": "However, supervised methods instead require a large amount of hand-annotated data, which is also extremely expensive and time-consuming to obtain.", "labels": [], "entities": []}, {"text": "Interestingly enough, estimates that a wide coverage WSD system for English would require a sense-tagged corpus of 3200 words with 1000 instances per word.", "labels": [], "entities": []}, {"text": "Assuming human throughput of one instance per minute, this amounts to an immense effort of 27 man-years.", "labels": [], "entities": []}, {"text": "One way of addressing the lack of manually sense-tagged data is to rely on semi-supervised learning, which, along with a smaller set of labeled data, also makes use of a typically much larger set of unlabeled data.", "labels": [], "entities": []}, {"text": "A related technique is that of active learning.", "labels": [], "entities": []}, {"text": "However, what differentiates active learning from ordinary semi-supervised learning is that the former requires subsequent manual labeling.", "labels": [], "entities": []}, {"text": "The underlying idea is to minimize the annotation effort by dynamically selecting the most informative unlabeled instances, i.e., the most informative contexts of a polysemous word to be manually labeled.", "labels": [], "entities": []}, {"text": "In this paper we address the WSD task for Croatian using active learning (AL).", "labels": [], "entities": [{"text": "WSD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9743908643722534}]}, {"text": "Croatian is an underresourced language, lacking large-scale lexical resources and sense-annotated corpora.", "labels": [], "entities": []}, {"text": "Our ultimate goal is to develop a cost-effective WSD system with a reasonable coverage for the most frequent Croatian words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9778059720993042}]}, {"text": "As a first step towards that goal, in this paper we present a preliminary, small-scale but thorough empirical study using different AL setups.", "labels": [], "entities": []}, {"text": "We adopt the lexical sample evaluation setup and evaluate our models on a chosen set of polysemous words.", "labels": [], "entities": []}, {"text": "The contribution of our work is two-fold.", "labels": [], "entities": []}, {"text": "First, we present a small sense-annotated datasetthe first such dataset for Croatian -which we also make freely available for research purposes.", "labels": [], "entities": []}, {"text": "Secondly, we investigate in detail the performance of various AL models on this dataset and derive preliminary findings and recommendations.", "labels": [], "entities": []}, {"text": "Although our focus is on Croatian, we believe our results generalize to other typologically similar (in particular Slavic) languages.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we give a brief overview of ALbased WSD.", "labels": [], "entities": [{"text": "ALbased WSD", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.7493730187416077}]}, {"text": "In Section 3, we describe the manually sense-annotated dataset for Croatian.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the AL-based WSD models, while in Section 5 we present and discuss the experimental results.", "labels": [], "entities": [{"text": "WSD", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8172652125358582}]}, {"text": "Lastly, Section 6 concludes the paper and outlines future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work we adopt the lexical sample style evaluation, i.e., we select a set of words and sample sentences from a corpus containing these words.", "labels": [], "entities": []}, {"text": "We next describe how we compiled and senseannotated the sample.", "labels": [], "entities": []}, {"text": "In this section we describe the AL experiments on our lexical sample dataset.", "labels": [], "entities": []}, {"text": "We randomly split the dataset into a training and a test set: for each of the six words, we use 400 instances for training and 100 for testing.", "labels": [], "entities": []}, {"text": "For AL experiments we use the same train-test split as before.", "labels": [], "entities": []}, {"text": "The difference is that, for each word, the initial training set L is a randomly chosen subset of the full training set.", "labels": [], "entities": []}, {"text": "In what follows, to obtain robust performance estimates, we run 50 trials of each experiment, each time random sampling anew the set L, and then averaging the results.", "labels": [], "entities": []}, {"text": "AL is governed by a number of parameters: the choice of the sampling method, train growth size G, and the size of the initial training set L.", "labels": [], "entities": []}, {"text": "To more clearly show the effectiveness of AL, we set G to 1 and the size of the initial training set to 20, but elaborate on this choice later.", "labels": [], "entities": [{"text": "AL", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.938980758190155}, {"text": "G", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9975872039794922}]}, {"text": "For the C parameter we use the same value as above, i.e., the value optimized using crossvalidation on the entire training set.", "labels": [], "entities": []}, {"text": "Arguably, this is not a realistic AL setup, as in practice the entire training is not labeled upfront.", "labels": [], "entities": []}, {"text": "In this work, however, we decided to simplify the setup as we observed that on our dataset the optimal C value is rather stable regardless of the training set size.", "labels": [], "entities": []}, {"text": "The purpose of AL is to reduce the labeling effort, i.e., to achieve a satisfactory level of accuracy with a smaller number of training instances.", "labels": [], "entities": [{"text": "AL", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.8844331502914429}, {"text": "labeling", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.960686981678009}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9980205297470093}]}, {"text": "To analyze the effectiveness of AL WSD on our lexical sample, we compute the learning curves for SVM-BoW and SVM-SG and the three considered uncertainty sampling methods.", "labels": [], "entities": [{"text": "AL WSD", "start_pos": 32, "end_pos": 38, "type": "TASK", "confidence": 0.4888896942138672}, {"text": "SVM-BoW", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.8523813486099243}]}, {"text": "The baselines are the learning curves obtained using random sampling (RAND).", "labels": [], "entities": []}, {"text": "shows the learning curves and the standard deviation bands.", "labels": [], "entities": []}, {"text": "The first thing we observe is that all uncertainty sampling methods outperform RAND.", "labels": [], "entities": [{"text": "RAND", "start_pos": 79, "end_pos": 83, "type": "TASK", "confidence": 0.44090965390205383}]}, {"text": "For example, when the training set reaches 100 instances, AL with uncertainty sampling outperforms RAND by \u223c2% of accuracy for both SVM-BoW and SVM-SG models.", "labels": [], "entities": [{"text": "RAND", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9040594696998596}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9992808699607849}]}, {"text": "In our view, this performance gain justifies the use of AL WSD on our dataset.", "labels": [], "entities": []}, {"text": "The second thing we observe is that the three uncertainty sampling methods generally perform comparably.", "labels": [], "entities": []}, {"text": "However, the least confident (LC) and maximum margin (MM) methods slightly outperform the maximum entropy (ME) method in the 100-150 instances range.", "labels": [], "entities": [{"text": "maximum margin (MM)", "start_pos": 38, "end_pos": 57, "type": "METRIC", "confidence": 0.893023669719696}]}, {"text": "The last thing we observe is that, with uncertainty sampling, labeling as few as 100 out of 400 training instances already gives \u223c0.94% of maximum accuracy for SVM-BoW, while random sampling requires a training set of twice that size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9990667700767517}]}, {"text": "Moreover, labeling 150 instances gives almost maximum accuracy for SVM-BoW.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9993879795074463}]}, {"text": "For SVM-SG, the effect of uncertainty sampling is even more pronounced -with 100 instances we already reach performance equivalent to that on the full training set.", "labels": [], "entities": []}, {"text": "We conclude that AL WSD with SVM-SG reduces the number of training instances to 100 without any drop in performance.", "labels": [], "entities": [{"text": "AL WSD", "start_pos": 17, "end_pos": 23, "type": "TASK", "confidence": 0.5001465082168579}]}, {"text": "Taking into account the previous observations, we decided to use the SVM-SG model and MM uncertainty sampling in subsequent experiments.", "labels": [], "entities": [{"text": "MM uncertainty sampling", "start_pos": 86, "end_pos": 109, "type": "METRIC", "confidence": 0.4545665383338928}]}, {"text": "To investigate the impact of the initial training set size Land the train growth size G, we run a grid search with L \u2208 {20, 50, 100} and G \u2208 {1, 5, 10}.", "labels": [], "entities": []}, {"text": "For each pair of parameter values, we carryout 50 AL runs per word, each time using a random sample of size L as the initial training set.", "labels": [], "entities": []}, {"text": "We thus obtain a total of 300 runs per parameter pair, which we average to produce corresponding learning curves.", "labels": [], "entities": []}, {"text": "We compare the AL WSD performance in terms of the Area Under Learning Curve (ALC), which we define as a sum of classifier accuracy scores across the iterations of the AL algorithm, normalized by the number of iterations.", "labels": [], "entities": [{"text": "Area Under Learning Curve (ALC)", "start_pos": 50, "end_pos": 81, "type": "METRIC", "confidence": 0.6803797142846244}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.8638639450073242}]}, {"text": "shows the ALC scores for different parameter combinations.", "labels": [], "entities": []}, {"text": "Expectedly, the larger the initial training set L, the more information is avail-  able to the learning algorithm upfront.", "labels": [], "entities": []}, {"text": "At the same time, using a train growth size G of one yields better models, as they are able to make more confident predictions on yet unlabeled instances in each iteration of the AL algorithm.", "labels": [], "entities": []}, {"text": "Nonetheless, we observe that in our case these two AL parameters do not considerably affect the model performance.", "labels": [], "entities": []}, {"text": "In the previous analyses we looked at learning curves averaged over the six words in our dataset.", "labels": [], "entities": []}, {"text": "For a more detailed analysis, we turn to the learning curves of the individual words, shown in.", "labels": [], "entities": []}, {"text": "We plot both the accuracy on the training set and the test set using the MM sampling method, as well as the RAND accuracy on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994946718215942}, {"text": "RAND accuracy", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.843669056892395}]}, {"text": "Note that a large gap between the curves on the training set and test set indicates model overfitting.", "labels": [], "entities": []}, {"text": "The plots reveal that MM outperforms the RAND baseline for all six words.", "labels": [], "entities": [{"text": "MM", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.6737360954284668}, {"text": "RAND baseline", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.5081933587789536}]}, {"text": "Moreover, the gain is most prominent for vatra, lak, and brusiti.", "labels": [], "entities": []}, {"text": "On odlikovati the full maximum accuracy can already be reached with as few as 60 training instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9993994235992432}]}, {"text": "In contrast, the word prljav is a problematic one: the learning curve does not seem to get saturated even after 400 instances.", "labels": [], "entities": []}, {"text": "This is probably due to the many NOTA labels for that words.", "labels": [], "entities": [{"text": "NOTA", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.6706097722053528}]}, {"text": "The train-test curve gap is the largest for lak, suggesting that the model overfits the most on that particular word.", "labels": [], "entities": []}, {"text": "We hypothesize that, for some reason, the instances of this word are more noisy than instances of other words.", "labels": [], "entities": []}, {"text": "Because disagreements in our dataset have been manually resolved, we think that latent variables area more likely explanation for the noise than mislabelings.", "labels": [], "entities": []}, {"text": "In other words, we believe that for some reason skip-gram contexts are less informative of the senses of the word lak than of the other words.", "labels": [], "entities": []}, {"text": "Another interesting observation is that for some words the accuracy rises above that of a model trained on the entire training set of 400 instances, after which it drops and eventually the two accuracy curves converge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9995847344398499}, {"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9982091188430786}]}, {"text": "This effect is most prominent for vatra and brusiti, and somewhat less for okvir and lak.", "labels": [], "entities": []}, {"text": "A similar effect has been observed by on some English verbs, suggesting that the effect can be traced down to model starting to overfit at some point.", "labels": [], "entities": []}, {"text": "We think that this hypothesis is plausible, as it is also confirmed by the fact that we observe no drop in the training error.", "labels": [], "entities": []}, {"text": "Moreover, we hypothesize that the drop inaccuracy is due to the sampling of a sequence of noisy examples from the training set.", "labels": [], "entities": []}, {"text": "By the same token as before, we tend to exclude mislabelings as the cause of the noise, but rather attribute the noise to non-informative contexts.", "labels": [], "entities": []}, {"text": "The existence of such \"bad examples\" was hypothesized by, who suggest that that adequate feature selection could solve the problem.", "labels": [], "entities": []}, {"text": "We leave a detailed investigation for future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cohen's \u03ba for the six chosen words.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of the gold standard sample.", "labels": [], "entities": [{"text": "gold standard sample", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8905127843221029}]}, {"text": " Table 4: Supervised models accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.99726402759552}]}]}