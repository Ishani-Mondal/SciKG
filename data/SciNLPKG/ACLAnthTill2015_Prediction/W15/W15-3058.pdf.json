{"title": [{"text": "Dependency Analysis of Scrambled References for Better Evaluation of Japanese Translations", "labels": [], "entities": [{"text": "Dependency Analysis of Scrambled References", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8717901468276977}, {"text": "Evaluation of Japanese Translations", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.5795670449733734}]}], "abstractContent": [{"text": "In English-to-Japanese translation, BLEU (Papineni et al., 2002), the de facto standard evaluation metric for machine translation (MT), has very weak correlation with human judgments (Goto et al., 2011; Goto et al., 2013).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9987407326698303}, {"text": "machine translation (MT)", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.8450140714645386}]}, {"text": "Therefore, RIBES (Isozaki et al., 2010; Hirao et al., 2014) was proposed.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8774768710136414}]}, {"text": "RIBES measures similarity of the word order of a machine-translated sentence and that of a corresponding human-translated reference sentence.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6403388977050781}, {"text": "similarity", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9684265851974487}]}, {"text": "RIBES has much stronger correlation than BLEU but most Japanese sentences have alternative word orders (scrambling), and one reference sentence is not sufficient for fair evaluation.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8612114787101746}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9972507357597351}]}, {"text": "(2014) proposed a solution to this problem.", "labels": [], "entities": []}, {"text": "This solution generates semantically equivalent word orders of reference sentences.", "labels": [], "entities": []}, {"text": "Automatically generated word orders are sometimes incomprehensible or misleading , and they introduced a heuristic rule that filters out such bad sentences.", "labels": [], "entities": []}, {"text": "However , their rule is too conservative and generated alternative word orders for only 30% of reference sentences.", "labels": [], "entities": []}, {"text": "In this paper, we present a rule-free method that uses a dependency parser to check scrambled sentences and generated alternatives for 80% of sentences.", "labels": [], "entities": []}, {"text": "The experimental results show that our method improves sentence-level correlation with human judgments.", "labels": [], "entities": []}, {"text": "In addition, strong system-level correlation of single reference RIBES is not damaged very much.", "labels": [], "entities": []}, {"text": "We expect this method can be applied to other languages such as German, Korean, * This work was done while the second author was a graduate student of Okayama Prefectural University.", "labels": [], "entities": []}, {"text": "Spearman's \u03c1 with adequacy NTCIR-7 JE RIBES JE BLEU NTCIR-9 JE RIBES JE BLEU EJ RIBES EJ BLEU NTCIR-10 JE RIBES JE BLEU EJ RIBES EJ BLEU 0.0 0.2 0.4 0.6 0.8 1.0 Figure 1: RIBES has better correlation with adequacy than BLEU (system-level correlation) Turkish, Hindi, etc.", "labels": [], "entities": [{"text": "JE RIBES JE BLEU NTCIR-9 JE RIBES JE BLEU EJ RIBES EJ BLEU NTCIR-10 JE RIBES JE BLEU EJ RIBES EJ BLEU 0.0", "start_pos": 35, "end_pos": 140, "type": "METRIC", "confidence": 0.6819901284964188}, {"text": "BLEU", "start_pos": 219, "end_pos": 223, "type": "METRIC", "confidence": 0.9975022673606873}]}], "introductionContent": [{"text": "For translation among European languages, BLEU () has strong correlation with human judgments and almost all MT papers use BLEU for evaluation of translation quality.", "labels": [], "entities": [{"text": "translation among European languages", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.8338735550642014}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9987930059432983}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9974426031112671}]}, {"text": "However, BLEU has very weak correlation with human judgments in English-toJapanese/Japanese-to-English translation, and anew metric RIBES () has strong correlation with human judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9984997510910034}, {"text": "RIBES", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.9768268465995789}]}, {"text": "RIBES measures similarity of the word order of a machine translated sentence and that of a human-translated reference sentence.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6428487300872803}, {"text": "similarity", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9719654321670532}]}, {"text": "compares RIBES and BLEU in terms of Spearman's \u03c1 with human judgments of adequacy based on NTCIR-7/9/10 data (.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.782817542552948}, {"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9970535039901733}, {"text": "NTCIR-7/9/10 data", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.9518813888231913}]}, {"text": "Japanese and English have completely different word order, and phrase-based SMT systems tend to output bad word orders.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.8819950222969055}]}, {"text": "RIBES correctly points out their word order problems.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.584094226360321}]}, {"text": "In this paper, we propose a method to improve \"sentence-level correlation\", which is useful for MT developers to find problems of their MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9842455983161926}, {"text": "MT", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.9476984739303589}]}, {"text": "If the sentence-level correlation is strong, low RIBES scores indicate bad translations, and we will find typical failure patterns from them.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.8425295352935791}]}, {"text": "However, improvement of sentence-level correlation is more difficult than system-level correlation and current automatic evaluation metrics do not have strong correlation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distribution of the number of generated  permutations (#perms=1 indicates the number of  sentences for which the method didn't generate al- ternative word orders)", "labels": [], "entities": []}, {"text": " Table 2: Details of system-level RIBES scores  (NTCIR-7 EJ)", "labels": [], "entities": [{"text": "RIBES", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.7691910266876221}, {"text": "NTCIR-7 EJ", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.6482470035552979}]}]}