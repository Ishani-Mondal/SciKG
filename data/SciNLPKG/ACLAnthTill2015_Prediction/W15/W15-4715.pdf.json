{"title": [{"text": "Generating and Evaluating Landmark-based Navigation Instructions in Virtual Environments", "labels": [], "entities": [{"text": "Evaluating Landmark-based Navigation Instructions", "start_pos": 15, "end_pos": 64, "type": "TASK", "confidence": 0.8085148483514786}]}], "abstractContent": [{"text": "Referring to landmarks has been identified to lead to improved navigation instructions.", "labels": [], "entities": []}, {"text": "However, a previous corpus study suggests that human \"wizards\" also choose to refer to street names and generate user-centric instructions.", "labels": [], "entities": []}, {"text": "In this paper, we conduct a task-based evaluation of two systems reflecting the wizards' behaviours and compare them against an improved version of previous landmark-based systems , which resorts to user-centric descriptions if the landmark is estimated to be invisible.", "labels": [], "entities": []}, {"text": "We use the GRUVE virtual interactive environment for evaluation.", "labels": [], "entities": [{"text": "GRUVE", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.6402500867843628}]}, {"text": "We find that the improved system, which takes visibility into account, outper-forms the corpus-based wizard strategies, however not significantly.", "labels": [], "entities": []}, {"text": "We also show a significant effect of prior user knowledge, which suggests the usefulness of a user modelling approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of generating successful navigation instructions has recently attracted increased attention from the dialogue and Natural Language Generation (NLG) communities, e.g. ( etc.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 123, "end_pos": 156, "type": "TASK", "confidence": 0.7643723984559377}]}, {"text": "Previous research suggests that landmarkbased route instructions (e.g. \"Walk towards the Castle\") are in general preferable because they are easy to understand, e.g. (. However, landmarks might not always be visible to the user.", "labels": [], "entities": []}, {"text": "A recent corpus study by on the MapTask and two Wizard-ofOz corpora, Spacebook1 and Spacebook2, 1 empirically investigated the type of reference objects human instruction givers tend to choose under different viewpoints.", "labels": [], "entities": []}, {"text": "It was found that human \"wizards\" do not always generate instructions based on landmarks, but also choose to refer to street names or generate user-centric instructions, such as \"Continue straight\".", "labels": [], "entities": []}, {"text": "This paper compares three alternative generation strategies for choosing possible reference objects: one system reflecting an improved version of a landmark-based policy, which will resort to a user-centric description if the landmark is not visible; and two systems reflecting the wizards' behaviours in Spacebook1 and Spacebook2.", "labels": [], "entities": []}, {"text": "We hypothesise the first system will outperform the other two in terms of human-likeness and naturalness, as defined in Section 3.", "labels": [], "entities": []}, {"text": "We use the GRUVE (Giving Route Instructions in Uncertain Virtual Environments) system) to evaluate these alternatives.", "labels": [], "entities": [{"text": "GRUVE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.937929093837738}, {"text": "Giving Route Instructions in Uncertain Virtual Environments)", "start_pos": 18, "end_pos": 78, "type": "TASK", "confidence": 0.6514906734228134}]}], "datasetContent": [{"text": "We used the GRUVE virtual environment for evaluation.", "labels": [], "entities": [{"text": "GRUVE virtual environment", "start_pos": 12, "end_pos": 37, "type": "DATASET", "confidence": 0.6921114722887675}]}, {"text": "GRUVE uses Google StreetView to simulate instruction giving and following in an interactive, virtual environment, also see.", "labels": [], "entities": [{"text": "GRUVE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9333136081695557}, {"text": "instruction giving and following", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.7244319170713425}]}, {"text": "We recruited 16 subjects, with an even split amongst males and females and age ranges between 20 and 56.", "labels": [], "entities": []}, {"text": "Six users were not native English speakers.", "labels": [], "entities": []}, {"text": "Before the experiment, users were asked about their previous experience.", "labels": [], "entities": []}, {"text": "After the experiment we asked them to rate all systems on a 4-point Likert scale regarding human-likeness and naturalness (where 1 was \"Agree\" and 4 was \"Disagree\").", "labels": [], "entities": [{"text": "Agree", "start_pos": 136, "end_pos": 141, "type": "METRIC", "confidence": 0.9781820774078369}]}, {"text": "Human-likeness is defined as an instruction that could have been produced by a human.", "labels": [], "entities": []}, {"text": "Naturalness is defined as being easily understood by a human.", "labels": [], "entities": []}, {"text": "The order of systems was randomised.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average results for objective (mean) and  subjective (mode) measures.", "labels": [], "entities": []}, {"text": " Table 3: Frequencies of reference objects chosen  by each system.", "labels": [], "entities": []}]}