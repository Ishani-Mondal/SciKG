{"title": [], "abstractContent": [{"text": "In this paper we gauge the utility of general-purpose, open-domain semantic parsing for textual entailment recognition by combining graph-structured meaning representations with semantic technologies and formal reasoning tools.", "labels": [], "entities": [{"text": "open-domain semantic parsing", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.6899160146713257}, {"text": "textual entailment recognition", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.8279634118080139}]}, {"text": "Our approach achieves high precision, and in two case studies we show that when reasoning over n-best analyses from the parser the performance of our system reaches state-of-the-art for rule-based textual entailment systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9977630376815796}]}, {"text": "1 Background and Motivation There is a growing interest in recent years in general-purpose semantic parsing into graph-based meaning representations, which provide greater expressive power than tree-based structures.", "labels": [], "entities": [{"text": "general-purpose semantic parsing", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.727452298005422}]}, {"text": "Recent efforts in this spirit include, for example, Abstract Meaning Representation (Ba-narescu et al., 2013), and Semantic Dependency Parsing (SDP) (Oepen et al., 2014; Oepen et al., 2015).", "labels": [], "entities": [{"text": "Abstract Meaning Representation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.8247430324554443}, {"text": "Semantic Dependency Parsing (SDP)", "start_pos": 115, "end_pos": 148, "type": "TASK", "confidence": 0.8117010692755381}]}, {"text": "Simultaneously, in the Semantic Web community , a range of generic semantic technologies for storing and processing graph-structured data has been made available, but these have not been much used for natural language processing tasks.", "labels": [], "entities": []}, {"text": "We propose a flexible, generic framework for precision-oriented Textual Entailment (TE) recognition that combines semantic parsing, graph-based representations of sentence meaning, and semantic technologies.", "labels": [], "entities": [{"text": "precision-oriented Textual Entailment (TE) recognition", "start_pos": 45, "end_pos": 99, "type": "TASK", "confidence": 0.8265432289668492}, {"text": "semantic parsing", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7621476948261261}]}, {"text": "During the decade since the TE task was defined , (logical) inference-based approaches have made some important contributions to the field.", "labels": [], "entities": [{"text": "TE task", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.9376953542232513}]}, {"text": "Systems such as Bos and Markert (2006) and Tatu and Moldovan (2006) employ automated proof search over logical representations of the input sentences.", "labels": [], "entities": []}, {"text": "Other systems, such as Bar-Haim et al.", "labels": [], "entities": []}, {"text": "(2007), apply transformational rules to linguistic representations of the sentence pairs, and determine entailment through graph subsumption.", "labels": [], "entities": []}, {"text": "Because inference-based systems are vulnerable to incomplete knowledge in the rule set and errors in the mapping from natural language sentences to logical forms or linguistics representations, and because the definition of the TE task encourages a more relaxed, non-logical notion of entailment, the majority of TE systems have used more robust approaches, however.", "labels": [], "entities": []}, {"text": "Our work supports a notion of logical inference for TE by reasoning with formal rules over graph-structured meaning representations , while achieving results that are comparable with robust approaches.", "labels": [], "entities": [{"text": "TE", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.973256528377533}]}, {"text": "We use a freely available, grammar-driven semantic parser and a well-defined reduction of un-derspecified logical-form meaning representations into variable-free semantic graphs called Elementary Dependency Structures (EDS) (Oepen and L\u00f8nning, 2006).", "labels": [], "entities": []}, {"text": "We capitalize on a pre-existing storage and search infrastructure for EDSs using generic semantic technologies.", "labels": [], "entities": []}, {"text": "For entailment classification, we create inference rules that enrich the EDS graphs, apply the rules with a generic rea-soner, and use graph alignment as a decision tool.", "labels": [], "entities": [{"text": "entailment classification", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9566374719142914}, {"text": "graph alignment", "start_pos": 135, "end_pos": 150, "type": "TASK", "confidence": 0.7522760629653931}]}, {"text": "To test our generic setup, we perform two case studies where we replicate well-performing TE systems, one from the Parser Evaluation using Textual Entailments (PETE) task (Yuret et al., 2010), and one from SemEval 2014 Task 1 (Marelli et al., 2014).", "labels": [], "entities": [{"text": "Parser Evaluation using Textual Entailments (PETE) task", "start_pos": 115, "end_pos": 170, "type": "TASK", "confidence": 0.5928418139616648}, {"text": "SemEval 2014 Task 1", "start_pos": 206, "end_pos": 225, "type": "TASK", "confidence": 0.8161647766828537}]}, {"text": "The best published results for the PETE task, Lien (2014), were obtained through heuristic rules that align meaning representations based on structural similarity.", "labels": [], "entities": [{"text": "PETE task", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.8945797383785248}]}, {"text": "Lien and Kouylekov (2014) extend the same basic approach for SemEval 2014 by including lexical relations and negation handling.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.8876319825649261}, {"text": "negation handling", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.9448159337043762}]}, {"text": "We recast the handwritten heuristic rules from these systems as formal Semantic Web Rule Language (SWRL) rules, and run them with a generic reasoning tool over EDS 40 meaning representations.", "labels": [], "entities": []}, {"text": "The PETE contribution of Lien (2014) experimented with using n-best analyses from the parser to boost TE recall, and we can easily include n-best reasoning in our setup.", "labels": [], "entities": [{"text": "TE", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.61724454164505}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.8505129218101501}]}, {"text": "In Sections 2 and 3, we outline our approach and describe the semantic parsing setup and semantic technologies we employ.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.7442800402641296}]}, {"text": "Sections 4 and 5 detail our replication of the two TE shared tasks.", "labels": [], "entities": [{"text": "replication", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.5563275814056396}, {"text": "TE shared tasks", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7346658905347189}]}, {"text": "Finally, in Section 6, we sum up our effort and point to directions for future work.", "labels": [], "entities": []}, {"text": "2 General-purpose Semantic Parsing General-purpose, open-domain semantic parsing systems that output logical-form meaning representations are freely available today, but have not yet been widely used in TE systems.", "labels": [], "entities": [{"text": "open-domain semantic parsing", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.7114373246828715}, {"text": "TE", "start_pos": 203, "end_pos": 205, "type": "TASK", "confidence": 0.9688662886619568}]}, {"text": "For our replication of the PETE and SemEval tasks, we use the English Resource Grammar (ERG) (Flickinger, 2000), a broad-coverage HPSG-based parser.", "labels": [], "entities": []}, {"text": "The ERG has been continuously developed since around 1993, and today will typically allow parsing of 90-95% of the sentences in naturally occuring running texts of various domains and genres at average parse times of a couple of seconds per sentence.", "labels": [], "entities": [{"text": "parsing", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.9608680009841919}]}, {"text": "The ERG includes a Maximum Entropy parse ranking model that is trained on some 50,000 mixed-domain sentences ; the parser applies exact inference, i.e., constructs a complete parse forest and facilitates extraction of n-best lists of analyses in globally optimal rank order.", "labels": [], "entities": [{"text": "Maximum Entropy parse ranking", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6057839468121529}]}, {"text": "In our experiments, we use the ERG in its 1212 release version, together with its standard PET parser (Callmeier, 2002), and off-the-shelf models and settings.", "labels": [], "entities": []}, {"text": "The ERG outputs underspecified meaning representations in the Minimal Recursion Semantics (MRS) framework (Copestake et al., 2005).", "labels": [], "entities": [{"text": "Minimal Recursion Semantics (MRS)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.6928941309452057}]}, {"text": "The MRS logical-form meaning representations can be converted to EDSs, which are variable-free semantic dependency graphs.", "labels": [], "entities": [{"text": "MRS logical-form meaning", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6150202751159668}]}, {"text": "Kouylekov and Oepen (2014) recently showed that the Resource Description Framework (RDF) is suitable for representing various types of semantic graphs, and demonstrated how to embed EDS meaning representations in RDF.", "labels": [], "entities": []}, {"text": "We opt for EDS over MRS because its variable-free form integrates more naturally with RDF technologies, while still retaining the semantic information essential to entailment recognition.", "labels": [], "entities": [{"text": "MRS", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.6301707029342651}, {"text": "entailment recognition", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.7216084003448486}]}, {"text": "In the EDS example in Figure 1, each line depicts a graph node (each corresponding to one elementary predication in the original MRS), with node identifiers prefixed to the node labels (sep-arated by the colon), and a set of outgoing arcs (role-argument pairs) enclosed in parentheses.", "labels": [], "entities": []}, {"text": "The semantic arguments to the relation rep-resentend by the node are directed arcs to other nodes in the EDS graph.", "labels": [], "entities": [{"text": "EDS graph", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.9054977893829346}]}, {"text": "For instance, the node for would v modal is connected to the node for and c through an arc labeled ARG1.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.9248132109642029}]}, {"text": "The node labeled and c in turn has outgoing arcs to wake v up and fret v about.", "labels": [], "entities": []}, {"text": "The two pron nodes do not have outgoing arcs, they are connected to the structure through incoming arcs from the verb nodes.", "labels": [], "entities": []}, {"text": "Finally, each of the pronoun q nodes is connected to apron node through a BV (\"bound variable\") arc.", "labels": [], "entities": [{"text": "BV", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9871496558189392}]}, {"text": "A graphical visualization of the same graph is shown in Figure 3 (ignoring nodes and arcs shown in green there, which are added by our entailment processor).", "labels": [], "entities": []}, {"text": "There are two notable examples of logic-based TE systems that have used the ERG parser and MRS meaning representations: Wotzlaw and Coote (2013) present a TE system which combines the results of deep and shallow linguistic analyses into scope-resolved MRS representations.", "labels": [], "entities": []}, {"text": "The MRS expressions are translated into another, semantically equivalent first-order logic format, which, enriched with background knowledge , is used for the actual inference.", "labels": [], "entities": [{"text": "MRS", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8117515444755554}]}, {"text": "The system of Bergmair (2010) also uses MRS as an intermediate format in constructing meaning representations.", "labels": [], "entities": []}, {"text": "Input sentences are parsed with the ERG, and the resulting MRSs are translated into logical formulae that can be prosessed by an inference engine.", "labels": [], "entities": []}, {"text": "In contrast to these prior applications of generic semantic parsing using the ERG to the TE task, our work simplifies the scopally underspecified logical forms of MRS into more compact graph-structured representations of core predicate-argument relations, and we define TE-specialized notions of inference over these semantic graphs.", "labels": [], "entities": [{"text": "generic semantic parsing", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6267321209112803}]}, {"text": "3 Semantic Technologies and Textual Entailment Kouylekov and Oepen (2014) map different types of meaning representations, including the EDSs used in our work, to RDF graphs, stored in off-the-shelf RDF triple stores, and searched using SPARQL queries.", "labels": [], "entities": []}, {"text": "In our work, we build a TE system that utilizes their infrastructure as a basis for reasoning over EDS graphs.", "labels": [], "entities": []}, {"text": "(PETE id 5019).", "labels": [], "entities": [{"text": "PETE id 5019)", "start_pos": 1, "end_pos": 14, "type": "DATASET", "confidence": 0.9301936477422714}]}, {"text": "Textual Entailment was defined by Dagan et al.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8019539713859558}]}, {"text": "(2006) as the task of recognizing whether, given two text fragments, the meaning of one text entails the meaning of the other text.", "labels": [], "entities": []}, {"text": "The text fragments are conventionally referred to as the text T and the hypothesis H, respectively.", "labels": [], "entities": []}, {"text": "The notion of \"entail-ment\" used in TE is informal and based at least in part on general human knowledge of language and the world.", "labels": [], "entities": [{"text": "TE", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.969057559967041}]}, {"text": "Our textual entailment system uses graph alignment over EDS structures as the basis for entail-ment decisions.", "labels": [], "entities": []}, {"text": "We extend the approach by enriching the graphs in a forward-chaining spirit using SWRL rules, and the Jena reasoner 1.", "labels": [], "entities": [{"text": "Jena reasoner 1", "start_pos": 102, "end_pos": 117, "type": "DATASET", "confidence": 0.9563182592391968}]}, {"text": "After the reasoning step, the actual alignment is performed with a SPARQL query that tries to match the hypothesis graph to the text graph.", "labels": [], "entities": []}, {"text": "Along with a classification decision, the system outputs a \"proof\" by listing every SWRL rule that was used in the reasoning.", "labels": [], "entities": []}, {"text": "Ina sense, we are following the classical reasoning approach of trying to infer the hypothesis from the text.", "labels": [], "entities": []}, {"text": "3.1 SWRL Our subsumption approach to entailment recognition requires some rewriting of the EDS graphs produced by the ERG parser.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.9364992082118988}]}, {"text": "For example, the EDS graph in Figure 1 needs to be rewritten so that dependencies are propagated into the coordinate structure, which will facilitate the subsumption of subgraphs.", "labels": [], "entities": []}, {"text": "We use SWRL, a semantic web standard for reasoning over ontologies 2 , to encode rewriting rules for EDS graphs.", "labels": [], "entities": []}, {"text": "The graph structures are enriched with a set of forward-chaining SWRL rules, and, thus, our graph-rewriting approach can be seen as a form of forward-chaining 1 https://jena.apache.org/ 2 http://www.w3.org/Submission/SWRL/ inference.", "labels": [], "entities": []}, {"text": "The system uses two sets of SWRL rules, one for the text and one for the hypothesis graph.", "labels": [], "entities": []}, {"text": "The function of these rules is to further normalize and to add information to both graphs in order to make matching possible.", "labels": [], "entities": []}, {"text": "We adapt the rule sets for different data sets to accomodate variation in entail-ment phenomena.", "labels": [], "entities": []}, {"text": "The rule sets contain five types of rules: \u2022 abstraction rules \u2022 predicate simplification rules \u2022 structural rules \u2022 lexical relation rules \u2022 polarity marking rules Abstraction Rules We employ a number of abstraction rules to allow matching of indefinite and personal pronouns in the H graph to NPs in the T graph.", "labels": [], "entities": [{"text": "Abstraction", "start_pos": 165, "end_pos": 176, "type": "METRIC", "confidence": 0.9850885272026062}]}, {"text": "To be able to match the indefinite pronoun somebody to the personal pronoun he in e.g. He has a point he wants to make [...]", "labels": [], "entities": []}, {"text": "\u21d2 Somebody wants to make a point (PETE id 1026), the rules label both pronouns with the same abstraction label, i.e., they add an additional rdf:type property to these nodes, which can be used in subsequent testing for node equivalence.", "labels": [], "entities": [{"text": "PETE id 1026", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8162257671356201}]}, {"text": "Our rules also abstract over certain quantifiers.", "labels": [], "entities": []}, {"text": "In the data sets we have examined, the text and hypothesis sentence of an entailment pair often have quantifier variations that are clearly not relevant for recognizing the entailment relationship (e.g., A woman is cleaning a shrimp \u21d2 The woman is cleaning a shrimp, SemEval id 3364).", "labels": [], "entities": []}, {"text": "We group these quantifiers into candidate equivalence classes using rules of the form: 42", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of our reasoning-based system on the PETE test data.", "labels": [], "entities": [{"text": "PETE test data", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.9275619586308798}]}, {"text": " Table 2: Comparison of accuracy of RBS on the  SemEval test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9988073110580444}, {"text": "RBS", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.5397341251373291}, {"text": "SemEval test data", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.8369598388671875}]}, {"text": " Table 3: Precision, recall and F-measure of RBS  n-best on the SemEval test data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9992300271987915}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9994353652000427}, {"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996110796928406}, {"text": "RBS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.6688284873962402}, {"text": "SemEval test data", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.802227387825648}]}]}