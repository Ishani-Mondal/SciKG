{"title": [{"text": "A Bayesian Model for Generative Transition-based Dependency Parsing", "labels": [], "entities": [{"text": "Generative Transition-based Dependency Parsing", "start_pos": 21, "end_pos": 67, "type": "TASK", "confidence": 0.6204541474580765}]}], "abstractContent": [{"text": "We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 58, "end_pos": 93, "type": "TASK", "confidence": 0.6430372297763824}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9974695444107056}]}, {"text": "The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference.", "labels": [], "entities": []}, {"text": "We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees.", "labels": [], "entities": []}, {"text": "The UAS of the parser is on par with that of a greedy discriminative baseline.", "labels": [], "entities": [{"text": "UAS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9976468682289124}]}, {"text": "As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus.", "labels": [], "entities": []}, {"text": "We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7373283058404922}]}], "introductionContent": [{"text": "Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing).", "labels": [], "entities": [{"text": "Transition-based dependency parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5806627968947092}]}, {"text": "Beam-search decoding further improves performance (), but increases decoding time.", "labels": [], "entities": []}, {"text": "Graphbased parsers) perform global inference and although they are more accurate in some cases, inference tends to be slower.", "labels": [], "entities": []}, {"text": "In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing.", "labels": [], "entities": [{"text": "generative dependency parsing", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.9017507632573446}]}, {"text": "While generative models have been used widely and successfully for constituency parsing), their use in dependency parsing has been limited.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.8887003660202026}, {"text": "dependency parsing", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8616496026515961}]}, {"text": "Generative models offer a principled approach to semiand unsupervised learning, and can also be applied to natural language generation tasks.", "labels": [], "entities": [{"text": "natural language generation tasks", "start_pos": 107, "end_pos": 140, "type": "TASK", "confidence": 0.7298949956893921}]}, {"text": "Dependency grammar induction models ( are generative, but not expressive enough for high-accuracy parsing.", "labels": [], "entities": [{"text": "Dependency grammar induction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6461130678653717}]}, {"text": "A previous generative transition-based dependency parser obtains competitive accuracies, but training and decoding is computationally very expensive.", "labels": [], "entities": [{"text": "generative transition-based dependency parser", "start_pos": 11, "end_pos": 56, "type": "TASK", "confidence": 0.8611037880182266}]}, {"text": "Syntactic language models have also been shown to improve performance in speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7920533716678619}, {"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7958725094795227}]}, {"text": "However, the main limitation of most existing generative syntactic models is their inefficiency.", "labels": [], "entities": [{"text": "generative syntactic", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.9178348481655121}]}, {"text": "We propose a generative model for transitionbased parsing ( \u00a72).", "labels": [], "entities": [{"text": "transitionbased parsing", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.5961757153272629}]}, {"text": "The model, parameterized by Hierarchical Pitman-Yor Processes (HPYPs)), learns a distribution over derivations of parser transitions, words and POS tags ( \u00a73).", "labels": [], "entities": []}, {"text": "To enable efficient inference, we propose a novel algorithm for linear-time decoding in a generative transition-based parser ( \u00a74).", "labels": [], "entities": []}, {"text": "The algorithm is based on particle filtering (), a method for sequential Monte Carlo sampling.", "labels": [], "entities": [{"text": "particle filtering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8667564392089844}]}, {"text": "This method enables the beam-size during decoding to depend on the uncertainty of the model.", "labels": [], "entities": []}, {"text": "Experimental results ( \u00a75) show that the model obtains 88.5% UAS on the standard WSJ parsing task, compared to 88.9% fora greedy discriminative model with similar features.", "labels": [], "entities": [{"text": "UAS", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9994601607322693}, {"text": "WSJ parsing", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.7125927358865738}]}, {"text": "The model can accurately parse up to 200 sentences per second.", "labels": [], "entities": []}, {"text": "Although this performance is below state-of-theart discriminative models, it exceeds existing generative dependency parsing models in either accuracy, speed or both.", "labels": [], "entities": [{"text": "generative dependency parsing", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.9223339358965555}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.999568521976471}, {"text": "speed", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.9829948544502258}]}, {"text": "As a language model, the transition-based parser offers an inexpensive way to incorporate: A partially-derived dependency tree for the sentence Ms.", "labels": [], "entities": []}, {"text": "Waleson is a free-lance writer based in New York.", "labels": [], "entities": []}, {"text": "The next word to be predicted by the generative model is based.", "labels": [], "entities": [{"text": "generative", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.9681506156921387}]}, {"text": "Words in bold are on the stack.", "labels": [], "entities": []}, {"text": "syntactic structure into incremental word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7331809997558594}]}, {"text": "With supervised training the model's perplexity is comparable to that of n-gram models, although generated examples shows greater syntactic coherence.", "labels": [], "entities": []}, {"text": "With semi-supervised learning over a large unannotated corpus its perplexity is considerably better than that of a n-gram model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: HPYP parsing accuracies on the YM de- velopment set, for various lexicalised and unlexi- calised setups.", "labels": [], "entities": [{"text": "HPYP parsing", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.6857848465442657}, {"text": "YM de- velopment set", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.5457496345043182}]}, {"text": " Table 3: Effect of including elements in the model  conditioning contexts. Results are given on the  YM development set.", "labels": [], "entities": [{"text": "YM development set", "start_pos": 102, "end_pos": 120, "type": "DATASET", "confidence": 0.7015672028064728}]}, {"text": " Table 4: Speed and accuracy for different configu- rations of the decoding algorithm. Above the line,  POS tags are predicted by the model, below pre- tagged POS are used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995794892311096}]}, {"text": " Table 5: Parsing accuracies on the YM test  set. compared against previous published results.", "labels": [], "entities": [{"text": "YM test  set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.8364095489184061}]}, {"text": " Table 6: Language modelling test results. Above,  training and testing on WSJ. Below, training semi- supervised and testing on WMT.", "labels": [], "entities": [{"text": "Language modelling test", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7742012739181519}, {"text": "WSJ", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.959138035774231}, {"text": "WMT", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.955007016658783}]}]}