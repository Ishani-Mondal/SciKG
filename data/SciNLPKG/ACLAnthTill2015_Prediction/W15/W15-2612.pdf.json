{"title": [{"text": "Held-out versus Gold Standard: Comparison of Evaluation Strategies for Distantly Supervised Relation Extraction from Medline abstracts", "labels": [], "entities": [{"text": "Distantly Supervised Relation Extraction", "start_pos": 71, "end_pos": 111, "type": "TASK", "confidence": 0.7471541315317154}, {"text": "Medline abstracts", "start_pos": 117, "end_pos": 134, "type": "DATASET", "confidence": 0.6516203284263611}]}], "abstractContent": [{"text": "Distant supervision is a useful technique for creating relation classifiers in the absence of labelled data.", "labels": [], "entities": []}, {"text": "The approaches are often evaluated using a held-out portion of the distantly labelled data, thereby avoiding the need for lablelled data entirely.", "labels": [], "entities": []}, {"text": "However, held-out evaluation means that systems are tested against noisy data, making it difficult to determine their true accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9911371469497681}]}, {"text": "This paper examines the effectiveness of using held-out data to evaluate relation extraction systems by comparing the results that are produced with those generated using manually labelled versions of the same data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8472085297107697}]}, {"text": "We train clas-sifiers to detect two UMLS Metathesaurus relations (may-treat and may-prevent) in Medline abstracts.", "labels": [], "entities": []}, {"text": "A new evaluation data set for these relations is made available.", "labels": [], "entities": []}, {"text": "We show that evaluation against a distantly labelled gold standard tends to overestimate performance and that no direct connection can be found between improved performance against distantly and manually labelled gold standards.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction is a popular topic in the biomedical domain and has been the subject of several challenges (e.g. DDI challenge), BioNLP Shared Task ().", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9577635824680328}]}, {"text": "Many approaches rely on supervised learning techniques using manually labelled training data.", "labels": [], "entities": []}, {"text": "However, the creation of annotated training data is time-consuming, expensive and often requires expert knowledge.", "labels": [], "entities": []}, {"text": "Distant supervision (self-supervised learning) is a widely applied technique for training relation extraction systems () that avoids the need for annotated training data.", "labels": [], "entities": [{"text": "training relation extraction", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.6243575115998586}]}, {"text": "Training examples are annotated automatically using a knowledge base.", "labels": [], "entities": []}, {"text": "Facts from the knowledge base are matched against text and used as training examples.", "labels": [], "entities": []}, {"text": "For example, a knowledge base may assert that the entity pair CONDITION(\"hair loss\")-DRUG(\"paroxetine\") is an instance of the relationship adverse-drug effect.", "labels": [], "entities": [{"text": "CONDITION", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.991734504699707}, {"text": "DRUG", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.5839558243751526}]}, {"text": "Distant supervision approaches normally assume that sentences containing both entities assert the relation between them and, consequently, the following sentence would be used as a positive example of the adverse-drug effect relation: \"Findings on discontinuation and rechallenge supported the assumption that the hair loss was aside effect of the paroxetine.\"", "labels": [], "entities": []}, {"text": "(PMID=10442258) However, this assumption does not always hold which can lead to sentences containing entity pairs being mistakenly identified as asserting a particular relation between them.", "labels": [], "entities": [{"text": "PMID", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.9962373971939087}]}, {"text": "For example, the following sentence contains the same entity pair but does not assert the adverse-drug effect relation: \"There area few case reports on hair loss associated with tricyclic antidepressants and serotonin selective reuptake inhibitors (SSRIs), but none deal specifically with paroxetine.\"", "labels": [], "entities": []}, {"text": "(PMID=10442258) Consequently, data annotated using distant supervision is noisy and unlikely to be of as high quality as manually labelled data.", "labels": [], "entities": [{"text": "PMID", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.9964715242385864}]}, {"text": "Despite this distantly supervised relation extraction provides reasonable results compared to those based on supervised learning (see e.g. in).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7559410929679871}]}, {"text": "Distant supervision allows relation extraction systems to be created without manually labelled data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8719229102134705}]}, {"text": "However, this raises the issue of how such a system can be evaluated.", "labels": [], "entities": []}, {"text": "Previous approaches have carried out evaluation using existing data sets labelled with examples of the target relation () or a similar relation).", "labels": [], "entities": []}, {"text": "However, in the majority of scenarios the best use for any labeled data available is as training data.", "labels": [], "entities": []}, {"text": "Others, such as, generated their own gold standard to annotate relevant relations of their knowledge base.", "labels": [], "entities": []}, {"text": "But the effort required to generate manually labelled evaluation data somewhat negates the benefit of reduced development time provided by distant supervision.", "labels": [], "entities": []}, {"text": "An alternative approach, which does not require any labelled data, is held-out evaluation.", "labels": [], "entities": []}, {"text": "This approach splits facts from the knowledge base into two parts: one to generate distantly supervised training data and the other to generate distantly supervised evaluation data ().", "labels": [], "entities": []}, {"text": "This approach is often combined with a manual evaluation in which a subset of the predictions is selected to be examined in more detail.", "labels": [], "entities": []}, {"text": "For example, supplemented the held-out evaluation of their distant supervision approach for Freebase by selecting the top 1000 facts it predicted and evaluating them manually.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.9685247540473938}]}, {"text": "Others such as and work with the same knowledge base and are able to re-use the manually labelled data generated by.", "labels": [], "entities": []}, {"text": "However, this data is only available for some Freebase relations and evaluation data has to be generated for each new relation.", "labels": [], "entities": []}, {"text": "Approaches such as, and combine a held-out evaluation with a manual evaluation of a randomly chosen subset or the top-k predictions.", "labels": [], "entities": []}, {"text": "This technique is a more reliable evaluation method but requires more effort including (potentially) domain knowledge and needs to be repeated for each version of the classifier.", "labels": [], "entities": []}, {"text": "Held-out evaluation using distantly labelled data is a simple and quick technique for estimating the accuracy of distantly supervised relation extraction systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.998484194278717}, {"text": "relation extraction", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.6830319166183472}]}, {"text": "However, this evaluation data is noisy and it is unclear what effect this has on the accuracy of performance estimates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9989238381385803}]}, {"text": "The issue is explored in this paper by evaluating relation extraction systems for two biomedical relations using both manually and distantly labelled data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.768271267414093}]}, {"text": "We automatically generate labelled held-out data and then carryout a manual annotation to allow direct comparison.", "labels": [], "entities": []}, {"text": "A distantly supervised classifier is trained and evaluated on both data sets.", "labels": [], "entities": []}, {"text": "Similar as in we show that a large portion of the labels generated by distant supervision for the two relations are incorrect.", "labels": [], "entities": []}, {"text": "However we find that evaluating classifiers using heldout distantly supervised data tends to overestimate performance compared to manually labelled data and that improvements in performance observed in evaluation against distantly supervised data are not necessarily reflected in improved results when measured against manually labelled data.", "labels": [], "entities": []}, {"text": "To the best of our knowledge this is the first direct comparison of evaluating distantly supervised classifiers against distantly and manually labelled gold standards.", "labels": [], "entities": []}, {"text": "Analysis in previous work has been restricted to determining the true labels fora set of positively predicted labels.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The next section 2 describes the creation of the distantly supervised data and a manually labelled subset.", "labels": [], "entities": []}, {"text": "A comparison of the automatically and manually generated labels is carried out in Section 3.", "labels": [], "entities": []}, {"text": "Sections 4 evaluates a relation extraction system using different data sets and compares the performance obtained.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8067113757133484}]}, {"text": "The paper concludes with section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of manual and distantly labelled annotations", "labels": [], "entities": []}, {"text": " Table 2: Results for relation extraction system evaluated against DL and ML data sets", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8650501668453217}, {"text": "ML data sets", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.791474183400472}]}]}