{"title": [{"text": "* * * * * * * * * * ** * * ** * * * ** ** * ** * * * ** * * * * * * * * * * ** * *", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces Eve, a high-performance agent that plays a fast-paced image matching game in a spoken dialogue with a human partner.", "labels": [], "entities": [{"text": "image matching", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7519902586936951}]}, {"text": "The agent can be optimized and operated in three different modes of incremental speech processing that optionally include incremental speech recognition, language understanding , and dialogue policies.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.751318484544754}, {"text": "language understanding", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.7053314745426178}]}, {"text": "We present our framework for training and evaluating the agent's dialogue policies.", "labels": [], "entities": []}, {"text": "Ina user study involving 125 human participants, we evaluate three incremental architec-tures against each other and also compare their performance to human-human game-play.", "labels": [], "entities": []}, {"text": "Our study reveals that the most fully incremental agent achieves game scores that are comparable to those achieved in human-human gameplay, are higher than those achieved by partially and non-incremental versions, and are accompanied by improved user perceptions of efficiency , understanding of speech, and naturalness of interaction.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents and evaluates a game playing dialogue agent named Eve that relies on several forms of incremental language processing to achieve its best performance.", "labels": [], "entities": []}, {"text": "In recent years, the development and adoption of incremental processing techniques in dialogue systems has continued to advance, and more-and-more research systems have included some form of incremental processing; see for example ().", "labels": [], "entities": []}, {"text": "One compelling high-level motivation for systems builders to incorporate incremental processing into their systems is to reduce system response latency.", "labels": [], "entities": []}, {"text": "Recent studies have also demonstrated user preference of incremental systems over non-incremental counterparts, shown positive effects of incrementality on user ratings of system efficiency and politeness, and even shown increases in the fluency of user speech when appropriate incremental feedback is provided ().", "labels": [], "entities": []}, {"text": "Despite this progress, there remain many open questions about the use of incremental processing in systems.", "labels": [], "entities": []}, {"text": "One important research direction is to explore and clarify the implications and advantages of alternative incremental architectures.", "labels": [], "entities": []}, {"text": "Using pervasive incremental processing in a dialogue system poses a fundamental challenge to traditional system architectures, which generally assume turn-level or dialogue act level units of processing rather than much smaller and higher frequency incremental units (.", "labels": [], "entities": []}, {"text": "Rather than completely redesigning their architectures, system builders maybe able to gain some of the advantages of incrementality, such as reduced response latencies, by incorporating incremental processing in select system modules such as automatic speech recognition or language understanding.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 252, "end_pos": 270, "type": "TASK", "confidence": 0.7165296375751495}, {"text": "language understanding", "start_pos": 274, "end_pos": 296, "type": "TASK", "confidence": 0.6781259626150131}]}, {"text": "The extent to which all modules of a dialogue system need to operate incrementally to achieve specific effects needs further exploration.", "labels": [], "entities": []}, {"text": "Another important research direction is to develop effective optimization techniques for dialogue policies in incremental systems.", "labels": [], "entities": []}, {"text": "Incremental dialogue policies may need to make many finegrained decisions per second, such as whether to initiate a backchannel or interruption of a user utterance in progress.", "labels": [], "entities": []}, {"text": "Developing data-driven approaches to such decision-making may allow us to build more highly optimized, interactive, and ef-fective systems than are currently possible ().", "labels": [], "entities": []}, {"text": "Yet the computational techniques that can achieve this fine-grained optimization in practice are not yet clear.", "labels": [], "entities": []}, {"text": "Approaches that use (Partially Observable) Markov Decision Processes and a reinforcement learning framework to optimize fine-grained turn-taking control may ultimately prove effective (see e.g. (), but optimizing live system interactions in this way remains a challenge.", "labels": [], "entities": []}, {"text": "In this paper, we present a case study of a high-performance incremental dialogue system that contributes to both of these research directions.", "labels": [], "entities": []}, {"text": "First, our study investigates the effects of increasing levels of incremental processing on the performance and user perceptions of an agent that plays a fast-paced game where the value of rapid decision-making is emphasized.", "labels": [], "entities": []}, {"text": "Ina user study involving 125 human participants, we demonstrate a level of game performance that is broadly comparable to the performance of live human players.", "labels": [], "entities": []}, {"text": "Only the version of our agent which makes maximal use of incremental processing achieves this level of performance, along with significantly higher user ratings of efficiency, understanding of speech, and naturalness of interaction.", "labels": [], "entities": []}, {"text": "Our study also provides a practical approach to the optimization of dialogue policies for incremental understanding of users' referential language infinite domains; see e.g. ( . Our optimization approach delivers a high level of performance for our agent, and offers insights into how the optimal decision-making policy can vary as the level of incrementality in system modules is changed.", "labels": [], "entities": []}, {"text": "This supports a view of incremental policy optimization as a holistic process to be undertaken in conjunction with overall system design choices.", "labels": [], "entities": [{"text": "incremental policy optimization", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.7735115687052408}]}], "datasetContent": [{"text": "Our eavesdropper framework allows policies to not only be trained, but also evaluated in offline simulation, both in terms of total points scored and total points/s (which is the direct optimization metric).", "labels": [], "entities": []}, {"text": "An excerpt from our offline evaluation results, using hold-one-user-out cross-validation, is shown in.", "labels": [], "entities": []}, {"text": "In these offline results, the agent is sometimes able to achieve higher points/s than our human matchers did in human-human gameplay.", "labels": [], "entities": []}, {"text": "This is true for some image sets in all three incrementality types.", "labels": [], "entities": []}, {"text": "In general, we also observe that simulated points/s decreases as the level of incrementality in the system decreases.", "labels": [], "entities": []}, {"text": "Note that the total number of simulated points achieved by these policies is generally less than what human players scored; the agents optimized for points/s are less likely to score a point for each image, but makeup for this in speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 230, "end_pos": 235, "type": "METRIC", "confidence": 0.961329460144043}]}, {"text": "These offline results led us to hypothesize that, in live interaction with users, the FullInc agent would score higher than the less incremental versions in a time-constrained game.", "labels": [], "entities": []}, {"text": "In this section, we summarize our user study results, many of which are visualized in.", "labels": [], "entities": []}, {"text": "We evaluate our FullInc, PartInc, and NonInc agents by game score and by user's perceptions as captured in post-game questionnaires.", "labels": [], "entities": [{"text": "FullInc", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9227800965309143}, {"text": "PartInc", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.713615357875824}]}, {"text": "Users responded to a range of statements with answers on a five point Likert-scale ranging from Totally disagree (0) to Totally agree (4).", "labels": [], "entities": [{"text": "Totally disagree (0)", "start_pos": 96, "end_pos": 116, "type": "METRIC", "confidence": 0.8919028639793396}, {"text": "Totally agree (4)", "start_pos": 120, "end_pos": 137, "type": "METRIC", "confidence": 0.8927518606185914}]}, {"text": "We compare the responses of the director in human-human (HH) pairs to the responses of human directors playing with our agent as matcher.", "labels": [], "entities": []}, {"text": "All significance tests in this section are Wilcoxon rank sum tests.", "labels": [], "entities": []}, {"text": "We report scores in U.S. dollars paid to participants for correct TIs ($0.02/correct TI).", "labels": [], "entities": [{"text": "correct TIs", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.7745963037014008}]}, {"text": "The FullInc system achieved a mean score of $0.33 that is significantly better than the mean $0.25 for PartInc (p = 0.013) and the mean $0.23 for NonInc (p = 0.002).", "labels": [], "entities": [{"text": "mean score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9613127708435059}, {"text": "PartInc", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.945977509021759}]}, {"text": "No significant difference in score was observed between the PartInc and NonInc versions.", "labels": [], "entities": [{"text": "PartInc", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9504150152206421}]}, {"text": "These results suggest that, beyond incorporating online decoding in the ASR to reduce ASR latency, also incorporating an incremental NLU+policy is important to score maximization.", "labels": [], "entities": [{"text": "ASR", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.8364602327346802}, {"text": "ASR latency", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.6262007802724838}]}, {"text": "Our FullInc agent's performance in terms of score is quite strong, and comparable to HH scores.", "labels": [], "entities": [{"text": "FullInc agent", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8866842091083527}, {"text": "score", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9441066980361938}, {"text": "HH", "start_pos": 85, "end_pos": 87, "type": "DATASET", "confidence": 0.5139346718788147}]}, {"text": "Although the mean HH score of $0.36 was a little higher than that of our FullInc agent ($0.33), the difference is not significant.", "labels": [], "entities": [{"text": "HH score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9607135057449341}, {"text": "FullInc agent", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.8979595899581909}]}, {"text": "The best FullInc score of $0.50 achieved as part of the study is higher than 76% of HH teams, and its worst score of $0.14 is higher than 20% of HH teams.", "labels": [], "entities": [{"text": "FullInc score", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.7774869799613953}]}, {"text": "HH teams scored significantly higher than the PartInc (p = 0.038) and NonInc (p = 0.008) versions of the system, which underscores the importance of pervasive incremental processing to achieving human-like performance in some dialogue systems.", "labels": [], "entities": [{"text": "PartInc", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.8916894793510437}]}, {"text": "Human participants were significantly more satisfied with their score when working with a human matcher than with any version of our agent (for the FullInc version, p = 0.037).", "labels": [], "entities": [{"text": "FullInc", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.9098399877548218}]}, {"text": "Participants who played with the FullInc agent were significantly more satisfied with their score than those in the PartInc (p = 0.002) and NonInc (p = 0.017) conditions.", "labels": [], "entities": [{"text": "FullInc agent", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9107975363731384}, {"text": "PartInc", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.8389154076576233}]}, {"text": "These results generally mirror our findings for game score, and score and score satisfaction are clearly connected.", "labels": [], "entities": []}, {"text": "Human partners were perceived as significantly easier to play with than all agent versions.", "labels": [], "entities": []}, {"text": "We observed a trend (not quite significant) for people to consider it easier to play with the FullInc version than with NonInc version (p = 0.052).", "labels": [], "entities": [{"text": "FullInc version", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.9359032809734344}]}, {"text": "Human partners were rated as significantly more efficient than the FullInc (p = 0.038), PartInc (p < 0.0005) and NonInc (p < 0.0005) agents.", "labels": [], "entities": [{"text": "FullInc", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.837935745716095}, {"text": "PartInc", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.8381231427192688}]}, {"text": "Among the agent versions, the FullInc agent was rated significantly more efficient than PartInc (p = 0.001) and NonInc (p = 0.002).", "labels": [], "entities": [{"text": "PartInc", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.8755956888198853}]}, {"text": "This result echoes previous findings of increases in perceived efficiency for incremental systems, though herewith a differing system architecture and task.", "labels": [], "entities": []}, {"text": "Human partners elicited the most confidence that the two players were understanding each other.", "labels": [], "entities": []}, {"text": "This perceived understanding of each other's speech was significantly higher in FullInc than in PartInc (p = 0.010) and NonInc (p = 0.006).", "labels": [], "entities": [{"text": "understanding", "start_pos": 15, "end_pos": 28, "type": "METRIC", "confidence": 0.9482543468475342}, {"text": "FullInc", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9326255917549133}, {"text": "PartInc", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9023247957229614}]}, {"text": "It is interesting to consider that the NLU in these three versions is identical, and thus the level of actual understanding of user speech should be similar across conditions.", "labels": [], "entities": []}, {"text": "We speculate that the greater responsiveness of the FullInc system increased confidence that users were being understood.", "labels": [], "entities": [{"text": "FullInc", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.860657811164856}]}, {"text": "Perceived naturalness of user speech.", "labels": [], "entities": []}, {"text": "One of our survey items investigated whether people felt they could speak naturally to their partner, \"in the way I normally talk to another person\".", "labels": [], "entities": []}, {"text": "Human partners scored significantly higher than all agent versions here.", "labels": [], "entities": []}, {"text": "The FullInc agent scored significantly higher than the NonInc agent (p = 0.037).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Identification threshold and give-up  threshold in optimized policies for 4 image sets.", "labels": [], "entities": [{"text": "Identification threshold", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.9491402506828308}]}, {"text": " Table 2: Offline policy evaluation results for all three incrementality types and four image sets. 14  additional image sets are omitted for space reasons.", "labels": [], "entities": []}]}