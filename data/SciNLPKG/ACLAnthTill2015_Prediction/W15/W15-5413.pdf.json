{"title": [], "abstractContent": [{"text": "We describe the system built by the National Research Council (NRC) Canada for the 2015 shared task on Discriminating between similar languages.", "labels": [], "entities": [{"text": "National Research Council (NRC) Canada", "start_pos": 36, "end_pos": 74, "type": "DATASET", "confidence": 0.8930909548486982}, {"text": "2015 shared task on Discriminating between similar languages", "start_pos": 83, "end_pos": 143, "type": "TASK", "confidence": 0.5692954622209072}]}, {"text": "The NRC system uses various statistical classifiers trained on character and word ngram features.", "labels": [], "entities": [{"text": "NRC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8573627471923828}]}, {"text": "Predictions rely on a two-stage process: we first predict the language group, then discriminate between languages or variants within the group.", "labels": [], "entities": []}, {"text": "This year, we focused on two issues: 1) the ngram generation process, and 2) the handling of the anonymized (\"blinded\") Named Entities.", "labels": [], "entities": [{"text": "ngram generation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6983653455972672}]}, {"text": "Despite the slightly harder experimental conditions this year, our systems achieved an average accuracy of 95.24% (closed task) and 95.65% (open task), ending up second or (close) third on the closed task, and first on the open task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9995638728141785}]}], "introductionContent": [{"text": "Although language identification is largely considered a solved problem in the general setting, a number of frontier cases are still understudy.", "labels": [], "entities": [{"text": "language identification", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.8378606140613556}]}, {"text": "For example, when little data is available (eg single twitter post), when the input is mixed or when discriminating similar languages or language variants.", "labels": [], "entities": []}, {"text": "The Discriminating between similar languages (DSL) shared task offers precisely such a situation, by offering an interesting mix of close languages and variants, and relatively short, one-sentence texts.", "labels": [], "entities": [{"text": "Discriminating between similar languages (DSL) shared task", "start_pos": 4, "end_pos": 62, "type": "TASK", "confidence": 0.7547580930921767}]}, {"text": "This year, four groups contain similar languages: \u2022 Bosnian, Croatian and Serbian; \u2022 Indonesian and Malaysian; \u2022 Czech and Slovakian; \u2022 Bulgarian and Macedonian.", "labels": [], "entities": []}, {"text": "Two groups contain variants of the same language: \u2022 Portuguese: European vs. Brazilian; \u2022 Spanish: European vs. Argentinian.", "labels": [], "entities": []}, {"text": "In addition, instances to classify are single sentences, a more realistic and challenging situation than full-document language identification.", "labels": [], "entities": [{"text": "full-document language identification", "start_pos": 105, "end_pos": 142, "type": "TASK", "confidence": 0.6210199296474457}]}, {"text": "There are two interesting additions to the 2015 challenge.", "labels": [], "entities": []}, {"text": "A second test set, with Named Entity anonymized, was added to evaluate the influence of local information on the predictions.", "labels": [], "entities": []}, {"text": "In addition, sentences from \"other\", unknown languages were added to the test sets.", "labels": [], "entities": []}, {"text": "This means that group/language prediction is not limited to the 13 languages in the training set.", "labels": [], "entities": [{"text": "group/language prediction", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.5819186270236969}]}, {"text": "Following some good results at last year's evaluation (), we took part to this years evaluation in order to see how our system would handle the additional language pair, and the two challenges of anonymized named entities and more varied test data.", "labels": [], "entities": []}, {"text": "In addition, we wanted to further explore the way character ngrams should be more efficiently extracted from the raw text.", "labels": [], "entities": []}, {"text": "The overall longer term motivation is to use language and variant detection to help natural language processing, for example in machine translation ().", "labels": [], "entities": [{"text": "variant detection", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7473049461841583}, {"text": "natural language processing", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.7012348175048828}, {"text": "machine translation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7959102392196655}]}, {"text": "Discriminating similar languages may also be a first step to identify code switching in short messages (.", "labels": [], "entities": [{"text": "identify code switching in short messages", "start_pos": 61, "end_pos": 102, "type": "TASK", "confidence": 0.7427800943454107}]}, {"text": "The following section describes the models we used, and the features we extracted from the data.", "labels": [], "entities": []}, {"text": "We then briefly describe the data we trained on (Section 3), and summarize our experimental results in Section 4.", "labels": [], "entities": []}, {"text": "we simply all call \"language\" from now on) within the group.", "labels": [], "entities": []}, {"text": "This approach works best if the first stage (i.e. group) classifier has high accuracy, because if the wrong group is predicted, it is impossible to recover from that mistake in the second stage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9986044764518738}]}, {"text": "On the other hand, as most groups only comprise two languages, our two-stage process makes it possible to rely on a simple binary classifier within each group, and avoid the extra complexity that comes with multiclass modeling.", "labels": [], "entities": []}, {"text": "We first describe the features we extract from the data (Section 2.1).", "labels": [], "entities": []}, {"text": "We then provide a quick overview of how the probabilistic group classifier works (Section 2.2).", "labels": [], "entities": []}, {"text": "Finally, we describe the within-group language predictors in Section 2.3.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Number of errors (out of 14000 test sen- tences) made by the group-level classifier for the  eight runs (two runs per task and per test set).", "labels": [], "entities": [{"text": "Number of errors", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8869071205457052}]}, {"text": " Table 1: Number of sentences in the Discriminating Similar Languages Corpus Collections (DSLCC)  provided for the 2014 and 2015 shared tasks, plus test sets with (A) or without (B) named entities, across  groups and languages. The system for the closed task was trained on 2015 data only, the system for the  open task was trained on 2014 and 2015 data.", "labels": [], "entities": [{"text": "Discriminating Similar Languages Corpus Collections (DSLCC)", "start_pos": 37, "end_pos": 96, "type": "DATASET", "confidence": 0.7782207354903221}]}, {"text": " Table 3: Language prediction test accuracy for the closed task, both test sets and all runs. Our best  results are in bold (tied for second overall). Overall #err is larger than column sum due to \"Other\".", "labels": [], "entities": [{"text": "Language prediction test", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7673103411992391}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9270898103713989}, {"text": "err", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.991218626499176}]}, {"text": " Table 4: Language prediction test accuracy for the open task, both test sets and all runs.", "labels": [], "entities": [{"text": "Language prediction test", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7177532613277435}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9081321358680725}]}]}