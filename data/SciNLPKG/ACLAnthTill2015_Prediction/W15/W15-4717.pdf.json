{"title": [{"text": "Generating Descriptions of Spatial Relations between Objects in Images", "labels": [], "entities": [{"text": "Generating Descriptions of Spatial Relations between Objects in Images", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.8725921842787001}]}], "abstractContent": [{"text": "We investigate the task of predicting prepositions that can be used to describe the spatial relationships between pairs of objects depicted in images.", "labels": [], "entities": [{"text": "predicting prepositions", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8822311460971832}]}, {"text": "We explore the extent to which such spatial prepositions can be predicted from (a) language information, (b) visual information, and (c) combinations of the two.", "labels": [], "entities": []}, {"text": "In this paper we describe the dataset of object pairs and prepositions we have created, and report first results for predicting prepositions for object pairs, using a Naive Bayes framework.", "labels": [], "entities": []}, {"text": "The features we use include object class labels and geometrical features computed from object bounding boxes.", "labels": [], "entities": []}, {"text": "We evaluate the results in terms of accuracy against human-selected prepositions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9992140531539917}]}], "introductionContent": [{"text": "The task we investigate is predicting the prepositions that can be used to describe the spatial relationships between pairs of objects in images.", "labels": [], "entities": []}, {"text": "This is not the same as inferring the actual 3-D realworld spatial relationships between objects, but has some similarities with that task.", "labels": [], "entities": []}, {"text": "This is an important subtask in automatic image description (which is important not just for assistive technology, but also for applications such as text-based querying of image databases), but it is rarely addressed as a subtask in its own right.", "labels": [], "entities": [{"text": "automatic image description", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6207499305407206}]}, {"text": "If an image description method produces spatial prepositions it tends to be as a side-effect of the overall method, or else relationships are not between objects, but e.g. between objects and the 'scene').", "labels": [], "entities": []}, {"text": "An example of preposition selection as a separate sub-task is where the mapping is hard-wired manually.", "labels": [], "entities": [{"text": "preposition selection", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.8838310837745667}]}, {"text": "Our main data source is a corpus of images) in which objects have been annotated with rectangular bounding boxes and object class labels.", "labels": [], "entities": []}, {"text": "For a subset of 1,000 of the images we also have five human-created descriptions of the whole image (.", "labels": [], "entities": []}, {"text": "We collected additional annotations for the images (Section 2.3) which list, for each object pair, a set of prepositions that have been selected by human annotators as correctly describing the spatial relationship between the given object pair.", "labels": [], "entities": []}, {"text": "The aim is to create models for the mapping from image, bounding boxes and labels to spatial prepositions as indicated in.", "labels": [], "entities": []}, {"text": "In this we use a range of features to represent object pairs, computed from image, bounding boxes and labels.", "labels": [], "entities": []}, {"text": "We investigate the predictive power of different types of features within a Naive Bayes framework (Section 3), and report first results in terms of two measures of accuracy (Section 4).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9986104965209961}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Acc B (1..n) for v N B model and n \u2264 4.", "labels": [], "entities": [{"text": "Acc B", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9693247377872467}]}]}