{"title": [{"text": "Parsing Software Requirements with an Ontology-based Semantic Role Labeler", "labels": [], "entities": [{"text": "Parsing Software Requirements", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8240767121315002}, {"text": "Semantic Role Labeler", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.6390461126963297}]}], "abstractContent": [{"text": "Software requirements describe functional and non-functional aspects of a software system and form the basis for the development process.", "labels": [], "entities": []}, {"text": "Accordingly, requirements of existing systems can provide insights regarding the re-usability of already implemented software artifacts.", "labels": [], "entities": []}, {"text": "To facilitate direct comparison between requirements of existing and to be developed systems, we propose to automatically map requirements in natural language text to structured semantic representations.", "labels": [], "entities": []}, {"text": "For this task, we adapt techniques from semantic role labeling to a high-level ontology that defines concepts and relations for describing static software functionalities.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6373111506303152}]}, {"text": "The proposed method achieves a precision and recall of 77.9% and 74.5%, respectively, on an annotated software requirements dataset and significantly outperforms two baselines that are based on lexical and syntactic patterns.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9997450709342957}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9994970560073853}]}], "introductionContent": [{"text": "During the process of software development, developers and customers typically discuss and agree on requirements that specify the functionality of a system that is being developed.", "labels": [], "entities": []}, {"text": "Such requirements play a crucial role in the development lifecycle, as they form the basis for implementations, corresponding work plans, cost estimations and follow-up directives.", "labels": [], "entities": []}, {"text": "In general, software requirements can be expressed in various different ways, including the use of UML diagrams and storyboards.", "labels": [], "entities": []}, {"text": "Most commonly, however, expectations are expressed in natural language (), as shown in Example (1): (1) A user should be able to login to his account.", "labels": [], "entities": []}, {"text": "While requirements expressed in natural language have the advantage of being intelligible to both clients and developers, they can of course also be ambiguous, vague and incomplete.", "labels": [], "entities": []}, {"text": "Although formal languages could be used as an alternative that eliminates some of these problems, customers are rarely equipped with the mathematical and technical expertise for understanding highly formalised requirements.", "labels": [], "entities": []}, {"text": "To benefit from the advantages of both natural language and formal representations, we propose to induce the latter automatically from text in a semantic parsing task.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 145, "end_pos": 166, "type": "TASK", "confidence": 0.7980554103851318}]}, {"text": "Given the software requirement in Example (1), for instance, we would like to construct a representation that explicitly specifies the types of the entities involved (e.g., OB J EC T(account)) and relationships among them (e.g., AC T SO N(login, account)).", "labels": [], "entities": [{"text": "OB J EC T", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.5831270292401314}]}, {"text": "Though there exist ontologies and small-scale data sets with annotated concept instances, most previous approaches to inducing such annotations from text relied on hand-crafted grammars and heuristic postprocessing rules.", "labels": [], "entities": []}, {"text": "In this paper, we propose to identify and classify instances of ontology concepts and relations using statistical techniques from semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.633168359597524}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe previous approaches on converting software requirements to formal representations.", "labels": [], "entities": []}, {"text": "In Section 3, we provide a summary of a previously developed ontology and annotated data set which we use for training and testing our own approach.", "labels": [], "entities": []}, {"text": "In Section 4, we decribe the semantic role labeling architecture that we developed to automatically process software requirements.", "labels": [], "entities": []}, {"text": "In Section 5, we evaluate our proposed approach and compare it to two pattern-based baselines.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper in Section 6 with a discussion and outlook on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "As training and testing material for our semantic role labeling approach, we use a high-level ontology of static software functionalities and an existing data set of software requirements with annotated ontology instances ( ).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6308329602082571}]}, {"text": "The ontology by Roth et al. covers general concepts for describing static software functionalities.", "labels": [], "entities": []}, {"text": "The main concepts and their associated relations are as follows: Action An Action describes an operation that is performed by an Actor on any number of Object(s).", "labels": [], "entities": []}, {"text": "The participants of an Action are indicated by the relations HAS ACTOR and ACTS ON, respectively.", "labels": [], "entities": [{"text": "HAS", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9917759895324707}, {"text": "ACTOR", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.6109971404075623}, {"text": "ACTS ON", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9247928559780121}]}, {"text": "Actor (HAS ACTOR) A Actor is an active participant of an Action and can be the user of a system or a software system itself.", "labels": [], "entities": [{"text": "HAS ACTOR)", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.8514553904533386}]}, {"text": "Object (ACTS ON) A Object is any kind of entity involved in an Action other than the Actor.", "labels": [], "entities": []}, {"text": "Property (HAS PROPERTY) A Property is an attribute of an Object, a characteristic of an Action or an optional specification of an Actor.", "labels": [], "entities": []}, {"text": "The domain of the relation HAS PROPERTY will be the set of entities which possess a given Property.", "labels": [], "entities": [{"text": "HAS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9894821643829346}, {"text": "PROPERTY", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.5293676853179932}]}, {"text": "We evaluate the performance of the semantic role labeling approach described in Section 4, using the annotated dataset described in Section 3.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.647334078947703}]}, {"text": "As evaluation metrics, we apply labeled precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9143951535224915}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9989492297172546}]}, {"text": "We define labeled precision as the fraction of predicted labels of concept and relation instances that are correct, and labeled recall as the fraction of annotated labels that are correctly predicted by the parser.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.778300940990448}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9771919846534729}]}, {"text": "To train and test the statistical models underlying the semantic analysis components of our pipeline, we perform evaluation in a 5-fold cross-validation setting.", "labels": [], "entities": []}, {"text": "That is, given the 325 sentences from the annotated data set, we randomly create five folds of equal size (65 sentences) and use each fold once for testing while training on the remaining other folds.", "labels": [], "entities": []}, {"text": "As baselines, we apply two pattern-based models that are similar in spirit to earlier approaches to parsing software requirements (cf. Section 2).", "labels": [], "entities": [{"text": "parsing software requirements", "start_pos": 100, "end_pos": 129, "type": "TASK", "confidence": 0.9026922583580017}]}, {"text": "The first baseline simply uses word level patterns to identify instances of ontology concepts and relations.", "labels": [], "entities": []}, {"text": "The second baseline is similar to the first but also takes into account syntactic relationships between potential instances of ontology concepts.", "labels": [], "entities": []}, {"text": "For simplicity, we train both baseline models using the same architecture as our proposed method but only use a sub-set of the applied features.", "labels": [], "entities": []}, {"text": "In the first baseline, we only apply features indicating word forms, lemmata and parts-of-speech as well as the order between words.", "labels": [], "entities": []}, {"text": "For the second baseline, we use all features from the first baseline plus indicator features on syntactic relationships between words that potentially instantiate ontology concepts.", "labels": [], "entities": []}, {"text": "The results of both baselines and our full semantic role labeling model are summarized in.", "labels": [], "entities": []}, {"text": "Using all features described in Section 4.3, our model achieves a precision and recall of 77.9% and 74.5%, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9998459815979004}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.999757707118988}]}, {"text": "The corresponding F 1 -score, calculated as the harmonic mean between precision and recall, is 76.2%.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9924605935811996}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9972922205924988}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9944248199462891}]}, {"text": "The baselines only achieve F 1 -scores of 45.1% and 69.3%, respectively.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9935773015022278}]}, {"text": "A significance test based on random approximate shuffling confirmed that the differences in results between our model and each baseline is statistically significant (p<0.01).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Counts of annotated instances of concepts and relations in the dataset from Roth et al. (2014)", "labels": [], "entities": []}, {"text": " Table 3: Performance of our full model and two simplified baselines; all numbers in %", "labels": [], "entities": []}]}