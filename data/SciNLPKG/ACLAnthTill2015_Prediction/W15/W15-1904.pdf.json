{"title": [{"text": "Semi-automated typical error annotation for learner English essays: integrating frameworks", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes integration of three open source utilities: brat web annotation tool, Freeling suite of linguistic analyzers and Aspell spellchecker.", "labels": [], "entities": []}, {"text": "We demonstrate how their combination can be used to pre-annotate texts in a learner corpus of En-glish essays with potential errors and ease human annotators' work.", "labels": [], "entities": []}, {"text": "Spellchecker alerts and morphological an-alyzer tagging probabilities are used to detect students' possible errors of most typical sorts.", "labels": [], "entities": []}, {"text": "F-measure for the developed pre-annotation framework with regard to human annotation is 0.57, which already makes the system a substantial help to human annotators, but at the same time leaves room for further improvement.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9958393573760986}]}], "introductionContent": [{"text": "Nowadays, learner corpora accumulating typical learner texts together with typical errors often support language learning.", "labels": [], "entities": []}, {"text": "They allow researching into inter-relation of L1 and L2, and the process of language acquisition in general.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7196855396032333}]}, {"text": "Error annotation of such corpora is particularly valuable as it can provide various insights into the features of learners' interlanguage and contribute to error analysis.", "labels": [], "entities": []}, {"text": "For example, errors made by a learner convey a lot of information about how (s)he acquires a foreign language, and which categories are most problematic.", "labels": [], "entities": []}, {"text": "Another promising feature of error annotation is the possibility to detect L1-specific errors).", "labels": [], "entities": []}, {"text": "Also, error-tagged corpora help human annotators and teachers who are grading students' works.", "labels": [], "entities": []}, {"text": "All this consequently leads to more efficient language learning process.", "labels": [], "entities": []}, {"text": "Annotating learner texts with common linguistic annotation layers (tokens, morphology, syntax, etc) is challenging because of the nonconventional nature of such texts.", "labels": [], "entities": []}, {"text": "It is not easy to find out what was the author's intended utterance (target hypothesis) and how it should be marked up in the corpus.", "labels": [], "entities": []}, {"text": "Sometimes several 'readings' are possible, further complicating the situation.", "labels": [], "entities": []}, {"text": "As for the error annotation in learner corpora, being a very complicated and a time-consuming process, it is often put aside.", "labels": [], "entities": []}, {"text": "Meanwhile, these two problems can be merged into one solution.", "labels": [], "entities": []}, {"text": "Non-canonical features of learner texts can be of use when finding and correcting errors and revealing text structure.", "labels": [], "entities": []}, {"text": "'Strange', unconventional spelling or morphological forms provide clues about mismatches between the target hypothesis and surface form of the text).", "labels": [], "entities": []}, {"text": "Therefore, it is possible to perform some types of error annotation automatically, disregarding its complexity.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate our approach towards semi-automated pre-annotation of typical errors in learner English texts.", "labels": [], "entities": []}, {"text": "We propose a solution to facilitate learner corpora error annotation based on integrating three well-known opensource frameworks, particularly, Aspell, Freeling and brat.", "labels": [], "entities": [{"text": "learner corpora error annotation", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.5181245133280754}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we give an overview of other approaches to automatic error annotation, and how our approach differs from them.", "labels": [], "entities": [{"text": "automatic error annotation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.609457790851593}]}, {"text": "In Section 3 we describe the tools employed in the framework, testing corpus and general workflow.", "labels": [], "entities": []}, {"text": "Section 4 gives details on the system performance in comparison to humanannotated texts.", "labels": [], "entities": []}, {"text": "Section 5 points at a working prototype available online and briefly describes implementing the same tool-chain in one's own environment.", "labels": [], "entities": []}, {"text": "Finally, in Section 6 we conclude and describe directions of further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our pre-annotation was tested against errors spotted by human annotators in 800 documents from REALEC corpus (213 694 word tokens in total).", "labels": [], "entities": [{"text": "REALEC corpus", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.9366057813167572}]}, {"text": "After applying the framework, we encountered 10490 morphological errors 'issued' by Freeling and 3018 spelling errors by Aspell.", "labels": [], "entities": [{"text": "Freeling", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9542177319526672}, {"text": "Aspell", "start_pos": 121, "end_pos": 127, "type": "DATASET", "confidence": 0.906581461429596}]}, {"text": "This is consistent with the ratio of spelling mistakes inhuman annotations of the same texts ().", "labels": [], "entities": []}, {"text": "Initially, we checked strict coincidences of automatically detected 'pre-errors' with humanannotated error spans, so that only the tokens from our pre-annotation that exactly match those assigned by humans were counted.", "labels": [], "entities": []}, {"text": "Quite expected, performance was not very impressive, with Fmeasure only 0.05 (see).", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9899271726608276}]}, {"text": "The reason for such low values is that human experts often mark spans ranging across several words or even parts of words.", "labels": [], "entities": []}, {"text": "In fact, tagging several words is necessary for particular types of errors, for example, word order errors.", "labels": [], "entities": []}, {"text": "At the same time, our system annotates only separate words, and thus lags behind humans.", "labels": [], "entities": []}, {"text": "The figures for Aspell and Freeling parts of the framework separately were discouraging as well.", "labels": [], "entities": [{"text": "Aspell", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8908830285072327}, {"text": "Freeling", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.6733078360557556}]}], "tableCaptions": [{"text": " Table 1: Performance in comparison with human  judgments", "labels": [], "entities": []}]}