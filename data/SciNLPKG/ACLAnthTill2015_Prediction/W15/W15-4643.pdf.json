{"title": [{"text": "Optimising Turn-Taking Strategies With Reinforcement Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, reinforcement learning (RL) is used to learn an efficient turn-taking management model in a simulated slot-filling task with the objective of minimis-ing the dialogue duration and maximising the completion task ratio.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.7369593799114227}]}, {"text": "Turn-taking decisions are handled in a separate new module , the Scheduler.", "labels": [], "entities": []}, {"text": "Unlike most dialogue systems, a dialogue turn is split into micro-turns and the Scheduler makes a decision for each one of them.", "labels": [], "entities": []}, {"text": "A Fitted Value Iteration algorithm, Fitted-Q, with a linear state representation is used for learning the state to action policy.", "labels": [], "entities": []}, {"text": "Comparison between a non-incremental and an incremental hand-crafted strategies, taken as baselines, and an incremental RL-based strategy, shows the latter to be significantly more efficient, especially in noisy environments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most dialogue systems use a simple turn-taking model: the user speaks and when she finishes her utterance, the system detects along enough silence and speaks afterwards.", "labels": [], "entities": []}, {"text": "Quite often the latter cannot be interrupted neither.", "labels": [], "entities": []}, {"text": "On the contrary, incremental dialogue systems are able to understand the user's utterance on the fly thus enabling a richer set of turn-taking behaviours.", "labels": [], "entities": []}, {"text": "They can interrupt the user and quickly report a problem.", "labels": [], "entities": []}, {"text": "They can be interrupted as well.", "labels": [], "entities": []}, {"text": "In this paper, we explore the extent to which such capacity can improve the overall dialogue efficiency.", "labels": [], "entities": []}, {"text": "Reinforcement learning () is used to find optimal strategies.", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8276226222515106}]}, {"text": "Human beings use a rich set of incremental behaviours which help them recover from errors efficiently.", "labels": [], "entities": []}, {"text": "As soon as a conversation participant detects a problem, she is able to interrupt the speaker so that he can correct his utterance or repeat apart of it for example.", "labels": [], "entities": []}, {"text": "In this work, we implement in an expert handcrafted way 3 turntaking phenomena amongst those classified in the taxonomy proposed in ().", "labels": [], "entities": []}, {"text": "The resulting strategy is shown to achieve better performance than a non-incremental handcrafted strategy.", "labels": [], "entities": []}, {"text": "Then, it is compared to an automatically learned incremental strategy and the latter is shown to achieve even better results.", "labels": [], "entities": []}, {"text": "Machine learning algorithms often need important sets of data in order to converge.", "labels": [], "entities": []}, {"text": "In the field of dialogue systems, gathering data is expensive and as a consequence, researchers use simulated users for learning (.", "labels": [], "entities": []}, {"text": "To run the experiments in this work, a simulated user interacts with a service that manages a personal agenda ().", "labels": [], "entities": []}, {"text": "In our work, the turn-taking task is separated from the common dialogue management one and it is handled by a separated module called the.", "labels": [], "entities": []}, {"text": "A considerable asset of this architecture is that it can just be added to the agenda service in order to make it incremental.", "labels": [], "entities": []}, {"text": "Two versions of this module have been developed: the first one embeds the handcrafted strategy and the second one uses reinforcement learning to optimise turn-taking decisions with respect to objective criteria.", "labels": [], "entities": []}, {"text": "Our goal is to improve the dialogue efficiency, therefore, as evaluation criteria and in order to design a reward function, dialogue duration and task completion are used.", "labels": [], "entities": []}, {"text": "Fitted-Q (a Fitted Value Iteration algorithm) was used and we show that the optimal policy is quickly learned and that it outperforms both the non-incremental and the handcrafted strategies.", "labels": [], "entities": []}, {"text": "These three strategies are then compared under different noise conditions and the automatically learned strategy is proven to be the most robust to high levels of noise.", "labels": [], "entities": []}, {"text": "Section 2 presents some related work and Sec-: Simulated environment architecture tion 3 describes the simulated environment used for the experiments.", "labels": [], "entities": []}, {"text": "Then Section 4 describes the handcrafted turn-taking model as well as the RL one.", "labels": [], "entities": [{"text": "RL", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.49069517850875854}]}, {"text": "Section 5 presents the experimentation and the results and finally, Section 6 gives some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three dialogue scenario types involving diverse adding, modifying and deleting tasks were used for the experiments.", "labels": [], "entities": [{"text": "adding, modifying and deleting tasks", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.7466084559758505}]}, {"text": "For the training of the RL strategy, the simulated speech recognition WER was fixed at 0.15.", "labels": [], "entities": [{"text": "RL", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9570002555847168}, {"text": "WER", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8717606067657471}]}, {"text": "We trained the system 50 times and each training session is made of 3000 episodes.", "labels": [], "entities": []}, {"text": "The Fitted-Q algorithm was run every 500 episodes on the total batch from the beginning.", "labels": [], "entities": []}, {"text": "During the first 500 episodes, a pureexploration policy is used: it performs a WAIT action with a probability of 0.9 (hence a SPEAK action 10% of the times).", "labels": [], "entities": [{"text": "WAIT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9752562642097473}]}, {"text": "An -greedy ( = 0.1) policy is then used until episode 2500.", "labels": [], "entities": []}, {"text": "After that, a greedy policy is used (pure-exploitation).", "labels": [], "entities": []}, {"text": "Thus, 50 learning models are collected.", "labels": [], "entities": []}, {"text": "As a linear model is used for the Q-function representation, we simply average the parameters to get an average model.", "labels": [], "entities": []}, {"text": "The latter is then tested against the basic non-incremental strategy and our handcrafted baseline under different noise conditions by varying the WER parameter between 0 and 0.3 with a step of 0.03.", "labels": [], "entities": [{"text": "WER", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9816494584083557}]}], "tableCaptions": []}