{"title": [{"text": "A two-level classifier for discriminating similar languages", "labels": [], "entities": []}], "abstractContent": [{"text": "The BRUniBP team's submission is presented for the Discriminating between Similar Languages Shared Task 2015.", "labels": [], "entities": [{"text": "BRUniBP", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.649247944355011}, {"text": "Discriminating between Similar Languages Shared Task 2015", "start_pos": 51, "end_pos": 108, "type": "TASK", "confidence": 0.7172180499349322}]}, {"text": "Our method is a two phase classifier that utilizes both character and word-level features.", "labels": [], "entities": []}, {"text": "The evaluation shows 100% accuracy on language group identification and 93.66% accuracy on language identification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.999573290348053}, {"text": "language group identification", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.6270300447940826}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9994841814041138}, {"text": "language identification", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7362883239984512}]}, {"text": "The main contribution of the paper is a memory-efficient correlation based feature selection method.", "labels": [], "entities": []}], "introductionContent": [{"text": "The discrimination of similar languages (DSL) ( can be defined as the subtask of the language identification (LI) problem.", "labels": [], "entities": [{"text": "discrimination of similar languages (DSL)", "start_pos": 4, "end_pos": 45, "type": "TASK", "confidence": 0.771698364189693}, {"text": "language identification (LI)", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.8361359596252441}]}, {"text": "LI is a fundamental task in the area of natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.8088152805964152}]}, {"text": "The primary goal of LI is to determine the language of a written text.", "labels": [], "entities": [{"text": "determine the language of a written text", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.659346124955586}]}, {"text": "In practical applications, LI acts as a preprocessor of various NLP techniques as for example machine translation, sentiment analysis or even web search.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8344682157039642}, {"text": "sentiment analysis", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.9678339958190918}]}, {"text": "LI is currently an actively researched topic, DSL is also in the focus of interest (.", "labels": [], "entities": [{"text": "LI", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.532636821269989}, {"text": "DSL", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.8488473296165466}]}, {"text": "Unlike well-separated languages, multilingualism, varieties or dialects of language can seriously degrade the quality of LI.", "labels": [], "entities": []}, {"text": "DSL, noisy data, nonwell-formatted text, short sentences, mixed language (i.e. tweets) are other examples of challenging problems in this field.", "labels": [], "entities": []}, {"text": "In this paper we focus on the DSL problem on a shared task.", "labels": [], "entities": [{"text": "DSL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9647904634475708}]}, {"text": "Our experiment shows that discrimination between Bosnian, Croatian and Serbian and between Argentinian and Peninsular Spanish are the most challenging tasks for our methods.", "labels": [], "entities": []}, {"text": "Most state of the art methods solve the DSL task in two phases.", "labels": [], "entities": [{"text": "DSL task", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.8686749935150146}]}, {"text": "In the first phase the language group is to be identified, in the second phase the language is to be selected.", "labels": [], "entities": []}, {"text": "The first decision of the model is more coarse and high level, the second labelling is to be more specialized as different language groups have different separating features.", "labels": [], "entities": []}, {"text": "Regarding the information representation, most methods work with statistical features of the source text.", "labels": [], "entities": [{"text": "information representation", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.7380580902099609}]}, {"text": "The statistical features are n-grams at the word and at the character level.", "labels": [], "entities": []}, {"text": "The parameter n in the fixed-length character and word n-gram models ranges from 1 to 6.", "labels": [], "entities": []}, {"text": "In our approach a maximum entropy classifier and SVM with different kernels were evaluated.", "labels": [], "entities": []}, {"text": "The results show that maximum entropy delivers comparable results to SVM while it is considerably faster.", "labels": [], "entities": []}, {"text": "To tackle the issue of zero probabilities resulting from unseen n-grams, Katz's backoff smoothing is applied.", "labels": [], "entities": []}, {"text": "Training a classifier on a large number of features requires substantial computing resources, which we do not have readily accessible.", "labels": [], "entities": []}, {"text": "Features are pruned to less than 10,000 according to their pairwise Pearson correlation with the labels.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 68, "end_pos": 87, "type": "METRIC", "confidence": 0.8819909691810608}]}, {"text": "The code is available on GitHub.", "labels": [], "entities": []}, {"text": "Section 2 presents related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the dataset the methods are evaluated on.", "labels": [], "entities": []}, {"text": "Section 4 provides an overview of the architecture of our method and describes the classification method.", "labels": [], "entities": []}, {"text": "Section 5 gives insight into how the text is preprocessed before calculating the statistical features.", "labels": [], "entities": []}, {"text": "Section 6 presents the evaluation results.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our method is evaluated on the DSLCC dataset ( ), which dataset is provided for the shared task.", "labels": [], "entities": [{"text": "DSLCC dataset", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9700850546360016}]}, {"text": "As Tan et al. describe the collection and the preparation of the dataset in detail, we only provide a summary in.", "labels": [], "entities": []}, {"text": "The dataset contains 6 language groups of closely related languages and dialects plus one group called other.", "labels": [], "entities": []}, {"text": "The language groups are presented in the first column (Group) of the table.", "labels": [], "entities": []}, {"text": "The second column (Language) identifies the language, the third column (code) contains a short identifier for each language.", "labels": [], "entities": []}, {"text": "For each language, the dataset consists of 20 000 sentences.", "labels": [], "entities": []}, {"text": "Each list of sentences is divided into two parts as 18 000 sentence training sample and 2 000 sentence development sample.", "labels": [], "entities": []}], "tableCaptions": []}