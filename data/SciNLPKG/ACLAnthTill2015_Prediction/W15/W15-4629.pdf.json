{"title": [{"text": "Evaluating Spoken Dialogue Processing for Time-Offset Interaction", "labels": [], "entities": [{"text": "Evaluating Spoken Dialogue Processing", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7148450240492821}, {"text": "Interaction", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.6439560651779175}]}], "abstractContent": [{"text": "This paper presents the first evaluation of a full automated prototype system for time-offset interaction, that is, conversation between a live person and recordings of someone who is not temporally co-present.", "labels": [], "entities": []}, {"text": "Speech recognition reaches word error rates as low as 5% with general-purpose language models and 19% with domain-specific models, and language understanding can identify appropriate direct responses to 60-66% of user utterances while keeping errors to 10-16% (the remainder being indirect, or off-topic responses).", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8101252019405365}]}, {"text": "This is sufficient to enable a natural flow and relatively open-ended conversations , with a collection of under 2000 recorded statements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Time-offset interaction allows real-time synchronous conversational interaction with a person who is not only physically absent, but also not engaged in the conversation at the same time.", "labels": [], "entities": []}, {"text": "The basic premise of time-offset interaction is that when the topic of conversation is known, the participants' utterances are predictable to a large extent (.", "labels": [], "entities": []}, {"text": "Knowing what an interlocutor is likely to say, a speaker can record statements in advance; during conversation, a computer program selects recorded statements that are appropriate reactions to the interlocutor's utterances.", "labels": [], "entities": []}, {"text": "The selection of statements can be done in a similar fashion to existing interactive systems with synthetic characters (.", "labels": [], "entities": []}, {"text": "In we presented a proof of concept of time-offset interaction, which showed that given sufficiently interesting content, a reasonable interactive conversation could be demonstrated.", "labels": [], "entities": []}, {"text": "However that system had a very small amount of content, and would only really work if someone asked questions about a very limited set of topics.", "labels": [], "entities": []}, {"text": "There is a big gap from this proof of concept to evidence that the technique can work more generally.", "labels": [], "entities": []}, {"text": "One of the biggest questions is how much material needs to be recorded in order to support free-flowing conversation with naive interactors who don't know specifically what they can ask.", "labels": [], "entities": []}, {"text": "This question was addressed, at least for one specific case, in.", "labels": [], "entities": []}, {"text": "There we showed that an iterative development process involving two separated recording sessions, with Wizard of Oz testing in the middle, resulted in a body of material of around 2000 responses that could be used to answer over 95% of questions from the desired target audience.", "labels": [], "entities": []}, {"text": "In contrast, the 1400 responses from the first recording session alone was sufficient to answer less than 70% of users' questions.", "labels": [], "entities": []}, {"text": "Another question is whether current language processing technology is adequate to pick enough appropriate responses to carry on interesting and extended dialogues with a wide variety of interested interactors.", "labels": [], "entities": []}, {"text": "The proof of concept worked extremely well, even when people phrased questions very differently from the training data.", "labels": [], "entities": []}, {"text": "However, that system had very low perplexity, with fewer than 20 responses, rather than something two orders of magnitude bigger.", "labels": [], "entities": []}, {"text": "In this paper, we address the second question, of whether time-offset interaction can be automatically supported at a scale that can support interaction with people who know only the general topic of discussion, not what specific content is available.", "labels": [], "entities": []}, {"text": "In the next section, we review related work that is similar in spirit to time-offset interaction.", "labels": [], "entities": []}, {"text": "In Section 3 we review our materials, including the domain of interaction, the system architecture, dialogue policy, and collected training and test data.", "labels": [], "entities": []}, {"text": "In Section 4, we describe our evaluation methodology, including evaluation of speech recognition and classifier.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7717660963535309}]}, {"text": "In Section 5, we present our results, showing that over 70% of user utterances can be given a direct answer, and an even higher percentage can reach task success through a clarification process.", "labels": [], "entities": []}, {"text": "We conclude with a discussion and future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Classifier performance is best when training on all the data, and testing on transcriptions rather than speech recognizer output.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.685846358537674}]}, {"text": "shows the effect of the amount of training data on classifier performance when tested on transcribed text (a similar effect is observed when testing on speech recognizer output).", "labels": [], "entities": []}, {"text": "Lower curves represent better performance.", "labels": [], "entities": []}, {"text": "As expected, performance improves with additional training data -training on the full set of data cuts error rates by about a third compared to training on the elicitation questions alone.", "labels": [], "entities": [{"text": "error", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9812204837799072}]}, {"text": "Additional training data (both new questions and question-response links) are likely to improve performance even further.", "labels": [], "entities": []}, {"text": "The effect of speech recognition on classifier performance is shown in.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7489505410194397}]}, {"text": "Automatic speech recognition does impose a performance penalty compared to testing on transcriptions, but the penalty is not very large: classifier errors when testing with Google ASR are between 1 and 3 percentage points higher than with transcriptions, while PocketSphinx fares somewhat worse, with classifier errors about 5 to 8 percentage points     higher than with transcriptions.", "labels": [], "entities": [{"text": "Automatic speech recognition", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6456723113854727}, {"text": "PocketSphinx", "start_pos": 261, "end_pos": 273, "type": "DATASET", "confidence": 0.9041842222213745}]}, {"text": "At a 20% off-topic rate, the response error rates are 14% for transcriptions and 16% for Google ASR, meaning that almost two thirds of user utterances receive a direct appropriate response.", "labels": [], "entities": [{"text": "response error", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.5862264335155487}]}, {"text": "At 30% off-topics, errors drop to 10-11%, and direct appropriate responses drop to just shy of 60%.", "labels": [], "entities": [{"text": "errors", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9975294470787048}]}, {"text": "Informal impressions from current testing at a museum (section 6) suggests that these numbers are sufficient to enable a reasonable conversation flow.", "labels": [], "entities": []}, {"text": "Note that we also experimented with interpolating domain-specific with background LMs available from http://keithv.com/software.", "labels": [], "entities": []}, {"text": "Interpolation did not help but this is still an issue under investigation.", "labels": [], "entities": [{"text": "Interpolation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.781049907207489}]}, {"text": "Interpolation helped with speakers who had low WERs (smooth easy to recognize speech) but hurt in cases of speakers with high: Speech recognition results (WER).", "labels": [], "entities": [{"text": "WERs", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9923568964004517}, {"text": "Speech recognition results (WER)", "start_pos": 127, "end_pos": 159, "type": "METRIC", "confidence": 0.750370646516482}]}, {"text": "General LM stands for general-purpose LM, LM-ds stands for domain-specific LM trained with data collected until December 2014, and LM-ds-add stands for domain-specific LM trained with additional data collected until January 2015.", "labels": [], "entities": []}, {"text": "In the latter cases, having a background model meant that there were more choices for the speech recognizer to choose from, which instead of helping caused confusion.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.6895441114902496}]}, {"text": "We also noticed that PocketSphinx was less tolerant of environmental noises, which most of the time resulted in insertions and substitutions.", "labels": [], "entities": [{"text": "PocketSphinx", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9215712547302246}]}, {"text": "For example, as we can see in, the user input \"have you ever lived in israel\" was misrecognized by PocketSphinx as \"are you ever live in a israel\".", "labels": [], "entities": [{"text": "PocketSphinx", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.964264988899231}]}, {"text": "These misrecognitions do not necessarily confuse the classifier, but of course they often do.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Training data sets", "labels": [], "entities": [{"text": "Training data sets", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.823022743066152}]}, {"text": " Table 4: Speech recognition results (WER). Gen- eral LM stands for general-purpose LM, LM-ds  stands for domain-specific LM trained with data  collected until December 2014, and LM-ds-add  stands for domain-specific LM trained with addi- tional data collected until January 2015.", "labels": [], "entities": [{"text": "Speech recognition results (WER)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6111084818840027}]}]}