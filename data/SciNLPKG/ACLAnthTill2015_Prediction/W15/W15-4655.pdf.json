{"title": [{"text": "Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Reward Shaping", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8825742900371552}]}], "abstractContent": [{"text": "Statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users.", "labels": [], "entities": []}, {"text": "However in the reinforcement learning paradigm the dialogue manager (agent) often requires significant time to explore the state-action space to learn to behave in a desirable manner.", "labels": [], "entities": []}, {"text": "This is a critical issue when the system is trained on-line with real users where learning costs are expensive.", "labels": [], "entities": []}, {"text": "Reward shaping is one promising technique for addressing these concerns.", "labels": [], "entities": [{"text": "Reward shaping", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.952353447675705}]}, {"text": "Here we examine three recurrent neural network (RNN) approaches for providing reward shaping information in addition to the primary (task-orientated) environmental feedback.", "labels": [], "entities": [{"text": "reward shaping information", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.7938983043034872}]}, {"text": "These RNNs are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster.", "labels": [], "entities": []}, {"text": "In both simulated and real user scenarios these RNNs are shown to increase policy learning speed.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.8823072016239166}]}, {"text": "Importantly, they do not require prior knowledge of the user's goal.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken dialogue systems (SDS) offer a natural way for people to interact with computers.", "labels": [], "entities": [{"text": "Spoken dialogue systems (SDS)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.69862533112367}]}, {"text": "With the ability to learn from data (interactions) statistical SDS can theoretically be created faster and with less man-hours than a comparable handcrafted rule based system.", "labels": [], "entities": [{"text": "SDS", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.6331562399864197}]}, {"text": "They have also been shown to perform better (. Central to this is the use of partially observable Markov decision processes (POMDP) to model dialogue, which inherently manage the uncertainty created by errors in speech recognition and semantic decoding).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 212, "end_pos": 230, "type": "TASK", "confidence": 0.7221056222915649}]}, {"text": "The dialogue manager is a core component of an SDS and largely determines the quality of interaction.", "labels": [], "entities": []}, {"text": "Its behaviour is controlled by a policy which maps belief states to system actions (or distributions over sets of actions) and this policy is trained in a reinforcement learning framework) where rewards are received from the environment, the most informative of which occurs only at the dialogues conclusion, indicating task successor failure.", "labels": [], "entities": []}, {"text": "It is the sparseness of this environmental reward function which, by not providing any information at intermediate turns, requires exploration to traverse deeply many sub-optimal paths.", "labels": [], "entities": []}, {"text": "This is a significant concern when training SDS online with real users where one wishes to minimise client exposure to sub-optimal system behaviour.", "labels": [], "entities": [{"text": "SDS online", "start_pos": 44, "end_pos": 54, "type": "TASK", "confidence": 0.8829977512359619}]}, {"text": "In an effort to counter this problem, reward shaping () introduces domain knowledge to provide earlier informative feedback to the agent (additional to the environmental feedback) for the purpose of biasing exploration for discovering optimal behaviour quicker.", "labels": [], "entities": []}, {"text": "Reward shaping is briefly reviewed in Section 2.1.", "labels": [], "entities": [{"text": "Reward shaping", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9646303653717041}]}, {"text": "In the context of SDS, have motivated the use of reward shaping via analogy to the 'social signals' naturally produced and interpreted throughout a humanhuman dialogue.", "labels": [], "entities": [{"text": "SDS", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9648672342300415}]}, {"text": "This non-statistical reward shaping model used heuristic features for speeding up policy learning.", "labels": [], "entities": []}, {"text": "As an alternative, one may consider attempting to handcraft a finer grained environmental reward A uniform reward of -1 is common for all other, nonterminal turns, which promotes faster task completion.", "labels": [], "entities": []}, {"text": "Learning algorithms are another central element in improving the speed of convergence during policy training.", "labels": [], "entities": []}, {"text": "In particular the sample-efficiency of the learning algorithm can be the deciding factor in whether it can realistically be employed on-line.", "labels": [], "entities": []}, {"text": "See e.g. the GP-SARSA () and Kalman temporal-difference () methods which bootstrap estimates of sparse value functions from minimal numbers of samples (dialogues). function.", "labels": [], "entities": []}, {"text": "For example, proposed diffusing expert ratings of dialogues to the state transition level to produce a richer reward function.", "labels": [], "entities": []}, {"text": "Policy convergence may occur faster in this altered POMDP and dialogues generated by a task based simulated user may also alleviate the need for expert ratings.", "labels": [], "entities": []}, {"text": "However, unlike reward shaping, modifying the environmental reward function also modifies the resulting optimal policy.", "labels": [], "entities": [{"text": "reward shaping", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7149816751480103}]}, {"text": "We recently proposed convolutional and recurrent neural network (RNN) approaches for determining dialogue success.", "labels": [], "entities": []}, {"text": "This was used to provide a reinforcement signal for learning on-line from real users without requiring any prior knowledge of the user's task ().", "labels": [], "entities": []}, {"text": "Here we extend the RNN approach by introducing new training constraints in order to combine the merits of the above three works: (1) diffusing dialogue level ratings down to the turn level to (2) add reward shaping information for faster policy learning, whilst (3) not requiring prior task knowledge which is simply unavailable on-line.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly describe potential based reward shaping before introducing the RNNs we explore for producing reward shaping signals (basic RNN, long short-term memory (LSTM) and gated recurrent unit (GRU)).", "labels": [], "entities": [{"text": "reward shaping", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7466635704040527}]}, {"text": "The features the RNNs use along with the training constraint and loss are also described.", "labels": [], "entities": [{"text": "loss", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9753350615501404}]}, {"text": "The experimental evaluation is then presented in Section 3.", "labels": [], "entities": []}, {"text": "Firstly, the estimation accuracy of the RNNs is assessed.", "labels": [], "entities": [{"text": "estimation", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9850313663482666}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9101226925849915}]}, {"text": "The benefit of using the RNN for reward shaping in both simulated and real user scenarios is then also demonstrated.", "labels": [], "entities": [{"text": "reward shaping", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7387610226869583}]}, {"text": "Finally, conclusions are presented in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all experiments the Cambridge restaurant domain was used, which consists of approximately 150 venues each having 6 attributes (slots) of which 3 can be used by the system to constrain the search and the remaining 3 are informable properties once a database entity has been found.", "labels": [], "entities": [{"text": "Cambridge restaurant domain", "start_pos": 23, "end_pos": 50, "type": "DATASET", "confidence": 0.9531680941581726}]}, {"text": "The shared core components of the SDS in all experiments were a domain independent ASR, a confusion network (CNet) semantic input decoder (, the BUDS () belief state tracker that factorises the dialogue state using a dynamic Bayesian network and a template based natural language generator.", "labels": [], "entities": [{"text": "BUDS", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9120734930038452}]}, {"text": "All policies were trained by GP-SARSA () and the summary action space contains 20 actions.", "labels": [], "entities": [{"text": "GP-SARSA", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9233163595199585}]}, {"text": "Per turn reward was set to -1 and final reward 20 for task success else 0.", "labels": [], "entities": []}, {"text": "With this ontology, the size of the full feature vector was 147.", "labels": [], "entities": []}, {"text": "The turn number was expressed as a percentage of the maximum number of allowed turns, here 30.", "labels": [], "entities": []}, {"text": "The one-hot user dialogue act encoding was formed by taking only the most likely user act estimated by the CNet decoder.: RMSE of return prediction by using RNN/LSTM/GRU, trained on 18K and 1K dialogues and tested on sets testA and testB (see text).", "labels": [], "entities": [{"text": "CNet decoder.", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.9639710783958435}, {"text": "RMSE", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.978813648223877}, {"text": "return prediction", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.6386998742818832}]}], "tableCaptions": []}