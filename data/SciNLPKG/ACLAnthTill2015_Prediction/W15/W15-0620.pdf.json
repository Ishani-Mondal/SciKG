{"title": [{"text": "Oracle and Human Baselines for Native Language Identification", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.6937888463338217}]}], "abstractContent": [{"text": "We examine different ensemble methods, including an oracle, to estimate the upper-limit of classification accuracy for Native Language Identification (NLI).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.965398371219635}, {"text": "Native Language Identification (NLI)", "start_pos": 119, "end_pos": 155, "type": "TASK", "confidence": 0.8245804806550344}]}, {"text": "The oracle outperforms state-of-the-art systems by over 10% and results indicate that for many misclassified texts the correct class label receives a significant portion of the ensemble votes, often being the runner-up.", "labels": [], "entities": []}, {"text": "We also present a pilot study of human performance for NLI, the first such experiment.", "labels": [], "entities": []}, {"text": "While some participants achieve modest results on our simplified setup with 5 L1s, they did not outperform our NLI system, and this performance gap is likely to widen on the standard NLI setup.", "labels": [], "entities": []}], "introductionContent": [{"text": "Native Language Identification (NLI) is the task of inferring the native language (L1) of an author based on texts written in a second language (L2).", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7722204426924387}]}, {"text": "Machine Learning methods are usually used to identify language use patterns common to speakers of the same L1 ().", "labels": [], "entities": []}, {"text": "The motivations for NLI are manifold.", "labels": [], "entities": []}, {"text": "The use of such techniques can help SLA and ESL researchers identify important L1-specific learning and teaching issues, enabling them to develop pedagogical material that takes into consideration a learner's L1.", "labels": [], "entities": []}, {"text": "It has also been used to study language transfer hypotheses and extract common L1-related learner errors.", "labels": [], "entities": [{"text": "language transfer hypotheses", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.8307172060012817}]}, {"text": "NLI has drawn the attention of many researchers in recent years.", "labels": [], "entities": [{"text": "NLI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7119483351707458}]}, {"text": "With the influx of new researchers, the most substantive work in this field has come in the last few years, leading to the organization of the inaugural NLI Shared Task in 2013which was attended by 29 teams from the NLP and SLA areas ).", "labels": [], "entities": []}, {"text": "An interesting question about NLI research concerns an upper-bound on the accuracy achievable fora dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9979947805404663}]}, {"text": "More specifically, given a dataset, a selection of features and classifiers, what is the maximal performance that could be achieved by an NLI system that always picks the best candidate?", "labels": [], "entities": []}, {"text": "This question, not previously addressed in the context of NLI to date, is the primary focus of the present work.", "labels": [], "entities": []}, {"text": "Such a measure is an interesting and useful upper-limit baseline for researchers to consider when evaluating their work, since obtaining 100% classification accuracy may not be a reasonable or even feasible goal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.8569118976593018}]}, {"text": "In this study we investigate this issue with the aim of deriving such an upper-limit for NLI accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.956629753112793}]}, {"text": "A second goal of this work is to measure human performance for NLI, something not attempted to date.", "labels": [], "entities": []}, {"text": "To this end we design and run a crowdsourced experiment where human evaluators predict the L1 of texts from the NLI shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first experiment attempts to derive the potential accuracy upper-limit of our feature set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9986410737037659}]}, {"text": "We train a single linear Support Vector Machine (SVM) classifier for each feature type to create our classifier ensemble.", "labels": [], "entities": []}, {"text": "Linear SVMs have been shown to be effective for such text classification problems and was the classifier of choice in the 2013 NLI Shared Task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7376080751419067}, {"text": "2013 NLI Shared Task", "start_pos": 122, "end_pos": 142, "type": "DATASET", "confidence": 0.7630311995744705}]}, {"text": "We do not experiment with combining different machine learning algorithms here, instead we focus on gauging the potential of the feature set.", "labels": [], "entities": []}, {"text": "We employ a standard set of previously used feature types: character/word n-grams, Part-of-Speech (POS) n-grams, function words, Context-free grammar production rules, Tree Substitution Grammar fragments and Stanford Dependencies.", "labels": [], "entities": []}, {"text": "Descriptions of these features can be  found in \u00a74.1 of Tetreault et al.", "labels": [], "entities": []}, {"text": "We report classification accuracy under 10-fold cross-validation using the TOEFL11 training data and also on the test set from the 2013 shared task, shown in.", "labels": [], "entities": [{"text": "classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9312203526496887}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9163281917572021}, {"text": "TOEFL11 training data", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.913330058256785}]}, {"text": "For both we report a random baseline and the best performances on the Shared Task: the first number is the top performer from the shared task (, and the number in parentheses is the best published performance after the shared task () . The cross-validation and test results are very similar, with the oracle accuracy at 95%, suggesting that for each document there is inmost cases at least one feature type that correctly predicts it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 308, "end_pos": 316, "type": "METRIC", "confidence": 0.945014476776123}]}, {"text": "This drops to 88% with the Accuracy@2 combiner, still much higher than the plurality vote and the best results from the shared task.", "labels": [], "entities": [{"text": "Accuracy@2 combiner", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.885852500796318}]}, {"text": "This suggests that there is a noticeable tail of feature types dragging the plurality vote down.", "labels": [], "entities": []}, {"text": "In the second experiment we apply our methods to the submissions in the 2013 NLI Shared Task, aiming to quantify the potential upper limit for combining a range of different systems.", "labels": [], "entities": [{"text": "2013 NLI Shared Task", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.8868351131677628}]}, {"text": "The data comes from the closed-training sub-task.", "labels": [], "entities": []}, {"text": "5 Each team was allowed to submit up to 5 different runs for each task, allowing them to experiment with different feature and parameter variations of their system.", "labels": [], "entities": []}, {"text": "Each team's systems produce predictions using their own set of features and learning algorithms, with several of these systems using ensembles themselves.", "labels": [], "entities": []}, {"text": "In total, 115 runs were submitted by 29 teams, with the winning entry achieving the highest accuracy of 83.6% on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9994257688522339}]}, {"text": "We experiment under  two conditions: using only each team's best run and using all 115 runs.", "labels": [], "entities": []}, {"text": "Results are compared against the random baseline and winning entry.", "labels": [], "entities": []}, {"text": "shows the results for this experiment.", "labels": [], "entities": []}, {"text": "The oracle results are higher than the previous experiment, which is not unexpected given the much larger number of predictions per document.", "labels": [], "entities": []}, {"text": "Results for the other combiners are also higher here.", "labels": [], "entities": [{"text": "combiners", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.7635060548782349}]}, {"text": "The Accuracy@2 results are 92% in both conditions, much higher than the winning entry's 83%.", "labels": [], "entities": [{"text": "Accuracy@2", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9673566818237305}]}, {"text": "Results from the Accuracy@2 combiner, both here and in the previous experiment, show that a great majority of the texts are close to being correctly classified: this value is significantly higher than the plurality combiner and not much lower than the oracle.", "labels": [], "entities": [{"text": "Accuracy@2 combiner", "start_pos": 17, "end_pos": 36, "type": "METRIC", "confidence": 0.7369698584079742}]}, {"text": "This shows that the correct label receives a significant portion of the votes and when not the winning label, it is often the runner-up.", "labels": [], "entities": []}, {"text": "One implication of this concerns practical applications of NLI, e.g. in a manual analysis, where it maybe worthwhile for researchers to also consider the runner-up label in their evaluation.", "labels": [], "entities": []}, {"text": "This knowledge could also be used to increase NLI accuracy by aiming to develop more sophisticated classifiers that can take into account the top N labels in their decision making, similar to discriminative reranking methods applied in statistical parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9640449285507202}, {"text": "statistical parsing", "start_pos": 236, "end_pos": 255, "type": "TASK", "confidence": 0.7209568619728088}]}, {"text": "Using the Accuracy@2 combiner, we isolate the cases where the actual label was the runner up and extract the most frequent pairs of top 2 labels, presented in.", "labels": [], "entities": [{"text": "Accuracy@2 combiner", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8830027133226395}]}, {"text": "We see that a quarter of the errors are confusion between Hindi and Telugu.", "labels": [], "entities": []}, {"text": "The Korean and Turkish confusion could be due to both being Altaic languages.", "labels": [], "entities": []}, {"text": "We also examine the confusion matrices for the plurality, Accuracy@2 and oracle combiners, 8 shown   in.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9656229615211487}]}, {"text": "They show that Hindi-Telugu is the most commonly confused pair and confirm the directionality of the confusion: more Telugu texts are misclassified as Hindi than vice versa.", "labels": [], "entities": []}, {"text": "Our initial hypothesis was to use the Amazon Mechanical Turk to collect crowdsourced judgments.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.9473394950230917}]}, {"text": "However, unlike simpler NLP tasks, e.g. sentiment analysis and word sense disambiguation, which can be effectively annotated by untrained Turkers (, NLI requires raters with knowledge and exposure to writers with different L1s.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9596213698387146}, {"text": "word sense disambiguation", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.7558609843254089}]}, {"text": "Optimally, one would use a set of ESL teachers and researchers who have experience in working with ESL writers from all of the 11 L1s, though such people are rarity.", "labels": [], "entities": []}, {"text": "As a reasonable compromise, we chose 10 professors and researchers who have varied linguistic backgrounds, speak multiple languages, and have had exposure with the particular L1s, either as a speaker or through working with ESL students.", "labels": [], "entities": []}, {"text": "We also constrained the task from 11 L1s to 5 (Arabic, Chinese, German, Hindi, and Spanish) as we believed that 11 L1s would be too much of an overload on the judges.", "labels": [], "entities": []}, {"text": "The 5 L1s were selected since they all belong to separate language families.", "labels": [], "entities": []}, {"text": "The experiment consisted of rating 30 essays from TOEFL11-TEST, 15 of which most Shared Task systems could predict correctly (easy), and the remaining 15 were essays in which the Shared Task systems had difficulty (hard).", "labels": [], "entities": [{"text": "TOEFL11-TEST", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9192657470703125}]}, {"text": "The L1s were distributed evenly over the essays and easy/hard conditions (3 \"easy\" and 3 \"hard\" essays per L1).", "labels": [], "entities": []}, {"text": "shows the accuracy for each rater in this pilot study.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997099041938782}]}, {"text": "The top rater accurately identified 16 out of 30 L1s (53.3%), with the lowest raters at 30.0% overall and an average of 37.3%.", "labels": [], "entities": []}, {"text": "All raters did better on the \"easy\" cases than on the \"hard.\"", "labels": [], "entities": []}, {"text": "A paired-samples t-test was conducted to compare human accuracy in the easy and hard conditions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9966976642608643}]}, {"text": "A significant difference was found for easy (M =45.33, SD=11.67) and hard (M =30, SD=10.06), t(9)=\u22123.851, p = .004.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Oracle results using our feature set.", "labels": [], "entities": []}, {"text": " Table 3: Oracle results on the shared task systems.", "labels": [], "entities": []}, {"text": " Table 4: Most commonly predicted top 2 label pairs where  the runner-up is the true label.", "labels": [], "entities": []}, {"text": " Table 5: Comparing human participant performance  against an NLI system on 30 selected texts.", "labels": [], "entities": []}]}