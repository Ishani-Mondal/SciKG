{"title": [{"text": "NLEL UPV Autoritas participation at Discrimination between Similar Languages (DSL) 2015 Shared Task", "labels": [], "entities": [{"text": "NLEL UPV Autoritas", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.7905245025952657}, {"text": "Discrimination between Similar Languages (DSL) 2015 Shared Task", "start_pos": 36, "end_pos": 99, "type": "TASK", "confidence": 0.6453338950872421}]}], "abstractContent": [{"text": "In this paper we describe the participation of the Natural Language Engineering Lab (NLEL)-Universitat Polit\u00e8cnica de Va\u00ec encia and Autoritas Consulting team in the Discrimination between Similar Languages (DSL) 2015 shared task.", "labels": [], "entities": [{"text": "Natural Language Engineering Lab (NLEL)-Universitat Polit\u00e8cnica de Va\u00ec encia", "start_pos": 51, "end_pos": 127, "type": "TASK", "confidence": 0.5309036845962206}, {"text": "Discrimination between Similar Languages (DSL) 2015 shared task", "start_pos": 165, "end_pos": 228, "type": "TASK", "confidence": 0.7317408502101899}]}, {"text": "We have participated both in open and close submissions.", "labels": [], "entities": []}, {"text": "Our system for the open submission performs in two steps.", "labels": [], "entities": [{"text": "open submission", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.6569481492042542}]}, {"text": "Firstly, we apply a language detector to identify the distinct groups corresponding to families of languages/dialects, and then we distinguish between varieties with a probabilis-tic method.", "labels": [], "entities": []}, {"text": "For the close submission, we implemented our probabilistic method in a multi-class classifier for all the language varieties together.", "labels": [], "entities": []}, {"text": "Although our results on the development set were quite promising (93.07% and 86.08% respectively), a software bug (that we have detected only after the submission) dropped considerably our results in the final testing.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automatic language identification task aims to determine the language of a given text.", "labels": [], "entities": [{"text": "automatic language identification task", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.6891594380140305}]}, {"text": "The performance on this task is pretty high with long texts, but it becomes harder when texts are shorter.", "labels": [], "entities": []}, {"text": "This may occur in social media scenarios like.", "labels": [], "entities": []}, {"text": "Furthermore, in social media we may want to go beyond the language scope to identify also dialects or varieties.", "labels": [], "entities": []}, {"text": "The objective of the language variety identification is to determine the regional variety of a given language.", "labels": [], "entities": [{"text": "language variety identification", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6782599488894144}]}, {"text": "For example, to know whether a Spanish text is Peninsular, Argentinian, Mexican, and so forth.", "labels": [], "entities": []}, {"text": "Language variety identification maybe classified as an author profiling task.", "labels": [], "entities": [{"text": "Language variety identification", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7903177440166473}, {"text": "author profiling task", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7694378892580668}]}, {"text": "Author profiling aims at identifying the linguistic profile of an author on the basis of her writing style.", "labels": [], "entities": [{"text": "Author profiling", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7256556898355484}]}, {"text": "The objective is to determine author's traits such as age, gender, native language, personality traits or language varieties, among others.", "labels": [], "entities": []}, {"text": "It is noteworthy the interest in author profiling since 2013, as can be seen in the number of shared tasks: i) Age and gender identification at the Author Profiling task at PAN 1 at CLEF 2013 () and 2014 ().", "labels": [], "entities": [{"text": "author profiling", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8223974704742432}, {"text": "gender identification", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.6628022640943527}, {"text": "Author Profiling task at PAN 1 at CLEF 2013", "start_pos": 148, "end_pos": 191, "type": "DATASET", "confidence": 0.7642493844032288}]}, {"text": "In PAN 2015 ( ) personality recognition is also treated; ii) native language identification at BEA-8 workshop at NAACL-HLT 2013   . DSL is a hot research topic.", "labels": [], "entities": [{"text": "personality recognition", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.7411821335554123}, {"text": "native language identification", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.6361270447572073}, {"text": "BEA-8 workshop at NAACL-HLT 2013", "start_pos": 95, "end_pos": 127, "type": "DATASET", "confidence": 0.6683749556541443}]}, {"text": "The authors in) researched the identification of Arabic varieties in blogs and forums.", "labels": [], "entities": [{"text": "identification of Arabic varieties", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.846348762512207}]}, {"text": "They used character n-grams and Support Vector Machines, and reported accuracies between 70-80% in a 10-fold cross-validation evaluation.) the authors collected 1.000 news articles in two Portuguese varieties: Portugal and Brazil.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9796837568283081}]}, {"text": "They used word n-grams and character n-grams and reported accuracies over 90% in a 50-50 split evaluation.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9971562623977661}]}, {"text": "They used language probability distributions with log-likelihood function for probability estimation.", "labels": [], "entities": [{"text": "probability estimation", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7908842265605927}]}, {"text": "In), the authors collected tweets in four different Spanish varieties: Argentina, Colombia, Mexico and Spain.", "labels": [], "entities": []}, {"text": "They used four types of features combined with a meta-classifier: character n-gram with frequency profiles, character n-gram language models, LZW compresssion and syllable-based language models.", "labels": [], "entities": []}, {"text": "The reported accuracies were between 60-70% in a cross-validation evaluation.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9967412352561951}]}, {"text": "It is also interesting to analyse the submitted systems to the LT4VarDial task.", "labels": [], "entities": []}, {"text": "In the system presented in () the authors approached the task in two steps.", "labels": [], "entities": []}, {"text": "First, it predicted the language group with a 6-way probabilistic classifier.", "labels": [], "entities": []}, {"text": "Then, the variety was predicted with a voting combination of discriminative classifiers.", "labels": [], "entities": []}, {"text": "They used character and word n-grams and reported 95.71% of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9978614449501038}]}, {"text": "The system presented in) used a hierarchical classifier based on maximum-entropy classifiers.", "labels": [], "entities": []}, {"text": "The first level predicted the language group and the second the language variety within the predicted group.", "labels": [], "entities": []}, {"text": "They experimented with character and word n-grams, together with a list of words which exclusively belong to each language variety.", "labels": [], "entities": []}, {"text": "The reported accuracy was 92.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9967207312583923}]}, {"text": "The authors in) used linear Support Vector Machines with character and word n-grams.", "labels": [], "entities": []}, {"text": "They analysed in depth how the cost parameter influenced the classification results, and reported an overall accuracy over 95% after fixing a bug.", "labels": [], "entities": [{"text": "classification", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.965218722820282}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9995595812797546}]}, {"text": "The system reported in () combined character and word n-grams with feature selection techniques such as Information Gain and Parallel Text Feature Extraction.", "labels": [], "entities": [{"text": "Parallel Text Feature Extraction", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.6014871522784233}]}, {"text": "The authors reported that Naive Bayes performed better than Support Vector), the authors devoted their research to explore novel methods for DSL.", "labels": [], "entities": [{"text": "DSL", "start_pos": 141, "end_pos": 144, "type": "TASK", "confidence": 0.934447705745697}]}, {"text": "They obtained their best result using their langid.py tool (, with a 91.80% of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995260238647461}]}, {"text": "Our interest in DSL goes beyond the use of features such as n-grams.", "labels": [], "entities": []}, {"text": "Our objective is to better understand the linguistic differences between varieties as well as the relationship to other author profiling tasks.", "labels": [], "entities": []}, {"text": "In, we approached the DSL task with distributed representations.", "labels": [], "entities": []}, {"text": "We also compared with Emograph (Rangel and Rosso, 2015a), a graphbased approach which obtained competitive accuracies with PAN datasets) in the age and gender author profiling tasks.", "labels": [], "entities": [{"text": "PAN datasets", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.7553155720233917}, {"text": "gender author profiling tasks", "start_pos": 152, "end_pos": 181, "type": "TASK", "confidence": 0.6890921294689178}]}, {"text": "In this paper we describe our participation at the DSL 2015 shared task ().", "labels": [], "entities": [{"text": "DSL 2015 shared task", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7272303551435471}]}, {"text": "We approached the task by proposing a probabilistic method which tries to capture lexical differences between varieties.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we show the evaluation of the proposed methodology when participating in the DSL 2015 shared task.", "labels": [], "entities": [{"text": "DSL 2015 shared task", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.8359726518392563}]}, {"text": "Firstly, the dataset and the evaluation methodology are described.", "labels": [], "entities": []}, {"text": "Then, the official results are shown.", "labels": [], "entities": []}, {"text": "We detected a bug that is also described in this section.", "labels": [], "entities": []}, {"text": "Finally, we explain our participation in the open and close submissions respectively, and discuss a comparison between both submissions.", "labels": [], "entities": []}, {"text": "We used the DSLCC v.", "labels": [], "entities": [{"text": "DSLCC", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9291691780090332}]}, {"text": "The length of each sentence ranges from 20 to 100 tokens.", "labels": [], "entities": []}, {"text": "For each language or dialect, this dataset contains 18.000 instances for training, 2.000 instances for development and 1.000 instances for each test set.", "labels": [], "entities": []}, {"text": "A summary of the total number of instances is shown in  We used the training set to learn probabilities and the corresponding machine learning models.", "labels": [], "entities": []}, {"text": "We tested our methods with the development set using the Weka GUI 7).", "labels": [], "entities": [{"text": "Weka GUI 7", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9573164780934652}]}, {"text": "We built a Java application to predict documents in the test set by using the models previously learned with Weka.", "labels": [], "entities": []}, {"text": "In the following sections we explain the specific approach for both open and close submissions.", "labels": [], "entities": []}, {"text": "We present comparative results among development, test A and test B.", "labels": [], "entities": []}, {"text": "We also carried out a statistical significance test between results for both test sets.", "labels": [], "entities": []}, {"text": "We used the following notation for confidence levels: * at 95% and ** at 99%", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Languages in the DSLCC v.2.0 dataset.", "labels": [], "entities": [{"text": "DSLCC v.2.0 dataset", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.8716438810030619}]}, {"text": " Table 2: Number of instances per set.", "labels": [], "entities": []}, {"text": " Table 3: Identification accuracies for the open and  close submission for tests A and B.", "labels": [], "entities": [{"text": "Identification accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.9048470854759216}]}, {"text": " Table 4: Performance differences between Weka  GUI and our Java application in the development  set.", "labels": [], "entities": [{"text": "Weka", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9260486364364624}]}, {"text": " Table 5: Identification accuracies of the ldig lan- guage detector in the development set.", "labels": [], "entities": [{"text": "Identification", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.976015031337738}]}, {"text": " Table 6: Identification accuracies for the open sub- mission for development, test, and NE blinded  test.", "labels": [], "entities": [{"text": "Identification accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.9546685814857483}, {"text": "NE blinded  test", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.5886476238568624}]}, {"text": " Table 7: Identification accuracies for the close  submission for development, test, and NE blinded  test.", "labels": [], "entities": [{"text": "Identification accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.9449390769004822}, {"text": "NE blinded", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.8545071184635162}]}, {"text": " Table 8: Identification accuracies for the open and  close submissions in development set.", "labels": [], "entities": [{"text": "Identification accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.920736163854599}]}]}