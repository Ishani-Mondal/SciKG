{"title": [{"text": "Evaluating the performance of Automated Text Scoring systems", "labels": [], "entities": [{"text": "Automated Text Scoring", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.6101462741692861}]}], "abstractContent": [{"text": "Various measures have been used to evaluate the effectiveness of automated text scoring (ATS) systems with respect to a human gold standard.", "labels": [], "entities": [{"text": "text scoring (ATS)", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8108804225921631}]}, {"text": "However, there is no systematic study comparing the efficacy of these met-rics under different experimental conditions.", "labels": [], "entities": []}, {"text": "In this paper we first argue that measures of agreement are more appropriate than measures of association (i.e., correlation) for measuring the effectiveness of ATS systems.", "labels": [], "entities": [{"text": "correlation", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.9323607087135315}, {"text": "ATS", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9489557147026062}]}, {"text": "We then present a thorough review and analysis of frequently used measures of agreement.", "labels": [], "entities": []}, {"text": "We outline desirable properties for measuring the effectiveness of an ATS system, and experimentally demonstrate using both synthetic and real ATS data, that some commonly used measures (e.g., Cohen's kappa) lack these properties.", "labels": [], "entities": []}, {"text": "Finally, we identify the most appropriate measures of agreement and present general recommendations for best evaluation practices.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated assessment of text was introduced in the early 1960s in an attempt to address several issues with manual assessment (e.g., expense, speed, and consistency).", "labels": [], "entities": [{"text": "Automated assessment of text", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7216124832630157}, {"text": "speed", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.9800091981887817}, {"text": "consistency", "start_pos": 153, "end_pos": 164, "type": "METRIC", "confidence": 0.9955341815948486}]}, {"text": "Further advantages become more pronounced when it comes to scoring extended texts such as essays, a task prone to an element of subjectivity.", "labels": [], "entities": [{"text": "scoring extended texts such as essays", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.7904586493968964}]}, {"text": "Automated systems enable rigid application of scoring criteria, thus reducing the inconsistencies which may arise, in particular, when many human examiners are employed for large-scale assessment.", "labels": [], "entities": []}, {"text": "There is a substantial literature describing and evaluating ATS systems.", "labels": [], "entities": [{"text": "ATS", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.983173131942749}]}, {"text": "Such systems are increasingly used but remain controversial.", "labels": [], "entities": []}, {"text": "Although a comprehensive comparison of the capabilities of eight existing commercial essay scoring systems across five different performance metrics in the recent ATS competition organised by Kaggle 1 claimed that ATS systems grade similarly to humans, critics () have continued to dispute this.", "labels": [], "entities": [{"text": "ATS competition organised by Kaggle 1", "start_pos": 163, "end_pos": 200, "type": "DATASET", "confidence": 0.5797272622585297}]}, {"text": "For the evaluation of ATS systems, emphasis has been given to the \"agreement\" of machine-predicted scores (ordinal grades) with that of a human gold standard; that is, scores assigned by human examiners to the same texts that the machine is evaluated on.", "labels": [], "entities": [{"text": "ATS", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9400220513343811}]}, {"text": "Various metrics have been used, the most prominent being Pearson's correlation, percentage of agreement, and variations of Cohen's kappa statistic.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 57, "end_pos": 78, "type": "METRIC", "confidence": 0.8480802377065023}, {"text": "percentage of agreement", "start_pos": 80, "end_pos": 103, "type": "METRIC", "confidence": 0.843499998251597}]}, {"text": "Inconsistencies in the reporting of, and misconceptions in the interpretation of, these metrics in published work makes cross-system comparisons on publicly-available datasets more difficult.", "labels": [], "entities": []}, {"text": "The lack of careful motivation of any metric fuels opposition to the deployment of ATS.", "labels": [], "entities": [{"text": "ATS", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.6596299409866333}]}, {"text": "To date, several ATS systems are being used operationally for high-stakes assessment in addition to them being part of self-assessment and self-tutoring sys-tems, underscoring the need for common and wellmotivated metrics that establish true system performance.", "labels": [], "entities": []}, {"text": "In this paper, we define the task of ATS as the accurate prediction of gold-standard scores (predefined ordinal grades), and we experimentally examine the robustness and efficacy of measures of agreement fora number of different conditions under two different experimental setups.", "labels": [], "entities": [{"text": "ATS", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.6774886846542358}]}, {"text": "First, we use synthetic data to simulate various experimental conditions, and second, we use real ATS data to assess the effectiveness of the metrics under realistic scenarios.", "labels": [], "entities": []}, {"text": "For the latter, we run a series of experiments on the output of state-of-the-art ATS systems.", "labels": [], "entities": [{"text": "ATS", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.8164758086204529}]}, {"text": "We outline some deficiencies in commonly used metrics that have been previously overlooked, and consequently we propose more appropriate metrics for evaluating ATS systems focusing primarily on optimising system effectiveness and facilitating crosssystem comparison.", "labels": [], "entities": [{"text": "ATS", "start_pos": 160, "end_pos": 163, "type": "TASK", "confidence": 0.9518921971321106}]}, {"text": "The focus on measures of agreement is motivated by their use as the primary metric for evaluating system effectiveness in the recent Kaggle essay scoring competition.", "labels": [], "entities": [{"text": "Kaggle essay scoring competition", "start_pos": 133, "end_pos": 165, "type": "TASK", "confidence": 0.5515625178813934}]}, {"text": "To the best of our knowledge, there is no systematic study comparing the efficacy of different measures of agreement under different experimental conditions.", "labels": [], "entities": []}, {"text": "Although we focus on the task of ATS, the recommendations regarding the metrics covered in this paper extend naturally to many similar NLP tasks, i.e., those where the task is to accurately predict a gold-standard score.", "labels": [], "entities": [{"text": "ATS", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8536507487297058}]}, {"text": "The remainder of the paper is structured as follows: Section 2 defines our task and objectives.", "labels": [], "entities": []}, {"text": "Section 3 reviews a number of performance metrics relevant to the ATS task.", "labels": [], "entities": [{"text": "ATS task", "start_pos": 66, "end_pos": 74, "type": "TASK", "confidence": 0.9393033385276794}]}, {"text": "Section 4 describes a set of desired metric properties and presents an analysis of some prominently used metrics for the ATS task that uses the output of both simulated and real systems.", "labels": [], "entities": [{"text": "ATS task", "start_pos": 121, "end_pos": 129, "type": "TASK", "confidence": 0.936605304479599}]}, {"text": "Section 5 concludes with a discussion, general recommendations for evaluation practices and an outline of future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Cohen's \u03ba for an ats system on two sets  of essays. Although percentage agreement is 0.8 for  both sets of essays, C \u03ba = 0.6 (left) and C \u03ba = 0.49  (right).", "labels": [], "entities": []}, {"text": " Table 2: C \u03ba for two systems ats 1 and ats 2 for the  same set of gold scores. Although percentage agree- ment for ats 1 and ats 2 is 0.4 and 0.65 respectively,  C \u03ba for ats 1 and ats 2 is C \u03ba = 0.12 (left) and C \u03ba = 0  (right).", "labels": [], "entities": [{"text": "percentage agree- ment", "start_pos": 89, "end_pos": 111, "type": "METRIC", "confidence": 0.7856419831514359}]}]}