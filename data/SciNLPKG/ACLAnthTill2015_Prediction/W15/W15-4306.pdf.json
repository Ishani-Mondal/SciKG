{"title": [{"text": "USFD: Twitter NER with Drift Compensation and Linked Data", "labels": [], "entities": [{"text": "USFD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9688637256622314}, {"text": "NER", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.4861062467098236}]}], "abstractContent": [{"text": "This paper describes a pilot NER system for Twitter, comprising the USFD system entry to the W-NUT 2015 NER shared task.", "labels": [], "entities": [{"text": "USFD system", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.9047069549560547}, {"text": "W-NUT 2015 NER shared task", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6713858485221863}]}, {"text": "The goal is to correctly label entities in a tweet dataset, using an inventory often types.", "labels": [], "entities": []}, {"text": "We employ structured learning, drawing on gazetteers taken from Linked Data, and on un-supervised clustering features, and attempting to compensate for stylistic and topic drift-a key challenge in social media text.", "labels": [], "entities": []}, {"text": "Our result is competitive; we provide an analysis of the components of our methodology, and an examination of the target dataset in the context of this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media is a very challenging genre for Natural Language Processing (NLP)), providing high-volume linguistically idiosyncratic text rich in latent signals, the correct interpretation of which requires diverse contextual and author-based information.", "labels": [], "entities": []}, {"text": "Consequently, this noisy content renders NLP systems trained on more consistent, longer documents, such as newswire, mostly impotent ().", "labels": [], "entities": []}, {"text": "Suffering from a sustained dearth of annotated Twitter datasets, it maybe useful to understand what makes this genre tick, and how our existing techniques and resources can be generalised better to fit such a challenging text source.", "labels": [], "entities": []}, {"text": "This paper has focused on introducing our Named Entity Recognition (NER) entry to the WNUT evaluation challenge (, which builds on our earlier experiments with Twitter and news NER).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7490235865116119}, {"text": "WNUT evaluation challenge", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.643539826075236}]}, {"text": "In particular, we push data sources and representations, using what is know about Twitter so far to construct a model that informs our choices.", "labels": [], "entities": []}, {"text": "Specifically, we attempt to compensate for entity drift; to harness unsupervised word clustering in a principled fashion; to bring in large-scale gazetteers; to attenuate the impact of terms frequent in this text type; and to pick and choose targeted gazetteers for specific entity types.", "labels": [], "entities": [{"text": "entity drift", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.7055803835391998}, {"text": "word clustering", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7674803733825684}]}], "datasetContent": [{"text": "The training and development sets provided with the challenge were drawn from the corpus.", "labels": [], "entities": []}, {"text": "This was a set of 2394 tweets from late 2010, annotated with ten entity types, including the \"other\" type.", "labels": [], "entities": []}, {"text": "A later release in the challenge gave a set of 420 tweets from 2015, annotated in the same way (dev 2015).", "labels": [], "entities": []}, {"text": "As no other tweet corpora use this 10-class entity model, we stuck with this data for the supervised parts of our approach.", "labels": [], "entities": []}, {"text": "For language modelling, we used a set of 250 million tweets drawn from the Twitter garden hose, which is a fair 10% sample of all tweets ().", "labels": [], "entities": [{"text": "language modelling", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8086265623569489}, {"text": "Twitter garden hose", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.7741424739360809}]}, {"text": "These were reduced to just English tweets using langid.py, and then tokenized using the twokenizer tool (, which gives the same tokenization as used in the input and evaluation corpora.", "labels": [], "entities": []}, {"text": "In addition, we used three sources of gazetteers.", "labels": [], "entities": []}, {"text": "The first two were manually created, and covered named temporal expressions () and first person names).", "labels": [], "entities": []}, {"text": "The last comprised more recent data, drawn automatically from Freebase as part of a distant supervision approach to entity detection and relation annotation).", "labels": [], "entities": [{"text": "entity detection", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.7912455201148987}]}], "tableCaptions": [{"text": " Table 2: Results of the USFD W-NUT 2015 system.", "labels": [], "entities": [{"text": "USFD W-NUT 2015 system", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.9181762188673019}]}, {"text": " Table 2. As can be seen, the  best results were achieved for the person and geo-loc  entity types. It is also worth noting that performance  on the notypes task is significantly better across all met- rics, which indicates that the system is capable of iden- tifying entities correctly, but encounters issues with  their type classification.  We found that the biggest contributions to our sys- tem's performance were the Freebase gazetteer fea-", "labels": [], "entities": [{"text": "Freebase gazetteer fea", "start_pos": 423, "end_pos": 445, "type": "DATASET", "confidence": 0.9580509463946024}]}]}