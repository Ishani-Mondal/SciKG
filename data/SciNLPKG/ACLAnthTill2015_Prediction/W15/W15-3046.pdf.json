{"title": [{"text": "UPF-Cobalt Submission to WMT15 Metrics Task", "labels": [], "entities": [{"text": "UPF-Cobalt Submission", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7194412350654602}, {"text": "WMT15 Metrics", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.7808785438537598}]}], "abstractContent": [{"text": "An important limitation of automatic evaluation metrics is that, when comparing Machine Translation (MT) to a human reference , they are often unable to discriminate between acceptable variation and the differences that are indicative of MT errors.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.8268947958946228}]}, {"text": "In this paper we present UPF-Cobalt evaluation system that addresses this issue by penalizing the differences in the syntactic contexts of aligned candidate and reference words.", "labels": [], "entities": []}, {"text": "We evaluate our metric using the data from WMT workshops of the recent years and show that it performs competitively both at segment and at system levels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current automatic MT evaluation methods are grounded on the following key idea: the closer an MT is to a professional Human Translation (HT), the higher its quality.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.9409483075141907}, {"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9713529348373413}, {"text": "Human Translation (HT)", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.8320614993572235}]}, {"text": "Thus, metrics typically calculate evaluation scores based on some sort of similarity between machine and human translations.", "labels": [], "entities": []}, {"text": "The performance of evaluation systems is in its turn evaluated by calculating the correlation with human judgments.", "labels": [], "entities": []}, {"text": "Manual quality assessment can be conducted in various ways: adequacy and fluency scoring, calculating postediting cost or post-editing time, error analysis, ranking, etc.", "labels": [], "entities": []}, {"text": "In the latter case, humans are asked to compare the outputs of different MT systems and rank them in terms of quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.974395751953125}]}, {"text": "Rankingbased evaluation has gained a lot of attention in the recent years and is used in important evaluation campaigns such as the Metrics task at the Workshop on Machine Translation (WMT).", "labels": [], "entities": [{"text": "Metrics task at the Workshop on Machine Translation (WMT)", "start_pos": 132, "end_pos": 189, "type": "TASK", "confidence": 0.6717146851799705}]}, {"text": "This This work was supported by IULA (UPF) and the FI-DGR grant program of the Generalitat setting is preferred, since it has been shown to yield higher inter-annotator agreement than absolute quality assessment.", "labels": [], "entities": [{"text": "IULA (UPF)", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.7833977788686752}, {"text": "FI-DGR", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.8724411725997925}, {"text": "Generalitat setting", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.8441770076751709}]}, {"text": "In our opinion, one of the main reasons why the correlation between automatic evaluation and human rankings is still not satisfactory is that metrics' scores are not discriminative enough to approximate human comparisons.", "labels": [], "entities": []}, {"text": "Given various candidate translations of the same source sentence, all of them different from the reference, evaluation systems are often unable to determine which translation is better as they cannot tell apart candidatereference differences related to acceptable linguistic variation and the differences induced by MT errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 316, "end_pos": 318, "type": "TASK", "confidence": 0.9491702318191528}]}, {"text": "Furthermore, if all candidate translations contain a number of translation errors, metrics fail to predict the human ranking because they make no estimation of the relative importance of different types of MT errors for the overall translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 206, "end_pos": 208, "type": "TASK", "confidence": 0.9695430994033813}]}, {"text": "We suggest that the aforementioned limitations can be addressed by means of enhancing word comparison with contextual information.", "labels": [], "entities": [{"text": "word comparison", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.688549667596817}]}, {"text": "Variation between two translation options is acceptable if semantically similar words in the corresponding sentences occur in equivalent contexts.", "labels": [], "entities": []}, {"text": "In case of translation errors either the lexical choice is inappropriate or the syntactic contexts of the words are different (incorrect choice of function words, word order errors, etc.).", "labels": [], "entities": []}, {"text": "Our evaluation metric, UPF-Cobalt 1 exploits contextual information by means of weighting the contribution of each pair of lexically similar words in candidate and reference translations depending on whether they occur in similar syntactic environments.", "labels": [], "entities": []}, {"text": "Syntactic functions of the words in context are taken into consideration.", "labels": [], "entities": []}, {"text": "In this way, more fine-grained distinctions can be made regarding the relative importance of mistranslated material.", "labels": [], "entities": []}, {"text": "In this paper we present UPF-Cobalt submission to the WMT15 Metrics task.", "labels": [], "entities": [{"text": "WMT15 Metrics task", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.6256972750027975}]}, {"text": "Experiments show that UPF-Cobalt achieves competitive results, both at segment and at system levels.", "labels": [], "entities": [{"text": "UPF-Cobalt", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.6640697717666626}]}, {"text": "On WMT14 data, our metric would have been ranked as second-best performing metric at segment level, and tied with the first best-performing metric at system level.", "labels": [], "entities": [{"text": "WMT14 data", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.978156179189682}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 3 we present the experiments and analyze the results.", "labels": [], "entities": []}, {"text": "Section 4 examines relevant pieces of related work.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we give the conclusions and suggest directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments with the data from WMT13 and WMT14 Metrics tasks.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.8961873054504395}, {"text": "WMT14 Metrics tasks", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.8184639016787211}]}, {"text": "To evaluate our metric's performance at segment level, we use Kendall's Tau correlation (\u03c4 ) with human rankings, as defined in).", "labels": [], "entities": [{"text": "Kendall's Tau correlation (\u03c4 )", "start_pos": 62, "end_pos": 92, "type": "METRIC", "confidence": 0.7705551130431039}]}, {"text": "At system level, we use Pearson correlation coefficient (r).", "labels": [], "entities": [{"text": "Pearson correlation coefficient (r)", "start_pos": 24, "end_pos": 59, "type": "METRIC", "confidence": 0.8976354797681173}]}, {"text": "presents the results averaged overall into-English translation directions.", "labels": [], "entities": []}, {"text": "For the sake of comparison, we provide the results for the best performing metrics that participated in WMT13 and WMT14 Metrics tasks, as well as baseline metrics BLEU () and Meteor).", "labels": [], "entities": [{"text": "WMT13 and WMT14 Metrics tasks", "start_pos": 104, "end_pos": 133, "type": "DATASET", "confidence": 0.6884505748748779}, {"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9686283469200134}]}, {"text": "As shown in, our approach is competitive (UPF-Cobalt would have been ranked as the best performing metric on WMT13 data and as the second best on WMT14 data) and generalizes well  Context penalty.", "labels": [], "entities": [{"text": "UPF-Cobalt", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9073760509490967}, {"text": "WMT13 data", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.973937064409256}, {"text": "WMT14 data", "start_pos": 146, "end_pos": 156, "type": "DATASET", "confidence": 0.9722565114498138}]}, {"text": "To estimate the benefit of using our context penalty we substituted it with fragmentation penalty from Meteor, which explicitly penalizes differences in sequential word order.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.9065857529640198}]}, {"text": "As expected, this results in a significant drop in the correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9899595975875854}]}, {"text": "Thus, this new component is indeed crucial for our metric's performance.", "labels": [], "entities": []}, {"text": "MWA has been shown to outperform Meteor in the alignment task.", "labels": [], "entities": [{"text": "MWA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8590492010116577}, {"text": "alignment task", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.917774885892868}]}, {"text": "However, contrary to our expectations, simply using a more accurate aligner does not suffice to improve the correlation (Meteor achieves 0.354 correlation on this dataset).", "labels": [], "entities": [{"text": "correlation", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9919233918190002}, {"text": "Meteor", "start_pos": 121, "end_pos": 127, "type": "DATASET", "confidence": 0.9094606637954712}, {"text": "correlation", "start_pos": 143, "end_pos": 154, "type": "METRIC", "confidence": 0.8462363481521606}]}, {"text": "Manual inspection of the results shows that this is primarily due to the fact that MWA does not support phrase-level alignments.", "labels": [], "entities": []}, {"text": "This functionality is highly relevant for the evaluation task as it allows covering acceptable variation that involves multiword expressions.", "labels": [], "entities": []}, {"text": "We plan to integrate phrasal alignments in the metric in the future.", "labels": [], "entities": []}, {"text": "Removing this component implies a considerable decrease in the correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9728754758834839}]}, {"text": "Qualitative analysis of the results shows that its main contribution concerns cases of quasisynonyms, i.e. words that can be considered synonymous only given the similarity of their contexts.", "labels": [], "entities": []}, {"text": "The noise introduced by the component is neutralized by context penalty.", "labels": [], "entities": []}, {"text": "If unrelated words are aligned, their context penalty will be high and aligning them won't increase sentence-level evaluation score.", "labels": [], "entities": []}, {"text": "Also, in the ranking formulation of the evaluation task, distributional similarity helps to discriminate between low-quality translations.", "labels": [], "entities": []}, {"text": "That is to say, it allows distinguishing sentences where words are at least minimally related from sentences, in which, for instance, source-language words are simply left untranslated.", "labels": [], "entities": []}, {"text": "To test if giving different weights to contextual differences according to the dependency functions of the words involved, we put the values of all the weights to 1.", "labels": [], "entities": []}, {"text": "This negatively affects the results, confirming that some differences are stronger indicators of MT errors than others.", "labels": [], "entities": [{"text": "MT errors", "start_pos": 97, "end_pos": 106, "type": "TASK", "confidence": 0.7647542059421539}]}, {"text": "Thus, using the proposed weighting scheme the metric is capable of discriminating more or less serious MT errors based on the relative importance of mistranslated material.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9590166807174683}]}, {"text": "Eliminating this functionality produces a smaller decrease in the correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9490212798118591}]}, {"text": "Representing syntactic context as immediate neighbors of the word in a dependency graph allows covering a limited set of equivalent constructions, which are not frequent enough to have a significant impact on the results.", "labels": [], "entities": []}, {"text": "The framework is flexible and more complex context equivalence definitions can be integrated in the future.", "labels": [], "entities": []}, {"text": "To appreciate the advantages of the metric, Table 3 provides a qualitative comparison of UPFCobalt's performance with strong baseline metric Meteor.", "labels": [], "entities": [{"text": "UPFCobalt", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.6445820927619934}]}, {"text": "In this example, Meteor assigns low: Example of candidate and reference translations with the corresponding Meteor and UPF-Cobalt scores scores to both candidate translations, due to the differences in word order and the presence of function words absent in the reference.", "labels": [], "entities": [{"text": "Meteor and UPF-Cobalt scores", "start_pos": 108, "end_pos": 136, "type": "DATASET", "confidence": 0.7103633284568787}]}, {"text": "However, it is clear that Candidate 1 is perfectly acceptable, whereas Candidate 2 contains an error concerning the relation between the words \"voter\" and \"Obama\".", "labels": [], "entities": []}, {"text": "UPF-Cobalt correctly assigns a higher score to Candidate 1.", "labels": [], "entities": [{"text": "UPF-Cobalt", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9228766560554504}, {"text": "correctly", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.8091995120048523}]}, {"text": "Here all the content words are aligned and no context penalty is applied, since the syntactic contexts in which the words occur are equal or equivalent.", "labels": [], "entities": []}, {"text": "Thus, prep for relation in the candidate translation is equivalent to noun compound modifier relation nn in the reference and prep of label in the candidate corresponds to possession modifier poss in the reference.", "labels": [], "entities": []}, {"text": "UPF-Cobalt assigns a lower score to Candidate 2 due to the differences in the syntactic contexts of the words \"voter\" (context penalty -0.426) and \"Obama\" (context penalty -0.286), which constitute a translation error.", "labels": [], "entities": [{"text": "UPF-Cobalt", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9573462009429932}]}, {"text": "Thus, context penalty values calculated for each pair of aligned words can be used for spotting and locating translation errors.", "labels": [], "entities": [{"text": "spotting", "start_pos": 87, "end_pos": 95, "type": "TASK", "confidence": 0.972525954246521}, {"text": "locating translation errors", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6932123899459839}]}, {"text": "Qualitative analysis of the results also shows an interesting pattern in cases where UPF-Cobalt is outperformed by other metrics.", "labels": [], "entities": [{"text": "UPF-Cobalt", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.7817414999008179}]}, {"text": "This pattern is particularly relevant in the ranking evaluation setting.", "labels": [], "entities": []}, {"text": "Ref: Nevada has already completed a pilot.", "labels": [], "entities": []}, {"text": "Cand1: Nevada already has completed the pilot project.", "labels": [], "entities": [{"text": "Cand1", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9106635451316833}]}, {"text": "Cand2: Nevada has already completed the pilot project.", "labels": [], "entities": [{"text": "Cand2", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9175403714179993}]}, {"text": "When ranking translations humans intend to avoid ties whenever possible.", "labels": [], "entities": []}, {"text": "Both Candidate 1 and Candidate 2 are essentially correct, but the second translation is more adequate with regards to the norms and conventions of target language use.", "labels": [], "entities": []}, {"text": "UPF-Cobalt assigns equal scores to both MTs.", "labels": [], "entities": [{"text": "UPF-Cobalt", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8913230895996094}, {"text": "MTs", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9127534031867981}]}, {"text": "Thus, it successfully avoids penalizing acceptable differences in word order (the differences that do not affect the output of the dependency parser).", "labels": [], "entities": []}, {"text": "However, it is notable to make more finegrained distinctions regarding the fluency of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9688340425491333}]}, {"text": "This issue can be addressed by integrating target language model features in the metric.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results on WMT13 and WMT14 datasets at segment and system levels", "labels": [], "entities": [{"text": "WMT13", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8780115246772766}, {"text": "WMT14 datasets", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8930552005767822}]}, {"text": " Table 2: Ablation test results", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9910207390785217}]}, {"text": " Table 3: Example of candidate and reference translations with the corresponding Meteor and  UPF-Cobalt scores", "labels": [], "entities": [{"text": "UPF-Cobalt", "start_pos": 93, "end_pos": 103, "type": "DATASET", "confidence": 0.6305698156356812}]}]}