{"title": [], "abstractContent": [{"text": "We tackle the question: how much supervision is needed to achieve state-of-the-art performance in part-of-speech (POS) tagging, if we leverage lexical representations given by the model of Brown et al.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.613099318742752}]}, {"text": "It has become a standard practice to use automatically induced \"Brown clusters\" in place of POS tags.", "labels": [], "entities": []}, {"text": "We claim that the underlying sequence model for these clusters is particularly well-suited for capturing POS tags.", "labels": [], "entities": []}, {"text": "We empirically demonstrate this claim by drastically reducing supervision in POS tagging with these representations.", "labels": [], "entities": [{"text": "supervision", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9846209287643433}, {"text": "POS tagging", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.861858457326889}]}, {"text": "Using either the bit-string form given by the algorithm of Brown et al.", "labels": [], "entities": []}, {"text": "(1992) or the (less well-known) embedding form given by the canonical correlation analysis algorithm of Stratos et al.", "labels": [], "entities": []}, {"text": "(2014), we can obtain 93% tagging accuracy with just 400 labeled words and achieve state-of-the-art accuracy (> 97%) with less than 1 percent of the original training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9614883661270142}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9974198341369629}]}], "introductionContent": [{"text": "While fully supervised POS tagging is largely considered a solved problem today, this is hardly the case for unsupervised POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.8321942090988159}, {"text": "POS tagging", "start_pos": 122, "end_pos": 133, "type": "TASK", "confidence": 0.8430308997631073}]}, {"text": "Despite much previous work), results on this task are complicated by varying assumptions and unclear evaluation metrics (.", "labels": [], "entities": []}, {"text": "Perhaps most importantly, they are not good enough to be practical.", "labels": [], "entities": []}, {"text": "Even with indirect supervision, for example the prototype-driven method of which assumes a set of word examples for each tag type, the best perposition accuracy remains in the range of mid-70%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9894428253173828}]}, {"text": "Recent work has taken a middle ground between fully supervised and unsupervised setups by exploiting existing resources, for example by projecting POS tags from a supervised language or using tag dictionaries (.", "labels": [], "entities": []}, {"text": "In this work, we focus on minimizing the amount of labeled data required to obtain a good POS tagger.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 90, "end_pos": 100, "type": "TASK", "confidence": 0.6973682343959808}]}, {"text": "The key to our approach is the use of lexical representations induced by the clustering model of.", "labels": [], "entities": []}, {"text": "We argue that this model is particularly appropriate for representing POS tags given their nearly deterministic nature (Section 2).", "labels": [], "entities": []}, {"text": "This sheds light on why the representations derived under this model reveal the underlying POS tag information of words.", "labels": [], "entities": []}, {"text": "We empirically demonstrate the validity of our observation by using these representations to drastically reduce the number of training examples required for good POS tagging performance on English, German, and Spanish newswire datasets.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 162, "end_pos": 173, "type": "TASK", "confidence": 0.8650390207767487}]}, {"text": "For instance, on the 12-tag English dataset, we obtain tagging accuracy of 93% with just 400 labeled words.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.6542730629444122}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9762613773345947}]}, {"text": "We obtain tagging accuracy of 97.03% (about a half percent behind fully supervised models) with just 0.74% of the original training data.", "labels": [], "entities": [{"text": "tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.930939793586731}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9667379856109619}]}, {"text": "Our aim is orthogonal to the discussion in Manning (2011) who investigates what is needed to go beyond the current state-of-the-art POS tagging performance.", "labels": [], "entities": [{"text": "POS tagging performance", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.7326799432436625}]}, {"text": "Our focus is on reaching that performance with as little supervision as possible.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Dev performance of MINITAGGER when only 200, 400, and 1000 labeled words are used.", "labels": [], "entities": [{"text": "MINITAGGER", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.45380711555480957}]}, {"text": " Table 4: (a) Smallest numbers of actively selected labels required by MINITAGGER to reach the target dev performance:  92-97%. The target performance is defined to be the accuracy of the fully supervised baseline rounded down to a whole  number", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9989389777183533}]}, {"text": " Table 5: Performance of MINITAGGER (MINI in short) and CRF on the dev and test portions. The Data column  specifies the amount of training data: \"all\" means all training data is used, \"active\" means only the labeled examples  actively selected (with the same CCA features) in", "labels": [], "entities": [{"text": "CRF", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.979828417301178}]}]}