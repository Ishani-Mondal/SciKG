{"title": [{"text": "NAVER Machine Translation System for WAT 2015", "labels": [], "entities": [{"text": "NAVER Machine Translation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7019090453783671}, {"text": "WAT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9831830263137817}]}], "abstractContent": [{"text": "In this paper, we describe NAVER machine translation system for English to Japanese and Korean to Japanese tasks at WAT 2015.", "labels": [], "entities": [{"text": "NAVER machine translation", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6588396231333414}, {"text": "WAT 2015", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.8328088223934174}]}, {"text": "We combine the traditional SMT and neural MT in both tasks.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9857814908027649}]}], "introductionContent": [{"text": "This paper explains the NAVER machine translation system for the 2nd Workshop on Asian Translation (WAT 2015) ().", "labels": [], "entities": [{"text": "NAVER machine translation", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.7235737641652426}, {"text": "2nd Workshop on Asian Translation (WAT 2015)", "start_pos": 65, "end_pos": 109, "type": "TASK", "confidence": 0.6596391863293118}]}, {"text": "We participate in two tasks; English to Japanese (EnJa) and Korean to Japanese (Ko-Ja).", "labels": [], "entities": []}, {"text": "Our system is a combined system of traditional statistical machine translation (SMT) and neural machine translation (NMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.7835340847571691}, {"text": "neural machine translation (NMT", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.7600268721580505}]}, {"text": "We adopt the tree-tostring syntax-based model as En-Ja SMT baseline, while we adopt the phrase-based model as Ko-Ja.", "labels": [], "entities": []}, {"text": "We propose improved SMT systems for each task and an NMT model based on the architecture using recurrent neural network (RNN) ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9928036332130432}]}, {"text": "We give detailed explanations of each SMT system in section 2 and section 3.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9949867725372314}]}, {"text": "We describe our NMT model in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "All scores of this section are reported in experiments on the official test data; test.txt of the ASPEC-JE corpus.", "labels": [], "entities": [{"text": "ASPEC-JE corpus", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.9234419763088226}]}, {"text": "shows the evaluation results of our En-Ja traditional SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.6763721108436584}]}, {"text": "The first row in the table indicates the baseline of the tree-to-string systaxbased model.", "labels": [], "entities": []}, {"text": "The second row shows the system that reflects the tree modification described in section 2.3.", "labels": [], "entities": []}, {"text": "The augmentation method drastically increased both the number of rules and the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9817246496677399}]}, {"text": "Our OOV handling methods described in  The decoding time of the rule-augmented treeto-string SMT is about 1.3 seconds per a sentence in our 12-core machine.", "labels": [], "entities": [{"text": "OOV handling", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.662241205573082}, {"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.725579559803009}]}, {"text": "Even though it is not a terrible problem, we are required to improve the decoding speed by pruning the rule table or using the incremental decoding method.", "labels": [], "entities": []}, {"text": "shows the evaluation results of our Ko-Ja traditional SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9113435745239258}]}, {"text": "We obtained the best result in the combination of two phrase-based SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.8601402640342712}]}, {"text": "shows effects of our NMT model.", "labels": [], "entities": []}, {"text": "\"Human\" indicates the pairwise crowdsourcing evaluation scores provided by WAT 2015 organizers.", "labels": [], "entities": [{"text": "WAT 2015 organizers", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.7111382484436035}]}, {"text": "In the table, \"T2S/PBMT only\" is the final T2S/PBMT systems shown in section 5.1 and section 5.2.", "labels": [], "entities": []}, {"text": "\"NMT only\" is the system using only RNN encoder-decoder without any traditional SMT methods.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.979520320892334}]}, {"text": "The last row is the combined system that reranks T2S/PBMT n-best translations by NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9542655944824219}]}, {"text": "Our T2S/PBMT system outputs 100,000-best translations in En-Ja and 10,000-best translations in Ko-Ja.", "labels": [], "entities": []}, {"text": "The final output is 1-best translation selected by considering only NMT score.", "labels": [], "entities": [{"text": "NMT score", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.7010791003704071}]}], "tableCaptions": [{"text": " Table 3: Effect of NMT.", "labels": [], "entities": []}]}