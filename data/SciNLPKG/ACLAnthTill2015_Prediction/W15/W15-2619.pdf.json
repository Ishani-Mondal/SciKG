{"title": [{"text": "Mining and Ranking Biomedical Synonym Candidates from Wikipedia", "labels": [], "entities": [{"text": "Ranking Biomedical Synonym", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.5655064185460409}, {"text": "Wikipedia", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.6533379554748535}]}], "abstractContent": [{"text": "Biomedical synonyms are important resources for Natural Language Processing in Biomedi-cal domain.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6481561958789825}]}, {"text": "Existing synonym resources (e.g., the UMLS) are not complete.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.886926531791687}]}, {"text": "Manual efforts for expanding and enriching these resources are prohibitively expensive.", "labels": [], "entities": []}, {"text": "We therefore develop and evaluate approaches for automated synonym extraction from Wikipedia.", "labels": [], "entities": [{"text": "synonym extraction", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8425898253917694}]}, {"text": "Using the inter-wiki links, we extracted the candidate synonyms (anchor-text e.g., \"increased thirst\") in a Wikipedia page and the title (e.g., \"polyuria\") of its corresponding linked page.", "labels": [], "entities": []}, {"text": "We rank synonym candidates with word embedding and pseudo-relevance feedback (PRF).", "labels": [], "entities": [{"text": "pseudo-relevance feedback (PRF)", "start_pos": 51, "end_pos": 82, "type": "METRIC", "confidence": 0.7326861500740052}]}, {"text": "Our results show that PRF-based re-ranking outperformed word embedding based approach and a strong baseline using inter-wiki link frequency.", "labels": [], "entities": []}, {"text": "A hybrid method, Rank Score Combination, achieved the best results.", "labels": [], "entities": [{"text": "Rank Score Combination", "start_pos": 17, "end_pos": 39, "type": "METRIC", "confidence": 0.8674717545509338}]}, {"text": "Our analysis also suggests that medical synonyms mined from Wikipedia can increase the coverage of existing synonym resources such as UMLS.", "labels": [], "entities": []}], "introductionContent": [{"text": "Biomedical synonym resources have been an important part of biomedical natural language processing (NLP).", "labels": [], "entities": [{"text": "biomedical natural language processing (NLP)", "start_pos": 60, "end_pos": 104, "type": "TASK", "confidence": 0.7831431542124067}]}, {"text": "Synonym resources have been used fora variety of tasks such as query expansion), reformulation (, and word sense disambiguation.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7373275011777878}, {"text": "word sense disambiguation", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.6889332334200541}]}, {"text": "Another important avenue of their use lies in eportals for clinical notes such as My HealtheVet patient portal, which allows patients to access clinical notes written by their healthcare providers.", "labels": [], "entities": [{"text": "My HealtheVet patient portal", "start_pos": 82, "end_pos": 110, "type": "DATASET", "confidence": 0.9089292734861374}]}, {"text": "While many organizations have been embracing these methods of patient-clinician communication, various studies ( have shown that patients often have difficulty in comprehending clinical notes.", "labels": [], "entities": []}, {"text": "A patient's ability to comprehend clinical notes is directly related to his/her ability to understand medical jargon).", "labels": [], "entities": []}, {"text": "Subsequently approaches have been developed to replace medical jargon with corresponding lay terms ().", "labels": [], "entities": []}, {"text": "Such approaches rely on high quality synonym resource(s).", "labels": [], "entities": []}, {"text": "The widely used biomedical knowledge resource, Unified Medical Language System (UMLS)) is a very valuable resource for such purposes.", "labels": [], "entities": []}, {"text": "The UMLS incorporates over 100 biomedical terminology resources including Consumer Health Vocabulary (CHV).", "labels": [], "entities": []}, {"text": "It also contains definitions for medical terms which can be used to simplify the clinical notes (.", "labels": [], "entities": []}, {"text": "Even though UMLS is a rich resource with avast quantity of medical terms, we found that several synonymous or related medical terms that we extracted through Wikipedia, were not present in the UMLS dictionaries.", "labels": [], "entities": [{"text": "UMLS dictionaries", "start_pos": 193, "end_pos": 210, "type": "DATASET", "confidence": 0.9204273223876953}]}, {"text": "We report this coverage in Section In this paper, we propose a data-driven approach for automatic extraction and ranking of medical synonyms from Wikipedia.", "labels": [], "entities": [{"text": "automatic extraction and ranking of medical synonyms from Wikipedia", "start_pos": 88, "end_pos": 155, "type": "TASK", "confidence": 0.811560783121321}]}, {"text": "Wikipedia is a free-access, free-content collaborative online encyclopedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.917029619216919}]}, {"text": "Our previous work suggests that about 40% content in Wikipedia contain health related information (.", "labels": [], "entities": []}, {"text": "Many studies have shown that Wikipedia contains high quality of biomedical content).", "labels": [], "entities": []}, {"text": "For example, evaluated that Wikipedia contains highly accurate medical articles.", "labels": [], "entities": []}, {"text": "They do however, also mention that some articles contain incomplete medical content.", "labels": [], "entities": []}, {"text": "concluded that Wikipedia has similar accuracy and depth as a professionally edited database.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9992656111717224}, {"text": "depth", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9800991415977478}]}, {"text": "Similarly, showed that Wikipedia contains high quality information on mental disorders.", "labels": [], "entities": []}, {"text": "As the result, Wikipedia is being increasingly used by healthcare providers.", "labels": [], "entities": []}, {"text": "Specifically, studies show that Wikipedia is widely used by junior physicians () and pharmacists.", "labels": [], "entities": []}, {"text": "Additionally Wikipedia is also being widely used by people who are looking for healthcare information.", "labels": [], "entities": []}, {"text": "Based on the search engine ranking and page view statistics, concluded that English Wikipedia is major source of health related information for online users.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.8914813995361328}]}, {"text": "Since Wikipedia is written collaboratively by anonymous volunteers, a majority of whom are laypeople, its content contains both biomedical jargons and lay terms.", "labels": [], "entities": []}, {"text": "This makes Wikipedia a rich resource linking medical jargon with synonymous lay phrases.", "labels": [], "entities": []}, {"text": "We leverage this resource by extracting inter-wiki links from Wikipedia to obtain (page title, anchor text) pairs.", "labels": [], "entities": []}, {"text": "A typical Wikipedia page includes a title and a description text in which anchor texts are linked (through interwiki links) to other Wikipedia pages.", "labels": [], "entities": []}, {"text": "As illustrated in, one of the anchor texts in the \"Diabetes mellitus\" Wikipedia page, \"increased thirst\" is linked to the corresponding page with the title term \"polydipsia\".", "labels": [], "entities": []}, {"text": "We treat the anchor text as a synonym candidate for the title term, which we treat as target concept.", "labels": [], "entities": []}, {"text": "Synonym candidates and their target concepts extracted from inter-wiki links are often synonymous pairs.", "labels": [], "entities": []}, {"text": "For example, the anchor texts \"frequent urination,\" \"increased thirst,\" and \"increased hunger\" are linked to the title pages of \"polyuria,\" \"polydipsia,\" and \"polyphagia\", respectively.", "labels": [], "entities": []}, {"text": "However, sometimes, the synonym candidates and their target concepts are only related but not synonymous.", "labels": [], "entities": []}, {"text": "For example \"nonketotic hyperosmolar coma\" and \"kidney failure\" are linked to the \"hyperosmolar hyperglycemic state\" and \"chronic kidney disease\" respectively.", "labels": [], "entities": []}, {"text": "In addition, as a crowdsourcing resource, Wikipedia has noise.", "labels": [], "entities": []}, {"text": "A typical case is where the interwiki links are tagged partially.", "labels": [], "entities": []}, {"text": "For example, only the \"attack\" in \"heart attack\" maybe linked to \"Myocardial Infarction\".", "labels": [], "entities": []}, {"text": "To improve the quality of synonym extraction, we explored several unsupervised methods to rank the synonymous pairs, which utilize distributed word representation (i.e., word embeddings), pseudo relevance feedback (PRF) based re-ranking, and ranking combinations.", "labels": [], "entities": [{"text": "synonym extraction", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.9115910828113556}]}, {"text": "To our knowledge, this is the first effort that uses word embedding-based ranking and PRF to improve synonym extraction from Wikipedia.", "labels": [], "entities": [{"text": "synonym extraction", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.8629951477050781}]}, {"text": "We compared our methods with a strong baseline method which uses entity-link frequency.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted all the (target concept, synonym candidate) pairs from Wikipedia except the pairs that contain special characters or numbers.", "labels": [], "entities": []}, {"text": "In total, we obtained 24M links, with 3.6M unique links for 1.6M distinct concepts.", "labels": [], "entities": []}, {"text": "shows the distribution of the number of synonym candidates extracted for each title term from the Wikipedia.", "labels": [], "entities": []}, {"text": "Out of the total 1,659,049 title-terms, 1,457,935 terms have less than three synonym candidates.", "labels": [], "entities": []}, {"text": "Our preliminary study suggests that many of these terms are person or location names, which are not of our interest.", "labels": [], "entities": []}, {"text": "Therefore, we did not include these terms when creating our gold-standard evaluation dataset and only evaluated our methods on terms with three or more synonym candidates.", "labels": [], "entities": []}, {"text": "We use word2vec software to create the Skip Gram word embeddings.", "labels": [], "entities": [{"text": "Skip Gram word embeddings", "start_pos": 39, "end_pos": 64, "type": "DATASET", "confidence": 0.9571574181318283}]}, {"text": "The word embeddings were trained on a combined text corpus of English Wikipedia, Simple English Wikipedia and articles from PubMed Open Access, which contain over 4 billion words in total.", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 81, "end_pos": 105, "type": "DATASET", "confidence": 0.8620210687319437}]}, {"text": "The text was lowercased and stripped of all punctuations except comma, apostrophe and period.", "labels": [], "entities": []}, {"text": "We set our word2vec training parameters based on the study of.", "labels": [], "entities": []}, {"text": "Specifically, we used 200-dimension vectors with a window size of 6.", "labels": [], "entities": []}, {"text": "We used hierarchical soft-max with a subsampling threshold of 0.001 for training.", "labels": [], "entities": []}, {"text": "There is no lexical resource suitable for evaluating our task performance.", "labels": [], "entities": []}, {"text": "Even UMLS does not coverall the synonyms and related terms we discovered from the Wikipedia.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.865715742111206}]}, {"text": "To evaluate our synonym ranking methods, we created a gold standard evaluation dataset from the Wikipedia data we extracted.", "labels": [], "entities": [{"text": "synonym ranking", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.8771662712097168}, {"text": "Wikipedia data", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.8879265189170837}]}, {"text": "Since the goal of this work is to extract synonym candidates for medical terms, we only chose medically relevant concepts for evaluation.", "labels": [], "entities": []}, {"text": "We randomly selected 4000 terms from the concepts (title terms) that are present either in the Consumer Health Vocabulary or in the Wikipedia Health Category tree to the depth of 4.", "labels": [], "entities": [{"text": "Wikipedia Health Category tree", "start_pos": 132, "end_pos": 162, "type": "DATASET", "confidence": 0.8907376229763031}]}, {"text": "An annotator with PhD degree in Biology further selected 1000 relevant medical terms from these 4000 terms.", "labels": [], "entities": []}, {"text": "We built an annotation GUI that presented to the annotators 1000 medical terms and their synonym candidates.", "labels": [], "entities": []}, {"text": "Each term and its synonym candidates were shown in a single annotation page.", "labels": [], "entities": []}, {"text": "The page order was randomized.", "labels": [], "entities": []}, {"text": "The annotation task was to judge whether the synonym candidate was a \"Synonym\", \"Related Term\" or \"Rejected or Unrelated Term\" of the target concept.", "labels": [], "entities": []}, {"text": "Two annotators conducted the annotation.", "labels": [], "entities": []}, {"text": "Both are premedical school students.", "labels": [], "entities": []}, {"text": "So far, 792 unique medical concepts were annotated, out of which 256 were annotated by both.", "labels": [], "entities": []}, {"text": "We used these 256 concepts for our evaluation.", "labels": [], "entities": []}, {"text": "We also used the entire 792 concepts and their synonyms to calculate the coverage by UMLS.", "labels": [], "entities": [{"text": "coverage", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9049293994903564}, {"text": "UMLS", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9584789872169495}]}, {"text": "A synonym candidate is defined as a \"Synonym\" if it has the exact same meaning as the target concept.", "labels": [], "entities": []}, {"text": "It is defined as a \"Related Term\" if it has a related meaning to the target concept.", "labels": [], "entities": []}, {"text": "We accept hypernyms, hyponyms, and words derived from the same root as \"Related terms\".", "labels": [], "entities": []}, {"text": "Additionally we also accept words with high correlations to the target concept, e.g., a very common symptom fora disease.", "labels": [], "entities": []}, {"text": "As an example, \"high blood sugar\" is a related term of \"diabetes mellitus\".", "labels": [], "entities": []}, {"text": "Candidates not in the above-mentioned categories were annotated as \"Unrelated or Rejected Terms\".", "labels": [], "entities": []}, {"text": "The gold standard of 256 concepts consists of 1507 (title term, synonym candidate) pairs and their corresponding annotations.", "labels": [], "entities": []}, {"text": "The linear weighted kappa for the inter-annotator agreement was 0.4762, with the 95% confidence interval ranges from 0.4413 to 0.5111.", "labels": [], "entities": []}, {"text": "This kappa value suggests that the annotators have moderate agreement ().", "labels": [], "entities": []}, {"text": "If we combine related terms and rejected terms into one category, the resulting dataset has a much higher kappa of 0.6250.", "labels": [], "entities": [{"text": "kappa", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9898087978363037}]}, {"text": "This contrasts with a low kappa of 0.3929 when related terms are instead combined with synonyms, suggesting that more annotator uncertainty lies in the boundary between related and rejected terms than between related and synonymous terms.", "labels": [], "entities": [{"text": "kappa", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9928736686706543}]}, {"text": "We use mean average precision (MAP) to evaluate the performances of our ranking methods, because our problem is similar to atypical Information Retrieval tasks.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 7, "end_pos": 35, "type": "METRIC", "confidence": 0.9313105245431265}, {"text": "Information Retrieval tasks", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.7954999903837839}]}, {"text": "Instead of using a set of relevant and irrelevant documents to evaluate our ranking output, we use a set of synonyms, related terms and rejected terms from our gold-standard annotation for evaluation.", "labels": [], "entities": []}, {"text": "We set two evaluation conditions: (1) combining the synonyms and related terms from the goldstandard annotation to form the set of relevant (positive) instances and treating rejected terms as irrelevant (negative) instances; and (2) using the synonyms from the gold annotation as positive (relevant) instances and treating the related and rejected terms as irrelevant (negative) instances.", "labels": [], "entities": []}, {"text": "By the above definition, condition 1 is a relaxed condition and condition 2 is strict.", "labels": [], "entities": []}, {"text": "For both conditions, only terms that were judged by both annotators as relevant (positive) instances are treated as positive.", "labels": [], "entities": []}, {"text": "We compute MAP by Equations (2) and (3).", "labels": [], "entities": [{"text": "MAP", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.7538332343101501}, {"text": "Equations", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.961821973323822}]}, {"text": "(3) where AveP is the average precision of a query (target concept in our case); k is the rank of the synonym candidates; P(k) is the precision of the ranking at rank k; \u2206 \u00ed \u00b5\u00ed\u00b1\u009f (\u00ed \u00b5\u00ed\u00b1\u0098) is the increase of recall of the ranking at rank k compared with the recall at rank k-1; MAP is the mean AveP of all the target concepts to evaluate on.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9268316626548767}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9977224469184875}, {"text": "recall", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.9954570531845093}, {"text": "AveP", "start_pos": 293, "end_pos": 297, "type": "METRIC", "confidence": 0.9636078476905823}]}], "tableCaptions": [{"text": " Table 1: Mean Average Precision values for Rele- vance Feedback of 5", "labels": [], "entities": [{"text": "Mean Average", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9378296732902527}, {"text": "Precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.5400488972663879}, {"text": "Rele- vance Feedback", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.8042996227741241}]}, {"text": " Table 2: Predictions for \"septicemia\"", "labels": [], "entities": []}]}