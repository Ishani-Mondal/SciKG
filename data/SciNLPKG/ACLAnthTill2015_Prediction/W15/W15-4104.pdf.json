{"title": [{"text": "Establishing sentential structure via realignments from small parallel corpora", "labels": [], "entities": [{"text": "Establishing sentential structure", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8628696997960409}]}], "abstractContent": [{"text": "The present article reports on efforts to improve the translation accuracy of a corpus-based hybrid MT system developed using the PRESEMT methodology.", "labels": [], "entities": [{"text": "translation", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9548450708389282}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9001718759536743}, {"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9266075491905212}]}, {"text": "This methodology operates on a phrasal basis, where phrases are linguistically-motivated but are automatically determined via a dedicated module.", "labels": [], "entities": []}, {"text": "Here, emphasis is placed on improving the structure of each translated sentence, by replacing the Example Based MT approach originally used in PRESEMT with a sub-sentential approach.", "labels": [], "entities": []}, {"text": "Results indicate that an improved accuracy can be achieved, as measured by objective metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9994742274284363}]}], "introductionContent": [{"text": "In the present article, a corpus-based methodology is studied, which allows the creation of MT systems fora variety of languages using a common set of software modules.", "labels": [], "entities": [{"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9785082936286926}]}, {"text": "This methodology has been specifically designed to address the scarcity of parallel corpora needed to train for instance a Statistical Machine Translation system, in particular for less widelyresourced languages.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 123, "end_pos": 154, "type": "TASK", "confidence": 0.7610510389010111}]}, {"text": "Thus the main source of information is a large collection of monolingual corpora in the target language (TL).", "labels": [], "entities": []}, {"text": "This collection is supplemented by a small parallel corpus of no more than a few hundred sentences, which the methodology employs to extract information about the structural transfer from the source language (SL) to the target one.", "labels": [], "entities": []}, {"text": "The aim in the present article is to investigate how the translation quality can be improved over the best results reported so far ( ).", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.942581295967102}]}, {"text": "Emphasis is placed on extracting the salient information from the small parallel corpus, to most accurately define the structure of the sentences being translated.", "labels": [], "entities": []}, {"text": "The efficacy of this effort is verified by a set of experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the present article, the Greek-to-English language pair is used for experimentation.", "labels": [], "entities": []}, {"text": "To ensure compatibility with earlier results, the standard language resources of PRESEMT are used, including the basic parallel corpus of 200 sentences and the two test sets of 200 sentences each, denoted as testsetA and testsetB (all these resources have been retrieved from the www.presemt.eu website).", "labels": [], "entities": [{"text": "PRESEMT", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.8020095229148865}]}, {"text": "Regarding the parameters related to the realignment templates, the value used for freq_thres is 0.50, while min_freq is set to 3.", "labels": [], "entities": []}, {"text": "Finally, parameter \ud97b\udf59 \ud97b\udf59 of eqn is set to 100 for the given experiments, indicating a strong preference to larger realignment templates.", "labels": [], "entities": [{"text": "eqn", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8664003610610962}]}, {"text": "These parameter values have been chosen by performing trial simulations during the development phase.", "labels": [], "entities": []}, {"text": "Different PMG modules resulting in different phrase sizes have been studied to investigate alternative SL phrasing schemes applied on the sentences to be translated.", "labels": [], "entities": []}, {"text": "This testis performed, to determine whether the proposed realignment method is robust.", "labels": [], "entities": []}, {"text": "Comparative evaluation with a selection of PMG modules with different phrase sizes can indicate the effectiveness of realignment templates in this MT methodology.", "labels": [], "entities": [{"text": "MT", "start_pos": 147, "end_pos": 149, "type": "TASK", "confidence": 0.9938648343086243}]}, {"text": "Experiments are performed by considering or not the context (cases: Align-nC, Align-C) or by varying Thus an infrequent realignment cannot be relied upon to provide structure-defining information.", "labels": [], "entities": []}, {"text": "the type of match when Align-C is applied (cases: Align-C0, Align-C1, Align-C2).", "labels": [], "entities": []}, {"text": "The best realignments have been compared to the baseline i.e. the case when the classic Structure selection algorithm is used ( . Regarding the PMG modules, the first version, termed PMG-s gives the highest reported translation accuracy, splitting sentences into smaller phrases 2 . The alternative PMG (PMG-b) evaluated, favours larger phrases than PMG-s and results in smaller average sentence lengths expressed in terms of phrases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 228, "end_pos": 236, "type": "METRIC", "confidence": 0.8739481568336487}]}, {"text": "The average sentence sizes for each phrasing scheme in both testsets can be seen in, while the numbers of realignment templates applied to the input sets for testsets A and B are detailed in.", "labels": [], "entities": []}, {"text": "The difference in realignments between the two testsets reflects the fact that TestsetA has smaller sentences of on average 15.3 words per sentence, while for TestsetB this is 22.6 words (the sentence size being increased by 48% in terms of words).", "labels": [], "entities": [{"text": "TestsetA", "start_pos": 79, "end_pos": 87, "type": "DATASET", "confidence": 0.9347007870674133}, {"text": "TestsetB", "start_pos": 159, "end_pos": 167, "type": "DATASET", "confidence": 0.9022572040557861}]}, {"text": "Hence, the occurrence of realignments is higher for TestsetB.", "labels": [], "entities": [{"text": "occurrence", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9904554486274719}, {"text": "TestsetB", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9824692010879517}]}, {"text": "MT setups are evaluated regarding the translation quality, based on a selection of widely-used MT metrics: BLEU (), NIST),) and TER ().", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9613451361656189}, {"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9987354874610901}, {"text": "NIST", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.7121177911758423}, {"text": "TER", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9959933757781982}]}, {"text": "For BLEU, NIST and Meteor, the score measures the translation accuracy and a higher score indicates a better translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9313775300979614}, {"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8943679332733154}, {"text": "Meteor", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.8928903341293335}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9691401720046997}]}, {"text": "For TER the score counts the error rate and thus a lower score indicates a more successful translation.", "labels": [], "entities": [{"text": "TER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9711067080497742}, {"text": "error rate", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9727285802364349}]}, {"text": "For reasons of uniformity, when comparing scores, an improvement in a metric is depicted as a positive change (for all metrics, including TER).", "labels": [], "entities": [{"text": "TER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9971237778663635}]}, {"text": "indicates how translation quality is improved when the best realignment case (AlignnC) is applied compared to the baseline, for different PMGs, using TestsetA.", "labels": [], "entities": [{"text": "AlignnC)", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9538505971431732}, {"text": "TestsetA", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.9142724871635437}]}, {"text": "The use of realignments improves metric scores in both PMGs, indicating the improved robustness of the MT system towards this choice.", "labels": [], "entities": [{"text": "metric scores", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9753326177597046}, {"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9511452913284302}]}, {"text": "The highest improvement of 1.63% observed for the BLEU score is obtained with PMG-s, which leads to sentences of larger length (with more phrases but of fewer words each).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.971345067024231}, {"text": "PMG-s", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9368165731430054}]}, {"text": "When applying the best realignment variant (Align-nC) to TestsetB with the two phrasing schemes (i.e. PMG-s, PMG-b) a substantial improvement is achieved, reaching 1.12% for BLEU (cf.).", "labels": [], "entities": [{"text": "TestsetB", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9764633178710938}, {"text": "BLEU", "start_pos": 174, "end_pos": 178, "type": "METRIC", "confidence": 0.9968173503875732}]}, {"text": "As before, PMG-s achieves the greatest improvement, showing that the realignment template algorithm benefits to a greater degree phrasing schemes that generate larger numbers of phrases per sentence.", "labels": [], "entities": []}, {"text": "A further evaluation effort has involved examining how the proposed realignment template method compares to a zero-baseline, where the SL structure is retained without change in TL.", "labels": [], "entities": [{"text": "TL", "start_pos": 178, "end_pos": 180, "type": "METRIC", "confidence": 0.9550132155418396}]}, {"text": "In this case, the improvement amounts to 0.53% in terms of the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9743732511997223}]}, {"text": "To compare against another benchmark, TestSetA was translated with a MOSES-based SMT (trained with a parallel corpus of approx. 1.2 million sentences -the parallel corpus is 4 orders of magnitude larger than that used by PRESEMT) and resulted in BLEU and NIST scores of 0.3795 and 7.039 respectively.", "labels": [], "entities": [{"text": "MOSES-based SMT", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.4496363252401352}, {"text": "BLEU", "start_pos": 246, "end_pos": 250, "type": "METRIC", "confidence": 0.9988987445831299}, {"text": "NIST", "start_pos": 255, "end_pos": 259, "type": "DATASET", "confidence": 0.5369060039520264}]}, {"text": "These MOSES scores are comparable to the scores achieved by PRESEMT with Align-nC (0.3626 and 7.086 for BLEU and NIST respectively).", "labels": [], "entities": [{"text": "MOSES", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.971358597278595}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9606132507324219}, {"text": "NIST", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.9211459159851074}]}], "tableCaptions": [{"text": " Table 2: Average sentence sizes in terms of  phrases for the two evaluation testsets when dif- ferent PMG modules are applied.", "labels": [], "entities": []}, {"text": " Table 3: Total number of realignments recorded  per testset, for different realignment variations.", "labels": [], "entities": [{"text": "Total number", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9367527365684509}]}]}