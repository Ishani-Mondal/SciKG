{"title": [{"text": "Using Topic Modeling and Similarity Thresholds to Detect Events", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a Retrospective Event Detection algorithm, called Eventy-Topic Detection (ETD), which automatically generates topics that describe events in a large, temporal text corpus.", "labels": [], "entities": [{"text": "Retrospective Event Detection", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6942313512166342}]}, {"text": "Our approach leverages the structure of the topic modeling framework, specifically the Latent Dirichlet Allocation (LDA), to generate topics which are then later labeled as Eventy-Topics or non-Eventy-Topics.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 87, "end_pos": 119, "type": "METRIC", "confidence": 0.8637667894363403}]}, {"text": "The system first runs daily LDA topic models, then calculates the cosine similarity between the topics of the daily topic models, and then runs our novel Bump-Detection algorithm.", "labels": [], "entities": []}, {"text": "Similar topics labeled as an Eventy-Topic are then grouped together.", "labels": [], "entities": []}, {"text": "The algorithm is demonstrated on two Terabyte sized corpuses-a Reuters News corpus and a Twitter corpus.", "labels": [], "entities": [{"text": "Terabyte sized corpuses-a Reuters News corpus", "start_pos": 37, "end_pos": 82, "type": "DATASET", "confidence": 0.6742089639107386}]}, {"text": "Our method is evaluated on a human annotated test set.", "labels": [], "entities": []}, {"text": "Our algorithm demonstrates its ability to accurately describe and label events in a temporal text corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vast amounts of research has been developed to help organize, search, index, browse and understand the immense number of electronic documents.", "labels": [], "entities": []}, {"text": "Topic models have emerged as a powerful technique to discover patterns of words that reflect the underlying topics that are combined to form documents.", "labels": [], "entities": []}, {"text": "Latent Dirichlet Allocation () defines topics as multinomial distributions over words, and documents as multinomial distributions over these topics.", "labels": [], "entities": []}, {"text": "LDA uses Dirichlet priors for both the documenttopic and topic-word distributions.", "labels": [], "entities": [{"text": "LDA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8572394847869873}]}, {"text": "Topic Detection and Tracking(TDT) is an area of research that was prominent in the 1990's ().", "labels": [], "entities": [{"text": "Topic Detection and Tracking(TDT)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9001493368829999}]}, {"text": "The goal of TDT is to detect the appearance of new topics and track their evolution overtime.", "labels": [], "entities": [{"text": "TDT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.77804034948349}]}, {"text": "Specifically relevant to our paper is the task of Retrospective Event Detection.", "labels": [], "entities": [{"text": "Retrospective Event Detection", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.9226075410842896}]}, {"text": "It is defined as the task of identifying all events in a corpus of stories.", "labels": [], "entities": [{"text": "identifying all events in a corpus of stories", "start_pos": 29, "end_pos": 74, "type": "TASK", "confidence": 0.755307987332344}]}, {"text": "In our Eventy-Topic Detection (ETD) algorithm we wish to leverage the powerful structure of topic models in the Retrospective Event Detection task.", "labels": [], "entities": [{"text": "Eventy-Topic Detection (ETD)", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.7936322271823884}, {"text": "Retrospective Event Detection task", "start_pos": 112, "end_pos": 146, "type": "TASK", "confidence": 0.8695792108774185}]}, {"text": "In particular, we develop an algorithm that is capable of identifying Eventy-Topics in a sequentially ordered, massive 'Big Data' sized corpus.", "labels": [], "entities": []}, {"text": "We define an Eventy-Topic to be a topic that solely describes a specific, time sensitive news event.", "labels": [], "entities": []}, {"text": "A topic that is consistently and persistently in the news is not an Eventy-Topic.", "labels": [], "entities": []}, {"text": "We run daily LDA topic models, then calculate the cosine similarities between the topics in all the models.", "labels": [], "entities": []}, {"text": "Eventy-Topics contain a noticeable spike around the date of the event in these cosine similarity graphs.", "labels": [], "entities": []}, {"text": "To detect these spikes, we smooth the cosine similarity values so that the bump has a monotonically increasing section, followed by a plateau, followed by a monotonically decreasing section.", "labels": [], "entities": []}, {"text": "We then then run a novel algorithm called Bump Detection that searches for these properties.", "labels": [], "entities": [{"text": "Bump Detection", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.751860111951828}]}, {"text": "Given a time-stamped corpus, our goal is to automatically detect and describe all of these EventyTopics.", "labels": [], "entities": []}, {"text": "Our algorithm is capable of detecting onetime (uni-modal) Eventy-Topics, such as \"Robin Williams Death\", as well as multi-time (multimodal) related Eventy-Topics, such as \"The Masters Golf Tournament\".", "labels": [], "entities": [{"text": "Robin Williams Death\"", "start_pos": 82, "end_pos": 103, "type": "DATASET", "confidence": 0.6982104331254959}, {"text": "The Masters Golf Tournament\"", "start_pos": 172, "end_pos": 200, "type": "DATASET", "confidence": 0.7888463735580444}]}], "datasetContent": [{"text": "Evaluation of our ETD algorithm was done by annotating a selected set of topics.", "labels": [], "entities": []}, {"text": "To expedite and strengthen the annotation process we first ran Bump Detection with a relatively low 1 minColdHotDiffThresh and then again with this parameter set to a relatively high 2 value.", "labels": [], "entities": [{"text": "Bump Detection", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.5074832439422607}]}, {"text": "The sampling for our annotation set was then divided into 3 strata.", "labels": [], "entities": []}, {"text": "\u2022 Strata I: topics that were not labeled as EventyTopics with a low minColdHotDiffTresh.", "labels": [], "entities": []}, {"text": "\u2022 Strata II: topics that were labeled as Eventy-Topics with a low, but not a high minColdHotDiffTresh.", "labels": [], "entities": []}, {"text": "\u2022 Strata III: topics that were labeled as EventyTopics with a high minColdHotDiffTresh.", "labels": [], "entities": []}, {"text": "The details of our sampling for annotation can be seen in.", "labels": [], "entities": []}, {"text": "Note that the annotation was done on topics and not on the results of the Event Grouping step.", "labels": [], "entities": [{"text": "Event Grouping step", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7113112012545267}]}, {"text": "Our annotation set consisted of randomly sampled 84 topics from Strata I, 11 topics from Strata II, and 22 topics from Strata III.", "labels": [], "entities": []}, {"text": "The vast majority of topics fell into Strata I, with the second most in Strata II, and the rest in Strata III (579).", "labels": [], "entities": []}, {"text": "The reason for dividing the sampled topics into different strata is because the annotation of our Eventy-Topic detection was different in each of these 3 Strata.", "labels": [], "entities": []}, {"text": "80/84 topics in Strata I were labeled as 'Non-Eventy-Topics', while 21/22 topics in Strata III were labeled as 'Eventy-Topics'.", "labels": [], "entities": []}, {"text": "6/11 topics sampled for Strata II were labeled as 'EventyTopics'.", "labels": [], "entities": []}, {"text": "Strata II topics were the most difficult to annotate.", "labels": [], "entities": []}, {"text": "Now that we had an annotated set of EventyTopics, we then tuned the parameters in our EventyTopic Detection algorithm to maximize performance over the annotated set.", "labels": [], "entities": []}, {"text": "The results of our Reuters News corpus Eventy-Topic Detection with optimal parameters 3 can be seen in", "labels": [], "entities": [{"text": "Reuters News corpus", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.8314069906870524}, {"text": "Eventy-Topic Detection", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6808112412691116}]}], "tableCaptions": [{"text": " Table 1: Cosine similarity pair mapping table.  date1:topic1_date2:topic2 \u2192 cosineSimilarity", "labels": [], "entities": []}, {"text": " Table 3: Sampling of Topics from Reuters Corpus for Annotation", "labels": [], "entities": [{"text": "Sampling of", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8888677954673767}, {"text": "Reuters Corpus", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.6882109344005585}, {"text": "Annotation", "start_pos": 53, "end_pos": 63, "type": "TASK", "confidence": 0.5948461890220642}]}, {"text": " Table 4: Accuracy of Eventy-Topic Detection with Optimized minColdHotDiffThresh", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9880818128585815}, {"text": "Eventy-Topic Detection", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.5823733061552048}]}]}