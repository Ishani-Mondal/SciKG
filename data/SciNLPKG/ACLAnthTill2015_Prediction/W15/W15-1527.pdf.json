{"title": [{"text": "Estimating User Location in Social Media with Stacked Denoising Auto-encoders", "labels": [], "entities": [{"text": "Estimating User Location", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7463391423225403}]}], "abstractContent": [{"text": "Only very few users disclose their physical locations , which maybe valuable and useful in applications such as marketing and security monitoring; in order to automatically detect their locations, many approaches have been proposed using various types of information, including the tweets posted by the users.", "labels": [], "entities": []}, {"text": "It is not easy to infer the original locations from textual data, because text tends to be noisy, particularly in social media.", "labels": [], "entities": []}, {"text": "Recently, deep learning techniques have been shown to reduce the error rate of many machine learning tasks, due to their ability to learn meaningful representations of input data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9771202504634857}]}, {"text": "We investigate the potential of building a deep-learning architecture to infer the location of Twitter users based merely on their tweets.", "labels": [], "entities": []}, {"text": "We find that stacked denoising auto-encoders are well suited for this task, with results comparable to state-of-the-art models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many real-world applications require the knowledge of the actual locations of users.", "labels": [], "entities": []}, {"text": "For example, online advertisers would like to target potential buyers in particular regions.", "labels": [], "entities": []}, {"text": "There are easy ways to obtain user locations, for example, social media service providers allow users to provide their locations, mostly through GPS locating or by manual specification.", "labels": [], "entities": []}, {"text": "However, only a small proportion of users actually provide location information.", "labels": [], "entities": []}, {"text": "The proportion of users who specify their locations in their profiles is reported to be 14.3% by; self-reported locations also tend to be unreliable because users can practically type anything they want, such as In your backyard or Wonderland.", "labels": [], "entities": [{"text": "Wonderland", "start_pos": 232, "end_pos": 242, "type": "DATASET", "confidence": 0.8898264169692993}]}, {"text": "When it comes to per-tweet GPS tagging, only 1.2% of all users use this functionality (.", "labels": [], "entities": [{"text": "per-tweet GPS tagging", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.6721311012903849}]}, {"text": "In view of such extreme sparsity, researchers have developed various ways of inferring users' locations using information such as interactions between users, locations declared by users in their social media profiles, users' time zones, the text they generate, etc.", "labels": [], "entities": []}, {"text": "The relation between geographical location and language has been studied since the 19th century as a sub-field of sociolinguistics known as dialectology In this work, our concern is how to estimate users' locations from the textual data that they generate on social media, and in particular to infer Twitter users' location using the messages they post on their Twitter accounts.", "labels": [], "entities": []}, {"text": "For each user, we put together all the tweets written by that user, in order to predict his/her physical location.", "labels": [], "entities": []}, {"text": "We focus on predicting users' locations with a deep learning architecture built with denoising auto-encoders proposed first by, since this approach was not yet applied to this task.", "labels": [], "entities": []}, {"text": "The contribution of our work consists in designing models for solving the task and in finding the right parameter values to make the proposed models achieve good results.", "labels": [], "entities": []}, {"text": "The first model predicts the U.S. region where the user is located and his/her U.S. state, while the second model predicts the longitude and latitude of the user's location.", "labels": [], "entities": [{"text": "longitude", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9573447704315186}]}], "datasetContent": [{"text": "In order to compare the performance of our system with that of other systems, we choose a publicly available dataset from Eisenstein et al.", "labels": [], "entities": []}, {"text": "1 , which has been used by several other researchers.", "labels": [], "entities": []}, {"text": "It includes about 380,000 tweets from 9,500 users from the contiguous United States (i.e., the U.S. excluding Hawaii, Alaska and all off-shore territories).", "labels": [], "entities": []}, {"text": "The dataset also provides geographical coordinates of each user.", "labels": [], "entities": []}, {"text": "A similar but much larger dataset that we use is from Roller et al.", "labels": [], "entities": []}, {"text": "; it contains 38 million tweets from 449,694 users, all from North America.", "labels": [], "entities": []}, {"text": "We regard each user's set of tweets as a training example (labelled with location), i.e., (x (i) , y (i) ) where x (i) represent all the tweets from the i-th user and y (i) is the location of the i-th user.", "labels": [], "entities": []}, {"text": "Metadata like user's profile and timezone will not be used in our work.", "labels": [], "entities": []}, {"text": "The SDA-1 model yields an accuracy of 61.1% and 34.8%, for region classification and state classification, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9996343851089478}, {"text": "region classification", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.785117506980896}, {"text": "state classification", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.6619546413421631}]}, {"text": "The results of all models are shown in.", "labels": [], "entities": []}, {"text": "Among all previous works that use the same dataset, only report the classification accuracy of their models; to present a comprehensive comparison, all models from their work, not just the best one, are listed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8091295957565308}]}, {"text": "Student's t-tests suggest that the differences between SDA-1 and the baseline models are statistically significant at a 99% level of confidence . It can be seen that our SDA-1 model performs best in both classification tasks.", "labels": [], "entities": []}, {"text": "It is surprising to find that the shallow architectures that we implemented, namely SVM and Naive Bayes, perform reasonably well.", "labels": [], "entities": []}, {"text": "They both outperform all models in) in terms of state-wise classification.", "labels": [], "entities": []}, {"text": "A possible explanation is that the features we use (frequencies of n-grams with n = 1, 2, 3) are more indicative than theirs (unigram term frequencies).", "labels": [], "entities": []}, {"text": "Classif: Classification accuracy for SDA-1 and other models shows the mean error distance for various models trained on the same dataset.", "labels": [], "entities": [{"text": "Classification", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.8802867531776428}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8461540341377258}, {"text": "mean error distance", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.7888117035230001}]}, {"text": "The difference between SDA-2 and the baseline model is statistically significant at a level of confidence of 99.9% 10 . Our model has the second best results and performs better than four models from previous work.", "labels": [], "entities": []}, {"text": "In addition, the fact that SDA-2 outperforms the baseline model by a large margin shows the advantages of a deep architecture and its ability to capture meaningful and useful abstractions from input data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification accuracy for SDA-1 and  other models", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9190383553504944}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9675425291061401}]}, {"text": " Table 2: Mean error distance of predictions for  SDA-2 and models from previous work.", "labels": [], "entities": [{"text": "Mean error distance", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9417206843694051}]}, {"text": " Table 3: Results from SDA-2 and the best models  of previous work; NA indicates Not Available", "labels": [], "entities": [{"text": "NA", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9540389776229858}]}]}