{"title": [{"text": "Pronoun-Focused MT and Cross-Lingual Pronoun Prediction: Findings of the 2015 DiscoMT Shared Task on Pronoun Translation", "labels": [], "entities": [{"text": "Pronoun-Focused MT", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6197447776794434}, {"text": "Cross-Lingual Pronoun Prediction", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.6527383724848429}, {"text": "DiscoMT Shared Task", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.502432624499003}, {"text": "Pronoun Translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.6575658321380615}]}], "abstractContent": [{"text": "We describe the design, the evaluation setup, and the results of the DiscoMT 2015 shared task, which included two sub-tasks, relevant to both the machine translation (MT) and the discourse communities: (i) pronoun-focused translation, a practical MT task, and (ii) cross-lingual pronoun prediction, a classification task that requires no specific MT expertise and is interesting as a machine learning task in its own right.", "labels": [], "entities": [{"text": "DiscoMT 2015 shared task", "start_pos": 69, "end_pos": 93, "type": "DATASET", "confidence": 0.7888120114803314}, {"text": "machine translation (MT)", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.817609965801239}, {"text": "pronoun-focused translation", "start_pos": 206, "end_pos": 233, "type": "TASK", "confidence": 0.7322850525379181}, {"text": "MT task", "start_pos": 247, "end_pos": 254, "type": "TASK", "confidence": 0.9091265201568604}, {"text": "cross-lingual pronoun prediction", "start_pos": 265, "end_pos": 297, "type": "TASK", "confidence": 0.7206726670265198}]}, {"text": "We focused on the Eng-lish-French language pair, for which MT output is generally of high quality, but has visible issues with pronoun translation due to differences in the pronoun systems of the two languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9728645086288452}, {"text": "pronoun translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7529307007789612}]}, {"text": "Six groups participated in the pronoun-focused translation task and eight groups in the cross-lingual pronoun prediction task.", "labels": [], "entities": [{"text": "pronoun-focused translation task", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.7719521522521973}, {"text": "cross-lingual pronoun prediction task", "start_pos": 88, "end_pos": 125, "type": "TASK", "confidence": 0.7664758861064911}]}], "introductionContent": [{"text": "Until just a few years ago, there was little awareness of discourse-level linguistic features in statistical machine translation (SMT) research.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 97, "end_pos": 134, "type": "TASK", "confidence": 0.7852332095305125}]}, {"text": "Since then, a number of groups have started working on discourse-related topics, and today there is a fairly active community that convened for the first time at the Workshop on Discourse in Machine Translation (DiscoMT) at the ACL 2013 conference in Sofia (Bulgaria).", "labels": [], "entities": [{"text": "Machine Translation (DiscoMT) at the ACL 2013 conference in Sofia", "start_pos": 191, "end_pos": 256, "type": "TASK", "confidence": 0.8670984953641891}]}, {"text": "This year sees a second DiscoMT workshop taking place at EMNLP 2015 in Lisbon (Portugal), and we felt that the time was ripe to make a coordinated effort towards establishing the state of the art for an important discourse-related issue in machine translation (MT), the translation of pronouns.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 240, "end_pos": 264, "type": "TASK", "confidence": 0.8695398926734924}]}, {"text": "Organizing a shared task involves clearly defining the problem, then creating suitable datasets and evaluation methodologies.", "labels": [], "entities": []}, {"text": "Having such a setup makes it possible to explore a variety of approaches for solving the problem at hand since the participating groups independently come up with various ways to address it.", "labels": [], "entities": []}, {"text": "All of this is highly beneficial for continued research as it creates a well-defined benchmark with a low entry barrier, a set of results to compare to, and a collection of properly evaluated ideas to start from.", "labels": [], "entities": []}, {"text": "We decided to base this shared task on the problem of pronoun translation.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.725233256816864}]}, {"text": "Historically, this was one of the first discourse problems to be considered in the context of SMT); yet, it is still far from being solved.", "labels": [], "entities": [{"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9942116141319275}]}, {"text": "For an overview of the existing work on pronoun translation, we refer the reader to Hardmeier (2014, Section 2.3.1).", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8173554241657257}]}, {"text": "The typical case is an anaphoric pronoun -one that refers to an entity mentioned earlier in the discourse, its antecedent.", "labels": [], "entities": []}, {"text": "Many languages have agreement constraints between pronouns and their antecedents.", "labels": [], "entities": []}, {"text": "In translation, these constraints must be satisfied in the target language.", "labels": [], "entities": [{"text": "translation", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9715369939804077}]}, {"text": "Note that source language information is not enough for this task.", "labels": [], "entities": []}, {"text": "To see why, consider the following example for EnglishFrench: The funeral of the Queen Mother will take place on Friday.", "labels": [], "entities": [{"text": "EnglishFrench", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9799542427062988}]}, {"text": "It will be broadcast live.", "labels": [], "entities": []}, {"text": "Les fun\u00e9railles de la reine-m\u00e8re auront lieu vendredi.", "labels": [], "entities": []}, {"text": "Elles seront retransmises en direct.", "labels": [], "entities": []}, {"text": "The example is taken from Hardmeier.", "labels": [], "entities": []}], "datasetContent": [{"text": "As already noted, the corpus data used in the DiscoMT shared task comes from the TED talks.", "labels": [], "entities": [{"text": "DiscoMT shared task", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6132961710294088}, {"text": "TED talks", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.8897493183612823}]}, {"text": "In the following, the datasets are briefly described.", "labels": [], "entities": []}, {"text": "Note that TED talks differ from other text types with respect to pronoun use.", "labels": [], "entities": []}, {"text": "TED speakers frequently use first-and second-person pronouns (singular and plural): first-person pronouns to refer to themselves and their colleagues or to themselves and the audience, and second-person pronouns to refer to the audience, to the larger set of viewers, or to people in general.", "labels": [], "entities": []}, {"text": "Moreover, they often use the pronoun they without a specific textual antecedent, in phrases such as \"This is what they think\", as well as deictic and third-person pronouns to refer to things in the spatio-temporal context shared by the speaker and the audience, such as props and slides.", "labels": [], "entities": []}, {"text": "In general, pronouns are abundant in TED talks, and anaphoric references are not always very clearly defined.", "labels": [], "entities": []}, {"text": "Evaluating machine translations for pronoun correctness automatically is difficult because standard assumptions fail.", "labels": [], "entities": [{"text": "Evaluating machine translations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7531881531079611}, {"text": "pronoun correctness", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7503019571304321}]}, {"text": "In particular, it is incorrect to assume that a pronoun is translated correctly if it matches the reference translation.", "labels": [], "entities": []}, {"text": "If the translation of an anaphoric pronoun is itself a pronoun, it has to agree with the translation of its antecedent, and a translation deviating from the reference maybe the only correct solution in some cases.", "labels": [], "entities": []}, {"text": "Doing this evaluation correctly would require a working solution to the cross-lingual pronoun prediction task, the second challenge of our shared task.", "labels": [], "entities": [{"text": "cross-lingual pronoun prediction", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.7058478991190592}]}, {"text": "Given the current state of the art, we have little choice but to do manual evaluation.", "labels": [], "entities": []}, {"text": "Our evaluation methodology is based on the gapfilling annotation procedure introduced by Hardmeier (2014, Section 9.4).", "labels": [], "entities": []}, {"text": "We employed two annotators, both of whom were professional translators, native speakers of Swedish with good command of French.", "labels": [], "entities": []}, {"text": "Tokens were presented to the annotators in the form of examples corresponding to a single occurrence of the English pronouns it or they.", "labels": [], "entities": []}, {"text": "For each example, the sentence containing the pronoun was shown to the annotator along with its machine translation (but not the reference translation) and up to 5 sentences of context in both languages.", "labels": [], "entities": []}, {"text": "In the MT output, any French pronouns aligned to the pronoun to be annotated were replaced with a placeholder.", "labels": [], "entities": [{"text": "MT output", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.8864780366420746}]}, {"text": "The annotators were then asked to replace the placeholder with an item selected from a list of pronouns that was based on the classes of the cross-lingual pronoun prediction task.", "labels": [], "entities": [{"text": "cross-lingual pronoun prediction task", "start_pos": 141, "end_pos": 178, "type": "TASK", "confidence": 0.7223167046904564}]}, {"text": "Compared to the perhaps more obvious methodology of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems.", "labels": [], "entities": []}, {"text": "Moreover, the annotation interface allowed the annotators to select BAD TRANSLATION if the MT output was not sufficiently well-formed to be annotated with a pronoun.", "labels": [], "entities": [{"text": "BAD TRANSLATION", "start_pos": 68, "end_pos": 83, "type": "METRIC", "confidence": 0.7895678281784058}]}, {"text": "However, they were instructed to be tolerant of ill-formed translations and to use the label BAD TRANSLATION only if it was necessary to make more than two modifications to the sentence, in addition to filling in the placeholder, to make the output locally grammatical.", "labels": [], "entities": [{"text": "BAD", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9841622114181519}, {"text": "TRANSLATION", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.5118767023086548}]}, {"text": "In earlier work, Hardmeier (2014) reported an annotation speed of about 60 examples per hour.", "labels": [], "entities": [{"text": "annotation speed", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.9403047561645508}]}, {"text": "While our annotators approached that figure after completed training, the average speed over the entire annotation period was about one third lower in this work, mostly because it proved to be more difficult than anticipated to settle on a consistent set of guidelines and reach an acceptable level of inter-annotator agreement.", "labels": [], "entities": []}, {"text": "We believe there are two reasons for this.", "labels": [], "entities": []}, {"text": "On the one hand, the MT output came from a number of systems of widely varying quality, while previous work considered different variants of a single system.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9920360445976257}]}, {"text": "Achieving consistent annotation turned out to be considerably more difficult for the lower-quality systems.", "labels": [], "entities": []}, {"text": "On the other hand, unlike the annotators used by, ours had a linguistic background as translators, but not in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.615788459777832}]}, {"text": "This is probably an advantage as far as unbiased annotations are concerned, but it may have increased the initial time to get used to the task and its purpose.", "labels": [], "entities": []}, {"text": "We computed inter-annotator agreement in terms of) and Scott's \u03c0, using the NLTK toolkit (, over 28 examples annotated by the two annotators.", "labels": [], "entities": [{"text": "Scott's \u03c0", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.5109986762205759}, {"text": "NLTK toolkit", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.9292509257793427}]}, {"text": "After two rounds of discussion and evaluation, we reached an agreement of \u03b1 = 0.561 and \u03c0 = 0.574.", "labels": [], "entities": []}, {"text": "These agreement figures are lower than those reported by Hardmeier, which we believe is mostly due to the factors discussed above.", "labels": [], "entities": [{"text": "agreement", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9612611532211304}]}, {"text": "Some of the disagreement also seems to stem from the annotators' different propensity to annotate examples with demonstrative pronouns.", "labels": [], "entities": []}, {"text": "This point was addressed in discussions with the annotators, but we did not have time for another round of formal annotator training and agreement evaluation.", "labels": [], "entities": []}, {"text": "We do not believe this had a major negative effect on the MT evaluation quality since, inmost cases where the annotators disagreed about whether to annotate \u00e7a/cela, the alternative personal pronoun would be annotated consistently if a personal pronoun was acceptable.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9918559789657593}]}, {"text": "In case of insurmountable difficulties, the annotators had the option to mark an example with the label DISCUSSION REQUIRED.", "labels": [], "entities": [{"text": "DISCUSSION", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.731881856918335}, {"text": "REQUIRED", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.5696004629135132}]}, {"text": "Such cases were resolved at the end of the annotation process.", "labels": [], "entities": []}, {"text": "In total, we annotated 210 examples for each of the six submitted systems as well as for the official baseline system.", "labels": [], "entities": []}, {"text": "The examples were paired across all systems, so the same set of English pronouns was annotated for each system.", "labels": [], "entities": []}, {"text": "In addition, the sample was stratified to ensure that all pronoun types were represented adequately.", "labels": [], "entities": []}, {"text": "The stratification was performed by looking at the pronouns aligned to the English pronouns in the reference translation and separately selecting a sample of each pronoun class (according to) in proportion to its relative frequency in the complete test set.", "labels": [], "entities": []}, {"text": "When rounding the individual sample sizes to integer values, we gave slight preference to the rarer classes by rounding the sample sizes upwards for the less frequent and downwards for the more frequent classes.", "labels": [], "entities": []}, {"text": "After completing the human evaluation, we calculated a set of evaluation scores by counting how often the output of a particular system matched the manual annotation specific to that system.", "labels": [], "entities": []}, {"text": "This is straightforward for the annotation labels corresponding to actual pronouns (ce, \u00e7a/cela, elle, elles, il, ils and on).", "labels": [], "entities": []}, {"text": "The examples labelled as BAD TRANSLATION were counted as incorrect.", "labels": [], "entities": [{"text": "BAD", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9451703429222107}, {"text": "TRANSLATION", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.6172167062759399}]}, {"text": "The label OTHER leads to complications because this label lumps together many different cases such as the use of a pronoun not available as an explicit label, the complete absence of a pronoun translation on the target side, the translation of a pronoun with a full noun phrase or other linguistic construct, etc.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9035711288452148}]}, {"text": "As a result, even if the MT output of an example annotated as OTHER contains a translation that is compatible with this annotation, we cannot be sure that it is in fact correct.", "labels": [], "entities": []}, {"text": "This must be kept in mind when interpreting aggregate metrics based on our annotations.", "labels": [], "entities": []}, {"text": "The evaluation scores based on manual annotations are defined as follows: Accuracy with OTHER (Acc+O) Our primary evaluation score is accuracy overall 210 examples, i.e., the proportion of examples for which the pronouns in the MT output are compatible with those in the manual annotation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9993693232536316}, {"text": "OTHER", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9932386875152588}, {"text": "Acc+O)", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9101053774356842}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9995724558830261}]}, {"text": "We include items labelled OTHER and count them as correct if the MT output contains any realisation compatible with that label.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.6871037483215332}, {"text": "MT output", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.7259387075901031}]}, {"text": "The standard automatic MT evaluation scores (BLEU, NIST, TER, METEOR;) do not offer specific insights about pronoun translation, but it is still useful to consider them first for an easy overview over the submitted systems.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.8738017976284027}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9978565573692322}, {"text": "NIST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.6454988718032837}, {"text": "TER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.991072952747345}, {"text": "METEOR", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9900637269020081}, {"text": "pronoun translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7440114617347717}]}, {"text": "They clearly reveal a group of systems (IDIAP, UU-TIEDEMANN and AUTO-POSTEDIT) built with the data of the official BASELINE system, with very similar scores ranging between 36.4 and 37.2 BLEU points.", "labels": [], "entities": [{"text": "UU-TIEDEMANN", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.6097279787063599}, {"text": "AUTO-POSTEDIT", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.8505337834358215}, {"text": "BASELINE", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.4974084794521332}, {"text": "BLEU", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.9986507296562195}]}, {"text": "The baseline itself achieves the best scores, but considering the inadequacy of BLEU for pronoun evaluation, we do not see this as a major concern in itself.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9982234835624695}]}, {"text": "The other submissions fall behind in terms of automatic MT metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9720265865325928}]}, {"text": "The UU-HARDMEIER system is similar to the other SMT systems, but uses different language and translation models, which evidently do not yield the same level of raw MT performance as the baseline system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9838264584541321}, {"text": "MT", "start_pos": 164, "end_pos": 166, "type": "TASK", "confidence": 0.9562161564826965}]}, {"text": "ITS2 is a rule-based system.", "labels": [], "entities": [{"text": "ITS2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9489590525627136}]}, {"text": "Since it is well known that n-gram-based evaluation metrics do not always do full justice to rule-based MT approaches not using n-gram language models), it is difficult to draw definite conclusions from this system's lower scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9404016137123108}]}, {"text": "Finally, the extremely low scores for the A3-108 system indicate serious problems with translation quality, an impression that we easily confirmed by examining the system output.", "labels": [], "entities": [{"text": "translation", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.9618402719497681}]}, {"text": "The results for the manual evaluation are shown in: we show aggregate scores such as accuracy, with and without OTHER, as well as F max scores for the individual pronouns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9996886253356934}, {"text": "OTHER", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9925416707992554}, {"text": "F max scores", "start_pos": 130, "end_pos": 142, "type": "METRIC", "confidence": 0.9743010997772217}]}, {"text": "We have chosen Acc+O to be the primary metric because it is well defined as it is calculated on the same instances for all participating systems, so it cannot be easily exploited by manipulating the system output in clever ways.", "labels": [], "entities": [{"text": "Acc+O", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9137956500053406}]}, {"text": "It turns out, however, that the rankings of our participating systems induced by this score and the Acc-O score are exactly identical.", "labels": [], "entities": [{"text": "Acc-O score", "start_pos": 100, "end_pos": 111, "type": "METRIC", "confidence": 0.9772341549396515}]}, {"text": "In both cases, the BASELINE system leads, followed relatively closely by IDIAP and UU-TIEDEMANN.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.975947916507721}, {"text": "IDIAP", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.7445714473724365}, {"text": "UU-TIEDEMANN", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8912479281425476}]}, {"text": "Then, UU-HARDMEIER and AUTO-POSTEDIT follow at a slightly larger distance, and finally A3-108 scores at the bottom.", "labels": [], "entities": [{"text": "UU-HARDMEIER", "start_pos": 6, "end_pos": 18, "type": "METRIC", "confidence": 0.6489190459251404}, {"text": "AUTO-POSTEDIT", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.897550642490387}, {"text": "A3-108", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9944702386856079}]}, {"text": "The micro-averaged Pron-F score would have yielded the same ranking as well, except for the first two systems, where IDIAP would have taken the lead from the BASELINE.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.8491727113723755}]}, {"text": "This is due to the fact that the IDIAP system has a higher number of examples labelled BAD TRANSLATION, while maintaining the same performance as the baseline for the examples with acceptable translations.", "labels": [], "entities": [{"text": "BAD TRANSLATION", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.7846015691757202}]}, {"text": "Rather than implying much about the quality of the systems, this observation confirms and justifies our decision to choose a primary score that is not susceptible to effects arising from excluded classes.", "labels": [], "entities": []}, {"text": "The low scores for the ITS2 system were partly due to a design decision.", "labels": [], "entities": [{"text": "ITS2 system", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.9152342677116394}]}, {"text": "The anaphora prediction component of ITS2 only generated the personal pronouns il, elle, ils and elles; this led to zero recall force and \u00e7a/cela and, as a consequence, to a large number of misses that would have been comparatively easy to predict with an n-gram model.", "labels": [], "entities": [{"text": "anaphora prediction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6929363906383514}, {"text": "ITS2", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8839740753173828}, {"text": "recall force", "start_pos": 121, "end_pos": 133, "type": "METRIC", "confidence": 0.9866641759872437}]}, {"text": "There does not seem to be a correlation between pronoun translation quality and the choice of (a) a two-pass approach with automatic post-editing (IDIAP, AUTO-POSTEDIT) or (b) a single-pass SMT system with some form of integrated pronoun model (UU-TIEDEMANN, UU-HARDMEIER).", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7604221105575562}, {"text": "AUTO-POSTEDIT", "start_pos": 154, "end_pos": 167, "type": "METRIC", "confidence": 0.8839718103408813}, {"text": "SMT", "start_pos": 190, "end_pos": 193, "type": "TASK", "confidence": 0.9606724381446838}]}, {"text": "Also, at the level of performance that current systems achieve, there does not seem to bean inherent advantage or disadvantage in doing explicit anaphora resolution (as IDIAP, UU-HARDMEIER, AUTO-POSTEDIT and ITS2 did) as opposed to considering unstructured context only (as in UU-TIEDEMANN and the BASELINE).", "labels": [], "entities": [{"text": "explicit anaphora resolution", "start_pos": 136, "end_pos": 164, "type": "TASK", "confidence": 0.7061074177424113}, {"text": "UU-HARDMEIER", "start_pos": 176, "end_pos": 188, "type": "DATASET", "confidence": 0.8728496432304382}, {"text": "BASELINE", "start_pos": 298, "end_pos": 306, "type": "METRIC", "confidence": 0.7448059320449829}]}, {"text": "One conclusion that is supported by relatively ample evidence in the results concerns the importance of the n-gram language model.", "labels": [], "entities": []}, {"text": "The BASELINE system, which only relies on n-gram modelling to choose the pronouns, achieved scores higher than those of all competing systems.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9279503226280212}]}, {"text": "Moreover, even among the submitted systems that included some form of pronoun model, those that relied most on the standard SMT models performed best.", "labels": [], "entities": [{"text": "SMT", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.9779734015464783}]}, {"text": "For example, the IDIAP submission exploited the SMT decoder's translation hypotheses by parsing the search graph, and UU-TIEDEMANN extended the baseline configuration with additional n-gramstyle models.", "labels": [], "entities": [{"text": "SMT decoder's translation", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.7051281183958054}]}, {"text": "By contrast, those systems that actively overrode the choices of the baseline n-gram model (UU-HARDMEIER and AUTO-POSTEDIT) performed much worse.", "labels": [], "entities": [{"text": "AUTO-POSTEDIT", "start_pos": 109, "end_pos": 122, "type": "METRIC", "confidence": 0.6241017580032349}]}, {"text": "Based on these somewhat depressing results, one might be tempted to conclude that all comparison between the submitted systems is meaningless because all they managed to accomplish was to \"dis- the output of a working baseline system to various degrees.", "labels": [], "entities": []}, {"text": "Yet, we should point out that it was possible for some systems to outperform the baseline at least for some of the rarer pronouns.", "labels": [], "entities": []}, {"text": "In particular, the IDIAP system beat the baseline on 4 out of 6 pronoun types, including the feminine plural pronoun elles, and the UU-TIEDEMANN system performed better on both types of feminine pronouns, elle and elles.", "labels": [], "entities": []}, {"text": "Results like these suggest that all hope is not lost.", "labels": [], "entities": []}, {"text": "For the automatic evaluation, we developed a scoring script that calculates the following statistics: \u2022 confusion matrix showing (i) the count for each gold/predicted pair, and (ii) the sums for each row/column; \u2022 accuracy; \u2022 precision (P), recall (R), and F-score for each label; \u2022 micro-averaged P, R, F-score (note that in our setup, micro-F is the same as accuracy); \u2022 macro-averaged P, R, F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9996546506881714}, {"text": "precision (P)", "start_pos": 226, "end_pos": 239, "type": "METRIC", "confidence": 0.934403583407402}, {"text": "recall (R)", "start_pos": 241, "end_pos": 251, "type": "METRIC", "confidence": 0.9495503753423691}, {"text": "F-score", "start_pos": 257, "end_pos": 264, "type": "METRIC", "confidence": 0.9753314256668091}, {"text": "accuracy", "start_pos": 360, "end_pos": 368, "type": "METRIC", "confidence": 0.9989017248153687}, {"text": "F-score", "start_pos": 394, "end_pos": 401, "type": "METRIC", "confidence": 0.9541746973991394}]}, {"text": "The script performs the scoring twice: \u2022 using coarse-grained labels (ce, {cela+\u00e7a}, elle, elles, il, ils, {OTHER+on}); \u2022 using fine-grained labels (ce, cela, elle, elles, il, ils, on, \u00e7a, OTHER).", "labels": [], "entities": [{"text": "OTHER", "start_pos": 189, "end_pos": 194, "type": "METRIC", "confidence": 0.9716424345970154}]}, {"text": "The official score was the macro-averaged Fscore using fine-grained labels.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9530677795410156}]}], "tableCaptions": [{"text": " Table 2: Statistics about the bilingual linguistic  resources for the shared task.", "labels": [], "entities": []}, {"text": " Table 3: Statistics about the talks that were in- cluded in DiscoMT.tst2015.", "labels": [], "entities": [{"text": "DiscoMT.tst2015", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.9687356948852539}]}, {"text": " Table 4: Baseline models for English-French ma- chine translation: case-insensitive BLEU scores.", "labels": [], "entities": [{"text": "English-French ma- chine translation", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.5628801822662354}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.966993510723114}]}, {"text": " Table 5: Pronoun-focused translation task: automatic metrics.", "labels": [], "entities": [{"text": "Pronoun-focused translation task", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8136273622512817}]}, {"text": " Table 6: Pronoun-focused translation task: manual evaluation metrics.", "labels": [], "entities": [{"text": "Pronoun-focused translation task", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7991527120272318}]}, {"text": " Table 7: F-score for all alignment links (F all ), and  for pronoun links (F pro ), for different alignment  models with grow-diag-final-and symmetrization.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9979255199432373}]}, {"text": " Table 8: Distribution of classes in the DiscoMT  2015 test set and the three training datasets.", "labels": [], "entities": [{"text": "DiscoMT  2015 test set", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.9497442841529846}]}, {"text": " Table 9: Results for the cross-lingual pronoun prediction task.", "labels": [], "entities": [{"text": "cross-lingual pronoun prediction task", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7985694408416748}]}]}