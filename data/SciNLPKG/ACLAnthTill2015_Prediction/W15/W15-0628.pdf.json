{"title": [{"text": "Using NLP to Support Scalable Assessment of Short Free Text Responses", "labels": [], "entities": [{"text": "Scalable Assessment of Short Free Text Responses", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.6717056461742946}]}], "abstractContent": [{"text": "Marking student responses to short answer questions raises particular issues for human markers, as well as for automatic marking systems.", "labels": [], "entities": [{"text": "Marking student responses to short answer questions", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8984582594462803}]}, {"text": "In this paper we present the Amati system , which aims to help human markers improve the speed and accuracy of their marking.", "labels": [], "entities": [{"text": "Amati", "start_pos": 29, "end_pos": 34, "type": "TASK", "confidence": 0.9200697541236877}, {"text": "speed", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9911551475524902}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9853090643882751}]}, {"text": "Amati supports an educator in incrementally developing a set of automatic marking rules, which can then be applied to larger question sets or used for automatic marking.", "labels": [], "entities": [{"text": "Amati", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9412733316421509}, {"text": "automatic marking", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7050579786300659}]}, {"text": "We show that using this system allows markers to develop mark schemes which closely match the judgements of a human expert, with the benefits of consistency, scalability and traceabil-ity afforded by an automated marking system.", "labels": [], "entities": [{"text": "consistency", "start_pos": 145, "end_pos": 156, "type": "METRIC", "confidence": 0.9978663325309753}]}, {"text": "We also consider some difficult cases for automatic marking, and look at some of the computational and linguistic properties of these cases.", "labels": [], "entities": [{"text": "automatic marking", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7846111953258514}]}], "introductionContent": [{"text": "In developing systems for automatic marking, observed that assessment based on short answer, free text input from students demands very different skills from assessment based upon multiple-choice questions.", "labels": [], "entities": [{"text": "automatic marking", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7379629015922546}]}, {"text": "Free text questions require a student to present the appropriate information in their own words, and without the cues sometimes provided by multiple choice questions (described respectively as improved verbalisation and recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 220, "end_pos": 226, "type": "METRIC", "confidence": 0.9976491332054138}]}, {"text": "Work by has demonstrated that automatic, online marking of student responses is both feasible (in that marking rules can be developed which mark at least as accurately as a human marker), and helpful to students, who find the online questions a valuable and enjoyable part of the assessment process.", "labels": [], "entities": []}, {"text": "Such automatic marking is also an increasingly important part of assessment in Massive Open Online Courses (MOOCs).", "labels": [], "entities": []}, {"text": "However, the process of creating marking rules is known to be difficult and time consuming (.", "labels": [], "entities": []}, {"text": "The rules should usually be hand-crafted by a tutor who is a domain expert, as small differences in the way an answer is expressed can be significant in determining whether responses are corrector incorrect.", "labels": [], "entities": []}, {"text": "Curating sets of answers to build mark schemes can prove to be a highly labourintensive process.", "labels": [], "entities": []}, {"text": "Given this requirement, and the current lack of availability of training data, a valuable progression from existing work in automatic assessment maybe to investigate whether NLP techniques can be used to support the manual creation of such marking rules.", "labels": [], "entities": []}, {"text": "In this paper, we present the Amati system, which supports educators in creating mark schemes for automatic assessment of short answer questions.", "labels": [], "entities": [{"text": "automatic assessment of short answer questions", "start_pos": 98, "end_pos": 144, "type": "TASK", "confidence": 0.6655629426240921}]}, {"text": "Amati uses information extraction-style templates to enable a human marker to rapidly develop automatic marking rules, and inductive logic programming to propose new rules to the marker.", "labels": [], "entities": []}, {"text": "Having been developed, the rules can be used either for marking further unseen student responses, or for online assessment.", "labels": [], "entities": []}, {"text": "Automatic marking also brings with it further advantages.", "labels": [], "entities": [{"text": "Automatic marking", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7885644137859344}]}, {"text": "Because rules are applied automatically, it improves the consistency of marking; have noted the potential of automated marking to improve the reliability of test scores.", "labels": [], "entities": [{"text": "consistency", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9911975264549255}]}, {"text": "In addition, because Amati uses symbolic/logical rules rather than stochastic rules, it improves the traceability of the marks (that is, the marker can give an explanation of why a mark was awarded, or not), and increases the maintainability of the mark scheme, because the educator can modify the rules in the context of better understanding of student responses.", "labels": [], "entities": []}, {"text": "The explanatory nature of symbolic mark schemes also support issues of auditing marks awarded in assessment.", "labels": [], "entities": []}, {"text": "Bodies such as the UK's Quality Assurance Agency 1 require that assessment be fully open for the purposes of external examination.", "labels": [], "entities": [{"text": "UK's Quality Assurance Agency 1", "start_pos": 19, "end_pos": 50, "type": "DATASET", "confidence": 0.897818128267924}]}, {"text": "Techniques which can show exactly why a particular mark was awarded (or not) fora given response fit well with existing quality assurance requirements.", "labels": [], "entities": []}, {"text": "All experiments in this paper were carried out using student responses collected from a first year introductory science module.", "labels": [], "entities": []}, {"text": "have identified several different eras of automatic marking of free text responses.", "labels": [], "entities": [{"text": "automatic marking of free text responses", "start_pos": 42, "end_pos": 82, "type": "TASK", "confidence": 0.7679474304119746}]}, {"text": "One era they have identified has treated automatic marking as essentially a form of information extraction.", "labels": [], "entities": [{"text": "automatic marking", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7109380662441254}, {"text": "information extraction", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.8070048391819}]}, {"text": "The many different ways that a student can correctly answer a question can make it difficult to award correct marks 2 . For example:", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of the evaluation was to determine whether a ruleset built using Amati could achieve performance comparable with human markers.", "labels": [], "entities": []}, {"text": "As such, there were two main aims.", "labels": [], "entities": []}, {"text": "First, to determine whether the proposed language was sufficiently expressive to build successful mark schemes, and second, to determine how well a mark scheme developed using the Amati system would compare against a human marker.", "labels": [], "entities": []}], "tableCaptions": []}