{"title": [{"text": "An Ontology-based Approach to Automatic Part-of-Speech Tagging Using Heterogeneously Annotated Corpora", "labels": [], "entities": [{"text": "Automatic Part-of-Speech Tagging", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.6171718736489614}]}], "abstractContent": [{"text": "In the LOD era, the conceptual interop-erability of language resources is established by using modular architectures like the Ontologies of Linguistic Annotations (Chiarcos, 2008a, OLiA).", "labels": [], "entities": [{"text": "OLiA", "start_pos": 181, "end_pos": 185, "type": "DATASET", "confidence": 0.7773500084877014}]}, {"text": "Available as apart of the Linguistic Linked Open Data (LLOD) cloud, 1 OLiA provides ontological representations of annotation schemes for over 70 languages, as well as their linking to a reference model.", "labels": [], "entities": [{"text": "Linguistic Linked Open Data (LLOD) cloud", "start_pos": 26, "end_pos": 66, "type": "DATASET", "confidence": 0.6864200085401535}]}, {"text": "We successfully train an ontology-based POS tagger on corpora with different tag sets of divergent granularity and partially compatible annotations.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.7399740517139435}]}, {"text": "Making use of OLiA, we achieve interoperability of annotation schemes, and, despite sparse training data, we do not only outperform state-of-the-art POS taggers in concept coverage, but also show how traing on heterogeneously annotated data produces richer morphosyn-tactic annotation with no or only marginal loss of precision.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 149, "end_pos": 160, "type": "TASK", "confidence": 0.7233253121376038}, {"text": "precision", "start_pos": 318, "end_pos": 327, "type": "METRIC", "confidence": 0.9960024952888489}]}], "introductionContent": [{"text": "Ontologies have long been recognized as a primary device for interoperability among annotations and linguistic descriptions), and they have been applied to facilitate querying, interoperability among modules in NLP pipelines (, or for post-processing (i.e., merging, enriching or disambiguating) the output of NLP tools (ParejaLora and Aguado.", "labels": [], "entities": []}, {"text": "In this paper, we describe a novel approach towards the next challenge along this trajectory, i.e., the development of NLP tools http://linguistic-lod.org that can directly produce and consume ontological descriptions.", "labels": [], "entities": []}, {"text": "In comparison with classical, string-based annotation, key advantages include a detailed assessment of classification accuracy for different annotation concepts (rather than for opaque strings representing bundles of these), a freely scalable degree of granularity (the system produces statements at all levels of granularity), and interoperability with state-of-the-art technologies from NLP and the Semantic Web.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9111341238021851}]}, {"text": "Another advantage is that annotations from different sources become interoperable, and tools can be trained on annotations from multiple corpora annotated according to different schemes.", "labels": [], "entities": []}, {"text": "In this regard, this paper describes a novel approach toward automatic part-of-speech (POS) annotation, and investigates the extent to which ontology-based annotations allow us to train NLP tools on corpora with divergent, but conceptually related annotations, and whether the increase in the granularity of analysis outweighs possible losses in precision arising from the heterogeneity of the training data.", "labels": [], "entities": [{"text": "automatic part-of-speech (POS) annotation", "start_pos": 61, "end_pos": 102, "type": "TASK", "confidence": 0.661897286772728}, {"text": "precision", "start_pos": 346, "end_pos": 355, "type": "METRIC", "confidence": 0.9976816177368164}]}], "datasetContent": [{"text": "Three neural networks were trained on the training sets: EWT/PTB data only, Susanne/Susa data only, and both training sets combined.", "labels": [], "entities": [{"text": "EWT/PTB data", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.6831071525812149}, {"text": "Susanne/Susa data", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.6437321528792381}]}, {"text": "Several state-of-the-art POS taggers have been trained on this data as baseline: TreeTagger (Schmid, 1999),) and Stanford (), all trained and tested on the same (non-combined) data as the neural networks.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7621190249919891}]}, {"text": "Training these on PTB annotations was straightforward.", "labels": [], "entities": [{"text": "PTB annotations", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.5647771954536438}]}, {"text": "On Susa, however, TreeTagger could not accomodate 270 unique tags and was thus skipped, and Lapos could be trained but showed very low performance on the full tagset.", "labels": [], "entities": [{"text": "Susa", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.5839996933937073}, {"text": "TreeTagger", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.9434226751327515}]}, {"text": "The Stanford tagger was successfully trained using state-of-the-art MaxEnt (left3words) models for EWT and Susanne, respectively.", "labels": [], "entities": [{"text": "EWT", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.9529834389686584}]}, {"text": "Like the training data for the neural network, the output of each tagger was mapped to OLiA Reference Model concepts by means of the corresponding Annotation and Linking Models.", "labels": [], "entities": []}, {"text": "This is the basis for comparative evaluation with the neural networks.", "labels": [], "entities": []}, {"text": "shows how NN:Combined yields again of informativity in comparison to the original annotations (and tools trained on that basis).", "labels": [], "entities": []}, {"text": "Neither of both original tagsets is a proper subset of (the ontological representation of) the other one (%concepts), and accordingly, NN:Combined (with structural pruning) predicts more triples than can actually be evaluated against the gold annotation (1-%triples).", "labels": [], "entities": []}, {"text": "We refer to this evaluation metric as (OLiA) concept coverage.", "labels": [], "entities": [{"text": "OLiA) concept coverage", "start_pos": 39, "end_pos": 61, "type": "METRIC", "confidence": 0.8507644385099411}]}, {"text": "While NN:Combined trivially again in concept coverage over tag-based tools by design, this is logically independent from accuracy, and it maybe suspected that training over heterogeneous annotations adds additional noise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9992156028747559}]}, {"text": "Yet, as we eventually observed, it reaches the precision of state-ofthe-art string-based POS taggers.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9892428517341614}, {"text": "POS taggers", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.6820067167282104}]}, {"text": "In order to evaluate this aspect, we employ two precision metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.989860475063324}]}, {"text": "Concept precision is calculated in the conventional way with the following definitions: A predicted concept is a true positive if also generated from the gold annotation, e.g., Noun from both predicted tag NNP and observed tag NN.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9350609183311462}]}, {"text": "Otherwise, it is a false positive, e.g., ProperNoun from predicted NNP but not from observed NN (common noun).", "labels": [], "entities": [{"text": "ProperNoun", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.946751058101654}]}, {"text": "For path precision, a path is considered to be a true positive only if all the concepts in the path are also generated from the gold tag.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.942288339138031}]}, {"text": "In the example above, the predicted tag NNP yields the path ProperNoun, Noun, while the gold tag NN yields CommonNoun, Noun, hence, a false positive.", "labels": [], "entities": [{"text": "CommonNoun", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.9357787370681763}]}, {"text": "For conventional taggers, path precision corresponds to standard tag precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.5756424069404602}]}, {"text": "2, Susa generates 66 unique concepts while 50 concepts are generated by PTB, the union of both is 77 unique concepts.", "labels": [], "entities": [{"text": "Susa", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.9808542728424072}, {"text": "PTB", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8530000448226929}]}, {"text": "To calculate concept and path precision for tag set-specific taggers (Tab.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.8086227774620056}]}, {"text": "3), concepts not predictable by the gold data are excluded from the evaluation.", "labels": [], "entities": []}, {"text": "Thus, 18.8% of the concepts predicted by NN:Combined for the EWT test set and 14.3% predicted for the Susanne test set are ignored in the evaluation, as they could not have been generated from the original gold annotation, but only from the 'other' tag set (Tab.", "labels": [], "entities": [{"text": "NN", "start_pos": 41, "end_pos": 43, "type": "DATASET", "confidence": 0.8239458203315735}, {"text": "EWT test set", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9814684391021729}, {"text": "Susanne test set", "start_pos": 102, "end_pos": 118, "type": "DATASET", "confidence": 0.96993088722229}]}, {"text": "2) Yet, the precision of these 'alien' concepts can evaluated on the (test set of the) PTB/Susanne intersection with double annotations (PTB\u222aSusa).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9984456896781921}, {"text": "PTB/Susanne intersection", "start_pos": 87, "end_pos": 111, "type": "DATASET", "confidence": 0.8701936602592468}]}, {"text": "The gold data in the test set is the union of PTB and Susa triples for the same word.", "labels": [], "entities": [{"text": "PTB", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.8391199111938477}]}, {"text": "provides overall evaluation results for the conventional taggers as well as the different neural network configurations in terms of concept and path precision on triple-represented annotations of EWT, Susanne and the merged PTBSusanne annotations on the PTB\u222aSusa test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.700210690498352}, {"text": "EWT", "start_pos": 196, "end_pos": 199, "type": "DATASET", "confidence": 0.9841969609260559}, {"text": "Susanne", "start_pos": 201, "end_pos": 208, "type": "DATASET", "confidence": 0.806914746761322}, {"text": "PTBSusanne annotations", "start_pos": 224, "end_pos": 246, "type": "DATASET", "confidence": 0.9411577880382538}, {"text": "PTB\u222aSusa test set", "start_pos": 254, "end_pos": 271, "type": "DATASET", "confidence": 0.9379570603370666}]}, {"text": "In general, path precision is lower than concept precision (Tab. 3).", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.8922541737556458}, {"text": "concept precision", "start_pos": 41, "end_pos": 58, "type": "METRIC", "confidence": 0.731664776802063}]}, {"text": "A likely reason is that tagging errors tend to occur between related POS.", "labels": [], "entities": [{"text": "tagging", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9556230306625366}]}, {"text": "For example, proper nouns are frequently erroneously tagged as common nouns, but concept precision still rewards the common superconcept.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9452874064445496}]}, {"text": "Thus, the higher the granularity of a tag set, the greater the discrepancy between path and concept precision.", "labels": [], "entities": []}, {"text": "The neural network trained only on EWT achieves the best path precision on the EWT test set, outperforming Lapos by almost 3%.", "labels": [], "entities": [{"text": "EWT", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9094831943511963}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9430647492408752}, {"text": "EWT test set", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.973171591758728}]}, {"text": "The neural network trained only on Susanne outperforms the Stanford tagger both bypath and concept precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9049661159515381}]}, {"text": "The neural network trained on both Susanne and EWT fell slightly short of the best tagger in path and concept precision on EWT, but still outperforms the best tagger (Stanford) on the Susanne test set.", "labels": [], "entities": [{"text": "EWT", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9573281407356262}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9765451550483704}, {"text": "EWT", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.9792752861976624}, {"text": "Susanne test set", "start_pos": 184, "end_pos": 200, "type": "DATASET", "confidence": 0.8923118511835734}]}, {"text": "Furthermore, concept precision of the combined neural network on the Susanne data is only 0.3% lower than the precision of the neural network trained on Susanne only.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.8307953476905823}, {"text": "Susanne data", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.969373494386673}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.997344434261322}, {"text": "Susanne", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.9187263250350952}]}, {"text": "Statistics over the most frequent 4 false predictions are given in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9553580582141876}]}, {"text": "4. The first column of Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.8928945958614349}]}, {"text": "4 contains the gold concept, the second column the predicted concept, the third column is the error e g,t for the concept pair g, t, counted as The fourth column of Tab.", "labels": [], "entities": []}, {"text": "4 shows the contribution of e g,t to the total error.", "labels": [], "entities": []}, {"text": "For NN:Combined, the key result is that we achieve a substantial increase in coverage (18.8%, resp.", "labels": [], "entities": [{"text": "coverage", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9979525804519653}]}, {"text": "2) while facing only a marginal drop of precision (around 1%, Tab.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9996671676635742}, {"text": "Tab.", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.904531717300415}]}, {"text": "3) between individually trained neural networks and NN:Combined.", "labels": [], "entities": []}, {"text": "The precision neural network predictions against individual corpora remains constantly high, and also for the merged test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9981583952903748}]}, {"text": "Furthermore, neural networks in any configuration reach state-of-the-art tagger performance; neural networks with structural pruning even outperform it, for both path and concept precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9456257224082947}]}, {"text": "3 shows little -if any -decay of precision if the neural network is trained over heterogeneous annotations of different corpora: In comparison to the best-performing conventional tagger considered (Lapos), NN:Combined (with structural pruning) loses 0.2% in path precision and 0.6% in concept precision, but yields again of 18.8%, resp.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.999452531337738}, {"text": "precision", "start_pos": 263, "end_pos": 272, "type": "METRIC", "confidence": 0.7918809652328491}, {"text": "precision", "start_pos": 293, "end_pos": 302, "type": "METRIC", "confidence": 0.7998046875}]}, {"text": "To our surprise we found that structural pruning -which we initially regarded as being too restrictive -outperforms other decoding strategies, whereas joint corpus pruning showed the lowest precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 190, "end_pos": 199, "type": "METRIC", "confidence": 0.9978531002998352}]}, {"text": "One reason is probably that not all deviations in annotation were eventually compatible, but that some of those mismatches were actual tagging errors, thus propagated into the neural learning algorithm.", "labels": [], "entities": []}, {"text": "Such original annotation errors in the linguistic analyses are possibly the main reason why the performance of the combined network is slightly lower than the performance of networks trained on homogeneous data.", "labels": [], "entities": []}, {"text": "The disjoint corpus pruning suffered less from annotation inconsistency, but its poor performance can probably be attributed to sparsity issues, i.e., rarely attested concept were incorrectly regarded as inconsistent with possible other concepts.: Confusion matrix conceptg are gold standard concepts, ordered by their percentage of the total error total(e).", "labels": [], "entities": []}, {"text": "eg,t is a relative count for conceptg erroneously predicted as conceptt to the total count of conceptg predictions.", "labels": [], "entities": []}, {"text": "It should be noted that our NN setting was deliberately minimalistic: We used minimal context information with the smallest-dimensional word embeddings available, and trivial backpropagation without employing anymore advanced procedures to improve convergency properties (e.g., deep learning).", "labels": [], "entities": []}, {"text": "Also, we did not optimize hyperparameters but followed a simple geometric (pyramidal) structure for their initial assessment.", "labels": [], "entities": []}, {"text": "Despite the lack of any such optimization, we were nevertheless able to prove an increase in coverage while maintaining state-of-the-art precision, thereby proving the feasibility and the potential of ontology-based neural learning over multiple heterogeneously annotated corpora.", "labels": [], "entities": [{"text": "coverage", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.992236316204071}, {"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9985377788543701}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics: tokens , tagsets with  number of POS tags", "labels": [], "entities": []}, {"text": " Table 3: Evaluation: Path and concept precision", "labels": [], "entities": []}]}