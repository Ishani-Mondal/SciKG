{"title": [{"text": "LIMSI @ WMT'15 : Translation Task", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.7199271321296692}, {"text": "Translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.970407247543335}]}], "abstractContent": [{"text": "This paper describes LIMSI's submissions to the shared WMT'15 translation task.", "labels": [], "entities": [{"text": "WMT'15 translation task", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.8244360486666361}]}, {"text": "We report results for French-English, Russian-English in both directions, as well as for Finnish-into-English.", "labels": [], "entities": []}, {"text": "Our submissions use NCODE and MOSES along with continuous space translation models in a post-processing step.", "labels": [], "entities": [{"text": "NCODE", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.9294993281364441}, {"text": "MOSES", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.5909715294837952}]}, {"text": "The main novelties of this year's participation are the following: for Russian-English, we investigate a tailored normalization of Russian to translate into English, and a two-step process to translate first into simplified Russian, followed by a conversion into inflected Rus-sian.", "labels": [], "entities": []}, {"text": "For French-English, the challenge is domain adaptation, for which only mono-lingual corpora are available.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7461823523044586}]}, {"text": "Finally, for the Finnish-to-English task, we explore unsupervised morphological segmentation to reduce the sparsity of data induced by the rich morphology on the Finnish side.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper documents LIMSI's participation to the machine translation shared task for three language pairs: French-English and Russian-English in both directions, as well as Finnish-into-English.", "labels": [], "entities": [{"text": "machine translation shared task", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.855652391910553}]}, {"text": "Each of these tasks poses its own challenges.", "labels": [], "entities": []}, {"text": "For French-English, the task differs slightly from previous years as it considers user-generated news discusssions.", "labels": [], "entities": [{"text": "user-generated news discusssions", "start_pos": 82, "end_pos": 114, "type": "TASK", "confidence": 0.7424368659655253}]}, {"text": "While the domain remains the same, the texts that need to be translated are of a less formal type.", "labels": [], "entities": []}, {"text": "To cope with the style shift, new monolingual corpora have been made available; they represent the only available in-domain resources to adapt statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 143, "end_pos": 180, "type": "TASK", "confidence": 0.7773682872454325}]}, {"text": "For Russian-English, the main source of difficulty is the processing of Russian, a morphologically rich language with a much more complex inflectional system than English.", "labels": [], "entities": []}, {"text": "To mitigate the effects of having too many Russian word forms, we explore ways to normalize Russian prior to translation into English, so as to reduce the number of forms by removing some \"redundant\" morphological information.", "labels": [], "entities": []}, {"text": "When translating into Russian, we consider a two-step scenario.", "labels": [], "entities": []}, {"text": "A conventional SMT system is first built to translate from English into a simplified version of Russian; a postprocessing step then restores the correct inflection wherever needed.", "labels": [], "entities": [{"text": "SMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9935150146484375}]}, {"text": "Finally, for Finnish-into-English, we report preliminary experiments that explore unsupervised morphological segmentation techniques to reduce the sparsity issue induced by the rich morphology of Finnish.", "labels": [], "entities": []}], "datasetContent": [{"text": "This year, the French-English translation task focuses on user-generated News discusssions, a less formal type of texts than the usual News articles of the previous WMT editions.", "labels": [], "entities": [{"text": "French-English translation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6261346191167831}, {"text": "News discusssions", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7469656467437744}, {"text": "WMT editions", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.8075141906738281}]}, {"text": "Therefore, the main challenge for this task is domain adaptation, for which only monolingual data are distributed.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.800384134054184}]}, {"text": "Standard NCODE and MOSES configurations with lexicalized reordering models were used for all the English-Russian and Russian-English experiments.", "labels": [], "entities": [{"text": "NCODE", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.8966826796531677}]}, {"text": "Alignments in both directions were computed with normalized Russian.", "labels": [], "entities": []}, {"text": "The models were tuned with kb-mira using 300-best lists.", "labels": [], "entities": []}, {"text": "The results reported in show a similar trend for NCODE and MOSES in both translation directions.", "labels": [], "entities": [{"text": "NCODE", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7915064096450806}, {"text": "MOSES", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.5500409007072449}]}, {"text": "Note that MOSES outperforms NCODE (+0.72 BLEU) for Ru-En task.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8923523426055908}, {"text": "NCODE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.5225581526756287}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.983333170413971}]}, {"text": "Using normalized Russian as the source language allows us to achieve a slight gain of +0.4 over the baseline for both systems.", "labels": [], "entities": []}, {"text": "Moreover, the addition of SOUL models yields a further improvement of 1.1 BLEU score (see).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9785585999488831}]}, {"text": "The English-into-normalizedRussian task has been performed for the sake of comparison, to assess the gain we could expect if we were able to always predict the right case for the normalized Russian output.", "labels": [], "entities": []}, {"text": "The comparison of BLEU scores between translating directly into Russian and producing an intermediate normalized Russian shows differences of 3.15 BLEU for NCODE and 3.44 BLEU for MOSES.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9990161657333374}, {"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9990666508674622}, {"text": "NCODE", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.8841211795806885}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9988958835601807}, {"text": "MOSES", "start_pos": 180, "end_pos": 185, "type": "DATASET", "confidence": 0.6567520499229431}]}, {"text": "These scores represent an upper-bound that unfortunately we were notable to reach with our post-processing scheme.: Results (BLEU) for English-Russian (Rx stands for normalized Russian) with NCODE and MOSES on the official test.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.968364804983139}, {"text": "NCODE", "start_pos": 191, "end_pos": 196, "type": "DATASET", "confidence": 0.9094271659851074}, {"text": "MOSES", "start_pos": 201, "end_pos": 206, "type": "METRIC", "confidence": 0.7838848233222961}]}, {"text": "The score for En-Rx was obtained over the normalized test.", "labels": [], "entities": [{"text": "En-Rx", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.8650034666061401}]}], "tableCaptions": [{"text": " Table 1: Statistical description of the training corpora", "labels": [], "entities": []}, {"text": " Table 2: Results (BLEU) for keeping the top 10%,  25% or 50% of the bi-sentences scored with MML,  before and after word alignment. The baseline sys- tem uses all the bilingual data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9987112283706665}, {"text": "word alignment", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.7112141251564026}]}, {"text": " Table 3: Results (BLEU) with and without the ad- ditional in-domain language model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9983477592468262}]}, {"text": " Table 4: Results (BLEU) for NCODE and MOSES  on respectively the in-house and official test set.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9601130485534668}, {"text": "NCODE", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.8447580337524414}, {"text": "MOSES", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.43029695749282837}]}, {"text": " Table 5: Reranking results (BLEU) using differ- ent feature sets individually and their combination.  For the all configurations these features are in- troduced during a reranking step.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9129171967506409}]}, {"text": " Table 6: Results (BLEU) for Russian-English  with NCODE and MOSES on the official test.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.963493674993515}, {"text": "NCODE", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8753414154052734}, {"text": "MOSES", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9490582346916199}]}, {"text": " Table 7: Results (BLEU) for English-Russian (Rx  stands for normalized Russian) with NCODE and  MOSES on the official test. The score for En-Rx  was obtained over the normalized test.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9980023503303528}, {"text": "NCODE", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.7924790382385254}, {"text": "MOSES", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9441387057304382}]}, {"text": " Table 8: BLEU scores for the Finnish to English  translation task, obtained with different configura- tions after a two-fold cross-validation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995038509368896}, {"text": "Finnish to English  translation task", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.6695464491844177}]}]}