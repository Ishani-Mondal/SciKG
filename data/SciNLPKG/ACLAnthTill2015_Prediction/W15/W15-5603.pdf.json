{"title": [{"text": "Comparative Analysis between Notations to Classify Named Entities using Conditional Random Fields", "labels": [], "entities": [{"text": "Comparative Analysis", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6893629431724548}]}], "abstractContent": [{"text": "Conditional Random Fields (CRF) is a probabilistic Machine Learning (ML) method based on structured prediction.", "labels": [], "entities": [{"text": "Conditional Random Fields (CRF)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6980549395084381}, {"text": "structured prediction", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7212701737880707}]}, {"text": "It has been applied in several areas, such as Natural Language Processing (NLP), image processing, computer vision, and bioinformatics.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7061598102251688}, {"text": "image processing", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7787427604198456}]}, {"text": "In this paper we analyse two different notations for identifying the words that compose a Named Entity (NE): BILOU and IO.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.9983900785446167}, {"text": "IO", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.933436930179596}]}, {"text": "We found out that IO notation presents better results in F-measure than BILOU notation in all categories of HAREM corpus.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.952370822429657}, {"text": "BILOU", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9969691634178162}, {"text": "HAREM corpus", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.8364582061767578}]}], "introductionContent": [{"text": "NER is the task of identifying Named Entities (NEs), mostly proper nouns, from free texts and to classify them within a set of pre-defined categories that includes Person, such as \"Carlos Ribeiro\"; and Place, such as \"Porto Alegre\".", "labels": [], "entities": []}, {"text": "NER has been largely applied in texts through methods such as supervised learning to classify addition to the above categories, also, diseases and genes in the abstracts of the medical field.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9614342451095581}]}, {"text": "Labeled data and a set of automatically extracted features are used to train models, such as Maximum Entropy Markov Models (MEMMs) or CRF].", "labels": [], "entities": []}, {"text": "The key difference between CRF and MMEMs is that MMEMs use exponential models by states for conditional probabilities of upcoming states, considering the current state.", "labels": [], "entities": []}, {"text": "Within this context, the method chosen for this study was CRF, that was evaluated in previous studies for this task.", "labels": [], "entities": [{"text": "CRF", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.5297214388847351}]}, {"text": "Different notations are used to annotate data for the NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 54, "end_pos": 62, "type": "TASK", "confidence": 0.9293349385261536}]}, {"text": "In previous studies, we used BILOU.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9921636581420898}]}, {"text": "This notation demarcates the NEs as follows: B (Begin), I (Inside), L (Last), O (Outside) and U (Unit), indicating the beginning, continuation and end of a compound NE, or whether the word does not refer to a NE or refers to an unit NE.", "labels": [], "entities": []}, {"text": "The IO notation] is a simpler alternative.", "labels": [], "entities": []}, {"text": "It defines whether a word is a NE or not I (Inside) or O (Outside), respectively.", "labels": [], "entities": []}, {"text": "Therefore, this paper presents a comparative study, which consists in two different notations for identifying the words that compose a Named Entity (NE): BILOU and IO.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 154, "end_pos": 159, "type": "METRIC", "confidence": 0.9975405931472778}, {"text": "IO", "start_pos": 164, "end_pos": 166, "type": "METRIC", "confidence": 0.8228587508201599}]}, {"text": "This article is structured as follows: Section 2 presents Related Work.", "labels": [], "entities": []}, {"text": "Section 3 describes the development of the NERP-CRF system.", "labels": [], "entities": []}, {"text": "Section 4 presents the evaluation process and the results we obtained.", "labels": [], "entities": []}, {"text": "Section 5 points to the conclusions and further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results from the experiments were obtained according to the metrics: Precision, Recall and F-Measure [Mota and.", "labels": [], "entities": [{"text": "Precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9988186955451965}, {"text": "Recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.998274564743042}, {"text": "F-Measure", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9986905455589294}]}, {"text": "Therefore, this evaluation aims to find the most appropriate annotation to the NER task in the HAREM corpus.", "labels": [], "entities": [{"text": "NER task", "start_pos": 79, "end_pos": 87, "type": "TASK", "confidence": 0.9066031277179718}, {"text": "HAREM corpus", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.8076598942279816}]}, {"text": "Our model has been demonstrating good results in comparison with other methods that use machine learning for the NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 113, "end_pos": 121, "type": "TASK", "confidence": 0.9140793979167938}]}, {"text": "Four experiments were carried out using the NERP-CRF system.", "labels": [], "entities": [{"text": "NERP-CRF", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.7175367474555969}]}, {"text": "For training, they all operated with the GC of the First HAREM, which encompasses 129 texts, and, for testing, with the GC of the Second HAREM, formed by over 129 texts.", "labels": [], "entities": []}, {"text": "The two sets total 258 texts and approximately 237.232 words.", "labels": [], "entities": []}, {"text": "The experiments differ from one another because of the following characteristics: Experiment 1: uses the BILOU notation and classifies the NEs according to the ten categories of HAREM; Experiment 2: uses the IO notation and classifies the NEs according to the ten categories of HAREM; Experiment 3: uses the BILOU notation and classifies the NEs in the categories Person, Organization, Place and Other.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9937415719032288}, {"text": "BILOU", "start_pos": 308, "end_pos": 313, "type": "METRIC", "confidence": 0.9701541066169739}]}, {"text": "These categories were chosen due to the fact that they have been more widely studied within the field of IE [Weber and Vieira 2014] Experiment 4: uses the IO notation and classifies the NEs according to the same categories of experiment 3.", "labels": [], "entities": [{"text": "IE", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9218292832374573}]}, {"text": "summarizes the performance of the ten categories with the BILOU and IO notations, respectively.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9978384375572205}, {"text": "IO", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9348176717758179}]}, {"text": "When comparing Experiments 1 and 2, it is found that NERP-CRF presented better results for the ten categories with the IO notation.", "labels": [], "entities": []}, {"text": "The highlight is for the category Event, which went from 14.347% to 19.745% in the F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9418650269508362}]}, {"text": "The IO notation contributed for that class to become more comprehensive and precise.", "labels": [], "entities": []}, {"text": "Experiments 3 and 4 were carried out to seethe learning behavior when the number of categories was reduced.", "labels": [], "entities": []}, {"text": "shows the performance of the BILOU and IO notations in the classification of NEs with the categories: Person, Place, Organization and Other.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9933649301528931}]}, {"text": "Again, there was a percentage increase of the F-measure when NERP-CRF identified them with the IO notation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9980955719947815}]}, {"text": "Only the category Place kept a very similar value.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Results of NERP-CRF for Experiments 1 and 2.", "labels": [], "entities": []}, {"text": " Table 3. Results of NERP-CRF for Experiments 3 and 4.", "labels": [], "entities": [{"text": "NERP-CRF", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.5250566005706787}]}]}