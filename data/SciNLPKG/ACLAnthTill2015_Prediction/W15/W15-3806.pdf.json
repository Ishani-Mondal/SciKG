{"title": [{"text": "Shallow Training is cheap but is it good enough? Experiments with Medical Fact Coding", "labels": [], "entities": [{"text": "Medical Fact Coding", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7517611483732859}]}], "abstractContent": [{"text": "A typical NLP system for medical fact coding uses multiple layers of supervision involving fact-attributes, relations and coding.", "labels": [], "entities": [{"text": "medical fact coding", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.610623836517334}]}, {"text": "Training such a system involves expensive and laborious annotation process involving all layers of the pipeline.", "labels": [], "entities": []}, {"text": "In this work, we investigate the feasibility of a shallow medical coding model that trains only on fact annotations, while disregarding fact-attributes and relations, potentially saving considerable annotation time and costs.", "labels": [], "entities": []}, {"text": "Our results show that the shallow system, despite using less supervision, is only 1.4% F1 points behind the multi-layered system on Disorders, and contrary to expectation, is able to improve over the latter by about 2.4% F1 points on Procedure facts.", "labels": [], "entities": [{"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9996083378791809}, {"text": "F1", "start_pos": 221, "end_pos": 223, "type": "METRIC", "confidence": 0.9985264539718628}]}, {"text": "Further, our experiments also show that training the shallow system using only sentence-level fact labels with no span information has no negative effect on performance, indicating further cost savings through weak supervision.", "labels": [], "entities": []}], "introductionContent": [{"text": "Medical fact coding is the joint task of recognizing the occurrences of medical facts from electronic patient medical records expressed in natural language, and linking each occurrence of a fact to a specific code in a medical taxonomy such as SNOMED . A representative sentence from a medical record along with its annotated facts is shown in.", "labels": [], "entities": [{"text": "Medical fact coding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5850742161273956}, {"text": "recognizing the occurrences of medical facts from electronic patient medical records expressed in natural language", "start_pos": 41, "end_pos": 155, "type": "TASK", "confidence": 0.5650782962640126}]}, {"text": "In the parlance of traditional natural language processing, this task is roughly equivalent to the tasks of named-entity recognition () and entity-linking 2 rolled into one.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.7471013963222504}]}, {"text": "Several open evaluations such as ShARe-CLEF ( and) have been run recently to address the twin problems of fact recognition (recognizing occurrences of medical facts in text) and fact-coding (linking each occurrence of a fact to a pre-assigned code).", "labels": [], "entities": [{"text": "fact recognition", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7149171084165573}]}, {"text": "These evaluations report performance numbers on both the tasks separately.", "labels": [], "entities": []}, {"text": "Often times, facts that occur in a medical text may not correspond to any pre-assigned codes, and are referred to as CUI-less facts in the Semeval evaluation.", "labels": [], "entities": []}, {"text": "In the aforementioned evaluations, the systems are expected to output and are evaluated against CUI-less facts as well.", "labels": [], "entities": []}, {"text": "However, in typical end-user applications such as medical billing, one does not care about the occurrences of unrecognized, non-billable facts.", "labels": [], "entities": [{"text": "medical billing", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7285692095756531}]}, {"text": "This work is targeted at such end applications where discovering only the occurrences of fact-codes recognized by a medical taxonomy is desirable.", "labels": [], "entities": []}, {"text": "Consequently, CUI-less facts are ignored in our evaluation framework.", "labels": [], "entities": []}, {"text": "In this work, we will focus only on the fact types of Disorders and Procedures, and use SNOMED as our medical taxonomy.", "labels": [], "entities": []}, {"text": "We also use Linkbase 4 as our knowledge-base for descriptions of the fact codes.", "labels": [], "entities": [{"text": "Linkbase 4", "start_pos": 12, "end_pos": 22, "type": "DATASET", "confidence": 0.9456416070461273}]}], "datasetContent": [{"text": "For tuning and developing our model, we used medical reports from an institution called Integris, which are partitioned into training and test sets.", "labels": [], "entities": []}, {"text": "We tuned our model only on Disorder facts and evaluated them on both Disorders and Procedures.", "labels": [], "entities": []}, {"text": "For evaluating the model on Disorders, we used another dataset from multiple institutions with its own train and test partitions which we call the Multi-inst dataset.", "labels": [], "entities": [{"text": "Multi-inst dataset", "start_pos": 147, "end_pos": 165, "type": "DATASET", "confidence": 0.7901540994644165}]}, {"text": "For evaluating procedures, we used a dataset consisting of Procedure Notes documents with its own train and test partitions.", "labels": [], "entities": []}, {"text": "The statistics of the datasets are summarized in.", "labels": [], "entities": []}, {"text": "The results of our experiments are summarized in.", "labels": [], "entities": []}, {"text": "The shallow coding model is only about 1.4% F1 points behind the traditional multilayered supervised model on Disorder facts, making it attractive for situations where cost savings are critical.", "labels": [], "entities": [{"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9994470477104187}]}, {"text": "On the more complex medical facttypes of Procedures, the shallow coding system outperforms the multi-layered system by 2.4 % F1 points.", "labels": [], "entities": [{"text": "F1", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9990842342376709}]}, {"text": "The fact that Procedure facts are harder is evident from the performance numbers of either system on Procedures compared with those on Disorders.", "labels": [], "entities": []}, {"text": "A a few example Procedure facts, along with their attribute level annotations are displayed in.", "labels": [], "entities": []}, {"text": "On complex fact-types involving long distance relations between the attributes, errors accumulate over the layers of the multi-layered system resulting in poorer performance.", "labels": [], "entities": []}, {"text": "In such a scenario, the shallow model maybe more attractive.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A sample of the database of fact codes and their", "labels": [], "entities": []}, {"text": " Table 3: Contribution of the two classifiers and the rejection", "labels": [], "entities": [{"text": "rejection", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.5096883177757263}]}, {"text": " Table 4: Statistics of the datasets and corresponding fact-", "labels": [], "entities": []}, {"text": " Table 5: Performance comparison: the shallow coding sys-", "labels": [], "entities": []}]}