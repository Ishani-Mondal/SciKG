{"title": [], "abstractContent": [{"text": "Argumentation mining obviously involves finding support relations between statements, but many interesting instances of argumen-tation also contain counter-considerations, which the author mentions in order to preempt possible objections by the readers.", "labels": [], "entities": [{"text": "Argumentation mining", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8405173420906067}]}, {"text": "A counter-consideration in monologue text thus involves a switch of perspective toward an imaginary opponent.", "labels": [], "entities": []}, {"text": "We present a classification approach to classifying counter-considerations and apply it to two different corpora: a selection of very short argumentative texts produced in a text generation experiment, and a set of newspaper commentaries.", "labels": [], "entities": []}, {"text": "As expected, the latter pose more difficulties, which we investigate in a brief error anaylsis.", "labels": [], "entities": []}], "introductionContent": [{"text": "The exchange of argument and objection is obviously most typical for dialogue, but to a good extent it is also present in monologue text: Authors do not only provide justifications for their own position -they can also mention potential objections and then refute or outweigh them.", "labels": [], "entities": []}, {"text": "In this way they demonstrate to have considered the position of \"the other side\", which altogether is designed to reinforce their own position.", "labels": [], "entities": []}, {"text": "We use the term 'counterconsideration' in a general sense to coverall such moves of an author, no matter whether they are directed at the conclusion of the text or at an intermediate argument, or at some support relation, and irrespective of whether they are explicitly refuted by the author or merely mentioned and left outweighed by the mass of arguments in favour of the main claim.", "labels": [], "entities": []}, {"text": "For an author, presenting a counter-consideration involves a switch of perspective by temporarily adopting the opposing viewpoint and then moving back to one's own.", "labels": [], "entities": []}, {"text": "This is a move that generally requires some form of explicit linguistic marking so that the reader can follow the line of argumentation.", "labels": [], "entities": []}, {"text": "The kinds of marking include explicit belief attribution followed by a contrastive connective signaling the return (\"Some people think that X.", "labels": [], "entities": []}, {"text": "However, this ...\"), and there can also be quite compact mentions of objections, as in \"Even though the project is expensive, we need to pursue it, because...\"", "labels": [], "entities": []}, {"text": "Detecting counter-considerations is thus a subtask of argumentation mining.", "labels": [], "entities": [{"text": "Detecting counter-considerations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8607179522514343}, {"text": "argumentation mining", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.9284257590770721}]}, {"text": "It involves identifying the two points of perspective switching, which we henceforth calla move from the proponent role to the opponent role and back.", "labels": [], "entities": [{"text": "perspective switching", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7737714648246765}]}, {"text": "Thus the task can be operationalized as labelling segments of argumentative text in terms of these two roles.", "labels": [], "entities": []}, {"text": "Then, counterconsiderations are segments labeled as \"opponent\".", "labels": [], "entities": []}, {"text": "We study this classification problem using two different corpora: a colletion of user-generated short \"microtexts\", where we expect the task to be relatively easy, and a set of argumentative newspaper pieces that explicitly argue in favour of or against a particular position ('ProCon').", "labels": [], "entities": [{"text": "ProCon", "start_pos": 278, "end_pos": 284, "type": "DATASET", "confidence": 0.9318154454231262}]}, {"text": "These texts are longer and more complex, and the opponent role can be encoded in quite subtle ways, so that we expect the classification to be more difficult.", "labels": [], "entities": [{"text": "classification", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.9572249054908752}]}, {"text": "After looking at related work, Section 3 describes our corpora and the machine learning experiments.", "labels": [], "entities": []}, {"text": "In Section 4, we evaluate the results and discuss the most common problems with the ProCon texts, and Section 5 concludes.", "labels": [], "entities": [{"text": "ProCon texts", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9812031984329224}]}], "datasetContent": [{"text": "Feature sets We compare three different feature sets: two simple bag-of-word models as baselines and one model with additional features from automatic linguistic analysis.", "labels": [], "entities": []}, {"text": "The first model (B) only extracts binary features for each lemma occurring in the target segment.", "labels": [], "entities": []}, {"text": "The second model (B+C) additionally extracts these features from the preceding and the subsequent segment, thus providing a small context window.", "labels": [], "entities": []}, {"text": "The full model (B+C+L) adds parsing-based features for the whole context window, such as pos-tags, lemma-and postag-based dependency-parse triples, the morphology of the main verb, as well as lemmabigrams.", "labels": [], "entities": []}, {"text": "Discourse connectives are taken from a list by and used both as individual items and as indicating a coherence relation (Cause, Contrast, etc.).", "labels": [], "entities": []}, {"text": "Furthermore, we use some positional statistics such as relative segment position, segment length, and punctuation count.", "labels": [], "entities": [{"text": "punctuation count", "start_pos": 102, "end_pos": 119, "type": "METRIC", "confidence": 0.8719839751720428}]}, {"text": "Approach The goal is to assign the labels 'proponent' and 'opponent' to the individual segments.", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9869714379310608}]}, {"text": "We trained a linear log-loss model using stochastic gradient descent learning as implemented in the Scikit learn library).", "labels": [], "entities": [{"text": "Scikit learn library", "start_pos": 100, "end_pos": 120, "type": "DATASET", "confidence": 0.7420370777448019}]}, {"text": "The learning rate is set to optimal decrease, and the class weights are adjusted according to class distribution.", "labels": [], "entities": []}, {"text": "We used a nested 5x3 cross validation (CV), with the inner CV for tuning the hyper parameters (the regularization parameter alpha and the number of best features to select) and the outer CV for evaluation.", "labels": [], "entities": []}, {"text": "We optimize macro averaged F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9274070262908936}]}, {"text": "The folding is stratified, randomly distributing the texts of the corpus while aiming to reproduce the overall label distribution in both training and test set.", "labels": [], "entities": []}, {"text": "All results are reported as average and standard deviation over the 50 folds resulting from 10 iterations of 5-fold cross validation.", "labels": [], "entities": [{"text": "standard", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9714275598526001}]}, {"text": "We use the following metrics: Cohen's Kappa \u03ba, Macro average F1, Precision, Recall and F1 for the opponent class.", "labels": [], "entities": [{"text": "Macro average F1", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.7504676977793375}, {"text": "Precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9974773526191711}, {"text": "Recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9935451745986938}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9993909597396851}]}, {"text": "Results The performance of the classifiers is shown in.", "labels": [], "entities": []}, {"text": "Comparing the results for the two datasets confirms our assumption that the task is much harder on the ProCon texts.", "labels": [], "entities": [{"text": "ProCon texts", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.9841947853565216}]}, {"text": "When comparing the different models, we observe that the simple baseline model without context performs poorly; adding context improves the results significantly.", "labels": [], "entities": []}, {"text": "The full featureset (B+C+L) always yields best results, except fora small drop of precision on the ProCon texts.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.998906135559082}, {"text": "ProCon texts", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.985711932182312}]}, {"text": "The improvement of the full model over B+C is significant for the microtexts (p < 0.003 for \u03ba, F1 macro and opponent F1, using Wilcoxon signed-rank test over the 50 folds), but not significant for the ProCon texts.", "labels": [], "entities": [{"text": "ProCon texts", "start_pos": 201, "end_pos": 213, "type": "DATASET", "confidence": 0.9865891933441162}]}, {"text": "Feature selection mostly supports the classification of the ProCon texts, where the mass of extracted features impairs the generalization.", "labels": [], "entities": [{"text": "ProCon texts", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9794866144657135}]}, {"text": "Typically only 25 features were chosen.", "labels": [], "entities": []}, {"text": "For the microtexts, reducing the features to the 50 best-performing ones still yields good but not the best results.", "labels": [], "entities": []}, {"text": "One reason for the difference in feature selection behaviour between the datasets might be that the proportion of proponent and opponent labels is more skewed for the ProCons than for the microtexts.", "labels": [], "entities": [{"text": "ProCons", "start_pos": 167, "end_pos": 174, "type": "DATASET", "confidence": 0.9642033576965332}]}, {"text": "Another reason might be the richer set of expressions marking the role switch in the ProCon texts.", "labels": [], "entities": [{"text": "ProCon texts", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.9780680239200592}]}, {"text": "A common observation for both corpora is that the connective aber ('but') in the subsequent segment is the best predictor for an opponent role.", "labels": [], "entities": []}, {"text": "Other important lexical items (also as part of dependency triples) are the modal particles nat\u00fcrlich ('of course', 'naturally') and ja (here in the reading: 'as is well-known'), and the auxiliary verb m\u00f6gen (here: 'may').", "labels": [], "entities": []}, {"text": "All of these occur in the opponent role segment itself, and they have in common that they \"color\" a statement as something that the author concedes (but will overturn in the next step), which corresponds to the temporary change of perspective.", "labels": [], "entities": []}, {"text": "As for differences between the corpora, we find that the connective zwar, which introduces a concessive minor clause, is very important in the microtexts but less prominent in ProCon.", "labels": [], "entities": [{"text": "ProCon", "start_pos": 176, "end_pos": 182, "type": "DATASET", "confidence": 0.9445324540138245}]}, {"text": "We attribute this to the microtext instruction of writing rather short texts, which supposedly leads the students to often formulating their counter-considerations as compact minor clauses, for which zwar ('granted that') is the perfect marker.", "labels": [], "entities": []}, {"text": "Presumably for the same reason, we observe that the concessive subordinator obwohl ('although') is among the top-10 features for microtexts but not even among the top-50 for ProCon.", "labels": [], "entities": [{"text": "ProCon", "start_pos": 174, "end_pos": 180, "type": "DATASET", "confidence": 0.9678385257720947}]}, {"text": "In ProCon, the group of connectives indicating the Contrast coherence relation is a very good feature, and it is absent from the microtext top-50; recall, though, that the single connective aber ('but') is their strongest predictor, and the very similar doch is also highly predictive.", "labels": [], "entities": [{"text": "ProCon", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.9471120238304138}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics: For details see Section 3.1.", "labels": [], "entities": []}, {"text": " Table 2: Results for role-identification, reported as average and standard deviation", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.8950002193450928}]}]}