{"title": [], "abstractContent": [{"text": "Crowdsourcing platforms area popular choice for researchers to gather text annotations quickly at scale.", "labels": [], "entities": []}, {"text": "We investigate whether crowdsourced annotations are useful when the labeling task requires medical domain knowledge.", "labels": [], "entities": [{"text": "labeling task", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.8999155461788177}]}, {"text": "Comparing a sentence classification model trained with expert-annotated sentences to the same model trained on crowd-labeled sentences, we find the crowdsourced training data to be just as effective as the manually produced dataset.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7048794627189636}]}, {"text": "We can improve the accuracy of the crowd-fueled model without collecting further labels by filtering out worker labels applied with low confidence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9991809725761414}]}], "introductionContent": [{"text": "Most text classification methods are based on supervised machine learning models that require large amounts of labeled training data.", "labels": [], "entities": [{"text": "text classification", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7946966886520386}]}, {"text": "Gathering a large amount of high-quality training data can be time-consuming and expensive.", "labels": [], "entities": []}, {"text": "To streamline the process, natural language processing (NLP) researchers have employed crowdsourcing platforms to quickly collect crowdsourced annotations at scale (.", "labels": [], "entities": []}, {"text": "In some NLP problems, the annotation task requires some degree of common linguistic knowledge that most non-experts are assumed to have.", "labels": [], "entities": []}, {"text": "By examining the accuracy of crowdsourced data and its usefulness in training models to perform common NLP tasks, previous research has shown that deficiencies in individual crowd worker accuracy can be overcome by taking consensus votes over multiple annotators or weighting the votes of annotators based on their overall performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9949423670768738}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9401631951332092}]}, {"text": "But how useful is crowdsourcing when the annotation task requires domain knowledge beyond common knowledge?", "labels": [], "entities": []}, {"text": "One example is interpretation of medical data.", "labels": [], "entities": [{"text": "interpretation of medical", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.9150843421618143}]}, {"text": "As hospitals transition to electronic patient records, there are increasingly more data than medical experts have time to manually annotate.", "labels": [], "entities": []}, {"text": "If crowdsourced medical annotations prove to be mostly accurate, it will accelerate research on using machine learning methods to support medical decisions.", "labels": [], "entities": []}, {"text": "Previous research has suggested that crowdsourced non-experts are capable of identifying distinct patterns of activity in electroencephalography readings () and predicting native protein structures).", "labels": [], "entities": []}, {"text": "To our knowledge there has been less work in using unscreened, crowdsourced workers to complete text labeling tasks that require comprehension of medical concepts.", "labels": [], "entities": [{"text": "text labeling tasks", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7695205609003702}]}, {"text": "Consider the task of determining whether these excerpts from a radiology report describe a normal or abnormal observation of the anatomical structure in parentheses: 1 \u2022 The mastoid air cells are well-pneumatized.", "labels": [], "entities": []}, {"text": "(mastoid) \u2022 Bilateral dysplastic vestibules and lateral semicircular canals.", "labels": [], "entities": []}, {"text": "(semicircular canal) \u2022 The external auditory canal is patent.", "labels": [], "entities": []}, {"text": "(EAC) Labeling some of these sentences might require a non-expert to do additional research.", "labels": [], "entities": [{"text": "EAC) Labeling", "start_pos": 1, "end_pos": 14, "type": "TASK", "confidence": 0.6524527668952942}]}, {"text": "(e.g. Should a mastoid air cell be pneumatized? Does lateral describe the condition of the semicircular canal, or is lateral semicircular canal a compound noun?)", "labels": [], "entities": []}, {"text": "In this work, we extend the study of crowdsourcing annotations to text-labeling tasks that require domain knowledge.", "labels": [], "entities": []}, {"text": "Specifically, we examine the usefulness of crowdsourced data for training models to classify radiology report sentences as normal or abnormal as in the examples above.", "labels": [], "entities": []}, {"text": "By comparing the performance of classification models trained on expert-generated and crowdsourced data sets, we show that crowdsourcing enables us to build supervised models without sacrificing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9832617044448853}]}, {"text": "Additionally, we show that as gains inaccuracy achieved by increasing the training set size level off, we can further improve the accuracy of our classifier -without gathering additional training data -by incorporating worker confidence votes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9989129304885864}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Crowd accuracy by confidence rating.  'In-class' accuracy gives the percent of crowd la- bels with that exact confidence rating that matched  the gold standard label; 'threshold' refers to the  percent of labels with the same or more confident  rating that matched the gold standard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9853996634483337}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.5751298666000366}]}]}