{"title": [], "abstractContent": [{"text": "Translations generated by current statistical systems often have a large variance, in terms of their quality against human references.", "labels": [], "entities": []}, {"text": "To cope with such variation, we propose to evaluate translations using a multi-level framework.", "labels": [], "entities": []}, {"text": "The method varies the evaluation criteria based on the clusters to which a translation belongs.", "labels": [], "entities": []}, {"text": "Our experiments on the WMT metric task data show that the multi-level framework consistently improves the performance of two benchmarking metrics, resulting in better correlation with human judgment.", "labels": [], "entities": [{"text": "WMT metric task data", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.6451336592435837}]}], "introductionContent": [{"text": "The aims of automatic Machine Translation (MT) evaluation metrics, which measure the quality of translations against human references, are twofold.", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.8645273943742117}]}, {"text": "Firstly, they enable rapid comparisons between different statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.7730945895115534}]}, {"text": "Secondly, they are necessary to the tuning of parameter values during system trainings.", "labels": [], "entities": []}, {"text": "To attain these goals, many machine translation metrics have been introduced in recent years.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7969621121883392}]}, {"text": "For example, metrics such as BLEU (), NIST, and TER () rely on word n-gram surface matching.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9950917959213257}, {"text": "NIST", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8236058354377747}, {"text": "TER", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.98711758852005}, {"text": "word n-gram surface matching", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.5891468301415443}]}, {"text": "Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and) and its extensions, TER-Plus (, and TESLA ().", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7708663940429688}]}, {"text": "In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM ( of their quality against human references.", "labels": [], "entities": []}, {"text": "As a result, current metrics often perform better fora portion of translations but worse against the others.", "labels": [], "entities": []}, {"text": "Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLUE.", "labels": [], "entities": [{"text": "BLUE", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9915812611579895}]}, {"text": "Figure 1 depicts the distributions of the two metrics' evaluation scores, computed on system outputs for two WMT test sets, i.e., the newstest2013.fr-en and newstest2012.en-cs.", "labels": [], "entities": [{"text": "WMT test sets", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.8380968372027079}]}, {"text": "As shown in Figures 1, the variances of the created evaluation scores are large across evaluation metrics as well as test sets.", "labels": [], "entities": []}, {"text": "Such widely varying evaluation quality, however, maybe clustered into multiple sub-regions, as illustrated in.", "labels": [], "entities": []}, {"text": "Here, we sample 300 sentences from the system output of the newstest2013.fr-en test set; we depict the Fmeasure based on dependency triplet (dependency type, governor word, and dependent word) on the Y-axis against the word-based F-measure on the X-axis.", "labels": [], "entities": [{"text": "newstest2013.fr-en test set", "start_pos": 60, "end_pos": 87, "type": "DATASET", "confidence": 0.9412058393160502}, {"text": "Fmeasure", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9532438516616821}]}, {"text": "We observe a straight line at the bottom left corner (blue box) of the graph represent-ing sentences which all have dependency triplet Fscore of zero; if we want to distinguish between them in terms of their quality score, we must rely on word matching rather than on syntax.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.5270006656646729}, {"text": "word matching", "start_pos": 239, "end_pos": 252, "type": "TASK", "confidence": 0.7125470340251923}]}, {"text": "The situation in the upper right corner (green box) of the graph is quite different.", "labels": [], "entities": []}, {"text": "Here, the word-based Fmeasure and dependency-based F-measure have a roughly linear correlation, suggesting that a combination of word-based and syntactic information might be a better measure of quality than either alone.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.6466370224952698}, {"text": "F-measure", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.7818962335586548}]}, {"text": "These observations imply that a metric may benefit from applying different sources of information at different quality levels.", "labels": [], "entities": []}, {"text": "In this paper, we propose a multi-level automatic evaluation framework for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9917739033699036}]}, {"text": "Our strategy first roughly classifies the translations into different quality levels.", "labels": [], "entities": []}, {"text": "Next, it rates the translations by exploiting several different information sources, with the weight on each source depending on its quality level.", "labels": [], "entities": [{"text": "translations", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.9577182531356812}]}, {"text": "We apply our method to two metrics: the Meteor and anew metric, DREEM, which is based on distributed representations.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.915812611579895}, {"text": "DREEM", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.8886899948120117}]}, {"text": "Our experiments on the WMT metric task data show that the multi-level framework consistently improves the performance of these two metrics.", "labels": [], "entities": [{"text": "WMT metric task data", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.6785119399428368}]}], "datasetContent": [{"text": "The multi-level evaluation framework works on the sentence level.", "labels": [], "entities": []}, {"text": "Specifically, we first assign each test sentence to one of the three categories: low-, medium-, or high-quality translations.", "labels": [], "entities": []}, {"text": "Next, we evaluate the translations within each category with a tailored set of weights of the metric on the information sources.", "labels": [], "entities": []}, {"text": "To this end, we deploy a simple strategy for the category clustering.", "labels": [], "entities": [{"text": "category clustering", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7133994400501251}]}, {"text": "Note that more sophisticate strategies could be deployed; we leave this to our future work.", "labels": [], "entities": []}, {"text": "Here, we first use a scoring function to compute a score between the translation and its reference.", "labels": [], "entities": []}, {"text": "Next, the category assignment of the translation is then determined by a pre-defined score threshold.", "labels": [], "entities": []}, {"text": "In detail, suppose we have a translation (t) and its reference (r).", "labels": [], "entities": []}, {"text": "The multi-level metric scores the translation pair as follows.", "labels": [], "entities": []}, {"text": "where M (t, r, w) is a metric, w is the weight, F (t, r) is the simple classification scoring function.", "labels": [], "entities": [{"text": "F", "start_pos": 48, "end_pos": 49, "type": "METRIC", "confidence": 0.9961322546005249}]}, {"text": "Also, \u03b8 is a threshold, and its value is automatically tuned on development data set.", "labels": [], "entities": [{"text": "development data set", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.797002891699473}]}, {"text": "For the classification function, we employ a formula which combines word-based F-measure (denoted as F W (t, r)) and a F-measure (denoted as FD (t, r)) based on dependency triplet (dependency type, governor word, dependent word), as follows: where the free parameter \u03bb is tuned on development data.", "labels": [], "entities": [{"text": "F W", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9247688353061676}, {"text": "FD", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9454313516616821}]}, {"text": "It is worth noting that, for languages which dependency parser is not available, we only use the word-based F-measure as the classification function.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.8372196555137634}]}, {"text": "Specifically, we use Equation 1 for IntoEnglish task, and the word-based F-measure for Out-of-English task in this paper.", "labels": [], "entities": [{"text": "Equation 1", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9706560671329498}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9119865298271179}]}, {"text": "Ina scenario where there are multiple references, we compute the score with each reference, then choose the highest one.", "labels": [], "entities": []}, {"text": "In addition, we treat the document-level score as the weighted average of sentence-level scores, with the weights being the reference lengths, as follows.", "labels": [], "entities": []}, {"text": "where Score i is the score of sentence i, and Dis the number of sentences in the document.", "labels": [], "entities": [{"text": "Score", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9600456357002258}, {"text": "Dis", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9894888997077942}]}, {"text": "We apply our multi-level approach to two metrics.", "labels": [], "entities": []}, {"text": "The first one is Meteor (), which has been widely used for machine translation evaluations.", "labels": [], "entities": [{"text": "machine translation evaluations", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.8898415962855021}]}, {"text": "The second one is DREEM, anew metric based on distributed representations generated by deep neural networks.", "labels": [], "entities": [{"text": "DREEM", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.8210475444793701}]}], "tableCaptions": [{"text": " Table 2: Correlations with human judgment on the WMT", "labels": [], "entities": [{"text": "WMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.4913489520549774}]}, {"text": " Table 3: The value of parameter \u03b1 in multi-level Meteor.", "labels": [], "entities": []}, {"text": " Table 4: The weights of each representation in the multi-", "labels": [], "entities": []}]}