{"title": [{"text": "Exploring the effect of semantic similarity for Phrase-based Machine Translation", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.8928131262461344}]}], "abstractContent": [{"text": "The paper investigates the use of semantic similarity scores as feature in the phrase based machine translation system.", "labels": [], "entities": [{"text": "phrase based machine translation", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.5913353711366653}]}, {"text": "We propose the use of partial least square regression to learn the bilingual word embedding using compositional distribu-tional semantics.", "labels": [], "entities": []}, {"text": "The model outperforms the baseline system which is shown by an increase in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9815073311328888}]}, {"text": "We also show the effect of varying the vector dimension and context window for two different approaches of learning word vectors.", "labels": [], "entities": []}], "introductionContent": [{"text": "The current state of the art Statistical Machine Translation (SMT) systems ( do not account for semantic information or semantic relatedness between the corresponding phrases while decoding the n-best list.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.815453976392746}]}, {"text": "The phrase pair alignments extracted from the parallel corpora offers further limitation of capturing contextual and linguistic information.", "labels": [], "entities": []}, {"text": "Since the efficiency of statistical system depends on the quality of parallel corpora, low resourced language pair fails to meet the desired standards of translation.", "labels": [], "entities": []}, {"text": "Word representation is being widely used in many Natural Language Processing (NLP) applications like information retrieval, machine translation and paraphrasing.", "labels": [], "entities": [{"text": "Word representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7213727831840515}, {"text": "information retrieval", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.7812916040420532}, {"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.8130189180374146}]}, {"text": "The word representation computed from continuous monolingual text provide useful information about the relationship between different words.", "labels": [], "entities": []}, {"text": "Distributional semantics offers a notion of capturing semantic similarity between words occurring in similar context, where similar meaning words are grouped closely in a high dimension word space model.", "labels": [], "entities": []}, {"text": "Each word is associated with an n-dimensional vector which represents its position in a vector space model and similar words are at small distance in comparison to relatively opposite meaning words.", "labels": [], "entities": []}, {"text": "The recent work in word vectors have shown to capture the linguistic relations and regularities.", "labels": [], "entities": [{"text": "word vectors", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.7174004018306732}]}, {"text": "The relation between words can be expressed as a simple mathematical relation between their corresponding word vectors.", "labels": [], "entities": []}, {"text": "The recent paper by) have shown through a word analogy task that the vec (\"man\") -vec (\"woman\") + vec(\"king\") should be close to vec(\"queen\").", "labels": [], "entities": [{"text": "word analogy", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.7659499049186707}]}, {"text": "Capturing of these relations along with word composition have shown significant improvements in various NLP and information retrieval tasks.", "labels": [], "entities": [{"text": "word composition", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7153835296630859}, {"text": "information retrieval", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7402168810367584}]}, {"text": "In this paper, we present our ideas of capturing the semantic similarity between phrase pairs in context of SMT and use the scores as features while decoding n-best list.", "labels": [], "entities": [{"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9853217005729675}]}, {"text": "We make use of word representations computed from two different methods: word2Vec (Mikolov et al., 2013a) and GloVe () and show the effect of varying the context window and vector dimension for Hindi-English language pair.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.9519229531288147}]}, {"text": "We use partial least squares (PLS) regression to learn the bilingual word embeddings using a bilingual dictionary, which is most readily available resource for any language pair.", "labels": [], "entities": []}, {"text": "In this work we are not optimizing over the vector dimension and context window, but provide insights (through experiments) on how these two parameters effect the similarity tasks.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first present the related work in vector space models and their utilization in machine translation domain (section 2).", "labels": [], "entities": [{"text": "machine translation domain", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.7576830784479777}]}, {"text": "Section 3 describes the two methods we have adopted for computing word embeddings.", "labels": [], "entities": []}, {"text": "The basic SMT setup, formulating transformation model and phrase similarity scores are described in section 4.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9969866871833801}, {"text": "formulating transformation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.9404783844947815}]}, {"text": "In section 5 we present our results and conclude the paper in section 6 with some future directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Average word cosine similarity scores on  test set. Context Window (CW)", "labels": [], "entities": []}, {"text": " Table 4: BLEU score of system using Word2Vec  model with a context window of 5.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9714855849742889}, {"text": "Word2Vec", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9391764998435974}]}, {"text": " Table 5: BLEU score of system using Word2Vec  model with a context window of 7.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9714528918266296}, {"text": "Word2Vec", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9403911828994751}]}, {"text": " Table 6: BLEU score of system using GloVe  model with a context window of 5.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9694961905479431}]}, {"text": " Table 7: BLEU score of system using GloVe  model with a context window of 7.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9689559638500214}]}]}