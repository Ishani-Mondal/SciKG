{"title": [], "abstractContent": [{"text": "This paper describes taking parsed sentences, going to meaning representations (the stopover), and then back to parsed sentences (the round trip).", "labels": [], "entities": []}, {"text": "Keeping to the same language tests the combined success of building meaning representations from parsed input and of generating parsed output.", "labels": [], "entities": []}, {"text": "Switching languages when manipulating meaning representations would achieve translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9639074802398682}]}, {"text": "Transfer shortfall is seen with meaning representations built from parsed parallel corpora data, with English-Japanese as an example.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen progress in the development of open-domain semantic parsers able to convert natural language input to representations that preserve much semantic content (see e.g., for an overview).", "labels": [], "entities": []}, {"text": "This becomes relevant for translation if there is also away back to a language string, that is, if there can also be generation from meaning representations.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9845376014709473}]}, {"text": "This paper describes a full pipeline: form (Historical) Penn-treebank parsed sentences, a semantic parser is used to create standard predicate logic based meaning representations (see e.g., Dowty,, which are converted to PENMAN notation to form the basis for generation, which proceeds as a manipulation of tree structure to produce an output parsed tree which can yield a language string.", "labels": [], "entities": [{"text": "Dowty", "start_pos": 190, "end_pos": 195, "type": "DATASET", "confidence": 0.8749236464500427}]}, {"text": "The method is illustrated by round tripping on English, so taking English parsed sentences, going to meaning representations, and then back to parsed sentences of English.", "labels": [], "entities": []}, {"text": "It is equally possible to change the front or back end of the pipeline, e.g., calculate a meaning representation for an English sentence but use generation rules designed for Japanese.", "labels": [], "entities": []}, {"text": "With no modification to the stopover meaning representation this arrives at a result with English words and concepts and yet Japanese parse structure.", "labels": [], "entities": []}, {"text": "Obtaining meaning representations from parsed parallel corpora is also illustrated to form the basis for capturing data to inform the gap that remains between the meaning representations needed to generate sentences of one language from another.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces the semantic parsing to start the pipeline.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7150546908378601}]}, {"text": "Section 4 details changes for generation.", "labels": [], "entities": []}, {"text": "Section 5 presents results of experiments carried out round tripping on English data.", "labels": [], "entities": [{"text": "English data", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.7116997539997101}]}, {"text": "Section 6 discusses the open issue of what remains for translation from one language to another, with an English to Japanese example.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, the smatch metric for measuring semantic annotation agreement rates and semantic parsing accuracy ) is used to evaluate the success of round tripping on English.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.7224281877279282}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.755679726600647}]}, {"text": "This is a metric to measure whole-sentence semantic analysis by calculating the degree of overlap between meaning representations.", "labels": [], "entities": [{"text": "whole-sentence semantic analysis", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8102238178253174}]}, {"text": "The representation seen at the end of section 4.1 is essentially compatible for calculating a smatch score.", "labels": [], "entities": []}, {"text": "This gives a meaning representation for the input sentence.", "labels": [], "entities": []}, {"text": "A meaning representation for the output sentence is achieved by feeding the resulting output of the round trip back into the Treebank Semantics system.", "labels": [], "entities": []}, {"text": "details results for 1452 annotated sentences (14,118 tokens) from four different registers that were manually selected to illustrate different levels of sentence complexity.", "labels": [], "entities": []}, {"text": "All sentences are from the Treebank Semantics Corpus 1 with sentences parsed to gold standard following the annotation scheme detailed in section 3.1, and so already unambiguous for feeding to The results show that in round tripping with English, so building a meaning representation A and generating back to an English sentence and then building a meaning representation B from the generated sentence, and then comparing A with B, it is possible to retain the bulk of semantic content with high precision and recall.", "labels": [], "entities": [{"text": "Treebank Semantics Corpus 1", "start_pos": 27, "end_pos": 54, "type": "DATASET", "confidence": 0.8780324161052704}, {"text": "precision", "start_pos": 496, "end_pos": 505, "type": "METRIC", "confidence": 0.9975702166557312}, {"text": "recall", "start_pos": 510, "end_pos": 516, "type": "METRIC", "confidence": 0.9966321587562561}]}, {"text": "The results also reflect that performance starts to decline on more challenging data.", "labels": [], "entities": []}, {"text": "In particular there is a notable reduction in F-score with the non-fiction data (from a technical manual describing the IBM 1401 Programming System).", "labels": [], "entities": [{"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9984512329101562}]}, {"text": "Weaknesses revealed typically involve complex interactions, such as happen with coordination, or stem from constructions that are difficult to provide a generalisable semantic analysis, such as comparatives.", "labels": [], "entities": []}, {"text": "On the generation side, improvements are possible with more construction and lexical specific pattern/action rules, reordering existing rules, or arranging for existing rules to be retriggered.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: smatch scores comparing meaning representations from original and generated sentences", "labels": [], "entities": []}]}