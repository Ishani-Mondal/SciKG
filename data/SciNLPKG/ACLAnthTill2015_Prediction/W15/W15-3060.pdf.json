{"title": [{"text": "Local System Voting Feature for Machine Translation System Combination", "labels": [], "entities": [{"text": "Machine Translation System Combination", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.8529587686061859}]}], "abstractContent": [{"text": "In this paper, we enhance the traditional confusion network system combination approach with an additional model trained by a neural network.", "labels": [], "entities": []}, {"text": "This work is motivated by the fact that the commonly used binary system voting models only assign each input system a global weight which is responsible for the global impact of each input system on all translations.", "labels": [], "entities": []}, {"text": "This prevents individual systems with low system weights from having influence on the system combination output, although in some situations this could be helpful.", "labels": [], "entities": []}, {"text": "Further , words which have only been seen by one or few systems rarely have a chance of being present in the combined output.", "labels": [], "entities": []}, {"text": "We train a local system voting model by a neural network which is based on the words themselves and the combinatorial occurrences of the different system outputs.", "labels": [], "entities": []}, {"text": "This gives system combination the option to prefer other systems at different word positions even for the same sentence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Adding more linguistic informed models (e.g. language model or translation model) additionally to the standard models into system combination seems to yield no or only small improvements.", "labels": [], "entities": []}, {"text": "The reason is that all these models should have already been applied during the decoding process of the individual systems (which serve as input hypotheses for system combination) and hence already fired before system combination.", "labels": [], "entities": []}, {"text": "To improve system combination with additional models, we need to define a model which cannot be applied by an individual system.", "labels": [], "entities": []}, {"text": "In state-of-the-art confusion network system combination the following models are usually applied: System voting (globalVote) models For each word the voting model for system i (1 \u2264 i \u2264 I) is 1 iff the word is from system i, otherwise 0.", "labels": [], "entities": []}, {"text": "Binary primary system model (primary) A model that marks the primary hypothesis.", "labels": [], "entities": []}, {"text": "Language model 3-gram language model (LM) trained on the input hypotheses.", "labels": [], "entities": []}, {"text": "Word penalty Counts the number of words.", "labels": [], "entities": [{"text": "Word penalty", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.6043636649847031}]}, {"text": "To gain improvements with additional models, it is better to define models which are not used by an individual system.", "labels": [], "entities": []}, {"text": "A simple model which cannot be applied by any individual system is the binary system voting model (globalVote).", "labels": [], "entities": []}, {"text": "This model is the most important one during system combination decoding as it determines the impact of each individual system.", "labels": [], "entities": [{"text": "system combination decoding", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6047441959381104}]}, {"text": "Each system i is assigned one globalVote model which fires if the word is generated by system i.", "labels": [], "entities": []}, {"text": "Nevertheless, this simple model is independent of the actual words and the score is only based on the global preferences of the individual systems.", "labels": [], "entities": []}, {"text": "This disadvantage prevents system combination from producing words which have only been seen by systems with low system weights (low globalVote model weights).", "labels": [], "entities": []}, {"text": "To give systems and words with low weights a chance to affect the final output, we define anew local system voting model (localVote) which makes decisions based on the current word options and not only on a general weight.", "labels": [], "entities": []}, {"text": "The local system voting model allows system combination to prefer different system outputs at different word positions even for the same sentence.", "labels": [], "entities": []}, {"text": "Motivated by the success of neural networks in language modelling () and translation modelling (), we choose feedforward neural networks to train the novel model.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8003001511096954}, {"text": "translation modelling", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.984166830778122}]}, {"text": "Instead of calculating the probabilities in a discrete space, the neural network projects the words into a continuous space.", "labels": [], "entities": []}, {"text": "This projection gives us the option to assign probability also to input sequences which were not observed in the training data.", "labels": [], "entities": []}, {"text": "In system combination each training sentence has to be translated by all individual system engines which is time consuming.", "labels": [], "entities": []}, {"text": "Due to this we have a small amount of training data and thus it is very likely that many input sequences of a test set have not be seen during training.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows: in Section 2, we discuss some related work.", "labels": [], "entities": []}, {"text": "In Section 3, the novel local system voting model is described.", "labels": [], "entities": []}, {"text": "In Section 4, experimental results are presented which are analyzed in Section 5.", "labels": [], "entities": []}, {"text": "The paper is concluded in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Corpus statistics Chinese\u2192English.", "labels": [], "entities": []}, {"text": " Table 6: Corpus statistics BOLT Arabic\u2192English.", "labels": [], "entities": [{"text": "BOLT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.941710352897644}]}, {"text": " Table 8: Word occurrence distribution for the  Chinese\u2192English setup. First column indicates  in how many systems a word appears. E.g.  120/14072 (0.9%) indicates that 14072 words only  appear in one individual input system from which  120 (0.9%) are present in the baseline system com- bination hypothesis.", "labels": [], "entities": []}, {"text": " Table 9: Word occurrence distribution for the  Arabic\u2192English setup. First column indicates in  how many systems a word appears. E.g. 214/5791  (3.7%) indicates that 5791 words only appear  in one individual input system from which 214  (3.7%) are present in the baseline system combi- nation hypothesis.", "labels": [], "entities": []}]}