{"title": [{"text": "Call Centre Conversation Summarization: A Pilot Task at Multiling 2015", "labels": [], "entities": [{"text": "Call Centre Conversation Summarization", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8052599132061005}]}], "abstractContent": [{"text": "This paper describes the results of the Call Centre Conversation Summarization task at Multiling'15.", "labels": [], "entities": [{"text": "Call Centre Conversation Summarization task", "start_pos": 40, "end_pos": 83, "type": "TASK", "confidence": 0.7719997525215149}]}, {"text": "The CCCS task consists in generating abstractive synopses from call centre conversations between a caller and an agent.", "labels": [], "entities": []}, {"text": "Synopses are summaries of the problem of the caller, and how it is solved by the agent.", "labels": [], "entities": []}, {"text": "Generating them is a very challenging task given that deep analysis of the dialogs and text generation are necessary.", "labels": [], "entities": [{"text": "text generation", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7750760912895203}]}, {"text": "Three languages were addressed: French, Italian and English translations of conversations from those two languages.", "labels": [], "entities": []}, {"text": "The official evaluation metric was ROUGE-2.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9964210987091064}]}, {"text": "Two participants submitted a total of four systems which had trouble beating the extractive baselines.", "labels": [], "entities": []}, {"text": "The datasets released for the task will allow more research on abstractive dialog sum-marization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speech summarization has been of great interest to the community because speech is the principal modality of human communications, and it is not as easy to skim, search or browse speech transcripts as it is for textual messages.", "labels": [], "entities": [{"text": "Speech summarization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7196141183376312}]}, {"text": "Speech recorded from call centres offers a great opportunity to study goal-oriented and focused conversations between an agent and a caller.", "labels": [], "entities": []}, {"text": "The Call Centre Conversation Summarization (CCCS) task consists in automatically generating summaries of spoken conversations in the form of textual synopses that shall inform on the content of a conversation and might be used for browsing a large database of recordings.", "labels": [], "entities": [{"text": "Call Centre Conversation Summarization (CCCS)", "start_pos": 4, "end_pos": 49, "type": "TASK", "confidence": 0.7622552684375218}]}, {"text": "Compared to news summarization where extractive approaches have been very successful, the CCCS task's objective is to foster work on abstractive summarization in order to depict what happened in a conversation instead of what people actually said.", "labels": [], "entities": [{"text": "news summarization", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.6930912435054779}]}, {"text": "The track leverages conversations from the Decoda and Luna corpora of French and Italian call centre recordings, both with transcripts available in their original language as well as English translation (both manual and automatic).", "labels": [], "entities": [{"text": "Decoda and Luna corpora of French and Italian call centre recordings", "start_pos": 43, "end_pos": 111, "type": "DATASET", "confidence": 0.8654248877005144}]}, {"text": "Recordings duration range from a few minutes to 15 minutes, involving two or sometimes more speakers.", "labels": [], "entities": []}, {"text": "In the public transportation and help desk domains, the dialogs offer a rich range of situations (with emotions such as anger or frustration) while staying in a coherent and focused domain.", "labels": [], "entities": []}, {"text": "Given transcripts, participants to the task shall generate abstractive summaries informing a reader about the main events of the conversations, such as the objective of the caller, whether and how it was solved by the agent, and the attitude of both parties.", "labels": [], "entities": []}, {"text": "Evaluation has been performed by comparing submissions to reference synopses written by quality assurance experts from call centres.", "labels": [], "entities": []}, {"text": "Both conversations and reference summaries are kindly provided by the SENSEI project.", "labels": [], "entities": [{"text": "SENSEI project", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.750535637140274}]}, {"text": "This paper reports on the results of the CCCS task in term ROUGE-2 evaluation metric.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9433508515357971}]}, {"text": "Two participants have submitted four systems to the task.", "labels": [], "entities": []}, {"text": "In addition, we provide three baselines which frame the performance that would be obtained by extractive systems.", "labels": [], "entities": []}, {"text": "The results are analysed according to language, human annotator coherence and the impact of automatic translation.", "labels": [], "entities": []}, {"text": "The remaining of the paper is organized as follows: Section 2 describes the synopsis generation task.", "labels": [], "entities": [{"text": "synopsis generation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.9621526300907135}]}, {"text": "Section 3 describes the CCCS corpus.", "labels": [], "entities": [{"text": "CCCS corpus", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9583684504032135}]}, {"text": "Section 4 describes the results from the systems of the participants.", "labels": [], "entities": []}, {"text": "Section 5 discusses future research avenues.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Decoda test set statistics.", "labels": [], "entities": [{"text": "Decoda test set", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.6980474789937338}]}, {"text": " Table 2: Luna test set statistics.", "labels": [], "entities": [{"text": "Luna test set", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.8157376448313395}]}, {"text": " Table 3: ROUGE-2 performance of the submitted  systems and baselines for each of the languages.  Confidence intervals are not given but are very  tight (\u00b10.005).", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9873263239860535}, {"text": "Confidence intervals", "start_pos": 98, "end_pos": 118, "type": "METRIC", "confidence": 0.9864067137241364}]}, {"text": " Table 5: ROUGE-2 performance on English ac- cording to whether the conversations have been  manually translated or automatically translated", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.989615797996521}]}]}