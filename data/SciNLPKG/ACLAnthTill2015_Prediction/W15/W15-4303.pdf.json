{"title": [{"text": "Toward Tweets Normalization Using Maximum Entropy", "labels": [], "entities": [{"text": "Tweets Normalization", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8352429270744324}]}], "abstractContent": [{"text": "The use of social network services and microblogs, such as Twitter, has created valuable text resources, which contain extremely noisy text.", "labels": [], "entities": []}, {"text": "Twitter messages contain so much noise that it is difficult to use them in natural language processing tasks.", "labels": [], "entities": []}, {"text": "This paper presents anew approach using the maximum entropy model for normalizing Tweets.", "labels": [], "entities": [{"text": "normalizing Tweets", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7291646301746368}]}, {"text": "The proposed approach addresses words that are unseen in the training phase.", "labels": [], "entities": []}, {"text": "Although the maximum entropy needs a training dataset to adjust its parameters, the proposed approach can normalize unseen data in the training set.", "labels": [], "entities": []}, {"text": "The principle of maximum entropy emphasizes incorporating the available features into a uniform model.", "labels": [], "entities": []}, {"text": "First, we generate a set of normalized candidates for each out-of-vocabulary word based on lexical, pho-nemic, and morphophonemic similarities.", "labels": [], "entities": []}, {"text": "Then, three different probability scores are calculated for each candidate using positional indexing, a dependency-based frequency feature and a language model.", "labels": [], "entities": []}, {"text": "After the optimal values of the model parameters are obtained in a training phase, the model can calculate the final probability value for candidates.", "labels": [], "entities": []}, {"text": "The approach achieved an 83.12 BLEU score in testing using 2,000 Tweets.", "labels": [], "entities": [{"text": "83.12", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9740222692489624}, {"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9600375294685364}]}, {"text": "Our experimental results show that the maximum entropy approach significantly outperforms previous well-known normalization approaches .", "labels": [], "entities": []}], "introductionContent": [{"text": "The advent of Web 2.0 and electronic communications has enabled the extensive creation and dissemination of user-generated content (UGC).", "labels": [], "entities": []}, {"text": "The UGC collections provide invaluable data sources in order to mine and extract beneficial information and knowledge, while, at the same time, resulting in less standardized language.", "labels": [], "entities": [{"text": "UGC collections", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9554085731506348}]}, {"text": "However, such content diverges from standard writing conventions.", "labels": [], "entities": []}, {"text": "As shown by experts, this divergence is due to the usage of a variety of coding strategies, including digit phonemes (you too \u2192 you2), phonetic transcriptions (you \u2192 u), vowel drops (dinner \u2192 dnnr), misspellings (convenience \u2192 convineince), and missing or incorrect punctuation marks (If I were you, I'd probably go. \u2192 If I were you Id probably go).", "labels": [], "entities": []}, {"text": "These alterations are due to three main parameters: 1) The small allowance of characters, 2) the constraints of the small keypads, and 3) using UGC in informal communications between friends and relatives.", "labels": [], "entities": []}, {"text": "Whatever their causes, these alterations considerably affect any standard natural language processing (NLP) system, due to the presence of many out of vocabulary (OOV) words, also known as non-standard words (NSWs) and unknown words.", "labels": [], "entities": []}, {"text": "Therefore, a text normalization process must be performed before any conven-tional NLP process is implemented.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7976945042610168}]}, {"text": "As defined by Liu, Weng, Wang, and Liu (2011), \"Text message normalization aims to replace the non-standard tokens that carry significant meanings with the context-appropriate standard words.\"", "labels": [], "entities": [{"text": "Text message normalization", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.8086272676785787}]}, {"text": "This paper proposes a novel normalization approach for Twitter messages.", "labels": [], "entities": []}, {"text": "Twitter is the most popular microblogging service in the world for news-casting, sharing thoughts, and staying in touch with friends.", "labels": [], "entities": []}, {"text": "Since its initial founding in 2006, it has gathered hundreds of millions of registered users.", "labels": [], "entities": []}, {"text": "Tweets refer to messages sent on Twitter, which is restricted to 140 characters, 20 characters less than the 160 allowed by SMS.", "labels": [], "entities": []}, {"text": "Because of this limitation, users have to transcribe Tweets with as much brevity as possible.", "labels": [], "entities": []}, {"text": "The normalization bears a resemblance to spelling correction.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.9375395476818085}]}, {"text": "The ultimate goal of which is the detection and correction of OOV words.", "labels": [], "entities": [{"text": "detection and correction of OOV words", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.7925237615903219}]}, {"text": "The spelling correction methods only focus on misspelled words while normalization systems consider all forms of OOV words, such as representing sounds phonetically (e.g. by the way \u2192 btw) and shortened forms (e.g. university \u2192 uni).", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8944683969020844}]}, {"text": "Thus, normalization approaches should address a higher volume of OOV words compared to spelling correction approaches that lead to more complexity.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8429436385631561}]}, {"text": "To address this complexity, we use maximum entropy) for utilizing and incorporating more probability functions.", "labels": [], "entities": []}, {"text": "Our approach is based on the hypothesis that integrating more probability functions will boost the performance of the method; however, the available information and number of probability functions for (OOV word, standard word) pairs are always limited.", "labels": [], "entities": []}, {"text": "Maximum entropy (Maxent) provides a criterion for integrating probability distributions based on partial knowledge.", "labels": [], "entities": [{"text": "Maxent)", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.8802172243595123}]}, {"text": "The Maxent produces the lowest biased estimation on the given information, that is, it is maximally neutral regarding missing information.", "labels": [], "entities": [{"text": "Maxent", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7863568067550659}, {"text": "estimation", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.8991788029670715}]}, {"text": "When defining some unknown events with a statistical model, we should always select the one that has maximum entropy.", "labels": [], "entities": []}, {"text": "Although the Maxent has already been used in the normalization sphere (e.g. utilized Maxent to classify deletion-based abbreviations), this paper explains how to employ Maxent for selecting the best-normalized candidate.", "labels": [], "entities": []}, {"text": "We have developed a method that does not require annotated training data and it normalizes unseen data.", "labels": [], "entities": []}, {"text": "Most of the normalization approaches substantially depend on the manually annotated data, while the labeled data is costly and time consuming to prepare.", "labels": [], "entities": [{"text": "normalization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9702906608581543}]}, {"text": "We generate normalized candidates for each detected OOV based on lexical, phonemic, and morphophonemic variations.", "labels": [], "entities": []}, {"text": "In addition, since our target dataset encompasses Twitter messages from Singaporeans and code-switching between Malay and English is frequent in the dataset, a MalayEnglish dictionary is utilized to generate candidates for Malay words.", "labels": [], "entities": []}, {"text": "Finally, maximum entropy presents a backbone to combine several conditional probabilities of normalized candidates.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 gives a survey of different approaches of normalizing noisy text.", "labels": [], "entities": [{"text": "normalizing noisy text", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.9145654439926147}]}, {"text": "Section 3 describes the preprocessing stage.", "labels": [], "entities": []}, {"text": "Section 4 illustrates the candidate generation stage.", "labels": [], "entities": []}, {"text": "The proposed candidate selection method is demonstrated in Section 5.", "labels": [], "entities": [{"text": "candidate selection", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9060552418231964}]}, {"text": "Finally, Section 6 concludes this paper with a summary and future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach in terms of BLEU score), since BLEU has become a well-known and adequate evaluation metric in normalization studies).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9828270375728607}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9983955025672913}]}, {"text": "The achieved baseline for the testing dataset is 42.01 BLEU score, that is, the volume of similarity between the testing text and the reference text (manually normalized text) in term of BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9757735729217529}, {"text": "BLEU score", "start_pos": 187, "end_pos": 197, "type": "METRIC", "confidence": 0.9737275838851929}]}, {"text": "In the training phase, we performed maximum likelihood training for \u03bb 1 , \u03bb 2 and \u03bb 3 between 0.0 and 1.0.", "labels": [], "entities": []}, {"text": "shows the tolerance of the performance while transition of \u03bb 1 and \u03bb 2 (when \u03bb 3 is fixed to 1.0).", "labels": [], "entities": []}, {"text": "depicts that the value of performance achieves the high-est when the \u03bb 1 and \u03bb 2 are close to 0.63 and 0.9, respectively.", "labels": [], "entities": []}, {"text": "It is found that the best performance is achieved by 0.6, 0.9, and 1.0 values for \u03bb 1 , \u03bb 2 , and \u03bb 3 , respectively.", "labels": [], "entities": []}, {"text": "This means that LM has the highest impact on the candidate selection, and that dependency-based frequency has a higher impact on candidate selection than positional.", "labels": [], "entities": []}, {"text": "We divided our dataset into six equal sets in order to perform 6-fold cross validation.", "labels": [], "entities": []}, {"text": "As shown in, the average of the obtained BLEU scores in six evaluation rounds was 83.12.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.998569130897522}]}, {"text": "The evaluation proves that our approach boosts the BLEU score by 41.11 (i.e. from 42.01 to 83.12).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9799056053161621}]}, {"text": "Since previous normalization studies used different data sources in their experiments, a direct comparison between our accuracy values is not meaningful.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9978932738304138}]}, {"text": "Therefore, we re-examined one of the state-of-the-art approaches using our dataset.: Normalization results for 6-fold cross validation test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 displays the average number of gener- ated candidates for each module. The lowest rate  is associated with the Malay dictionary module.  Two lexical edit operations generate the highest  number of candidates, which indicates the high- est recall and lowest precision. The rank of com- bination and phoneme modules are second and  third, respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 248, "end_pos": 254, "type": "METRIC", "confidence": 0.8772863745689392}, {"text": "precision", "start_pos": 266, "end_pos": 275, "type": "METRIC", "confidence": 0.9985621571540833}]}, {"text": " Table 2: An example of the positional indexes  obtained.", "labels": [], "entities": []}, {"text": " Table 3: Normalization results for 6-fold cross  validation test.", "labels": [], "entities": []}, {"text": " Table 4: Statistics of testing dataset.", "labels": [], "entities": []}]}