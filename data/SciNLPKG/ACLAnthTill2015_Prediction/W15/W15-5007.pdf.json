{"title": [{"text": "Evaluating Neural Machine Translation in English-Japanese Task", "labels": [], "entities": [{"text": "Evaluating Neural Machine Translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8179314136505127}]}], "abstractContent": [{"text": "In this paper, we evaluate Neural Machine Translation (NMT) models in English-Japanese translation task.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7905557254950205}, {"text": "English-Japanese translation task", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.6697755455970764}]}, {"text": "Various network architectures with different recurrent units are tested.", "labels": [], "entities": []}, {"text": "Additionally , we examine the effect of using pre-reordered data for the training.", "labels": [], "entities": []}, {"text": "Our experiments show that even simple NMT models can produce better translations compared with all SMT baselines.", "labels": [], "entities": [{"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9748740792274475}]}, {"text": "For NMT models, recovering unknown words is another key to obtaining good translations.", "labels": [], "entities": []}, {"text": "We describe a simple workaround to find missing translations with a back-off system.", "labels": [], "entities": []}, {"text": "To our surprise, performing pre-reordering on the training data hurts the model performance.", "labels": [], "entities": []}, {"text": "Finally, we provide a qualitative analysis demonstrates a specific error pattern in NMT translations which omits some information and thus fail to preserve the complete meaning.", "labels": [], "entities": [{"text": "NMT translations", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.763848215341568}]}], "introductionContent": [{"text": "In the last two decades, Statistical Machine Translation (SMT) with log-linear models in the core has shown promising results in the field.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.8688206195831298}]}, {"text": "However, as stated in, log-linear models may suffer from the underfitting problem and thus give poor performance.", "labels": [], "entities": []}, {"text": "While for recurrent neural networks (RNNs), as demonstrated in (, they brought significant improvement in Natural Language Processing tasks.", "labels": [], "entities": [{"text": "Natural Language Processing tasks", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.6849428862333298}]}, {"text": "In their research, RNNs are shown to be capable of giving more prediction power compared with conventional language models when large training data is given.", "labels": [], "entities": []}, {"text": "Using these neural language models to rescore SMT outputs generally gives better translation results).", "labels": [], "entities": [{"text": "SMT outputs", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9090780317783356}]}, {"text": "Other approaches rescore with RNNs that predict the next word by taking the word in current step and S as inputs).", "labels": [], "entities": []}, {"text": "Here, S is a vector representation summarizes the whole input sentence.", "labels": [], "entities": []}, {"text": "Neural machine translation is a brand-new approach that samples translation results directly from RNNs.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8070310552914938}]}, {"text": "Most published models involve an encoder and a decoder in the network architecture, called Encoder-Decoder approach.", "labels": [], "entities": []}, {"text": "gives a general overview of this approach.", "labels": [], "entities": []}, {"text": "In, the vector output S of the encoder RNN represents the whole input sentence.", "labels": [], "entities": []}, {"text": "Hence, S contains all information required to produce the translation.", "labels": [], "entities": []}, {"text": "In order to boost up the performance,) used stacked Long Short-Term Memory (LSTM) units for both encoder and decoder, their ensembled models outperformed phrasebased SMT baseline in English-French translation task.", "labels": [], "entities": [{"text": "SMT", "start_pos": 166, "end_pos": 169, "type": "TASK", "confidence": 0.8707269430160522}]}, {"text": "Recently, by scaling up neural network models and incorporating some techniques during the training, the performance of NMT models have already achieved the state-of-the-art in English-French translation task () and English-German translation task (.", "labels": [], "entities": [{"text": "English-German translation", "start_pos": 216, "end_pos": 242, "type": "TASK", "confidence": 0.6318501979112625}]}, {"text": "In this paper, we describe our works on applying NMT to English-Japanese translation task.", "labels": [], "entities": [{"text": "English-Japanese translation task", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.692336748043696}]}, {"text": "The main contributions of this work are detailed as follows: \u2022 We examined the effect of using different network architecture and recurrent units for English-Japanese translation \u2022 We empirically evaluated NMT models trained on pre-reordered data \u2022 We demonstrate a simple solution to recover unknown words in the translation results with a back-off system \u2022 We provide a qualitative analysis on the translation results of NMT models", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we are curious to see how NMT models work in English-Japanese translation and how well the existing approaches for unknown words fit into this setting.", "labels": [], "entities": []}, {"text": "As Japanese language drastically differs from English in terms of word order and grammar structure.", "labels": [], "entities": []}, {"text": "NMT models must capture the semantics of long-range dependencies in a sentence in order to translate it well.", "labels": [], "entities": []}, {"text": "We use Japanese-English Scientific Paper Abstract Corpus (ASPEC-JE) as training data and focus on evaluating the models for English-Japanese translation task.", "labels": [], "entities": [{"text": "Japanese-English Scientific Paper Abstract Corpus (ASPEC-JE)", "start_pos": 7, "end_pos": 67, "type": "DATASET", "confidence": 0.6029943339526653}]}, {"text": "In order to make the training time-efficient, we pick 1.5M sentences according to similarity score then filter out long sentences with more than 40 words in either English or Japanese side.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 82, "end_pos": 98, "type": "METRIC", "confidence": 0.9575198292732239}]}, {"text": "This processing step gives 1.1M sentences for training.", "labels": [], "entities": []}, {"text": "We randomly separate out 1,280 sentences as valid data.", "labels": [], "entities": []}, {"text": "As almost zero pre-knowledge of NMT experiments in English-Japanese translation can be found in publications, our purpose is to conduct a thorough experiment so that we can evaluate and compare different model architectures and recurrent units.", "labels": [], "entities": []}, {"text": "However, the limitation of computational resource and time disallows us to massively test various models, training schemes, and hyper-parameters.", "labels": [], "entities": []}, {"text": "In our experiments, we evaluated four kinds of models as follow: \u2022 LSTM Search: Soft-attention model with LSTM recurrent units Most of the details of these models are common.", "labels": [], "entities": []}, {"text": "The recurrent layers of all the models contain 1024 neurons each.", "labels": [], "entities": []}, {"text": "The size of word embedding is 1000.", "labels": [], "entities": []}, {"text": "We truncate the sourceside and target-side vocabulary sizes to 80k and 40k respectively.", "labels": [], "entities": []}, {"text": "For all models, we insert a dense layer contains 600 neurons immediately before the output layer.", "labels": [], "entities": []}, {"text": "We basically use SGD with learning rate decay as optimization method, the batch size is 60 and initial learning rate is 1.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 95, "end_pos": 116, "type": "METRIC", "confidence": 0.8478761116663615}]}, {"text": "The gradients are clipped to ensure L2 norm lower than 3.", "labels": [], "entities": [{"text": "L2 norm", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9021829664707184}]}, {"text": "Although we sort the training data according to the input length, the order of batches is shuffled before training.", "labels": [], "entities": []}, {"text": "For LSTM units, we set the bias of forget gate to 1 before training.", "labels": [], "entities": [{"text": "bias", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9752219915390015}, {"text": "forget gate", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.8849652409553528}]}, {"text": "During the translation, we set beam size to 20, if no valid translation is obtained, then another trail with beam size of 1000 will be performed.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9794102907180786}]}], "tableCaptions": [{"text": " Table 1: Automatic evaluation results in  WAT2015", "labels": [], "entities": [{"text": "WAT2015", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.7798126339912415}]}]}