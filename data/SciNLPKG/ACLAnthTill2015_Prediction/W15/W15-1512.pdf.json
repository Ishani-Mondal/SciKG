{"title": [{"text": "Learning Distributed Representations for Multilingual Text Sequences", "labels": [], "entities": [{"text": "Learning Distributed Representations for Multilingual Text Sequences", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.6363229666437421}]}], "abstractContent": [{"text": "We propose a novel approach to learning distributed representations of variable-length text sequences in multiple languages simultaneously.", "labels": [], "entities": []}, {"text": "Unlike previous work which often derive representations of multi-word sequences as weighted sums of individual word vectors , our model learns distributed representations for phrases and sentences as a whole.", "labels": [], "entities": []}, {"text": "Our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efficiently encode meaning-equivalent text sequences of multiple languages in the same semantic space.", "labels": [], "entities": []}, {"text": "Our learned embeddings achieve state-of-the-art performance in the often used crosslingual document classification task (CLDC) with an accuracy of 92.7 for English to German and 91.5 for German to English.", "labels": [], "entities": [{"text": "crosslingual document classification task (CLDC)", "start_pos": 78, "end_pos": 126, "type": "TASK", "confidence": 0.7134148308209011}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9981613755226135}]}, {"text": "By learning text sequence representations as a whole, our model performs equally well in both classification directions in the CLDC task in which past work did not achieve.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representations of words, also known as word embeddings, are critical components of many neural network based NLP systems.", "labels": [], "entities": []}, {"text": "Such representations overcome the sparsity of natural languages by representing words with high-dimensional vectors in a continuous space.", "labels": [], "entities": []}, {"text": "These vectors encode semantic information of words, leading to success in a wide range of tasks, such as sequence tagging, sentiment analysis, and parsing.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.7233349829912186}, {"text": "sentiment analysis", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.9638512432575226}, {"text": "parsing", "start_pos": 147, "end_pos": 154, "type": "TASK", "confidence": 0.9623045325279236}]}, {"text": "As a natural extension, being able to learn representations for larger language structures such as phrases or sentences, has also been of interest to the community lately, for instance ().", "labels": [], "entities": []}, {"text": "In the multilingual context, most of the recent work in bilingual representation learning such as () only focus on learning embeddings for words and use simple functions, e.g., idf-weighted sum, to synthesize representations for larger text sequences from their word members.", "labels": [], "entities": []}, {"text": "In contrast, our work aims to learn representations for phrases and sentences as a whole so as to represent non-compositional meanings.", "labels": [], "entities": []}, {"text": "In essence, we extend the paragraph vector approach proposed by to the bilingual context to efficiently encode meaningequivalent multi-word sequences in the same semantic space.", "labels": [], "entities": []}, {"text": "Our method only utilizes parallel data and eschews the use of word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.697891965508461}]}, {"text": "When tested on the often used crosslingual document classification (CLDC) tasks, our learned embeddings yield stateof-the-art performance with an accuracy of 92.7 for English to German and 91.5 for German to English.", "labels": [], "entities": [{"text": "crosslingual document classification (CLDC) tasks", "start_pos": 30, "end_pos": 79, "type": "TASK", "confidence": 0.7860317996570042}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9962659478187561}]}, {"text": "One notable feature of our model is that it performs equally well in both classification directions in the CLDC task in which past work did not achieve as we detail later in the experiment section.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performances on CLDC English-German.  Each model is trained on one language and tested  on the other one. The numbers reported are the per- centage of correctly predicted test documents. The  first four baselines (Klementiev et al., 2012) are less  sensitive to languages, so we do not observe large  difference between the tasks en\u2192de and de\u2192en.  Other methods that involve weighted sum of word  vectors by (Chandar A P et al., 2014) and (Hermann", "labels": [], "entities": [{"text": "CLDC English-German", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.8681997954845428}]}]}