{"title": [{"text": "Predicting Machine Translation Adequacy with Document Embeddings", "labels": [], "entities": [{"text": "Predicting Machine Translation Adequacy", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8831912726163864}]}], "abstractContent": [{"text": "This paper describes USAAR's submission to the the metrics shared task of the Workshop on Statistical Machine Translation (WMT) in 2015.", "labels": [], "entities": [{"text": "USAAR", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.9190569519996643}, {"text": "Statistical Machine Translation (WMT) in 2015", "start_pos": 90, "end_pos": 135, "type": "TASK", "confidence": 0.8369281813502312}]}, {"text": "The goal of our submission is to take advantage of the semantic overlap between hypothesis and reference translation for predicting MT output adequacy using language independent document embeddings.", "labels": [], "entities": [{"text": "MT output", "start_pos": 132, "end_pos": 141, "type": "TASK", "confidence": 0.7744195461273193}]}, {"text": "The approach presented here is learning a Bayesian Ridge Regressor using document skip-gram em-beddings in order to automatically evaluate Machine Translation (MT) output by predicting semantic adequacy scores.", "labels": [], "entities": [{"text": "Machine Translation (MT) output", "start_pos": 139, "end_pos": 170, "type": "TASK", "confidence": 0.8484905262788137}]}, {"text": "The evaluation of our submission-measured by the correlation with human judgements-shows promising results on system-level scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation is becoming an utility in everyday life.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9413395524024963}]}, {"text": "The increased availability of real-time machine translation services relying on Statistical Machine Translation (SMT) allows users who do not understand the language of the source text to quickly gist the text and understand its general meaning.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7301171272993088}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.7687027206023535}]}, {"text": "For these users, accurate meaning of translated words is more important than the fluency of the translated sentence.", "labels": [], "entities": []}, {"text": "However, SMT suffers from poor lexical choices.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.992038369178772}]}, {"text": "Fluent but inadequate translations are commonly produced due to the strong bias towards the language model component that prefers consecutive words based on the data that the system is trained on.", "labels": [], "entities": []}, {"text": "Current state of art MT evaluation metrics are generally able to identify problems with grammaticality of the translation but less evidently accuracy of translated semantics, e.g. incorrect translation of ambiguous words or wrong assignment of semantic roles.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9323830008506775}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9981105327606201}]}, {"text": "In the example below, the ideal Machine Translation (MT) evaluation metric should appropriately penalise poor lexical choice, such as braked, and reward or at least allow leeway for semantically similar translations, such as external trade.", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8508593738079071}]}], "datasetContent": [{"text": "MT output is usually evaluated by automatic language-independent metrics that can be applied to MT output, independent of the target language.", "labels": [], "entities": [{"text": "MT output", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.880835771560669}, {"text": "MT output", "start_pos": 96, "end_pos": 105, "type": "TASK", "confidence": 0.8745309114456177}]}, {"text": "Automatic metrics typically compute the closeness (adequacy) of a hypothesis to a reference translation and differ from each other by how this closeness is measured.", "labels": [], "entities": []}, {"text": "The most popular MT evaluation metrics are IBM BLEU () and NIST, used not only for tuning MT systems, but also as evaluation metrics for translation shared tasks, such as the Workshop on Statistical Machine Translation (WMT).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.902794361114502}, {"text": "IBM", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.5083069205284119}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9280000329017639}, {"text": "NIST", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8198177218437195}, {"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9106311202049255}, {"text": "Statistical Machine Translation (WMT)", "start_pos": 187, "end_pos": 224, "type": "TASK", "confidence": 0.7924423962831497}]}, {"text": "IBM BLEU uses n-gram precision by matching machine translation output against one or more reference translations.", "labels": [], "entities": [{"text": "IBM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7962186932563782}, {"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9143531918525696}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9458913207054138}]}, {"text": "It accounts for adequacy and fluency by calculating word precision, i.e. the n-gram precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.7330167889595032}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.7903292775154114}]}, {"text": "In order to deal with the over generation of common words, precision counts are clipped, meaning that a reference word is exhausted after it is matched against the same word in the hypothesis.", "labels": [], "entities": [{"text": "precision counts", "start_pos": 59, "end_pos": 75, "type": "METRIC", "confidence": 0.9858646094799042}]}, {"text": "This is then called the modified n-gram precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9220336675643921}]}, {"text": "For BLEU, the modified n-gram precision is calculated with N=4, the results being combined by using the geometric mean.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9929377436637878}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.8981308937072754}]}, {"text": "Instead of recall, BLEU computes the Brevity Penalty (BP) (see formula in 2), thus penalising candidate translations which are shorter than the reference translations.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9979269504547119}, {"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9952054023742676}, {"text": "Brevity Penalty (BP)", "start_pos": 37, "end_pos": 57, "type": "METRIC", "confidence": 0.9792862057685852}]}, {"text": "The NIST metric is derived from IBM BLEU.", "labels": [], "entities": [{"text": "NIST", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6690042614936829}, {"text": "IBM", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.736409604549408}, {"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9470263719558716}]}, {"text": "The NIST score is the arithmetic mean of modified n-gram precision for N=5 scaled by the BP.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.6977534890174866}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9180430173873901}, {"text": "BP", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9280533194541931}]}, {"text": "Additionally, NIST also considers the information gain of each n-gram, giving more weight to more informative (less frequent) n-grams and less weight to less informative (more frequent) n-grams.", "labels": [], "entities": [{"text": "NIST", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.9636534452438354}]}, {"text": "Another often used machine translation evaluation metric is METEOR.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.8603804707527161}, {"text": "METEOR", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.6632609367370605}]}, {"text": "Unlike IBM BLEU and NIST, METEOR evaluates a candidate translation by calculating precision and recall on the unigram level and combining them into a parametrised harmonic mean.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.978909432888031}, {"text": "NIST", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.856644332408905}, {"text": "METEOR", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.6933857798576355}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9991331696510315}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.995244562625885}]}, {"text": "The result from the harmonic mean is then scaled by a fragmentation penalty which penalizes gaps and differences in word order.", "labels": [], "entities": []}, {"text": "METEOR is described in detail in Section 3.1.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6393051743507385}]}, {"text": "Besides these evaluation metrics, several other metrics are used for the evaluation of MT output.", "labels": [], "entities": [{"text": "MT output", "start_pos": 87, "end_pos": 96, "type": "TASK", "confidence": 0.8755927681922913}]}, {"text": "Some of these are the WER (word error-rate) metric based on the Levensthein distance, the position-independent error rate metric PER () and the translation edit rate metric TER () with its newer version TERp (.", "labels": [], "entities": [{"text": "WER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9364897608757019}, {"text": "position-independent error rate metric PER", "start_pos": 90, "end_pos": 132, "type": "METRIC", "confidence": 0.72908154129982}, {"text": "translation edit rate metric TER", "start_pos": 144, "end_pos": 176, "type": "METRIC", "confidence": 0.6140047132968902}]}, {"text": "The semantics of both hypotheses and reference translations is considered by MEANT (.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.6688038110733032}]}, {"text": "MEANT, based on HMEANT (, is a fully automatic semantic MT evaluation metric, measuring semantic fidelity by determining the degree of parallelism of verb frames and semantic roles between hypothesis and reference translations.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9236845970153809}, {"text": "MT evaluation", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.8195626735687256}]}, {"text": "Some approaches aim at combining several linguistic and semantic aspects.", "labels": [], "entities": []}, {"text": "as well as introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.8735983769098917}]}, {"text": "Human MT evaluation approaches employ the knowledge of human annotators to assess the quality of automatically produced translations along the two axes of target language correctness and semantic fidelity.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 6, "end_pos": 19, "type": "TASK", "confidence": 0.8895618319511414}]}, {"text": "The Linguistics Data Consortium (LDC) introduced a MT evaluation task that elicits quality judgement of MT output from human annotators using a numerical scale).", "labels": [], "entities": [{"text": "Linguistics Data Consortium (LDC)", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.7866830577452978}, {"text": "MT evaluation task", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9084787766138712}, {"text": "MT output", "start_pos": 104, "end_pos": 113, "type": "TASK", "confidence": 0.8700887262821198}]}, {"text": "These judgements were split into two categories: adequacy, the degree of meaning preservation, and fluency, target language correctness.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.7020507156848907}]}, {"text": "Adequacy judgements require annotators to rate the amount of meaning expressed in the reference translation that is also present in the translation hypothesis.", "labels": [], "entities": []}, {"text": "Fluency judgements require annotators to rate how well the translation hypothesis in the target language is formed, disregarding the sentence meaning.", "labels": [], "entities": []}, {"text": "Although evaluators are asked to assess the fluency and adequacy of a hypothesis translation on a Likert scale separately, reported high correlation between annotators' adequacy and fluency scores.", "labels": [], "entities": []}, {"text": "MT output is also evaluated by measuring human post-editing time for productivity, or by asking evaluators to rank MT system outputs (by ordering a set of translation hypotheses according to their quality).", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9673279523849487}, {"text": "MT system outputs", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.8814010421435038}]}, {"text": "show that this task is very easy to accomplish for evaluators, since it does not imply specific skills, a homogeneous group being enough to perform this task.", "labels": [], "entities": []}, {"text": "This is also the method applied during the last years WMTs, where humans are asked to rank machine translation output by using APPRAISE), a software tool that integrates facilities for such a ranking task.", "labels": [], "entities": [{"text": "WMTs", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.8146860599517822}]}, {"text": "An indirect human evaluation method, that is also employed for error analysis, are reading comprehension tests (e.g.,).", "labels": [], "entities": [{"text": "error analysis", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.6867322027683258}]}, {"text": "Other evaluation metrics try to measure the effort that is necessary for \"repairing\" MT output, that is, for transforming it into a linguistically correct and faithful translation.", "labels": [], "entities": [{"text": "MT output", "start_pos": 85, "end_pos": 94, "type": "TASK", "confidence": 0.8825689852237701}]}, {"text": "One such metric is HTER (), which uses human annotators to generate targeted reference translations by means of postediting, the rationale being that by this the shortest path between a hypothesis and its correct version can be found.  with Explicit ORdering)) is an MT evaluation metric which tries to consider both grammatical and semantic knowledge.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 267, "end_pos": 280, "type": "TASK", "confidence": 0.8964253664016724}]}, {"text": "The metric is based on the alignment between a hypothesis translation and a reference translation containing four modules.", "labels": [], "entities": []}, {"text": "The number of modules to be used depends on the availability of resources fora specific language.", "labels": [], "entities": []}, {"text": "The first module generates the alignments based on the surface forms of the words in the hypothesis and reference translation.", "labels": [], "entities": []}, {"text": "The next module performs the alignment on word stems, followed by the alignment of words listed as synonyms in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9111402034759521}]}, {"text": "The last module is responsible for the paraphrase matching between the hypothesis and reference translation, based on the provided or the self-extracted paraphrase tables.", "labels": [], "entities": [{"text": "paraphrase matching", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.788711816072464}]}, {"text": "For the final score calculation all matches are generalised to phrase/chunk matches with a start position and phrase length in each sentence.", "labels": [], "entities": []}, {"text": "Different from other evaluation metrics, ME-TEOR makes the distinction between content words and function words in the hypothesis (h c , hf ) and reference (r c , r f ) translation.", "labels": [], "entities": []}, {"text": "This distinction is made by a provided function words list.", "labels": [], "entities": []}, {"text": "From the final alignment between hypothesis and reference translation, precision (P ) and recall (R) is calculated by weighting content words and function words differently.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 71, "end_pos": 85, "type": "METRIC", "confidence": 0.9531116485595703}, {"text": "recall (R)", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9716594070196152}]}, {"text": "This is described by as follows.", "labels": [], "entities": []}, {"text": "For each of the matchers (m i ) count the number of content and function words covered by matches of this type in the hypothesis (m i (h c ), mi (h r )) and reference (m i (r c ), mi (r r )) translation.", "labels": [], "entities": []}, {"text": "The weighted precision (P ) and recall (R) is computed by using the matcher weights w i ...w n and the function word weight \u03b3 as shown in 5 and 6.", "labels": [], "entities": [{"text": "weighted precision (P )", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.8243678450584412}, {"text": "recall (R)", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9641460329294205}]}, {"text": "The harmonic mean is calculated by the formula in equation 7.", "labels": [], "entities": []}, {"text": "METEOR also accounts for word order differences and gaps by scaling F mean by the fragmentation penalty (P en).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6715293526649475}, {"text": "F mean", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9843904078006744}, {"text": "fragmentation penalty (P en)", "start_pos": 82, "end_pos": 110, "type": "METRIC", "confidence": 0.8807167609532675}]}, {"text": "The fragmentation penalty (P en) in equation 8 is computed by using the total number of matched words (m) and the number of chunks (ch).", "labels": [], "entities": [{"text": "fragmentation penalty (P en)", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.8605112334092458}]}, {"text": "The final score is then: The parameters \u03b1, \u03b2, \u03b3, \u03b4 and w i ...w n are parameters that can be used for tuning METEOR fora given task.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.7099707722663879}]}, {"text": "This year's USAAR submission to the WMT metrics shared task concentrated on evaluating translations into German and into English, assigning a score both at sentence and system level.", "labels": [], "entities": [{"text": "USAAR", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.8959538340568542}, {"text": "WMT metrics shared", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6027108430862427}]}, {"text": "All submissions to the metrics task were evaluated 4 at system level by computing their Pearson correlation coefficient with human judgements.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 88, "end_pos": 119, "type": "METRIC", "confidence": 0.9599712491035461}]}, {"text": "For the evaluation of translations into English our best submission is COMET, achieving on average a correlation coefficient of 0.788\u00b10.026.", "labels": [], "entities": [{"text": "COMET", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.5880782008171082}, {"text": "correlation coefficient", "start_pos": 101, "end_pos": 124, "type": "METRIC", "confidence": 0.957410991191864}]}, {"text": "For the evaluation of translations from English into German, COMET is again our best submission with a correlation coefficient of 0.448\u00b10.40.: System-level Spearman's correlation coefficient for COSINE, ZWICKEL and COMET.", "labels": [], "entities": [{"text": "COMET", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8217387199401855}, {"text": "COSINE", "start_pos": 195, "end_pos": 201, "type": "DATASET", "confidence": 0.6831233501434326}, {"text": "COMET", "start_pos": 215, "end_pos": 220, "type": "DATASET", "confidence": 0.8655439615249634}]}, {"text": "pairs into English and into German.", "labels": [], "entities": []}, {"text": "From the results in we notice that COMET was the metric performing best for both translations into English and German, achieving a coefficient of 0.665\u00b10.069 for translations into English and 0.588\u00b10.072 for translations from German into English.", "labels": [], "entities": [{"text": "COMET", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9421817660331726}]}], "tableCaptions": [{"text": " Table 1: Pearson correlation coefficient for COSINE, ZWICKEL and COMET.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.9069069027900696}, {"text": "COSINE", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.6646424531936646}, {"text": "COMET", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8510609269142151}]}, {"text": " Table 2: System-level Spearman's correlation coefficient for COSINE, ZWICKEL and COMET.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient", "start_pos": 23, "end_pos": 57, "type": "METRIC", "confidence": 0.6222685426473618}, {"text": "COSINE", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.6250905394554138}, {"text": "COMET", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.8053571581840515}]}]}