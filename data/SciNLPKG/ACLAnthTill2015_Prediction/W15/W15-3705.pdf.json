{"title": [], "abstractContent": [{"text": "We report on first annotation experiments on narrative segments.", "labels": [], "entities": []}, {"text": "Narrative segments area pragmatic intermediate layer that allows studying more complex nar-ratological phenomena.", "labels": [], "entities": []}, {"text": "Our experiments show that segmenting on limited context information alone is difficult.", "labels": [], "entities": [{"text": "segmenting", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.984990656375885}]}, {"text": "High inter-annotator agreement on this task can be achieved by coupling the segmentation with summarization and aligning parts of the summaries to segments of the text.", "labels": [], "entities": [{"text": "summarization", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.9562751054763794}]}], "introductionContent": [{"text": "In this paper, we present ongoing work and first insights into the manual annotation of narrative segments.", "labels": [], "entities": [{"text": "manual annotation of narrative segments", "start_pos": 67, "end_pos": 106, "type": "TASK", "confidence": 0.7702836513519287}]}, {"text": "We introduce the notion of narrative segments as a pragmatic intermediate layer, that is a first step towards annotation of more complex narratological phenomena and has the prospects of being identifiable automatically.", "labels": [], "entities": []}, {"text": "Furthermore, narrative segments can serve as an abstraction layer for applications such as social network extraction.", "labels": [], "entities": [{"text": "social network extraction", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.6417620778083801}]}, {"text": "If narratives describe connected events, we define a narrative segment as a coherent and separable sub-sequence of the events in a full narrative.", "labels": [], "entities": []}, {"text": "A narrative segment ends, e.g., when place or time of the events change.", "labels": [], "entities": []}, {"text": "(1) . ] With a whirl of skirts and with the brilliant sparkle still in her eyes, she cluttered out of the door and down the stairs to the street.", "labels": [], "entities": []}, {"text": "Where she stopped the sign read: \"[.", "labels": [], "entities": []}, {"text": "]\" In (1), (O. Henry: The Gift of the Magi), an undefined amount of time passes between the character running down the stairs and stopping at the sign.", "labels": [], "entities": [{"text": "O. Henry: The Gift of the Magi)", "start_pos": 12, "end_pos": 43, "type": "DATASET", "confidence": 0.8451861871613396}]}, {"text": "Since the time and place of the events change, this would be the beginning of anew segment.", "labels": [], "entities": []}, {"text": "Coincidentally, there is also a paragraph boundary at this position.", "labels": [], "entities": []}, {"text": "Working quantitatively with a specific theory requires annotations of text(s).", "labels": [], "entities": []}, {"text": "Unfortunately, instantiating a theory such that it is annotatable is challenging, especially within Digital Humanities.", "labels": [], "entities": []}, {"text": "The annotation process, however, can also be a productive way of validating and objectifying a theory.", "labels": [], "entities": []}, {"text": "In this paper, we showcase how to systematically explore different ways of formalizing and annotating narrative segments, a category that is implicitly present in narratological theory, but not spelled out in detail.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted three experiments to explore different ways of setting up the task regarding the aspects discussed above.", "labels": [], "entities": []}, {"text": "In all experiments we ask annotators to detect narrative segments and calculate inter-annotator agreement as a measure of Does the yellow sentence start anew narr. unit?", "labels": [], "entities": []}, {"text": "A narrative unit starts, whenever \u2022 the speed of narration changes (e.g., more time passing than before as in \"Ten days later, . .", "labels": [], "entities": []}, {"text": "\"), \u2022 time and place change (e.g., flashbacks as in \"Ten years ago, I was a successful businessman in . .", "labels": [], "entities": []}, {"text": "\"), or \u2022 the narrator changes (e.g., longer segments of director indirect speech, attributed to a character in the narration; internal monologue).", "labels": [], "entities": []}, {"text": "Figure 1: Worker instructions in Exp.", "labels": [], "entities": []}, {"text": "1 the \"annotatability\".", "labels": [], "entities": []}, {"text": "shows a schematic overview of the experiments.", "labels": [], "entities": []}, {"text": "The first experiment was conducted as a crowd sourcing classification task using CrowdFlower 2 . The workers were presented a sentence (in yellow) within a context often sentences before and after.", "labels": [], "entities": [{"text": "crowd sourcing classification task", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.8406930714845657}]}, {"text": "They were given a yes/no question, but with an additional \"I can't tell\" option.", "labels": [], "entities": []}, {"text": "The workers annotated all sentences from two narrative texts, Chris Farrington: Able Seaman (J. London) and The Winepress (J. Essberger), in random order.", "labels": [], "entities": [{"text": "Farrington: Able Seaman (J. London) and The Winepress (J. Essberger)", "start_pos": 68, "end_pos": 136, "type": "DATASET", "confidence": 0.7126547594865164}]}, {"text": "The exact definitions are shown in.", "labels": [], "entities": []}, {"text": "Due to difficulties in automatic parsing, we opted for annotating full sentences in this experiment.", "labels": [], "entities": [{"text": "automatic parsing", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.5228805840015411}]}, {"text": "In this experiment, we collected two annotations for each of 19 short stories from (paid) students of German literature.", "labels": [], "entities": []}, {"text": "As a general design change, we asked the annotators to first read the entire text and only make boundary annotations in a second step.", "labels": [], "entities": []}, {"text": "We also made several definitions for cases that were difficult in previous experiments: a) Dramatic scenes (dialogues) typically belong to a narrative segment, b) encyclopedic parts (e.g., landscape descriptions) and c) events that are not \"really\" happening in the narrative (e.g., thoughts, possibilities) can constitute segments on their own.", "labels": [], "entities": []}, {"text": "Additionally, we allowed the annotators to mark segment boundaries on different levels.", "labels": [], "entities": []}, {"text": "This allows finer distinction between segment boundaries of different granularities.", "labels": [], "entities": []}, {"text": "We asked the annotators to first mark the most clear, top-level segmentations and in a second (and third) step subdivide the segments into smaller pieces.", "labels": [], "entities": []}, {"text": "A boundary of level n is also a boundary of level n + 1.", "labels": [], "entities": []}, {"text": "Corpus The stories have been selected randomly out of the TextGrid 3 corpus, the only restrictions being on the genre (narratives) and length (2k \u2212 12k tokens).", "labels": [], "entities": [{"text": "TextGrid 3 corpus", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9449737270673116}]}, {"text": "In total, the corpus contains 4.692 sentences (avg. length: 21.9 tokens).", "labels": [], "entities": []}, {"text": "Agreement We calculated \u03ba agreement using boundary similarity as a measure for observed agreement . Boundary similarity is  based on an edit distance measure and penalizes near misses less than full misses.", "labels": [], "entities": [{"text": "Boundary similarity", "start_pos": 100, "end_pos": 119, "type": "METRIC", "confidence": 0.9664247930049896}]}, {"text": "We used a near miss window of two avgerage sentences (44).", "labels": [], "entities": []}, {"text": "In the third experiment, we asked the annotators from the second experiment to summarize the text and then align parts of the summary with specific text segments.", "labels": [], "entities": [{"text": "summarize the text", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8655968904495239}]}, {"text": "The idea behind this experiment was to couple the segmentation task with a \"real\" task that makes sense outside of the annotation task and guides decisions on granularity.", "labels": [], "entities": []}, {"text": "We evaluated only the (now implicit) segmentation of the texts, using the same measures as before.", "labels": [], "entities": []}, {"text": "An advantage of this setup is that the summaries allow insight into the annotators' intentions.", "labels": [], "entities": []}, {"text": "shows the resulting segmentations of the two annotators and the corresponding agreement scores on the right side.", "labels": [], "entities": []}, {"text": "In terms of the scores, the agreement is much higher than in Exp.", "labels": [], "entities": [{"text": "agreement", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9991577863693237}, {"text": "Exp", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8782224655151367}]}], "tableCaptions": [{"text": " Table 2: Quantitative analysis results of Exp. 1", "labels": [], "entities": [{"text": "Quantitative analysis", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8873530328273773}, {"text": "Exp", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9034875631332397}]}, {"text": " Table 3: Annotator agreement in Experiment 2", "labels": [], "entities": []}]}