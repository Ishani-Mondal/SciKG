{"title": [{"text": "Targeted Paraphrasing on Deep Syntactic Layer for MT Evaluation", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9855450987815857}]}], "abstractContent": [{"text": "In this paper, we present a method of improving quality of machine translation (MT) evaluation of Czech sentences via targeted paraphrasing of reference sentences on a deep syntactic layer.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation of Czech sentences", "start_pos": 59, "end_pos": 113, "type": "TASK", "confidence": 0.8609974980354309}]}, {"text": "For this purpose, we employ NLP framework Treex and extend it with modules for targeted paraphrasing and word order changes.", "labels": [], "entities": [{"text": "NLP framework Treex", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.8256683548291525}, {"text": "word order changes", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.6543500125408173}]}, {"text": "Automatic scores computed using these paraphrased reference sentences show higher correlation with human judgment than scores computed on the original reference sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the very first appearance of machine translation (MT) systems, a necessity for their objective evaluation and comparison has emerged.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.850323623418808}]}, {"text": "The traditional human evaluation is slow and unreproducible; thus, it cannot be used for tasks like tuning and development of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.9543817639350891}]}, {"text": "Wellperforming automatic MT evaluation metrics are essential precisely for these tasks.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.953076034784317}]}, {"text": "The pioneer metrics correlating well with human judgment were BLEU () and NIST).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9941607117652893}, {"text": "NIST", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.8573641180992126}]}, {"text": "They are computed from an n-gram overlap between the translated sentence (hypothesis) and one or more corresponding reference sentences, i.e., translations made by a human translator.", "labels": [], "entities": []}, {"text": "Due to its simplicity and language independence, BLEU still remains the de facto standard metric for MT evaluation and tuning, even though other, better-performing metrics exist,).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9976950287818909}, {"text": "MT evaluation and tuning", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.8754459619522095}]}, {"text": "Furthermore, the standard practice is using only one reference sentence and BLEU then tends to perform badly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9961612224578857}]}, {"text": "There are many translations of a single sentence and even a perfectly correct translation might get a low score as BLEU disregards synonymous expressions and word order variants (see).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9919651746749878}]}, {"text": "This is especially valid for morphologically rich languages with free word order like the Czech language (.", "labels": [], "entities": []}, {"text": "In this paper, we use deep syntactic layer for targeted paraphrasing of reference sentences.", "labels": [], "entities": []}, {"text": "For every hypothesis, we create its own reference sentence that is more similar in wording but keeps the meaning and grammatical correctness of the original reference sentence.", "labels": [], "entities": []}, {"text": "Using these new paraphrased references makes the MT evaluation metrics more reliable.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.867021918296814}]}, {"text": "In addition, correct paraphrases have additional application in many other NLP tasks.", "labels": [], "entities": []}, {"text": "As far as we know, this is the first rule-based model specifically designed for targeted paraphrased reference sentence generation to improve MT evaluation quality.", "labels": [], "entities": [{"text": "paraphrased reference sentence generation", "start_pos": 89, "end_pos": 130, "type": "TASK", "confidence": 0.6715672612190247}, {"text": "MT evaluation", "start_pos": 142, "end_pos": 155, "type": "TASK", "confidence": 0.8996089994907379}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pearson correlation of a metric and human judgment on original references, paraphrased refer- ences and paraphrased reordered references. Ex.Meteor represents Meteor metric with exact match only  (i.e. no paraphrase support).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8598161339759827}]}]}