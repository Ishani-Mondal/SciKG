{"title": [{"text": "Non-canonical language is not harder to annotate than canonical language", "labels": [], "entities": []}], "abstractContent": [{"text": "As researchers developing robust NLP fora wide range of text types, we are often confronted with the prejudice that annotation of non-canonical language (whatever that means) is somehow more arbitrary than annotation of canonical language.", "labels": [], "entities": []}, {"text": "To investigate this, we present a small annotation study where annotators were asked, with minimal guidelines, to identify main predicates and arguments in sentences across five different domains , ranging from newswire to Twitter.", "labels": [], "entities": []}, {"text": "Our study indicates that (at least such) annotation of non-canonical language is not harder.", "labels": [], "entities": []}, {"text": "However , we also observe that agreements in social media domains correlate less with model confidence, suggesting that maybe annotators disagree for different reasons when annotating social media data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, our research group received the reviews of a paper we submitted to a major, influential journal.", "labels": [], "entities": []}, {"text": "The paper included a description of in-house linguistic annotation of Twitter data.", "labels": [], "entities": []}, {"text": "One reviewer complained that \"the use of Twitter as a corpus might be problematic because of the characteristic use of nonstandard/typical language.\"", "labels": [], "entities": []}, {"text": "What the reviewer presumably meant is that linguistic annotation of Twitter data is more arbitrary than annotation of standard or canonical language, e.g., newswire.", "labels": [], "entities": []}, {"text": "We believe this premise, or prejudice, is false.", "labels": [], "entities": []}, {"text": "\"Standard language\", as found in newswire and textbooks, for example, is a very biased sample of the linguistic productions in a language community, and the vast majority of the language we process and produce through the course of a day is very different from newswire and textbooks, be it spoken language, literature, or social media text.", "labels": [], "entities": []}, {"text": "Why, then, is newswire considered more standard or more canonical than other text types?", "labels": [], "entities": []}, {"text": "Obviously, this may simply be because journalists are trained writers and produce fewer errors.", "labels": [], "entities": []}, {"text": "But think, fora minute, about languages in which no newspapers are written.", "labels": [], "entities": []}, {"text": "What, then, is canonical language?", "labels": [], "entities": []}, {"text": "Can spoken language be canonical?", "labels": [], "entities": []}, {"text": "Or is newswire called canonical, because, historically, it is what corpora are made of, and the only data that was available to the NLP community fora long time?", "labels": [], "entities": []}, {"text": "This discussion is more than a fight of words.", "labels": [], "entities": []}, {"text": "The use of the word 'canonical' alludes to the fact that non-canonical language presents a challenge to the NLP community, but a lot of the reason for NLP tools performing poorly on social media texts and the like seems to be a historical coincidence.", "labels": [], "entities": []}, {"text": "Most resources, e.g., syntactic and semantic treebanks, are human-annotated subsets of newswire corpora, simply because most electronic text corpora were newswire corpora when the NLP community began building treebanks.", "labels": [], "entities": []}, {"text": "The question is whether annotating non-canonical language, say social media text, is inherently harder than annotating more canonical language, say newswire.", "labels": [], "entities": []}, {"text": "We believe some types of non-canonical language pose interesting processing challenges, e.g., with more mixed language, more ad hoc spelling conventions, and more texts directed at smaller audiences with more knowledge required during interpretation.", "labels": [], "entities": []}, {"text": "However, newswire also comes with its 148 complexities (headlinese, creative language use, citations, etc.), and if it was not for the skewed distribution of linguistic resources, we do not see why processing social media should be harder than processing newswire.", "labels": [], "entities": []}, {"text": "The skewed distribution underlines the need for new resources, and consequently, raises the important question whether annotating non-canonical language, e.g., social media text, is inherently harder than annotating canonical language.", "labels": [], "entities": []}, {"text": "There is no priori reason why this should be the case.", "labels": [], "entities": []}, {"text": "A full investigation of this question would take a lot of annotation studies, controlling for task, annotator groups, languages, etc.; something which is out of the scope of this squib.", "labels": [], "entities": []}, {"text": "Instead, we present a pilot study of a single, specific linguistic annotation task (identifying main verbs and arguments) with two annotators and 50 sentences for each of five different domains (250 annotated sentences in total).", "labels": [], "entities": []}, {"text": "Obviously, this is but a toy experiment, and our results should betaken with a grain of salt.", "labels": [], "entities": []}, {"text": "However, our design is replicable, the annotated data available, 1 and we hope that others will take up replicating these experiments on a larger scale.", "labels": [], "entities": []}, {"text": "Meanwhile, we leave the world with what our toy experiment suggests.", "labels": [], "entities": []}, {"text": "Note that we cannot just compare reported interannotator agreement scores across existing projects.", "labels": [], "entities": []}, {"text": "Such scores are affected by sample biases, training of annotators, and the completeness of annotation guidelines.", "labels": [], "entities": []}, {"text": "Thus, in this position paper we present an annotation study where we asked the same annotators to annotate canonical and non-canonical language (over five domains, ranging from newswire to Twitter) with minimal guidelines.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data characteristics (50 sentences each).", "labels": [], "entities": []}, {"text": " Table 2: Frequency counts for arguments in the annotated  data (50 sentences per domain, two annotators each).", "labels": [], "entities": [{"text": "Frequency counts", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9627500474452972}]}, {"text": " Table 3: Agreement statistics between the two annotators.", "labels": [], "entities": []}, {"text": " Table 4: Correlation (Spearman's \u03c1) between annotator  agreement (how many arguments match out of both) and  system confidence (average per-edge confidence).", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9446067810058594}, {"text": "Spearman's \u03c1)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.6704436987638474}]}]}