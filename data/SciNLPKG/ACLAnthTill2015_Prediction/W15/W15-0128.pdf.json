{"title": [{"text": "Layers of Interpretation: On Grammar and Compositionality", "labels": [], "entities": []}], "abstractContent": [{"text": "With the recent resurgence of interest in semantic annotation of corpora for improved semantic parsing, we observe a tendency which we view as ill-advised, to conflate sentence meaning and speaker meaning into a single mapping, whether done by annotators or by a parser.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7629974484443665}]}, {"text": "We argue instead for the more traditional hypothesis that sentence meaning, but not speaker meaning, is composi-tional, and accordingly that NLP systems would benefit from reusable, automatically derivable, task-independent semantic representations which target sentence meaning, in order to capture exactly the information in the linguistic signal itself.", "labels": [], "entities": []}, {"text": "We further argue that compositional construction of such sentence meaning representations affords better consistency, more comprehensiveness, greater scal-ability, and less duplication of effort for each new NLP application.", "labels": [], "entities": [{"text": "consistency", "start_pos": 105, "end_pos": 116, "type": "METRIC", "confidence": 0.954440712928772}]}, {"text": "For concreteness, we describe one well-tested grammar-based method for producing sentence meaning representations which is efficient for annotators, and which exhibits many of the above benefits.", "labels": [], "entities": []}, {"text": "We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Kate and Wong (2010) define 'semantic parsing' as \"the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.\"", "labels": [], "entities": [{"text": "semantic parsing'", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.822273850440979}]}, {"text": "At this level of generality, semantic parsing has been a cornerstone of NLU from its early days, including work seeking to support dialogue systems, database interfaces, or machine translation (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7509646117687225}, {"text": "machine translation", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.7519733905792236}]}, {"text": "What distinguishes most current work in semantic parsing from such earlier landmarks of old-school NLU is (a) the use of (highly) taskand domain-specific meaning representations (e.g. the RoboCup or GeoQuery formal language) and (b) alack of emphasis on natural language syntax, i.e. a tacit expectation to map (more or less) directly from a linguistic surface form to an abstract representation of its meaning.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.730655238032341}]}, {"text": "This approach risks conflating a distinction that has long played an important role in the philosophy of language and theoretical linguistics, viz.", "labels": [], "entities": []}, {"text": "the contrast between those aspects of meaning that are determined by the linguistic signal alone (called 'timeless', 'conventional', 'standing', or 'sentence' meaning), on the one hand, and aspects of meaning that are particular to a context of use ('utterer', 'speaker', or 'occasion' meaning, or 'interpretation'), on the other hand.", "labels": [], "entities": []}, {"text": "Relating this tradition to computational linguistics, Nerbonne) notes: Linguistic semantics does not furnish a characterization of the interpretation of utterances in use, which is what one finally needs for natural language understanding applications-rather, it (mostly) provides a characterization of conventional content, that part of meaning determined by linguistic form.", "labels": [], "entities": []}, {"text": "Interpretation is not determined by form, however, nor by its derivative content.", "labels": [], "entities": [{"text": "Interpretation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9312500357627869}]}, {"text": "In order to interpret correctly, one must exploit further knowledge sources and processes that ...", "labels": [], "entities": []}, {"text": "probably are not linguistic at all: domain knowledge, commonsense, communicative purpose, extralinguistic tasks, assumptions of interlocutors about each other.", "labels": [], "entities": []}, {"text": "In the currently widespread approach to semantic parsing, the results of linguistic research in semantics are largely disregarded in favor of learning correlations between domain-typical linguistic forms and task-specific meaning representations, using the linguistic signal as well as domain-specific information (e.g. a database schema) as sources of constraint on the search space for the machine action that is most likely the one desired.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7692742943763733}]}, {"text": "We see two interrelated drawbacks to such an approach: First, to the extent that grammatical structure is taken into account, the same problems must be solved anew with each new task.", "labels": [], "entities": []}, {"text": "Second, as a result, such task-specific solutions seem unlikely to scale to general-purpose natural language understanding.", "labels": [], "entities": [{"text": "general-purpose natural language understanding", "start_pos": 76, "end_pos": 122, "type": "TASK", "confidence": 0.623590774834156}]}, {"text": "In order to reach that lofty goal, we argue, there must be some task-independent model of the conventional content of linguistic utterance types which can be paired with domain-specific knowledge and reasoning in order to reach appropriate interpretations of utterances in context.", "labels": [], "entities": []}, {"text": "Success in many semantically-sensitive NLP tasks requires algorithms that can glean a representation of at least a subset of speaker meaning.", "labels": [], "entities": []}, {"text": "But what machines have access to is not any direct representation of a human interlocutor's intended speaker meaning, but rather only natural language utterances.", "labels": [], "entities": []}, {"text": "Such utterances involve tokens of sentence (or sentence fragment) types, which in turn have computable sentence meaning.", "labels": [], "entities": []}, {"text": "While sentence meaning does not determine situated speaker meaning, it is an important cue to it.", "labels": [], "entities": []}, {"text": "We argue here that sentence meaning, but not speaker meaning, is compositional (see, and accordingly that NLP systems would benefit from reusable, automatically derivable, task-independent semantic representations which target sentence meaning, in order to capture exactly the information in the linguistic signal itself.", "labels": [], "entities": []}, {"text": "Furthermore, we argue that such sentence meaning representations are best built compositionally, because the compositional approach affords better consistency, more comprehensiveness, and greater scalability.", "labels": [], "entities": []}, {"text": "In this position paper we begin by providing a working definition of compositionality and briefly survey different types of semantic annotation with an eye towards classifying them as compositional or not ( \u00a72).", "labels": [], "entities": []}, {"text": "\u00a73 provides an overview of the English Resource), a resource for producing semantic representations, covering most of what falls within our definition of compositional, at scale.", "labels": [], "entities": []}, {"text": "\u00a74 articulates the three main benefits of a compositional approach to producing semantic annotations, viz.", "labels": [], "entities": []}, {"text": "comprehensiveness, consistency and scalability.", "labels": [], "entities": [{"text": "consistency", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9925562739372253}]}, {"text": "In \u00a75, we present a small inter-annotator agreement study to quantify the consistency of semantic representations produced via grammar-based sembanking.", "labels": [], "entities": []}, {"text": "Finally, in \u00a76, we consider how ERG-based semantic representations can be used as the backbone of even richer annotations that incorporate information which is not compositionally derivable.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Exact match ERS and Elementary Dependency Match across three annotators.", "labels": [], "entities": [{"text": "Exact match ERS", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7492585182189941}]}]}