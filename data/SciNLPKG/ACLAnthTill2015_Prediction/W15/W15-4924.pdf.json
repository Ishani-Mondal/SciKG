{"title": [{"text": "Word Alignment Based Parallel Corpora Evaluation and Cleaning Using Machine Learning Techniques", "labels": [], "entities": [{"text": "Word Alignment Based Parallel Corpora Evaluation and Cleaning", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.7689273729920387}]}], "abstractContent": [{"text": "This paper presents a method for cleaning and evaluating parallel corpora using word alignments and machine learning algorithms.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.703940823674202}]}, {"text": "It is based on the assumption that parallel sentences have many word alignments while non-parallel sentences have few or none.", "labels": [], "entities": []}, {"text": "We show that it is possible to build an automatic classifier, which identifies most of non-parallel sentences in a parallel corpus.", "labels": [], "entities": []}, {"text": "This method allows us to do (1) automatic quality evaluation of parallel corpus, and (2) automatic parallel corpus cleaning.", "labels": [], "entities": [{"text": "parallel corpus cleaning", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.5883208711942037}]}, {"text": "The method allows us to get cleaner parallel corpora, smaller statistical models, and faster MT training, but this does not always guarantee higher BLEU scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9705331325531006}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9989359974861145}]}, {"text": "An open-source implementation of the tool described in this paper is available from https://github.com/tilde-nlp/c-eval.", "labels": [], "entities": []}], "introductionContent": [{"text": "In statistical machine translation, translation quality is largely dependent on the amount of parallel data available.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6767210563023885}, {"text": "translation", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.954671323299408}]}, {"text": "In practice, a large chunk of data considered parallel might not be so, and it can interfere with good data and reduce translation quality.", "labels": [], "entities": []}, {"text": "The problem of low quality parallel corpora is getting more and more important because it is becoming popular to build parallel corpora from web data using fully automatic methods.", "labels": [], "entities": []}, {"text": "The quality of such corpora often is very low, especially in case of multilingual corpora, which are built by people who do not know the languages they are working with.", "labels": [], "entities": []}, {"text": "As a result, we get corpora with broken encoding, many alignment errors and even texts in different languages.", "labels": [], "entities": []}, {"text": "The problem can be mitigated by removing blatantly obvious non-parallel text that can be detected with handwritten rules.", "labels": [], "entities": []}, {"text": "But that does not help in cases where there are alignment errors or two sentences are kind-of parallel but the translation is wrong or incomplete.", "labels": [], "entities": []}, {"text": "The cleaning of such parallel text would require human involvement since devising rules for catching such errors would be nearly impossible.", "labels": [], "entities": []}, {"text": "The idea presented in this work is to compare word alignments in a parallel text with those found in a non-parallel text.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.69753497838974}]}, {"text": "The intuition being that truly parallel text should have many alignments on word level while unrelated nonparallel text should have few to no alignments.", "labels": [], "entities": []}, {"text": "Since word alignment computation is already a step in the training process of many phrase-based statistical machine translation systems, it can be used as input data for the corpus evaluation and cleaning method that we propose.", "labels": [], "entities": [{"text": "word alignment computation", "start_pos": 6, "end_pos": 32, "type": "TASK", "confidence": 0.8724897305170695}, {"text": "phrase-based statistical machine translation", "start_pos": 83, "end_pos": 127, "type": "TASK", "confidence": 0.5915453657507896}, {"text": "corpus evaluation", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.6765495538711548}]}, {"text": "Another benefit of cleaning a corpus is a reduced size, which leads to smaller storage and computational costs of statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.6541606883207957}]}], "datasetContent": [{"text": "Firstly, we evaluated the tool by looking at the BLEU score () changes, qualitative changes and the quality score of EUBokshop (OPUS edition) corpus, which is known to be cluttered with bad data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9769908785820007}, {"text": "EUBokshop (OPUS edition) corpus", "start_pos": 117, "end_pos": 148, "type": "DATASET", "confidence": 0.9144688546657562}]}, {"text": "It has been automatically extracted from web data (PDF files), containing parallel corpora for 24 official European Union languages ().", "labels": [], "entities": []}, {"text": "For testing we chose the Latvian, English and French language pairs.", "labels": [], "entities": []}, {"text": "We evaluated several well-known corpora with the Corpus Cleaner tool as well as whether the results were consistent with qualitative evaluation.", "labels": [], "entities": []}, {"text": "A number of different models were built and used to test if models were language independent.", "labels": [], "entities": []}, {"text": "Since the main use for this cleaning method is machine translation, we evaluated how the cleaning method affects the BLEU score.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7329268157482147}, {"text": "BLEU score", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9754540324211121}]}, {"text": "For the MT evaluation we trained an SMT system with the original EU Bookshop corpus and noted the BLEU score.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9898239970207214}, {"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9737550020217896}, {"text": "EU Bookshop corpus", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.9857595165570577}, {"text": "BLEU score", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9644680321216583}]}, {"text": "We applied the same procedure to the cleaned version of the corpus.", "labels": [], "entities": []}, {"text": "summarizes the BLEU scores and the amount of good lines after cleaning for the explored language pairs can be seen.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9982341527938843}]}, {"text": "The BLEU score for both the original and cleaned MT systems was nearly identical with the cleaned corpus having a slightly lower BLEU score than the original.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9762691557407379}, {"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9443435072898865}, {"text": "BLEU score", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.98455610871315}]}, {"text": "However, this does not necessarily mean no improvement.", "labels": [], "entities": []}, {"text": "Generally, in MT systems the less data you have, the less likely you are to have correct translations, and as it has been shown by, phrase-based SMT is quite robust to noise.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9794886112213135}, {"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.6681806445121765}]}, {"text": "Therefore bigger corpus despite containing more corrupt lines is not that detrimental to machine translation since it gets lost in translation anyway.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7404655814170837}]}, {"text": "While the BLEU score nearly did not change for the cleaned corpora, the corpus size, however, did.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9790565669536591}]}, {"text": "The cleaned corpora was respectively about 70% and 40% the size of the original.", "labels": [], "entities": []}, {"text": "This means that training and memory costs were much lower than the original corpus required.", "labels": [], "entities": []}, {"text": "Moreover, the huge difference in cleaned corpus size in comparison with the original producing the same BLEU score indicates that indeed the corrupt lines that the MT system also had deemed unfit were filtered out.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9756608009338379}, {"text": "MT", "start_pos": 164, "end_pos": 166, "type": "TASK", "confidence": 0.6967534422874451}]}, {"text": "To qualitatively evaluate the cleaning method, we randomly took 200 lines from the original as well as the cleaned corpora for Latvian-English and Latvian-French language pairs.", "labels": [], "entities": []}, {"text": "We manually evaluated them for incorrect or erroneous alignment.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The manual evaluation was done by one evaluator.", "labels": [], "entities": []}, {"text": "As apart of the corpora cleaning process, we implemented a corpus evaluation solution.", "labels": [], "entities": []}, {"text": "The percentage score of a corpus shows the amount of good lines in the text.", "labels": [], "entities": []}, {"text": "As models for cleaning could be constructed from any corpora that is recognized of good quality, we set to determine if the models are language independent.", "labels": [], "entities": []}, {"text": "That is, if different models (made from approximately equal quality corpora) would produce the same results fora given parallel corpus.", "labels": [], "entities": []}, {"text": "The models were trained on the DGT-TM 2007 corpus consisting of EN-LV, EN-FR, EN-LT, and FR-LV language pairs.", "labels": [], "entities": [{"text": "DGT-TM 2007 corpus", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9722591837247213}]}, {"text": "The graph lines represent the score of each corpus using the corresponding model (along the X axis).", "labels": [], "entities": []}, {"text": "Models themselves were evaluated using WEKA tool.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.6749560832977295}]}, {"text": "The results are shown below in.", "labels": [], "entities": []}, {"text": "The results show, overall, that the lower the quality of corpus, the more varied the cleaning results from different models will be.", "labels": [], "entities": []}, {"text": "It can be concluded that while there is a difference in the performance of the models (worst case up to 20%), it evens outwith the increase of the quality of the corpora (approx. 5% variation).", "labels": [], "entities": []}, {"text": "To sum up, for precise corpus evaluation, it would be best to use a model that has been built for the particular language pair.", "labels": [], "entities": [{"text": "corpus evaluation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.655718594789505}]}, {"text": "To see how the method fares with already good data, we evaluated the DGT-TM EnglishLithuanian corpus with the DGT-TM EnglishLatvian model as well as the DGT-TM FrenchEnglish corpus with the DGT-TM LatvianEnglish model.", "labels": [], "entities": [{"text": "DGT-TM EnglishLithuanian corpus", "start_pos": 69, "end_pos": 100, "type": "DATASET", "confidence": 0.8975578943888346}, {"text": "DGT-TM EnglishLatvian model", "start_pos": 110, "end_pos": 137, "type": "DATASET", "confidence": 0.8942479292551676}, {"text": "DGT-TM FrenchEnglish corpus", "start_pos": 153, "end_pos": 180, "type": "DATASET", "confidence": 0.8283470074335734}, {"text": "DGT-TM LatvianEnglish model", "start_pos": 190, "end_pos": 217, "type": "DATASET", "confidence": 0.8528044621149699}]}, {"text": "It removed approximately 3% of good sentences, which we think is acceptable.", "labels": [], "entities": []}, {"text": "Similarly OPUS EU Constitution corpus, which is considered fairly accurate, saw about 5% cut and showed considerably more stable results across all models than EU Bookshop corpora signaling reliable performance in case of high quality corpora.", "labels": [], "entities": [{"text": "OPUS EU Constitution corpus", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.9046732932329178}, {"text": "EU Bookshop corpora", "start_pos": 160, "end_pos": 179, "type": "DATASET", "confidence": 0.9147520661354065}]}], "tableCaptions": [{"text": " Table 1. Machine learning algorithm performance  comparison for Fast Align features.", "labels": [], "entities": []}, {"text": " Table 2. BLEU score for original and cleaned EU  Bookshop corpora (OPUS), good line amount after  cleaning.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9788955748081207}, {"text": "EU  Bookshop corpora (OPUS)", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.8643398185571035}]}, {"text": " Table 4. Corpora quality evaluation by Corpus  Cleaner and qualitative evaluation (QE)", "labels": [], "entities": [{"text": "Corpus  Cleaner", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8377873003482819}]}, {"text": " Table 5. Corpora quality evaluation by Corpus  Cleaner and qualitative evaluation (QE)", "labels": [], "entities": [{"text": "Corpus  Cleaner", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8384749591350555}]}]}