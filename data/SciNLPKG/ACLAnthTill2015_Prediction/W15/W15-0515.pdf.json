{"title": [{"text": "And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue", "labels": [], "entities": [{"text": "Distinguishing Factual and Emotional Argumentation in Online Dialogue", "start_pos": 19, "end_pos": 88, "type": "TASK", "confidence": 0.7245454676449299}]}], "abstractContent": [{"text": "We investigate the characteristics of factual and emotional argumentation styles observed in online debates.", "labels": [], "entities": []}, {"text": "Using an annotated set of FACTUAL and FEELING debate forum posts, we extract patterns that are highly correlated with factual and emotional arguments, and then apply a bootstrapping methodology to find new patterns in a larger pool of unanno-tated forum posts.", "labels": [], "entities": [{"text": "FACTUAL", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.7657080292701721}, {"text": "FEELING", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9094723463058472}]}, {"text": "This process automatically produces a large set of patterns representing linguistic expressions that are highly correlated with factual and emotional language.", "labels": [], "entities": []}, {"text": "Finally , we analyze the most discriminating patterns to better understand the defining characteristics of factual and emotional arguments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human lives are being lived online in transformative ways: people can now ask questions, solve problems, share opinions, or discuss current events with anyone they want, at anytime, in any location, on any topic.", "labels": [], "entities": []}, {"text": "The purposes of these exchanges are varied, but a significant fraction of them are argumentative, ranging from hot-button political controversies (e.g., national health care) to religious interpretation (e.g., Biblical exegesis).", "labels": [], "entities": []}, {"text": "And while the study of the structure of arguments has along lineage in psychology) and rhetoric, large shared corpora of natural informal argumentative dialogues have only recently become available.", "labels": [], "entities": []}, {"text": "Natural informal dialogues exhibit a much broader range of argumentative styles than found in traditional work on argumentation).", "labels": [], "entities": []}, {"text": "Recent work has begun to model different aspects of these natural informal arguments, with tasks including stance classification (), argument summarization (, sarcasm detection (, and work on the detailed structure of arguments.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.8860574662685394}, {"text": "argument summarization", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.7922086119651794}, {"text": "sarcasm detection", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.8233378231525421}]}, {"text": "Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents (, and in systems that support human argumentative behavior . Our research examines FACTUAL versus FEELING argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) (.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.9696155488491058}, {"text": "summarization", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.69049072265625}, {"text": "FEELING", "start_pos": 234, "end_pos": 241, "type": "METRIC", "confidence": 0.9130464792251587}, {"text": "Internet Argument Corpus (IAC)", "start_pos": 298, "end_pos": 328, "type": "DATASET", "confidence": 0.7696501314640045}]}, {"text": "This corpus includes quote-response pairs that were manually annotated with respect to whether the response is primarily a FACTUAL or FEELING based argument, as Section 2.1 describes in more detail.", "labels": [], "entities": [{"text": "FEELING", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.8460808992385864}]}, {"text": "provides examples of responses in the IAC (paired with preceding quotes to provide context), along with the response's FAC-TUAL vs. FEELING label.", "labels": [], "entities": [{"text": "IAC", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.4122295677661896}, {"text": "FAC-TUAL", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9574001431465149}, {"text": "FEELING", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.9134237766265869}]}, {"text": "FACTUAL responses may try to bolster their argument by providing statistics related to a position, giving historical or scientific background, or presenting specific examples or data.", "labels": [], "entities": [{"text": "FACTUAL responses", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.588834673166275}]}, {"text": "There is clearly a relationship between a proposition being FACTUAL versus OBJECTIVE or VERIDICAL, although each of these different labelling tasks may elicit differences from annotators (Wiebe and).", "labels": [], "entities": [{"text": "FACTUAL", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.5701035857200623}, {"text": "OBJECTIVE", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.7780106663703918}, {"text": "VERIDICAL", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9765985012054443}]}], "datasetContent": [{"text": "We evaluate the effectiveness of the learned patterns by applying them to the test set of 586 posts (347 FACT and 239 FEELING posts, maintaining the original ratio of FACT to FEEL data in train).", "labels": [], "entities": [{"text": "FACT", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.6772766709327698}, {"text": "FEELING", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9894165396690369}, {"text": "FACT", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.6916874051094055}, {"text": "FEEL", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.7845948338508606}]}, {"text": "We classify each post as FACTUAL or FEELING using the same procedure as during bootstrapping: a post is labeled as FACTUAL or FEELING if it matches at least three high-precision patterns for that category.", "labels": [], "entities": [{"text": "FEELING", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9685928821563721}, {"text": "FEELING", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9564633369445801}]}, {"text": "If a document contains three patterns for both categories, then we leave it unlabeled.", "labels": [], "entities": []}, {"text": "We ran the bootstrapping algorithm for four iterations.", "labels": [], "entities": []}, {"text": "The upper section of shows the Precision and Recall results for the patterns learned during bootstrapping.", "labels": [], "entities": [{"text": "Precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.995757520198822}, {"text": "Recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9657273292541504}]}, {"text": "The Iter 0 row shows the performance of the patterns learned only from the original, annotated training data.", "labels": [], "entities": [{"text": "Iter 0 row", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9442347685496012}]}, {"text": "The remaining rows show the results for the patterns learned from the unannotated texts during bootstrapping, added cumulatively.", "labels": [], "entities": []}, {"text": "We show the results after each iteration of bootstrapping.", "labels": [], "entities": []}, {"text": "shows that recall increases after each bootstrapping iteration, demonstrating that the patterns learned from the unannotated texts yield substantial gains in coverage over those learned only from the annotated texts.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9991311430931091}, {"text": "coverage", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.990688681602478}]}, {"text": "Recall increases from 22.8% to 40.9% for FACT, and from 8.0% to 18.8% for FEEL.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.995093584060669}, {"text": "FACT", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.41829240322113037}, {"text": "FEEL", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.6477228403091431}]}, {"text": "The precision for the FACTUAL class is reasonably good, but the precision for the FEELING class is only moderate.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996459484100342}, {"text": "FACTUAL class", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.6984200775623322}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9995495676994324}, {"text": "FEELING", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9761700630187988}]}, {"text": "However, although precision typically decreases during boostrapping due to the addition of imperfectly labeled data, the precision drop during bootstrapping is relatively small.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9992244243621826}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9992033839225769}]}, {"text": "We also evaluated the performance of a Naive Bayes (NB) classifier to assess the difficulty of this task with a traditional supervised learning algorithm.", "labels": [], "entities": []}, {"text": "We trained a Naive Bayes classifier with unigram features and binary values on the training data, and identified the best Laplace smoothing parameter using the development data.", "labels": [], "entities": []}, {"text": "The bottom row of Table 1 shows the results for the NB classifier on the test data.", "labels": [], "entities": [{"text": "NB classifier", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.5671746730804443}]}, {"text": "These results show that the NB classifier yields substantially higher recall for both categories, undoubtedly due to the fact that the classifier uses  all unigram information available in the text.", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9993712306022644}]}, {"text": "Our pattern learner, however, was restricted to learning linguistic expressions in specific syntactic constructions, usually requiring more than one word, because our goal was to study specific expressions associated with FACTUAL and FEELING argument styles.", "labels": [], "entities": [{"text": "FEELING", "start_pos": 234, "end_pos": 241, "type": "METRIC", "confidence": 0.8451728820800781}]}, {"text": "shows that the lexico-syntactic patterns did obtain higher precision than the NB classifier, but with lower recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9985314607620239}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.99826979637146}]}, {"text": "shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration.", "labels": [], "entities": [{"text": "Iter 0)", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9586453239123026}]}, {"text": "The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process.", "labels": [], "entities": []}, {"text": "The key take-away from this set of experiments is that distinguishing FACTUAL and FEELING argumets is clearly a challenging task.", "labels": [], "entities": [{"text": "FEELING", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.8753933310508728}]}, {"text": "There is substantial room for improvement for both precision and recall, and surprisingly, the FEELING class seems to be harder to accurately recognize than the FACTUAL class.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.999446451663971}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9991450309753418}, {"text": "FEELING", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9945507645606995}, {"text": "FACTUAL class", "start_pos": 161, "end_pos": 174, "type": "DATASET", "confidence": 0.7524125277996063}]}, {"text": "In the next section, we examine the learned patterns and their syntactic forms to better understand the language used in the debate forums.", "labels": [], "entities": []}, {"text": "provides examples of patterns learned for each class that are characteristic of that class.", "labels": [], "entities": []}, {"text": "We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases.", "labels": [], "entities": []}, {"text": "In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible, or they involve assessment or evaluations of the arguments.", "labels": [], "entities": []}, {"text": "They are typically also very creative and diverse, which maybe why it is hard to get higher accuracies for FEEL-ING classification, as shown by. shows the distribution of syntactic forms (templates) among all of the high-precision patterns identified for each class during bootstrapping.", "labels": [], "entities": [{"text": "FEEL-ING classification", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.6862916797399521}]}, {"text": "The x-axes show the syntactic templates and the y-axes show the percentage of all patterns that had a specific syntactic form.", "labels": [], "entities": []}, {"text": "counts each lexicosyntactic pattern only once, regardless of how many times it occurred in the data set.", "labels": [], "entities": []}, {"text": "counts the number of instances of each lexico-syntactic pattern.", "labels": [], "entities": []}, {"text": "For example, shows that the Adj Noun syntactic form produced 1,400 different patterns, which comprise 22.6% of the distinct patterns learned.", "labels": [], "entities": [{"text": "Adj Noun syntactic form", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.8861700743436813}]}, {"text": "captures the fact that there are 7,170 instances of the Adj Noun patterns, which comprise 17.8% of all patterns instances in the data set.", "labels": [], "entities": [{"text": "Adj Noun patterns", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9628357291221619}]}], "tableCaptions": [{"text": " Table 2: Examples of Characteristic Argumentation Style Patterns for Each Class", "labels": [], "entities": []}, {"text": " Table 3: Number of New Patterns Added after Each  Round of Bootstrapping", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.975827693939209}, {"text": "Bootstrapping", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.36545783281326294}]}]}