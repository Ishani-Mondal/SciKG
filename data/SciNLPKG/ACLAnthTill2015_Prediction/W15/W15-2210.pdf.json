{"title": [{"text": "Non-Deterministic Oracles for Unrestricted Non-Projective Transition-Based Dependency Parsing", "labels": [], "entities": [{"text": "Unrestricted Non-Projective Transition-Based Dependency Parsing", "start_pos": 30, "end_pos": 93, "type": "TASK", "confidence": 0.5942006349563599}]}], "abstractContent": [{"text": "We study non-deterministic oracles for training non-projective beam search parsers with swap transitions.", "labels": [], "entities": []}, {"text": "We map out the spurious ambiguities of the transition system and present two non-deterministic oracles as well as a static oracle that minimizes the number of swaps.", "labels": [], "entities": []}, {"text": "An evaluation on 10 treebanks reveals that the difference between static and non-deterministic oracles is generally insignificant for beam search parsers but that non-deterministic oracles can improve the accuracy of greedy parsers that use swap transitions.", "labels": [], "entities": [{"text": "beam search parsers", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7815319697062174}, {"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9969046711921692}]}], "introductionContent": [{"text": "Training transition-based dependency parsers relies on an oracle -a function that, given a parser configuration and a gold dependency tree, returns the correct transition.", "labels": [], "entities": []}, {"text": "The sequence of transitions required to derive a given dependency tree is, however, typically not unique, and for certain configurations more than one transition is correct.", "labels": [], "entities": []}, {"text": "This issue has typically been dealt with by defining a canonical order among the transitions, thereby resolving such ambiguities in a deterministic way.", "labels": [], "entities": []}, {"text": "In addition to the determinism, standard oracles also make the assumption that the gold tree can be recovered from the current configuration.", "labels": [], "entities": []}, {"text": "Oracles with this behavior are known as static oracles.", "labels": [], "entities": []}, {"text": "Recently, much work has been devoted to the development of dynamic oracles that do away with both of these assumptions).", "labels": [], "entities": []}, {"text": "Dynamic oracles have been shown to be very successful for training greedy parsers.", "labels": [], "entities": []}, {"text": "Since greedy parsers typically suffer from error propagation, dynamic oracles enable the parsers to learn to do \"the next best thing\" after having made a mistake, resulting in considerable improvements in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 205, "end_pos": 212, "type": "TASK", "confidence": 0.9546350240707397}, {"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.8627443909645081}]}, {"text": "Nevertheless, greedy transition-based parsers still lag behind search-based parsers that explore a larger set of possible transition sequences.", "labels": [], "entities": []}, {"text": "Searchbased parsers are typically realized through beam search and trained using global learning, where a discriminative model is trained to score not just single transitions, but a sequence of transitions (.", "labels": [], "entities": []}, {"text": "The combination of non-greedy inference and global learning enables search-based parsers to overcome the error propagation problem.", "labels": [], "entities": []}, {"text": "However, since the model is globally trained, oracles that can recover from past mistakes and do the next best thing are not applicable in this scenario.", "labels": [], "entities": []}, {"text": "On the other hand, it is an open question whether search-based parsers should be trained using static oracles, or whether their performance can be further increased by using a non-deterministic oracle, i.e., an oracle that considers all possible transition sequences that can derive the gold dependency tree.", "labels": [], "entities": []}, {"text": "We evaluate the hypothesis that transition-based parsers with beam search can be improved by using non-deterministic oracles during training.", "labels": [], "entities": []}, {"text": "We do this in the context of the transition system by, henceforth SwapStandard, which extends the ArcStandard system) with a swap transition to accommodate nonprojective dependency trees.", "labels": [], "entities": []}, {"text": "This system has been shown to be very effective with beam search, even rivaling graph-based dependency parsers.", "labels": [], "entities": [{"text": "beam search", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9410832226276398}]}, {"text": "Empirically, we find that the utility of non-deterministic oracles for training beam search parsers is rather limited and typically the difference compared to a static oracle is insignificant.", "labels": [], "entities": [{"text": "training beam search parsers", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6775212064385414}]}, {"text": "However, our experiments also show that the non-deterministic oracles can be beneficial when training greedy parsers, a result that has not previously been shown for nonprojective systems.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is the first characterization of a non-deterministic oracle for the SwapStandard system, based on a thorough analysis of spurious ambiguities.", "labels": [], "entities": []}, {"text": "As a side-result, we also arrive at a static oracle that minimizes the number of swap transitions required to parse any non-projective dependency tree, thereby solving a previously open problem).", "labels": [], "entities": []}, {"text": "In addition, we provide the first empirical evaluation of non-deterministic oracles for training beam search parsers, as well as the first evaluation with greedy parsers using a nonprojective transition system.", "labels": [], "entities": [{"text": "training beam search parsers", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.6467482373118401}]}], "datasetContent": [{"text": "In total we experiment with five different oracles.", "labels": [], "entities": []}, {"text": "The three static ones, EAGER, LAZY, and MINIMAL, all use a single unique transition sequence for every sentence in the training data.", "labels": [], "entities": [{"text": "EAGER", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.94732666015625}, {"text": "LAZY", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9536539316177368}, {"text": "MINIMAL", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.6912516355514526}]}, {"text": "The two non-deterministic oracles, ND-SW and ND-ALL, create latent transition sequences on the fly relying on the current parameters and may change across training iterations.", "labels": [], "entities": []}, {"text": "Our main hypothesis is that the latent sequences created by the non-deterministic oracles are easier to learn and generalize better to unseen data, leading to increased accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9970755577087402}]}, {"text": "We evaluate the oracles on ten treebanks.", "labels": [], "entities": []}, {"text": "Specifically, we use the nine treebanks from the SPMRL 2014 Shared Task (), comprising Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, and Swedish.", "labels": [], "entities": [{"text": "SPMRL 2014 Shared Task", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.6094639301300049}]}, {"text": "For these treebanks we use the train/dev/test splits provided by the Shared Task organizers.", "labels": [], "entities": []}, {"text": "Additionally, we use the English Penn Treebank () converted to Stanford dependencies) with the standard split, sections 2-21 for training, section 24 for development, and section 23 for test.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.829922616481781}]}, {"text": "A breakdown of the characteristics of the training sets of each treebank is shown in  table shows the total number of sentences and the percentage of projective sentences.", "labels": [], "entities": []}, {"text": "It also shows the total number of swap transitions required by EAGER, and the reduction of swaps of LAZY and MINIMAL relative to EAGER.", "labels": [], "entities": [{"text": "LAZY", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9761297106742859}, {"text": "MINIMAL", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.6292015910148621}]}, {"text": "For instance, in the Arabic treebank 97.32% of the sentences are projective and the LAZY and MINIMAL reduce the number of swaps by 80.59% and 80.79%, respectively.", "labels": [], "entities": [{"text": "Arabic treebank", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.8254110217094421}, {"text": "LAZY", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9916651844978333}, {"text": "MINIMAL", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9606495499610901}]}, {"text": "For about half the treebanks LAZY is already minimal and we exclude MINIMAL.", "labels": [], "entities": [{"text": "treebanks", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.834033191204071}, {"text": "LAZY", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.622029721736908}, {"text": "MINIMAL", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.6472020745277405}]}, {"text": "Korean is the only strictly projective treebank, although some of the treebanks have very few nonprojective arcs in their training sets, particularly Hebrew and French.", "labels": [], "entities": []}, {"text": "This means that the number of SH-SW ambiguities considered by the nondeterministic oracles during training is extremely small.", "labels": [], "entities": []}, {"text": "The ND-SW oracle thus exhibits a very tiny amount of spurious ambiguity in these cases.", "labels": [], "entities": []}, {"text": "Nevertheless, ND-ALL will still consider the SH-LA ambiguity.", "labels": [], "entities": [{"text": "ND-ALL", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.880414605140686}]}, {"text": "The last row of shows the percentage of sentences that exhibit no spurious ambiguity under the ND-ALL oracle.", "labels": [], "entities": []}, {"text": "This fraction ranges between almost 0% and up to about 10%, which means that there are indeed plenty of spurious ambiguities in the training data.", "labels": [], "entities": []}, {"text": "We adopt a realistic evaluation setting and use predicted part-of-speech tags and morphological features.", "labels": [], "entities": []}, {"text": "Specifically, we use MarMoT (), a state-of-the-art CRF tagger that jointly predicts part-of-speech tags and morphology.", "labels": [], "entities": [{"text": "CRF tagger", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.7706954181194305}]}, {"text": "We train the parsers on 10-fold jackknifed training data.", "labels": [], "entities": []}, {"text": "For the development and test sets the tagger is trained on the full training set.", "labels": [], "entities": []}, {"text": "We evaluate the parsers using labeled attachment score (LAS), i.e., the percentage of arcs that have the correct heads and labels.", "labels": [], "entities": [{"text": "labeled attachment score (LAS)", "start_pos": 30, "end_pos": 60, "type": "METRIC", "confidence": 0.873212864001592}]}, {"text": "We omit the unlabeled version of this metric as we observed that it is closely correlated with LAS.", "labels": [], "entities": [{"text": "LAS", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.8884245753288269}]}, {"text": "We test for significance using the Wilcoxon signed rank test and mark significance at the p < 0.05 and p < 0.01 levels with \u2020 and \u2021, respectively.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.8612849712371826}]}, {"text": "Since the parsers trained using the non-deterministic oracles rely on a latent sequence, they might require more training iterations before reaching good performance.", "labels": [], "entities": []}, {"text": "Moreover, during initial experiments on the development data we saw that the learning curves are not monotonically increasing.", "labels": [], "entities": []}, {"text": "To test our main hypothesis -that beam-search parsers can profit from training with a non-deterministic oracle -we tune the number of training iterations on the development sets for each oracle and treebank.", "labels": [], "entities": []}, {"text": "shows the learning curves of the beam search parser on German and Hungarian.", "labels": [], "entities": [{"text": "beam search parser", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.786448081334432}, {"text": "German and Hungarian", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.8877448836962382}]}, {"text": "These two plots are chosen since they paint a rather divergent picture, where EAGER is clearly underperforming for German, and ND-ALL is considerably worse than other oracles for Hungarian.", "labels": [], "entities": [{"text": "EAGER", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9942822456359863}, {"text": "ND-ALL", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9331871867179871}]}, {"text": "For most of the other treebanks, however, the learning curves are surprisingly similar for all oracles.: Test set result with beam search comparing the best static and non-deterministic oracles.", "labels": [], "entities": []}, {"text": "displays the LAS on the development sets for each oracle after tuning.", "labels": [], "entities": [{"text": "LAS", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9965968728065491}]}, {"text": "When LAZY is already minimal, we omit MINIMAL.", "labels": [], "entities": [{"text": "LAZY", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9735456705093384}, {"text": "MINIMAL", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.6347149014472961}]}, {"text": "Since Korean is projective, we only compare ND-ALL and EAGER, which thus reduces to comparing a static and a non-deterministic oracle for ArcStandard.", "labels": [], "entities": [{"text": "EAGER", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9696952700614929}]}], "tableCaptions": [{"text": " Table 1: Data set statistics (training sets).", "labels": [], "entities": []}, {"text": " Table 2: Beam search results on dev sets. The best non-deterministic and static oracles are bold.", "labels": [], "entities": []}, {"text": " Table 3: Test set result with beam search comparing the best static and non-deterministic oracles.", "labels": [], "entities": []}, {"text": " Table 3. In  about half the cases the non-deterministic oracle  does slightly better than the static one, but in the  other half it is the other way around. The only sig- nificant difference is the improvement of the non-", "labels": [], "entities": []}, {"text": " Table 4: Greedy results on dev sets. The best non-deterministic and static oracles are bold.", "labels": [], "entities": []}, {"text": " Table 5: Test set result with greedy search comparing the best static and non-deterministic oracles.", "labels": [], "entities": []}]}