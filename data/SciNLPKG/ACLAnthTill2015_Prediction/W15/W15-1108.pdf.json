{"title": [{"text": "Utility-based evaluation metrics for models of language acquisition: A look at speech segmentation", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7105395346879959}]}], "abstractContent": [{"text": "Models of language acquisition are typically evaluated against a \"gold standard\" meant to represent adult linguistic knowledge, such as orthographic words for the task of speech seg-mentation.", "labels": [], "entities": []}, {"text": "Yet adult knowledge is rarely the target knowledge for the stage of acquisition being modeled, making the gold standard an imperfect evaluation metric.", "labels": [], "entities": []}, {"text": "To supplement the gold standard evaluation metric, we propose an alternative utility-based metric that measures whether the acquired knowledge facilitates future learning.", "labels": [], "entities": []}, {"text": "We take the task of speech segmentation as a case study, assessing previously proposed models of segmen-tation on their ability to generate output that (i) enables creation of language-specific seg-mentation cues that rely on stress patterns, and (ii) assists the subsequent acquisition task of learning word meanings.", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7554084658622742}]}, {"text": "We find that behavior that maximizes gold standard performance does not necessarily maximize the utility of the acquired knowledge, highlighting the benefit of multiple evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Over the past decades, computational modeling has become an increasingly useful tool for studying the ways children acquire their native language.", "labels": [], "entities": [{"text": "computational modeling", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7204601466655731}]}, {"text": "Modeling allows researchers to explicitly evaluate learning strategies by whether these strategies would enable acquisition success.", "labels": [], "entities": []}, {"text": "But how do researchers determine if a particular learning strategy is successful?", "labels": [], "entities": []}, {"text": "Traditionally, models have been evaluated against adult linguistic knowledge, typically captured in an explicit \"gold standard\".", "labels": [], "entities": []}, {"text": "If the modeled learner succeeds at acquiring this adult linguistic knowledge, then it is said to have succeeded and the learning strategy is held up as a viable option for how the acquisition process might work.", "labels": [], "entities": []}, {"text": "Gold standard evaluation has two key benefits.", "labels": [], "entities": [{"text": "Gold standard evaluation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5158621470133463}]}, {"text": "First, it provides a uniform measure of evaluation, especially when gold standards are relatively similar across corpora (e.g. orthographic segmentation for speech).", "labels": [], "entities": []}, {"text": "Second, this kind of evaluation is typically straightforward to implement for labeled corpora, and so is easy to use for model comparison.", "labels": [], "entities": []}, {"text": "Still, there are several potential disadvantages to gold standard evaluation.", "labels": [], "entities": [{"text": "gold standard evaluation", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.49168814222017926}]}, {"text": "First, the choice of an appropriate gold standard is non-trivial for many linguistic tasks since there is disagreement about what the adult knowledge actually is (e.g., speech segmentation, grammatical categorization, syntactic parsing).", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.717327818274498}, {"text": "syntactic parsing", "start_pos": 218, "end_pos": 235, "type": "TASK", "confidence": 0.7639873027801514}]}, {"text": "Second, implementation may require a large amount of time-consuming manual annotation (e.g. visual scene labeling for word-object mapping).", "labels": [], "entities": [{"text": "word-object mapping", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.6998847424983978}]}, {"text": "Third, and perhaps most importantly, it is unclear that adult knowledge is the appropriate output for some modeled learning strategies, particularly those that are meant to occur early in acquisition.", "labels": [], "entities": []}, {"text": "For example, consider the early stages of speech segmentation that rely only on probabilistic cues.", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7119199931621552}]}, {"text": "The earliest evidence of speech segmentation comes at six months) and it appears that probabilistic cues to segmentation, which are language-independent because their implementation does not depend on the specific language being acquired, give way to language-dependent cues between eight and nine months.", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7056160718202591}]}, {"text": "So, accurate models of this early stage of speech segmentation should output the knowledge that a nine-month-old has, and this may differ quite significantly from the knowledge an adult has about how to segment speech.", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7472037076950073}]}, {"text": "Unfortunately, addressing this last issue with gold standard evaluation is non-trivial.", "labels": [], "entities": []}, {"text": "One strategy might be to create a gold standard representing ageappropriate knowledge.", "labels": [], "entities": []}, {"text": "However, without empirical data that can identify exactly what children's knowledge at a particular age is, this is difficult.", "labels": [], "entities": []}, {"text": "Because of this, few (if any) age-specific gold standards exist for the many acquisition tasks that we wish to evaluate learning strategies for.", "labels": [], "entities": []}, {"text": "An alternative is to compare model results against qualitative patterns that have been reported in the developmental literature.", "labels": [], "entities": []}, {"text": "For instance, compares his segmentation model results against qualitative patterns of over-and undersegmentation reported in diary data.", "labels": [], "entities": []}, {"text": "Still, such comparisons are often difficult to make since the behavioral data may come from children of different ages than the modeled learners (e.g., the segmentation patterns mentioned above come from two-and three-yearolds while the modeled learners are at most nine months old).", "labels": [], "entities": []}, {"text": "So, the essence of the evaluation problem is this: the true target for model output is potentially unknown, but we still wish to evaluate different models.", "labels": [], "entities": []}, {"text": "Fortunately for language acquisition modelers, this is exactly the problem faced in computer science when unsupervised learning algorithms are applied and a gold standard does not exist.", "labels": [], "entities": [{"text": "language acquisition modelers", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.8223403890927633}]}, {"text": "There are two main ways a model without a gold standard can be explicitly evaluated): 1.", "labels": [], "entities": []}, {"text": "Apply real-world, expert knowledge to determine if the output is reasonable.", "labels": [], "entities": []}, {"text": "2. Measure the \"utility\" of the output.", "labels": [], "entities": []}, {"text": "Adding these two evaluation approaches to a language acquisition modeler's toolbox can help alleviate the issues surrounding gold standards.", "labels": [], "entities": []}, {"text": "Still, the first option of applying expert knowledge is often time intensive, since this typically involves querying human knowledge.", "labels": [], "entities": []}, {"text": "Moreover, given the key concern about what the output of language acquisition models ought to look like anyway, it is unclear that querying linguistic experts is appropriate.", "labels": [], "entities": []}, {"text": "Given this, we focus on measuring the utility of the model's output) to supplement a gold standard analysis.", "labels": [], "entities": []}, {"text": "This means we must be more precise about \"utility\".", "labels": [], "entities": []}, {"text": "Because children acquire linguistic knowledge and then apply that acquired knowledge to learn more of their native language system, one definition of utility for language acquisition is for the model output to facilitate further knowledge acquisition.", "labels": [], "entities": []}, {"text": "Importantly, determining what future knowledge is acquired is often much easier than determining the exact state of that knowledge, as with a gold standard.", "labels": [], "entities": []}, {"text": "This is because we often have empirical data about the order in which linguistic knowledge is acquired (e.g., language-independent cues to speech segmentation are used to identify languagedependent cues, which are then used to facilitate further segmentation).", "labels": [], "entities": []}, {"text": "We can use these empirical data to identify what a model's output should be used for, and assess if the acquired knowledge helps the learner acquire the appropriate additional knowledge.", "labels": [], "entities": []}, {"text": "Then, if a modeled strategy yields this kind of useful knowledge, the modeled strategy should be counted as successful; in contrast, if the acquired knowledge isn't useful (or is actively harmful), then this is a mark of failure.", "labels": [], "entities": []}, {"text": "Under this view, a strategy's utility is equivalent to its ability to prepare the learner for subsequent acquisition tasks.", "labels": [], "entities": []}, {"text": "As we will see when we apply this utility-based evaluation to speech segmentation strategies, we may still encounter some familiar evaluation issues.", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7089438587427139}]}, {"text": "In particular, to evaluate whether a model's output prepares a learner for subsequent acquisition tasks, we must have some idea as to what counts as \"good enough\" preparation for those subsequent tasks.", "labels": [], "entities": []}, {"text": "The simplest answer seems to be that \"good enough\" for the subsequent task means that the output for that task is \"good enough\" for the next task after that.", "labels": [], "entities": []}, {"text": "In some sense then, the best indicator of utility would be that the modeled strategy yields adult level knowledge once the entire acquisition process is complete.", "labels": [], "entities": []}, {"text": "However, it is currently impractical to model the entire language acquisition process.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7030978351831436}]}, {"text": "Instead, we have to restrict ourselves to smaller seg-ments of the entire process -here, two sequential stages.", "labels": [], "entities": []}, {"text": "Given the available empirical data, it maybe that we have a better idea about what children's knowledge is for the second stage than we do for the first stage.", "labels": [], "entities": []}, {"text": "That is, an age-appropriate gold standard maybe available for the subsequent acquisition task.", "labels": [], "entities": []}, {"text": "For both utility evaluations we do here, we have something like this for each subsequent task, though it is likely still an imperfect approximation of young children's knowledge.", "labels": [], "entities": []}, {"text": "We note that this utility-based approach differs from a joint inference approach, where two tasks occur simultaneously and information from one task helpfully informs the other).", "labels": [], "entities": []}, {"text": "Joint inference is appropriate when we have empirical evidence that children accomplish both tasks at the same time.", "labels": [], "entities": [{"text": "Joint inference", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7632310092449188}]}, {"text": "In contrast, the utility-based evaluation approach is appropriate when empirical evidence suggests children accomplish tasks sequentially.", "labels": [], "entities": []}, {"text": "In this paper, we consider the task of speech segmentation and investigate different ways of assessing the utility of previously proposed strategies.", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7124218344688416}]}, {"text": "Notably, these strategies have generally succeeded when evaluated against some version of a gold standard).", "labels": [], "entities": []}, {"text": "We first briefly review speech segmentation in infants, and then describe the segmentation strategies previously investigated: a Bayesian segmentation strategy) and a subtractive segmentation strategy.", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7272349148988724}]}, {"text": "We then evaluate each modeled strategy on two utility measures relating to (i) the creation of language-dependent segmentation cues relying on stress, and (ii) the subsequent acquisition task of learning word meanings.", "labels": [], "entities": []}, {"text": "We find that the strategies differ significantly in their ability to identify stress segmentation cues and facilitate word meaning acquisition, with the Bayesian strategy yielding more useful output than the subtractive segmentation strategy.", "labels": [], "entities": [{"text": "identify stress segmentation cues", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.7236458286643028}, {"text": "word meaning acquisition", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.7952797611554464}]}, {"text": "We discuss how these utility results relate to other qualitative patterns, such as oversegmentation, noting that behavior that maximizes performance against a gold standard does not necessarily maximize the utility of the acquired knowledge for subsequent learning.", "labels": [], "entities": []}, {"text": "Originally, the word-object model was evaluated on a small subset of 700 utterances from the Rollins corpus from CHILDES) which was labeled with visually salient objects (O in the).", "labels": [], "entities": [{"text": "Rollins corpus from CHILDES", "start_pos": 93, "end_pos": 120, "type": "DATASET", "confidence": 0.9200448393821716}]}, {"text": "We used this corpus to evaluate the segmentation strategies.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9656818509101868}]}, {"text": "We first trained the segmentation strategies on the 28,391 utterances of the UCI Brent Syllables corpus so that the modeled learners using those strategies could infer a lexicon of word forms with associated probabilities of occurrence.", "labels": [], "entities": [{"text": "UCI Brent Syllables corpus", "start_pos": 77, "end_pos": 103, "type": "DATASET", "confidence": 0.9700993001461029}]}, {"text": "We then applied the resulting knowledge to the Rollins corpus subset, letting each strategy segment those utterances as best it could, given the knowledge it had inferred from the training set.", "labels": [], "entities": [{"text": "Rollins corpus subset", "start_pos": 47, "end_pos": 68, "type": "DATASET", "confidence": 0.9284694194793701}]}, {"text": "The word-object mapping model was then applied with the inferred segmentations as part of the observed input (W).", "labels": [], "entities": []}, {"text": "Due to the stochastic nature of the inference process, we repeated this process five times and present averaged results.", "labels": [], "entities": []}, {"text": "We present lexical precision scores due to the importance of inferring high quality mappings during early word meaning learning.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9691539406776428}, {"text": "word meaning learning", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.6901467243830363}]}, {"text": "However, to measure precision we need to identify what constitutes a \"correct\" mapping.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9993404746055603}]}, {"text": "created a gold standard referential lexicon by hand and we follow their basic guidelines in creating our own.", "labels": [], "entities": []}, {"text": "One consideration when dealing with non-adult segmentation is the possibility of legitimate mappings between non-words and objects.", "labels": [], "entities": [{"text": "non-adult segmentation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7390294373035431}]}, {"text": "For instance, the undersegmenation abunny might reasonably be mapped onto the object BUNNY.", "labels": [], "entities": [{"text": "BUNNY", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9046168327331543}]}, {"text": "Our gold standard referential lexicon allows these combinations of determiners and content words as legitimate \"words\" for an object to be mapped to, unlike the original study.", "labels": [], "entities": []}, {"text": "In contrast, an oversgementation like du or ckie for duckie was not allowed as a correct \"word\" for the object DUCK.", "labels": [], "entities": []}, {"text": "This is because neither unit (du or ckie) captures the true word form.", "labels": [], "entities": []}, {"text": "For instance, it isn't good if the child thinks every instance of /ki/ -key, ckie, etc.", "labels": [], "entities": []}, {"text": "Given this, oversegmention errors are worse than undersegmentations, since they damage the ability to form a reasonable word-object mapping.", "labels": [], "entities": []}, {"text": "presents the evaluation results for all modeled learners, including the segmentation word token F-scores, the rate of oversegmentation errors, and the referential lexicon precision scores.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.6762774586677551}, {"text": "referential lexicon precision scores", "start_pos": 151, "end_pos": 187, "type": "METRIC", "confidence": 0.6395566686987877}]}, {"text": "We additionally show the word-object mapping results based on the adult orthographic segmentation as an upper-bound comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word token F-score results on the UCI Brent  Syllables corpus as reported by Phillips and Pearl (in  press) for the Bayesian learners (Batch vs. Online, Un- igram vs. Bigram), the subtractive segmenter, and the  random oracle baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.8353673219680786}, {"text": "UCI Brent  Syllables corpus", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.9612782895565033}]}, {"text": " Table 2: Stress pattern results for all learners on bisyl- labic word types. Percentages are calculated out of all  bisyllabic words identified by the model.", "labels": [], "entities": []}, {"text": " Table 3: Stress pattern results for all learners on trisyl- labic word types. Percentages are calculated out of all  trisyllabic words identified by the model.", "labels": [], "entities": []}, {"text": " Table 4: Average results over five runs from all modeled  learners, showing word token F-score segmentation per- formance, the rate of oversegmentation errors, and the  precision of the inferred referential lexicon.", "labels": [], "entities": [{"text": "precision", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9991270899772644}]}]}