{"title": [{"text": "Alignment of Eye Movements and Spoken Language for Semantic Image Understanding", "labels": [], "entities": [{"text": "Alignment of Eye Movements", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8193927854299545}, {"text": "Semantic Image Understanding", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.7272281249364217}]}], "abstractContent": [{"text": "Extracting meaning from images is a challenging task that has generated much interest in recent years.", "labels": [], "entities": [{"text": "Extracting meaning from images", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.92865951359272}]}, {"text": "In domains such as medicine, image understanding requires special expertise.", "labels": [], "entities": [{"text": "image understanding", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.9151550829410553}]}, {"text": "Experts' eye movements can act as pointers to important image regions, while their accompanying spoken language descriptions, informed by their knowledge and experience, call attention to the concepts and features associated with those regions.", "labels": [], "entities": []}, {"text": "In this paper, we apply an unsupervised alignment technique, widely used in machine translation to align parallel corpora, to align observers' eye movements with the verbal narrations they produce while examining an image.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.711967870593071}]}, {"text": "The resulting alignments can then be used to create a database of low-level image features and high-level semantic annotations corresponding to perceptually important image regions.", "labels": [], "entities": []}, {"text": "Such a database can in turn be used to automatically annotate new images.", "labels": [], "entities": []}, {"text": "Initial results demonstrate the feasibility of a framework that draws on recognized bitext alignment algorithms for performing unsupervised automatic semantic annotation of image regions.", "labels": [], "entities": []}, {"text": "Planned enhancements to the methods are also discussed.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to identify and describe the important regions of an image is useful fora range of visionbased reasoning tasks.", "labels": [], "entities": []}, {"text": "When expert observers communicate the outcome of vision-based reasoning, spoken language is the most natural and convenient instrument for conveying their understanding.", "labels": [], "entities": []}, {"text": "This paper reports on a novel approach for semantically annotating important regions of an image with natural language descriptors.", "labels": [], "entities": []}, {"text": "The proposed method builds on prior work in machine translation for bitext alignment () but differs in the insight that such methods can be applied to align multimodal visual-linguistic data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7496163845062256}, {"text": "bitext alignment", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.6028152853250504}]}, {"text": "Using these methods, we report on initial steps to integrate eye movements and transcribed spoken narratives elicited from expert observers inspecting medical images.", "labels": [], "entities": []}, {"text": "Our work relies on the fact that observers' eye movements over an image reveal what they consider to be the important image regions, their relation to one another, and their relation to the image inspection objectives.", "labels": [], "entities": []}, {"text": "Observers' co-captured narrations about the image naturally express relevant meaning and, especially inexpert domains, special knowledge and experience that guide vision-based problem-solving and decision-making.", "labels": [], "entities": []}, {"text": "Despite being co-captured, precise time synchronization between eye movement and narrations cannot be assumed (.", "labels": [], "entities": []}, {"text": "Therefore, techniques are needed to integrate the visual data with the linguistic data.", "labels": [], "entities": []}, {"text": "When meaningfully aligned, such visual-linguistic data can be used to annotate important image regions with appropriate lexical concepts.", "labels": [], "entities": []}, {"text": "We treat the problem of integrating the two data streams as analogous to the alignment of a parallel corpus, or bitext, in machine translation, in which the words of a sentence in one language are aligned to their corresponding translations in another language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7258258163928986}]}, {"text": "For our problem, eye movements on images are considered to be the visual language comprising visual units of analysis, while the transcribed narratives contain the linguistic units of analysis.", "labels": [], "entities": []}, {"text": "Previous work has investigated the association of words with pictures, words with videos, and words with objects and image regions).", "labels": [], "entities": []}, {"text": "But the combination of perceptual information (via eye movements) and more naturally obtained conceptual information (via narratives) will greatly improve the understanding of the semantics of an image, allowing image regions that are relevant for an image inspection task to be annotated with meaningful linguistic descriptors.", "labels": [], "entities": []}, {"text": "In this ongoing work, we use dermatological images as a test case to learn the alignments between the two types of units of analysis.", "labels": [], "entities": []}, {"text": "We primarily seek to determine whether a bitext alignment approach can be used to align multimodal data consisting of eye movements and transcribed narratives.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Alignment precision, recall, and F-measure for 5 images. Higher values indicate better align- ment.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.933443546295166}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8241131901741028}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9997550845146179}, {"text": "F-measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9993782043457031}]}]}