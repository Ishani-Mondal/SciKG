{"title": [{"text": "Designing an Algorithm for Generating Named Spatial References", "labels": [], "entities": [{"text": "Generating Named Spatial References", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.621262937784195}]}], "abstractContent": [{"text": "We describe an initial version of an algorithm for generating named references to locations of geographic scale.", "labels": [], "entities": []}, {"text": "We base the algorithm design on evidence from corpora and experiments, which show that named entity usage is extremely frequent, even in less obvious scenes, and that names are normally used as the first focus on a global region.", "labels": [], "entities": []}, {"text": "The current algorithm normally selects the Frames of Reference that humans also select, but it needs improvement to mix frames via a mereologi-cal mechanism.", "labels": [], "entities": []}], "introductionContent": [{"text": "Geospatial data of public interest such as weather prediction data and river level data are increasingly made publicly available, e.g. DataPoint from the Met Office in the UK, River Level data from SEPA in Scotland and Global Forecast system data from NOAA in the US.", "labels": [], "entities": [{"text": "River Level data from SEPA", "start_pos": 176, "end_pos": 202, "type": "DATASET", "confidence": 0.739820945262909}]}, {"text": "We are interested in developing computational techniques for expressing the information content extracted from these datasets in natural language using data-to-text natural language generation () techniques.", "labels": [], "entities": [{"text": "data-to-text natural language generation", "start_pos": 152, "end_pos": 192, "type": "TASK", "confidence": 0.6987378746271133}]}, {"text": "For example, from precipitation prediction data corresponding to several locations across Scotland, we are developing techniques to automatically generate the statement Heavy rain likely to fall as snow on higher ground in the northeast of Scotland.", "labels": [], "entities": [{"text": "precipitation prediction", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.6565272659063339}]}, {"text": "An important subtask here is to automatically generate the spatial referring expression (SRE) higher ground in the northeast of Scotland to linguistically express the location of the snowing event found in the precipitation prediction data.", "labels": [], "entities": [{"text": "spatial referring expression (SRE)", "start_pos": 59, "end_pos": 93, "type": "METRIC", "confidence": 0.6907186011473337}]}, {"text": "This paper presents corpus analysis and experimental studies to guide the design of an algorithm for SRE generation.", "labels": [], "entities": [{"text": "SRE generation", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.9944096505641937}]}, {"text": "Studies of human written show abroad range of descriptors such as north, east, coastal, inland, urban, and rural to specify locations.", "labels": [], "entities": []}, {"text": "Descriptors belong to one of many perspectives on the scene, or Frames of Reference or FoR for short, such as direction, coastal proximity, population density and altitude.", "labels": [], "entities": []}, {"text": "Our own corpus studies (Section 2) show that geographic names are the dominant descriptors in weather forecast texts, route descriptions and river level forecast reports.", "labels": [], "entities": []}, {"text": "Our experiment to empirically understand the extent of usage of geographical names in SREs (Section 3) also shows that names are the most used descriptors, as well as the FoR that sets the first focus on a region.", "labels": [], "entities": [{"text": "FoR", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.626442551612854}]}, {"text": "Using this empirical knowledge we propose an initial version of an algorithm (Section 4) that automatically generates SREs using names as well as other descriptors.", "labels": [], "entities": []}], "datasetContent": [{"text": "Even though the corpus analysis returned fruitful insights, we remained with a major shortfall to design a computational algorithm for an NLG system.", "labels": [], "entities": []}, {"text": "We expect such an algorithm to be used in data-to-text systems -i.e. systems that write text from information stored in data bases -so a dataand-text parallel corpus is more suitable to inform us what our SREG algorithm must consider.", "labels": [], "entities": []}, {"text": "Thus we resorted to experiments with human participants to collect spatial expressions, while having full access to the data underlying the text.", "labels": [], "entities": []}, {"text": "The above results were not formally verified with statistical tests because we believe our sample ofname-1st If both names and other FoR were used, but named entities were used as first focus.", "labels": [], "entities": []}, {"text": "both-1st If names and other FoR don't compete for first focus, but remain on the same level, so the resulting subregion is a union of multiple sub-regions.", "labels": [], "entities": []}, {"text": "For example: northwestern Fruitport...", "labels": [], "entities": [{"text": "Fruitport", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.713118314743042}]}, {"text": "not in the far northeast or southeast.", "labels": [], "entities": []}, {"text": "Fruitport, Breading and Meatcott are named regions but far north-east and south-east are directions.", "labels": [], "entities": [{"text": "Fruitport", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9886061549186707}, {"text": "Breading", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.7955144643783569}, {"text": "Meatcott", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.638217568397522}]}, {"text": "None is apart of the other, so the named areas and not far northeast and south-east complement each other at the same focus level.", "labels": [], "entities": []}, {"text": "none If no FoR, but only vague descriptors were used.", "labels": [], "entities": [{"text": "FoR", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.7817102074623108}]}, {"text": "Finally we counted all possible combinations of FoR usage and aligned those with experimental conditions, as displayed in.", "labels": [], "entities": []}, {"text": "The first intriguing observation is that 5 responses did not use any FoR, according to our annotation.", "labels": [], "entities": [{"text": "FoR", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8624072074890137}]}, {"text": "2 of them used only a quantifier (much, most), 2 only the name of the country (Musicland), and 1 used both (some parts of Musicland).", "labels": [], "entities": [{"text": "Musicland", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.9668256640434265}, {"text": "Musicland", "start_pos": 122, "end_pos": 131, "type": "DATASET", "confidence": 0.9804435968399048}]}, {"text": "Using only the name of the country does not successfully complete the task, because it does not answer the question \"where in the country will it rain?\".", "labels": [], "entities": []}, {"text": "Quantifiers were also not annotated as other FoR because they are extremely vague.", "labels": [], "entities": [{"text": "FoR", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8584084510803223}]}, {"text": "We were aiming at FoR that help a hearer more precisely identify referenced locations.", "labels": [], "entities": [{"text": "FoR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.7873302102088928}]}, {"text": "Even more interesting, 2 SREs created named entities in the no-name condition, i.e. where no name was available as per task.", "labels": [], "entities": [{"text": "SREs created named entities", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8292164504528046}]}, {"text": "One participant decided to name an unnamed subregion of Musicland as Drum County and referred to it 'by its name'.", "labels": [], "entities": [{"text": "Drum County", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.8122804164886475}]}, {"text": "Although odd, this suggests how people strongly feel the necessity for named entities when describing geographies.", "labels": [], "entities": []}, {"text": "This is very similar to another response in the pilot experiment, where the participant described one unnamed subregion as the penultimate state before reaching the coast, and later stated in the comments that names should be on the map.", "labels": [], "entities": []}, {"text": "Hypothesis 1 states that people use names with a high frequency in any condition where names are available.", "labels": [], "entities": []}, {"text": "If we exclude the no-name condition from the count, this hypothesis is supported with 97% (90/93) of name usage in the good-fit condition and 98% (91/93) in the poor-fit condition . We did not observe a significant difference in name usage between good-fit and poor-fit conditions, \u03c7 2 (1,N=186) = 0.21, p = .65.", "labels": [], "entities": []}, {"text": "Hypothesis 2 was also supported, again excluding the no-name condition.", "labels": [], "entities": [{"text": "Hypothesis", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9588562846183777}]}, {"text": "People very often (113/126 or 90%) use names as the first-focus area and other FoR as the second focus-area.", "labels": [], "entities": [{"text": "FoR", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.6967547535896301}]}, {"text": "After testing the above hypotheses, we observed the same phenomenon as identified by Turner and colleagues: that people resort to other FoR more often when the fit between (rain) patch and region is poorer.", "labels": [], "entities": []}, {"text": "In the good-fit condition 54% (50/93) of responses used other FoR, while 87% (81/93) of poor-fit responses contain other FoR.", "labels": [], "entities": [{"text": "FoR", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.8877628445625305}]}, {"text": "This means that there is a significant need for other FoR when moving from a good-fit to a poor-fit scenario, \u03c7 2 (1,N=186) = 26.18, p < .001.", "labels": [], "entities": []}, {"text": "To test how the algorithm currently performs, we ran it using 7 weather forecast datasets provided by the UK's meteorology agency: MetOffice.", "labels": [], "entities": [{"text": "MetOffice", "start_pos": 131, "end_pos": 140, "type": "DATASET", "confidence": 0.8904228806495667}]}, {"text": "The data contained numerical predictions fora region in the UK (Grampian), and each dataset also accompanies a textual summary, against which we used to compare our algorithm.", "labels": [], "entities": [{"text": "UK (Grampian)", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8200537264347076}]}, {"text": "We chose DICE to evaluate how comparable each output was.", "labels": [], "entities": []}, {"text": "This metrics has been widely used by the Referring Expression community ( . The results are displayed in.", "labels": [], "entities": []}, {"text": "To compare MetOffice's FoR choices with those by our algorithm, we ran it using 6 different density thresholds: 0.0, 0.2, 0.4, 0.6, 0.8 and 1.0.", "labels": [], "entities": []}, {"text": "A density threshold is in this sense the minimum event density a descriptor can have to be accepted as a candidate.", "labels": [], "entities": []}, {"text": "If you recall the explanation of the algorithm above, a Frame of Reference is rejected if all its descriptors are rejected, but equally if all its descriptors cannot be rejected.", "labels": [], "entities": []}, {"text": "For example, it only makes sense to select Inland as a descriptor if Coastal is not a candidate; if both Inland and Coastal are equally valid, then we can say the event (e.g. rain) is taking place in the entire region, as far as coastal proximity is concerned.", "labels": [], "entities": [{"text": "Inland", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.8846650123596191}, {"text": "Inland", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.8991441130638123}]}, {"text": "As explained above, the fixed density threshold in the original algorithm was 0.0, which means that 1 single point was enough to make a descriptor invalid.", "labels": [], "entities": []}, {"text": "By running the algorithm with different density thresholds, we are able to have an idea of some optimal threshold, where non-zero-density descriptors still get rejected.", "labels": [], "entities": []}, {"text": "From this initial evaluation, we could verify: Comparison of 1st-focus FoR choice between MetOffice texts and the algorithm running with different density thresholds.", "labels": [], "entities": [{"text": "MetOffice texts", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.9095454514026642}]}, {"text": "Assigning 2 (or more) 1st-focus FoR to a dataset is very similar to assigning \"both-1st\" to experimental responses.", "labels": [], "entities": []}, {"text": "Please refer to Section 3.2 fora more detailed discussion on multiple 1st-focus FoR.", "labels": [], "entities": []}, {"text": "Abbreviations: nam = NamedArea; dir = Directions; cst = CoastalProximity; MO = MetOffice; BL = Baseline; DT = Density Threshold; D = DICE score; * = all descriptors reach the threshold, so no FoR is discriminative enough to be chosen; -= no descriptor reaches the threshold, so no FoR qualifies as candidate to be chosen. that, at its current state, the algorithm is performing relatively well in choosing the 'favourite' frame, which is NamedAreas.", "labels": [], "entities": [{"text": "CoastalProximity", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.8750933408737183}, {"text": "MO", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9669219851493835}, {"text": "DT = Density Threshold", "start_pos": 105, "end_pos": 127, "type": "METRIC", "confidence": 0.7867615967988968}, {"text": "DICE score", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.8741675317287445}]}, {"text": "Another important observation is that the algorithm reached, at this relatively small evaluation, its optimal density threshold at 0.4, as indicated by the DICE value of 0.7, which is higher than the baseline of 0.6.", "labels": [], "entities": [{"text": "DICE", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9916056394577026}]}, {"text": "The baseline is simply the most common FoR in the dataset, which is named entities.", "labels": [], "entities": []}, {"text": "Surely a more substantial evaluation with a larger dataset will be required before we are safe to make stronger claims about thresholds and performance.", "labels": [], "entities": []}, {"text": "It is important to highlight how we annotated our corpus texts.", "labels": [], "entities": []}, {"text": "Frames were considered chosen if they were the first-focus FoR in the description (see 3.1 fora discussion on first vs. secondfocus FoR).", "labels": [], "entities": []}, {"text": "For instance, if \"in Aberdeen and in the west\" was the expression, both names and direction were annotated as first-focus frames; if \"in western Aberdeen\" was the case, then only name was considered first-focus, with direction annotated as second-focus and therefore outside the comparison with the algorithm.", "labels": [], "entities": [{"text": "Aberdeen", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9415472745895386}]}, {"text": "This is necessary because, although we gained valuable knowledge about first and second-focus with previous studies, the functionality for focus is not yet present in the algorithm, thus we are not yet ready to evaluate it for this mechanism.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results showing types of SREs per condition. SREs can contain only names, only  other FoR, none, or mix names with other FoR. When mixed, names can be the first or second focus, or  both types can be first focus.", "labels": [], "entities": []}, {"text": " Table 3: Event densities of a dataset used in the  evaluations.", "labels": [], "entities": []}]}