{"title": [{"text": "Looking hard: Eye tracking for detecting grammaticality of automatically compressed sentences", "labels": [], "entities": [{"text": "detecting grammaticality of automatically compressed sentences", "start_pos": 31, "end_pos": 93, "type": "TASK", "confidence": 0.8530225058396658}]}], "abstractContent": [{"text": "Natural language processing (NLP) tools are often developed with the intention of easing human processing, a goal which is hard to measure.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7384853661060333}]}, {"text": "Eye movements in reading are known to reflect aspects of the cog-nitive processing of text (Rayner et al., 2013).", "labels": [], "entities": []}, {"text": "We explore how eye movements reflect aspects of reading that are of relevance to NLP system evaluation and development.", "labels": [], "entities": [{"text": "NLP system evaluation", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.819770614306132}]}, {"text": "This becomes increasingly relevant as eye tracking is becoming available in consumer products.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.8733468651771545}]}, {"text": "In this paper we present an analysis of the differences between reading automatic sentence compressions and manually simplified newswire using eye-tracking experiments and readers' evaluations.", "labels": [], "entities": [{"text": "reading automatic sentence compressions", "start_pos": 64, "end_pos": 103, "type": "TASK", "confidence": 0.5749819800257683}]}, {"text": "We show that both manual simplification and automatic sentence compression provide texts that are easier to process than standard newswire, and that the main source of difficulty in processing machine-compressed text is ungrammaticality.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7234447598457336}]}, {"text": "Especially the proportion of regressions to previously read text is found to be sensitive to the differences in human-and computer-induced complexity.", "labels": [], "entities": []}, {"text": "This finding is relevant for evaluation of automatic summarization, simplification and translation systems designed with the intention of facilitating human reading.", "labels": [], "entities": [{"text": "summarization", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.9356860518455505}]}], "introductionContent": [{"text": "Intuitively, the readability of a text should reflect the effort that a reader must put into recognizing the meaning encoded in the text.", "labels": [], "entities": []}, {"text": "As a concept, readability thus integrates both content and form.", "labels": [], "entities": []}, {"text": "Sentence-level readability assessment is desirable from a computational point of view because smaller operational units allow systems to take rich information into account with each decision.", "labels": [], "entities": [{"text": "Sentence-level readability assessment", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8841227293014526}]}, {"text": "This computer-centric approach is in contrast to traditional human-centric readability metrics which are explicitly constructed for use at text level (cf. and) and are by their own definitions unsuitable for automatic application (cf. for an evaluation of readability-formula usability).", "labels": [], "entities": []}, {"text": "The standard approach to assessing text readability in natural language processing (NLP) is to ask readers to judge the quality of the output in terms of comprehensibility, grammaticality and meaning preservation (cf.).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.740817646185557}]}, {"text": "An alternative is to use existing text collections categorized by readability level for learning models of distinct categories of readability e.g. age or grade levels (.", "labels": [], "entities": []}, {"text": "In this paper we seek to establish whether readers share an intuitive conceptualization of the readability of single sentences, and to what extent this conceptualization is reflected in their reading behavior.", "labels": [], "entities": []}, {"text": "We research this by comparing subjective sentence-level readability judgments to recordings of readers' eye movements and by testing to what extent these measures co-vary across sentences of varying length and complexity.", "labels": [], "entities": []}, {"text": "These analyses enable us to evaluate whether sentence-level simplification operations can be meaningfully and directly assessed using eye tracking, which would be of relevance to both manual and automated simplification efforts.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the sentence compression training and evaluation data we extracted a subset of ordinary and simplified newswire texts from the Danish DSim corpus ().", "labels": [], "entities": [{"text": "sentence compression training", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.8151188890139262}, {"text": "Danish DSim corpus", "start_pos": 131, "end_pos": 149, "type": "DATASET", "confidence": 0.9585280021031698}]}, {"text": "In we give a schematic overview of how the data for our experiments was obtained.", "labels": [], "entities": []}, {"text": "For model development and selection we extracted all pairs of original and simplified sentences under the following criteria: 1.", "labels": [], "entities": [{"text": "model development", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7639220356941223}]}, {"text": "No sentence pair differs by more than 150 characters excluding punctuation.", "labels": [], "entities": []}, {"text": "2. The simplified sentence must be a strict subset of the original and contain a minimum of four tokens.", "labels": [], "entities": []}, {"text": "3. The original sentence must have at least one additional token compared to the simplified sentence and this difference must be nonpunctuation and of minimum three characters' length.", "labels": [], "entities": []}, {"text": "This results in a corpus of 2,332 sentence pairs, close to 4% of the DSim corpus.", "labels": [], "entities": [{"text": "DSim corpus", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.8212690353393555}]}, {"text": "Descriptive statistics of this corpus are shown in.", "labels": [], "entities": []}, {"text": "We followed the train-dev-test split of the DSim corpus forming a training set of 1,973 sentence pairs, a development set of 239 pairs, and a test set of 118 pairs.", "labels": [], "entities": [{"text": "DSim corpus", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.7042317390441895}]}, {"text": "For our experiment with eye tracking and subjective evaluation we created a similar dataset, denoted \"augmented compressions\" in, from sentence pairs displaying similar compressions and in addition exactly one lexical substitution.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8346661627292633}]}, {"text": "We augmented these pairs by simply changing the synonym back to the original word choice, resulting in a valid compression.", "labels": [], "entities": []}, {"text": "We obtained an automatically compressed version of these sentences from the trained model 2 . This results in a corpus of sentence triples consisting of an original, an expert simplification and a system generated version.", "labels": [], "entities": []}, {"text": "In some cases the system output was identical to either the original input or to the expert simplification.", "labels": [], "entities": []}, {"text": "We therefore selected the evaluation data to include only sentence triples where all three versions were in fact different from one another resulting in 140 sentence triples, i.e. 420 individual stimuli.", "labels": [], "entities": []}, {"text": "On average the system deleted 15 tokens per sentence while the experts average around 12 token deletions per sentence.", "labels": [], "entities": []}, {"text": "The experiment described in the following section consisted of an eye tracking part and a subjective evaluation part.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.7008920162916183}]}, {"text": "The eye tracking part of the experiment was carried out first and was followed by the subjective evaluation part, which was carried out by email invitation to an online survey.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7874841392040253}]}, {"text": "We recruited 24 students aged 20 to 36 with Danish as first language, 6 male and 18 female.", "labels": [], "entities": []}, {"text": "All had normal or corrected-to-normal vision.", "labels": [], "entities": []}, {"text": "None of the participants had been diagnosed with dyslexia.", "labels": [], "entities": []}, {"text": "A total of 20 participants completed the evaluation task.", "labels": [], "entities": []}, {"text": "The experiment was a balanced and randomized Latin-square design.", "labels": [], "entities": []}, {"text": "This design ensured that each participant saw only one version from each sentence-triple from one half of the dataset while being eye-tracked.", "labels": [], "entities": []}, {"text": "Afterwards participants were asked to assign relative ranks between all three versions in each sentence-triple in the half of the dataset which they had not previously seen.", "labels": [], "entities": []}, {"text": "In total, each version of each sentence was read by four participants in the eye-tracking experiment and ranked by 9-11 other participants.", "labels": [], "entities": []}, {"text": "In the subjective evaluation task participants had to produce a strict ordering by readability of all three versions of each sentence, with the rank '1' designating the most readable sentence.", "labels": [], "entities": []}, {"text": "Presentation order was fully randomized.", "labels": [], "entities": [{"text": "Presentation order", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7708090841770172}]}, {"text": "In many natural language generation and manipulation setups, it is important that the system is able to recognize acceptable output, and it is typical of this type of setup that neither system-intrinsic scoring functions or as standard automatic evaluation procedures are reliably meeting this requirement.", "labels": [], "entities": [{"text": "natural language generation and manipulation", "start_pos": 8, "end_pos": 52, "type": "TASK", "confidence": 0.8011599779129028}]}, {"text": "In such cases it is common to obtain expensive specialized human evaluations of the output.", "labels": [], "entities": []}, {"text": "Our results are encouraging as they suggest that behavioral metrics like regressions and reading time that can be obtained from nave subjects simply reading system output may provide an affordable alternative.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the full specialized corpus, 2.332 sentence pairs in total. Except for the row \"Total\",  statistics are per sentence. \"Difference Tokens\" report the average, standard deviation and range of the  proportional change in number of tokens per sentence.", "labels": [], "entities": []}]}