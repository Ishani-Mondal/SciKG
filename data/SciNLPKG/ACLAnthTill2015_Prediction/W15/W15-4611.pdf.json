{"title": [{"text": "Towards Taxonomy of Errors in Chat-oriented Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a taxonomy of errors in chat-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "Compared to human-human conversations and task-oriented dialogues, little is known about the errors made in chat-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "Through a data collection of chat dialogues and analyses of dialogue breakdowns, we classified errors and created a taxonomy.", "labels": [], "entities": []}, {"text": "Although the proposed taxonomy may not be complete, this paper is the first to present a taxonomy of errors in chat-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "We also highlight the difficulty in pinpointing errors in such systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The last decade has seen an emergence of systems that can engage in chat, small talk, or open-domain conversation.", "labels": [], "entities": []}, {"text": "Such systems can be useful for cultivating trust between a system and users), entertaining users, and obtaining preferences from users for recommendations (.", "labels": [], "entities": []}, {"text": "Error analysis is important to improve any system.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8998519778251648}]}, {"text": "However, little is known about the types of errors that can be made in chat-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "This is in contrast with many studies on task-oriented dialogue systems in which various taxonomies of errors have been proposed).", "labels": [], "entities": []}, {"text": "This paper presents a taxonomy of errors in chat-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "In our approach, we collect dialogues with a chat-oriented dialogue system and identify breakdowns (situations in which users cannot proceed with the conversation) as possible points of errors.", "labels": [], "entities": []}, {"text": "Then, we classify the errors that led to such breakdowns to create a taxonomy.", "labels": [], "entities": []}, {"text": "By having such a taxonomy, we hope to better grasp the main causes of breakdown in current chatoriented dialogue systems; thereby, making it possible to make improvements.", "labels": [], "entities": []}, {"text": "The contributions of this paper are that this is the first attempt to create a taxonomy of errors in chat-oriented dialogue systems and that we quantitatively show, by the distribution of error categories and inter-annotator agreement, the possibilities and difficulties in pinpointing errors in chat-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "In Section 2, we cover related work on creating a taxonomy of errors in dialogue systems.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our data collection followed by the annotation of breakdowns in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we discuss the taxonomy we devised.", "labels": [], "entities": []}, {"text": "In Section 6, we evaluate the taxonomy in terms of the distribution of errors and inter-annotator agreement.", "labels": [], "entities": []}, {"text": "In Section 7, we summarize the paper and mention future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the validity of our taxonomy, we asked annotators to label system utterances in our data with our error categories.", "labels": [], "entities": []}, {"text": "One way to check the validity of a taxonomy is to observe the distribution of the annotations.", "labels": [], "entities": []}, {"text": "When the annotations are biased towards certain categories, it is an indication that the taxonomy is not decomposing the phenomena appropriately.", "labels": [], "entities": []}, {"text": "Another way for verifying the taxonomy is to check inter-annotator agreement.", "labels": [], "entities": []}, {"text": "If the inter-annotator agreement is high, it is an indication that the categories are appropriately separated from each other.", "labels": [], "entities": []}, {"text": "We assigned three annotators for each subset of a-j (See Section 4; we did not use subset k because it had a small number of dialogues).", "labels": [], "entities": []}, {"text": "Within each subset, we asked the annotators to annotate system utterances in the ten dialogues that had obligatory comments for breakdowns so that they could use the comments as hints to facilitate annotation.", "labels": [], "entities": []}, {"text": "For each system utterance in question, a single error category label (i.e. sub-category label) was annotated.", "labels": [], "entities": []}, {"text": "We instructed the annotators to check error categories from the utterance level to the environment level; that is, they first check whether the system utterance is an utterance-level error, if it is not, the check proceeds to the response level.", "labels": [], "entities": []}, {"text": "For checking the response-level error, it was recommended that the annotators hide the context so that they can just focus on the adjacency pairs.", "labels": [], "entities": []}, {"text": "With this annotation process, 580 system utterances were annotated by 3 annotators with our error categories, resulting in 1740 (580 \u00d7 3) annotations.", "labels": [], "entities": []}, {"text": "Note that we could not use the same annotators for all data because of the high burden of this annotation.: Number of annotations given to each subcategory.", "labels": [], "entities": []}, {"text": "Ratio is calculated overall annotations.", "labels": [], "entities": []}, {"text": "shows the distribution of annotations summarized by the main categories.", "labels": [], "entities": []}, {"text": "As can be seen from the table, the response-level error has the most annotations (more than 50%), followed by the context-level error.", "labels": [], "entities": []}, {"text": "We also see quite a few utterance-level and environment-level errors.", "labels": [], "entities": []}, {"text": "shows the distribution of annotations by sub-category.", "labels": [], "entities": []}, {"text": "Within the utterance-level category, the semantic error is dominant.", "labels": [], "entities": []}, {"text": "For the other levels, the errors seem to be equally distributed under each main category, although the number of RESNon-understandings is larger and that of RESMisunderstandings is less than the others.", "labels": [], "entities": []}, {"text": "This is an indication that the taxonomy has a good categorization of errors since the distribution is not biased to only a small number of categories.", "labels": [], "entities": []}, {"text": "shows Fleiss' \u03ba for main and subcategories of errors.", "labels": [], "entities": [{"text": "Fleiss' \u03ba", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9312255680561066}, {"text": "main", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.986534595489502}]}, {"text": "The kappa values were calculated within each subset because the annotators were different for each subset.", "labels": [], "entities": []}, {"text": "The average value indicates the macro-average over the subsets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of collected dialogues", "labels": [], "entities": []}, {"text": " Table 2: Distributions of breakdown annotations  for rest1046 data set", "labels": [], "entities": [{"text": "rest1046 data set", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.9919542868932089}]}, {"text": " Table 3: Number of annotations given to each  main category. UTT, RES, CON, and ENV denote  utterance, response, context, and environment lev- els, respectively.", "labels": [], "entities": [{"text": "UTT", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.6108956336975098}, {"text": "RES", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9607840180397034}, {"text": "ENV", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8174049854278564}]}, {"text": " Table 4: Number of annotations given to each sub- category. Ratio is calculated over all annotations.", "labels": [], "entities": [{"text": "Ratio", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9754028916358948}]}, {"text": " Table 5: Fleiss' \u03ba for main and sub-categories of  errors. # of Utts indicates number of annotated  utterances in each subset.", "labels": [], "entities": [{"text": "Utts", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.7505303025245667}]}, {"text": " Table 6: Confusion matrix for main categories", "labels": [], "entities": []}]}