{"title": [{"text": "An Optimal Quadratic Approach to Monolingual Paraphrase Alignment", "labels": [], "entities": [{"text": "Monolingual Paraphrase Alignment", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.674348404010137}]}], "abstractContent": [{"text": "We model the problem of monolingual textual alignment as a Quadratic Assignment Problem (QAP) which simultaneously maximizes the global lexico-semantic and syntactic similarities of two sentence-level texts.", "labels": [], "entities": [{"text": "monolingual textual alignment", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6543543438116709}]}, {"text": "Because QAP is an NP-complete problem, we propose a branch-and-bound approach to efficiently find an optimal solution.", "labels": [], "entities": []}, {"text": "When compared with other methods and studies, our results are competitive.", "labels": [], "entities": []}], "introductionContent": [{"text": "Textual alignment between two sentences involves the identification of words and phrases considered to be semantically equivalent or very close in meaning (within the context of the respective sentences).", "labels": [], "entities": [{"text": "Textual alignment", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7493226528167725}]}, {"text": "Monolingual alignment is particularly useful for the task of text-to-text semantic similarity ().", "labels": [], "entities": [{"text": "Monolingual alignment", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8336434066295624}, {"text": "text-to-text semantic similarity", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.6097425421079}]}, {"text": "shows an example of human generated alignments between two sentences from the corpus used by, which is a modified corpus of human-aligned paraphrases initially described in.", "labels": [], "entities": []}, {"text": "While monolingual text alignment has been tackled as a task of its own only recently), text alignment has been explored intensely in the area of machine translation.", "labels": [], "entities": [{"text": "text alignment", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.6867231577634811}, {"text": "text alignment", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.8253475427627563}, {"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7822677195072174}]}, {"text": "Brunning (2010) distinguishes among three levels of alignment in machine translation: document alignment, sentence alignment, and word/phrase level alignment.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.6748572438955307}, {"text": "sentence alignment", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.745023250579834}, {"text": "word/phrase level alignment", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.6323128044605255}]}, {"text": "We focus hereon word-level alignment.", "labels": [], "entities": [{"text": "word-level alignment", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7745625972747803}]}, {"text": "Furthermore, we focus on monolingual word alignment in the context of sentence-to-sentence similarity tasks such as textual entailment and paraphrase identification.", "labels": [], "entities": [{"text": "monolingual word alignment", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6190116902192434}, {"text": "textual entailment", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.6927861422300339}, {"text": "paraphrase identification", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.823839008808136}]}, {"text": "We focus on word-level (as opposed to phraselevel) alignment fora number of reasons.", "labels": [], "entities": [{"text": "phraselevel) alignment", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7154931624730428}]}, {"text": "First, the vast majority of gold alignments in the two datasets we use (95-96%) are word-level alignments (the rest are phrase-level).", "labels": [], "entities": []}, {"text": "Similarly, report that word-level alignments constitute more than 95% of the alignments in recent human-annotated corpora.", "labels": [], "entities": []}, {"text": "A second reason is the fact that our formulation of the monolingual alignment task based on the Quadratic Assignment Problem (QAP) ( fits well with word-level alignment.", "labels": [], "entities": [{"text": "monolingual alignment task", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7402225732803345}, {"text": "word-level alignment", "start_pos": 148, "end_pos": 168, "type": "TASK", "confidence": 0.7156327068805695}]}, {"text": "Third, the key ingredients in our solution (the word-to-word semantic similarity measures and dependency relations) apply directly to words.", "labels": [], "entities": []}, {"text": "The role of word-to-word semantic similarity measures and contextual information for monolingual alignment has been explored in the past.", "labels": [], "entities": []}, {"text": "However, the jury is still out therewith respect to how to best combine these types of information for monolingual alignment as one of the most recent work in this area has illustrated ().", "labels": [], "entities": [{"text": "monolingual alignment", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.6877002716064453}]}, {"text": "showed that use of local contextul information in combination with hand-crafted dependency type equivalences yields better results than methods that exploit local context, e.g..", "labels": [], "entities": []}, {"text": "Indeed, our approach combines in unique ways word-to-word semantic similarity measures with contextual information in the form of dependency-relations among words and with a combinatorial optimization formulation based on the QAP problem.", "labels": [], "entities": []}, {"text": "As dependencies can capture longer-distance relationships between words in a sentence, we can say that our method uses more than just local context for aligning texts.", "labels": [], "entities": []}, {"text": "Furthermore, because the QAP formulation provides a global optimal solution, our method is indeed accounting for the full sentential context.", "labels": [], "entities": []}, {"text": "Indeed, our QAP formulation simultaneously accounts for word-level similarities and similari- ties between corresponding syntactic/grammatical relations in a globally optimal manner.", "labels": [], "entities": []}, {"text": "In contrast, method for sentence level monolingual alignment finds a local maximum, which only in certain, lucky circumstances may also be a global maximum.", "labels": [], "entities": [{"text": "sentence level monolingual alignment", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.6161871403455734}]}, {"text": "Optimization methods have been proposed for phrase-level monolingual alignment) in the context of a paraphrase task that rely on integer linear programming.", "labels": [], "entities": [{"text": "phrase-level monolingual alignment", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.669887642065684}]}, {"text": "Our optimization method is based on a different paradigm, the QAP formulation, and we rely on word-to-word semantic similarity measures, some of which are totally unsupervised such as Latent Semantic Analysis, and syntactic relation identity as opposed to edit distances.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.55892214179039}]}, {"text": "used string similarity and WordNet for computing semantic relatedness.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9602885246276855}, {"text": "computing semantic relatedness", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6246513326962789}]}, {"text": "We evaluated the proposed method on two datasets.", "labels": [], "entities": []}, {"text": "The first one is the SEMILAR corpus (, a subset of 701 randomly selected pairs from the Microsoft Research Paraphrase Corpus (MSRP) ().", "labels": [], "entities": [{"text": "SEMILAR corpus", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.6805377006530762}, {"text": "Microsoft Research Paraphrase Corpus (MSRP)", "start_pos": 88, "end_pos": 131, "type": "DATASET", "confidence": 0.7797945312091282}]}, {"text": "The pairs were manually annotated with tokens and phrase alignments.", "labels": [], "entities": [{"text": "phrase alignments", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.6743364781141281}]}, {"text": "The second dataset is the evaluation corpus used by, called the Edinburg corpus, a modified corpus of humanaligned paraphrases, initially described in.", "labels": [], "entities": [{"text": "Edinburg corpus", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.8108237385749817}]}], "datasetContent": [{"text": "We evaluated and compared the three alignment methods presented in the previous section (GRD, w-OPT and QAP) on two datasets that were manually annotated with alignments between sentences: the SEMILAR corpus () and the Edinburg corpus ().", "labels": [], "entities": [{"text": "GRD", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.5971068739891052}, {"text": "QAP", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9610159397125244}, {"text": "SEMILAR corpus", "start_pos": 193, "end_pos": 207, "type": "DATASET", "confidence": 0.7299003601074219}, {"text": "Edinburg corpus", "start_pos": 219, "end_pos": 234, "type": "DATASET", "confidence": 0.9064549505710602}]}, {"text": "The SEMILAR corpus consists of a set of 701 instances extracted from the MSRP corpus and which were tokenized, tagged and parsed with the Stanford Core NLP library and then manually annotated with tokens and phrase alignments.", "labels": [], "entities": [{"text": "SEMILAR corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8217313587665558}, {"text": "MSRP corpus", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.8959214389324188}, {"text": "Stanford Core NLP library", "start_pos": 138, "end_pos": 163, "type": "DATASET", "confidence": 0.9121402651071548}]}, {"text": "The Edinburg corpus contains 714 annotated instances used for training, and 306 instances used for evaluation, also pre-processed and parsed for syntactic dependencies using the Stanford NLP Parser.", "labels": [], "entities": [{"text": "Edinburg corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9194129407405853}, {"text": "Stanford NLP Parser", "start_pos": 178, "end_pos": 197, "type": "DATASET", "confidence": 0.8424346248308817}]}, {"text": "As in, we used Because we do word-level alignments and the human-annotated data and METEOR output include phrase-level alignments we have to have away to consistently assess the output of the method.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.8824256062507629}]}, {"text": "We assessed the phrase-level alignments using word-level alignments as explained next.", "labels": [], "entities": []}, {"text": "If the gold data contains a phrase-level alignment then if the output of a method contains an alignment between any two words in the gold-aligned phrases then we consider the system word-level alignment as a hit.", "labels": [], "entities": []}, {"text": "Using this method, the METEOR alignments are evaluated at word-level and therefore can be directly compared to our methods' alignments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.7057597041130066}]}, {"text": "It should be noted that phrase-level alignments are very few.", "labels": [], "entities": []}, {"text": "On the Edinburg corpus there are only 95 phrase alignments produced by METEOR out of 5,046 alignments (word-and phrase-level) and on the SEMILAR corpus METEOR produces only 30 phrase-level alignments out of 10,112 alignments.", "labels": [], "entities": [{"text": "Edinburg corpus", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9800361394882202}, {"text": "SEMILAR corpus METEOR", "start_pos": 137, "end_pos": 158, "type": "DATASET", "confidence": 0.8412809173266093}]}, {"text": "This method of evaluating neither penalizes nor rewards METEOR.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9744414687156677}]}, {"text": "shows the alignment performance results on the SEMILAR corpus, for all three alignment methods and the METEOR baseline.", "labels": [], "entities": [{"text": "SEMILAR corpus", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.7070870697498322}, {"text": "METEOR baseline", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.7418312132358551}]}, {"text": "For space reasons, we picked two representative word-toword similarity metrics, JCN) and LSA, and report comparative results among the three alignment methods.", "labels": [], "entities": [{"text": "JCN", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.7388834953308105}, {"text": "LSA", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9174087047576904}]}, {"text": "Also, we illustrate the impact of the \u2126 parameters using the QAP method and a third word-to-word metric,).", "labels": [], "entities": [{"text": "QAP", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.7989643812179565}]}, {"text": "Note that by changing the \u2126 value within some restrictive bounds, one could control fora better precision, at the expense of the recall, or viceversa, while keeping the overall F-score more or less the same.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9994470477104187}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.999504804611206}, {"text": "F-score", "start_pos": 177, "end_pos": 184, "type": "METRIC", "confidence": 0.9958095550537109}]}, {"text": "The other wordto-word metrics that we experimented with, show a similar trend in performance, with very small variations from the ones we reported.", "labels": [], "entities": []}, {"text": "It is important to note that for the QAP method we used \u03b1 = 0.5 which was chosen following the same process explained in the next section.", "labels": [], "entities": []}, {"text": "The QAP method significantly outperforms both GRD and w-OPT alignments for both JCN and LSA word-to-word similarity metrics (p < 0.0018).", "labels": [], "entities": [{"text": "JCN", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.8198320269584656}]}, {"text": "The difference in performance between GRD and w-OPT is significant only on the LSA metric (p < 0.0058).", "labels": [], "entities": [{"text": "LSA metric", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.702174723148346}]}, {"text": "Note that the high performance scores for all methods are due to the high lexical overlap, a characteristic of the SEMILAR instances, which was inherited from the original MSRP corpus.", "labels": [], "entities": [{"text": "SEMILAR", "start_pos": 115, "end_pos": 122, "type": "TASK", "confidence": 0.7725106477737427}, {"text": "MSRP corpus", "start_pos": 172, "end_pos": 183, "type": "DATASET", "confidence": 0.7254554778337479}]}], "tableCaptions": [{"text": " Table 1: Alignment percent scores on SEMILAR  corpus", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.7422161102294922}, {"text": "SEMILAR", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.48571786284446716}]}, {"text": " Table 2: Alignments percent scores on Edinburg  corpus", "labels": [], "entities": [{"text": "Edinburg  corpus", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9878071546554565}]}]}