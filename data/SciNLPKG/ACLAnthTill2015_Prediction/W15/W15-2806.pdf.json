{"title": [{"text": "Semantic Tuples for Evaluation of Image to Sentence Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "The automatic generation of image captions has received considerable attention.", "labels": [], "entities": [{"text": "automatic generation of image captions", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7720780432224273}]}, {"text": "The problem of evaluating caption generation systems, though, has not been that much explored.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8813338279724121}]}, {"text": "We propose a novel evaluation approach based on comparing the underlying visual semantics of the candidate and ground-truth captions.", "labels": [], "entities": []}, {"text": "With this goal in mind we have defined a semantic representation for visually descriptive language and have augmented a subset of the Flickr-8K dataset with semantic annotations.", "labels": [], "entities": [{"text": "Flickr-8K dataset", "start_pos": 134, "end_pos": 151, "type": "DATASET", "confidence": 0.9792513847351074}]}, {"text": "Our evaluation metric (BAST) can be used not only to compare systems but also to do error analysis and get a better understanding of the type of mistakes a system does.", "labels": [], "entities": [{"text": "BAST", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.7424312829971313}]}, {"text": "To compute BAST we need to predict the semantic representation for the automatically generated captions.", "labels": [], "entities": [{"text": "BAST", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.7951510548591614}]}, {"text": "We use the Flickr-ST dataset to train classi-fiers that predict STs so that evaluation can be fully automated 1 .", "labels": [], "entities": [{"text": "Flickr-ST dataset", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.9206016361713409}]}], "introductionContent": [{"text": "In recent years, the task of automatically generating image captions has received considerable attention.", "labels": [], "entities": [{"text": "automatically generating image captions", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.6691579818725586}]}, {"text": "The task of evaluating such sentences, though, has not been that much explored, and mainly holds on metrics such as BLEU () and ROUGE (, originally proposed for evaluating machine translation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9982744455337524}, {"text": "ROUGE", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.9894914627075195}, {"text": "machine translation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.6678518503904343}]}, {"text": "These metrics have been shown to poorly correlate with human evaluations.", "labels": [], "entities": []}, {"text": "Their main problem comes from the fact that they uniquely consider n-grams agreement between the reference and candidate sentences, focusing thus only on the lexical informa-tion and obviating the agreement at the visual semantic level.", "labels": [], "entities": []}, {"text": "These limitations are illustrated in. have proposed to address these limitations by making use of a Term Frequency Inverse Document Frequency (TF-IDF) that places higher weight on n-grams that frequently occur in the reference sentence describing an image, while reducing the influence of popular words that are likely to be less visually informative.", "labels": [], "entities": [{"text": "Term Frequency Inverse Document Frequency (TF-IDF)", "start_pos": 100, "end_pos": 150, "type": "METRIC", "confidence": 0.6884538531303406}]}, {"text": "In this paper, we consider a different alternative to overcome the limitations of BLEU and ROUGE metrics, by introducing a novel approach specifically tailored to evaluate systems for image caption generation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.997901201248169}, {"text": "ROUGE", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.977097749710083}, {"text": "image caption generation", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.8297376235326132}]}, {"text": "To do this, we first define a semantic representation for visually descriptive language, that allows measuring to which extent an automatically generated caption of an image matches the underlying visual semantics of human authored captions.", "labels": [], "entities": []}, {"text": "To implement this idea we have augmented a subset of the Flickr-8K dataset) with a visual semantic representation, which we call Semantic Tuples (ST).", "labels": [], "entities": [{"text": "Flickr-8K dataset", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9800133407115936}]}, {"text": "This representation shares some similarity with the more standard PropBank () style Semantic Roles (SRL).", "labels": [], "entities": [{"text": "Semantic Roles (SRL)", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.5998044967651367}]}, {"text": "However, SRL was designed to have high coverage of all the linguistic phenomena present in natural language sentences.", "labels": [], "entities": [{"text": "SRL", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9046120047569275}]}, {"text": "In contrast, our ST representation is simpler and focuses on the aspects of the predicate structure that are most relevant for capturing the semantics of visually descriptive language.", "labels": [], "entities": []}, {"text": "This ST representation is then used to measure the agreement between the underlying semantics of an automatically generated caption and the semantics of the gold reference captions at different levels of granularity.", "labels": [], "entities": []}, {"text": "We do this by aggregating the STs from the gold captions and forming a Bag of Aggregated Semantic Tuples represen-Ref: A man sliding down a huge sand dune on a sunny day SA: A man slides during the day on a dune.", "labels": [], "entities": []}, {"text": "SB: A dinosaur eats huge sand and remembers a sunny day.", "labels": [], "entities": []}, {"text": "The limitations of the BLEU evaluation metric: SA and SB are two automatically generated sentences that we wish to compare against the manually authored Ref.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9788227677345276}]}, {"text": "However, while SB does not relate to the image, it obtains higher n-gram similarity than SA, which is the basis of BLEU and ROUGE.", "labels": [], "entities": [{"text": "similarity", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.781307578086853}, {"text": "SA", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9531440138816833}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9980687499046326}, {"text": "ROUGE", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9801294207572937}]}], "datasetContent": [{"text": "We believe that one of the main reasons why most of the evaluations used to measure caption generation performance involve computing surface metrics is that until now there was no dataset annotated with underlying semantics.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9250889122486115}]}, {"text": "To address this limitation we decided to create anew dataset of images annotated with semantic tuples as described in the previous section.", "labels": [], "entities": []}, {"text": "Our dataset has the advantage that every image is annotated with both the underlying semantics in the form of semantic tuples and natural language captions that constitute different lexical realizations of the underlying visual semantics.", "labels": [], "entities": []}, {"text": "To create our dataset we used a subset of the Flickr-8K dataset with captions, proposed in (.", "labels": [], "entities": [{"text": "Flickr-8K dataset", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9881357252597809}]}, {"text": "This dataset consists of 8,000 images of people and animals performing some action taken from Flickr, with five crowd-sourced descriptive captions for each one.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.9637226462364197}]}, {"text": "These captions are sought to be concrete descriptions of what can be seen in the image rather than abstract or conceptual descriptions of non-visible elements (e.g. people or street names, or the mood of the image).", "labels": [], "entities": []}, {"text": "We asked human annotators to annotate 250 image captions, corresponding to 50 images taken from the development set of Flickr-8K.", "labels": [], "entities": [{"text": "Flickr-8K", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.9692646861076355}]}, {"text": "In order to ensure the alignment between the information contained in the captions and their corresponding semantic tuples, annotators were not allowed to look at the referent image while annotating every caption.", "labels": [], "entities": []}, {"text": "Annotators were asked to list all the unique tuples present in the caption.", "labels": [], "entities": []}, {"text": "Then, for each argument of the tuple, they had to decide if its value is null, tacit or explicit (i.e. an argument value that can be associated with a text span in the caption).", "labels": [], "entities": []}, {"text": "For explicit argument values we asked the annotator to mark the corresponding span in the text.", "labels": [], "entities": []}, {"text": "That is, instead of giving a value for the argument, we ask them to mark in the caption the evidence for that argument.", "labels": [], "entities": []}, {"text": "To create the STs that we use for evaluation we first need to compute the argument values.", "labels": [], "entities": []}, {"text": "We assume that we can compute a function that maps spans of text to argument variables, and we call this the grounding function.", "labels": [], "entities": []}, {"text": "Currently, we use a very simple mapping from spans to argument values: they map to lowercase lemmatized forms.", "labels": [], "entities": []}, {"text": "Given the annotated data and a grounding function, we refer to the process of computing argument values for argument spans as projecting the annotations.", "labels": [], "entities": []}, {"text": "With our approach for decoupling surface (i.e. argument spans) from semantics (argument values) we can address some common problems in caption generation evaluation.", "labels": [], "entities": [{"text": "caption generation evaluation", "start_pos": 135, "end_pos": 164, "type": "TASK", "confidence": 0.9041101336479187}]}, {"text": "The idea is simple, we can use the same annotation with different grounding functions to get useful projections of the original annotation.", "labels": [], "entities": []}, {"text": "One clear problem when evaluating caption generation systems is how to handle synonymity, i.e. the fact that two surface forms might refer to the same semantic concept.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9123550653457642}]}, {"text": "For example, if the reference caption is: \"A boy is playing in a park\", the candidate caption: \"A kid playing on the park\" should not be penalized for using the surface form boy instead of kid.", "labels": [], "entities": []}, {"text": "We can address this problem by building a grounding function that maps the argument span boy and the argument span kid to the same argument variable.", "labels": [], "entities": []}, {"text": "We could automatically build such function using a thesaurus.", "labels": [], "entities": []}, {"text": "Another common problem when evaluating caption generation is the fact that the same visual entity can be described with different levels of specificity.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.9333137571811676}]}, {"text": "For example, for the previous reference caption it is clear that \"A person is playing in a park\" should have a higher evaluation score than \"A dog playing in a park\".", "labels": [], "entities": []}, {"text": "This is because any human reading the caption would agree that person is just a 'coarser' way of referring to the same entity.", "labels": [], "entities": []}, {"text": "With our approach we could handle this problem by having a coarser grounding function that maps the argument span kid and the argument span person to the same argument value human.", "labels": [], "entities": []}, {"text": "The important thing is that for any grounding function we can project the annotations and compute the evaluation, thus we can analyze the performance of a system in different dimensions.", "labels": [], "entities": []}, {"text": "Our goal is to define an evaluation metric that measures the similarity between the STs of the ground-truth captions for an image and the STs of a generated image caption.", "labels": [], "entities": []}, {"text": "We wish to define a metric that is useful not only to compare systems, but also that allows for error analysis and some insight on the types of mistakes performed by any given system.", "labels": [], "entities": []}, {"text": "To do this we will first use the STs corresponding to the ground-truth captions to compute what we calla Bag of Aggregated Semantic Tuples representation (BAST).", "labels": [], "entities": []}, {"text": "shows a reference caption and its corresponding STs and BAST.", "labels": [], "entities": [{"text": "STs", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9970989227294922}, {"text": "BAST", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.985348641872406}]}, {"text": "Notice that for simplicity we show a single reference caption, in reality if there are k captions for an image, we will first compute the STs corresponding to all of them.", "labels": [], "entities": []}, {"text": "The BAST representation is computed in the following manner: 1.", "labels": [], "entities": [{"text": "BAST", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.6043230891227722}]}, {"text": "For the locatives and predicate arguments compute the union of all the corresponding argument values appearing in any ST.", "labels": [], "entities": []}, {"text": "For the patient and agent we will compute a single set which we refer to as the participants set.", "labels": [], "entities": []}, {"text": "We call this portion of the BAST the bag of single arguments representation.", "labels": [], "entities": [{"text": "BAST", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.8493635058403015}]}, {"text": "2. We compute the same representation but now we look at pairs of argument values, meaning: predicate+participant, participant+locative and predicate+locative.", "labels": [], "entities": []}, {"text": "We call these the bag of argument pairs.", "labels": [], "entities": []}, {"text": "3. Similarly we can also compute a bag of argument triplets for predicate+participant+locative We can also compute the BAST representation of an automatically generated caption.", "labels": [], "entities": [{"text": "BAST representation of an automatically generated caption", "start_pos": 119, "end_pos": 176, "type": "TASK", "confidence": 0.5373888952391488}]}, {"text": "This can be done via human annotation of the caption's STs or using a model that predicts STs from captions (such a model is described in the next section).", "labels": [], "entities": []}, {"text": "Now if we have the ground-truth BAST and the BAST of the candidate caption we can compute standard precision, recall and F1 metrics over the different components of the BAST.", "labels": [], "entities": [{"text": "BAST", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.7327134609222412}, {"text": "BAST", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9985858201980591}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9810687899589539}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9987161159515381}, {"text": "F1 metrics", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9754382967948914}]}, {"text": "More specifically, for the single argument component of the BAST we compute: \u2022 Predicate-Precision: This is the number of predicted predicates present in the BAST of the candidate caption that where also present in the BAST of the ground-truth reference captions for the corresponding image.", "labels": [], "entities": [{"text": "BAST", "start_pos": 60, "end_pos": 64, "type": "TASK", "confidence": 0.7702332735061646}]}, {"text": "That is this is the number of correctly predicted predicates.", "labels": [], "entities": []}, {"text": "\u2022 Predicate-Recall: This is the number of predicted predicates present in the BAST of the ground-truth captions that were also present in the BAST of the candidate caption.", "labels": [], "entities": [{"text": "Predicate-Recall", "start_pos": 2, "end_pos": 18, "type": "METRIC", "confidence": 0.9382205009460449}, {"text": "BAST", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.6537925601005554}]}, {"text": "\u2022 Predicate-F1: This is the standard metric, i.e. the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "Predicate-F1", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9985138773918152}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9972000122070312}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9896780252456665}]}, {"text": "We can compute the same metrics for other arguments and for argument pairs and triplets of arguments.", "labels": [], "entities": []}, {"text": "shows an example of computing the BAST evaluation metric for two captions.", "labels": [], "entities": [{"text": "BAST evaluation", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.5641143321990967}]}], "tableCaptions": [{"text": " Table 1: F1 score of the automatic BAST extractor  taking as reference the manually annotated tuples  for the sentences generated by the two models.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9730218946933746}, {"text": "BAST extractor", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.8871200084686279}]}, {"text": " Table 2: Results with current metrics for the two models described in the text. MSCOCO* is the subset  of MSCOCO used in the NeuralTalk reference experiments. The first row are the results reported in the  NeuralTalk project web-site.", "labels": [], "entities": [{"text": "NeuralTalk project web-site", "start_pos": 207, "end_pos": 234, "type": "DATASET", "confidence": 0.851975659529368}]}]}