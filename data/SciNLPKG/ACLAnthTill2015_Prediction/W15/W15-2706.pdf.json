{"title": [{"text": "Identification and Disambiguation of Lexical Cues of Rhetorical Relations across Different Text Genres", "labels": [], "entities": [{"text": "Identification and Disambiguation of Lexical Cues of Rhetorical Relations across Different Text Genres", "start_pos": 0, "end_pos": 102, "type": "TASK", "confidence": 0.792367533995555}]}], "abstractContent": [{"text": "Lexical cues are linguistic expressions that can signal the presence of a rhetorical relation.", "labels": [], "entities": []}, {"text": "However, such cues can be ambiguous as they may signal more than one relation or may not always function as a relation indicator.", "labels": [], "entities": []}, {"text": "In this study, we first conduct a corpus-based analysis to derive a set of n-grams as potential lexical cues.", "labels": [], "entities": []}, {"text": "These cues are then utilized in graph-based prob-abilistic models to determine the syntactic context in which the cue is signaling the presence of a particular relation.", "labels": [], "entities": []}, {"text": "Evaluation results are reported for various cues of the CIRCUMSTANCE relation, confirming the value of syntactic features for the task of cue disambiguation in the context of Rhetorical Structure Theory.", "labels": [], "entities": [{"text": "cue disambiguation", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7450447380542755}, {"text": "Rhetorical Structure Theory", "start_pos": 175, "end_pos": 202, "type": "TASK", "confidence": 0.78458704551061}]}, {"text": "Moreover, using a graph to encode syntactic information is shown to be a more generalizable and effective approach compared to the direct usage of syntactic features.", "labels": [], "entities": []}], "introductionContent": [{"text": "A semantically sound text consists of discourse units that are connected through discourse relations, which are also referred to as rhetorical relations.", "labels": [], "entities": []}, {"text": "Despite the efforts to build robust theoretical foundations and taxonomies for such relations, current methods for their automatic analysis and discovery in written discourse have yet to improve.", "labels": [], "entities": [{"text": "automatic analysis and discovery in written discourse", "start_pos": 121, "end_pos": 174, "type": "TASK", "confidence": 0.7472163949693952}]}, {"text": "However, providing robust models to analyze and identify rhetorical relations can benefit various research directions in computational linguistics such as text generation and summarization, and machine translation).", "labels": [], "entities": [{"text": "text generation and summarization", "start_pos": 155, "end_pos": 188, "type": "TASK", "confidence": 0.8229959905147552}, {"text": "machine translation", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.7897460162639618}]}, {"text": "One of the widely accepted frameworks for discourse analysis and understanding is Rhetorical Structure Theory (RST) (.", "labels": [], "entities": [{"text": "discourse analysis and understanding", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.7350351363420486}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.8037165602048238}]}, {"text": "In RST, discourse structure has a form of a tree, where the leaves correspond to elementary discourse units, and the internal nodes correspond to contiguous text spans.", "labels": [], "entities": [{"text": "RST", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.972282886505127}]}, {"text": "Each internal node is marked with a rhetorical relation that holds between its child nodes.", "labels": [], "entities": []}, {"text": "provides an example of an RST tree taken from the RST corpus ().", "labels": [], "entities": [{"text": "RST corpus", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.840131551027298}]}, {"text": "One of the notable differences of RST with other similar theories is that it is structured on the intentions of the writers to use those relations).", "labels": [], "entities": [{"text": "RST", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9539962410926819}]}, {"text": "This distinctive feature can make it even more difficult to build models for automatic identification of rhetorical relations in the context of RST.", "labels": [], "entities": [{"text": "automatic identification of rhetorical relations", "start_pos": 77, "end_pos": 125, "type": "TASK", "confidence": 0.7262418866157532}, {"text": "RST", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.8669741749763489}]}, {"text": "Rhetorical relations can be either explicit or implicit.", "labels": [], "entities": []}, {"text": "Explicit relations are the ones that are signaled by cues, such as lexical cues, mood, modality, and intonation), while no cue is present in implicit relations.", "labels": [], "entities": []}, {"text": "In this study, we are focused on explicit relations in written text that are signaled by the presence of lexical cues.", "labels": [], "entities": []}, {"text": "Lexical cues are defined as linguistic expressions that function as explicit indicators of a discourse relation.", "labels": [], "entities": []}, {"text": "For example, in the sentence provided in, but and because can be considered lexical cues signaling the existence of the CONCESSION relation and the EXPLANATION-ARGUMENTATIVE relation, respectively.", "labels": [], "entities": []}, {"text": "Since this study is part of a larger project to identify rationales in written discourse, we focus on the three relations of CIRCUMSTANCE, EVAL-UATION, and ELABORATION that are commonly present in rationales.", "labels": [], "entities": []}, {"text": "With the aim of proposing a cue-based approach to extract rhetorical relations, we have carried out some corpus-based experiments on RST annotated corpora.", "labels": [], "entities": [{"text": "extract rhetorical relations", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8528704245885214}, {"text": "RST annotated corpora", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.8817773461341858}]}, {"text": "As a result of these experiments, we have generated a list of key n-grams as potential lexical cues for each relation.", "labels": [], "entities": []}, {"text": "Such a corpus-based method may result in the discovery of underexplored lexical cues.", "labels": [], "entities": []}, {"text": "Even though lexical cues can be exploited to label rhetorical relations, they are not always unambiguous.", "labels": [], "entities": []}, {"text": "Some linguistic expressions mayor may not function as a lexical cue, or they may signal different types of relations in different sentences.", "labels": [], "entities": []}, {"text": "Hence, here, we propose a graph-based probabilistic model that takes into account the syntactic features of sentences.", "labels": [], "entities": []}, {"text": "These models are intended to determine in what syntactic context a lexical cue is indeed signaling the presence of a particular relation.", "labels": [], "entities": []}, {"text": "The models are then applied and tested on two corpora that belong to different text genres: news articles and online reviews.", "labels": [], "entities": []}, {"text": "The evaluation results of the approach are presented and discussed for the CIRCUMSTANCE relation.", "labels": [], "entities": [{"text": "CIRCUMSTANCE relation", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.8094929158687592}]}, {"text": "The CIRCUMSTANCE relation exists when a context of time or situation is presented, wherein the main events and ideas provided in the sentence can be interpreted in.", "labels": [], "entities": []}, {"text": "CIRCUMSTANCE is chosen as the relation of focus since revealed that the cue-based approaches can be well-suited for the detection of CIRCUMSTANCE across different genres, while the ELABORATION relation is not normally signaled.", "labels": [], "entities": [{"text": "ELABORATION", "start_pos": 181, "end_pos": 192, "type": "METRIC", "confidence": 0.8920826315879822}]}, {"text": "In addition, the features of the underlying text genre can significantly influence how EVALUA-TION is signaled (.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: An overview of the previous research on lexical cue disambiguation is provided in Section 2.", "labels": [], "entities": [{"text": "lexical cue disambiguation", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6302203039328257}]}, {"text": "In Section 3, an explanation of the underlying corpora and the methods used to extract and disambiguate the cues is provided.", "labels": [], "entities": []}, {"text": "The evaluation results are presented in Section 4.", "labels": [], "entities": []}, {"text": "A discussion of the findings is given in Section 5, followed by a conclusion of the study in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given that our ultimate goal is to detect rationales from written discourse, our approach is evaluated for the CIRCUMSTANCE relation as it is the only cue-based relation that is known to be frequently present in rationales ().", "labels": [], "entities": []}, {"text": "We carried out experiments using different forms of POS representations based on the number of POS tags surrounding the cue and the granularity of the tags.", "labels": [], "entities": []}, {"text": "We conducted experiments using the entire POS tagged instance, using two POS tags before and two tags after the cue, and using one before and one after the cue.", "labels": [], "entities": []}, {"text": "We also used three levels of POS tag granularity, including the finest, that is, the Penn English Treebank 4 tagset used by OpenNLP.", "labels": [], "entities": [{"text": "Penn English Treebank 4 tagset", "start_pos": 85, "end_pos": 115, "type": "DATASET", "confidence": 0.9674725651741027}, {"text": "OpenNLP", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.9544858932495117}]}, {"text": "We also used a VBZ, MD, VBG: In addition to the POS tags in the Penn English Treebank tag set, experiments are conducted using tags grouped according to different levels of granularity.", "labels": [], "entities": [{"text": "VBG", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.7573539614677429}, {"text": "Penn English Treebank tag set", "start_pos": 64, "end_pos": 93, "type": "DATASET", "confidence": 0.9796760201454162}]}, {"text": "medium and a coarse granularity that are created by gathering together similar tags into one highlevel tag.", "labels": [], "entities": []}, {"text": "shows the tags that are grouped in each of these two granularity levels.", "labels": [], "entities": []}, {"text": "Note that the tags not mentioned in the granularity levels are used as is.", "labels": [], "entities": []}, {"text": "Using these three variations of the two POS tag attributes resulted in nine different experimental settings.", "labels": [], "entities": []}, {"text": "We achieved our best results on both corpora using one tag before and one tag after the cue and the medium granularity level.", "labels": [], "entities": []}, {"text": "In this section, we report results for experiments using this particular POS setting.", "labels": [], "entities": []}, {"text": "The final algorithm built on probability values is evaluated using the Weka workbench . It classifies instances via regression 6 , and a stratified ten-fold cross validation is followed to evaluate the model.", "labels": [], "entities": [{"text": "Weka workbench", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.9430075585842133}]}, {"text": "To gain insight into the effectiveness of the model in the disambiguation of different cues, results are reported for each of the seven cues independently.", "labels": [], "entities": []}, {"text": "The SMOTE filter was used when significant class imbalance was encountered.", "labels": [], "entities": [{"text": "SMOTE filter", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.5693899095058441}]}, {"text": "demonstrates the results when the RST corpus was used to build the graphs, and the SFU corpus was used to build and test the final model.", "labels": [], "entities": [{"text": "RST corpus", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.7137475609779358}, {"text": "SFU corpus", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.953511506319046}]}, {"text": "shows the results of the evaluation, where graphs are built on the SFU corpus and used on the RST dataset.", "labels": [], "entities": [{"text": "SFU corpus", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.9696244895458221}, {"text": "RST dataset", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.7446490079164505}]}, {"text": "As can be seen, the measures of precision, recall, and F-score are reported, along with their average value.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9997900128364563}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9997140765190125}, {"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.999321460723877}]}, {"text": "The weighted average of F-score is also provided, taking into account the distribution of relation instances that contain the cues in the test set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9985906481742859}]}, {"text": "This metric is provided while bearing in mind that the test set may not bean accurate representative of the general distribution of relations.", "labels": [], "entities": []}, {"text": "According to the results, on average, we were able to classify CIRCUMSTANCE with an F-    score of 0.66% in the SFU review dataset, while the weighted average of F-score is 0.71%.", "labels": [], "entities": [{"text": "F-    score", "start_pos": 84, "end_pos": 95, "type": "METRIC", "confidence": 0.9932620525360107}, {"text": "SFU review dataset", "start_pos": 112, "end_pos": 130, "type": "DATASET", "confidence": 0.9615994294484457}, {"text": "F-score", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9982175230979919}]}, {"text": "In addition, an average F-score of 0.68% and a weighted average of 0.69% are achieved for the RST corpus.", "labels": [], "entities": [{"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9991738200187683}, {"text": "RST corpus", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.802977591753006}]}], "tableCaptions": [{"text": " Table 2: Classification results of a ten-fold cross  validation on the SFU corpus. Probability values  used as the underlying feature set are inferred from  the graph models built on the RST corpus.", "labels": [], "entities": [{"text": "SFU corpus", "start_pos": 72, "end_pos": 82, "type": "DATASET", "confidence": 0.9693340361118317}, {"text": "RST corpus", "start_pos": 188, "end_pos": 198, "type": "DATASET", "confidence": 0.7373180389404297}]}, {"text": " Table 3: Classification results of a ten-fold cross  validation on the RST dataset. Probability values  used as the underlying feature set are inferred from  the graph models built on the SFU corpus.", "labels": [], "entities": [{"text": "RST dataset", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.7750280499458313}, {"text": "SFU corpus", "start_pos": 189, "end_pos": 199, "type": "DATASET", "confidence": 0.9639252424240112}]}, {"text": " Table 4: Classification results on the SFU corpus  when the syntactic features are used directly to  train a model on the RST corpus", "labels": [], "entities": [{"text": "SFU corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9210359454154968}]}, {"text": " Table 5: Classification results on the RST corpus  when the syntactic features are used directly to  train a model on the SFU corpus", "labels": [], "entities": [{"text": "RST corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8640637099742889}, {"text": "SFU", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.8937178254127502}]}]}