{"title": [{"text": "On Strategies of Human Multi-Document Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7736955285072327}]}], "abstractContent": [{"text": "In this paper, using a corpus with manual alignments of human-written summaries and their source news, we show that such summaries consist of information that has specific linguistic features, revealing human content selection strategies, and that these strategies produce indicative results that are competitive with a state of the art system for Portuguese.", "labels": [], "entities": []}, {"text": "Neste artigo, a partir de um corpus com alinhamentos manuais entre sum\u00e1rios e suas respectivas not\u00edcias-fonte, evidencia-se que tais sum\u00e1rios s\u00e3o compostos por informa\u00e7\u00f5es que possuem caracter\u00edsticas lingu\u00edsticas espec\u00edficas, revelando estrat\u00e9gias humanas de sumariza\u00e7\u00e3o, e que essas estrat\u00e9gias produzem resultados iniciais que s\u00e3o competitivos com um sistema do estado da arte para o portugu\u00eas.", "labels": [], "entities": []}], "introductionContent": [{"text": "The increasing of new technologies has had an impact on the amount of available textual information on the web.", "labels": [], "entities": []}, {"text": "Consequently, Multi-document Summarization (MDS) appears to be a useful Natural Language Processing (NLP) application to promote quick access to large quantities of information, since it produces a unique summary from a collection or cluster of texts on the same topic or related topics.", "labels": [], "entities": [{"text": "Multi-document Summarization (MDS)", "start_pos": 14, "end_pos": 48, "type": "TASK", "confidence": 0.8664251804351807}]}, {"text": "Within a generic perspective, the multi-document summary should ideally contain the most relevant information of the topic that is being discussed in the source texts.", "labels": [], "entities": []}, {"text": "Moreover, MDS should not only focus on the extraction of relevant information, but also deal with the multi-document challenges, such as redundant, complementary and contradictory information, different writing styles and varied referential expressions.", "labels": [], "entities": [{"text": "MDS", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9371905326843262}]}, {"text": "There are two ways of approaching MDS.", "labels": [], "entities": [{"text": "MDS", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8044952750205994}]}, {"text": "The superficial/shallow approach uses little linguistic information (or statistics) to build summaries.", "labels": [], "entities": []}, {"text": "The deep approach is characterized by the usage of deep linguistic knowledge, i.e., syntactic, semantic or discourse information.", "labels": [], "entities": []}, {"text": "The superficial approach usually requires low-cost processing, but produces summaries that tend to have lower linguistic quality.", "labels": [], "entities": []}, {"text": "The deep approach is said to produce summaries of higher quality in terms of information, coherence and cohesion, but it demands various high-cost resources.", "labels": [], "entities": []}, {"text": "The deep and superficial MDS applications commonly produce extracts (i.e., summaries generated by concatenating sentences taken exactly as they appear in the source texts), but deep approach can also generate abstracts (i.e., with rewriting operations).", "labels": [], "entities": []}, {"text": "To select the sentences to compose the summaries, MDS may take into account human strategies from single-document summarization, codified in features such as sentence position and word frequency.", "labels": [], "entities": []}, {"text": "Regarding human multi-document summarization (HMDS), only redundancy has been widely applied as criterion for content selection, which is based on the empirical observation that the most repeated information covers the main topic of the cluster.", "labels": [], "entities": [{"text": "multi-document summarization (HMDS)", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.804341322183609}, {"text": "content selection", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7406680285930634}]}, {"text": "In this context, this work is focused on the investigation of HMDS content selection strategies.", "labels": [], "entities": [{"text": "HMDS content selection", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.9109821120897929}]}, {"text": "Particularly, fora corpus of news texts, we study some superficial and deep sentence features that maybe useful for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.9919981360435486}]}, {"text": "Since the source sentences in this corpus are aligned to the sentences of the correspondent reference (human) summary, we show that a machine learning technique could identify that a few features characterize well the aligned sentences (i.e., the sentences whose content was selected to the summary), achieving 70.8% of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 320, "end_pos": 328, "type": "METRIC", "confidence": 0.9986851811408997}]}, {"text": "We also show that additional experiments with the best learned HMDS strategy indicated that it may produce competitive results with a state of the art system for Portuguese, outperforming it fora small test corpus.", "labels": [], "entities": []}, {"text": "Consequently, this work contributes to the understanding of the HMDS task and to the improvement of the automatic process by providing linguistic insights.", "labels": [], "entities": [{"text": "HMDS task", "start_pos": 64, "end_pos": 73, "type": "TASK", "confidence": 0.8731809258460999}]}, {"text": "To describe this work, we organized the paper in 5 sections.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the main human content selection strategies and the correspondent features of the literature.", "labels": [], "entities": []}, {"text": "In Section 3, the used methodology is reported.", "labels": [], "entities": []}, {"text": "In Section 4, results are discussed, and, in Section 5, some final remarks are made.", "labels": [], "entities": []}], "datasetContent": [{"text": "Besides the results of the machine learning, we were also interested in checking the quality and informativeness of the summaries produced by the JRip rules.", "labels": [], "entities": []}, {"text": "These are the criteria that are usually assessed in summaries.", "labels": [], "entities": []}, {"text": "In order to do this summary evaluation, we manually created anew test corpus with the same characteristcs of CSTNews.", "labels": [], "entities": [{"text": "CSTNews", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9284624457359314}]}, {"text": "The test corpus consists of 6 clusters, and each of them contains: (i) 3 news texts on the same topic, (ii) 3 human multi-document summaries (abstracts), produced by different computational linguists, with 70% compression rate, (iii) sentential alignments among source texts and human summaries, and (iv) CST annotation in the texts.", "labels": [], "entities": []}, {"text": "We restricted the corpus to only 6 clusters because text annotation and summary writing tasks are expensive and time consuming tasks.", "labels": [], "entities": []}, {"text": "The summary building process is as follows.", "labels": [], "entities": []}, {"text": "Given a cluster of the test corpus, we first apply the JRip rules to select the sentences that are worthy to be in the summary (only sentences classified as \"yes\" are considered).", "labels": [], "entities": []}, {"text": "Having the rank, we start selecting the best ranked sentences to compose the summary, always checking for redundancy between the newly selected sentence and eventual previously selected sentences to the summary.", "labels": [], "entities": []}, {"text": "We use the information provided by CST to eliminate redundancy, by discarding the candidate sentence that has relations of the redundancy category with the ones already selected to the summary.", "labels": [], "entities": [{"text": "CST", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8185949325561523}]}, {"text": "For example, if the relation between two sentences is Identity, the new sentence is ignored; if the relation is Equivalence, we eliminate the longest sentence (considering the number of words in the sentence); if the relation is Subsumption, we eliminate the sentence that is subsumed.", "labels": [], "entities": [{"text": "Equivalence", "start_pos": 112, "end_pos": 123, "type": "METRIC", "confidence": 0.9568917751312256}]}, {"text": "We select as many sentences to the summary as the compression rate allows.", "labels": [], "entities": []}, {"text": "For comparison, the summaries generated by another method of MDS for the same 6 clusters were also judged considering the same textual properties.", "labels": [], "entities": []}, {"text": "In this case, the automatic method used to generate the comparison summaries was RSumm [, which is one of the state of the art systems for Portuguese.", "labels": [], "entities": []}, {"text": "The evaluation of the properties related to quality was performed by 10 computational linguists.", "labels": [], "entities": []}, {"text": "For each automatic summary, the judges scored each of the 5 textual properties through an online form.", "labels": [], "entities": []}, {"text": "For all properties, judges had a scale from 1 to 5 points, being 1=very poor, 2=poor, 3=barely acceptable, 4=good, and 5=very good.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The values are presented in two ways: (i) absolute values (which is the number of votes for the corresponding scale), and (ii) percentage.", "labels": [], "entities": []}, {"text": "Looking to the average values, one may see that the JRip rules outperform Rsumm in all the evaluated criteria, indicating that the used features in this study are better at dealing with textuality factors in the summaries.", "labels": [], "entities": []}, {"text": "Regarding informativeness evaluation, we used the traditional automatic ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure, which is mandatory in the area.", "labels": [], "entities": [{"text": "informativeness evaluation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8027478456497192}, {"text": "ROUGE", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.7779970765113831}]}, {"text": "ROUGE computes the number of common n-grams among the automatic and reference/human summaries, being able to rank automatic summaries as well as humans would do, as its author has shown.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5976155400276184}]}, {"text": "shows average ROUGE results for 1-grams (referenced by ROUGE-1), 2-grams (ROUGE-2) and the longest common subsequence (ROUGE-L) overlap, in terms of Recall (R), Precision (P) and F-measure (F), for both JRip rules and RSumm.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9827457666397095}, {"text": "ROUGE-2", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9549499750137329}, {"text": "ROUGE-L) overlap", "start_pos": 119, "end_pos": 135, "type": "METRIC", "confidence": 0.8895281553268433}, {"text": "Recall (R)", "start_pos": 149, "end_pos": 159, "type": "METRIC", "confidence": 0.9441423565149307}, {"text": "Precision (P)", "start_pos": 161, "end_pos": 174, "type": "METRIC", "confidence": 0.9423828274011612}, {"text": "F-measure (F)", "start_pos": 179, "end_pos": 192, "type": "METRIC", "confidence": 0.9534815549850464}]}, {"text": "Basically, recall computes the On Strategies of Human Multi-Document Summarization amount of common n-grams in relation to the number of n-grams in the reference summaries; precision computes the number of common n-grams in relation to the ngrams in the automatic summary; the f-measure is the harmonic mean of the previous 2 measures, being an unique indicator of the system performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9974889755249023}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9993459582328796}]}, {"text": "One may see that the JRip rules outperform RSumm in all the measures.", "labels": [], "entities": [{"text": "RSumm", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.5639773607254028}]}, {"text": "If we consider the f-measure for ROUGE-1, which is by far the most used in the literature, we may see that the JRip rules are approximately 6.7% better than RSumm.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9697317481040955}, {"text": "RSumm", "start_pos": 157, "end_pos": 162, "type": "DATASET", "confidence": 0.8156672716140747}]}, {"text": "It is important to say, however, that such results are only indicative of what we may expect from the rules and the discriminative power of the studied features, since the test set for quality evaluation and ROUGE was too small (only 6 clusters).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 208, "end_pos": 213, "type": "METRIC", "confidence": 0.9737348556518555}]}, {"text": "For a more reliable result, we would need to run the rules fora bigger corpus.", "labels": [], "entities": []}, {"text": "We could not do that for CSTNews because this corpus was already used for creating the rules (during the training), and using it for testing would result in a biased evaluation.", "labels": [], "entities": []}, {"text": "And, besides CSTNews, we are not aware of other corpora with the data/annotation we need for our rules to work.", "labels": [], "entities": [{"text": "CSTNews", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.8638460040092468}]}, {"text": "Having the reservations been made, it is interesting that the rules could outperform RSumm (even fora small test corpus), since highly deeper and more informed approaches have struggled to do that (see, e.g.,).", "labels": [], "entities": []}, {"text": "This shows how effective the learned HDMS strategy is.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Linguistic quality evaluation of summaries with DUC criteria  Criteria Method Very poor (1)  Poor (2)  Barely Acceptable (3)  Good (4)  Very Good (5)  Average", "labels": [], "entities": [{"text": "DUC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.7376506924629211}, {"text": "Barely Acceptable", "start_pos": 113, "end_pos": 130, "type": "METRIC", "confidence": 0.7627993822097778}]}, {"text": " Table 4. Informativeness evaluation of summaries with ROUGE  Avg. ROUGE-1  Avg. ROUGE-2  Avg. ROUGE-L  R  P  F  R  P  F  R  P  F  JRip rules 0.444  0.517  0.464  0.200  0.242  0.212  0.373  0.441  0.392  RSumm  0.425  0.448  0.435  0.191  0.202  0.196  0.358  0.378  0.367", "labels": [], "entities": []}]}