{"title": [], "abstractContent": [{"text": "We present a method that improves data selection by combining a hybrid word/part-of-speech representation for corpora, with the idea of distinguishing between rare and frequent events.", "labels": [], "entities": [{"text": "data selection", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.6856041550636292}]}, {"text": "We validate our approach using data selection for machine translation, and show that it maintains or improves BLEU and TER translation scores while substantially improving vocabulary coverage and reducing data selection model size.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8055990934371948}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9986020922660828}, {"text": "TER translation scores", "start_pos": 119, "end_pos": 141, "type": "METRIC", "confidence": 0.8703864812850952}]}, {"text": "Paradoxically, the coverage improvement is achieved by abstracting away over 97% of the total training corpus vocabulary using simple part-of-speech tags during the data selection process.", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.978857696056366}]}], "introductionContent": [{"text": "Data selection uses a small set of domain-relevant data to select additional training items from a much larger, out-of-domain dataset.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6514734327793121}]}, {"text": "Its goal is to filter Big Data down to Good Data: finding the best, most relevant data to use to train a model fora particular task.", "labels": [], "entities": []}, {"text": "The prevalent data selection method, crossentropy difference, can produce domain-specific systems that are usually as good as or better than systems using all available training data.", "labels": [], "entities": []}, {"text": "The size of these domain-specific systems scales roughly linearly with the amount of selected data: a system trained on the most domain-relevant 10% of the full out-of-domain dataset will be only one tenth of the size of a system trained using all the available data.", "labels": [], "entities": []}, {"text": "This can be a large win in settings where training time matters, and also where compactness of the final system matters, e.g. running speech recognition or translation on mobile devices.", "labels": [], "entities": [{"text": "speech recognition or translation", "start_pos": 134, "end_pos": 167, "type": "TASK", "confidence": 0.6467853114008904}]}, {"text": "While data selection thus eliminates the need to train systems on the entire pool of available data, the data selection process itself does not scale well (it still requires a language model built on the entire pool) and, more significantly, it comes at a cost: training on selected subsets leads to reductions in vocabulary coverage compared to training on the full out-of-domain data pool.", "labels": [], "entities": []}, {"text": "This coverage is important, because most NLP systems face the problem of handling words that were not seen in training the system, i.e. out-of-vocabulary (OOV) words.", "labels": [], "entities": [{"text": "coverage", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9407159686088562}]}, {"text": "In automatic speech recognition (ASR), for example, OOV words pose a substantial problem, since the system will hallucinate a phonetically similar word in its vocabulary when an OOV word is encountered.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 3, "end_pos": 37, "type": "TASK", "confidence": 0.7761699855327606}]}, {"text": "In machine translation (MT), our focal application in this paper, OOVs can sometimes be transliterated, but often they are ignored or passed through without translation, and gaps in vocabulary coverage can have a significant effect on MT performance.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8715306639671325}, {"text": "MT", "start_pos": 235, "end_pos": 237, "type": "TASK", "confidence": 0.989957869052887}]}, {"text": "We introduce a method that preserves the data selection benefit of reducing translation system size.", "labels": [], "entities": []}, {"text": "Our method performs as well or better than the standard cross-entropy difference method, as measured by downstream MT results.", "labels": [], "entities": []}, {"text": "To this we add the benefits of substantially improved lexical coverage, as well as lower memory requirements for the data selection model itself.", "labels": [], "entities": []}, {"text": "This improvement stems from constructing a hybrid representation of the text that abstracts away words that are infrequent in either of the indomain and general corpora.", "labels": [], "entities": []}, {"text": "They are replaced with their part-of-speech (POS) tags, permitting their n-gram statistics to be robustly aggregated: intuitively, if a domain-relevant sentence includes a rare word in some non-rare context (e.g. \"An earthquake in Port-au-Prince\"), then another sentence with the same context but a different rare word is probably also just as relevant (e.g. \"An earthquake in Kodari\").", "labels": [], "entities": []}, {"text": "While this method requires pre-processing the corpora to POS tag the data, the idea should generalize to automaticallyderived word classes.", "labels": [], "entities": []}, {"text": "We present results using data selection to train domain-relevant SMT systems, yielding favorable performance compared against the standard approaches of and.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9578432440757751}]}, {"text": "Paradoxically, this is achieved by a selection process in which the specific lexical items for infrequent words -up to 97% of the total vocabulary -are replaced with POS tags.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our data selection approach in a realistic small-in-domain-corpus setting, in two ways.", "labels": [], "entities": []}, {"text": "First, as an intrinsic evaluation, we look at vocabulary coverage of the selected data relative to the in-domain training set, i.e. how many words from the in-domain corpus are out-of-vocabulary for selected data, since models trained on those data would not not be able to handle those words.", "labels": [], "entities": []}, {"text": "Second, as an extrinsic evaluation, we use statistical machine translation as a downstream task.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.6348564624786377}]}, {"text": "We define our in-domain corpus as the TED talk translations in the WIT 3 TED Chinese-English corpus (), a good example of a subdomain with little available training data.", "labels": [], "entities": [{"text": "TED talk translations", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.7924432555834452}, {"text": "WIT 3 TED Chinese-English corpus", "start_pos": 67, "end_pos": 99, "type": "DATASET", "confidence": 0.8789041876792908}]}, {"text": "We used the IWSLT dev2010 and test2010 sets (also from WIT 3 ) for tuning and evaluation.", "labels": [], "entities": [{"text": "IWSLT dev2010", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.906819611787796}, {"text": "WIT 3", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.911069929599762}]}, {"text": "The larger pool from which we selected data was constructed from an aggregation of 47 LDC Chinese-English parallel datasets.", "labels": [], "entities": []}, {"text": "1  We used the KenLM toolkit to build all language models used in this work (i.e., both for data selection and for the MT systems used for extrinsic evaluation).", "labels": [], "entities": [{"text": "data selection", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.7400660216808319}]}, {"text": "In all cases the models were 4-gram LMs.", "labels": [], "entities": []}, {"text": "We used the Stanford part-of-speech tagger () when constructing our hybrid representations, to generate the POS tags for each of the English and Chinese sides of the corpora.", "labels": [], "entities": []}, {"text": "We consider three ways of applying data selection using the standard (fully lexicalized) corpus representation and our hybrid representation.", "labels": [], "entities": []}, {"text": "The first two use the monolingual MooreLewis method (Equation 1) to respectively compute relevance scores using the English (output) side and the Chinese (input) side of the parallel corpora.", "labels": [], "entities": []}, {"text": "The third uses bilingual Moore-Lewis (Equation 2) to compute the bilingual score over both sides.", "labels": [], "entities": []}, {"text": "Each of these three variants produces aversion of the full pool in which the sentences are ranked by relevance score, from lowest score  (most domain-like) to highest score (least domainlike).", "labels": [], "entities": []}, {"text": "For each of those ranked pools, we consider increasingly larger subsets of the data: the best n = 50K, the best n = 100K, and soon.", "labels": [], "entities": []}, {"text": "The largest subset we consider consists of the best n = 4M sentence pairs out of the 6M available.", "labels": [], "entities": []}, {"text": "As noted, each of the bilingual Moore-Lewis method and our hybrid word/POS variation produces aversion of the additional training pool in which sentences are ranked by relevance.", "labels": [], "entities": []}, {"text": "We then select increasingly larger slices of the data from 50k to 4M, as described in Section 4.1, and report results.", "labels": [], "entities": []}, {"text": "As shown in, the hybridselected models show consistently improved vocabulary coverage when compared head-to-head with models trained on data selected via a MooreLewis method, across all subsets.", "labels": [], "entities": [{"text": "coverage", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.5114201307296753}]}, {"text": "The only exception is when examining the vocabulary coverage in one language while selecting data based on the other one (e.g. selecting data using the English half but measuring the TED vocabulary coverage in Chinese), where our method provides only negligible improvement.", "labels": [], "entities": []}, {"text": "Overall, the in-domain (TED) vocabulary coverage is up to 10% better with our proposed method, and the general-data (LDC) vocabulary coverage is up to 20% better.", "labels": [], "entities": [{"text": "general-data (LDC) vocabulary coverage", "start_pos": 103, "end_pos": 141, "type": "METRIC", "confidence": 0.584261879324913}]}, {"text": "illustrates what this looks like in more detail fora single slice containing the top 2M sentence pairs.", "labels": [], "entities": []}, {"text": "The table shows how many more vocabulary items are covered by the 2M sentence slice selected using our hybrid representation (the Hyb columns) than are covered by the best 2M sentences selected using the standard lexical representation (the Std columns).", "labels": [], "entities": []}, {"text": "Our method shows this improved vocabulary coverage regardless of whether one compares the vocabulary coverage of the methods on the English side (the first three rows) or the Chinese side (the second three rows) of the corpora.", "labels": [], "entities": []}, {"text": "Furthermore, the results also hold regardless of which of the three ways of performing cross-entropy- En Mono-en 67% 72% 42% 52% Mono-Zh 70% 71% 48% 54% Bilingual 68% 72% 42% 52% Zh Mono-En 70% 71% 38% 46% Mono-Zh 69% 73% 43% 62% Bilingual 69% 73% 37% 54%: Vocabulary coverage comparison between standard and hybrid-based data selection, for data-selected samples of 2M sentences.", "labels": [], "entities": []}, {"text": "based data selection one uses.", "labels": [], "entities": []}, {"text": "The three ways are: monolingual Moore-Lewis for the English and Chinese sides of the parallel corpus (Mono-En and Mono-Zh, respectively), as well as bilingual Moore-Lewis (Bilingual).", "labels": [], "entities": []}, {"text": "When selecting 2M sentences, shows that the hybrid representation provides up to an extra 4-5% in-domain vocabulary coverage in either language.", "labels": [], "entities": []}, {"text": "Furthermore, the hybrid-based methods obtain up to 10% more general-domain vocabulary coverage for English, and up to 19% more Chinese general-domain vocabulary coverage.", "labels": [], "entities": []}, {"text": "All improvements are absolute percentage increases.", "labels": [], "entities": []}, {"text": "shows that our hybrid method's pool vocabulary coverage increases more rapidly than the baseline.", "labels": [], "entities": [{"text": "coverage", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.8202897906303406}]}, {"text": "The standard approach shows vocabulary coverage increasing more or less linearly with the amount of selection data.", "labels": [], "entities": []}, {"text": "By contrast, our proposed method appears to asymptotically approach full in-domain vocabulary coverage, particularly for Chinese.", "labels": [], "entities": []}, {"text": "Similarly, shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well.", "labels": [], "entities": []}, {"text": "Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance.", "labels": [], "entities": []}, {"text": "Accordingly, we trained SMT systems using cdec) on subsets of selected data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9943388104438782}]}, {"text": "All SMT systems were tuned using MIRA () on the dev2010 data from, and then evaluated on the test2010 IWSLT test set using both BLEU () and TER ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9897554516792297}, {"text": "MIRA", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9961375594139099}, {"text": "dev2010 data", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8317882418632507}, {"text": "test2010 IWSLT test set", "start_pos": 93, "end_pos": 116, "type": "DATASET", "confidence": 0.8023580610752106}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9983512163162231}, {"text": "TER", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9916967153549194}]}, {"text": "To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model system.", "labels": [], "entities": []}, {"text": "Note that the hybrid word/POS representations were only used to compute the cross-entropy difference scores for determining sentences' relevance; the MT systems themselves are trained using the sentences containing the original words.", "labels": [], "entities": []}, {"text": "shows our MT results using both BLEU and TER.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9303714632987976}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9992570281028748}, {"text": "TER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.997943103313446}]}, {"text": "The horizontal line is a static baseline that uses all the available training data without data selection.", "labels": [], "entities": []}, {"text": "The dashed grey line is from systems trained on data selected via the standard Moore-Lewis cross-entropy-difference method, and the black line is from systems trained on data selected with our hybrid approach.", "labels": [], "entities": []}, {"text": "To account for variability in MT tuning, each of the curves in is the average of three tuning/decoding runs.", "labels": [], "entities": [{"text": "MT tuning", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.8952520191669464}]}, {"text": "In terms of system accuracy, our results confirm prior work on data selection, demonstrating that in comparison to training using all available data, comparable or even better MT performance can be obtained using only a fraction of the outof-domain data available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9959386587142944}, {"text": "MT", "start_pos": 176, "end_pos": 178, "type": "TASK", "confidence": 0.9886788725852966}]}, {"text": "shows SMT results for the same subset size of 2M sentences used for the coverage results in.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9535819292068481}]}, {"text": "Systems trained on data selected using the hybrid representation are up to +0.5 BLEU better, regardless of whether the selection process is monolingual or bilingual.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9986404776573181}]}, {"text": "Indeed, at least for BLEU, it appears that our hybrid method may tend to converge to comparable performance more quickly, a possibility worthy of future experimentation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9899514317512512}]}, {"text": "The TER results are mixed for this data selection subset size.", "labels": [], "entities": [{"text": "TER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9650779366493225}]}, {"text": "The MT evaluation scores are low in absolute terms, due to only using the general-domain data, yet are still not inconsistent with prior research done using this dataset).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8057372570037842}]}, {"text": "Fluctuations in the performance curves are also consistent with prior work, as IWSLT scores are very jittery.", "labels": [], "entities": [{"text": "Fluctuations", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9726406335830688}, {"text": "IWSLT", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.6344801783561707}]}, {"text": "We averaged results over three tuning runs, for stability.", "labels": [], "entities": []}, {"text": "Despite that, shows how high-variance TER scores are on this task.", "labels": [], "entities": [{"text": "TER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9948234558105469}]}], "tableCaptions": [{"text": " Table 1: Chinese-English Parallel Data.", "labels": [], "entities": [{"text": "Chinese-English Parallel Data", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.7258685429890951}]}, {"text": " Table 2: Chinese and English vocabulary for the  baseline selection process.", "labels": [], "entities": []}, {"text": " Table 3: Chinese and English vocabulary for the  proposed selection process.", "labels": [], "entities": []}, {"text": " Table 5: SMT system score comparison between  standard and hybrid-based data selection, for data- selected samples of 2M sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9927768707275391}]}]}