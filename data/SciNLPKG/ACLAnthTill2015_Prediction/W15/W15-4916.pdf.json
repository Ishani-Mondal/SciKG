{"title": [{"text": "Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation", "labels": [], "entities": [{"text": "Translation Quality Estimation", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.9074539343516032}]}], "abstractContent": [{"text": "In this paper we analyse the use of popular automatic machine translation evaluation metrics to provide labels for quality estimation at document and paragraph levels.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.778606136639913}]}, {"text": "We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts.", "labels": [], "entities": []}, {"text": "To better understand these limitations, we designed experiments with human annotators and proposed away of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs.", "labels": [], "entities": []}, {"text": "Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation (QE) of machine translation (MT) () is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references.", "labels": [], "entities": [{"text": "Quality estimation (QE) of machine translation (MT)", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8595682897351005}, {"text": "predicting the quality of new, unseen machine translation", "start_pos": 82, "end_pos": 139, "type": "TASK", "confidence": 0.7325178252326118}]}, {"text": "This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.6950228214263916}]}, {"text": "Most current work on QE is done at the sentence level.", "labels": [], "entities": [{"text": "QE", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9500474333763123}]}, {"text": "A popular application of sentence-level QE is to support post-editing of MT ().", "labels": [], "entities": [{"text": "sentence-level QE", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.5223142504692078}, {"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.8485796451568604}]}, {"text": "As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the final version -HTER ().", "labels": [], "entities": []}, {"text": "There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required.", "labels": [], "entities": []}, {"text": "This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it.", "labels": [], "entities": []}, {"text": "The quality of a document is often seen as some form of aggregation of the quality of its sentences.", "labels": [], "entities": []}, {"text": "We claim, however, that document-level quality assessment should consider more information than sentence-level quality.", "labels": [], "entities": [{"text": "document-level quality assessment", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.610775758822759}]}, {"text": "This includes, for example, the topic and structure of the document and the relationship between its sentences.", "labels": [], "entities": []}, {"text": "While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text.", "labels": [], "entities": []}, {"text": "Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose.", "labels": [], "entities": []}, {"text": "Document-level quality prediction is a rather understudied problem.", "labels": [], "entities": [{"text": "Document-level quality prediction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8039693037668864}]}, {"text": "Recent work has looked into document-level prediction) using automatic metrics such as BLEU () and TER () as quality labels.", "labels": [], "entities": [{"text": "document-level prediction", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.792787492275238}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9984331727027893}, {"text": "TER", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9883098602294922}]}, {"text": "However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive.", "labels": [], "entities": [{"text": "mean error", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.8597120046615601}]}, {"text": "In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9992831349372864}, {"text": "TER score", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9386567175388336}]}, {"text": "Other studies have considered document-level information in order to improve, analyse or au-tomatically evaluate MT output (not for QE purposes).", "labels": [], "entities": [{"text": "MT output", "start_pos": 113, "end_pos": 122, "type": "TASK", "confidence": 0.8717000186443329}]}, {"text": "report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. and show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9729700088500977}]}, {"text": "explore rhetorical structure (RST) trees for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation.", "labels": [], "entities": [{"text": "rhetorical structure (RST)", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.71929771900177}, {"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.7914726138114929}]}, {"text": "Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations.", "labels": [], "entities": []}, {"text": "Previous work on document-level QE use automatic evaluation metrics as quality labels that do not consider document-level structures and are developed for inter-system rather than intra-system evaluation.", "labels": [], "entities": []}, {"text": "Also, previous work on evalution of MT does not focus on complete evaluation at documentlevel.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9500274062156677}]}, {"text": "In this paper, we show that the use of BLEU and other automatic metrics as quality labels do not help to successfully distinguish different quality levels.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9964738488197327}]}, {"text": "We discuss the role of document-wide information for document-level quality estimation and present two experiments with human annotators.", "labels": [], "entities": [{"text": "document-level quality estimation", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.6001381874084473}]}, {"text": "In the first experiment, translators are asked to subjectively assess paragraphs in terms of cohesion and coherence (herein, SUBJ).", "labels": [], "entities": []}, {"text": "In the second experiment, a two-pass post-editing experiment is performed in order to measure the difference between corrections made with and without wider contexts (the tow passes are called PE1 and PE2, repectively).", "labels": [], "entities": [{"text": "PE1", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.8147890567779541}]}, {"text": "The task of assessing paragraphs according to cohesion and coherence is highly subjective and thus the results of the first study did not show high agreement among annotators.", "labels": [], "entities": []}, {"text": "The results of the two-stage post-editing experiment showed significant differences from the post-editing of sentences without context to the second stage where sentences were further corrected in context.", "labels": [], "entities": []}, {"text": "This is an indication that certain translation issues can only be solved by relying on wider contexts, which is a crucial information for document-level QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 153, "end_pos": 155, "type": "TASK", "confidence": 0.6573807001113892}]}, {"text": "A manual analysis was conducted to evaluate differences between PE1 and PE2.", "labels": [], "entities": [{"text": "PE1", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.6817397475242615}, {"text": "PE2", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8523634076118469}]}, {"text": "Although several of the changes were found to be related to style or other non-discourse related phenomena, many discourse related changes were performed that were only possible given the wider context available.", "labels": [], "entities": []}, {"text": "In the remainder of this paper we first present related work in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss the use of BLEU-style metrics for QE at document level.", "labels": [], "entities": [{"text": "BLEU-style", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9928620457649231}, {"text": "QE", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.8925580978393555}]}, {"text": "Section 4 describes the experimental setup used in the paper.", "labels": [], "entities": []}, {"text": "Section 5 presents the first study were the annotators assess quality in terms of cohesion and coherence, while Section 6 shows the two-pass post-editing experiment and its results.", "labels": [], "entities": []}, {"text": "The conclusions and future work are presented in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "As discussed in Section 2, although the use of BLEU-style metrics as quality scores for document-level QE clearly seems inadequate, previous work resorted to these automatic metrics because of the lack of better labels.", "labels": [], "entities": [{"text": "BLEU-style", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9905796647071838}]}, {"text": "In order to better understand this problem, we conducted an experiment with French-English translations from the LIG corpus ().", "labels": [], "entities": [{"text": "LIG corpus", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.943769246339798}]}, {"text": "We took the first part of the corpus containing 119 source documents on the news domain (from various WMT news test sets), their MT by a phrase-based SMT system, a post-edited version of these translations by a human translator, and a reference translation.", "labels": [], "entities": [{"text": "WMT news test sets", "start_pos": 102, "end_pos": 120, "type": "DATASET", "confidence": 0.8864197134971619}, {"text": "MT", "start_pos": 129, "end_pos": 131, "type": "TASK", "confidence": 0.8613007068634033}]}, {"text": "We used a range of automatic metrics such as BLEU, TER, METEOR-ex (exact match) and METEOR-st (stem match), which are based on a comparison between machine translations and human references, and the \"human-targeted\" version of BLEU and TER, where machine translations are compared against their post-editions: HBLEU and HTER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9982561469078064}, {"text": "TER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9707213640213013}, {"text": "METEOR-ex (exact match)", "start_pos": 56, "end_pos": 79, "type": "METRIC", "confidence": 0.8227376341819763}, {"text": "METEOR-st (stem match)", "start_pos": 84, "end_pos": 106, "type": "METRIC", "confidence": 0.839043140411377}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.99261873960495}, {"text": "HBLEU", "start_pos": 310, "end_pos": 315, "type": "DATASET", "confidence": 0.8825045228004456}, {"text": "HTER", "start_pos": 320, "end_pos": 324, "type": "DATASET", "confidence": 0.9186652302742004}]}, {"text": "shows the results of the average score (AVG) for each metric considering all documents, as well as the standard deviation (STDEV).", "labels": [], "entities": [{"text": "average score (AVG)", "start_pos": 25, "end_pos": 44, "type": "METRIC", "confidence": 0.8948460817337036}, {"text": "standard deviation (STDEV)", "start_pos": 103, "end_pos": 129, "type": "METRIC", "confidence": 0.9179224252700806}]}, {"text": "0.64 0.05 We conducted a similar analysis on the EnglishGerman (EN-DE) news test set from WMT13 (, which contains 52 documents, both at document and paragraph levels.", "labels": [], "entities": [{"text": "EnglishGerman (EN-DE) news test set from WMT13", "start_pos": 49, "end_pos": 95, "type": "DATASET", "confidence": 0.9153070052464803}]}, {"text": "Three MT systems were considered in this analysis: UEDIN (an SMT system), PROMT (a hybrid system) and RBMT-1 (a rule-based system).", "labels": [], "entities": [{"text": "MT", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9687392711639404}, {"text": "RBMT-1", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.6129313707351685}]}, {"text": "Average metric scores are shown in.", "labels": [], "entities": []}, {"text": "For all the metrics and corpora, the STDEV values for documents are very small (below 0.1), indicating that all documents are considered similar in terms of quality according to these metrics (the scores are all very close to the mean).", "labels": [], "entities": [{"text": "STDEV", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.884864091873169}]}, {"text": "At paragraph level, the scores variation increases, with BLEU showing the highest variation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9992778897285461}]}, {"text": "However, the very high STDEV values for BLEU (very close to the actual average score for all documents) is most likely due to the fact that BLEU does not perform well for short segments such as a paragraph due to the n-gram sparsity at this level, as shown in Stanojevi\u00b4c.", "labels": [], "entities": [{"text": "STDEV", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9877501130104065}, {"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9926853179931641}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9791714549064636}]}, {"text": "Overall, it is important to emphasise that BLEUstyle metrics were created to evaluate different MT systems based on the same input, as opposed to evaluating different outputs of a single MT system, as we do here.", "labels": [], "entities": [{"text": "BLEUstyle", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9873508810997009}, {"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9685624241828918}]}, {"text": "The experiments in Section 6 attempt to shed some light on alternative ways to accurately measure document-level quality, with an emphasis on designing a label for document-level quality prediction.", "labels": [], "entities": [{"text": "document-level quality prediction", "start_pos": 164, "end_pos": 197, "type": "TASK", "confidence": 0.5885832409063975}]}, {"text": "In the following experiments, we consider a paragraph as a \"document\".", "labels": [], "entities": []}, {"text": "This decision was made to make the annotation feasible, given the time and resources available.", "labels": [], "entities": []}, {"text": "Although the datasets are different for the two subtasks, they were taken from the same larger corpus and annotated by the the same group of translators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average metric scores for automatic metrics in the WMT13 EN-DE corpus.", "labels": [], "entities": [{"text": "WMT13 EN-DE corpus", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.9322848518689474}]}, {"text": " Table 3: WMT13 English source corpus.", "labels": [], "entities": [{"text": "WMT13 English source corpus", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.9335493892431259}]}, {"text": " Table  4. The number of annotators per set is different  because some of them did not complete the task.", "labels": [], "entities": []}, {"text": " Table 4: Spearman's correlation for the SUBJ task.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.6466113030910492}, {"text": "SUBJ task", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.8891505599021912}]}, {"text": " Table 5: HTER values for PE1 against MT and PE1 against PE2 and Spearman's rank correlation values  for PE2 against PE1.", "labels": [], "entities": [{"text": "HTER", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9847354888916016}, {"text": "MT", "start_pos": 38, "end_pos": 40, "type": "DATASET", "confidence": 0.7547643780708313}, {"text": "Spearman's rank correlation", "start_pos": 65, "end_pos": 92, "type": "METRIC", "confidence": 0.819518655538559}]}]}