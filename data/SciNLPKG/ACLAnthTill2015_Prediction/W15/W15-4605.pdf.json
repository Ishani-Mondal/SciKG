{"title": [{"text": "Reinforcement Learning in Multi-Party Trading Dialog", "labels": [], "entities": [{"text": "Reinforcement Learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9194073379039764}, {"text": "Multi-Party Trading Dialog", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.6293992102146149}]}], "abstractContent": [{"text": "In this paper, we apply reinforcement learning (RL) to a multi-party trading scenario where the dialog system (learner) trades with one, two, or three other agents.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6716970443725586}]}, {"text": "We experiment with different RL algorithms and reward functions.", "labels": [], "entities": [{"text": "RL", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9642081260681152}]}, {"text": "The negotiation strategy of the learner is learned through simulated dialog with trader sim-ulators.", "labels": [], "entities": []}, {"text": "In our experiments, we evaluate how the performance of the learner varies depending on the RL algorithm used and the number of traders.", "labels": [], "entities": []}, {"text": "Our results show that (1) even in simple multi-party trading dialog tasks, learning an effective negotiation policy is a very hard problem; and (2) the use of neural fitted Q iteration combined with an incremental reward function produces negotiation policies as effective or even better than the policies of two strong hand-crafted baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Trading dialogs area kind of interaction in which an exchange of ownership of items is discussed, possibly resulting in an actual exchange.", "labels": [], "entities": []}, {"text": "These kinds of dialogs are pervasive in many situations, such as marketplaces, business deals, school lunchrooms, and some kinds of games, like Monopoly or Settlers of Catan ().", "labels": [], "entities": [{"text": "Monopoly or Settlers of Catan", "start_pos": 144, "end_pos": 173, "type": "DATASET", "confidence": 0.6611413836479187}]}, {"text": "Most of these dialogs are noncooperative, in the sense that mere recognition of the desire for one party to engage in a trade does not provide sufficient inducement for the other party to accept the trade.", "labels": [], "entities": []}, {"text": "Usually a trade will only be accepted if it is in the perceived interest of each party.", "labels": [], "entities": []}, {"text": "Trading dialogs can be considered as a kind of negotiation, in which participants use various tactics to try to reach an agreement.", "labels": [], "entities": [{"text": "Trading dialogs", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8934987187385559}]}, {"text": "It is common to have dialogs that may involve multiple offers or even multiple trades.", "labels": [], "entities": []}, {"text": "In this way, trading dialogs are different from other sorts of negotiation in which a single decision (possibly about multiple issues) is considered, for example partitioning a set of items (.", "labels": [], "entities": [{"text": "trading dialogs", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.9080209136009216}, {"text": "partitioning a set of items", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.829523766040802}]}, {"text": "Another difference between trading dialogs and partitioning dialogs is what happens when a deal is not made.", "labels": [], "entities": []}, {"text": "In partitioning dialogs, if an agreement is not reached, then participants get nothing, so there is a very strong incentive to reach a deal, which allows pressure and can result in a \"chicken game\", where people give up value in order to avoid a total loss.", "labels": [], "entities": [{"text": "partitioning dialogs", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.9081868529319763}]}, {"text": "By contrast, in trading dialogs, if no deal is made, participants stick with the status quo.", "labels": [], "entities": []}, {"text": "Competitive two-party trading dialogs may result in a kind of stasis, where the wealthier party will pass up mutually beneficial deals, in order to maintain primacy.", "labels": [], "entities": []}, {"text": "On the other hand, multi-party trading dialogs involving more than two participants changes the dynamic again, because now a single participant cannot necessarily even block another from acquiring a missing resource, because it might be available through trades with a third party.", "labels": [], "entities": []}, {"text": "A player who does not engage in deals may lose relative position, if the other participants make mutually beneficial deals.", "labels": [], "entities": []}, {"text": "In this paper, we present a first approach toward learning dialog policies for multi-party trading dialogs.", "labels": [], "entities": []}, {"text": "We introduce a simple, but flexible gamelike scenario, where items can have different values for different participants, and also where the value of an item can depend on the context of other items held.", "labels": [], "entities": []}, {"text": "We examine a number of strategies for this game, including random, simple, and complex hand-crafted strategies, as well as several reinforcement learning (RL) algorithms, and examine performance with different numbers and kinds of opponents.", "labels": [], "entities": []}, {"text": "In most of the previous work on statistical dialog management, RL was applied to cooperative slot-filling dialog domains.", "labels": [], "entities": [{"text": "statistical dialog management", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.7710113326708475}, {"text": "RL", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.646453857421875}]}, {"text": "For example, RL was used to learn the policies of dialog systems for food ordering, tourist information (, flight information (), appointment scheduling, and email access.", "labels": [], "entities": [{"text": "RL", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.7615467309951782}, {"text": "food ordering", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.7341412305831909}, {"text": "appointment scheduling", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.7322972863912582}]}, {"text": "In these typical slotfilling dialog systems, the reward function depends on whether the user's goal has been accomplished or not.", "labels": [], "entities": []}, {"text": "For example, in the food ordering system presented by, the dialog system earns higher rewards when it succeeds in taking the order from the user.", "labels": [], "entities": []}, {"text": "Recently, there has been an increasing amount of research on applying RL to negotiation dialog domains, which are generally more complex than slot-filling dialog because the system needs to consider its own goal as well as the user's goal, and may need to keep track of more information, e.g., what has been accepted or rejected so far, proposals and arguments on the table, etc.", "labels": [], "entities": []}, {"text": "Georgila and Traum (2011) applied RL to the problem of learning negotiation dialog system policies for different cultural norms (individualists, collectivists, and altruists).", "labels": [], "entities": [{"text": "RL", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9795397520065308}]}, {"text": "The domain was negotiation between a florist and a grocer who had to agree on the temperature of a shared retail space.", "labels": [], "entities": []}, {"text": "used RL to learn the dialog system policy in a two-issue negotiation domain where two participants (the user and the system) organize a party, and need to decide on both the day that the party will take place and the type of food that will be served.", "labels": [], "entities": []}, {"text": "Also, Heeman (2009) modeled negotiation dialog fora furniture layout task, and modeled negotiation dialog between a seller and buyer.", "labels": [], "entities": [{"text": "negotiation dialog between a seller and buyer", "start_pos": 87, "end_pos": 132, "type": "TASK", "confidence": 0.8349218453679766}]}, {"text": "More recently, Efstathiou and Lemon (2014) focused on non-cooperative aspects of trading dialog, and used multi-agent RL to learn negotiation policies in a resource allocation scenario.", "labels": [], "entities": []}, {"text": "Finally, applied RL to the problem of learning cooperative persuasive policies using framing, and learned models for cultural decision-making in a simple negotiation game (the Ultimatum Game).", "labels": [], "entities": [{"text": "RL", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9695591330528259}]}, {"text": "In contrast to typical slot-filling dialog systems, in these negotiation dialogs, the dialog system is rewarded based on the achievement of its own goals rather than those of its interlocutor.", "labels": [], "entities": []}, {"text": "For example, in Georgila (2013), the dialog system gets a higher reward when its party plan is accepted by the other participant.", "labels": [], "entities": [{"text": "Georgila (2013)", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.5353893041610718}]}, {"text": "Note that in all of the previous work mentioned above, the focus was on negotiation dialog between two participants only, ignoring cases where negotiation takes place between more than two interlocutors.", "labels": [], "entities": [{"text": "negotiation dialog", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9238246977329254}]}, {"text": "However, in the real world, multiparty negotiation is quite common.", "labels": [], "entities": [{"text": "multiparty negotiation", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8563919961452484}]}, {"text": "In this paper, as a first study on multi-party negotiation, we apply RL to a multi-party trading scenario where the dialog system (learner) trades with one, two, or three other agents.", "labels": [], "entities": [{"text": "multi-party negotiation", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7300655543804169}]}, {"text": "We experiment with different RL algorithms and reward functions.", "labels": [], "entities": [{"text": "RL", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9642081260681152}]}, {"text": "The negotiation strategy of the learner is learned through simulated dialog with trader simulators.", "labels": [], "entities": []}, {"text": "In our experiments, we evaluate how the performance of the learner varies depending on the RL algorithm used and the number of traders.", "labels": [], "entities": []}, {"text": "To the best of our knowledge this is the first study that applies RL to multi-party (more than two participants) negotiation dialog management.", "labels": [], "entities": [{"text": "RL", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9850102066993713}, {"text": "negotiation dialog management", "start_pos": 113, "end_pos": 142, "type": "TASK", "confidence": 0.8679210344950358}]}, {"text": "We are not aware of any previous research on dialog using RL to learn the system's policy in multi-party negotiation.", "labels": [], "entities": []}, {"text": "Our paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an introduction to RL.", "labels": [], "entities": [{"text": "RL", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.939556896686554}]}, {"text": "Section 3 describes our multi-party trading domain.", "labels": [], "entities": []}, {"text": "Section 4 describes the dialog state and set of actions for both the learner and the trader simulators, as well as the reward functions of the learner and the handcrafted policies of the trader simulators.", "labels": [], "entities": []}, {"text": "In Section 5, we present our evaluation methodology and results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 summarizes the paper and proposes future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the learner's policies learned with (1) different algorithms i.e., LinQ, LSPI, and NFQ (see Section 2), (2) different reward functions i.e., Equations 3 and 4 (see Section 4.1), and (3) different numbers of traders.", "labels": [], "entities": [{"text": "LSPI", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.8571124076843262}]}, {"text": "The evaluation is performed in trading dialogs with different numbers of participants (from 2 players to 4 players), and different trader simulator's policies (hand-crafted policy or random policy as presented in Section 4.2).", "labels": [], "entities": []}, {"text": "More specifically, there are 9 different setups: H: 2-party dialog, where the trader simulator follows a hand-crafted policy.", "labels": [], "entities": []}, {"text": "R: 2-party dialog, where the trader simulator follows a random policy.", "labels": [], "entities": []}, {"text": "HxH: 3-party dialog, where both trader simulators follow hand-crafted policies.", "labels": [], "entities": [{"text": "HxH", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9085067510604858}]}, {"text": "HxR: 3-party dialog, where one trader simulator follows a hand-crafted policy and the other one follows a random policy.", "labels": [], "entities": [{"text": "HxR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9118056297302246}]}, {"text": "RxR: 3-party dialog, where both trader simulators follow random policies.", "labels": [], "entities": [{"text": "RxR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7685511708259583}]}, {"text": "HxHxH: 4-party dialog, where all three trader simulators follow hand-crafted policies.", "labels": [], "entities": [{"text": "HxHxH", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9327409863471985}]}, {"text": "HxHxR: 4-party dialog, where two trader simulators follow hand-crafted policies and the other one follows a random policy.", "labels": [], "entities": [{"text": "HxHxR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9295608401298523}]}, {"text": "HxRxR: 4-party dialog, where one trader simulator follows a hand-crafted policy and the other ones follow random policies.", "labels": [], "entities": [{"text": "HxRxR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9270424842834473}]}, {"text": "RxRxR: 4-party dialog, where all three trader simulators follow random policies.", "labels": [], "entities": [{"text": "RxRxR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8409563899040222}]}, {"text": "There are also 9 different learner policies: AlwaysKeep: weak baseline which always passes the turn.", "labels": [], "entities": [{"text": "AlwaysKeep", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.8462347984313965}]}, {"text": "Random: weak baseline which randomly selects one action from all possible valid actions.", "labels": [], "entities": []}, {"text": "LinQ-End: learned policy using LinQ and reward given at the end of the dialog.", "labels": [], "entities": [{"text": "LinQ-End", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.87899249792099}, {"text": "LinQ", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8787732720375061}]}, {"text": "LSPI-End: learned policy using LSPI and reward given at the end of the dialog.", "labels": [], "entities": []}, {"text": "NFQ-End: learned policy using NFQ and reward given at the end of the dialog.", "labels": [], "entities": [{"text": "NFQ-End", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8955718874931335}]}, {"text": "LinQ-Incr: learned policy using LinQ and an incremental reward.", "labels": [], "entities": [{"text": "LinQ-Incr", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.942143976688385}, {"text": "LinQ", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9219805598258972}]}, {"text": "LSPI-Incr: learned policy using LSPI and an incremental reward.", "labels": [], "entities": []}, {"text": "NFQ-Incr: learned policy using NFQ and an incremental reward.", "labels": [], "entities": [{"text": "NFQ-Incr", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9409511685371399}]}, {"text": "Handcraft1: strong baseline following the handcrafted policy presented in Section 4.2.", "labels": [], "entities": []}, {"text": "Handcraft2: strong baseline similar to Handcraft1 except the plan is randomly selected from the set of plans produced by step 6, rather than picking only the highest utility one (see Section 4.2).", "labels": [], "entities": [{"text": "Handcraft2", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9033029079437256}]}, {"text": "We use the Pybrain library ( for the RL algorithms LinQ, LSPI, and NFQ.", "labels": [], "entities": [{"text": "Pybrain library", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9309572577476501}]}, {"text": "The learning parameters follow the default Pybrain settings except for the discount factor \u03b3; we set the discount factor \u03b3 to 1.", "labels": [], "entities": []}, {"text": "We consider 2000 dialogs as one epoch, and learning is finished when the number of epochs becomes 200 (400,000 dialogs).", "labels": [], "entities": []}, {"text": "The policy at the epoch where the average reward reaches its highest value is used in the evaluation.", "labels": [], "entities": []}, {"text": "We evaluate the learner's policy against trader simulators.", "labels": [], "entities": []}, {"text": "We calculate the average reward of the learner's policy in 20000 dialogs.", "labels": [], "entities": []}, {"text": "Furthermore, we show how fast the learned policies converge as a function of the number of epochs in training.", "labels": [], "entities": []}, {"text": "In terms of comparing the average rewards of policies (see), NFQ-Incr achieves the best performance in almost every situation.", "labels": [], "entities": []}, {"text": "In 2-party trading, the performance of NFQ-Incr is almost the same as that of Handcraft2 which achieves the best score, and better than the performance of Handcraft1.", "labels": [], "entities": [{"text": "NFQ-Incr", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8265878558158875}, {"text": "Handcraft2", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9168868660926819}, {"text": "Handcraft1", "start_pos": 155, "end_pos": 165, "type": "DATASET", "confidence": 0.9570724368095398}]}, {"text": "In both 3-party and 4-party trading, the performance of NFQ-Incr is better than that of the two strong baselines, and achieves the best score.", "labels": [], "entities": [{"text": "NFQ-Incr", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8289945125579834}]}, {"text": "In contrast to NFQ-Incr, the performance of the other learned policies is much worse than that of the two strong baselines.", "labels": [], "entities": [{"text": "NFQ-Incr", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.9352125525474548}]}, {"text": "As the number of trader simulators who follow a random policy increases, the difference in performance between NFQ-Incr and the other learned policies tends to also increase.", "labels": [], "entities": []}, {"text": "One reason is that, as the number of trader simulators who follow a random policy increases, the variability of dialog flow also increases.", "labels": [], "entities": []}, {"text": "Trader simulators that follow a hand-crafted policy behave more strictly than trader simulators that follow a random policy.", "labels": [], "entities": []}, {"text": "For example, if the trader simulator following a hand-crafted policy reaches its goal, then there is nothing else to do except for Keep.", "labels": [], "entities": [{"text": "Keep", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9933544397354126}]}, {"text": "In contrast, if a trader simulator following a random policy reaches its goal, there is still a chance that it will accept an offer which will be beneficial to the learner.", "labels": [], "entities": []}, {"text": "As a result there are more chances for the learner to gain better outcomes, when the complexity of the dialog is higher.", "labels": [], "entities": []}, {"text": "In summary, our results show that combining NFQ with an incremental reward produces the best results.", "labels": [], "entities": []}, {"text": "Moreover, the learning curve in 2-party trading ( in the Appendix) indicates that, basically, only the NFQ-Incr achieves stable learning.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8599948883056641}, {"text": "NFQ-Incr", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.8784400820732117}]}, {"text": "NFQ-Incr reaches its best performance from epoch 140 to epoch 190.", "labels": [], "entities": [{"text": "NFQ-Incr", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.962966799736023}]}, {"text": "On the other hand, LSPI somehow converges fast, but its performance is not so high.", "labels": [], "entities": [{"text": "LSPI", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.7909169793128967}]}, {"text": "Moreover, LinQ converges in the first epoch, but it performs the worst.", "labels": [], "entities": []}], "tableCaptions": []}