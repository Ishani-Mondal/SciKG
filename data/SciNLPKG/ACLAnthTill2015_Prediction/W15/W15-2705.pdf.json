{"title": [{"text": "Semantically Enriched Models for Modal Sense Classification", "labels": [], "entities": [{"text": "Modal Sense Classification", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.7050904333591461}]}], "abstractContent": [{"text": "Modal verbs have different interpretations depending on their context.", "labels": [], "entities": []}, {"text": "Previous approaches to modal sense classification achieve relatively high performance using shallow lexical and syntactic features.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.730995794137319}]}, {"text": "In this work we uncover the difficulty of particular modal sense distinctions by eliminating both distributional bias and sparsity of existing small-scale annotated corpora used in prior work.", "labels": [], "entities": []}, {"text": "We build a semantically enriched model for modal sense classification by novelly applying features that relate to lexical, proposition-level, and discourse-level semantic factors.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6838578581809998}]}, {"text": "Besides improved classification performance, especially for difficult sense distinctions, closer examination of interpretable feature sets allows us to obtain a better understanding of relevant semantic and contex-tual factors in modal sense classification.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 230, "end_pos": 256, "type": "TASK", "confidence": 0.6732442677021027}]}], "introductionContent": [{"text": "Factuality recognition is an important subtask in information extraction.", "labels": [], "entities": [{"text": "Factuality recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9257957637310028}, {"text": "information extraction", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.8807639479637146}]}, {"text": "Beyond bare filtering aspects of veridicality recognition, classification of modal senses plays an important role in text understanding, plan recognition, and the emerging field of argumentation mining.", "labels": [], "entities": [{"text": "veridicality recognition", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.730403795838356}, {"text": "text understanding", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.8330090045928955}, {"text": "plan recognition", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.8294756412506104}, {"text": "argumentation mining", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.8874296844005585}]}, {"text": "Communication revolves about hypothetical, planned, apprehended or desired states of affairs.", "labels": [], "entities": []}, {"text": "Such 'extrapropositional' meanings are often linguistically marked using modal verbs, adverbs, or attitude verbs, as in (1) for hypothetical situations.", "labels": [], "entities": []}, {"text": "He must've hurt himself. b. He has certainly found the place by now. c. We anticipate that no one will leave.", "labels": [], "entities": []}, {"text": "Following's seminal work informal semantics, recent computational approaches such as distinguish different modal 'senses', most prominently, epistemic (2.a), deontic/bouletic (2.b) and circumstantial/dynamic (2.c) modality.", "labels": [], "entities": []}, {"text": "Geez, Buddha must be so annoyed!", "labels": [], "entities": []}, {"text": "(epistemic -possibility) b.", "labels": [], "entities": []}, {"text": "We must have clear European standards.", "labels": [], "entities": []}, {"text": "(deontic -permission/request) c.", "labels": [], "entities": []}, {"text": "She can't even read them.", "labels": [], "entities": []}, {"text": "(dynamic -ability) Modal sense tagging is typically framed as a supervised classification task, as in, who manually annotated the modal verbs must, may, can, could, shall and should in the MPQA corpus of.", "labels": [], "entities": [{"text": "Modal sense tagging", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7484920223553976}, {"text": "MPQA corpus", "start_pos": 189, "end_pos": 200, "type": "DATASET", "confidence": 0.9561625123023987}]}, {"text": "The obtained data set comprises 1340 instances.", "labels": [], "entities": []}, {"text": "Maximum entropy classifiers trained on this data yield accuracies from 68.7 to 93.5 for the different lexical classifier models.", "labels": [], "entities": []}, {"text": "While these accuracies seem high, we note a strong distributional bias in their data set.", "labels": [], "entities": []}, {"text": "Due to the small data set size (200-600 instances per modal verb) and its distributional bias, classifiers trained on this corpus are prone to overfitting and hardly beat the majority baseline.", "labels": [], "entities": []}, {"text": "Indeed, none of the classification models in Ruppenhofer and Rehbein (2012) (henceforth R&R) is able to beat the baseline with uniform settings across all modal verb types.", "labels": [], "entities": []}, {"text": "Of particular concern in our work are specific sense ambiguities that are difficult to discriminate, such as dynamic vs. deontic readings of can (3.a), epistemic vs. dynamic readings of could (3.b) or epistemic vs. deontic readings of should (3.c).", "labels": [], "entities": []}, {"text": "In this paper we reexamine prior work on modal sense classification and show that specific distinctions are difficult for state-of-the art models.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.6434970796108246}]}, {"text": "We show that modal sense classification is a challenging problem that profits from lexical, propositionlevel and discourse-level semantic information.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6686629255612692}]}, {"text": "Our goals and contributions are as follows: (i) We investigate the impact of semantic and discourse-related factors for modal sense classification, looking in particular at difficult modal sense distinctions.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 120, "end_pos": 146, "type": "TASK", "confidence": 0.63008580605189}]}, {"text": "Accordingly, we define a range of semantically inspired linguistic feature classes.", "labels": [], "entities": []}, {"text": "The feature groups are related to lexical and propositional semantics, as well as discourselevel semantics, ranging from tense and aspect to speaker/hearer orientation.", "labels": [], "entities": []}, {"text": "As an example, one of our hypotheses is that aspectual event types play a decisive role in deontic vs. epistemic sense disambiguation for modal verbs such as must.", "labels": [], "entities": [{"text": "deontic vs. epistemic sense disambiguation", "start_pos": 91, "end_pos": 133, "type": "TASK", "confidence": 0.6884092450141907}]}, {"text": "Our intuition is that events are more likely to co-occur with the deontic sense of must (4.a,b), whereas statives are more likely to co-occur with the epistemic sense (4.c).", "labels": [], "entities": []}, {"text": "The prisoners must return their weapons. b. Prisoners of war must be returned to their home countries. c. They must be so scared.", "labels": [], "entities": []}, {"text": "(ii) As a precondition for the aims of this work, we construct a large corpus that is balanced for modal sense distribution and less prone to overfitting compared to prior work.", "labels": [], "entities": []}, {"text": "To this end we apply a paraphrase-driven cross-lingual modal sense projection approach using parallel corpora.", "labels": [], "entities": [{"text": "cross-lingual modal sense projection", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.5920348316431046}]}, {"text": "We show that this automatic acquisition method yields modal sense annotations of very high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9811912178993225}]}, {"text": "(iii) Using this corpus as training data, we devise a novel, semantically enriched model for modal sense classification.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6258648037910461}]}, {"text": "We assess the impact of diverse feature groups for modal sense classification in unbiased classification settings and analyze to what extent they contribute to solving difficult disambiguation problems.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.6809617479642233}]}, {"text": "We review related work in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 outlines an automatic modal sense projection approach using parallel corpora.", "labels": [], "entities": [{"text": "modal sense projection", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.718930701414744}]}, {"text": "We apply this method to bilingual corpora and evaluate the quality of the obtained data set.", "labels": [], "entities": []}, {"text": "Section 4 motivates and describes semantic and discourseoriented features for modal sense classification.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.6595364809036255}]}, {"text": "These are examined in classification experiments in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.8574157953262329}]}, {"text": "We reconstruct the modal sense classifier of to compare against prior work.", "labels": [], "entities": []}, {"text": "We evaluate the performance of different models in unbiased classification experiments, using the harvested senselabeled corpora for training.", "labels": [], "entities": []}, {"text": "We analyze the impact of different feature groups on disambiguation performance and relate them to specific difficult disambiguation classes.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.9777014255523682}]}], "datasetContent": [{"text": "Our experiments have several objectives: (i.)", "labels": [], "entities": []}, {"text": "We aim to show that modal sense classification, especially difficult sense distinctions, can profit from semantic and discourse-oriented features.", "labels": [], "entities": [{"text": "modal sense classification", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.7399541735649109}]}, {"text": "To this end we construct contrasting classifier models with different feature sets: R&R's shallow lexical and syntactic path features (F R&R ), a feature set consisting of only our newly designed semantic features (F Sem ), and a combined set F all consisting of both F R&R and F Sem . However, any classifier trained only on the highly unbalanced MPQA data set will have difficulty separating the effect of distributional bias in the training data from the predictive force of its feature set.", "labels": [], "entities": [{"text": "MPQA data set", "start_pos": 348, "end_pos": 361, "type": "DATASET", "confidence": 0.9512443741162618}]}, {"text": "A classifier that follows the majority class in the training data will neutralize the potential impact of its feature set.", "labels": [], "entities": []}, {"text": "In order to counterbalance the distributional bias and also the sparsity inherent in the data, we evaluate the different classifier models in different classification settings: (ii.)", "labels": [], "entities": []}, {"text": "We extend the training set using heuristically labeled instances obtained from modal sense projection (cf. Section 3), thereby eliminating sparsity and reducing distributional bias.", "labels": [], "entities": []}, {"text": "We further evaluate classifiers trained on perfectly balanced data.", "labels": [], "entities": []}, {"text": "This eliminates the distributional bias in training and will allow us to carve out the impact of the different feature sets.", "labels": [], "entities": []}, {"text": "(iv.) Finally we measure the impact of individual feature groups via ablation (Section 5.3).", "labels": [], "entities": [{"text": "ablation", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.973259449005127}]}, {"text": "A note on notation: Subscripts on classifier names indicate the source of the training data.", "labels": [], "entities": []}, {"text": "CL M denotes a classifier trained only on MPQA data; CL M H combines MPQA and heuristicallytagged data; CL H is a classifier trained only on heuristically-tagged data.", "labels": [], "entities": [{"text": "MPQA data", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.8701587021350861}]}, {"text": "Superscripted +b or \u2212b indicates a balanced vs. unbalanced training set.", "labels": [], "entities": []}, {"text": "Replicating R&R's modal sense classifier.", "labels": [], "entities": [{"text": "R&R's modal sense classifier", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.779331386089325}]}, {"text": "We replicate R&R's classifier by reimplementing their feature set, 4 a mixture of target and contextual features that take into account surface, lemma and PoS information, as well as syntactic labels and path features linking targets to surrounding words and constituents (cf. R&R,).", "labels": [], "entities": []}, {"text": "We train one classifier per modal verb, using R&R's best feature setting (context feature window=3 tokens left and right of target, target-specific features).", "labels": [], "entities": []}, {"text": "Averaged accuracies for the replicated classifiers appear in as CL \u2212b M (feature set F R&R ).", "labels": [], "entities": []}, {"text": "Our scores are very similar to their published results, which appear in the same table in the column headed \"R&R\".", "labels": [], "entities": [{"text": "R&R", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.7164644797643026}]}, {"text": "5 Extending and balancing training data sets.", "labels": [], "entities": []}, {"text": "From the 11,610 heuristically sense tagged instances (Section 3), we construct balanced (+b) training corpora for each modal verb.", "labels": [], "entities": []}, {"text": "The composition of this data is shown in.", "labels": [], "entities": []}, {"text": "To alleviate training data sparsity, we add this data to the (unbalanced) MPQA data; this configuration results in CL \u2212b M H . Finally, we re-balance both CL M and CL M H by under-and oversampling.", "labels": [], "entities": [{"text": "MPQA data", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.9535375535488129}]}, {"text": "Classification setup and test data.", "labels": [], "entities": [{"text": "Classification setup", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9762350022792816}]}, {"text": "Training on balanced data reduces distributional bias, but evaluating performance on an unbalanced, naturallydistributed data set gives us a more realistic picture.", "labels": [], "entities": []}, {"text": "To this end, and in order to compare to prior work, our test data is drawn exclusively from MPQA.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.9729613661766052}]}, {"text": "For CL +b H , we evaluate on R&R's full data set; the composition of the test set appears in the Following R&R we use the Stanford parser for processing and induce maximum entropy models using OpenNLP with default parameter settings.", "labels": [], "entities": [{"text": "R&R's full data set", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.614194610289165}, {"text": "OpenNLP", "start_pos": 193, "end_pos": 200, "type": "DATASET", "confidence": 0.9342878460884094}]}, {"text": "5 R&R performed 10-fold cross-validation (CV) for evaluation.", "labels": [], "entities": [{"text": "cross-validation (CV)", "start_pos": 24, "end_pos": 45, "type": "METRIC", "confidence": 0.7764499187469482}]}, {"text": "We perform 5-fold cross-validation instead.", "labels": [], "entities": []}, {"text": "When doing oversampling, we generally perform a mixture of over-and undersampling, targeting about half the size of the larger class.", "labels": [], "entities": []}, {"text": "The data sets are available at http: //projects.cl.uni-heidelberg.de/modals.", "labels": [], "entities": []}, {"text": "For unbalanced classifiers, we compare to the MFS baseline (BL Maj M ), taking the most frequent sense for each modal verb from the MPQA training data.", "labels": [], "entities": [{"text": "MFS baseline", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.6908415853977203}, {"text": "BL Maj M )", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9113497883081436}, {"text": "MPQA training data", "start_pos": 132, "end_pos": 150, "type": "DATASET", "confidence": 0.8814912438392639}]}, {"text": "For balanced classifiers, we compare to the random baseline (BL Ran ), determined by the (evenly distributed) number of class labels seen in training for each modal verb.", "labels": [], "entities": [{"text": "BL Ran )", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9121529261271158}]}, {"text": "compares accuracy of classifiers trained on \u00b1balanced data, from different sources, and with different feature sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9992272853851318}]}, {"text": "We report results for individual classifiers (per modal verb) and macroand micro-average across all verbs.", "labels": [], "entities": []}, {"text": "The two boldfaced numbers per table row indicate the best models for unbalanced and for balanced data.", "labels": [], "entities": []}, {"text": "For the balanced classifiers, where we find more interesting differences, we test significance using McNemar's test (p<0.05).", "labels": [], "entities": [{"text": "significance", "start_pos": 82, "end_pos": 94, "type": "METRIC", "confidence": 0.9491447806358337}, {"text": "McNemar's test", "start_pos": 101, "end_pos": 115, "type": "METRIC", "confidence": 0.5371616880098978}]}, {"text": "Within a row (for +b classifiers and micro-averages), a superscript on a number indicates which classifier is significantly outperformed by the result.", "labels": [], "entities": []}, {"text": "Across feature sets, we compare micro-averages and mark significance by subscripts (R=F R&R , S=F Sem ).", "labels": [], "entities": [{"text": "mark significance", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.7609498798847198}, {"text": "F R&R", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.7468327730894089}]}, {"text": "We first discuss the classifiers trained on unbalanced data.", "labels": [], "entities": []}, {"text": "With F R&R , CL \u2212b M yields performance comparable to R&R's results, at 84.44% accuracy, 1.02pp below the majority baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9952408075332642}]}, {"text": "Individual lexical classifiers also approach R&R's performance, though never beating the baseline.", "labels": [], "entities": [{"text": "R&R", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.7664288679758707}]}, {"text": "7: Cross-validation, one run: representative class distributions of training and test data.", "labels": [], "entities": []}, {"text": "Changing from F R&R to F Sem and F All , classifier CL \u2212b M for could is now able to beat the baseline.", "labels": [], "entities": []}, {"text": "The effect is stronger for F Sem , which reflects the impact of the semantic features.", "labels": [], "entities": []}, {"text": "Interestingly, accuracy of F Sem is comparable to F R&R , even though the classifiers learn only on the basis of semantic features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994933605194092}, {"text": "F R&R", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.6450386941432953}]}, {"text": "Combining the two feature sets (F All ) produces minimal differences for CL \u2212b M , but yields stronger gains for CL \u2212b M H . may/might and shall/should.", "labels": [], "entities": []}, {"text": "The addition of heuristically-tagged data in CL \u2212b M H helps for some verbs, but hurts for others.", "labels": [], "entities": []}, {"text": "Despite the larger training set size, individual classifier performances tend to drop, meaning they do not profit much from the reduced training bias.", "labels": [], "entities": []}, {"text": "For classifiers trained on balanced data, the picture changes.", "labels": [], "entities": []}, {"text": "Accuracies on balanced data are lower, reflecting the lack of distributional bias.", "labels": [], "entities": []}, {"text": "But all results are well above the random BL.", "labels": [], "entities": [{"text": "BL", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9126744270324707}]}, {"text": "8 8 All comparisons to the random baseline are significant Compared to CL +b M and CL +b H , we observe the best results for CL +b M H , which mixes MPQA and out-of-domain data.", "labels": [], "entities": []}, {"text": "Here, the best performance is obtained with F All . In fact, CL +b M H with 83.12% on balanced mixed data closely approaches the performance of the classifiers trained on biased training data and their majority baseline, with about 2pp difference, and being almost identical to R&R's published results.", "labels": [], "entities": []}, {"text": "Looking at individual modal classifiers, we see even more interesting results.", "labels": [], "entities": []}, {"text": "can and could, both with 3-fold sense distinctions and lowest performance overall, suffer the greatest loss in the balanced setting, in ranges of 41-57% for F R&R . These verbs are hard to classify, and here we see a marked performance rise as the training data changes (from CL +b M to CL +b H ), though these differences are not significant.", "labels": [], "entities": []}, {"text": "Comparing F Sem to F R&R , we obtain better results overall, always above 50% accuracy.", "labels": [], "entities": [{"text": "F R&R", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.7323217391967773}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9989293217658997}]}, {"text": "With F All we reach a range of 54-63%, achieving strong gains of more than +20pp for could, and about +5pp for can.", "labels": [], "entities": [{"text": "F", "start_pos": 5, "end_pos": 6, "type": "METRIC", "confidence": 0.9444828629493713}]}, {"text": "We also note an almost continous rise for should with a final +5pp gain over F R&R . Across different feature sets, CL +b M H performs best, that is, combining MPQA and out-of-domain data is effective.", "labels": [], "entities": []}, {"text": "To summarize, with increasingly refined models and a tendency of CL M H and CL H outperforming CL M , we obtain a coherent picture: semantic features contribute important information and reach their best performance with a mixture of training sets.", "labels": [], "entities": []}, {"text": "We also note that F Sem and F All jointly yield significant gains over F R&R for could, must, should, can and may.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Cross-validation, one run: representative class distributions of training and test data.", "labels": [], "entities": []}, {"text": " Table 4: Classifier accuracy for various training data and feature sets. See text for details.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.979943573474884}]}, {"text": " Table 5: Confusion analysis: CL +b  H using F R&R", "labels": [], "entities": [{"text": "Confusion analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9543152153491974}, {"text": "F R&R", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.8746409118175507}]}]}