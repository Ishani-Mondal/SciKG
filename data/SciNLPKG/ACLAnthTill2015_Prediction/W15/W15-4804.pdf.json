{"title": [{"text": "Count-based State Merging for Probabilistic Regular Tree Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach to obtain language models from a tree corpus using proba-bilistic regular tree grammars (prtg).", "labels": [], "entities": []}, {"text": "Starting with a prtg only generating trees from the corpus, the prtg is generalized step by step by merging nonterminals.", "labels": [], "entities": []}, {"text": "We focus on bottom-up deterministic prtg to simplify the calculations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Constituent parsing plays an important role in natural language processing (nlp).", "labels": [], "entities": [{"text": "Constituent parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7919538617134094}, {"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6608753005663554}]}, {"text": "One can easily read off a pcfg from a tree corpus and use it for parsing.", "labels": [], "entities": []}, {"text": "This might work quite well, but it can be even more fruitful to introduce a state behaviour that is not visible in the corpus (.", "labels": [], "entities": []}, {"text": "The ExpectationMaximization Algorithm can be used to train probabilities if the state behaviour is fixed ().", "labels": [], "entities": []}, {"text": "This can be improved by adapting the state behaviour automatically by cleverly splitting and merging states).", "labels": [], "entities": []}, {"text": "More generally, finding a grammar by examining terminal objects is one of the problems investigated in the field of grammatical inference.", "labels": [], "entities": [{"text": "grammatical inference", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.7547951638698578}]}, {"text": "There are many results for the string case, e.g., on how to learn deterministic stochastic finite (string) automata from text ().", "labels": [], "entities": []}, {"text": "For the tree case, there are, e.g., results for identifying function distinguishable regular tree languages from text).", "labels": [], "entities": []}, {"text": "There is also a generalization of n-grams to trees including smoothing techniques ().", "labels": [], "entities": []}, {"text": "The mentioned results for deterministic stochastic finite (string) automata were generalized to an algorithm that learns stochastic deterministic tree automata from trees ().", "labels": [], "entities": []}, {"text": "Given a tree corpus, this approach yields a single grammar.", "labels": [], "entities": []}, {"text": "The authors experimentally showed that if the corpus is too small, the grammar tends to be too general.", "labels": [], "entities": []}, {"text": "In contrast, the split-merge approach of produces a sequence of different grammars.", "labels": [], "entities": []}, {"text": "One can use cross validation to select a grammar from that sequence that suitably abstracts away from the training corpus.", "labels": [], "entities": []}, {"text": "Because of the intricate combination of splitting and merging however, the behavior is very difficult to analyse theoretically.", "labels": [], "entities": []}, {"text": "Our approach is similar to the split-merge procedure in that a sequence of grammars is obtained.", "labels": [], "entities": []}, {"text": "However, it differs by operating without splitting and relying on merging alone, thereby obtaining a simpler framework.", "labels": [], "entities": []}, {"text": "Our goal is to create a sequence of probabilistic regular tree grammars (prtg) from a corpus such that every prtg abstracts away from the corpus more than its predecessors in the sequence (cf. Algorithm 1).", "labels": [], "entities": []}, {"text": "We start with a prtg that admits no more and no less than the trees in the corpus.", "labels": [], "entities": []}, {"text": "The rules of the prtg are then changed step by step to make the grammar more general.", "labels": [], "entities": []}, {"text": "We generalize a prtg by merging nonterminals, which means we replace several nonterminals by a single new one.", "labels": [], "entities": []}, {"text": "The weights of the resulting prtg are assigned by maximum likelihood estimation on the corpus.", "labels": [], "entities": []}, {"text": "To make the approach easier, we only consider bottom-up deterministic prtg.", "labels": [], "entities": []}, {"text": "On the one hand, this simplifies our calculations, e.g., the maximum likelihood estimate; on the other hand, this simplifies the application of the prtg as language model, since the search for the most probable tree fora given yield does not have to consider several derivations fora singletree.", "labels": [], "entities": [{"text": "maximum likelihood estimate", "start_pos": 61, "end_pos": 88, "type": "METRIC", "confidence": 0.721859335899353}]}], "datasetContent": [], "tableCaptions": []}