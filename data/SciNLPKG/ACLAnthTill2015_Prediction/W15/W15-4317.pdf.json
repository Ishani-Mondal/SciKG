{"title": [{"text": "NCSU_SAS_WOOKHEE: A Deep Contextual Long-Short Term Memory Model for Text Normalization", "labels": [], "entities": [{"text": "Text Normalization", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8009028136730194}]}], "abstractContent": [{"text": "To address the challenges of normalizing online conversational texts prevalent in social media, we propose a contextual long-short term memory (LSTM) recurrent neural network based approach, augmented with a self-generated dictionary normalization technique.", "labels": [], "entities": [{"text": "normalizing online conversational texts", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.8048980534076691}]}, {"text": "Our approach utilizes a sequence of characters as well as the part-of-speech associated with words without harnessing any external lexical resources.", "labels": [], "entities": []}, {"text": "This work is evaluated on the English Tweet data set provided by the ACL 2015 W-NUT Normal-ization of Noisy Text shared task.", "labels": [], "entities": [{"text": "English Tweet data set", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.7400680929422379}, {"text": "ACL 2015 W-NUT Normal-ization of Noisy Text shared task", "start_pos": 69, "end_pos": 124, "type": "DATASET", "confidence": 0.8656975362035964}]}, {"text": "The results , by achieving second place (F1 score: 81.75%) in the constrained track of the competition, indicate that the proposed LSTM-based approach is a promising tool for normalizing non-standard language.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9881761074066162}]}], "introductionContent": [{"text": "Recent years have seen increasing use of online social media such as Twitter and Facebook that has generated a growing body of text where nonstandard language is prevalent.", "labels": [], "entities": []}, {"text": "These nonstandard lexical items take many different forms, including unintentional errors based on users' cognitive misconceptions and typographical errors, and intentional non-canonical language such as abbreviations, word lengthening by duplication of characters, Internet slang, phonetic substitutions, and creative use of language.", "labels": [], "entities": [{"text": "word lengthening", "start_pos": 219, "end_pos": 235, "type": "TASK", "confidence": 0.7048828154802322}]}, {"text": "A key challenge posed by these non-standard texts is the negative impact on traditional natural language processing (NLP) pipeline processes, evidenced by noticeable underperformance of their predictive accuracy in various domains such as part-of-speech tagging () and named entity recognition () compared to more standard text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.8708290457725525}, {"text": "part-of-speech tagging", "start_pos": 239, "end_pos": 261, "type": "TASK", "confidence": 0.7121514827013016}, {"text": "named entity recognition", "start_pos": 269, "end_pos": 293, "type": "TASK", "confidence": 0.6652407149473826}]}, {"text": "As an approach to addressing this challenge, text normalization techniques have been widely investigated, ranging from extracting domain specific lexical variants), unified letter transformation (, dictionary based methods using string substitution () in an unsupervised manner, to character-level edit operation predictions utilizing conditional random fields in a supervised manner.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7740720808506012}, {"text": "unified letter transformation", "start_pos": 165, "end_pos": 194, "type": "TASK", "confidence": 0.647727757692337}]}, {"text": "Because language data consists of sequential information, such as streams of characters and sequences of words, many NLP approaches leverage computational models that can effectively deal with temporal data, such as hidden Markov models and conditional random fields).", "labels": [], "entities": []}, {"text": "More recently, deep learning models (e.g., multi-layer feedforward neural networks, recurrent neural networks, recursive neural networks) have been used in NLP to achieve state-of-the-art performance in areas such as speech recognition) and sentiment analysis).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 217, "end_pos": 235, "type": "TASK", "confidence": 0.8120521306991577}, {"text": "sentiment analysis", "start_pos": 241, "end_pos": 259, "type": "TASK", "confidence": 0.9652518928050995}]}, {"text": "The success of deep learning has been attainable with the emergence of effective training methods for deep networks, such as pretraining () and optimization techniques) that significantly diminish problems associated with vanishing and exploding gradient that are often observed in multi-layer neural network training.", "labels": [], "entities": []}, {"text": "In this work, we leverage long-short term memory models (LSTMs), a variant of recurrent neural networks, to conduct text normalization on the data set given from the W-NUT English lexical normalization shared task (.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7513889372348785}, {"text": "W-NUT English lexical normalization shared task", "start_pos": 166, "end_pos": 213, "type": "TASK", "confidence": 0.6801973978678385}]}, {"text": "We additionally harness the part-of-speech tagger created by Noah's Ark research team (), a free resource to the constrained task.", "labels": [], "entities": []}, {"text": "Similar to Chrupa\u0142a's work that predicts Levenshtein edit operations between canonical and non-canonical forms of words, this proposed approach predicts word-level edit operations based on character-level inputs.", "labels": [], "entities": [{"text": "predicts Levenshtein edit operations between canonical and non-canonical forms of words", "start_pos": 32, "end_pos": 119, "type": "TASK", "confidence": 0.744815932078795}]}], "datasetContent": [{"text": "Before submitting our test set result to the W-NUT English lexical normalization shared task, we ran a 5-fold cross validation on the training set to evaluate the proposed approach.", "labels": [], "entities": [{"text": "W-NUT English lexical normalization shared task", "start_pos": 45, "end_pos": 92, "type": "TASK", "confidence": 0.6728837142388026}]}, {"text": "To conduct the experiment, we split the training set into 5 partitions based on a Tweet-level separation, and trained an LSTM model, iteratively using 4 out of the 5 partitions in each fold.", "labels": [], "entities": []}, {"text": "In the first evaluation, we examine two variations of our approach to measure the impact of dictionary-based normalization as an intermediate step: applying phase 1 and phase 3, in which we do not leverage dictionary-based normalization but predict labels based on an LSTM model after atmention and hash-tag and URL filtering, and (2) applying all three phases.", "labels": [], "entities": []}, {"text": "The evaluation is conducted on contextual models that take three word inputs.", "labels": [], "entities": []}, {"text": "describes the result of these two approaches for each fold.", "labels": [], "entities": []}, {"text": "For pairwise comparison of the two approaches, we conduct a Wilcoxon signed-rank test on F1 rates.", "labels": [], "entities": [{"text": "F1 rates", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9722941815853119}]}, {"text": "The result indicates that there is a statistically significant improvement in F1 rates (79.19% by achieving 5.4% marginal improvement) for \"with dictionary normalization\" over \"without dictionary normalization\" (Z=-2.023, p=0.043).", "labels": [], "entities": [{"text": "F1 rates", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.977079451084137}]}, {"text": "summarizes the comparison on the two approaches enriched with the dictionary normalization.", "labels": [], "entities": [{"text": "dictionary normalization", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.5917173475027084}]}, {"text": "The contextual model outperforms the non-contextual model in terms of the F1 score, but the difference does not elicit a statistically significant difference.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9859717190265656}]}, {"text": "To construct a final model for the test set prediction, we utilize an ensemble method on contextual LSTM models with dictionary normalization.", "labels": [], "entities": []}, {"text": "Given a test set, we calculate the prediction probability from each of the 5 models induced from the five-fold cross validation, multiply the probability values from the softmax layer, and choose the label with the highest resulting probability.", "labels": [], "entities": []}, {"text": "In the evaluation through the W-NUT competition, this approach (NCSU_SAS_WOOKHEE.cm) achieved a precision score of 91.36%, recall score of 73.98%, and F1 score of 81.75%, placing second in the constrained text normalization track.", "labels": [], "entities": [{"text": "NCSU_SAS_WOOKHEE.cm)", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.661043698589007}, {"text": "precision score", "start_pos": 96, "end_pos": 111, "type": "METRIC", "confidence": 0.9859489500522614}, {"text": "recall score", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.9909236431121826}, {"text": "F1 score", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9922961890697479}, {"text": "text normalization", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.7395696043968201}]}], "tableCaptions": [{"text": " Table 1: 5-fold cross validation results of LSTMs without dictionary normalization and with dic- tionary normalization.", "labels": [], "entities": []}, {"text": " Table 2: 5-fold cross validation results of LSTMs: non-contextual model vs. contextual model.", "labels": [], "entities": []}]}