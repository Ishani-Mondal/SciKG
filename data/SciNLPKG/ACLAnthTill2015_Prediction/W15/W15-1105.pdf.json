{"title": [{"text": "Fusion of Compositional Network-based and Lexical Function Distributional Semantic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional Semantic Models (DSMs) have been successful at modeling the meaning of individual words, with interest recently shifting to compositional structures, i.e., phrases and sentences.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7195241898298264}]}, {"text": "Network-based DSMs represent and handle semantics via operators applied on word neighborhoods, i.e., semantic graphs containing a target's most similar words.", "labels": [], "entities": []}, {"text": "We extend network-based DSMs to address compositionality using an activation model (motivated by psycholinguistics) that operates on the fused neighborhoods of variable size activation.", "labels": [], "entities": []}, {"text": "The proposed method is evaluated against and combined with the lexical function method proposed by (Baroni and Zamparelli, 2010).", "labels": [], "entities": []}, {"text": "We show that, by fusing a network-based with a lexical function model, performance gains can be achieved.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector Space Models (VSMs) have proven their efficiency at representing word semantics, which are vital components for numerous natural language applications, such as paraphrasing and textual entailment (, affective text analysis (, etc.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.6511978954076767}, {"text": "affective text analysis", "start_pos": 206, "end_pos": 229, "type": "TASK", "confidence": 0.7030320962270101}]}, {"text": "VSMs constitute the most-widely used implementation of Distributional Semantic Models (DSMs) (.", "labels": [], "entities": []}, {"text": "A fundamental task addressed in the framework of DSMs is the computation of semantic similarity between words, adopting the distributional hypothesis of meaning, i.e., \"similarity of context implies similarity of meaning\".", "labels": [], "entities": []}, {"text": "DSMs have been successful when applied to the representation of word lexical semantics, enabling the computation of word semantic similarity.", "labels": [], "entities": [{"text": "representation of word lexical semantics", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.6681715548038483}, {"text": "word semantic similarity", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.6658616364002228}]}, {"text": "However, the application of DSMs for representing the semantics of more complex structures, e.g., phrases or sentences, is not trivial since the meaning of such structures is the result of various compositional phenomena that are inherent properties of natural language creativity.", "labels": [], "entities": []}, {"text": "The key idea behind current approaches in semantic composition (using DSMs) is the combination of word vectors using simple functions, e.g., vector addition or multiplication (, or other transformational functions.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.8158384263515472}, {"text": "vector addition", "start_pos": 141, "end_pos": 156, "type": "TASK", "confidence": 0.7270235270261765}]}, {"text": "Regardless of the used function, the resulting representations adhere to the paradigm of VSMs, while the cosine between the (composed) vectors is used for estimating similarity.", "labels": [], "entities": []}, {"text": "Such efforts proved to be effective when computing the similarity between twoword phrases, however, their limitations were revealed for the case of longer structures, where the composition of meaning becomes more complex.", "labels": [], "entities": []}, {"text": "proposed an approach based on deep learning for building language models that address the prob-lem of language creativity.", "labels": [], "entities": []}, {"text": "The models appear to constantly gain support in comparison with the traditional DSMs.", "labels": [], "entities": []}, {"text": "A preliminary comparative analysis of them is provided in () with respect to a number of tasks related to lexical semantics.", "labels": [], "entities": []}, {"text": "In this work, we extend a recent network-based implementation of DSMs () in order to represent the semantics of compositional structures.", "labels": [], "entities": []}, {"text": "The used framework consists of activation models motivated by semantic priming).", "labels": [], "entities": []}, {"text": "For each structure, an activation area (i.e, semantic neighborhood) is computed which is regarded as a sub-space within the network.", "labels": [], "entities": []}, {"text": "The novelty of the present work is twofold.", "labels": [], "entities": []}, {"text": "First, we propose various approaches for the creation of activation areas for compositional structures, within a framework alternative to VSMs.", "labels": [], "entities": []}, {"text": "Second, we investigate the fusion of the proposed network-based model with VSM-based transformational approaches from the literature.", "labels": [], "entities": [{"text": "VSM-based transformational", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7941129505634308}]}, {"text": "In addition, we investigate the role of words as operators on the meaning of the structures they occur in by measuring their transformative degree.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: in Section 2 we describe work related to DSMs.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 94, "end_pos": 98, "type": "TASK", "confidence": 0.9418651461601257}]}, {"text": "In Section 3 we describe the work on which we based the proposed models.", "labels": [], "entities": []}, {"text": "We present the proposed models in Section 4.", "labels": [], "entities": []}, {"text": "The lexical function model is described in Section 5, and a fusion model integrating the former with network-based models is proposed.", "labels": [], "entities": []}, {"text": "We describe the experimental procedure that we followed and evaluate the proposed models in Section 6.", "labels": [], "entities": []}, {"text": "We elaborate on the effects of modifiers in compositional structures in Section 7, concluding in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "The procedure for creating the network and conducting the experiments is described in Section 6.1.", "labels": [], "entities": []}, {"text": "In Section 6.2, we evaluate the proposed models and compare them with results from the literature.", "labels": [], "entities": []}, {"text": "We defined our vocabulary (network nodes) by intersecting the English vocabulary found in the AS-PELL 2 dictionary and the Wikipedia dump 3 to derive an English vocabulary of approximately 135K words.", "labels": [], "entities": []}, {"text": "Using it, a corpus comprising of webharvested document snippets was constructed by downloading 1000 snippets for each word in the vocabulary.", "labels": [], "entities": []}, {"text": "Word-level similarities were computed among all vocabulary entries' pairs.", "labels": [], "entities": []}, {"text": "To this end, the Normalized Google Distance (G) was utilized, proposed in and motivated by Kolmogorov complexity.", "labels": [], "entities": []}, {"text": "Let G be defined as where w 1 and w 2 are two vocabulary words under investigation, | D | is the total number of documents in the corpus, | D | w 1 , w 2 | is the total number of documents containing both w 1 and w 2 , and We used a variation of (9), proposed in (), referred to as \"Google-based Semantic Relatedness\" (G ).", "labels": [], "entities": []}, {"text": "This variation defines a similarity measure, bounded within the range and defined as where G(w 1 , w 2 ) is computed according to.", "labels": [], "entities": [{"text": "G", "start_pos": 91, "end_pos": 92, "type": "METRIC", "confidence": 0.9678119421005249}]}, {"text": "In this work, D denotes the sentence rather than the document, as the co-occurrence of words was defined at sentence-level.", "labels": [], "entities": []}, {"text": "This metric was adopted based on its good performance in word-level semantic similarity tasks (.", "labels": [], "entities": [{"text": "word-level semantic similarity tasks", "start_pos": 57, "end_pos": 93, "type": "TASK", "confidence": 0.6753785461187363}]}, {"text": "We used sizes of \u03b8 = {10, 25, 50, 100, 150, 500} for the case of fixed-size neighborhoods, and \u03b8 = {1, 5, ..., 40} for the extended activation models described in Section 4.1.", "labels": [], "entities": []}, {"text": "We used both the baseline and the extended activation layers for the M model, the latter being defined as M . For Mk and P k , we set k = {1, ..., 5}.", "labels": [], "entities": []}, {"text": "For the lf model described in (5), we computed co-occurence counts for bigrams occurring at least 50 times in the corpus.", "labels": [], "entities": []}, {"text": "Positive Pointwise Mutual Information (PPMI) was applied to reweigh them.", "labels": [], "entities": []}, {"text": "We used a) Singular Value Decomposition (SVD), and b) Non-Negative Matrix Factorization (NMF) () to reduce the dimensionality of the space down to a) 300, and b) 500 dimensions.", "labels": [], "entities": [{"text": "Non-Negative Matrix Factorization (NMF)", "start_pos": 54, "end_pos": 93, "type": "METRIC", "confidence": 0.6909046719471613}]}, {"text": "To train lf, we selected corpus bigrams comprising of a modifier and a noun.", "labels": [], "entities": []}, {"text": "We used a) Least Squares (LSR), and b) Ridge (RR) () regression.", "labels": [], "entities": [{"text": "Least Squares (LSR)", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.9261075019836426}, {"text": "Ridge (RR)", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9509006589651108}]}, {"text": "The DIStributional SEmantics Composition Toolkit (DISSECT 4 , () was used to implement lf, as well as the widely-used additive (add) and multiplicative (mult) models proposed in.", "labels": [], "entities": []}, {"text": "We combined the best performing model configurations on NNs (see Section 6.2) in order to implement the proposed fusion models.", "labels": [], "entities": []}, {"text": "For evaluation purposes, we used the widely-used datasets comprising of 108 noun-noun (NN), adjective-noun (AN), and verb-object (VO) phrase pairs, evaluated by human judgements and averaged per phrase pair.", "labels": [], "entities": []}, {"text": "The models were evaluated using Spearman's correlation coefficient.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient", "start_pos": 32, "end_pos": 66, "type": "METRIC", "confidence": 0.5245980024337769}]}, {"text": "Evaluation results are presented in Table 1.", "labels": [], "entities": []}, {"text": "Due to space limitations, only the best performing network-based model configurations are reported here.", "labels": [], "entities": []}, {"text": "Also, since the mult model performs poorly when the composed vectors contain negative values, as is the case with SVD, we only report results for the NMF variations for it.", "labels": [], "entities": []}, {"text": "Finally, since training the lf model with RR had significantly superior performance over LSR in all configurations, we only report evaluations of the former.", "labels": [], "entities": []}, {"text": "The lf model, when using RR in combination with NMF, performs best (.76) for the case of NNs.", "labels": [], "entities": []}, {"text": "Best performances for ANs and VOs are obtained by the add model (.63 and .59, respectively).", "labels": [], "entities": [{"text": "ANs", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.8542879819869995}, {"text": "VOs", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.7417470216751099}]}, {"text": "Regarding network-based models, performance is improved when using the extended activation model over the baseline.", "labels": [], "entities": []}, {"text": "This is confirmed by the absolute 5%, 11% and 10% increase for the case of NN, AN, and VO pairs, respectively, for the M metric.", "labels": [], "entities": [{"text": "AN", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9688766002655029}, {"text": "VO", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.8406789302825928}]}, {"text": "All the extended network-based models perform consistently better than the baseline of M , in the case of NNs, although their performance drops for the case of ANs and VOs.", "labels": [], "entities": []}, {"text": "In the case of P k , the scheme that constructs neighborhoods via the selection of the most similar neighbors performs better than the intersection-or the union-based scheme.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of models on NN, AN, and VO  phrase pairs. Evaluations are reported using Spearman's  correlation coefficient with human ratings.", "labels": [], "entities": []}]}