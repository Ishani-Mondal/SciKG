{"title": [{"text": "An Analytic and Empirical Evaluation of Return-on-Investment-Based Active Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "Return-on-Investment (ROI) is a cost-conscious approach to active learning (AL) that considers both estimates of cost and of benefit in active sample selection.", "labels": [], "entities": [{"text": "Return-on-Investment (ROI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.637296587228775}, {"text": "active learning (AL)", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7163398861885071}]}, {"text": "We investigate the theoretical conditions for successful cost-conscious AL using ROI by examining the conditions under which ROI would optimize the area under the cost/benefit curve.", "labels": [], "entities": []}, {"text": "We then empirically measure the degree to which optimality is jeopardized in practice when the conditions are violated.", "labels": [], "entities": []}, {"text": "The reported experiments involve an English part-of-speech annotation task.", "labels": [], "entities": []}, {"text": "Our results show that ROI can indeed successfully reduce total annotation costs and should be considered as a viable option for machine-assisted annotation.", "labels": [], "entities": [{"text": "ROI", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.7231877446174622}]}, {"text": "On the basis of our experiments, we make recommendations for benefit esti-mators to be employed in ROI.", "labels": [], "entities": [{"text": "ROI", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.7839240431785583}]}, {"text": "In particular, we find that the more linearly related a benefit estimate is to the true benefit, the better the estimate performs when paired in ROI with an imperfect cost estimate.", "labels": [], "entities": []}, {"text": "Lastly, we apply our analysis to help explain the mixed results of previous work on these questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "In active learning (AL), a sample selection algorithm sequentially chooses instances, or \"samples,\" to be labeled/annotated by an oracle.", "labels": [], "entities": [{"text": "active learning (AL)", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.6852088570594788}]}, {"text": "Each annotated instance results in a measurable benefit, such as an increase in model accuracy, and incurs a specific cost, such as the time needed to obtain the label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9884357452392578}]}, {"text": "Unfortunately some AL research has ignored the fact that instances have varying costs.", "labels": [], "entities": []}, {"text": "Decision-theoretic approaches (e.g.,) can incorporate per-instance cost but typically ignore it during experimentation, due in part to the difficulty of subtracting cost from benefit when they are measured in different units (.", "labels": [], "entities": []}, {"text": "Return-on-investment (ROI) is a cost-conscious technique that avoids this requirement by selecting the instance x * having maximum net benefit per unit cost, i.e., This approach to AL was independently proposed by, , and ; in addition, evaluated the effectiveness of ROI.", "labels": [], "entities": [{"text": "Return-on-investment (ROI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6247615143656731}]}, {"text": "Unfortunately, the published results regarding the usefulness of ROI are mixed.", "labels": [], "entities": [{"text": "ROI", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.7453907132148743}]}, {"text": "In addition, despite its intuitive appeal as a practical cost-conscious algorithm, there has been little theoretical justification for the ROI approach to AL.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to provide an initial theoretical analysis of ROI that, in turn, allows us to identify the conditions needed for the successful application of ROI in a practical environment.", "labels": [], "entities": []}, {"text": "We also empirically assess the degree to which violated conditions affect the overall performance of ROI and shed some light on the previously published results.", "labels": [], "entities": [{"text": "ROI", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.8422309756278992}]}, {"text": "The paper is organized as follows: related work is presented in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 examines the conditions under which ROI would be optimal.", "labels": [], "entities": [{"text": "ROI", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.6122679114341736}]}, {"text": "Section 4 discusses the experimental methodology.", "labels": [], "entities": []}, {"text": "Section 5 experimentally assesses the extent to which the conditions hold in practice -but outside the context of AL -while Section 5 explores the overall effect on AL.", "labels": [], "entities": []}, {"text": "Finally, Section 6 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our methodology for empirically assessing the degree to which the conditions of Section 3 hold in practice and define what we mean by practical contexts.", "labels": [], "entities": []}, {"text": "Space constraints limit our experiments to a single task: English partof-speech (POS) tagging on the POS-tagged Wall Street Journal text in the Penn Treebank version 3.", "labels": [], "entities": [{"text": "English partof-speech (POS) tagging", "start_pos": 58, "end_pos": 93, "type": "TASK", "confidence": 0.5631777544816335}, {"text": "Wall Street Journal text in the Penn Treebank version 3", "start_pos": 112, "end_pos": 167, "type": "DATASET", "confidence": 0.8577230095863342}]}, {"text": "For this task, we employ Maximum Entropy Markov Models (MEMMs) to model the distribution of tags given words, p(t|w).", "labels": [], "entities": []}, {"text": "The model choice is motivated primarily by the speed of retraining.", "labels": [], "entities": []}, {"text": "AL typically begins with a small set of randomly selected instances: we use 100 instances annotated \"from scratch\" (i.e., without AL).", "labels": [], "entities": []}, {"text": "However, we do account for the cost incurred by annotating the seed set using the cost simulation described below.", "labels": [], "entities": []}, {"text": "Each experiment is run 5 times with a different random seed.", "labels": [], "entities": []}, {"text": "For TVE (a committee-based approach; see below), we use a committee size of 5 and train all members in parallel.", "labels": [], "entities": [{"text": "TVE", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7221789956092834}]}, {"text": "We additionally score instances in parallel, using 4 threads; the remaining processors are used for training the cost model, evaluating benefit, and garbage collection.", "labels": [], "entities": [{"text": "garbage collection", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.7398776113986969}]}, {"text": "For noncommittee methods, we found that extra scoring threads do not improve results.", "labels": [], "entities": []}, {"text": "All simulations are run on dual hex-core Intel Westmere 2.67 GHz CPUs equipped with 24 GB of RAM.", "labels": [], "entities": [{"text": "Westmere 2.67 GHz CPUs", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.9019797891378403}]}], "tableCaptions": []}