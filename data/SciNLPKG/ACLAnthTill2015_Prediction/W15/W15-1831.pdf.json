{"title": [], "abstractContent": [{"text": "This article describes areal (non-synthetic) active-learning experiment to obtain supersense annotations for Dan-ish.", "labels": [], "entities": []}, {"text": "We compare two instance selection strategies, namely lowest-prediction confidence (MAX), and sampling from the confidence distribution (SAMPLE).", "labels": [], "entities": [{"text": "lowest-prediction confidence (MAX)", "start_pos": 53, "end_pos": 87, "type": "METRIC", "confidence": 0.9217850089073181}, {"text": "SAMPLE", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.6655029654502869}]}, {"text": "We evaluate their performance during the annotation process, across domains for the final resulting system, as well as against in-domain adjudicated data.", "labels": [], "entities": []}, {"text": "The SAMPLE strategy yields competitive models that are more robust than the overly length-biased selection criterion of MAX.", "labels": [], "entities": [{"text": "MAX", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.7961296439170837}]}], "introductionContent": [{"text": "Most successful natural language processing (NLP) systems rely on a set of labeled training examples to induce models in a supervised manner.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.8006927271684011}]}, {"text": "However, labeling instances to create a training set is time-consuming and expensive.", "labels": [], "entities": []}, {"text": "One way to alleviate this problem is to resort to active learning (AL), where a learner chooses which instancesfrom a large pool of unlabeled data-to give to the human expert for annotation.", "labels": [], "entities": [{"text": "active learning (AL)", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6369241118431092}]}, {"text": "After each annotation by the expert, the system retrains the learner, and the learner chooses anew instance to annotate.", "labels": [], "entities": []}, {"text": "There are many active learning strategies.", "labels": [], "entities": []}, {"text": "The simplest and most widely used is uncertainty sampling (, where the learner queries the instance it is most uncertain about ().", "labels": [], "entities": [{"text": "uncertainty sampling", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7420431971549988}]}, {"text": "Instead, in query-by-committee an entire committee of models is used to select the examples with highest disagreement.", "labels": [], "entities": []}, {"text": "At the same time most studies on active learning are actually synthetic, i.e. the human supervision was just emulated by holding out already labeled data.", "labels": [], "entities": []}, {"text": "In this study, we perform areal active learning experiment.", "labels": [], "entities": []}, {"text": "Since speed plays a major role, we do not resort to an ensemble-based query-bycommittee approach but use a single model for selection.", "labels": [], "entities": []}, {"text": "We evaluate two selection strategies fora sequence tagging task, supersense tagging.", "labels": [], "entities": [{"text": "sequence tagging task", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7436657249927521}, {"text": "supersense tagging", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7565493583679199}]}], "datasetContent": [{"text": "The goal of an AL setup is to augment the set of training instances.", "labels": [], "entities": []}, {"text": "In this section we evaluate the performance of the AL-generated annotations during the annotation process inform of learning curves (Section 5.1).", "labels": [], "entities": []}, {"text": "In order to gauge the robustness of a system trained on data obtained from AL, we break the evaluation down by domain (Section 5.2).", "labels": [], "entities": []}, {"text": "Finally, we compare a system trained exclusively on annotated and ajudicated newswire data with systems trained on a combination of training seed and AL data.", "labels": [], "entities": []}, {"text": "We evaluate all systems on micro-averaged F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.7823047637939453}]}], "tableCaptions": [{"text": " Table 1: Annotators and their setup, namely their  instance selection strategy and the unlabeled sub- corpus.", "labels": [], "entities": []}, {"text": " Table 2: Super-sense tagging data sets", "labels": [], "entities": [{"text": "Super-sense tagging", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7911329567432404}]}, {"text": " Table 3: Performance across domains for different  training setups. The best system for each dataset  is given in bold, and the worst system in italics.", "labels": [], "entities": []}, {"text": " Table 4: Descriptive statistics for the four gener- ated datasets.", "labels": [], "entities": []}, {"text": " Table 5: Cross-domain performance against held- out newswire data", "labels": [], "entities": []}]}