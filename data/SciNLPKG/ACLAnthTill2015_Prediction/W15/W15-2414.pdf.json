{"title": [{"text": "Modeling dative alternations of individual children", "labels": [], "entities": []}], "abstractContent": [{"text": "We address the question whether children can acquire mature use of higher-level grammatical choices from the linguistic input, given only general prior knowledge and learning biases.", "labels": [], "entities": []}, {"text": "We do soon the basis of a case study with the dative alternation in English, building on a study by de Marneffe et al.", "labels": [], "entities": []}, {"text": "(2012) who model the production of the dative alternation by seven young children, using data from the Child Language Data Exchange System corpus.", "labels": [], "entities": [{"text": "Child Language Data Exchange System corpus", "start_pos": 103, "end_pos": 145, "type": "DATASET", "confidence": 0.803142045934995}]}, {"text": "Using mixed-effects logistic modelling on the aggregated data of these children, De Marneffe et al. report that the children's choices can be predicted both by their own utterances and by child-directed speech.", "labels": [], "entities": []}, {"text": "Here we bring the computational model-ing down to the individual child, using memory-based learning and incremental learning curve studies.", "labels": [], "entities": []}, {"text": "We observe that for all children, their dative choices are best predicted by a model trained on child-directed speech.", "labels": [], "entities": []}, {"text": "Yet, models trained on two individual children for which sufficient data is available are about as accurate.", "labels": [], "entities": []}, {"text": "Furthermore, models trained on the dative alternations of these children provide approximations of dative alternations in caregiver speech that are about as accurate as training and testing on caregiver data only.", "labels": [], "entities": []}], "introductionContent": [{"text": "The production of language is the result of a great number of choices made by the individual speaker, where each choice maybe affected by various factors that, according to a large body of work, range from simple word frequencies to subtle semantic factors.", "labels": [], "entities": []}, {"text": "For instance, which variant of the dative alternation speakers produce has been shown in a corpus study to be partially affected by the animacy and givenness of the recipient and theme).", "labels": [], "entities": []}, {"text": "An inanimate recipient tends to co-occur with a prepositional dative construction (\"bring more jobs and more federal spending to their little area\").", "labels": [], "entities": []}, {"text": "Somehow and at some point in language acquisition, children learn these preferences, but it takes several years before children approximate adult language use.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7170005440711975}]}, {"text": "Monitoring and modeling this process of development may shed light on the inner workings of language learning in general, but to keep experiments under control, most studies, including the one presented here, zoom in on a representative but specific phenomenon.", "labels": [], "entities": []}, {"text": "The dative alternation has been the topic of several studies in which computational models are trained on naturalistic data (), such as offered by the Child Language Data Exchange System (CHILDES)), a publicly available database of children's speech produced in a natural environment.", "labels": [], "entities": [{"text": "dative alternation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.806073397397995}, {"text": "Child Language Data Exchange System (CHILDES))", "start_pos": 151, "end_pos": 197, "type": "DATASET", "confidence": 0.802304819226265}]}, {"text": "These approaches address what is conventionally known as \", which can be phrased as the question how children learn not to generalize a syntactic alternation to cases that block alternation, such as the verb 'donate', which only allows the prepositional dative construction.", "labels": [], "entities": []}, {"text": "In contrast, the present contribution continues a line of research introduced by de, who formulate three research questions: (1) do children show sensitivity to linguistic probability in their own syntactic choices, and if so, (2) are those probabilities driven by the same factors that affect adult production?", "labels": [], "entities": []}, {"text": "And finally, (3) do children assign the same weight to various factors as their caretakers?", "labels": [], "entities": []}, {"text": "If so, then this may support the hypothesis that from early on children are sen-sitive to (complex) variable distributional patterns.", "labels": [], "entities": []}, {"text": "At the highest theoretical level, the present study addresses the question whether children can acquire mature use of higher-level grammatical choices from the linguistic input, given only general prior knowledge and learning biases-or is a rich system of domain-specific abstract linguistic knowledge required from the outset?", "labels": [], "entities": []}, {"text": "See, for example,, fora recent sample of the debate.", "labels": [], "entities": []}, {"text": "The present study addresses this question by applying a well-developed exemplar-based machine learning model incrementally to children's linguistic experiences, represented by samples of child and caregiver productions from the CHILDES corpora, gathered for the prior study of de.", "labels": [], "entities": [{"text": "CHILDES corpora", "start_pos": 228, "end_pos": 243, "type": "DATASET", "confidence": 0.8479782342910767}]}, {"text": "In terms of computational theory, the model used in the present study is one of the class of mathematical kernel methods from Machine Learning theory, which encompass classical learning models such as exemplar theory.", "labels": [], "entities": []}, {"text": "More generally, we compare the predictions of an exemplar-based machine learning method to choices made by individual human subjects as a direct test of the model's cognitive plausibility for learning.", "labels": [], "entities": []}, {"text": "Following we use the tools-to-theories heuristic of in that we see our model as a mathematically and computationally simple and transparent emulation of the complex individual subject.", "labels": [], "entities": []}, {"text": "What we emulate is the subject trying to model the data he or she observes as examples stored in memory).", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the same data as de, which were extracted from the CHILDES database.", "labels": [], "entities": [{"text": "CHILDES database", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.9545857906341553}]}, {"text": "De Marneffe et al. focused on seven children: Abe, Adam, Naomi, Nina, Sarah, Shem, and Trevor, based on the amount of data available for them compared to other children, in terms of both their total number of utterances and the number of utterances containing one of the variants of the dative alternation.", "labels": [], "entities": []}, {"text": "The utterances were taken from the children's production between the ages of 2-5 years.", "labels": [], "entities": []}, {"text": "The data yielded a sufficient number of utterances to investigate two verbs in depth, give and show, which are the only ones considered in this study.", "labels": [], "entities": []}, {"text": "On top of this filtering, De Marneffe et al. selected only dative constructions following the \"verb NP NP\" (double object) construction or \"verb NP PP\" (prepositional dative) construction.", "labels": [], "entities": []}, {"text": "For all seven children, conversations with caregivers were included as well.", "labels": [], "entities": []}, {"text": "lists the basic statistics of available child and child-directed utterances with dative alternations, and the age range of the individual children (in days).", "labels": [], "entities": []}, {"text": "For two children, Adam and Nina, we have more than one hundred dative attestations in their own speech.", "labels": [], "entities": []}, {"text": "For both children we also have more than one hundred datives in the speech directed to them by their caregivers; fora third child, Shem, we also have over a hundred caregiver utterances containing datives.", "labels": [], "entities": []}, {"text": "Following the encoding of the data by De Marneffe et al. in their computational modeling experiment with mixed-effects logistic regression, all attestations of both dative constructions in their utterance context are converted to feature vectors.", "labels": [], "entities": []}, {"text": "Each vector (examplar) is metadated with the exact day of attestation, and labeled with the dative.", "labels": [], "entities": []}, {"text": "The Theme and Recipient length features are manually corrected due to the fact that in the original data used by De Marneffe et al. some recipients and themes mistakenly included other material such as adverbials.", "labels": [], "entities": []}, {"text": "The third column of lists the gain ratio weights for each feature (cf. Equation 3).", "labels": [], "entities": []}, {"text": "These weights seem to suggest four groups of features: 1.", "labels": [], "entities": []}, {"text": "Theme pronoun status and Recipient pronoun status are by far the most predictive features.", "labels": [], "entities": []}, {"text": "Theme pronoun status has a weight about 2.5 times higher than that of Recipient pronoun status, and over three times higher than the third highest weight; 2.", "labels": [], "entities": []}, {"text": "There is a second-tier group of informative features with again ratio of about 0.07\u22120.08: Prime, Theme, Recipient, Recipient givenness levels, Theme corrected lenght, and Recipient corrected length; 3.", "labels": [], "entities": [{"text": "Recipient corrected length", "start_pos": 171, "end_pos": 197, "type": "METRIC", "confidence": 0.7924748857816061}]}, {"text": "A third-tier group of features has weights in the range of 0.02 \u2212 0.05: Theme givenness levels, Theme animacy, and Recipient toy animacy; 4.", "labels": [], "entities": []}, {"text": "A fourth-tier group has near-zero weights, carrying hardly any predictive information: Verb, Recipient animacy, and Theme toy animacy.", "labels": [], "entities": []}, {"text": "Perhaps somewhat surprisingly, the identity of the verb (give or show) is virtually unrelated to the  Our experiments are run per individual child, in an iterative experiment that tracks the child on a dayby-day basis and computes a learning curve.", "labels": [], "entities": []}, {"text": "illustrates how the iterative learning curve experiment takes its first steps.", "labels": [], "entities": []}, {"text": "At each point of the curve, all dative choices attested so far constitute the training set, while all new dative choices attested in the single next day on which datives are observed constitute the test set.", "labels": [], "entities": []}, {"text": "Hence, the first training set is the first day on which the child generated one or more dative constructions; the first test set is derived from the next day the child produced datives.", "labels": [], "entities": []}, {"text": "In the second step, the test set of the first step is added to the training data, and the next test set consists of all datives produced by the child on a next day.", "labels": [], "entities": []}, {"text": "At each step the incrementally learning memory-based classifier adds the new examples to memory, after which it classifies the new test set, which may only contain one or a handful of attestations.", "labels": [], "entities": []}, {"text": "All single predictions per day are recorded as a sequence of predictions and whether these predictions were corrector incorrect.", "labels": [], "entities": []}, {"text": "At each point of the curve a correctness score can be produced that aggregates overall predictions so far.", "labels": [], "entities": [{"text": "correctness score", "start_pos": 29, "end_pos": 46, "type": "METRIC", "confidence": 0.9643410444259644}]}, {"text": "At the end of the curve we achieve an aggregate score overall predictions.", "labels": [], "entities": []}, {"text": "The desired outcome of a learning curve experiment is obviously a metric expressing the success of predicting the right choices.", "labels": [], "entities": []}, {"text": "In order for indi- training test: Visualisation of the first steps of a learning curve experiment.", "labels": [], "entities": []}, {"text": "In the first step, the training material contains all dative attestations observed in the first day of attestations, and the test material contains all dative attestations found in the next day with datives.", "labels": [], "entities": []}, {"text": "In the second, step, the latter material is added to the training set, and the third day of attestations is now the test set.", "labels": [], "entities": []}, {"text": "vidual experimental outcomes to be comparable, they should not be based on different skews in the distribution between the two dative choices.", "labels": [], "entities": []}, {"text": "Accuracy (the percentage of correct predictions) will not do, as it is biased to the majority class.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.994335949420929}]}, {"text": "When a child would choose one dative construction in 90% of the cases, a classifier trained on that child would easily score 90% accurate predictions by only guessing the majority outcome, while a classifier that is able to attain 80% correct predictions fora child that chooses between the two alternations in a 50%-50% distribution is intrinsically more successful and interesting.", "labels": [], "entities": []}, {"text": "To eliminate the effect that class skew may have on our evaluation metric we evaluate our classifier predictions in the learning curve experiments with the area under the curve (AUC) metric).", "labels": [], "entities": []}, {"text": "The AUC metric computes, per class, the surface under a curve or a point classifier in the two-dimensional receiver operation characteristic (ROC) space, where the one dimension is the true positive rate (or recall) of predicting the class, and the other dimension is the false positive rate of mispredicting the class.", "labels": [], "entities": [{"text": "recall", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.9747875332832336}]}, {"text": "displays the AUC score of the outcome of a classifier (a point classifier as it produces a single score rather than a curve) on a class, depicted by the large dot; the AUC score is the area of the gray surface.", "labels": [], "entities": [{"text": "AUC score", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9667982459068298}, {"text": "AUC score", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9653798937797546}]}, {"text": "We compute the AUC score of both dative choices, and take the micro-average of the two AUC scores; i.e. each score is weighted by the relative proportion of occurrence of its choice.", "labels": [], "entities": [{"text": "AUC score", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9192782640457153}]}, {"text": "The resulting number is a score between 0.5 and 1.0 that is insensitive to the skew between the two dative choices in a particular child's data, where 0.5 means baseline performance (random or majority guessing), and 1.0 means perfect prediction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Basic statistics for the seven children used  in the study: numbers of utterances and age range  in days (cds = child-directed speech).", "labels": [], "entities": []}, {"text": " Table 3: Aggregated AUC scores of MBL at the end of the learning curves of the seven children, training  on four different selections of material. Best performances are printed in bold.", "labels": [], "entities": [{"text": "Aggregated AUC scores", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8017105261484782}, {"text": "MBL", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.6181734800338745}]}, {"text": " Table 4: Comparison of AUC scores when testing  on CDS data from other children, trained either  on the child's datives or on the child's caregiver's  datives.", "labels": [], "entities": [{"text": "AUC", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.7910398244857788}]}]}