{"title": [{"text": "Do Distributed Semantic Models Dream of Electric Sheep? Visualizing Word Representations through Image Synthesis", "labels": [], "entities": [{"text": "Visualizing Word Representations", "start_pos": 56, "end_pos": 88, "type": "TASK", "confidence": 0.6079246600468954}]}], "abstractContent": [{"text": "We introduce the task of visualizing distributed semantic representations by generating images from word vectors.", "labels": [], "entities": []}, {"text": "Given the corpus-based vector encoding the word broccoli, we convert it to a visual representation by means of a cross-modal mapping function, and then use the mapped representation to generate an image of broccoli as \"dreamed\" by the distributed model.", "labels": [], "entities": []}, {"text": "We propose a baseline dream synthesis method based on averaging pictures whose visual representations are topologi-cally close to the mapped vector.", "labels": [], "entities": []}, {"text": "Two experiments show that we generate dreams that generally belong to the the right semantic category, and are sometimes accurate enough for subjects to distinguish the intended concept from a related one.", "labels": [], "entities": []}], "introductionContent": [{"text": "When researchers \"visualize\" distributed/distributional semantic models, they typically present 2D scatterplots illustrating the distances between a set of word representations (Van der Maaten and.", "labels": [], "entities": []}, {"text": "We propose a much more direct approach to visualization.", "labels": [], "entities": []}, {"text": "Given a vector representing a word in a corpus-derived distributed space, we generate a picture depicting how the denotatum of the word looks like, according to the model.", "labels": [], "entities": []}, {"text": "Given, say, the word2vec vector of broccoli, we want to know how broccoli looks like to word2vec (see for the answer).", "labels": [], "entities": []}, {"text": "Besides the inherent coolness of the task, it has many potential applications.", "labels": [], "entities": []}, {"text": "Current qualitative analysis of distributed semantic models is limited to assessing the relation between words, e.g., by looking at, or plotting, nearest neighbour sets, but it lacks methods to inspect the properties of a specific word directly.", "labels": [], "entities": []}, {"text": "Our image synthesis approach will allow researchers to \"see\", in a very literal sense, how a model represents a single word.", "labels": [], "entities": [{"text": "image synthesis", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.750114381313324}]}, {"text": "Moreover, in the spirit of the \"A picture is worth a thousand words\" adage, the generated images will allow researchers to quickly eyeball the results, getting the gist of what a model is capturing much faster than from textual neighbour lists.", "labels": [], "entities": []}, {"text": "For example, a more \"topical\" model might produce pictures depicting the wider scenes in which objects occur (a ball being dribbled by soccer players), whereas a model capturing strictly conceptual aspects might produce narrow views of the denoted objects (a close-up of the ball).", "labels": [], "entities": []}, {"text": "Image synthesis could also be used to explore the effect of different input corpora on representations: e.g., given a historical corpus, generate images for the car word representations induced from early 20th-century vs. 21st-century texts.", "labels": [], "entities": [{"text": "Image synthesis", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7965158820152283}]}, {"text": "As a last example, proposed to examine the topics of Topic Models by associating them with images retrieved from the Web.", "labels": [], "entities": []}, {"text": "Given that topics are represented by vectors, we could directly generate images representing these topics.", "labels": [], "entities": []}, {"text": "In cognitive science, there is a lively debate on whether abstract words have embodied representations, (), an issue that has recently attracted the attention of the distributed semantics community).", "labels": [], "entities": []}, {"text": "An intriguing application of image synthesis would be to produce and assess imagery for abstract concepts.", "labels": [], "entities": [{"text": "image synthesis", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7508273720741272}]}, {"text": "Recent work in neuroscience attempts to generate images of \"what people think\", as encoded in vector-based representations of fMRI patterns ().", "labels": [], "entities": []}, {"text": "With our method, we could then directly compare images produced from corpus-based representations to what humans visualize when thinking of the same words.", "labels": [], "entities": []}, {"text": "In the long term, we would like to move beyond words, towards generating images depicting the meaning of phrases (e.g., an angry cat vs. acute cat vs. a white cat) and sentences.", "labels": [], "entities": []}, {"text": "This would nicely complement current work on generating verbal descriptions of images) with the inverse task of generating images from verbal descriptions.", "labels": [], "entities": []}, {"text": "Generating images from vectorial word representations is of course extremely challenging.", "labels": [], "entities": []}, {"text": "However, various relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue.", "labels": [], "entities": []}, {"text": "First, tools such as word2vec () and Glove () produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise.", "labels": [], "entities": []}, {"text": "Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation).", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.7451317012310028}]}, {"text": "Finally, the computer vision community is starting to explore the task of image generation (, typically in an attempt to understand the inner workings of visual feature extraction algorithms).", "labels": [], "entities": [{"text": "image generation", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.737384632229805}]}, {"text": "The main aim of this paper is to present proofof-concept evidence that the task is feasible.", "labels": [], "entities": []}, {"text": "To this end, we rely on state-of-the-art word representation and cross-modality mapping methods, but we adopt an image synthesis strategy that could be seen as an interesting baseline to compare other approaches against.", "labels": [], "entities": [{"text": "word representation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7331448197364807}, {"text": "cross-modality mapping", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7104182541370392}, {"text": "image synthesis", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.7171580344438553}]}, {"text": "Briefly, our pipeline works as follows.", "labels": [], "entities": []}, {"text": "Our input is given by pre-computed word representations (word2vec) and a set of labeled images together with their pre-compiled representations in a high-level visual feature space (specifically, we use activations on one of the top layers (fc7) of a convolutional neural network as high-level image representations).", "labels": [], "entities": []}, {"text": "Given an input word vector, we use a linear cross-modal function to map it into visual space, and we retrieve then nearest image representations.", "labels": [], "entities": []}, {"text": "Finally, we overlay the actual images corresponding to these nearest neighbours in order to derive a visualization of the mapped word, a method we refer to as averaging.", "labels": [], "entities": []}, {"text": "For example, the first image in below is our visualization of broccoli, obtained by projecting the broccoli word vector onto visual space, retrieving the 20 nearest images and averaging them.", "labels": [], "entities": []}, {"text": "Importantly, we apply this synthesis method to words that are not used to train the cross-modal mapping function, and that do not match the label of any picture in the image data set.", "labels": [], "entities": []}, {"text": "So, for example, our system had to map broccoli onto visual space without having ever been exposed to labeled broccoli images (zero-shot setting), and it generated the broccoli image by averaging pictures that do not depict broccoli.", "labels": [], "entities": []}], "datasetContent": [{"text": "Task definition and data collection In this experiment we presented a dream, and asked subjects if they thought it was more likely to denote the correct dreamed word or a confounder randomly picked from the seen word set (we did not use the \"dream\" terminology to explain the task to subjects).", "labels": [], "entities": [{"text": "Task definition", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7233019769191742}]}, {"text": "Since the confounder is a randomly picked term, the task is relatively easy.", "labels": [], "entities": []}, {"text": "At the same time, since the confounders are picked from a set of concrete concepts, just like the dreamed words, it sometimes happens that the two concepts are quite related, as illustrated in.", "labels": [], "entities": []}, {"text": "Moreover, all confounders were used to train the mapping function, and their pictures are present in the averaging pool.", "labels": [], "entities": []}, {"text": "These factors could introduce a bias in favour of them.", "labels": [], "entities": []}, {"text": "We tested all 510 McRae 4 \u03bb is 10-fold cross-validated on the training data.", "labels": [], "entities": [{"text": "McRae 4 \u03bb", "start_pos": 18, "end_pos": 27, "type": "DATASET", "confidence": 0.8869380156199137}]}, {"text": "The idea of generating a more abstract depiction of something by averaging a number of real pictures is popular in contemporary art) and it has recently been adopted in computer vision, as away to visualize large sets of images of the same concept, e.g., averaging across different cat breeds ().", "labels": [], "entities": []}, {"text": "words, collecting 20 ratings for each.", "labels": [], "entities": []}, {"text": "We randomized word order both across and within trials.", "labels": [], "entities": [{"text": "word order", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.7117218226194382}]}, {"text": "We used the CrowdFlower 6 platform to collect the judgments, limiting participation to subjects from English-speaking countries who self-declared English as their native language.", "labels": [], "entities": []}, {"text": "Results Subjects show a consistent preference for the correct (dreamed) word (median proportion of votes in favor of it: 90%).", "labels": [], "entities": []}, {"text": "Preference for the correct word is significantly different from chance in 419/510 cases (twosided exact binomial tests, corrected for multiple comparisons with the false discovery rate method, \u03b1 = .05).", "labels": [], "entities": [{"text": "Preference", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9862751960754395}]}, {"text": "Subjects expressed a significant preference for the confounder in only 5 cases (budgie/parakeet, cake/pie, camel/ox, shotgun/revolver, squid/octopus).", "labels": [], "entities": []}, {"text": "For the first two dreams in, subjects showed a significant preference for the dreamed word, despite the fact that the confounder is a related term.", "labels": [], "entities": []}, {"text": "Still, when the two words are closely related, it is more likely that subjects will beat random.", "labels": [], "entities": []}, {"text": "The figure also shows two interesting examples in which dreamed word and confounder are related, and subjects significantly preferred the latter.", "labels": [], "entities": []}, {"text": "The tongs/utensil case is very challenging, because any tongs picture would also bean utensil picture (and the dreamed object does not look like tongs to start with).", "labels": [], "entities": []}, {"text": "For zebra/baboon, we conjecture that subjects could makeup an animal in the dream, but one lacking the salient black-and-white pattern of zebras.", "labels": [], "entities": []}, {"text": "Task definition and data collection In this experiment, we matched each dreamed word with its own dream and a confounder dream generated from the most similar dreamed term (see for examples).", "labels": [], "entities": [{"text": "Task definition", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.695488452911377}]}, {"text": "Word similarity was measured in a space defined by subject-generated properties describing the concepts of interest (this method is known to produce high-quality similarity estimates, better than those obtained with textbased distributional models, see, e.g.,).", "labels": [], "entities": []}, {"text": "Subjects were asked which of the two images is more likely to contain the thing denoted by the word.", "labels": [], "entities": []}, {"text": "This is a very challenging task, as inmost cases target and confounder are closely related concepts, and thus their dreams must have considerable granularity to allow subjects to make the correct choice.", "labels": [], "entities": []}, {"text": "Again, we used CrowdFlower to collect 20 votes per item, with the same modalities of Experiment 1.", "labels": [], "entities": []}, {"text": "Results We expected the simple averaging method to fail completely at the level of accuracy required by this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9990884065628052}]}, {"text": "The results, however, suggest at least a trend in the right direction.", "labels": [], "entities": []}, {"text": "This time, the median proportion of votes for the correct dream is at 60%.", "labels": [], "entities": []}, {"text": "In 165/510 cases, there is a significant preference for the correct dream (same statistical testing setup as above), and in 57 cases for the confounder.", "labels": [], "entities": []}, {"text": "A manual annotation of higher-level categories of dreamed word and confounder (e.g., garment, mammal, etc.) revealed that the proportion of votes for the correct dream was much higher in the 100 cases in which the two items belonged to different categories (80% vs. 55% for same-category pairs).", "labels": [], "entities": []}, {"text": "The top row of illustrates cases where the pairs belong to the same category, and yet subjects still showed a strong preference for the correct dream.", "labels": [], "entities": []}, {"text": "In the tractor/truck case, both dreams represent vehicles, but the correct one is evoking the rural environment a tractor.", "labels": [], "entities": []}, {"text": "For swan/dove, we can make out birds in both dreams, but the swan dream is clearly of a larger, aquatic bird.", "labels": [], "entities": []}, {"text": "Still, the more common case is the one where, if the two concepts are closely related, subjects assign random preferences, as they did for the examples in the second row.", "labels": [], "entities": []}], "tableCaptions": []}