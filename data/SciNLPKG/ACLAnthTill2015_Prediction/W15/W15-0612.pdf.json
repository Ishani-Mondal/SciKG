{"title": [{"text": "Identifying Patterns For Short Answer Scoring Using Graph-based Lexico-Semantic Text Matching", "labels": [], "entities": [{"text": "Identifying Patterns", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9272322058677673}, {"text": "Short Answer Scoring", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.6521244049072266}, {"text": "Graph-based Lexico-Semantic Text Matching", "start_pos": 52, "end_pos": 93, "type": "TASK", "confidence": 0.5673754140734673}]}], "abstractContent": [{"text": "Short answer scoring systems typically use regular expressions, templates or logic expressions to detect the presence of specific terms or concepts among student responses.", "labels": [], "entities": [{"text": "Short answer scoring", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7919162511825562}]}, {"text": "Previous work has shown that manually developed regular expressions can provide effective scoring, however manual development can be quite time consuming.", "labels": [], "entities": []}, {"text": "In this work we present anew approach that uses word-order graphs to identify important patterns from human-provided rubric texts and top-scoring student answers.", "labels": [], "entities": []}, {"text": "The approach also uses semantic metrics to determine groups of related words, which can represent alternative answers.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two datasets: (1) the Kaggle Short Answer dataset (ASAP-SAS, 2012), and (2) a short answer dataset provided by Mohler et al.", "labels": [], "entities": [{"text": "Kaggle Short Answer dataset (ASAP-SAS, 2012)", "start_pos": 50, "end_pos": 94, "type": "DATASET", "confidence": 0.856455041302575}]}, {"text": "We show that our automated approach performs better than the best performing Kaggle entry and generalizes as a method to the Mohler dataset.", "labels": [], "entities": [{"text": "Mohler dataset", "start_pos": 125, "end_pos": 139, "type": "DATASET", "confidence": 0.8995667695999146}]}], "introductionContent": [{"text": "In recent years there has been a significant rise in the number of approaches used to automatically score essays.", "labels": [], "entities": []}, {"text": "These involve checking grammar, syntax and lexical sophistication of student answers).", "labels": [], "entities": []}, {"text": "While essays are evaluated for the quality of writing, short answers are brief and evoke very specific responses (often restricted to specific terms or concepts) from students.", "labels": [], "entities": []}, {"text": "Hence the use of features that check grammar, structure or organization may not be sufficient to grade short answers.", "labels": [], "entities": []}, {"text": "Regular expressions, text templates or patterns have been used to determine whether a student answer matches a specific word or a phrase present in the rubric text.", "labels": [], "entities": []}, {"text": "For example, Moodle (2011) allows for the use of a \"Regular Expression ShortAnswer question\" type which allows instructors or question developers to code correct answers as regular expressions.", "labels": [], "entities": [{"text": "Moodle (2011)", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.8572777956724167}]}, {"text": "Consider the question: \"What are blue, red and yellow?\"", "labels": [], "entities": []}, {"text": "This question can evoke a very specific response: \"They are colors.\"", "labels": [], "entities": []}, {"text": "However, there are several ways (with the term \"color\" spelled differently, for instance) to answer this question.", "labels": [], "entities": []}, {"text": "E.g. (1) they are colors; (2) they are colours; (3) they're colours; (4) they're colors; (5) colours; or (6) colors.", "labels": [], "entities": []}, {"text": "Instead of having to enumerate all the alternatives to this question, the answer can be coded as a regular expression: (they('|\\s(a))re\\s)?colo(u)?rs.", "labels": [], "entities": []}, {"text": "Manually generated regular expressions have been used as features in generating models that score short answers in the Kaggle Short Answer Scoring competition).", "labels": [], "entities": [{"text": "Kaggle Short Answer Scoring competition", "start_pos": 119, "end_pos": 158, "type": "TASK", "confidence": 0.7237592458724975}]}, {"text": "Tandalla (2012)'s approach, the best performing one of the competition, achieved a Quadratic Weighted (QW) Kappa of 0.70 using just regular expressions as features.", "labels": [], "entities": [{"text": "Quadratic Weighted (QW) Kappa", "start_pos": 83, "end_pos": 112, "type": "METRIC", "confidence": 0.9587840338548025}]}, {"text": "However, regular expression generation can be tedious and time consuming, and the performance of these features is constrained by the ability of humans to generate good regular expressions.", "labels": [], "entities": [{"text": "regular expression generation", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.6386420726776123}]}, {"text": "Automating this approach would ensure that the process is repeatable, and the results consistent.", "labels": [], "entities": []}, {"text": "We propose an approach to identify patterns to score short answers using the rubric text and topscoring student responses.", "labels": [], "entities": []}, {"text": "The approach involves (1) identification of classes of semantically related words or phrases that a human evaluator would expect to see among the best answers, and (2) combining these semantic classes in a meaningful way to generate patterns.", "labels": [], "entities": [{"text": "identification of classes of semantically related words or phrases", "start_pos": 26, "end_pos": 92, "type": "TASK", "confidence": 0.7107401688893636}]}, {"text": "These patterns help capture the main concepts or terms that are representative of a good student response.", "labels": [], "entities": []}, {"text": "We use a word order graph) to represent the rubric text.", "labels": [], "entities": []}, {"text": "The graph captures order of tokens in the text.", "labels": [], "entities": []}, {"text": "We use a lexico-semantic matching technique to identify the degree of relatedness across tokens or phrases.", "labels": [], "entities": []}, {"text": "The matching process helps identify alternate ways of expressing the response.", "labels": [], "entities": []}, {"text": "An answer containing the text diet of koalas would be coded as follows: The patterns generated contain (1) positional constraints (?=, which indicates that the search for the text should start at the beginning, and (2) the choice operator (|), which captures alternate ways of expressing the same term, e.g. diet or eat or grub.", "labels": [], "entities": []}, {"text": "We look for match (or non-match) between the set of generated patterns and new short answers.", "labels": [], "entities": []}, {"text": "We evaluate our patterns on short answers from the Kaggle Automated Student Assessment Prize (ASAP) competition, the largest publicly available short answer dataset ().", "labels": [], "entities": [{"text": "Kaggle Automated Student Assessment Prize (ASAP) competition", "start_pos": 51, "end_pos": 111, "type": "DATASET", "confidence": 0.7923399872250028}]}, {"text": "We compare our results with the those from the competition's best model, which uses manually generated regular expressions.", "labels": [], "entities": []}, {"text": "Our aim with this experiment is to demonstrate that automatically generated patterns produce results that are comparable to manually generated patterns.", "labels": [], "entities": []}, {"text": "We also tested our approach on a different short answer dataset curated by.", "labels": [], "entities": []}, {"text": "One of the main contributions of this paper is the use of an automated approach to generate patterns that can be used to grade short answers effectively, while spending less time and effort.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 discusses related work that use manually constructed patterns or answer templates to grade student responses.", "labels": [], "entities": []}, {"text": "Section 3 contains a description of our approach to automatically generate patterns to grade short answers.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 discuss the experiments conducted to evaluate the performance of our patterns in scoring short answers.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}, {"text": "developed the use of a short-answer scoring system called C-rater, which focuses on semantic information in the text.", "labels": [], "entities": []}, {"text": "They used a paraphrase-recognition based approach to score answers.", "labels": [], "entities": []}, {"text": "proposed the use of a short answer assessment system called WebLAS.", "labels": [], "entities": [{"text": "WebLAS", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9409435987472534}]}, {"text": "They extracted regular expressions from a model answer to generate the scoring key.", "labels": [], "entities": []}, {"text": "Regular expressions are formed with exact as well as near-matches of words or phrases.", "labels": [], "entities": []}, {"text": "Student answers are scored based on the degree of match between the answer and scoring key.", "labels": [], "entities": []}, {"text": "Unlike Bachman et al., we do not use patterns to directly match and score student answers.", "labels": [], "entities": []}, {"text": "In our approach, text patterns are supplied as features to a learning algorithm such as Random Forest) in order to accurately predict scores.", "labels": [], "entities": []}, {"text": "used templates to identify the presence of sample phrases or keywords among student responses.", "labels": [], "entities": []}, {"text": "Marking schemes were developed based on keys specified by human item developers.", "labels": [], "entities": []}, {"text": "The templates contained lists of alternative (stemmed) tokens fora word or phrase that could be used by the student.", "labels": [], "entities": []}, {"text": "used hand-coded patterns to capture different ways of expressing the correct answer.", "labels": [], "entities": []}, {"text": "They automated the approach of template creation, but the automated ones did not outperform the manually generated templates.", "labels": [], "entities": [{"text": "template creation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9114432632923126}]}, {"text": "used manually encoded first-order predicate representations of answers to score responses.", "labels": [], "entities": []}, {"text": "reformulated queries as declarative sentence segments to aid query-answer matching.", "labels": [], "entities": [{"text": "query-answer matching", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7374716103076935}]}, {"text": "Their approach worked under the condition that the (exact) content words appearing in a query would also appear in the answer.", "labels": [], "entities": []}, {"text": "Consider the sample query \"When was the paperclip invented?\", and the sample answer: \"The paperclip is a very useful device.", "labels": [], "entities": []}, {"text": "It was patented by Johan Vaaler in 1899.\"", "labels": [], "entities": []}, {"text": "The word patented is related in meaning to the term invented, but since the exact word is not used in the query, it will not match the answer.", "labels": [], "entities": []}, {"text": "We propose a technique that uses related words as part of the patterns in order to avoid overlooking semantically close matches.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of the Kaggle ASAP Short Answer Scoring competition was to identify tools that would help score answers comparable to humans).", "labels": [], "entities": [{"text": "Kaggle ASAP Short Answer Scoring competition", "start_pos": 15, "end_pos": 59, "type": "TASK", "confidence": 0.7046755601962408}]}, {"text": "Short answers along with prompt texts (and in some cases sample answers) were made avail-able to competitors.", "labels": [], "entities": []}, {"text": "The dataset contains 10 different prompts scored on either a scale of 0-2 or 0-3.", "labels": [], "entities": []}, {"text": "There were a total of 17207 training and 5224 test answers.", "labels": [], "entities": []}, {"text": "Around 153 teams participated in the competition.", "labels": [], "entities": []}, {"text": "The metric used for evaluation is QW Kappa.", "labels": [], "entities": [{"text": "QW Kappa", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.5349398553371429}]}, {"text": "The human benchmark for the dataset was 0.90.", "labels": [], "entities": []}, {"text": "The best team achieved a score of 0.77.", "labels": [], "entities": []}, {"text": "Our aim with this experiment is to compare systemgenerated patterns with Tandalla's manually generated regular expressions.", "labels": [], "entities": []}, {"text": "The goal is to determine the scoring performance of automated patterns, while keeping everything (but the regular expressions) in the best performing approach's code constant.", "labels": [], "entities": []}, {"text": "We substituted the manual regular expressions used by Tandalla in his code with the automated patterns.", "labels": [], "entities": []}, {"text": "We then ran Tandalla's code to generate the models and obtain predictions for the test set.", "labels": [], "entities": []}, {"text": "We evaluate our approach on each of the 10 prompt sets from the Kaggle short answer dataset.", "labels": [], "entities": [{"text": "Kaggle short answer dataset", "start_pos": 64, "end_pos": 91, "type": "DATASET", "confidence": 0.8846408426761627}]}, {"text": "The final predictions produced by Tandalla's code is the average of four learning models' (two Random Forests and two Gradient Boosting Machines) predictions.", "labels": [], "entities": []}, {"text": "The learners were used to build regression (and not discrete) models.", "labels": [], "entities": []}, {"text": "We used content tokens and phrase patterns to generate two sets of predictions, one for each run of Tandalla's code.", "labels": [], "entities": []}, {"text": "We stacked the output by taking the average of the two sets of predictions.", "labels": [], "entities": []}, {"text": "We compare our model with the following: 1.", "labels": [], "entities": []}, {"text": "Tandalla's model with manually generated regular expressions: This is the gold standard, since manual regular expressions were apart of the best performing model.", "labels": [], "entities": []}, {"text": "In this section we evaluate our approach on an alternate short answer scoring dataset generated by.", "labels": [], "entities": []}, {"text": "The aim is to show that our method is not specific to a single type of short answer, and could be used successfully on other datasets to build scoring models.", "labels": [], "entities": []}, {"text": "Mohler et al. use a combination of graph-based alignment and lexical similarity measures to grade short answers.", "labels": [], "entities": []}, {"text": "They evaluate their model on a dataset containing 10 assignments and 2 examinations.", "labels": [], "entities": []}, {"text": "The dataset contains 81 questions with a total of 2273 answers.", "labels": [], "entities": []}, {"text": "The dataset was graded by two human judges on a scale of 0-5.", "labels": [], "entities": []}, {"text": "Human judges have an agreement of 57.7%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9831427931785583}]}, {"text": "Mohler et al. apply a 12-fold cross validation over the entire dataset to evaluate their models.", "labels": [], "entities": []}, {"text": "On average, the train fold contains 1894 data points while the test fold contains 379 data points.", "labels": [], "entities": []}, {"text": "Models are constructed with data from assignments containing questions on a variety of programming concepts such as the role of a header file, offset notation in arrays and the advantage of linked lists over arrays.", "labels": [], "entities": []}, {"text": "Although all the questions are from the same domain (e.g. computer programming) the answers they evoke are very different.", "labels": [], "entities": []}, {"text": "Mohler et al. achieved a correlation of 0.52 with the average human grades, with a hybrid model that used Support Vector Machines as a ranking algorithm.", "labels": [], "entities": []}, {"text": "The hybrid model contained a combination of graph-nodes alignment, bag-of-words and lexical similarity features.", "labels": [], "entities": []}, {"text": "The best Root Mean Square Error (RMSE) of 0.98 was achieved by the hybrid model, which used Support Vector Regression as the learner.", "labels": [], "entities": [{"text": "Root Mean Square Error (RMSE)", "start_pos": 9, "end_pos": 38, "type": "METRIC", "confidence": 0.8927079779761178}]}, {"text": "The best median RMSE computed across each individual question was 0.86.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.937125563621521}]}, {"text": "We use the same dataset to extract text patterns.", "labels": [], "entities": []}, {"text": "Since patterns are prompt or question specific we cannot create models using the entire dataset like Mohler et al. do.", "labels": [], "entities": []}, {"text": "Patterns extracted from across different questions may not be representative of the content of individual questions or assignments.", "labels": [], "entities": []}, {"text": "Questions within each assignment are on the same topic.", "labels": [], "entities": []}, {"text": "contains a list of all questions from Assignment 5, which is about insertion, selection and merge sort algorithms.", "labels": [], "entities": [{"text": "merge sort", "start_pos": 92, "end_pos": 102, "type": "TASK", "confidence": 0.6648844480514526}]}, {"text": "We therefore extract patterns containing content tokens and phrases for each assignment.", "labels": [], "entities": []}, {"text": "The data for each assignment is divided into train and test sets (80% train and 20% test).", "labels": [], "entities": []}, {"text": "The train set contains a total of 1820 data points and the test set contains a total of 453 data points.", "labels": [], "entities": []}, {"text": "The train data is used to extract content tokens and phrase patterns from sample answers.", "labels": [], "entities": []}, {"text": "Most short answer grading systems use term vectors as features (), since they work as a good baseline.", "labels": [], "entities": []}, {"text": "Term vectors contain frequency of terms in an answer.", "labels": [], "entities": []}, {"text": "We use a combination of term vectors and automatically extracted patterns as features.", "labels": [], "entities": []}, {"text": "We use a Random Forest regressor as the learner to build models.", "labels": [], "entities": []}, {"text": "The learner is trained on the average of the human grades.", "labels": [], "entities": []}, {"text": "We stack results from models created with each type of pattern to compute final results.", "labels": [], "entities": []}, {"text": "Our approach's correlation overall the test data is 0.61.", "labels": [], "entities": [{"text": "correlation", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9758892059326172}]}, {"text": "The RMSE is 0.86, and the median RMSE computed over questions is 0.77.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9967544674873352}, {"text": "RMSE", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9064297676086426}]}, {"text": "The improvement in correlation of our stacked model overs performance of 0.52 is significant (one tailed test, pvalue = 0.02 < 0.05, thus the null hypothesis that this difference is a chance occurrence maybe rejected).", "labels": [], "entities": [{"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.95948725938797}]}, {"text": "Correlation achieved by using just term vectors is 0.56 (difference from Mohler et al.'s result is not significant).", "labels": [], "entities": [{"text": "Correlation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9668709635734558}]}, {"text": "These results indicate that the use of patterns results in an improvement in performance.", "labels": [], "entities": []}, {"text": "The above process was repeated at the granularity level of questions.", "labels": [], "entities": []}, {"text": "Data points from each question were divided into train and test sets, and models were built for each training set.", "labels": [], "entities": []}, {"text": "There were a total of 1142 training and 1131 test data points.", "labels": [], "entities": []}, {"text": "Results from the stacked model are computed overall the test predictions.", "labels": [], "entities": []}, {"text": "This model achieved a correlation of 0.61, and an RMSE of 0.88.", "labels": [], "entities": [{"text": "correlation", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9768257141113281}, {"text": "RMSE", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9984655380249023}]}, {"text": "The median RMSE computed over each of the questions is 0.82.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9515076875686646}]}, {"text": "As can be seen from our stacked model performs better in terms of correlation, RMSE and median RMSE over questions than Mohler et al.'s best models.", "labels": [], "entities": [{"text": "correlation", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9742789268493652}, {"text": "RMSE", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.94883131980896}, {"text": "RMSE", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.5656163692474365}]}, {"text": "One of the reasons for improved performance could be that models were built over individual assignments or questions rather than over the entire data.", "labels": [], "entities": []}, {"text": "Patterns are particularly effective when built over assignments containing the same type of responses.", "labels": [], "entities": []}, {"text": "Short answer scoring can be very sensitive to the content of answers.", "labels": [], "entities": [{"text": "Short answer scoring", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6986019810040792}]}, {"text": "Hence using data from across a variety of assignments could result in a poorly generalized model.", "labels": [], "entities": []}], "tableCaptions": []}