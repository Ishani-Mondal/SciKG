{"title": [{"text": "Reconciling Heterogeneous Descriptions of Language Resources", "labels": [], "entities": [{"text": "Reconciling Heterogeneous Descriptions of Language", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.9166615962982178}]}], "abstractContent": [{"text": "Language resources area cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task.", "labels": [], "entities": []}, {"text": "This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies.", "labels": [], "entities": []}, {"text": "In this paper we present a first attempt to collect and harmonize the meta-data of different repositories, thus making them queriable and browsable in an integrated way.", "labels": [], "entities": []}, {"text": "We make use of RDF and linked data technologies for this and provide a first level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT.", "labels": [], "entities": [{"text": "Dublin Core", "start_pos": 207, "end_pos": 218, "type": "DATASET", "confidence": 0.9890090227127075}, {"text": "DCAT", "start_pos": 223, "end_pos": 227, "type": "DATASET", "confidence": 0.7318000197410583}]}, {"text": "Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes such as the type, license or intended use of a resource-into normalized values.", "labels": [], "entities": []}, {"text": "Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates .", "labels": [], "entities": []}], "introductionContent": [{"text": "Language resources are the cornerstone of linguistic research as well as of computational linguistics.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.7178700417280197}]}, {"text": "Within NLP, for instance, most tools developed require a corpus to be trained (e.g. language models, statistical taggers, statistical parsers, and statistical machine translation systems) or they require lexico-semantic resources as background knowledge to perform some task (e.g. word sense disambiguation).", "labels": [], "entities": [{"text": "statistical taggers", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.705431267619133}, {"text": "statistical machine translation", "start_pos": 147, "end_pos": 178, "type": "TASK", "confidence": 0.6248651842276255}, {"text": "word sense disambiguation)", "start_pos": 281, "end_pos": 307, "type": "TASK", "confidence": 0.7194656953215599}]}, {"text": "As the number of language resources available keeps growing, the task of discovering and finding resources that are pertinent to a particular task becomes increasingly difficult.", "labels": [], "entities": []}, {"text": "While there area number of repositories that collect and index metadata of language resources, such as META-SHARE (),, LRE-Map (), Datahub.io and OLAC, they do not provide a complete solution to the discovery problem for two reasons.", "labels": [], "entities": []}, {"text": "First, integrated search overall these different repositories is not possible, as they use different data models, different vocabularies and expose different interfaces and APIs.", "labels": [], "entities": []}, {"text": "Second, these repositories must strike a balance between quality and coverage, either opting for coverage at the expense of quality of metadata, or vice versa.", "labels": [], "entities": []}, {"text": "When collecting metadata from multiple resources, we understand that there are two principal challenges: property harmonization and duplication detection.", "labels": [], "entities": [{"text": "duplication detection", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.9068989455699921}]}, {"text": "Harmonization is the challenge of verifying that there is not only structural and syntactic interoperability between the resources in that they use the same property, for example Dublin Core's language property, but also that they use the same value.", "labels": [], "entities": [{"text": "Dublin Core's language property", "start_pos": 179, "end_pos": 210, "type": "DATASET", "confidence": 0.9488040804862976}]}, {"text": "For example, the following values of the language property are likely to be equivalent: \"French\", \"Modern French\", \"fran\u00e7ais\", \"fr\", \"fra\" and \"fre\".", "labels": [], "entities": []}, {"text": "It is difficult to write queries on a dataset if every property has many equivalent values and thus it is essential to use a single representation.", "labels": [], "entities": []}, {"text": "Secondly, we wish to detect duplicates that occur either due to the original representation or from multiple sources.", "labels": [], "entities": []}, {"text": "It is clear that if a large number of records in fact describe the same resource then queries for that resource will return too many resources that may lead to errors (or annoyance) for users.", "labels": [], "entities": []}, {"text": "For example, the \"Universal Declaration of Human Rights\" is available in 444 languages 2 and listing each translation as a single resource (as the CLARIN VLO does) does not correctly capture the nature of the resource.", "labels": [], "entities": [{"text": "Universal Declaration of Human Rights\"", "start_pos": 18, "end_pos": 56, "type": "DATASET", "confidence": 0.9414981802304586}, {"text": "CLARIN VLO", "start_pos": 147, "end_pos": 157, "type": "DATASET", "confidence": 0.8960828483104706}]}, {"text": "Furthermore, these resources may not match some queries, such as for example 'resources in more than one language', and as such it is preferable to merge these individual records into a single complex record.", "labels": [], "entities": []}, {"text": "As the main contribution of this paper, we present the methods used to harmonize data across repositories.", "labels": [], "entities": []}, {"text": "Due to the different kinds of values and target taxonomies chosen for each property, these methods vary but all are based on stateof-the-art NLP techniques, including word sense disambiguation, and make major improvements to the data quality of our metadata records.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 167, "end_pos": 192, "type": "TASK", "confidence": 0.6739629010359446}]}, {"text": "Second, we show indeed that duplicate metadata records are pervasive and that they occur both within and across repositories.", "labels": [], "entities": []}, {"text": "We then present a simple yet effective approach to detect duplicates within and across repositories.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: we give an overview of work related to harmonization of data as well as an overview of existing metadata repositories for linguistic data in Section 2.", "labels": [], "entities": []}, {"text": "We describe our metadata collection and schema matching strategy in Section 3.", "labels": [], "entities": [{"text": "metadata collection", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7131204009056091}, {"text": "schema matching", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.6899207681417465}]}, {"text": "We describe our techniques for metadata harmonization in Section 4.", "labels": [], "entities": [{"text": "metadata harmonization", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.8843207657337189}]}, {"text": "We describe our methods for duplication detection in Section 5.", "labels": [], "entities": [{"text": "duplication detection", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.9867023527622223}]}, {"text": "The performance of the different techniques is reported in each of these sections.", "labels": [], "entities": []}, {"text": "We discuss our methodology and approach from a wider point of view in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The sizes of the resources in terms of  number of metadata records and total data size", "labels": [], "entities": []}, {"text": " Table 2: The distribution of the 10 most used for- mats within the analyzed sample of URLs. Note  XML is associated with two MIME types.", "labels": [], "entities": []}, {"text": " Table 3: Excerpt output of language mapping.  * indicates mismatches.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy of language mappings", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9863141775131226}]}, {"text": " Table 5: The number of intra-repository duplicate  labels and URLs for resources", "labels": [], "entities": []}, {"text": " Table 6: Number of duplicate inter-repository records by type", "labels": [], "entities": []}]}