{"title": [{"text": "LORIA System for the WMT15 Quality Estimation Shared Task", "labels": [], "entities": [{"text": "LORIA", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8785747289657593}, {"text": "WMT15 Quality Estimation Shared", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.7156456708908081}]}], "abstractContent": [{"text": "We describe our system for WMT2015 Shared Task on Quality Estimation, task 1, sentence-level prediction of post-edition effort.", "labels": [], "entities": [{"text": "WMT2015 Shared Task on Quality Estimation", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.6032569209734598}, {"text": "sentence-level prediction", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7174243330955505}]}, {"text": "We use baseline features, Latent Semantic Indexing based features and features based on pseudo-references.", "labels": [], "entities": []}, {"text": "SVM algorithm allows to estimate the linear regression between the features vectors and the HTER score.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.6175288558006287}]}, {"text": "We use a selection algorithm in order to put aside needless features.", "labels": [], "entities": []}, {"text": "Our best system leads to a performance in terms of Mean Absolute Error equal to 13.34 on official test while the official baseline system leads to a performance equal to 14.82.", "labels": [], "entities": [{"text": "Mean Absolute Error", "start_pos": 51, "end_pos": 70, "type": "METRIC", "confidence": 0.8936082720756531}]}], "introductionContent": [{"text": "This paper describes the LORIA submission to the WMT'15 Shared Task on Quality Estimation.", "labels": [], "entities": [{"text": "LORIA", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.8002846240997314}, {"text": "WMT'15 Shared Task on Quality Estimation", "start_pos": 49, "end_pos": 89, "type": "TASK", "confidence": 0.6788228352864584}]}, {"text": "We participated to the Task 1.", "labels": [], "entities": [{"text": "Task 1", "start_pos": 23, "end_pos": 29, "type": "TASK", "confidence": 0.6054840981960297}]}, {"text": "This task consists in predicting the edition effort needed to correct a translated sentence.", "labels": [], "entities": []}, {"text": "The organizers provide English sentences automatically translated into Spanish, and the corresponding post-edited sentences.", "labels": [], "entities": []}, {"text": "The edition effort is measured by edit-distance rate (HTER ()) between the translated sentence and its post-edited version.", "labels": [], "entities": [{"text": "edit-distance rate (HTER", "start_pos": 34, "end_pos": 58, "type": "METRIC", "confidence": 0.8832938820123672}]}, {"text": "Classically, our system extracts numerical features from sentences and applies a machine learning approach between numeric vectors and HTER scores.", "labels": [], "entities": []}, {"text": "As last year, no information is given about the Machine Translation (MT) system used to build data.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.8345663130283356}]}, {"text": "Therefore, it is only possible to use blackbox features, or to use other MT systems whom output is compared to the evaluated target sentence.", "labels": [], "entities": []}, {"text": "Our submission deals with the both kinds of features.", "labels": [], "entities": []}, {"text": "First, we use a Latent Semantic Analysis approach to measure the lexical similarity between a source and a target sentence.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.5709397395451864}]}, {"text": "To our knowledge, this approach has never been used in the scope of Quality Estimation.", "labels": [], "entities": [{"text": "Quality Estimation", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.6630667001008987}]}, {"text": "Second, we use the output of 3 online MT systems, and we extract information about the intersection between the evaluated target sentence and the 3 translated sentences by online systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9764400124549866}]}, {"text": "This intersection is measured in terms of shared 1,2,3,4-grams.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 give details about experimental protocol and used data.", "labels": [], "entities": []}, {"text": "We describe the features we use in Section 3.", "labels": [], "entities": []}, {"text": "Then, we give results (Section 4) and we conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe how we obtain results starting from training, development and test corpus.", "labels": [], "entities": []}, {"text": "The training and development corpus are composed of a set of triplets.", "labels": [], "entities": []}, {"text": "Each triplet is made up of a source sentence, its automatic translation, and a score representing the translation quality.", "labels": [], "entities": []}, {"text": "For our experiments, we use the corpora the organizers provide.", "labels": [], "entities": []}, {"text": "The source language is English, the target language is Spanish.", "labels": [], "entities": []}, {"text": "For each source sentence s, a machine translation system (unknown to the participants) gives a translation t (we keep notations sand t throughout this article for source and target sentences from the evaluation campaign data).", "labels": [], "entities": []}, {"text": "t is manually post-edited into pe.", "labels": [], "entities": []}, {"text": "The score of (s, t) is the HTER score between t and pe (noted hter).", "labels": [], "entities": [{"text": "HTER score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9744386374950409}]}, {"text": "We use the official training corpus tr composed of 11272 triplets (s, t, hter), and the official development corpus dev composed of 1000 triplets.", "labels": [], "entities": []}, {"text": "Filtering the features some features may not be useful because they provide more noise than information, or because training data is not sufficiently big to estimate the link between them and the scores.", "labels": [], "entities": []}, {"text": "Therefore, it maybe useful to apply an algorithm in order to select interesting features.", "labels": [], "entities": []}, {"text": "For that, we use a backward algorithm) we yet described in ().", "labels": [], "entities": []}, {"text": "This year, we did not use the initial step consisting in evaluating the correlations between features (see ().", "labels": [], "entities": []}, {"text": "The algorithm is applied on the dev corpus in order to minimise the MAE (Mean Absolute Error) score defined by M AE(r, r ) = where r is the set of n predicted scores on dev, and r is the set of HTER reference scores.", "labels": [], "entities": [{"text": "MAE (Mean Absolute Error) score", "start_pos": 68, "end_pos": 99, "type": "METRIC", "confidence": 0.8785466296332223}, {"text": "M AE", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.8042606711387634}]}], "tableCaptions": [{"text": " Table 1: List of baseline features. id are given to  refer later to a specific feature. S, T are for 'source'  or 'target' feature.", "labels": [], "entities": []}, {"text": " Table 2: MAE score on dev of each baseline fea- ture, and of the whole 17 baseline features", "labels": [], "entities": [{"text": "MAE score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9760431349277496}]}, {"text": " Table 3: Performance in terms of MAE on dev  of LSI feature according to the number of topics.  The LSI feature is associated with the 17 baseline  features.", "labels": [], "entities": [{"text": "MAE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9963473677635193}]}, {"text": " Table 5: Performance in terms of MAE on dev of the whole set of features", "labels": [], "entities": [{"text": "MAE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.992388904094696}]}]}