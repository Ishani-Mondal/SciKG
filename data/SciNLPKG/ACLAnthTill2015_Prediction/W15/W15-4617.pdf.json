{"title": [{"text": "Automated Speech Recognition Technology for Dialogue Interaction with Non-Native Interlocutors", "labels": [], "entities": [{"text": "Automated Speech Recognition", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6775037944316864}]}], "abstractContent": [{"text": "Dialogue interaction with remote interlocutors is a difficult application area for speech recognition technology because of the limited duration of acoustic context available for adaptation, the narrow-band and compressed signal encoding used in telecommunications, high variability of spontaneous speech and the processing time constraints.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8069149851799011}]}, {"text": "It is even more difficult in the case of interacting with non-native speakers because of the broader allophonic variation, less canonical prosodic patterns, a higher rate of false starts and incomplete words, unusual word choice and smaller probability to have a grammatically well formed sentence.", "labels": [], "entities": []}, {"text": "We present a comparative study of various approaches to speech recognition in non-native context.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.787832647562027}]}, {"text": "Comparing systems in terms of their accuracy and real-time factor we find that a Kaldi-based Deep Neural Network Acoustic Model (DNN-AM) system with on-line speaker adaptation by far outperforms other available methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9989587068557739}]}], "introductionContent": [{"text": "Designing automatic speech recognition (ASR) and spoken language understanding (SLU) modules for spoken dialog systems (SDSs) poses more intricate challenges than standalone ASR systems, for many reasons.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR) and spoken language understanding (SLU)", "start_pos": 10, "end_pos": 84, "type": "TASK", "confidence": 0.7929242803500249}]}, {"text": "First, speech recognition latency is extremely important in a spoken dialog system for smooth operation and a good caller experience; one needs to ensure that recognition hypotheses are obtained in near real-time.", "labels": [], "entities": [{"text": "speech recognition latency", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.7102103034655253}]}, {"text": "Second, one needs to deal with the lack of (or minimal) context, since responses in dialogic situations can often be short and succinct.", "labels": [], "entities": []}, {"text": "This also means that one might have to deal with minimal data for model adaptation.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.7323302924633026}]}, {"text": "Third, these responses being typically spontaneous in nature, often exhibit pauses, hesitations and other disfluencies.", "labels": [], "entities": []}, {"text": "Fourth, dialogic applications might have to deal with audio bandwidth limitations that will also have important implications for the recognizer design.", "labels": [], "entities": []}, {"text": "For instance, in telephonic speech, the bandwidth (300-3200 Hz) is lesser than that of the hifidelity audio recorded at 44.1 kHz.", "labels": [], "entities": [{"text": "bandwidth", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9809790849685669}]}, {"text": "All these issues can drive up the word error rate (WER) of the ASR component.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.8833582202593485}, {"text": "ASR", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9765321016311646}]}, {"text": "Ina recent study comparing several popular ASRs such as),) and cloud-based APIs from Apple 1 , Google 2 and AT&T 3 in terms of their suitability for use in SDSs, In) there was found no particular consensus on the best ASR, but observed that the open-source Kaldi ASR performed competently in comparison with the other closed-source industry-based APIs.", "labels": [], "entities": []}, {"text": "Moreover, in a recent study, () it was found that Kaldi significantly outperformed other opensource recognizers on recognition tasks on German Verbmobil and English Wall Street Journal corpora.", "labels": [], "entities": [{"text": "opensource recognizers", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.8006838858127594}, {"text": "German Verbmobil and English Wall Street Journal corpora", "start_pos": 136, "end_pos": 192, "type": "DATASET", "confidence": 0.8420486450195312}]}, {"text": "The Kaldi online ASR was also shown to outperform the Google ASR API when integrated into the Czech-based ALEX spoken dialog framework).", "labels": [], "entities": [{"text": "Kaldi online ASR", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.5553130805492401}]}, {"text": "The aforementioned issues with automatic speech recognition in SDSs are only exacerbated in the case of non-native speakers.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6514477332433065}, {"text": "SDSs", "start_pos": 63, "end_pos": 67, "type": "TASK", "confidence": 0.7109116315841675}]}, {"text": "Not only do non-native speakers pause, hesitate and make false starts more often than native speakers of a language, but their speech is also characterized by a broader allophonic variation, a less canonical prosodic pattern, a higher rate of incomplete words, unusual word choices and a lower probabil- ity of producing grammatically well-formed sentences.", "labels": [], "entities": []}, {"text": "An important application scenario for nonnative dialogic speech recognition is the case of conversation-based Computer-Assisted Language Learning (CALL) systems.", "labels": [], "entities": [{"text": "nonnative dialogic speech recognition", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.6301512345671654}]}, {"text": "For instance, Subarashii is an interactive dialog system for learning Japanese (), where the ASR component of the system was built using the HTK speech recognizer () with both native and non-native acoustic models.", "labels": [], "entities": [{"text": "HTK speech recognizer", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.7220211823781332}]}, {"text": "In general, the performance of the system after SLU was good for in-domain utterances, but not for outof-domain utterances.", "labels": [], "entities": []}, {"text": "As another example, in Robot Assisted Language Learning) and CALL applications for Korean-speaking learners of English (, whose authors showed that acoustic models trained on the Wall Street Journal corpus with an additional 17 hours of Korean children's transcribed English speech for adaptation produced as low as 22.8% WER across multiple domains tested.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 179, "end_pos": 205, "type": "DATASET", "confidence": 0.9701402336359024}, {"text": "WER", "start_pos": 322, "end_pos": 325, "type": "METRIC", "confidence": 0.9983575940132141}]}, {"text": "In the present work, we investigate the online and offline performance of a Kaldi Large Vocabulary Continuous Speech Recognition (LVCSR) system in conjunction with the opensource and distributed HALEF spoken dialog system ().), a web server running Apache Tomcat, and a speech server, which consists of an MRCP server) in addition to text-to-speech (TTS) engines-Festival () and Mary-as well as support for and) ASRs.", "labels": [], "entities": [{"text": "Kaldi Large Vocabulary Continuous Speech Recognition (LVCSR)", "start_pos": 76, "end_pos": 136, "type": "TASK", "confidence": 0.6912808550728692}]}, {"text": "In contrast to Sphinx-4 which is tightly integrated into the speech server code base, Kaldi-based ASR is installed on an own server, which is communicating with the speech server via TCP socket.", "labels": [], "entities": [{"text": "ASR", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.5869511365890503}]}, {"text": "Often ASR configurations in live SDSs differ from batch systems that may result in different behaviour w.r.t.", "labels": [], "entities": [{"text": "ASR", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.966719925403595}]}, {"text": "WER, latency, etc.", "labels": [], "entities": [{"text": "WER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9699468016624451}, {"text": "latency", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.9893301129341125}]}], "datasetContent": [{"text": "The evaluation was performed using vocal productions obtained from language learners in the scope of large-scale internet-based language assessment.", "labels": [], "entities": []}, {"text": "The production length is a major distinction of this data from the data one may expect to find in the spoken dialogue domain.", "labels": [], "entities": []}, {"text": "The individual utterance is a quasi-spontaneous monologue elicited by a certain evaluation setup.", "labels": [], "entities": []}, {"text": "The utterances were collected from six different test questions comprising two different speaking tasks: 1) providing an opinion based on personal experience and 2) summarizing or discussing material provided in a reading and/or listening passage.", "labels": [], "entities": [{"text": "summarizing or discussing material provided in a reading and/or listening passage", "start_pos": 165, "end_pos": 246, "type": "TASK", "confidence": 0.6883165194438055}]}, {"text": "The longest utterances are expected to last up to a minute.", "labels": [], "entities": []}, {"text": "The average speaking rate is about 2 words per second.", "labels": [], "entities": []}, {"text": "Every speaker produces up to six such utterances.", "labels": [], "entities": []}, {"text": "Speakers had a brief time to familiarize themselves with the task and prepare an approximate production plan.", "labels": [], "entities": []}, {"text": "Although in strict terms, these productions are different from the true dialogue behavior, they are suitable for the purposes of the dialogic speech recognition system development.", "labels": [], "entities": [{"text": "dialogic speech recognition system development", "start_pos": 133, "end_pos": 179, "type": "TASK", "confidence": 0.7634868502616883}]}, {"text": "The evaluation of the speech recognition system was performed using the data obtained in the same fashion as the training material.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8453191518783569}]}, {"text": "Two sets are used: the development set (dev), containing 593 utterances (68329 tokens, 3575 singletons, 0% OOV rate) coming from 100 speakers with the total amount of audio exceeding 9 hours; and the test set (test), that contains 599 utterances (68112 tokens, 3709 singletons, 0.18% OOV rate) coming from 100 speakers (also more than 9 hours of speech in total).", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9743154644966125}]}, {"text": "We attempted to have a nonbiased random speaker sampling, covering abroad range of native languages, English speaking proficiency levels, demographics, etc.", "labels": [], "entities": []}, {"text": "However, no extensive effort has been spent to ensure that frequencies of the stratified sub-populations follow their natural distribution.", "labels": [], "entities": []}, {"text": "Comparative results are presented in.", "labels": [], "entities": []}, {"text": "As it can be learned from, the \"DNN iVector\" method of speech recognition outperforms Kaldi's default \"DNN fMLLR\" setup.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7539491653442383}]}, {"text": "This can be explained by the higher variability of non-native speech.", "labels": [], "entities": []}, {"text": "In this case the reduced complexity of the i-Vector speaker adaptation matches better the task that we attempt to solve.", "labels": [], "entities": []}, {"text": "There is only a very minor degradation of the accuracy with the reduction of the i-Vector support data from the whole interaction to a single utterance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996408224105835}]}, {"text": "As expected, the \"online\" scenario loses some accuracy to the \"offline\" in the utterance beginning, as we could verify by analyzing multiple recognition results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9988856911659241}]}, {"text": "It is also important to notice that the accuracy of the \"DNN i-Vector\" system compares favorably with human performance in the same task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9996578693389893}]}, {"text": "In fact, experts have the average WER of about 15%, while Turkers in a crowdsourcing environment perform significantly worse, around 30% WER ().", "labels": [], "entities": [{"text": "WER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9978164434432983}, {"text": "WER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.99594646692276}]}, {"text": "Our proposed system is therefore already approaching the level of broadly defined average human accuracy in the task of non-native speech transcription.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9975197911262512}, {"text": "non-native speech transcription", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6375891367594401}]}, {"text": "The \"DNN i-Vector\" ASR method vastly outperforms the baseline in terms of processing speed.", "labels": [], "entities": [{"text": "ASR", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7697169780731201}]}, {"text": "Even with the large vocabulary model in atypical 10-second spoken turn we expect to have only 3 seconds of ASR-specific processing latency.", "labels": [], "entities": [{"text": "ASR-specific processing", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.8478263318538666}]}, {"text": "Indeed, in order to obtain an expected de-: Accuracy and speed of the explored ASR configurations; WER -Word Error Rate; (dev) -as measured on the development set; (test) -as measured on the test set; xRT -Real Time factor, i.e. the ratio between processing time and audio duration; SI -Speaker Independent mode.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9983718991279602}, {"text": "WER -Word Error Rate", "start_pos": 99, "end_pos": 119, "type": "METRIC", "confidence": 0.918575382232666}, {"text": "xRT -Real Time factor", "start_pos": 201, "end_pos": 222, "type": "METRIC", "confidence": 0.9000864148139953}]}, {"text": "lay one shall subtract the duration of an utterance from the total processing time as the \"online\" recognizer commences speech processing at the moment that speech is started.", "labels": [], "entities": []}, {"text": "That 3 seconds delay is very close to the natural inter-turn pause of 0.5 -1.5 seconds.", "labels": [], "entities": [{"text": "delay", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9808738827705383}]}, {"text": "Better language modeling is expected to bring the xRT factor below one.", "labels": [], "entities": [{"text": "xRT", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.991245687007904}]}, {"text": "The difference of the xRT factor between the \"online\" and \"offline\" modes can be explained with somewhat lower quality of acoustic normalization in the \"online\" case.", "labels": [], "entities": [{"text": "xRT", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9883545637130737}]}, {"text": "Larger numbers of hypotheses fit within the decoder's search beam and, thus, increase the processing time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy and speed of the explored ASR configurations; WER -Word Error Rate; (dev) -as  measured on the development set; (test) -as measured on the test set; xRT -Real Time factor, i.e. the  ratio between processing time and audio duration; SI -Speaker Independent mode.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978105425834656}, {"text": "WER -Word Error Rate", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.9224060535430908}, {"text": "xRT -Real Time factor", "start_pos": 168, "end_pos": 189, "type": "METRIC", "confidence": 0.8837310194969177}]}]}