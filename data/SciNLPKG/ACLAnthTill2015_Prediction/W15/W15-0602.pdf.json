{"title": [{"text": "Feature selection for automated speech scoring", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.679285486539205}]}], "abstractContent": [{"text": "Automated scoring systems used for the evaluation of spoken or written responses in language assessments need to balance good empirical performance with the interpretability of the scoring models.", "labels": [], "entities": [{"text": "evaluation of spoken or written responses in language assessments", "start_pos": 39, "end_pos": 104, "type": "TASK", "confidence": 0.6907087763150533}]}, {"text": "We compare several methods of feature selection for such scoring systems and show that the use of shrinkage methods such as Lasso regression makes it possible to rapidly build models that both satisfy the requirements of validity and in-tepretability, crucial in assessment contexts as well as achieve good empirical performance.", "labels": [], "entities": [{"text": "validity", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9776831269264221}]}], "introductionContent": [{"text": "In this paper we compare different methods of selecting the best feature subset for scoring models used in the context of large-scale language assessments, with a particular look at the assessment of spoken responses produced by test-takers.", "labels": [], "entities": []}, {"text": "The basic approach to automatically scoring written or spoken responses is to collect a training corpus of responses that are scored by human raters, use machine learning to estimate a model that maps response features to scores from this corpus , and then use this model to predict scores for unseen responses.", "labels": [], "entities": []}, {"text": "While this method is often quite effective in terms of producing scoring models that exhibit good agreement with human raters, it can lend itself to criticism from the educational * Currently at Civis Analytics measurement community if it fails to address certain basic considerations for assessment design and scoring that are common practice in that field.", "labels": [], "entities": []}, {"text": "For instance, argue that automated scoring not only has to be reliable (i.e., exhibiting a good empirical performance as demonstrated, for example, by correlations between predicted and human scores), but also valid.", "labels": [], "entities": [{"text": "automated scoring", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.64812932908535}]}, {"text": "One very important aspect of validity is to what extent the automated scoring model reflects important dimensions of the construct measured by the test (a construct is the set of knowledge, skills, and abilities measured by a test).", "labels": [], "entities": [{"text": "validity", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.896439254283905}]}, {"text": "For example, a speaking proficiency test for non-native speakers may claim that it assesses aspects such as fluency, pronunciation, and content accuracy in a test-taker's spoken response(s).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.8795989155769348}]}, {"text": "If the features that contribute to the scoring models can be seen as measuring all of these aspects of spoken language well, the model would be considered valid from a construct point of view.", "labels": [], "entities": []}, {"text": "However, if certain dimensions of the construct are not represented (well) by the feature set used in the scoring model, and/or features contained in the model address aspects not considered to be relevant for measuring the test construct, the construct validity of the scoring model would not be considered ideal (cf. also and who make similar argument).", "labels": [], "entities": []}, {"text": "Furthermore, relative contributions by features to each construct dimension should be easily obtainable from the scoring model.", "labels": [], "entities": []}, {"text": "To satisfy this requirement, machine-learning approaches such as support vector machines (SVMs) with non-linear kernels maybe less ideal than a simple straightforward linear regression model, where the contribution of each feature in the model is immediately obvious.", "labels": [], "entities": []}, {"text": "Finally, the contribution of each feature to the final score should be consistent with the relevant constructs: if all of the features in the model are designed to be positively correlated with human scores, the coefficients of all such features in the final model should be positive as well.", "labels": [], "entities": []}, {"text": "Fulfilling all of these requirements when building automated scoring models is not trivial and has, in the past, typically involved the participation and advice of human content and measurement experts whose role it is to optimize the feature set so that it adheres to the aforementioned criteria as much as possible, while still allowing for good empirical performance of the resulting automated scoring model ().", "labels": [], "entities": []}, {"text": "However, there are certain limitations to this manual process of scoring-model building, not the least of which is the aspect of time it takes to build models with iterative evaluations and changes in the feature set composition.", "labels": [], "entities": [{"text": "scoring-model building", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7278492450714111}]}, {"text": "Alternatively, one can compute a large number of potential features and then use automatic feature selection to identify the most suitable subset.", "labels": [], "entities": []}, {"text": "This second approach is commonly used in studies that aim to maximize the performance of machine-learning systems (cf. for example, among many others), but to our knowledge, it has not yet been applied in the assessment context where model performance needs to be balanced with model validity in terms of construct coverage and other constraints such as feature polarity.", "labels": [], "entities": []}, {"text": "We consider several methods of automatic feature selection commonly applied to linear models ().", "labels": [], "entities": [{"text": "automatic feature selection", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.702437162399292}]}, {"text": "These include subset selection methods such as step-wise feature selection as well as shrinkage methods such as Lasso regression.", "labels": [], "entities": []}, {"text": "We focus on feature selection methods that can be scaled to a large number of features which exclude, for example, the best-subset approach, which becomes unfeasible for more than 30-40 features.", "labels": [], "entities": []}, {"text": "We also exclude methods that use derived input such as principal component regression or partial least squares because the contribution of each feature in the final model would be more difficult to interpret.", "labels": [], "entities": []}, {"text": "Finally, we consider feature selection methods which make it possible to restrict the coefficients to positive values.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7107319980859756}]}, {"text": "Such restriction is not specific to automated scoring and therefore various algorithms have been developed to address this requirement (see, for example, Lipovetsky (2009) for further discussion).", "labels": [], "entities": [{"text": "automated scoring", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.633843183517456}]}, {"text": "We consider several of such methods including non-negative least squares regression and a constrained version of Lasso regression.", "labels": [], "entities": []}, {"text": "In this paper we address the following questions: (a) What methods of automatic feature selection can address all or most of the requirements of automated scoring and therefore are most suitable for this purpose?", "labels": [], "entities": [{"text": "automatic feature selection", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.6837484240531921}]}, {"text": "(b) Does more constrained selection affect the performance of such scoring models?", "labels": [], "entities": []}, {"text": "(c) How do models based on automated feature selection compare to models based on human expert feature selection in terms of empirical performance and construct coverage?", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 provides a description of the data used in this study, further details about the feature-selection methods, and the parameter setting for these methods.", "labels": [], "entities": []}, {"text": "Section 3 presents the comparison between different featureselection methods in terms of performance, coefficient polarity, and construct coverage of the selected feature subset.", "labels": [], "entities": []}, {"text": "Finally, Section 4 summarizes the results of our experiments.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Maximum and minimum number of features se- lected by each model (N min and N max ), average ratio of  features assigned positive coefficients to the total N fea- tures (P/N ) and average Pearson's r between predicted  and observed scores r resp across 10 folds", "labels": [], "entities": [{"text": "Pearson's r between predicted  and observed scores r resp", "start_pos": 197, "end_pos": 254, "type": "METRIC", "confidence": 0.8445345520973205}]}, {"text": " Table 3: Model performance on the unseen evalua- tion set using different feature-selection methods. The  agreement between two human raters for this data is  r resp =0.62 for single responses. The human-human  agreement for the aggregated speaker-level score, r sp ,  was not available for this particular data since only a  small subset of responses were scored by two human  raters. Based on other data from the same test, r sp be- tween two human raters is expected to be around 0.9", "labels": [], "entities": []}, {"text": " Table 4: Relative weights of features representing differ- ent constructs covered by the scoring models.", "labels": [], "entities": [{"text": "Relative", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9463510513305664}]}]}