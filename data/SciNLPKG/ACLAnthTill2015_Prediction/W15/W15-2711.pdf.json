{"title": [], "abstractContent": [{"text": "High agreement is a common objective when annotating data for word senses.", "labels": [], "entities": [{"text": "agreement", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9185475707054138}]}, {"text": "However, a number of factors make perfect agreement impossible, e.g. the limitations of sense inventories, the difficulty of the examples or the interpretation preferences of the annotators.", "labels": [], "entities": []}, {"text": "Estimating potential agreement is thus a relevant task to supplement the evaluation of sense annotations.", "labels": [], "entities": []}, {"text": "In this article we propose two methods to predict agreement on word-annotation instances.", "labels": [], "entities": []}, {"text": "We experiment with a continuous representation and a three-way discretization of observed agreement.", "labels": [], "entities": []}, {"text": "In spite of the difficulty of the task, we find that different levels of agreement can be identified-in particular, low-agreement examples are easier to identify.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sense-annotation tasks show less-than-perfect agreement scores.", "labels": [], "entities": []}, {"text": "However, variation in agreement is not the result of featureless, white noise in the annotations; Krippendorff (2011) defines disagreement as by chance-caused by unavoidable inconsistencies in annotator behavior-and systematic-caused by properties of the data.", "labels": [], "entities": []}, {"text": "Our goal is to predict the agreement of senseannotated examples by examining their linguistic properties.", "labels": [], "entities": []}, {"text": "If we can identify properties predictive of low or high agreement, then we can claim that some of the agreement variation in the data is indeed systematic.", "labels": [], "entities": []}, {"text": "provide an interpretation of Kripperdorff's \u03b1 coefficient to describe the reliability of a whole annotation task and the way that observed agreement (A o ) is calculated for each example.", "labels": [], "entities": [{"text": "reliability", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9591680765151978}, {"text": "observed agreement (A o )", "start_pos": 130, "end_pos": 155, "type": "METRIC", "confidence": 0.8455642312765121}]}, {"text": "Strictly speaking, the value of \u03b1 only provides an indication of the replicability of an annotation task, but we propose that the difficulty of annotating a particular example will influence its local observed agreement.", "labels": [], "entities": []}, {"text": "Thus, easy examples will have a high A o , that will be lower for more difficult examples.", "labels": [], "entities": [{"text": "A o", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9861617982387543}]}, {"text": "Identifying low-agreement examples by their linguistic features would help characterize contexts that make words difficult to annotate.", "labels": [], "entities": []}, {"text": "Estimating the agreement of examples has an immediate application for data collection, as away of estimating the proportion of examples of each difficulty level that one wants to sample.", "labels": [], "entities": []}, {"text": "Moreover, a model of (dis)agreement can help interpret the mispredictions of a word-sense disambiguation system without requiring the data to be multiply annotated.", "labels": [], "entities": []}, {"text": "Observed agreement A o is a continuous-valued variable in the unit interval and we tackle its prediction as a regression task (Section 4.1).", "labels": [], "entities": []}, {"text": "We also experiment with a discretized version of observed agreement into low, mid and high agreement, which is predicted using classification (Section 4.2).", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the scikit-learn 2 implementation for all learning algorithms, and train and test on 10-fold cross validation.", "labels": [], "entities": []}, {"text": "Regression We use L2-regularized linear regression.", "labels": [], "entities": []}, {"text": "The baselines for regression are MEAN, where all instances receive the mean A o of the dataset, and MEDIAN, that assigns the median A o . Classification We use a maximum-entropy classifier.", "labels": [], "entities": [{"text": "MEAN", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.97300785779953}, {"text": "MEDIAN", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9766666293144226}]}, {"text": "The baselines for classification are MFC, where all instances receive most frequent class, and the two random baselines: STRA, where the assigned values are randomly selected via stratified sampling from the distribution of classes in the dataset, and UNI where values are assigned from the uniform distribution of the three labels.", "labels": [], "entities": [{"text": "STRA", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9768029451370239}, {"text": "UNI", "start_pos": 252, "end_pos": 255, "type": "DATASET", "confidence": 0.6824257969856262}]}, {"text": "shows the results for regression in terms of mean absolute error (MAE).", "labels": [], "entities": [{"text": "regression", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9677231907844543}, {"text": "mean absolute error (MAE)", "start_pos": 45, "end_pos": 70, "type": "METRIC", "confidence": 0.9097827076911926}]}, {"text": "This metric is more suitable than root-mean-square error (RMSE) when evaluating regression in the: Mean absolute error of prediction for regression and for mean and median baselines.", "labels": [], "entities": [{"text": "root-mean-square error (RMSE)", "start_pos": 34, "end_pos": 63, "type": "METRIC", "confidence": 0.8487609624862671}, {"text": "Mean absolute error of prediction", "start_pos": 99, "end_pos": 132, "type": "METRIC", "confidence": 0.9101533651351928}]}, {"text": "Datasets where the system outperforms the bestperforming baseline are marked in bold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset characteristics in terms of language, sense inventory, task (al:all-words, ls:lexical sam- ple), no. of sentences, no. of instances, no. of annotators, type of annotators (ex:expert,cs:crowdsourced),  \u03b1, observed agreement and percentage of LOW/MID/HIGH agreement examples.", "labels": [], "entities": []}, {"text": " Table 3: Agreement prediction as classification  compared against the most-frequent, stratified and  uniform baseline. Datasets where the system out- performs the hardest baseline are marked in bold,  error reduction in parentheses.", "labels": [], "entities": [{"text": "Agreement prediction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.772493988275528}, {"text": "error reduction", "start_pos": 202, "end_pos": 217, "type": "METRIC", "confidence": 0.9707424342632294}]}]}