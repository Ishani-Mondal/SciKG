{"title": [{"text": "UMMU@QALB-2015 Shared Task: Character and Word level SMT pipeline for Automatic Error Correction of Arabic Text", "labels": [], "entities": [{"text": "UMMU@QALB-2015", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8777080774307251}, {"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8672897815704346}, {"text": "Automatic Error Correction of Arabic Text", "start_pos": 70, "end_pos": 111, "type": "TASK", "confidence": 0.6072248717149099}]}], "abstractContent": [{"text": "In this paper we present the LIUM (Laboratoire d'Informatique de l'Universit du Maine) and CMU-Q (Carnegie Mel-lon University in Qatar) joint submission in the Arabic shared task on automatic spelling error correction.", "labels": [], "entities": [{"text": "CMU-Q (Carnegie Mel-lon University in Qatar)", "start_pos": 91, "end_pos": 135, "type": "DATASET", "confidence": 0.7831419557332993}, {"text": "automatic spelling error correction", "start_pos": 182, "end_pos": 217, "type": "TASK", "confidence": 0.6494198888540268}]}, {"text": "Our best system is a sequential combination of two statistical machine translation systems (SMT) trained on top of the MADAMIRA output.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6167451937993368}, {"text": "MADAMIRA", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.5397815704345703}]}, {"text": "The first is a Character-based one, used to produce a first correction at the character level.", "labels": [], "entities": []}, {"text": "Characters are then glued to form the input to the second system working at the Word level.", "labels": [], "entities": []}, {"text": "This sequential combination achieves an F 1 score of (69.42) that is better than the best F 1 score reported on the 2014 test set (67.91).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9944453438123068}, {"text": "F 1 score", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9884740114212036}, {"text": "2014 test set", "start_pos": 116, "end_pos": 129, "type": "DATASET", "confidence": 0.7561335364977518}]}, {"text": "The UMMU best submission to the QALB-15 shared task is ranked first over 10 submission on the L2 test condition and second over 12 submission on the L1 testsset.", "labels": [], "entities": [{"text": "UMMU", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8464282155036926}, {"text": "QALB-15 shared task", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.5286485453446707}]}], "introductionContent": [{"text": "Errors such as incorrect spelling, word choice, or grammar, limit the effectiveness of NLP models: language errors are problematic when provided as input to NLP systems, which are often not robust enough to handle unexpected variations.", "labels": [], "entities": []}, {"text": "The difficulty of spelling errors are language-dependent: the more complex the orthography, morphology, or syntax of a language, the more likely it is to have errors in aspects requiring complex human/machine processing.", "labels": [], "entities": []}, {"text": "For morphologically rich languages such as Arabic, spelling errors are very frequent, even among native speakers.", "labels": [], "entities": []}, {"text": "This is because Modern Standard Arabic (MSA), the unifying language of formal text, is not the native language of any Arab . Arabic word morphology is agglutinative: particles and pronouns are written as part of a word.", "labels": [], "entities": []}, {"text": "This adds an additional challenge to the writer (native or non native) and could be a principal source of spelling mistakes.", "labels": [], "entities": []}, {"text": "In this paper, we describe an approach performing a sequential combination of two statistical machine translation systems for automatic spelling error correction for Arabic.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6926450133323669}, {"text": "automatic spelling error correction", "start_pos": 126, "end_pos": 161, "type": "TASK", "confidence": 0.6260343492031097}]}, {"text": "Our system learns models of correction by training on paired examples of errors and their corrections.", "labels": [], "entities": []}, {"text": "The training, tuning and test data are provided by the Shared task organizers.", "labels": [], "entities": []}, {"text": "Compared to the first edition of this shared task, this year's version proposes two sub-tasks tackling two text genres: (1) news corpus (news articles extracted from Aljazeera); (2) a corpus of sentences written by learners of Arabic as a Second Language.", "labels": [], "entities": []}, {"text": "These two corpora are extracted from the QALB corpus ( ).", "labels": [], "entities": [{"text": "QALB corpus", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9652706682682037}]}, {"text": "We tested our system and showed that it performs well on both corpora.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we review the main previous efforts for automatic spelling correction, in Section 3.", "labels": [], "entities": [{"text": "automatic spelling correction", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.6556093394756317}]}, {"text": "We then give an overview of the various spelling mistakes done while writing an Arabic text, in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we detail our error correction system.", "labels": [], "entities": [{"text": "error correction", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.8051767349243164}]}, {"text": "We present in section 6 the results obtained for the different experiments we conducted using the shared task 2015 dev set.", "labels": [], "entities": []}, {"text": "Before concluding, we section 7 details the UMMU official results on QALB-15 test set.", "labels": [], "entities": [{"text": "UMMU", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8384769558906555}, {"text": "QALB-15 test set", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.9449098308881124}]}], "datasetContent": [{"text": "We train four models depending on the used training unit and the nature source side (with or without pre-processing).", "labels": [], "entities": []}, {"text": "Each system is evaluated independently and best systems are combined.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Train, dev and test data distribution", "labels": [], "entities": []}, {"text": " Table 2: F1-score on Dev14, Test14 and L2Dev  obtained using MADAMIRA correction", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991878867149353}, {"text": "Dev14", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9588716626167297}, {"text": "Test14", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.8954442143440247}, {"text": "MADAMIRA", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.8261380791664124}]}, {"text": " Table 3: Word level SMT for spelling error correc- tion.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.6146817207336426}]}, {"text": " Table 4: Character level SMT error correction.", "labels": [], "entities": [{"text": "SMT error correction", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7670659422874451}]}, {"text": " Table 5 and Table 6.", "labels": [], "entities": []}, {"text": " Table 6: Character level SMT error correction  with MADAMIRA pre-process.", "labels": [], "entities": [{"text": "SMT error correction", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8139573931694031}, {"text": "MADAMIRA", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9290741682052612}]}, {"text": " Table 7: Top-down sequential combination", "labels": [], "entities": []}, {"text": " Table 8: Bottom up sequential combination.", "labels": [], "entities": []}, {"text": " Table 9: The UMMU Official results on the 2015  test set. First column shows the system rank ac- cording to the F 1 score.", "labels": [], "entities": [{"text": "UMMU", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.9145609140396118}, {"text": "2015  test set", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.7362722357114156}, {"text": "F 1 score", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9655452370643616}]}, {"text": " Table 10: The UMMU-2 results on the 2015 test- set.", "labels": [], "entities": [{"text": "UMMU-2", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.8280035853385925}, {"text": "2015 test- set", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8706265538930893}]}]}