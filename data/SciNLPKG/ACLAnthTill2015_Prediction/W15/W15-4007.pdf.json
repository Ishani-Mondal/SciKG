{"title": [{"text": "Observed versus latent features for knowledge base and text inference", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we show the surprising effectiveness of a simple observed features model in comparison to latent feature models on two benchmark knowledge base completion datasets, FB15K and WN18.", "labels": [], "entities": [{"text": "FB15K", "start_pos": 179, "end_pos": 184, "type": "DATASET", "confidence": 0.9448955655097961}, {"text": "WN18", "start_pos": 189, "end_pos": 193, "type": "DATASET", "confidence": 0.9348941445350647}]}, {"text": "We also compare latent and observed feature models on a more challenging dataset derived from FB15K, and additionally coupled with textual mentions from a web-scale corpus.", "labels": [], "entities": [{"text": "FB15K", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.9786645770072937}]}, {"text": "We show that the observed features model is most effective at capturing the information present for entity pairs with textual relations, and a combination of the two combines the strengths of both model types.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing information about real-world entities and their relations in structured knowledge bases (KBs) enables numerous applications.", "labels": [], "entities": [{"text": "Representing information about real-world entities and their relations in structured knowledge bases (KBs)", "start_pos": 0, "end_pos": 106, "type": "TASK", "confidence": 0.6654065569241842}]}, {"text": "Large, collaboratively created knowledge bases have become recently available (some examples are Freebase (,, and DBPedia (), but even though they are impressively large, their coverage is far from complete.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9739434719085693}, {"text": "DBPedia", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.916947603225708}]}, {"text": "This has motivated research in automatically deriving new facts to extend a manually built knowledge base, by using information from the existing knowledge base and information from textual mentions of entities in documents.", "labels": [], "entities": []}, {"text": "Many statistical models for predicting new links in knowledge bases have been applied to this task, with most successful ones being latent feature models that learn continuous representations for entities and relations, and observed feature models which predict based on observable * Parts of this research were conducted during the author's internship at Microsoft features in the knowledge graphs.", "labels": [], "entities": [{"text": "predicting new links in knowledge bases", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.8217284878094991}]}, {"text": "Additionally, studies have looked at the contribution of text-based extraction to knowledge base completion (.", "labels": [], "entities": [{"text": "text-based extraction", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7395699322223663}, {"text": "knowledge base completion", "start_pos": 82, "end_pos": 107, "type": "TASK", "confidence": 0.6929120222727457}]}, {"text": "In this paper we compare empirically a very simple observed features model to state-of-the-art latent feature models recently applied to two commonly used datasets for knowledge base completion: a dataset adapted from the Freebase KB, called FB15K () and a dataset derived from the WordNet graph WN18, also introduced in.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 168, "end_pos": 193, "type": "TASK", "confidence": 0.6194352408250173}, {"text": "Freebase KB", "start_pos": 222, "end_pos": 233, "type": "DATASET", "confidence": 0.9658064246177673}, {"text": "FB15K", "start_pos": 242, "end_pos": 247, "type": "DATASET", "confidence": 0.9467206001281738}, {"text": "WordNet graph WN18", "start_pos": 282, "end_pos": 300, "type": "DATASET", "confidence": 0.9443825284639994}]}, {"text": "We show that the simple observed features model substantially outperforms latent feature models, possibly due to the arguably unrealistic redundancy in the KB graphs of these datasets.", "labels": [], "entities": []}, {"text": "Nevertheless, it is intriguing that the latent feature models studied are notable to learn the target concept as well, even given a large number of latent features.", "labels": [], "entities": []}, {"text": "We also construct a harder, perhaps more realistic dataset derived from FB15K, in which we remove near-duplicate or inverse-duplicate relations.", "labels": [], "entities": [{"text": "FB15K", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.9730353951454163}]}, {"text": "We show that in this new dataset our studied latent feature models substantially outperform the observed feature models.", "labels": [], "entities": []}, {"text": "When we augment the newly constructed dataset with textual mentions derived from the ClueWeb 12 web-scale document collection, we see that the observed features model is more powerful than the latent feature models, but also that a combination of the two is superior to either of them.", "labels": [], "entities": [{"text": "ClueWeb 12 web-scale document collection", "start_pos": 85, "end_pos": 125, "type": "DATASET", "confidence": 0.9317736744880676}]}], "datasetContent": [{"text": "We perform experiments with latent and observed feature models and their combination.", "labels": [], "entities": []}, {"text": "We first present results on the FB15K dataset, which was originality constructed (using Freebase) by and was subsequently used in several research studies (.", "labels": [], "entities": [{"text": "FB15K dataset", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9894379079341888}]}, {"text": "The number of relations and triples in the training, development and test portions of the datasets are given in.", "labels": [], "entities": []}, {"text": "Given a set of triples in a set disjoint from a training knowledge graph, we test models on predicting the subject or object of each triple, given the relation type and the other argument.", "labels": [], "entities": []}, {"text": "We rank all entities in the training knowledge base in order of their likelihood of filling the argument position.", "labels": [], "entities": []}, {"text": "We report the mean reciprocal rank of the correct entity, as well as HITS@10 -the percent of test triples for which the correct argument was ranked in the top ten.", "labels": [], "entities": [{"text": "mean reciprocal rank", "start_pos": 14, "end_pos": 34, "type": "METRIC", "confidence": 0.7696626385052999}, {"text": "HITS@10 -", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9659087508916855}]}, {"text": "We use filtered measures following the protocol proposed in -that is, when we rank entities fora given position, we remove all other entities that are known to be part of an existing triple in the training, development, or test set.", "labels": [], "entities": []}, {"text": "This avoids penalizing the model for ranking other correct fillers higher than the tested argument.", "labels": [], "entities": []}, {"text": "We thus report filtered mean reciprocal rank (labeled MRR in the Figures), and filtered HITS@10.", "labels": [], "entities": [{"text": "mean reciprocal rank", "start_pos": 24, "end_pos": 44, "type": "METRIC", "confidence": 0.8144981463750204}, {"text": "MRR", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.8827928304672241}, {"text": "HITS", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9916008710861206}]}, {"text": "In the figures we present MRR values scaled by 100, so that the maximum possible MRR is 100.", "labels": [], "entities": [{"text": "MRR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9631973505020142}, {"text": "MRR", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.974460244178772}]}, {"text": "We present experiments of different models introduced in this paper on these datasets, and additionally include results reported in prior work.", "labels": [], "entities": []}, {"text": "We also evaluate the impact of our use of types as hard constraints in training and testing, and how these constraints impact latent feature models versus observed feature models.", "labels": [], "entities": []}, {"text": "presents the results under two settings: using automatically derived types versus not using them.", "labels": [], "entities": []}, {"text": "The results not using types are presented in the right half of the The first six rows report performance measures obtained using latent feature models.", "labels": [], "entities": []}, {"text": "The first three models presented are the ones defined in Section 3.2 and implemented in this work.", "labels": [], "entities": []}, {"text": "We evaluate these models when type constraints are used or when they are not used.", "labels": [], "entities": []}, {"text": "The next three rows report results from prior work by directly copying reported numbers from the respective papers.", "labels": [], "entities": []}, {"text": "Since these papers did not use type constraints, we list the results in the right two columns only.", "labels": [], "entities": []}, {"text": "The model TransE was proposed in () but we use the results from the implementation of (, because these results were higher.", "labels": [], "entities": []}, {"text": "The TransH (bern.) results are obtained by the model presented in ().", "labels": [], "entities": []}, {"text": "The last three rows show results from observed feature models, as defined in Section 3.1, where the first model uses only node features, the second uses only direct link features, and the third uses both feature types.", "labels": [], "entities": []}, {"text": "The type constraints were defined using the method presented in Section 3.3.1.", "labels": [], "entities": []}, {"text": "We choose the best settings for the method based on coverage of the correct triples in the validation set.", "labels": [], "entities": []}, {"text": "Given the hard pruning of candidates by type filtering, the method using types has less than 100 percent achievable accuracy -the oracle HITS@10 by using type constraints is 98.3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9892300963401794}, {"text": "HITS", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9600841999053955}]}, {"text": "We found that the number of latent features did not have a large impact on performance for model E, but did have large impact for the other two models.", "labels": [], "entities": []}, {"text": "Using 500 hidden dimensions was optimal for these two models.", "labels": [], "entities": []}, {"text": "Even though the form of the scoring function for DISTMULT is exactly the same as defined in (, we obtain much higher performance.", "labels": [], "entities": [{"text": "DISTMULT", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.6350440979003906}]}, {"text": "We attribute this to the larger number of hidden dimensions (500 vs 100), and the use of the softmax-based loss function with 200 negative examples and batch training.", "labels": [], "entities": []}, {"text": "1 As seen, the impact of the type constraints is large and positive, especially on the MRR values.", "labels": [], "entities": [{"text": "MRR", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.4159596264362335}]}, {"text": "Our implementation of these embedding models outperforms the other recent results by TransH (), which we also attribute to the loss function and optimization.", "labels": [], "entities": []}, {"text": "The most striking result on this dataset is seen in the last two rows of the  seethe performance of the observed feature models based on direct links.", "labels": [], "entities": []}, {"text": "The performance of these models (in MRR) is much higher that the performance of the latent feature models obtained in this work and in prior work.", "labels": [], "entities": [{"text": "MRR", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.6826679706573486}]}, {"text": "This is perhaps not so surprising when we look at the number of test set triples (e 1 , r, e 2 ) for which either (e 1 , r , e 2 ) or (e 2 , r , e 1 ) occur in the training set -i.e., which are directly linked in the training knowledge graph.", "labels": [], "entities": []}, {"text": "This number is almost 81% (reported in), and explains why the observed features model which uses this information directly can do so well.", "labels": [], "entities": []}, {"text": "What is more surprising is that latent feature models have not approached this performance, even given a large number of latent feature dimensions.", "labels": [], "entities": []}, {"text": "We see this is an interesting datapoint which can motivate analysis and improvement in the state-of-the-art in knowledge base completion using latent variable models.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.6227600673834482}]}, {"text": "Two other interesting results from these experiments are that the observed feature model using only entity features (NodeFeat) has almost the same performance as the latent feature model E and both can be seen as learning a unigram distribution over entities for argument positions of relations.", "labels": [], "entities": []}, {"text": "Additionally, the observed feature models are not substantially affected by the use of type constraints, since they effectively learn to model similar type concepts using the features.", "labels": [], "entities": []}, {"text": "We also tested the models on WN18, and report results from prior work using latent feature models as well as our implementation of the observed feature models in.", "labels": [], "entities": [{"text": "WN18", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9730730652809143}]}, {"text": "As seen, the observed feature models using link features strongly outperform prior work in the MRR measure (achieving around 45% error reduction over the best previously reported results), and are comparable to the best models according to the HITS@10 measure.", "labels": [], "entities": [{"text": "MRR", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.5585788488388062}, {"text": "error reduction", "start_pos": 129, "end_pos": 144, "type": "METRIC", "confidence": 0.9507012069225311}, {"text": "HITS@10 measure", "start_pos": 244, "end_pos": 259, "type": "DATASET", "confidence": 0.7578995674848557}]}, {"text": "As shown in, 94.0 of test triple entities are directly linked in the training KB, explaining the success of these simple models.", "labels": [], "entities": []}, {"text": "Given our analysis of the FB15k and WN18 datasets and the power of a simple observed features model, we are motivated to construct a more realistic knowledge base completion dataset for which we can assume that trivially entailed facts (due to relation symmetry or the presence of inverse relations) have already been inferred and the task is to entail facts requiring non-trivial inference.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.9773470759391785}, {"text": "WN18 datasets", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8225910067558289}]}, {"text": "To this effect we construct a subset of FB15K, which we term FB15KSelected, and which represents a more challenging learning setting.", "labels": [], "entities": [{"text": "FB15K", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.89995938539505}, {"text": "FB15KSelected", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.8718896508216858}]}, {"text": "The dataset FB15KSelected 2 was constructed by first limiting the set of relations in FB15K to the most frequently used 401 relations (a setting using this subset of frequent relations was also used in ().", "labels": [], "entities": [{"text": "FB15KSelected", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.958712100982666}, {"text": "FB15K", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.9838435053825378}]}, {"text": "We then automatically detected near-duplicate and inverse relations by checking whether the set of entity pairs in the relations is either almost the same (at least 97% of the pairs are in the intersection), or whether the set of inverse entity pairs is almost the same e.g. comparing [e 1 , e 2 ] for r to the set [e 2 , e 1 ] for r . For example, this process detected that the relation /award/award nominee is inverse of /award nominee/award.", "labels": [], "entities": []}, {"text": "Given this information, we filtered the set of relations to keep only one of a set of inverse or duplicate relations; this resulted in 237 relations, and we limited the training, validation, and development set triples to these relations.", "labels": [], "entities": []}, {"text": "We also filtered from the validation and test sets any triples whose entity pairs were directly linked in the training database.", "labels": [], "entities": []}, {"text": "Such direct links could admittedly be legitimately present in a realistic scenario but we excluded them to avoid additional trivial cases which could have not been detected via the prior filtering step.", "labels": [], "entities": []}, {"text": "The statistics for this resulting dataset are shown in.", "labels": [], "entities": []}, {"text": "While for this more realistic dataset we have excluded all direct KB links for test entity pairs, there is a realistic source of direct relations between test entity pairs -textual relations expressed by sentences containing these pairs of entities.", "labels": [], "entities": []}, {"text": "We use the ClueWeb12 3 corpus coupled with Freebase mention annotations () to extract textual relations for all entity pairs in the knowledge base.", "labels": [], "entities": [{"text": "ClueWeb12 3 corpus", "start_pos": 11, "end_pos": 29, "type": "DATASET", "confidence": 0.812091072400411}]}, {"text": "We extract textual patterns from 200 million dependency-parsed sentences and we represent the textual relations via the fully lexicalized dependency path connecting two entities, as shown in.", "labels": [], "entities": []}, {"text": "After pruning, we use 25,000 unique textual relations and add links to the training knowledge graph based on these relations.", "labels": [], "entities": []}, {"text": "There are 6.6 million links induced from the textual relations for the FB15KSelected knowledge base.", "labels": [], "entities": [{"text": "FB15KSelected knowledge base", "start_pos": 71, "end_pos": 99, "type": "DATASET", "confidence": 0.9855635762214661}]}, {"text": "Of the test KB triples, 23.3% of the entity pairs have textual mentions.", "labels": [], "entities": []}, {"text": "For the training set, 31% of the entity pairs that have a KB link have a textual mention, and having a mention increases the chance that a random entity pair has a relation from .1% to 4.2% -a forty-fold increase.", "labels": [], "entities": []}, {"text": "shows the results for this dataset -the upper portion contains results for models not using textual mentions, and the lower part contains results of models also using the text.", "labels": [], "entities": []}, {"text": "The results are shown using the MRR and HITS@10 measures, and these are further broken down into overall/with textual mentions/without textual mentions (a/t/nt).", "labels": [], "entities": [{"text": "MRR", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9017412662506104}, {"text": "HITS@10", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.946394662062327}]}, {"text": "For the setting where no textual mentions are used, we see that the latent feature models outperform the observed feature models (since there are no direct links in the training set for test triples, the observed features model LinkFeat has performance which is random subject to the type constraints (and where ties are broken by order of appearance in the training set)).", "labels": [], "entities": []}, {"text": "Using node features only is best for the observed feature models, and the overall MRR of this model (23.5), is substantially below that of the best latent feature model, E+DISTMULT with overall MRR of 26.6.", "labels": [], "entities": [{"text": "MRR", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9990634322166443}, {"text": "DISTMULT", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.5224428772926331}, {"text": "MRR", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.9983265995979309}]}, {"text": "A model which combines the latent and observed features (shown in row 7), does not bring substantial improvement.", "labels": [], "entities": []}, {"text": "The second (lower) part of the Figure shows model results when the training knowledge graph is expanded with textual relations.", "labels": [], "entities": []}, {"text": "First, for the best latent feature model E+DISTMULT which treats knowledge base and textual relations uniformly, using \u03c4 = 1 as in the universal schema approach (, we see no improvement from using the textual mentions.", "labels": [], "entities": []}, {"text": "Indeed, there is an improvement in MRR on the test triples that have mentions (23.1 to 24.0), but performance degrades on the more numerous test cases without mentions.", "labels": [], "entities": [{"text": "MRR", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9176955223083496}]}, {"text": "When \u03c4 is optimized via grid search to a value of \u03c4 = .1 we see a good improvement to overall MRR 27.4 due to using text, which holds for cases with mentions as well as ones without mentions.", "labels": [], "entities": [{"text": "MRR 27.4", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.5519032031297684}]}, {"text": "The observed features model benefits from text strongly, and in particular the MRR on test triples with mentions increases from 19.3 to 39.6.", "labels": [], "entities": [{"text": "MRR", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.98099285364151}]}, {"text": "The performance on triples without mentions is very low, however.", "labels": [], "entities": []}, {"text": "Since the latent and observed feature models have complementary strengths, their combination (last row in the substantially outperforms both kinds of models, reaching an overall MRR of 29.3 and overall HITS@10 of 46.2.", "labels": [], "entities": [{"text": "MRR", "start_pos": 178, "end_pos": 181, "type": "METRIC", "confidence": 0.9979962706565857}, {"text": "HITS", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9994072914123535}]}, {"text": "The MRR on test triples with mentions is almost doubled compared to the models not using text.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9910466074943542}]}], "tableCaptions": []}