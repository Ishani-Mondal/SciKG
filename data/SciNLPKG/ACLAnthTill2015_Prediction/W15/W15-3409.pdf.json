{"title": [{"text": "Extracting Bilingual Lexica from Comparable Corpora Using Self-Organizing Maps", "labels": [], "entities": [{"text": "Extracting Bilingual Lexica from Comparable Corpora Using Self-Organizing Maps", "start_pos": 0, "end_pos": 78, "type": "TASK", "confidence": 0.8290669123331705}]}], "abstractContent": [{"text": "This paper aims to present a novel method of extracting bilingual lexica from comparable corpora using one of the artificial neural network algorithms, self-organizing maps (SOMs).", "labels": [], "entities": []}, {"text": "The proposed method is very useful when a seed dictionary for translating source words into target words is insufficient.", "labels": [], "entities": [{"text": "translating source words into target words", "start_pos": 62, "end_pos": 104, "type": "TASK", "confidence": 0.8161664108435313}]}, {"text": "Our experiments have shown stunning results when contrasted with one of the other approaches.", "labels": [], "entities": []}, {"text": "For future work, we need to fine-tune various parameters to achieve stronger performances.", "labels": [], "entities": []}, {"text": "Also we should investigate how to construct good synonym vectors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual lexicon extraction from comparable corpora has been studied by many researchers since the late 1990s.", "labels": [], "entities": [{"text": "Bilingual lexicon extraction from comparable corpora", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8640124301115671}]}, {"text": "To our knowledge, one of the basic approaches is the context vector-based approach) called the standard approach in the literatures, and many other studies have been derived from this approach.", "labels": [], "entities": []}, {"text": "Some of these are concerned with similarity score measurement, the size of the context window, and the size of the seed dictionary).", "labels": [], "entities": []}, {"text": "The extended approach, one of such approaches,) has been proposed in order to reduce the load on the seed dictionary.", "labels": [], "entities": []}, {"text": "It gathers k nearest neighbors to augment the context of the word to be translated.", "labels": [], "entities": []}, {"text": "In spite of their efforts, using comparable corpora for extracting such lexica yields quite poor performances unless orthographic features are used.", "labels": [], "entities": []}, {"text": "However, such features may bring other costs.", "labels": [], "entities": []}, {"text": "Under the circumstances like this, this paper is motivated to propose an efficient method in which comparable corpora with a minimum of resources are considered for extracting bilingual lexica.", "labels": [], "entities": [{"text": "extracting bilingual lexica", "start_pos": 165, "end_pos": 192, "type": "TASK", "confidence": 0.8505722284317017}]}, {"text": "The SOM-based approach, we propose in this paper, can yield stronger performances with the same experimental circumstance than earlier studies can do.", "labels": [], "entities": [{"text": "SOM-based", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9730726480484009}]}, {"text": "In order to show this, we compare the proposed method to the standard approach.", "labels": [], "entities": []}, {"text": "Of course, it does not meaning our method outperforms for every data.", "labels": [], "entities": []}, {"text": "We just show the proposed method is reasonable for this field.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: Section 2 presents several works closely related to our method.", "labels": [], "entities": []}, {"text": "Section 3 describes our method (the SOM-based approach) in more detail.", "labels": [], "entities": [{"text": "SOM-based", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.9535226821899414}]}, {"text": "Section 4 shows experimental results with discussions, and Section 5 concludes the paper and presents future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we evaluate the proposed method for two language pairs -Korean-French (KR-FR) and Korean-Spanish (KR-ES).", "labels": [], "entities": []}, {"text": "Regarding the comparison, we implemented the standard approach mentioned in Section 2.1.", "labels": [], "entities": []}, {"text": "Note that the standard approach implemented here is not complete.", "labels": [], "entities": []}, {"text": "There are many chances to show better performances by fine-tuning several parameters, such as the size of the context window, and association/similarity measures.", "labels": [], "entities": [{"text": "association/similarity", "start_pos": 130, "end_pos": 152, "type": "METRIC", "confidence": 0.7771459619204203}]}, {"text": "However, we can briefly estimate them because both methods are implemented by using same resources.", "labels": [], "entities": []}, {"text": "Several parameters are fixed as follows: the context size of the window as 5, and the association measure as a chi-square test, and the similarity measure as a cosine similarity.", "labels": [], "entities": [{"text": "association measure", "start_pos": 86, "end_pos": 105, "type": "METRIC", "confidence": 0.9315600395202637}, {"text": "similarity measure", "start_pos": 136, "end_pos": 154, "type": "METRIC", "confidence": 0.9556138217449188}]}, {"text": "These measures were empirically chosen from our experimental data.", "labels": [], "entities": []}, {"text": "We used three comparable corpora () in Korean, French, and Spanish.", "labels": [], "entities": []}, {"text": "Each corpus included around 800k sentences collected from the Web 1 . The Korean corpus consists of news articles and some are derived from different sources).", "labels": [], "entities": [{"text": "Korean corpus", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9374524652957916}]}, {"text": "The others also consists of news articles (around 400k sentences), and some are combined with the European parliament proceedings (400k randomly sampled sentences) ().", "labels": [], "entities": []}, {"text": "The Korean corpus has around 280k word types (180k for French and 185k for Spanish), and the average number of words per sentence is 16.2 (15.9 for French and 16.1 for Spanish).", "labels": [], "entities": [{"text": "Korean corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7477168142795563}]}, {"text": "Consequently, the balance of three corpora is well-formed.", "labels": [], "entities": []}, {"text": "We extracted nouns from these corpora for our test sets as well as input data.", "labels": [], "entities": []}, {"text": "We considered only nouns to reduce the sizes of the dimensions of either synonym vectors or SOMs.", "labels": [], "entities": []}, {"text": "Thus, we finally collected almost 190k Korean noun types (45k for French and 58k for Spanish).", "labels": [], "entities": []}, {"text": "The reason why the number of Korean noun types was higher than others was due to Korean characteristics.", "labels": [], "entities": []}, {"text": "We should split the Korean words into morpheme units because there area lot of compound words and omitted morphemes.", "labels": [], "entities": []}, {"text": "Furthermore, we collected very finely segmented Korean nouns to eliminate indulgent compound nouns that were possibly missed during a word segmentation task.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.7965175608793894}]}, {"text": "All collected nouns were considered candidates of both test sets and seed words independently.", "labels": [], "entities": []}, {"text": "After the input data was prepared, we built synonym vectors, as mentioned previously.", "labels": [], "entities": []}, {"text": "We already introduced the method how to construct synonym vectors.", "labels": [], "entities": []}, {"text": "However, this paper doesn't mainly propose the efficient way of representing words semantically in vector spaces.", "labels": [], "entities": []}, {"text": "If synonym vectors are built based on context vectors and their similarity scores, the size of the vector dimension would be very huge.", "labels": [], "entities": []}, {"text": "It would cause many timeconsuming problems.", "labels": [], "entities": []}, {"text": "In this paper, we simply use word2vec 2 to build synonym vectors.", "labels": [], "entities": []}, {"text": "As far as we know, word2vec cannot yield semantically related vectors as output.", "labels": [], "entities": []}, {"text": "However, we used this tool to reduce vector sizes and assume these outputs (i.e. vectors) are reasonable as the input data for training SOMs.", "labels": [], "entities": []}, {"text": "Some parameters for building synonym vectors can be presented as follows: window size is 5, word vector size is 100, and training iteration is 100.", "labels": [], "entities": []}, {"text": "We manually built evaluation dictionaries to evaluate our method because such dictionaries for KR-FR/-ES are publicly unavailable.", "labels": [], "entities": []}, {"text": "Each dictionary contains 200 high-frequency nouns.", "labels": [], "entities": []}, {"text": "The reason why we picked high-frequency nouns is that these nouns have more chances to have neighbors than low-frequency words.", "labels": [], "entities": []}, {"text": "In order to evaluate whether the proposed approach is valid (i.e. whether trained SOM can extract new input data that not trained), we need to train words having many neighbors.", "labels": [], "entities": []}, {"text": "These 200 source words were selected if actual translations were in their corpora.", "labels": [], "entities": []}, {"text": "Thus, the 200th source word did not indicate a 200th high-frequency word.", "labels": [], "entities": []}, {"text": "The KR\u2192FR 3 dictionary had total of 288 translations (451 translations in the FR\u2192KR dictionary), and the KR\u2192ES dictionary contained 377 translations (687 translations in the ES\u2192KR dictionary).", "labels": [], "entities": [{"text": "KR\u2192FR 3 dictionary", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.6181544899940491}, {"text": "FR\u2192KR dictionary", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.8546339571475983}, {"text": "KR\u2192ES dictionary", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.6869538128376007}, {"text": "ES\u2192KR dictionary", "start_pos": 174, "end_pos": 190, "type": "DATASET", "confidence": 0.6811414957046509}]}, {"text": "Additionally, regretfully, there were several duplicated translations for every language.", "labels": [], "entities": []}, {"text": "In the case of KR-FR, the Korean words had 447 French translations (420 types) and the French words had 209 Korean translations (189 types).", "labels": [], "entities": []}, {"text": "In the case of KR-ES, the Korean words had 456 Spanish translations (369 types) and the Spanish words had 509 Korean translations (421 types).", "labels": [], "entities": []}, {"text": "We did not perform any heuristic process to give each source word a unique sense.", "labels": [], "entities": []}, {"text": "Instead, we assumed related source words corresponding to a single translation were semantically the same.", "labels": [], "entities": []}], "tableCaptions": []}