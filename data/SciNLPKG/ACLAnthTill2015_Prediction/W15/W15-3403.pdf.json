{"title": [{"text": "Knowledge-lean projection of coreference chains across languages", "labels": [], "entities": []}], "abstractContent": [{"text": "Common technologies for automatic coreference resolution require either a language-specific rule set or large collections of manually annotated data, which is typically limited to newswire texts in major languages.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8786399364471436}]}, {"text": "This makes it difficult to develop coreference resolvers fora large number of the so-called low-resourced languages.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.9371387362480164}]}, {"text": "We apply a direct projection algorithm on a multi-genre and multilingual corpus (English, German, Russian) to automatically produce coreference annotations for two target languages without exploiting any linguistic knowledge of the languages.", "labels": [], "entities": []}, {"text": "Our evaluation of the projected annotations shows promising results, and the error analysis reveals structural differences of referring expressions and coreference chains for the three languages , which can now be targeted with more linguistically-informed projection algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference resolution requires relatively expensive resources, usually in terms of manual annotation.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9139955937862396}]}, {"text": "To alleviate this problem for low-resourced languages, techniques of annotation projection can be applied.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.706660121679306}]}, {"text": "In this paper, we report on experiments with projecting nominal coreference chains across bilingual corpora.", "labels": [], "entities": []}, {"text": "Our goal is to see how well a knowledge-lean projection algorithm works for two relatively similar languages (English-German) and for less similar languages (English-Russian).", "labels": [], "entities": [{"text": "knowledge-lean projection", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6633888185024261}]}, {"text": "Furthermore, we are interested in differences incurred by the text genre and therefore use three different genres: argumentative newspaper articles, narratives, and medicine instruction leaflets.", "labels": [], "entities": []}, {"text": "Our general aim is to explore the limitations of a knowledge-lean approach to the problem, so that it is easy to generalize to other low-resourced languages.", "labels": [], "entities": []}, {"text": "For the annotation of the corpus, we created common annotation guidelines that make few assumptions on the structural features of the target languages.", "labels": [], "entities": []}, {"text": "We used the guidelines to annotate texts of the three genres in the three languages, and provide results on inter-annotator agreement (see Section 3).", "labels": [], "entities": []}, {"text": "For projection, we use a procedure based on sentence and word alignment as calculated by a standard tool (GIZA++) that was trained on corpora of moderate size.", "labels": [], "entities": [{"text": "projection", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9715980887413025}, {"text": "sentence and word alignment", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6085550561547279}]}, {"text": "Thus at this point we deliberately do not apply linguistic knowledge on the languages involved.", "labels": [], "entities": []}, {"text": "The experiments and results are described in Section 4.", "labels": [], "entities": []}, {"text": "We present a qualitative error analysis showing that a number of structural divergences are responsible for many of the problems; this suggests that limited syntactic knowledge can be helpful for improving performance in follow-up work.", "labels": [], "entities": []}, {"text": "Section 5 compares our results to the most closely related earlier work, and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate both the quality of the identification of mentions and the extraction of coreference chains using the CoNLL scorer 7 . 1. Evaluation of the identification of mentions.", "labels": [], "entities": [{"text": "CoNLL scorer 7", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.8038484255472819}]}, {"text": "We compute the scores for the identification of mentions using the strict mention matching as in the) and CONLL-2012 shared tasks (Pradhan et al., 2012), so that we score only those projected markable spans that are exactly the same as the gold ones.", "labels": [], "entities": [{"text": "CONLL-2012 shared tasks", "start_pos": 106, "end_pos": 129, "type": "DATASET", "confidence": 0.8092686136563619}]}, {"text": "The values for English-German and English-Russian are given in as mentions.) to get complete performance characteristics.", "labels": [], "entities": []}, {"text": "We also use strict matching as in the evaluation of the identification of mentions and evaluate the projected markables against all the markables of the gold standard.", "labels": [], "entities": []}, {"text": "These scores depend on the identification of mentions evaluated in the previous step.", "labels": [], "entities": []}, {"text": "We report the micro-averaged Precision, Recall and F-1 scores in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9597135186195374}, {"text": "Recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9771037697792053}, {"text": "F-1", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9914613962173462}]}, {"text": "In addition, shows the distribution of macro-averaged F1-scores for two of the metrics (MUC and B 3 ) for both language pairs as boxplots.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.8943327069282532}, {"text": "MUC", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.8373302221298218}]}, {"text": "3: Results for German and Russian: micro-averaged Precision, Recall, F1-score for different genres noun, pronoun or numeral as head; otherwise, the RE is discarded.", "labels": [], "entities": [{"text": "Precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.891128420829773}, {"text": "Recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9910128712654114}, {"text": "F1-score", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9932951331138611}, {"text": "RE", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.9829763174057007}]}, {"text": "Results are given in Table 6 with the tag 'min'.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the experimental corpus", "labels": [], "entities": []}, {"text": " Table 2: Types of NPs in the three genres (%)", "labels": [], "entities": []}, {"text": " Table 3. For the medical leaflets, the results are  somewhat lower: \u03ba = 0.76 with binary overlap and  0.67 with proportional overlap; the MUC score is  70%. For the NP type attribute, Cohen's kappa for  the texts from all genres on average is \u03ba = 0.94.", "labels": [], "entities": [{"text": "MUC score", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9557059407234192}]}, {"text": " Table 3: Inter-annotator agreement for news and  stories", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of the automatic word align- ment", "labels": [], "entities": []}, {"text": " Table 5: Number of REs and coreference chains transferred through bilingual projections", "labels": [], "entities": []}, {"text": " Table 6: Results for German and Russian: micro-averaged Precision, Recall, F1-score for different  genres", "labels": [], "entities": [{"text": "Precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9286569952964783}, {"text": "Recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9918586611747742}, {"text": "F1-score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9977935552597046}]}]}