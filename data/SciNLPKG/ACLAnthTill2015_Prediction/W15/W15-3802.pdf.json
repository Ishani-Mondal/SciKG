{"title": [{"text": "Making the most of limited training data using distant supervision", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic recognition of relationships between key entities in text is an important problem which has many applications.", "labels": [], "entities": [{"text": "Automatic recognition of relationships between key entities in text", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8786298500166999}]}, {"text": "Supervised machine learning techniques have proved to be the most effective approach to this problem.", "labels": [], "entities": []}, {"text": "However, they require labelled training data which may not be available in sufficient quantity (or at all) and is expensive to produce.", "labels": [], "entities": []}, {"text": "This paper proposes a technique that can be applied when only limited training data is available.", "labels": [], "entities": []}, {"text": "The approach uses a form of distant supervision but does not require an external knowledge base.", "labels": [], "entities": []}, {"text": "Instead, it uses information from the training set to acquire new labelled data and combines it with manually labelled data.", "labels": [], "entities": []}, {"text": "The approach was tested on an adverse drug data set using a limited amount of manually labelled training data and shown to outperform a supervised approach.", "labels": [], "entities": [{"text": "adverse drug data set", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.7746544778347015}]}], "introductionContent": [{"text": "Relation extraction is a widely explored problem that has been applied to a range of domains () using a variety of techniques;).", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9673584401607513}]}, {"text": "In the biomedical domain relation extraction has been used to identify a wide range of types of relation, including adverse drug effects (ADE), gene regulations and drug-drug interactions.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.796572208404541}]}, {"text": "Community evaluation exercises, such as the BioNLP Shared Task or the Drug-Drug Interaction (DDI) challenge (), have shown that supervised learning techniques normally produce better results than other approaches.", "labels": [], "entities": []}, {"text": "Supervised learning techniques rely on labeled training data but these are not available for all relations of interest and are also difficult and time-consuming to create.", "labels": [], "entities": []}, {"text": "Other approaches maybe more appropriate in situations where training data is limited or unavailable.", "labels": [], "entities": []}, {"text": "Minimally supervised approaches, such as seed and bootstrapping techniques), are provided with a small set of seed instances (examples of related information) or patterns and acquire further examples from a large corpus by applying an iterative process.", "labels": [], "entities": []}, {"text": "While these approaches do not require labelled training data they often suffer from low precision or semantic drift (.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9984806180000305}]}, {"text": "Distant supervision combines the advantages of minimally supervised and supervised approaches to relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.9180931746959686}]}, {"text": "Distant supervision makes use of an external knowledge source that provides information about pairs of entities which are related.", "labels": [], "entities": []}, {"text": "Sentences containing both entities in a pair are identified from a corpus and used in place of labeled training examples.", "labels": [], "entities": []}, {"text": "For example, knowledge that hair loss is a drug-related adverse effect of paroxetine would allow further positive examples to be identified by searching for other sentences containing the same drug and side-effect.", "labels": [], "entities": []}, {"text": "Many knowledge sources only contain positive entity pairs.", "labels": [], "entities": []}, {"text": "Therefore negative examples are often generated using a closedworld assumption.", "labels": [], "entities": []}, {"text": "Given the known positive entity pairs, negative entity pairs are generated by producing new combinations of entities.", "labels": [], "entities": []}, {"text": "Negative example sentences are generated by selecting sentences containing these negative entity pairs.", "labels": [], "entities": []}, {"text": "The example in shows the limitations of distant supervision since related entities might express a different relation.", "labels": [], "entities": []}, {"text": "This can lead to examples being falsely labelled as positive examples of a relation.", "labels": [], "entities": []}, {"text": "Classifiers trained using data generated using distant supervision do not generally perform as well as those trained using manually labelled data.", "labels": [], "entities": []}, {"text": "However, distant supervision allows large data sets to be generated at low cost.", "labels": [], "entities": []}, {"text": "The majority of distant supervision approaches use structured knowledge sources such as Wikipedia () or Freebase ().", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.9503361582756042}]}, {"text": "However there may not be a suitable knowledge base available fora particular relation of interest.", "labels": [], "entities": []}, {"text": "This paper addresses the problem of developing relation extraction systems in situations where only a small amount of training data is available.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.871501624584198}]}, {"text": "We introduce a method for relation extraction that can be used when only limited amounts of training data are available.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.9409711956977844}]}, {"text": "The approach is based on distant supervision but, rather than relying on a knowledge base, seed pairs are extracted from Medline articles.", "labels": [], "entities": [{"text": "Medline articles", "start_pos": 121, "end_pos": 137, "type": "DATASET", "confidence": 0.9627259075641632}]}, {"text": "Sentences from the Medline Baseline Repository containing these seed pairs are extracted to generate a large distantly labelled training data set.", "labels": [], "entities": [{"text": "Medline Baseline Repository", "start_pos": 19, "end_pos": 46, "type": "DATASET", "confidence": 0.9308648308118185}]}, {"text": "Using this data manually labelled data can be extended and combined to a hybrid mixture model which outperforms both the supervised and the distantly supervised models.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: 1) introduces a method which can be used to train a relational classifier when only a small set of labelled training data is available, 2) provides a method for combining distant supervision with supervised learning methods and 3) presents distant supervision without the need of a knowledge base.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "The next section presents the background on relation extraction from biomedical documents.", "labels": [], "entities": [{"text": "relation extraction from biomedical documents", "start_pos": 44, "end_pos": 89, "type": "TASK", "confidence": 0.89519362449646}]}, {"text": "Section 3 introduces the data set which is used for the experiments.", "labels": [], "entities": []}, {"text": "The techniques for generating the distantly supervised training data and relational classifier are described in sections 4 and 5.", "labels": [], "entities": []}, {"text": "Section 6 describes the experiment and the results.", "labels": [], "entities": []}, {"text": "Conclusions are presented in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment we examine different sizes of manually labelled training data.", "labels": [], "entities": []}, {"text": "Starting with a single abstract for training we slowly increase the number of seed abstracts to 200.", "labels": [], "entities": []}, {"text": "In parallel we generate for each training set a different distantly labelled data set using the given ADE seed facts of the training data.", "labels": [], "entities": []}, {"text": "The more information the manually labelled data contains, the more different seeds can be extracted which increases the size of the distantly labelled data.", "labels": [], "entities": []}, {"text": "Thereafter we combine in each step both data sets to a mixture-model.", "labels": [], "entities": []}, {"text": "In order to provide reliable results we repeat this experiment five times (five evaluation rounds) with a different selection of abstracts for training and test.", "labels": [], "entities": []}, {"text": "In each evaluation round the abstracts utilised for training are chosen randomly.", "labels": [], "entities": []}, {"text": "The results show that the performance for all models improves as the amount of data increases.", "labels": [], "entities": []}, {"text": "Performance of the supervised classifier increases sharply as the number of abstracts is increased from 1 to 10 abstracts.", "labels": [], "entities": []}, {"text": "Increasing the size of the training data to 50 abstracts produces a further improvement of approximately 30%.", "labels": [], "entities": []}, {"text": "These results demonstrate that even small amounts of training data are sufficient to provide reasonable results on the ADE data set.", "labels": [], "entities": [{"text": "ADE data set", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.9269511898358663}]}, {"text": "Performance of the distantly supervised classifier shows a similar pattern.", "labels": [], "entities": []}, {"text": "Increasing the number of seed abstracts results in a larger distantly labelled training data set which improves classification results.", "labels": [], "entities": []}, {"text": "The distantly supervised classifier outperforms the supervised one when there are fewer than 100 seed abstracts.", "labels": [], "entities": []}, {"text": "The reason for this is the supervised classifier does not have access to a sufficient volume of training data while the distant supervision is able to generate more.", "labels": [], "entities": []}, {"text": "As the number of seed abstracts increases the situation is reversed with the supervised classifier outperforming the distantly supervised one.", "labels": [], "entities": []}, {"text": "When more than 100 abstracts are available the supervised classifier has the advantage of having access to enough accurately labelled examples to train a relation ex-   The mixture model produces the best results of all approaches when 5 or more abstracts are used.", "labels": [], "entities": []}, {"text": "This result is interesting since the manually labelled data is simply extended using a simple form of distant supervision that is straightforward to apply.", "labels": [], "entities": []}, {"text": "The mixture model tends to achieve higher precision but lower recall than the distantly supervised approach, possibly because the training data used by the mixture model is more accurate and contains fewer \"false positive\" examples.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9989959597587585}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9992327690124512}]}, {"text": "On the other hand the precision and recall of the mixture model are often higher than the supervised model.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9996862411499023}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995530247688293}]}, {"text": "The increase in recall is presumably caused by having access to additional training data and the precision scores suggest that the classifier is not harmed by some of these containing noisy labels.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9996079802513123}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9993593096733093}]}, {"text": "The difference in performance between the supervised and the mixture-models gets smaller as the number of seed abstracts increases.", "labels": [], "entities": []}, {"text": "shows the mean size of the different sets of training data.", "labels": [], "entities": []}, {"text": "The amount of distantly labelled data is much larger than the manually labelled data at each classification step.", "labels": [], "entities": []}, {"text": "Larger amounts of manually labelled data increase the number of ADE seed instances that can be extracted which leads to more distantly supervised examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Effect of varying size of training data set", "labels": [], "entities": []}, {"text": " Table 3: ADE training data size (mean across five  runs)", "labels": [], "entities": [{"text": "ADE training data size", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.6149597764015198}]}]}