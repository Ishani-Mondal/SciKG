{"title": [{"text": "A fuzzier approach to machine translation evaluation: A pilot study on post-editing productivity and automated metrics in commercial settings", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.8887420694033304}]}], "abstractContent": [{"text": "Machine Translation (MT) quality is typically assessed using automatic evaluation metrics such as BLEU and TER.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8668180465698242}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.998794674873352}, {"text": "TER", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9877699017524719}]}, {"text": "Despite being generally used in the industry for evaluating the usefulness of Translation Memory (TM) matches based on text similarity, fuzzy match values are not as widely used for this purpose in MT evaluation.", "labels": [], "entities": [{"text": "Translation Memory (TM) matches", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.8783716360727946}, {"text": "MT evaluation", "start_pos": 198, "end_pos": 211, "type": "TASK", "confidence": 0.9753526449203491}]}, {"text": "We designed an experiment to test if this fuzzy score applied to MT output stands up against traditional methods of MT evaluation.", "labels": [], "entities": [{"text": "MT output", "start_pos": 65, "end_pos": 74, "type": "TASK", "confidence": 0.8867678046226501}, {"text": "MT evaluation", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.9616542458534241}]}, {"text": "The results obtained seem to confirm that this metric performs at least as well as traditional methods for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.9703137874603271}]}], "introductionContent": [{"text": "In recent years, Machine Translation PostEditing (MTPE) has been introduced in real translation workflows as part of the production process.", "labels": [], "entities": [{"text": "Machine Translation PostEditing (MTPE)", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.8485859930515289}]}, {"text": "MTPE is used to reduce production costs and increase the productivity of professional translators.", "labels": [], "entities": [{"text": "MTPE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.595284640789032}]}, {"text": "This productivity gain is usually reflected in translation rate discounts.", "labels": [], "entities": [{"text": "translation", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9219065308570862}]}, {"text": "However, the question of how to assess Machine Translation (MT) output in order to determine a fair compensation for the post-editor is still open.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8354864776134491}]}, {"text": "Shortcomings of traditional metrics, such as BLEU () and TER (), when applied to MTPE include unclear correlation with productivity gains, technical difficulties for their estimation by general users and lack of intuitiveness.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9985066056251526}, {"text": "TER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9977741837501526}]}, {"text": "A more common metric already used in translation tasks for evaluating text similarity is the Translation Memory (TM) fuzzy match score.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.9020184278488159}, {"text": "Translation Memory (TM) fuzzy match score", "start_pos": 93, "end_pos": 134, "type": "METRIC", "confidence": 0.6728082969784737}]}, {"text": "Based on the fuzzy score analysis, rate discounts due to TM leverage are then applied.", "labels": [], "entities": []}, {"text": "We designed an experiment to test if this fuzzy score applied to MT output stands up against traditional methods of MT evaluation.", "labels": [], "entities": [{"text": "MT output", "start_pos": 65, "end_pos": 74, "type": "TASK", "confidence": 0.8867678046226501}, {"text": "MT evaluation", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.9616542458534241}]}, {"text": "The remainder of this paper is structured as follows: Section 2 presents the rationale behind the experiment.", "labels": [], "entities": []}, {"text": "Section 3 explains the pilot experiment itself.", "labels": [], "entities": []}, {"text": "Section 4 reports the results obtained and what they have revealed, and finally Section 5 summarizes our work and discusses possible paths to explore in the light of our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following similar works, the experiment aimed to replicate areal production environment.", "labels": [], "entities": []}, {"text": "Two in-house translators were asked to translate the same file from English into Spanish using one of their most common translation tools (memoQ 4 ).", "labels": [], "entities": []}, {"text": "This tool was chosen because of its feature for recording time spent in each segment.", "labels": [], "entities": []}, {"text": "Other tools which also record this value and other useful segment-level indicators, such as keystrokes 5 , or MTPE effort 6 , were discarded due to them not being part of the everyday resources of the translators involved in the experiment.", "labels": [], "entities": []}, {"text": "Translators were only allowed to use the TM, the terminology database and the MT output included in the translation package.", "labels": [], "entities": []}, {"text": "Other memoQ's productivity enhancing features were disabled (especially, predictive text, sub-segment leverage and automatic fixing of fuzzy matches) to allow better comparisons with translation environments which 2 See the pricing MTPE guidelines at https://www.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Final word counts.", "labels": [], "entities": []}, {"text": " Table 3: Results obtained for both translators.", "labels": [], "entities": []}, {"text": " Table 4: Pearson correlation between produc- tivity and evaluation measures.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.902735024690628}]}, {"text": " Table 5: BLEU results as computed by differ- ent evaluation tools.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.99800044298172}]}]}