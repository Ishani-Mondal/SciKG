{"title": [{"text": "What's in a Domain? Multi-Domain Learning for Multi-Attribute Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Multi-Domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains.", "labels": [], "entities": [{"text": "Multi-Domain learning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7652581930160522}]}, {"text": "However , real-world datasets often have multiple metadata attributes that can divide the data into domains.", "labels": [], "entities": []}, {"text": "It is not always apparent which single attribute will lead to the best domains , and more than one attribute might impact classification.", "labels": [], "entities": []}, {"text": "We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes.", "labels": [], "entities": []}, {"text": "Experimentally, they outperform the multi-domain learning baseline, even when it selects the single \"best\" attribute.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-Domain Learning () algorithms learn when training instances are spread across many domains, which impact model parameters.", "labels": [], "entities": []}, {"text": "These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences.", "labels": [], "entities": []}, {"text": "However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use.", "labels": [], "entities": []}, {"text": "Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors.", "labels": [], "entities": []}, {"text": "For multi-domain learning, we should use the metadata attribute most likely to characterize a domain: a change in vocabulary (i.e. features) that most impacts the classification decision.", "labels": [], "entities": []}, {"text": "This choice is not easy.", "labels": [], "entities": []}, {"text": "First, we may not know which metadata attribute is most likely to fit this role.", "labels": [], "entities": []}, {"text": "Perhaps the location most impacts the review language, but it could easily be the price of the meal.", "labels": [], "entities": []}, {"text": "Second, multiple metadata attributes could impact the classification decision, and picking a single one might reduce classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.9772282242774963}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.8662393093109131}]}, {"text": "Therefore, we seek multi-domain learning algorithms which can simultaneously learn from many types of domains (metadata attributes).", "labels": [], "entities": []}, {"text": "We introduce the multi-attribute multi-domain (MAMD) learning problem, in which each learning instance is associated with multiple metadata attributes, each of which may impact feature behavior.", "labels": [], "entities": []}, {"text": "We present extensions to two popular multi-domain learning algorithms, FEDA and MDR (.", "labels": [], "entities": [{"text": "FEDA", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9126782417297363}]}, {"text": "Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes.", "labels": [], "entities": []}, {"text": "We evaluate our algorithms using two different data sets -a data set of restaurant reviews (, and a dataset of transcribed speech segments from floor debates in the United States Congress ().", "labels": [], "entities": []}, {"text": "We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our multi-attribute algorithms we consider two datasets.", "labels": [], "entities": []}, {"text": "First, we use two subsets of the restaurant reviews dataset (1,180,308 reviews) introduced by with the goal of labeling reviews as positive or negative.", "labels": [], "entities": []}, {"text": "The first subset (50K-RND) randomly selects 50,000 reviews while the second (50K-BAL) is a class-balanced sample.", "labels": [], "entities": []}, {"text": "Following the approach of, scores above and below 3-stars indicated positive and negative reviews, while 3-star reviews were discarded.", "labels": [], "entities": []}, {"text": "Second, we use the transcribed segments of speech from the United States Congress floor debates (Convote), introduced by.", "labels": [], "entities": [{"text": "Convote)", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.7512399256229401}]}, {"text": "The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bill under discussion in the floor debate.", "labels": [], "entities": []}, {"text": "In the WordSalad datasets, each restaurant review can have many metadata attributes, including a unique identifier, name (which may not be unique), address (we extract the zipcode), and type (Italian, Chinese, etc.).", "labels": [], "entities": [{"text": "WordSalad datasets", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.9883853495121002}]}, {"text": "We select the 20 most common metadata attributes (excluding latitude, longitude, and the average rating).", "labels": [], "entities": []}, {"text": "In the Convote dataset, each speech segment is associated with the political party affiliation of the speaker (democrat, independent, or republican) and the speaker identifier (we use bill identifiers for creating folds in our 10-fold crossvalidation setup).", "labels": [], "entities": [{"text": "Convote dataset", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9141870737075806}]}, {"text": "In addition to our new algorithms, we evaluate several baselines.", "labels": [], "entities": []}, {"text": "All methods use confidenceweighted (CW) learning).", "labels": [], "entities": []}, {"text": "BASE A single classifier trained on all the data, and which ignores metadata attributes and uses unigram features.", "labels": [], "entities": [{"text": "BASE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6613516211509705}]}, {"text": "For CW, we use the best-performing setting from  -the \"variance\" algorithm, which computes approximate but closedform updates, which also lead to faster learning.", "labels": [], "entities": []}, {"text": "Parameters are tuned over a validation set within each training fold.: Average accuracy (\u00b1 standard error) using 10-fold cross-validation for methods that use all attributes, either directly (our proposed methods) or for selecting the \"best\" single attribute using one of the strategies described earlier.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9536827802658081}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9927761554718018}]}, {"text": "Formatting and significance symbols are the same as in.", "labels": [], "entities": [{"text": "Formatting", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7266146540641785}]}, {"text": "META Identical to BASE with a unique bias feature added for each attribute value (Joshi et al., 2012).", "labels": [], "entities": [{"text": "BASE", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.6791760921478271}]}, {"text": "1-META A special case of META where a unique bias feature is added only fora single attribute.", "labels": [], "entities": []}, {"text": "To use multi-domain learning directly, we could select a single attribute as the domain.", "labels": [], "entities": []}, {"text": "We consider several strategies for picking this attribute and evaluate both FEDA and MDR in this setting.", "labels": [], "entities": [{"text": "FEDA", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9910863041877747}]}, {"text": "1-MEAN Choose an attribute randomly, equivalent to the expected (mean) error overall attributes.", "labels": [], "entities": []}, {"text": "1-TUNE Select the best performing attribute on a validation set.", "labels": [], "entities": []}, {"text": "1-ORCL Select the best performing attribute on the test set.", "labels": [], "entities": []}, {"text": "Though impossible in practice, this gives the oracle upper bound on multi-domain learning.", "labels": [], "entities": []}, {"text": "All experiments use ten-fold cross-validation.", "labels": [], "entities": []}, {"text": "We report the mean accuracy, along with standard error.", "labels": [], "entities": [{"text": "mean", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9812904000282288}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8877992033958435}, {"text": "standard error", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.9307381212711334}]}, {"text": "shows the results of single-attribute multidomain learning methods for the WordSalad datasets.", "labels": [], "entities": [{"text": "WordSalad datasets", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.9935532212257385}]}, {"text": "The table shows the three best-performing metadata attributes (as decided by the highest accuracy among all the methods across all 20 metadata attributes).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9967343807220459}]}, {"text": "Clearly, several of the attributes can provide meaningful domains, which demonstrates that methods that can select multiple attributes at once are desirable.", "labels": [], "entities": []}, {"text": "We also see that our modification to MDR (MDR-NV) works the best.", "labels": [], "entities": []}, {"text": "shows the results of single-attribute multidomain learning methods for the Convote dataset.", "labels": [], "entities": [{"text": "Convote dataset", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9622035324573517}]}, {"text": "The first observation to be made on this dataset is that neither the PARTY, nor the SPEAKER attribute individually achieve significant improvement over the META baseline, which uses both these attributes as features.", "labels": [], "entities": [{"text": "PARTY", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9880406260490417}, {"text": "SPEAKER", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.8570379614830017}, {"text": "META baseline", "start_pos": 156, "end_pos": 169, "type": "DATASET", "confidence": 0.8536262512207031}]}, {"text": "This is in contrast with the results on the WordSalad dataset, where some attributes by themselves showed an improvement over the META baseline.", "labels": [], "entities": [{"text": "WordSalad dataset", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9897958040237427}, {"text": "META baseline", "start_pos": 130, "end_pos": 143, "type": "DATASET", "confidence": 0.8550770282745361}]}, {"text": "Thus, this dataset represents a more challenging setup for our multi-attribute multi-domain learning methods -they need to exploit the two weak attributes simultaneously.", "labels": [], "entities": []}], "tableCaptions": []}