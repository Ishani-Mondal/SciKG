{"title": [{"text": "Improving Syntax-Augmented Machine Translation by Coarsening the Label Set", "labels": [], "entities": [{"text": "Improving Syntax-Augmented Machine Translation", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8724406957626343}]}], "abstractContent": [{"text": "We present anew variant of the Syntax-Augmented Machine Translation (SAMT) formalism with a category-coarsening algorithm originally developed for tree-to-tree grammars.", "labels": [], "entities": [{"text": "Syntax-Augmented Machine Translation (SAMT) formalism", "start_pos": 31, "end_pos": 84, "type": "TASK", "confidence": 0.7964232436248234}]}, {"text": "We induce bilingual labels into the SAMT grammar, use them for category coars-ening, then project back to monolingual labeling as in standard SAMT.", "labels": [], "entities": []}, {"text": "The result is a \"collapsed\" grammar with the same expressive power and format as the original, but many fewer nonterminal labels.", "labels": [], "entities": []}, {"text": "We show that the smaller label set provides improved translation scores by 1.14 BLEU on two Chinese-English test sets while reducing the occurrence of sparsity and ambiguity problems common to large label sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9993333220481873}]}], "introductionContent": [{"text": "The formulation of statistical machine translation in terms of synchronous parsing has become both theoretically and practically successful.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.7142807642618815}]}, {"text": "Ina parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output.", "labels": [], "entities": [{"text": "MT formalism", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.8666862845420837}]}, {"text": "SCFG translation grammars can be extracted automatically from data.", "labels": [], "entities": [{"text": "SCFG translation grammars", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7817431489626566}]}, {"text": "While formally syntactic approaches with a single grammar nonterminal have often worked well, the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories ().", "labels": [], "entities": []}, {"text": "Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source-and/or target-side parse trees provided by a statistical parser.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.712430477142334}]}, {"text": "The MT system may exactly adopt the parser's label set or modify it in someway.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.931979238986969}]}, {"text": "Larger label sets are able to represent more precise, fine-grained categories.", "labels": [], "entities": []}, {"text": "On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the Syntax-Augmented MT formalism (), a monolingually labeled version of Hiero that can create up to 4000 \"extended\" category labels based on pairs of parse nodes.", "labels": [], "entities": []}, {"text": "We take a standard SAMT grammar with target-side labels and extend its labeling to a bilingual format.", "labels": [], "entities": [{"text": "SAMT grammar", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.7894540131092072}]}, {"text": "We then coarsen the bilingual labels following the \"label collapsing\" algorithm of.", "labels": [], "entities": [{"text": "label collapsing\"", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7828594048817953}]}, {"text": "This represents a novel extension of the tree-to-tree collapsing algorithm to the SAMT formalism.", "labels": [], "entities": [{"text": "SAMT formalism", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.5872183442115784}]}, {"text": "After removing the source-side labels, we obtain anew SAMT grammar with coarser target-side labels than the original.", "labels": [], "entities": [{"text": "SAMT grammar", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.8329894244670868}]}, {"text": "Coarsened grammars provide improvement of up to 1.14 BLEU points over the baseline SAMT results on two Chinese-English test sets; they also outperform a Hiero baseline by up to 0.60 BLEU on one of the sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9948000907897949}, {"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9904937148094177}]}, {"text": "Aside from improved translation quality, in analysis we find significant reductions in derivational ambiguity and rule sparsity, two problems that make large nonterminal sets difficult to work with.", "labels": [], "entities": []}, {"text": "Section 2 provides a survey of large syntax-based MT label sets, their associated problems of derivational ambiguity and rule sparsity, and previous attempts at addressing those problems.", "labels": [], "entities": [{"text": "MT label sets", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.8658137122790018}]}, {"text": "The section also summarizes the tree-to-tree label collapsing algorithm and the process of SAMT rule extraction.", "labels": [], "entities": [{"text": "tree-to-tree label collapsing", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.6789454321066538}, {"text": "SAMT rule extraction", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.9119408329327902}]}, {"text": "We then describe our method of label collapsing in SAMT grammars in Section 3.", "labels": [], "entities": [{"text": "label collapsing", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7623312771320343}, {"text": "SAMT grammars", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.5711316466331482}]}, {"text": "Experimental results are presented in Section 4 and analyzed in Section 5.", "labels": [], "entities": []}, {"text": "Finally, Section 6 offers some conclusions and avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on Chinese-to-English MT, using systems trained from the FBIS corpus of approximately 302,000 parallel sentence pairs.", "labels": [], "entities": [{"text": "Chinese-to-English MT", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.4764396995306015}, {"text": "FBIS corpus", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.9683430790901184}]}, {"text": "We parse both sides of the training data with the Berkeley parsers) for Chinese and English.", "labels": [], "entities": []}, {"text": "The English side is lowercased after parsing; the Chinese side is segmented beforehand.", "labels": [], "entities": []}, {"text": "Unidirectional word alignments are obtained with GIZA++ ( and symmetrized, resulting in a parallel parsed corpus with Viterbi word alignments for each sentence pair.", "labels": [], "entities": []}, {"text": "Our modified version of Thrax takes the parsed and aligned corpus as input and returns a list of rules, which can then be label-collapsed and scored as previously described.", "labels": [], "entities": []}, {"text": "In Thrax, we retain most of the default settings for Hiero-and SAMT-style grammars as specified in the extractor's configuration file.", "labels": [], "entities": [{"text": "Hiero-and SAMT-style grammars", "start_pos": 53, "end_pos": 82, "type": "DATASET", "confidence": 0.7094781796137491}]}, {"text": "Inheriting from Hiero, we require the right-hand side of all rules to contain at least one pair of aligned terminals, no more than two nonterminals, and no more than five terminals and nonterminal elements combined.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.9545349478721619}]}, {"text": "Nonterminals are not allowed to be adjacent on the source side, and they may not contain unaligned boundary words.", "labels": [], "entities": []}, {"text": "Rules themselves are not extracted from any span in the training data longer than 10 tokens.", "labels": [], "entities": []}, {"text": "Our initial bilingual SAMT grammar uses 2699 unique source-side labels and 4181 unique targetside labels, leading to the appearance of 29,088 joint bilingual labels in the rule set.", "labels": [], "entities": [{"text": "SAMT grammar", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.8283025920391083}]}, {"text": "We provide the joint labels (along with their counts) to the label collapsing algorithm, while we strip out the source-side labels to create the baseline SAMT grammar with 4181 unique target-side labels.", "labels": [], "entities": [{"text": "label collapsing", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7276867032051086}]}, {"text": "summarizes how the number of target labels, unique extracted rules, and the average number of pruned rules available per sentence change as the initial grammar is label-collapsed to three progressively coarser degrees.", "labels": [], "entities": []}, {"text": "Once the collapsing process has occurred exhaustively, the original SAMT grammar becomes a Hiero-format grammar with a single nonterminal.", "labels": [], "entities": []}, {"text": "Each of the five grammars in is used to build an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9780705571174622}]}, {"text": "All systems are tuned and decoded with cdec (, an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.6050193905830383}]}, {"text": "We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric.", "labels": [], "entities": [{"text": "NIST Open MT 2006 data set", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.9401069978872935}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9969974756240845}]}, {"text": "Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences.", "labels": [], "entities": [{"text": "NIST 2003 data set", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.9569999277591705}, {"text": "NIST 2008 data set", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.9803440123796463}]}, {"text": "The tuning set and both test sets all have four English references.", "labels": [], "entities": []}, {"text": "We evaluate systems on BLEU (), METEOR (Denkowski and Lavie, 2011), and TER (), as calculated in all three cases by MultEval version 0.5.0. 1 These scores for the MT '03 test set are shown in, and those for the MT '08 test set in, combined by MultEval over three optimization runs on the tuning set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9981574416160583}, {"text": "METEOR", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9781344532966614}, {"text": "TER", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9958076477050781}, {"text": "MT '03 test set", "start_pos": 163, "end_pos": 178, "type": "DATASET", "confidence": 0.8303160905838013}, {"text": "MT '08 test set", "start_pos": 211, "end_pos": 226, "type": "DATASET", "confidence": 0.878329074382782}, {"text": "MultEval", "start_pos": 243, "end_pos": 251, "type": "DATASET", "confidence": 0.9405309557914734}]}, {"text": "MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization.", "labels": [], "entities": [{"text": "MultEval", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8791250586509705}, {"text": "statistical significance testing", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.6535792648792267}]}, {"text": "This process) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance.", "labels": [], "entities": []}, {"text": "We report these results in the tables as well for three MERT runs and a p-value of 0.05.", "labels": [], "entities": [{"text": "MERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.680705726146698}]}, {"text": "Systems that were judged statistically different from the SAMT baseline have triangles in the appropriate \"Sig.", "labels": [], "entities": [{"text": "SAMT baseline", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.7336849421262741}]}, {"text": "SAMT?\" columns; systems judged different from the Hiero baseline have triangles under the \"Sig.", "labels": [], "entities": [{"text": "SAMT?\"", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5975064039230347}, {"text": "Hiero baseline", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9568070471286774}]}, {"text": "An up-triangle () indicates that the system was better, while a down-triangle () means that the baseline was better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Grammar statistics for different degrees of label  collapsing: number of target-side labels, unique rules in  the whole grammar, and average number of pruned rules  after filtering to individual sentences.", "labels": [], "entities": [{"text": "label  collapsing", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.743696004152298}]}, {"text": " Table 2: MT '03 test set results. The first section gives automatic metric scores; the remaining sections indicate  whether each system is statistically significantly better () or worse () than the SAMT and Hiero baselines.", "labels": [], "entities": [{"text": "MT '03 test set", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.7457319080829621}, {"text": "SAMT", "start_pos": 199, "end_pos": 203, "type": "DATASET", "confidence": 0.8573161363601685}, {"text": "Hiero baselines", "start_pos": 208, "end_pos": 223, "type": "DATASET", "confidence": 0.8800572156906128}]}, {"text": " Table 3: MT '08 test set results. The first section gives automatic metric scores; the remaining sections indicate  whether each system is statistically significantly better () or worse () than the SAMT and Hiero baselines.", "labels": [], "entities": [{"text": "MT '08 test set", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.7692284107208252}, {"text": "SAMT", "start_pos": 199, "end_pos": 203, "type": "DATASET", "confidence": 0.8574389219284058}, {"text": "Hiero baselines", "start_pos": 208, "end_pos": 223, "type": "DATASET", "confidence": 0.8812997341156006}]}]}