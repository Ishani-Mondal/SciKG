{"title": [{"text": "Applying Pairwise Ranked Optimisation to Improve the Interpolation of Translation Models", "labels": [], "entities": [{"text": "Applying", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8214125037193298}, {"text": "Interpolation of Translation", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.6692457993825277}]}], "abstractContent": [{"text": "In Statistical Machine Translation we often have to combine different sources of parallel training data to build a good system.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.856611986955007}]}, {"text": "One way of doing this is to build separate translation models from each data set and linearly interpolate them, and to date the main method for optimising the interpolation weights is to min-imise the model perplexity on a heldout set.", "labels": [], "entities": []}, {"text": "In this work, rather than optimising for this indirect measure, we directly optimise for BLEU on the tuning set and show improvements in average performance over two data sets and 8 language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9988105297088623}]}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) requires large quantities of parallel training data in order to produce high quality translation systems.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8448354204495748}]}, {"text": "This training data, however, is often scarce and must be drawn from whatever sources are available.", "labels": [], "entities": []}, {"text": "If these data sources differ systematically from each other, and/or from the test data, then the problem of combining these disparate data sets to create the best possible translation system is known as domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 203, "end_pos": 220, "type": "TASK", "confidence": 0.7306229174137115}]}, {"text": "One approach to domain adaptation is to build separate models for each training domain, then weight them to create a system tuned to the test domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7372511178255081}]}, {"text": "In SMT, a successful approach to building domain specific language models is to build one from each corpus, then linearly interpolate them, choosing weights that minimise the perplexity on a suitable heldout set of in-domain data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9902129769325256}]}, {"text": "This method has been applied by many authors (e.g. ( ), and is implemented in popular language modelling tools like IRSTLM and SRILM.", "labels": [], "entities": [{"text": "IRSTLM", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.8453127145767212}, {"text": "SRILM", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.8746753334999084}]}, {"text": "Similar interpolation techniques have been developed for translation model interpolation) for phrase-based systems but have not been as widely adopted, perhaps because the efficacy of the methods is not as clearcut.", "labels": [], "entities": [{"text": "translation model interpolation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.9012352228164673}]}, {"text": "In this previous work, the authors used standard phrase extraction heuristics to extract phrases from a heldout set of parallel sentences, then tuned the translation model (i.e. the phrase table) interpolation weights to minimise the perplexity of the interpolated model on this set of extracted phrases.", "labels": [], "entities": [{"text": "phrase extraction heuristics", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7787247200806936}]}, {"text": "In this paper, we try to improve on this perplexity optimisation of phrase table interpolation weights by addressing two of its shortcomings.", "labels": [], "entities": []}, {"text": "The first problem is that the perplexity is not well defined because of the differing coverage of the phrase tables, and their partial coverage of the phrases extracted from the heldout set.", "labels": [], "entities": []}, {"text": "Secondly, perplexity may not correlate with the performance of the final SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9906902313232422}]}, {"text": "So, instead of optimising the interpolation weights for the indirect goal of translation model perplexity, we optimise them directly for translation performance.", "labels": [], "entities": [{"text": "translation model perplexity", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.8688608805338541}]}, {"text": "We do this by incorporating these weights into SMT tuning using a modified version of Pairwise Ranked Optimisation (PRO) ().", "labels": [], "entities": [{"text": "SMT tuning", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.9112993478775024}, {"text": "Pairwise Ranked Optimisation (PRO)", "start_pos": 86, "end_pos": 120, "type": "METRIC", "confidence": 0.8162597517172495}]}, {"text": "In experiments on two different domain adaptation problems and 8 language pairs, we show that our method achieves comparable or improved performance, when compared to the perplexity minimisation method.", "labels": [], "entities": []}, {"text": "This is an encouraging result as it shows that PRO can be adapted to optimise translation parameters other than those in the standard linear model.", "labels": [], "entities": [{"text": "PRO", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8965171575546265}]}], "datasetContent": [], "tableCaptions": []}