{"title": [{"text": "Discriminative Training of 150 Million Translation Parameters and Its Application to Pruning", "labels": [], "entities": [{"text": "Pruning", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.5579823851585388}]}], "abstractContent": [{"text": "Until recently, the application of discrimina-tive training to log linear-based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.6348297496636709}]}, {"text": "In this paper, we propose to scale up discriminative training of (He and Deng, 2012) to train features with 150 million parameters , which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning.", "labels": [], "entities": []}, {"text": "The experimental results confirm the effectiveness of our proposals on NIST MT06 set over a strong baseline.", "labels": [], "entities": [{"text": "NIST MT06", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.7630817592144012}]}], "introductionContent": [{"text": "State-of-the-art statistical machine translation systems based on a log-linear framework are parameterized by {\u03bb, \u03a6}, where the feature weights \u03bb are discriminatively trained) by directly optimizing them against a translation-oriented metric such as BLEU.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6299313306808472}, {"text": "BLEU", "start_pos": 250, "end_pos": 254, "type": "METRIC", "confidence": 0.9723508358001709}]}, {"text": "The feature parameters \u03a6 can be roughly divided into two categories: dense feature that measures the plausibility of each translation rule from a particular aspect, e.g., the rule translation probabilities p(f |e) and p(e|f ); and sparse feature that fires when certain phenomena is observed, e.g., when a frequent word pair co-occured in a rule.", "labels": [], "entities": []}, {"text": "In contrast to \u03bb, feature parameters in \u03a6 are usually modeled by generative models for dense features, or by indicator functions for sparse ones.", "labels": [], "entities": []}, {"text": "It is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion.", "labels": [], "entities": []}, {"text": "The maximum expected BLEU training of) is a recent effort towards this direction, and in this paper, we extend their work to a scaled-up task of discriminative training of the features of a strong hierarchical phrase-based model and confirm its effectiveness empirically.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9982813596725464}]}, {"text": "In this work, we further consider the application of discriminative training to pruned model.", "labels": [], "entities": []}, {"text": "Various pruning techniques) have been proposed recently to filter translation rules.", "labels": [], "entities": [{"text": "translation rules", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8855691254138947}]}, {"text": "One common consequence of pruning is that the probability distribution of many surviving rules become deficient, i.e. f p(f |e) < 1.", "labels": [], "entities": []}, {"text": "In practice, others have chosen either to leave the pruned rules as it-is, or simply to re-normalize the probability mass by distributing the pruned mass to surviving rules proportionally.", "labels": [], "entities": []}, {"text": "We argue that both approaches are suboptimal, and propose a more principled method to re-distribute the probability mass, i.e. using discriminative training with some translation criterion.", "labels": [], "entities": []}, {"text": "Our experimental results demonstrate that at various pruning levels, our approach improves performance consistently.", "labels": [], "entities": []}, {"text": "Particularly at the level of 50% of rules being pruned, the discriminatively trained models performs better than the unpruned baseline grammar.", "labels": [], "entities": []}, {"text": "This shows that discriminative training makes it possible to achieve smaller models that perform comparably or even better than the baseline model.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are two-folded: First of all, we scale up the maximum expected BLEU training proposed in) in a number of ways including using 1) a hierarchical phrase-based model, 2) a richer feature set, and 3) a larger training set with a much larger parameter set, resulting in more than 150 million parameters in the model being updated, which is one order magnitude higher than the phrase-based model reported in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9738546013832092}]}, {"text": "We are able to show a reasonable improvement over this strong baseline.", "labels": [], "entities": []}, {"text": "Secondly, we combine discriminative training with pruning techniques to reestimate parameters of pruned grammar.", "labels": [], "entities": []}, {"text": "Our approach is shown to alleviate the loss due to pruning, and sometimes can even outperform the baseline unpruned grammar.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are designed to serve two goals: 1) to show the performance of discriminative training of feature parameters \u03b8 in a large-scale task; and 2) to show the effectiveness of DT when applied to pruned grammar.", "labels": [], "entities": []}, {"text": "Our baseline system is a state-of-the-art hierarchical phrase-based system as described in (, trained on six million parallel sentences corpora that are available to the DARPA BOLT Chinese-English task.", "labels": [], "entities": [{"text": "DARPA BOLT Chinese-English task", "start_pos": 170, "end_pos": 201, "type": "TASK", "confidence": 0.49456963688135147}]}, {"text": "The training corpora includes a mixed genre of news wire, broadcast news, web-blog and comes from various sources such as LDC, HK Hansard and UN data.", "labels": [], "entities": [{"text": "LDC", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.9568203687667847}, {"text": "HK Hansard and UN data", "start_pos": 127, "end_pos": 149, "type": "DATASET", "confidence": 0.7448140978813171}]}, {"text": "In total, there are 50 dense features in our translation system.", "labels": [], "entities": []}, {"text": "In addition to the standard features which include the rule translation probabilities, we incorporate features that are found useful for developing a state-of-the-art baseline, e.g. provenancebased lexical features).", "labels": [], "entities": [{"text": "rule translation", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7233370840549469}]}, {"text": "We use a large 6-gram language model, which we train on a 10 billion words monolingual corpus, including the English side of our parallel corpora plus other corpora such as Gigaword (LDC2011T07) and Google News.", "labels": [], "entities": [{"text": "Gigaword (LDC2011T07)", "start_pos": 173, "end_pos": 194, "type": "DATASET", "confidence": 0.8422902971506119}, {"text": "Google News", "start_pos": 199, "end_pos": 210, "type": "DATASET", "confidence": 0.8235795199871063}]}, {"text": "To prevent possible over-fitting, we only kept the rules that have at most three terminal words (plus up to two nonterminals) on the source side, resulting in a grammar with 167 million rules.", "labels": [], "entities": []}, {"text": "Our discriminative training procedure includes updating both \u03bb and \u03b8, and we follow) to optimize them in an alternate manner.", "labels": [], "entities": []}, {"text": "That is, when we optimize \u03b8 via EBW, we keep \u03bb fixed and when we optimize \u03bb, we keep \u03bb fixed.", "labels": [], "entities": []}, {"text": "We use PRO () to tune \u03bb.", "labels": [], "entities": [{"text": "PRO", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.8138633966445923}]}, {"text": "For discriminative training of \u03b8, we use a subset of 550 thousands of parallel sentences selected from the entire training data, mainly to allow for faster experimental cycle; they mainly come from news and web-blog domains.", "labels": [], "entities": []}, {"text": "For each sentence of this subset, we generate 500-best of unique hypotheses using the baseline model.", "labels": [], "entities": []}, {"text": "The 1-best and the oracle BLEU scores for this subset are 40.19 and 47.06 respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9945598244667053}]}, {"text": "Following, we focus on discriminative training of p(f |e) and p(e|f ), which in practice affects around 150 million of parameters; hence the title.", "labels": [], "entities": []}, {"text": "For the tuning and development sets, we set aside 1275 and 1239 sentences respectively from LDC2010E30 corpus.", "labels": [], "entities": [{"text": "LDC2010E30 corpus", "start_pos": 92, "end_pos": 109, "type": "DATASET", "confidence": 0.9711749255657196}]}, {"text": "The tune set is used by PRO for tuning \u03bb while the dev set is used to decide the best DT model.", "labels": [], "entities": [{"text": "PRO", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.664496660232544}]}, {"text": "As for the blind test set, we report the performance on the NIST MT06 evaluation set, which consists of 1644 sentences from news and web-blog domains.", "labels": [], "entities": [{"text": "NIST MT06 evaluation set", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.9229582995176315}]}, {"text": "Our baseline system's performance on MT06 is 39.91 which is among the best number ever published so far in the community.", "labels": [], "entities": [{"text": "MT06", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8146060109138489}]}, {"text": "compares the key components of our baseline system with that of).", "labels": [], "entities": []}, {"text": "As shown, we are working with a stronger system than (, especially in terms of the number of parameters under consideration |\u03b8|.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Our system compares to He&Deng's (2012).", "labels": [], "entities": []}, {"text": " Table 2: The statistics of grammars pruned at various level (column 1), including the number of unique source and  target phrases (columns 2 & 3)", "labels": [], "entities": []}]}