{"title": [{"text": "Coherence Modeling for the Automated Assessment of Spontaneous Spoken Responses", "labels": [], "entities": [{"text": "Automated Assessment of Spontaneous Spoken Responses", "start_pos": 27, "end_pos": 79, "type": "TASK", "confidence": 0.691720316807429}]}], "abstractContent": [{"text": "This study focuses on modeling discourse coherence in the context of automated assessment of spontaneous speech from non-native speakers.", "labels": [], "entities": []}, {"text": "Discourse coherence has always been used as a key metric inhuman scoring rubrics for various assessments of spoken language.", "labels": [], "entities": []}, {"text": "However, very little research has been done to assess a speaker's coherence in automated speech scoring systems.", "labels": [], "entities": []}, {"text": "To address this, we present a corpus of spoken responses that has been annotated for discourse coherence quality.", "labels": [], "entities": []}, {"text": "Then, we investigate the use of several features originally developed for essays to model coherence in spoken responses.", "labels": [], "entities": []}, {"text": "An analysis on the annotated corpus shows that the prediction accuracy for human holistic scores of an automated speech scoring system can be improved by around 10% relative after the addition of the coherence features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.8280730247497559}]}, {"text": "Further experiments indicate that a weighted F-Measure of 73% can be achieved for the automated prediction of the coherence scores.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9851722717285156}]}], "introductionContent": [{"text": "In recent years, much research has been conducted into developing automated assessment systems to automatically score spontaneous speech from nonnative speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners.", "labels": [], "entities": [{"text": "reliability", "start_pos": 226, "end_pos": 237, "type": "METRIC", "confidence": 0.9652430415153503}]}, {"text": "Various features related to different aspects of speaking proficiency have been exploited, such as delivery features for pronunciation, prosody, and fluency, as well as language use features for vocabulary and grammar, and content features ().", "labels": [], "entities": []}, {"text": "However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring.", "labels": [], "entities": [{"text": "topic development", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7837751805782318}, {"text": "automated speech scoring", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.6620170871416727}]}, {"text": "This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response.", "labels": [], "entities": []}, {"text": "Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text readability.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 138, "end_pos": 165, "type": "TASK", "confidence": 0.6537104249000549}, {"text": "document summarization", "start_pos": 167, "end_pos": 189, "type": "TASK", "confidence": 0.6185310482978821}]}, {"text": "For example, measured the overall coherence of a text by utilizing Latent Semantic Analysis (LSA) to calculate the semantic relatedness between adjacent sentences.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA)", "start_pos": 67, "end_pos": 97, "type": "METRIC", "confidence": 0.6485121001799902}]}, {"text": "introduced an HMM-based model for the document-level analysis of topics and topic transitions.", "labels": [], "entities": [{"text": "document-level analysis of topics and topic transitions", "start_pos": 38, "end_pos": 93, "type": "TASK", "confidence": 0.6924607115132468}]}, {"text": "presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level.", "labels": [], "entities": []}, {"text": "provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (), word cooccurrence patterns, and entity-grid.", "labels": [], "entities": []}, {"text": "In addition to studies on well-formed text, researchers have also addressed coherence modeling on text produced by language learners, which may contain many spelling and grammar errors.", "labels": [], "entities": []}, {"text": "Utilizing LSA and Random Indexing methods, measured the global coherence of students' essays by calculating the semantic relatedness between sentences and the corresponding prompts.", "labels": [], "entities": []}, {"text": "In addition, Burstein et. al combined entity-grid features with writing quality features produced by an automated assessment system of essays to predict the coherence scores of student essays.", "labels": [], "entities": []}, {"text": "Recently, systematically analyzed a variety of coherence modeling methods within the framework of an automated assessment system for non-native free text responses and indicated that features based on Incremental Semantic Analysis (ISA), local histograms of words, the part-ofspeech IBM model, and word length were the most effective.", "labels": [], "entities": []}, {"text": "In contrast to these previous studies involving well-formed text or learner text containing errors, this paper focuses on modeling coherence in spontaneous spoken responses as well as investigating discourse features in an attempt to extend the construct coverage of an automated speech scoring system.", "labels": [], "entities": []}, {"text": "Ina related study, investigated coherence modeling for spoken language in the context of a story retelling task for the automated diagnosis of children with language impairment.", "labels": [], "entities": []}, {"text": "They annotated transcriptions of children's narratives with coherence scores as well as markers of narrative structure and narrative quality; furthermore they built models to predict the coherence scores based on Coh-Metrix features and the manually annotated narrative features.", "labels": [], "entities": []}, {"text": "The current study differs from this one in that it deals with free spontaneous spoken responses provided by students at a university level; these responses therefore contain more varied and more complicated information than the child narratives.", "labels": [], "entities": []}, {"text": "The main contributions of this paper can be summarized as follows: First, we obtained coherence annotations on a corpus of spontaneous spoken responses drawn from a university-level English language proficiency assessment, and demonstrated an improvement of around 10% relative in the accuracy of the automated prediction of human holistic scores with the addition of the coherence annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 285, "end_pos": 293, "type": "METRIC", "confidence": 0.998528003692627}]}, {"text": "Second, we applied the entitygrid features and writing quality features from an automated essay scoring system to predict the coherence scores; the experimental results have shown promising correlations between some of these features and the coherence scores.", "labels": [], "entities": []}], "datasetContent": [{"text": "As demonstrated in Section 2.2, the collapsed average coherence score can be used to improve the performance of an automated speech scoring system.", "labels": [], "entities": [{"text": "collapsed average coherence score", "start_pos": 36, "end_pos": 69, "type": "METRIC", "confidence": 0.7995479255914688}]}, {"text": "Therefore, this study treats coherence prediction as a binary classification task: low-coherent vs. high-coherent, where the low-coherent responses are those with average scores 1 and 1.5, and the high-coherent responses are those with average scores 2, 2.5, and 3.", "labels": [], "entities": [{"text": "coherence prediction", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8687243163585663}]}, {"text": "For coherence modeling, we again use the J48 decision tree from the Weka machine learning toolkit () and run 4-fold crossvalidation on the 600 annotated responses.", "labels": [], "entities": [{"text": "coherence modeling", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8205389976501465}, {"text": "J48 decision tree", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.8551515738169352}, {"text": "Weka machine learning toolkit", "start_pos": 68, "end_pos": 97, "type": "DATASET", "confidence": 0.9081952720880508}]}, {"text": "The correlation coefficient (r) and the weighted average F-Measure In this experiment, we examine the performance of the entity-grid features and a set of features produced by the e-rater\u00ae system (an automated writing assessment system for learner essays)) to predict the coherence scores of the spontaneous spoken responses, where all the features are extracted from human transcriptions of the responses. are used as evaluation metrics.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.9656467437744141}, {"text": "F-Measure", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.939227283000946}]}], "tableCaptions": [{"text": " Table 1. Distribution of coherence annotations from two  annotators", "labels": [], "entities": []}, {"text": " Table 2. Improvement to an automated speech scoring  system after the addition of human-assigned coherence  scores and measures, showing both Pearson r correla- tions and the ratio of correctly matched holistic scores  between the system and human experts", "labels": [], "entities": [{"text": "Pearson r correla- tions", "start_pos": 143, "end_pos": 167, "type": "METRIC", "confidence": 0.9598217844963074}]}, {"text": " Table 3. Performance of entity grid and e-rater features  on the coherence modeling task", "labels": [], "entities": [{"text": "coherence modeling", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7306511998176575}]}]}