{"title": [], "abstractContent": [{"text": "Modern Standard Arabic (MSA) has a wealth of natural language processing (NLP) tools and resources.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.7062941292921702}]}, {"text": "In comparison, resources for dialectal Arabic (DA), the unstandardized spoken varieties of Arabic, are still lacking.", "labels": [], "entities": [{"text": "dialectal Arabic (DA)", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.660419249534607}]}, {"text": "We present ELISSA, a machine translation (MT) system for DA to MSA.", "labels": [], "entities": [{"text": "ELISSA", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.8049741983413696}, {"text": "machine translation (MT)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8434927463531494}]}, {"text": "ELISSA employs a rule-based approach that relies on morphological analysis, transfer rules and dictionaries in addition to language models to produce MSA paraphrases of DA sentences.", "labels": [], "entities": [{"text": "ELISSA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9207434058189392}]}, {"text": "ELISSA can be employed as a general preprocessor for DA when using MSA NLP tools.", "labels": [], "entities": [{"text": "DA", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9274587035179138}]}, {"text": "A manual error analysis of ELISSA's output shows that it produces correct MSA translations over 93% of the time.", "labels": [], "entities": [{"text": "ELISSA's output", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.85479203859965}, {"text": "MSA translations", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.8786339163780212}]}, {"text": "Using ELISSA to produce MSA versions of DA sentences as part of an MSA-pivoting DA-to-English MT solution, improves BLEU scores on multiple blind test sets between 0.6% and 1.4%.", "labels": [], "entities": [{"text": "MSA-pivoting DA-to-English MT", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.6388718883196512}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9975162744522095}]}], "introductionContent": [{"text": "Much work has been done on Modern Standard Arabic (MSA) natural language processing (NLP) and machine translation (MT), especially Statistical MT (SMT).", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA) natural language processing (NLP)", "start_pos": 27, "end_pos": 89, "type": "TASK", "confidence": 0.6221849670012792}, {"text": "machine translation (MT)", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.8370264887809753}, {"text": "Statistical MT (SMT)", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.805189561843872}]}, {"text": "MSA has a wealth of resources in terms of morphological analyzers, disambiguation systems, and parallel corpora.", "labels": [], "entities": []}, {"text": "In comparison, research on dialectal Arabic (DA), the unstandardized spoken varieties of Arabic, is still lacking in NLP in general and MT in particular.", "labels": [], "entities": [{"text": "dialectal Arabic (DA)", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.65297030210495}, {"text": "MT", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.8216666579246521}]}, {"text": "In this paper we present ELISSA, our DA-to-MSA MT system, and show how it can help improve the translation of highly dialectal Arabic text into English by pivoting on MSA.", "labels": [], "entities": [{"text": "ELISSA", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.941826343536377}, {"text": "DA-to-MSA MT", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.5755579769611359}, {"text": "translation of highly dialectal Arabic text", "start_pos": 95, "end_pos": 138, "type": "TASK", "confidence": 0.7749708592891693}]}, {"text": "The ELISSA approach can be summarized as follows.", "labels": [], "entities": []}, {"text": "First, ELISSA uses different techniques to identify dialectal words and multi-word constructions (phrases) in a source sentence.", "labels": [], "entities": []}, {"text": "Then, ELISSA produces MSA paraphrases for the selected words and phrase using a rule-based component that depends on the existence of a dialectal morphological analyzer, a list of morphosyntactic transfer rules, and DA-MSA dictionaries.", "labels": [], "entities": []}, {"text": "The resulting MSA is in a lattice form that we pass to a language model for nbest decoding.", "labels": [], "entities": []}, {"text": "The output of ELISSA, whether a top-1 choice sentence or n-best sentences, is passed to an MSA-English SMT system to produce the English translation sentence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.5679015517234802}]}, {"text": "ELISSA-based MSA-pivoting for DA-to-English SMT improves BLEU scores) on three blind test sets between 0.6% and 1.4%.", "labels": [], "entities": [{"text": "ELISSA-based MSA-pivoting", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8235528767108917}, {"text": "DA-to-English SMT", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.4490267336368561}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9989640712738037}]}, {"text": "A manual error analysis of translated words shows that ELISSA produces correct MSA translations over 93% of the time.", "labels": [], "entities": [{"text": "MSA translations", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.8616178929805756}]}, {"text": "The rest of this paper is structured as follows: Section 2 motivates the use of ELISSA to improve DA-English SMT with an example.", "labels": [], "entities": [{"text": "ELISSA", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.6898423433303833}, {"text": "DA-English SMT", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.6721558570861816}]}, {"text": "Section 3 discusses some of the challenges associated with processing Arabic and its dialects.", "labels": [], "entities": []}, {"text": "Section 4 presents related work.", "labels": [], "entities": []}, {"text": "Section 5 details ELISSA and its approach and Section 6 presents results evaluating ELISSA under a variety of conditions.", "labels": [], "entities": [{"text": "ELISSA", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.6924638152122498}]}, {"text": "shows a motivating example of how pivoting on MSA can dramatically improve the translation quality of a statistical MT system that is trained on mostly MSA-to-English parallel corpora.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.8790053129196167}]}, {"text": "In this example, we use Google Translate's online ArabicEnglish SMT system.", "labels": [], "entities": []}, {"text": "The table is divided into two parts.", "labels": [], "entities": []}, {"text": "The top part shows a dialectal (Levantine) sentence, its reference translation to English, and its Google Translate translation.", "labels": [], "entities": []}, {"text": "The Google Translate translation clearly struggles with most of the DA words, which were probably unseen in the training data (i.e., out-of-vocabulary -OOV) and were con-DA source bhAlHAl\u00af h hAy mA Hyktbwlw \u03c2HyT AlSfHh Al\u0161xSy\u00af h tb\u03c2w wlA bdn yAh yb\u03c2tln kwmyntAt l\u00c2nw mAxbrhwn AymtA rH yrwH \u03c2Albld.", "labels": [], "entities": [{"text": "\u03c2HyT AlSfHh Al\u0161xSy\u00af h tb\u03c2w wlA bdn yAh yb\u03c2tln kwmyntAt l\u00c2nw mAxbrhwn AymtA rH yrwH \u03c2Albld", "start_pos": 207, "end_pos": 296, "type": "DATASET", "confidence": 0.6407729317160213}]}], "datasetContent": [{"text": "In this section, we present two evaluations of ELISSA.", "labels": [], "entities": [{"text": "ELISSA", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.5513522028923035}]}, {"text": "The first is an extrinsic evaluation of ELISSA as part of MSA-pivoting for DA-to-English SMT.", "labels": [], "entities": [{"text": "DA-to-English SMT", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.653611034154892}]}, {"text": "And the second is an intrinsic evaluation of the quality of ELISSA's MSA output.", "labels": [], "entities": [{"text": "ELISSA's MSA output", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.8476976305246353}]}, {"text": "We use the open-source Moses toolkit () to build a phrase-based SMT system trained on mostly MSA data (64M words on the Arabic side) obtained from several LDC corpora including some limited DA data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.7869068384170532}]}, {"text": "Our system uses a standard phrase-based architecture.", "labels": [], "entities": []}, {"text": "The parallel corpus is word-aligned using GIZA++.", "labels": [], "entities": []}, {"text": "Phrase translations of up to 10 words are extracted in the Moses phrase table.", "labels": [], "entities": [{"text": "Phrase translations", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7458658516407013}, {"text": "Moses phrase table", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.7911410331726074}]}, {"text": "The language model for our system is trained on the English side of the bitext augmented with English Gigaword (.", "labels": [], "entities": []}, {"text": "This is only done on the baseline systems.", "labels": [], "entities": []}, {"text": "The English data is tokenized using simple punctuation-based rules.", "labels": [], "entities": []}, {"text": "The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme () using the MADA+TOKAN morphological analyzer and tokenizer v3.1.", "labels": [], "entities": [{"text": "Arabic Treebank (ATB)", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.8790480971336365}]}, {"text": "The Arabic text is also Alif/Ya normalized.", "labels": [], "entities": []}, {"text": "MADA-produced Arabic lemmas are used for word alignment.", "labels": [], "entities": [{"text": "MADA-produced Arabic lemmas", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.8366814057032267}, {"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8108720183372498}]}, {"text": "We use the same development (dev) and test sets used by Salloum and Habash (2011) (we will call them speech-dev and speech-test, respectively) and we compare to them in the next sections.", "labels": [], "entities": []}, {"text": "We also evaluate on two web-crawled blind test sets: the Levantine test set presented in Zbib et al.", "labels": [], "entities": [{"text": "Levantine test set", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9113461573918661}]}, {"text": "(2012) (we will call it web-lev-test) and the Egyptian Dev-MTv2 development data of the DARPA BOLT program (we will call it web-egy-test).", "labels": [], "entities": [{"text": "Egyptian Dev-MTv2 development data", "start_pos": 46, "end_pos": 80, "type": "DATASET", "confidence": 0.8223480880260468}]}, {"text": "The speech-dev set has 1,496 sentences with 32,047 untokenized Arabic words.", "labels": [], "entities": []}, {"text": "The speech-test set has 1,568 sentences with 32,492 untokenized Arabic words.", "labels": [], "entities": []}, {"text": "The web-levtest set has 2,728 sentences with 21,179 untokenized Arabic words.", "labels": [], "entities": [{"text": "web-levtest set", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8396947085857391}]}, {"text": "The web-egy-test set has 1,553 sentences with 21,495 untokenized Arabic words.", "labels": [], "entities": [{"text": "web-egy-test set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8991918861865997}]}, {"text": "The two speech test sets contain multi-dialect (e.g., Iraqi, Levantine, Gulf, and Egyptian) broadcast conversational (BC) segments (with three reference translations), and broadcast news (BN) segments (with only one reference, replicated three times).", "labels": [], "entities": []}, {"text": "The web-egy-test has two references while the web-levtest has only one reference.", "labels": [], "entities": []}, {"text": "Results are presented in terms of BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9987531900405884}]}, {"text": "All evaluation results are case insensitive.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Revisiting our motivating example, but with ELISSA-based DA-to-MSA middle step. ELISSA's output is  Alif/Ya normalized. Parentheses are added for illustrative reasons to highlight how multi-word DA constructions are  selected and translated. Superscript indices link the selected words and phrases with their MSA translations.", "labels": [], "entities": []}, {"text": " Table 4: Results for the speech-dev set in terms of BLEU. The 'Diff.' column shows result differences from the  baseline. The rows of the table are the different systems (baseline and ELISSA's experiments). The name of the  system in ELISSA's experiments denotes the combination of selection method. In all ELISSA's experiments, all word- based translation methods are tried. Phrase-based translation methods are used when phrase-based selection is used  (i.e., the last three rows). The best system is in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9984914064407349}, {"text": "Diff.'", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9753109614054362}]}, {"text": " Table 5: Results for the three blind test sets (table columns) in terms of BLEU. The 'Diff.' columns show result  differences from the baselines. The rows of the table are the different systems (baselines and ELISSA's experiments).  The best systems are in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9992782473564148}, {"text": "Diff.'", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9760724703470866}]}]}