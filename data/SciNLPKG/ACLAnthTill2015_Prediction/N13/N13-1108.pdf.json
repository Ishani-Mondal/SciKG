{"title": [], "abstractContent": [{"text": "In natural language question answering (QA) systems, questions often contain terms and phrases that are critically important for retrieving or finding answers from documents.", "labels": [], "entities": [{"text": "natural language question answering (QA)", "start_pos": 3, "end_pos": 43, "type": "TASK", "confidence": 0.7857232647282737}]}, {"text": "We present a learnable system that can extract and rank these terms and phrases (dubbed mandatory matching phrases or MMPs), and demonstrate their utility in a QA system on In-ternet discussion forum data sets.", "labels": [], "entities": [{"text": "In-ternet discussion forum data sets", "start_pos": 173, "end_pos": 209, "type": "DATASET", "confidence": 0.703741329908371}]}, {"text": "The system relies on deep syntactic and semantic analysis of questions only and is independent of relevant documents.", "labels": [], "entities": []}, {"text": "Our proposed model can predict MMPs with high accuracy.", "labels": [], "entities": [{"text": "MMPs", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9657953977584839}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9954003691673279}]}, {"text": "When used in a QA system features derived from the MMP model improve performance significantly over a state-of-the-art baseline.", "labels": [], "entities": []}, {"text": "The final QA system was the best performing system in the DARPA BOLT-IR evaluation.", "labels": [], "entities": [{"text": "DARPA", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.5788773894309998}, {"text": "BOLT-IR", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.5911033153533936}]}], "introductionContent": [{"text": "In most question answering (QA) systems and search engines term-weights are assigned in a context independent fashion using simple TF-IDF like models (.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.8548606276512146}]}, {"text": "Even the more recent advances in information retrieval techniques for query term weighting ( typically rely on bag-of-words models and corpus statistics, such as inverse-document-frequency (IDF), to assign weights to terms in questions.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7245283424854279}, {"text": "query term weighting", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.6692731579144796}, {"text": "inverse-document-frequency (IDF)", "start_pos": 162, "end_pos": 194, "type": "METRIC", "confidence": 0.7806147783994675}]}, {"text": "While such solutions may work for keyword queries of the type common on search engines such as Google, they do not exploit syntactic and semantic information when it comes to well formed natural language questions.", "labels": [], "entities": []}, {"text": "In this paper we propose anew model that identifies important terms and phrases in a natural language question, providing better query analysis that ultimately leads to significant improvements in a QA system.", "labels": [], "entities": []}, {"text": "To motivate the work presented here, consider the query \"How does one apply fora New York daycare license?\".", "labels": [], "entities": []}, {"text": "A bag-of-words model would likely assign a high score to \"New licenses for daycare centers in York county, PA\" because of high word overlap, but it does not answer the question, and also the state is wrong.", "labels": [], "entities": []}, {"text": "A matching component that uses the phrases \"New York,\" \"day care,\" and \"license\" is likely to do better.", "labels": [], "entities": []}, {"text": "However, a better matching component will understand that in the context of this query all three phrases \"New York,\" \"day care\" and \"license\" are important, and that \"New York\" needs to modify \"day care.\"", "labels": [], "entities": []}, {"text": "A snippet that does not contain 1 these important phrases, is unlikely an answer.", "labels": [], "entities": []}, {"text": "We call these important phrases mandatory matching phrases.", "labels": [], "entities": []}, {"text": "In this paper, we explore deep syntactic and semantic analyses of questions to determine and rank MMPs.", "labels": [], "entities": []}, {"text": "Unlike existing work (, where term/concept weights are learned from a set of questions and judged documents based on corpusbased statistics, we annotate questions and build a trainable system to select and score MMPs.", "labels": [], "entities": []}, {"text": "This model relies heavily on existing syntactic parsers and semantic-oriented named-entity recognizers, but does not need question answer pairs.", "labels": [], "entities": []}, {"text": "This is espe-cially attractive at the initial system-building stage when no or little answer data is available.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: firstly, we propose a framework to select and rank important question phrases (MMPs) for question answering in Section 3.", "labels": [], "entities": [{"text": "rank important question phrases (MMPs)", "start_pos": 88, "end_pos": 126, "type": "TASK", "confidence": 0.6507589391299656}, {"text": "question answering", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.841621458530426}]}, {"text": "This framework seamlessly incorporates lexical, syntactic and semantic information, resulting in an MMP prediction F-measure as high as 88.6%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.6021350622177124}]}, {"text": "Secondly, we show that features derived from identified MMPs improve significantly a relevance classification model, in Section 4.2.", "labels": [], "entities": [{"text": "relevance classification", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8171572685241699}]}, {"text": "Thirdly, we show that using the improved relevance model into our QA system results in a statistically significant 5 point improvement in F-measure, in Section 5.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9880058169364929}]}, {"text": "This finding is further corroborated by the results on the official 2012 BOLT IR (IR, 2012) task where the combined system yielded the best performance in the evaluation.", "labels": [], "entities": [{"text": "2012", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.7731887698173523}, {"text": "BOLT IR (IR, 2012) task", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.633161373436451}]}], "datasetContent": [{"text": "The BOLT evaluation consists of 146 questions, mostly event-or topic-related, e.g., \"What are people saying about the ending of NASA's space shuttle program?\".", "labels": [], "entities": [{"text": "BOLT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.90679931640625}]}, {"text": "A system answer, if correct, is mapped manually to a facet, which is one semantic unit that answers the question.", "labels": [], "entities": []}, {"text": "For each question, facets are collected across all participants' submission.", "labels": [], "entities": []}, {"text": "A facet-based F-measure is computed for each participating site.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.975241482257843}]}, {"text": "The recall from which the official Fmeasure is computed is weighted by snippet citations (a citation is a reference to the original document that supports the correct facet).", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9906944036483765}, {"text": "Fmeasure", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.49483492970466614}]}, {"text": "In other words, a snippet with more citations leads to a higher recall than one with less citations.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9990907907485962}]}, {"text": "The performances of 4 participating sites are listed in  . Among 4 participating sites, our system has the highest performance.", "labels": [], "entities": []}, {"text": "SITE 1 has about the same level of precision, with lower recall, while SITE 3 has the best recall, but lower precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9994507431983948}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9995013475418091}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9992996454238892}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9979598522186279}]}, {"text": "The results validate that the MMP question analysis technique presented in this paper is quite effective.", "labels": [], "entities": [{"text": "MMP question analysis", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.9246346553166708}]}], "tableCaptions": [{"text": " Table 1: The performances of the MMP classifier while  incrementally adding features.", "labels": [], "entities": []}, {"text": " Table 3: F-measure for Relevance Prediction.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9880483150482178}, {"text": "Relevance Prediction", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7610137164592743}]}, {"text": " Table 4: End-to-End system result on 59 questions.", "labels": [], "entities": [{"text": "End-to-End", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9642257690429688}]}, {"text": " Table 5. Note that  the F-measure is weighted and is not necessarily a  number between the precision and the recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9964300990104675}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9987590312957764}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9972461462020874}]}, {"text": " Table 5: Official BOLT 2012 IR evaluation results.", "labels": [], "entities": [{"text": "Official BOLT 2012 IR evaluation", "start_pos": 10, "end_pos": 42, "type": "DATASET", "confidence": 0.7507659792900085}]}]}