{"title": [{"text": "Minibatch and Parallelization for Online Large Margin Structured Learning", "labels": [], "entities": [{"text": "Online Large Margin Structured Learning", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.732138854265213}]}], "abstractContent": [{"text": "Online learning algorithms such as perceptron and MIRA have become popular for many NLP tasks thanks to their simpler architecture and faster convergence over batch learning methods.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.5639244318008423}]}, {"text": "However, while batch learning such as CRF is easily parallelizable, online learning is much harder to parallelize: previous efforts often witness a decrease in the converged accuracy, and the speedup is typically very small (\u223c3) even with many (10+) processors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9875268340110779}]}, {"text": "We instead present a much simpler architecture based on \"mini-batches\", which is trivially parallelizable.", "labels": [], "entities": []}, {"text": "We show that, unlike previous methods, minibatch learning (in serial mode) actually improves the converged accuracy for both perceptron and MIRA learning , and when combined with simple paral-lelization, minibatch leads to very significant speedups (up to 9x on 12 processors) on state-of-the-art parsing and tagging systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9697147011756897}]}], "introductionContent": [{"text": "Online structured learning algorithms such as the structured perceptron) and k-best MIRA () have become more and more popular for many NLP tasks such as dependency parsing and part-of-speech tagging.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.7897348403930664}, {"text": "dependency parsing", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.8247095048427582}, {"text": "part-of-speech tagging", "start_pos": 176, "end_pos": 198, "type": "TASK", "confidence": 0.7296483218669891}]}, {"text": "This is because, compared to their batch learning counterparts, online learning methods offer faster convergence rates and better scalability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding.", "labels": [], "entities": []}, {"text": "However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser) usually require the order of days to train on the Treebank on a commodity machine.", "labels": [], "entities": []}, {"text": "There are mainly two ways to address this scalability problem.", "labels": [], "entities": []}, {"text": "On one hand, researchers have been developing modified learning algorithms that allow inexact search ().", "labels": [], "entities": []}, {"text": "However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times.", "labels": [], "entities": []}, {"text": "For example the best-performing method in still requires 5-6 hours to train a very fast parser.", "labels": [], "entities": []}, {"text": "On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via parallelization.", "labels": [], "entities": []}, {"text": "While batch learning such as CRF () is often trivially parallelizable () since each update is a batch-aggregate of the update from each (independent) example, online learning is much harder to parallelize due to the dependency between examples, i.e., the update on the first example should in principle influence the decoding of all remaining examples.", "labels": [], "entities": []}, {"text": "Thus if we decode and update the first and the 1000th examples in parallel, we lose their interactions which is one of the reasons for online learners' fast convergence.", "labels": [], "entities": []}, {"text": "This explains why previous work such as the iterative parameter mixing (IPM) method of witnesses a decrease in the accuracies of parallelly-learned models, and the speedup is typically very small (about 3 in their experiments) even with 10+ processors.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9745338559150696}]}, {"text": "We instead explore the idea of \"minibatch\" for online large-margin structured learning such as perceptron and MIRA.", "labels": [], "entities": []}, {"text": "We argue that minibatch is advantageous in both serial and parallel settings.", "labels": [], "entities": []}, {"text": "First, for minibatch perceptron in the serial set-ting, our intuition is that, although decoding is done independently within one minibatch, updates are done by averaging update vectors in batch, providing a \"mixing effect\" similar to \"averaged parameters\" of which is also found in IPM (, and online EM (.", "labels": [], "entities": [{"text": "IPM", "start_pos": 283, "end_pos": 286, "type": "DATASET", "confidence": 0.9449069499969482}]}, {"text": "Secondly, minibatch MIRA in the serial setting has an advantage that, different from previous methods such as SGD which simply sum up the updates from all examples in a minibatch, a minibatch MIRA update tries to simultaneously satisfy an aggregated set of constraints that are collected from multiple examples in the minibatch.", "labels": [], "entities": [{"text": "minibatch MIRA", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.49948129057884216}]}, {"text": "Thus each minibatch MIRA update involves an optimization over many more constraints than in pure online MIRA, which could potentially lead to a better margin.", "labels": [], "entities": [{"text": "MIRA update", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.8370120227336884}]}, {"text": "In other words we can view MIRA as an online version or stepwise approximation of SVM, and minibatch MIRA can be seen as a better approximation as well as a middleground between pure MIRA and SVM.", "labels": [], "entities": []}, {"text": "More interestingly, the minibatch architecture is trivially parallelizable since the examples within each minibatch could be decoded in parallel on multiple processors (while the update is still done in serial).", "labels": [], "entities": []}, {"text": "This is known as \"synchronous minibatch\" and has been explored by many researchers, but all previous works focus on probabilistic models along with SGD or EM learning methods while our work is the first effort on large-margin methods.", "labels": [], "entities": []}, {"text": "We make the following contributions: \u2022 Theoretically, we present a serial minibatch framework (Section 3) for online large-margin learning and prove the convergence theorems for minibatch perceptron and minibatch MIRA.", "labels": [], "entities": []}, {"text": "\u2022 Empirically, we show that serial minibatch could speedup convergence and improve the converged accuracy for both MIRA and perceptron on state-of-the-art dependency parsing and part-of-speech tagging systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9524371027946472}, {"text": "dependency parsing", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7344975173473358}, {"text": "part-of-speech tagging", "start_pos": 178, "end_pos": 200, "type": "TASK", "confidence": 0.7064715474843979}]}, {"text": "\u2022 In addition, when combined with simple (synchronous) parallelization, minibatch MIRA Algorithm 1 Generic Online Learning.", "labels": [], "entities": [{"text": "MIRA Algorithm 1 Generic Online Learning", "start_pos": 82, "end_pos": 122, "type": "TASK", "confidence": 0.5199093868335088}]}, {"text": "Input: data D = {(x (t) , y (t) )} n t=1 and feature map \u03a6 Output: weight vector w 1: repeat", "labels": [], "entities": [{"text": "Output", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.8877683281898499}]}], "datasetContent": [{"text": "We conduct experiments on two typical structured prediction problems: incremental dependency parsing and part-of-speech tagging; both are done on state-of-the-art baseline.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7004241496324539}, {"text": "part-of-speech tagging", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.7098464518785477}]}, {"text": "We also compare our parallelized minibatch algorithm with the iterative parameter mixing (IPM) method of.", "labels": [], "entities": []}, {"text": "We perform our experiments on a commodity 64-bit Dell Precision T7600 workstation with two 3.1GHz 8-core CPUs (16 processors in total) and 64GB RAM.", "labels": [], "entities": [{"text": "Dell Precision T7600 workstation", "start_pos": 49, "end_pos": 81, "type": "DATASET", "confidence": 0.8816731572151184}]}, {"text": "We use Python 2.7's multiprocessing module in all experiments.", "labels": [], "entities": []}], "tableCaptions": []}