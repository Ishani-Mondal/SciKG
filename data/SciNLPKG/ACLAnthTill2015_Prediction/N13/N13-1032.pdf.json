{"title": [{"text": "Improving reordering performance using higher order and structural features", "labels": [], "entities": [{"text": "Improving reordering", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9450647234916687}]}], "abstractContent": [{"text": "Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order.", "labels": [], "entities": []}, {"text": "This model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the Traveling Salesman Problem (TSP).", "labels": [], "entities": []}, {"text": "However, for efficiently solving the TSP, the model is restricted to pairwise features which examine only a pair of words and their neighborhood.", "labels": [], "entities": [{"text": "TSP", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9107003211975098}]}, {"text": "In this work, we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies.", "labels": [], "entities": []}, {"text": "In addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering.", "labels": [], "entities": []}, {"text": "Our experiments , involving Urdu-English, show that the proposed approach outperforms a state-of-the-art PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hi-ero, by 3 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9931164979934692}, {"text": "MT", "start_pos": 220, "end_pos": 222, "type": "TASK", "confidence": 0.9438524842262268}, {"text": "BLEU", "start_pos": 244, "end_pos": 248, "type": "METRIC", "confidence": 0.993303120136261}]}], "introductionContent": [{"text": "Handling the differences in word orders between pairs of languages is crucial in producing good machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.740117996931076}]}, {"text": "This is especially true for language pairs such as Urdu-English which have significantly different sentence structures.", "labels": [], "entities": []}, {"text": "For example, the typical word order in Urdu is Subject Object Verb whereas the typical word order in English is Subject Verb Object.", "labels": [], "entities": []}, {"text": "Phrase based systems () rely on a lexicalized distortion model and the target language model to produce output words in the correct order.", "labels": [], "entities": [{"text": "Phrase based", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.876129537820816}]}, {"text": "This is known to be inadequate when the languages are very different in terms of word order (refer to in Section 3).", "labels": [], "entities": []}, {"text": "Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.7798158228397369}]}, {"text": "Most techniques for pre-ordering () depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages).", "labels": [], "entities": []}, {"text": "Recent work) has shown that it is possible to learn a reordering model from a relatively small number of hand aligned sentences . This eliminates the need of a source or target parser.", "labels": [], "entities": []}, {"text": "In this work, we build upon the work of which solves the reordering problem by treating it as an instance of the Traveling Salesman Problem (TSP).", "labels": [], "entities": [{"text": "Traveling Salesman Problem (TSP)", "start_pos": 113, "end_pos": 145, "type": "TASK", "confidence": 0.5716250489155451}]}, {"text": "They learn a model which assigns costs to all pairs of words in a sentence, where the cost represents the penalty of putting a word immediately preceding another word.", "labels": [], "entities": []}, {"text": "The best permutation is found via the chained LinKernighan heuristic for solving a TSP.", "labels": [], "entities": []}, {"text": "Since this model relies on solving a TSP efficiently, it cannot capture features other than pairwise features that examine the words and neighborhood for each pair of words in the source sentence.", "labels": [], "entities": []}, {"text": "In the remainder of this paper we refer to this model as the TSP model.", "labels": [], "entities": []}, {"text": "Our aim is to go beyond this limitation of the TSP model and use a richer set of features instead of using pairwise features only.", "labels": [], "entities": []}, {"text": "In particular, we are interested in features that allow us to examine triples of words/POS tags in the candidate reordering per-mutation (this is akin to going from bigram to trigram language models), and also structural features that allow us to examine the properties of the segmentation induced by the candidate permutation.", "labels": [], "entities": []}, {"text": "Togo beyond the set of features incorporated by the TSP model, we do not solve the search problem which would be NP-hard.", "labels": [], "entities": []}, {"text": "Instead, we restrict ourselves to an n-best list produced by the base TSP model and then search in that list.", "labels": [], "entities": []}, {"text": "Using a richer set of features, we learn a model to rerank these nbest reorderings.", "labels": [], "entities": []}, {"text": "The parameters of the model are learned using the averaged perceptron algorithm.", "labels": [], "entities": []}, {"text": "In addition to using a richer set of source side features we also indirectly capture target side features by interpolating the score assigned by our model with the score assigned by the decoder of a MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 199, "end_pos": 201, "type": "TASK", "confidence": 0.930816650390625}]}, {"text": "To justify the use of these informative features, we point to the example in.", "labels": [], "entities": []}, {"text": "Here, the head (driver) of the underlined English Noun Phrase (The driver of the car) appears to the left of the Noun Phrase whereas the head (chaalak {driver}) of the corresponding Urdu Noun Phrase (gaadi {car} ka {of} chaalak {driver}) appears to the right of the Noun Phrase.", "labels": [], "entities": []}, {"text": "To produce the correct reordering of the source Urdu sentence the model has to make an unusual choice of putting gaadi {car} before bola {said}.", "labels": [], "entities": []}, {"text": "We say this is an unusual choice because the model examines only pairwise features and it is unlikely that it would have seen sentences having the bigram \"car said\".", "labels": [], "entities": []}, {"text": "If the exact segmentation of the source sentence was known, then the model could have used the information that the word gaadi {car} appears in a segment whose head is the noun chaalak {driver} and hence its not unusual to put gaadi {car} before bola {said} (because the construct \"NP said\" is not unusual).", "labels": [], "entities": []}, {"text": "However, since the segmentation of the source sentence is not known in advance, we use a heuristic (explained later) to find the segmentation induced by a reordering.", "labels": [], "entities": []}, {"text": "We then extract features (such as first word current segment, end word current segment) to approximate these long range dependencies.", "labels": [], "entities": []}, {"text": "Using this richer set of features with UrduEnglish as the source language pair, our approach outperforms the following state of the art systems: (i) a PBSMT system which uses TSP model for reordering (by 1.3 BLEU points), (ii) a hierarchical PBSMT system (by 3 BLEU points).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 208, "end_pos": 212, "type": "METRIC", "confidence": 0.994236946105957}, {"text": "BLEU", "start_pos": 261, "end_pos": 265, "type": "METRIC", "confidence": 0.9940118193626404}]}, {"text": "The overall fir gaadi ka chaalak kuch bola Gloss: then car of driver said something English: Then the driver of the car said something.", "labels": [], "entities": []}, {"text": "reordering: fir chaalak ka gaadi bola kuch: Example motivating the use of structural features gain is 6.3 BLEU points when compared to a standard PBSMT system which uses a lexicalized distortion model).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9992863535881042}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss our approach of re-ranking the n-best reorderings produced by the TSP model.", "labels": [], "entities": []}, {"text": "This includes a discussion of the model used, the features used and the algorithm used for learning the parameters of the model.", "labels": [], "entities": []}, {"text": "It also includes a discussion on the modification to the Chained Lin-Kernighan heuristic to produce n-best reorderings.", "labels": [], "entities": []}, {"text": "Next, in Section 3 we describe our experimental setup and report the results of our experiments.", "labels": [], "entities": []}, {"text": "In Section 4 we present some discussions based on our study.", "labels": [], "entities": []}, {"text": "In section 5 we briefly describe some prior related work.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we present some concluding remarks and highlight possible directions for future work.", "labels": [], "entities": []}, {"text": "2 Re-ranking using higher order and structural features As mentioned earlier, the TSP model) looks only at local features fora word pair (w i , w j ).", "labels": [], "entities": []}, {"text": "We believe that for better reordering it is essential to look at higher order and structural features (i.e., features which look at the overall structure of a sentence).", "labels": [], "entities": []}, {"text": "The primary reason why Visweswariah et al.", "labels": [], "entities": []}, {"text": "(2011) consider only pairwise bigram features is that with higher order features the reordering problem can no longer be cast as a TSP and hence cannot be solved using existing efficient heuristic solvers.", "labels": [], "entities": []}, {"text": "However, we do not have to deal with an NP-Hard search problem because instead of considering all possible reorderings we restrict our search space to only the n-best reorderings produced by the base TSP model.", "labels": [], "entities": []}, {"text": "Formally, given a set of reorderings, \u03a0 = [\u03c0 1 , \u03c0 2 , \u03c0 3 , ...., \u03c0 n ], fora source sentence s, we are interesting in assigning a score, score(\u03c0), to each of these reorderings and pick the reordering which has the highest score.", "labels": [], "entities": []}, {"text": "In this paper, we parametrize this score as: where, \u03b8 is the weight vector and \u03c6(\u03c0) is a vector of features extracted from the reordering \u03c0.", "labels": [], "entities": [{"text": "\u03c6(\u03c0)", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8994159996509552}]}, {"text": "The aim then is to find, In the following sub-sections, we first briefly describe our overall approach towards finding \u03c0 * . Next, we describe our modification to the LinKernighan heuristic for producing n-best outputs for TSP instead of the 1-best output used by).", "labels": [], "entities": []}, {"text": "We then discuss the features used for re-ranking these n-best outputs, followed by a discussion on the learning algorithm used for estimating the parameters of the model.", "labels": [], "entities": []}, {"text": "Finally, we describe how we interpolate the score assigned by our model with the score assigned by the decoder of a SMT engine to indirectly capture target side features.", "labels": [], "entities": [{"text": "SMT engine", "start_pos": 116, "end_pos": 126, "type": "TASK", "confidence": 0.9172657430171967}]}], "datasetContent": [{"text": "We evaluated our reordering approach on UrduEnglish.", "labels": [], "entities": [{"text": "UrduEnglish", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.9755075573921204}]}, {"text": "We use two types of evaluation, one intrinsic and one extrinsic.", "labels": [], "entities": []}, {"text": "For intrinsic evaluation, we compare the reordered source sentence in Urdu with a reference reordering obtained from the hand alignments using BLEU (referred to as monolingual BLEU or mBLEU by ).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.996483564376831}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9257993698120117}]}, {"text": "Additionally, we evaluate the effect of reordering on MT performance using BLEU (extrinsic evaluation).", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9866493344306946}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9988027811050415}]}, {"text": "As mentioned earlier, our training process involves two phases : (i) Generating n-best reorderings for the training data and (ii) using these n-best reorderings to train a perceptron model.", "labels": [], "entities": []}, {"text": "We use the same data for training the reordering model as well as our perceptron model.", "labels": [], "entities": []}, {"text": "This data contains 180K words of manual alignments (part of the NIST MT-08 training data) and 3.9M words of automatically generated machine alignments (1.7M words from the NIST MT-08 training data 1 and 2.2M words extracted from sources on the web 2 ).", "labels": [], "entities": [{"text": "NIST MT-08 training data", "start_pos": 64, "end_pos": 88, "type": "DATASET", "confidence": 0.8881236016750336}, {"text": "NIST MT-08 training data 1", "start_pos": 172, "end_pos": 198, "type": "DATASET", "confidence": 0.9301379084587097}]}, {"text": "The machine alignments were generated using a supervised maximum entropy model and then corrected using an improved correction model).", "labels": [], "entities": []}, {"text": "We first divide the training data into 10 folds.", "labels": [], "entities": []}, {"text": "The n-best reorderings for each fold are then generated using a model trained on the remaining 9 folds.", "labels": [], "entities": []}, {"text": "This division into 10 folds is done for reasons explained earlier in Section 2.1.", "labels": [], "entities": []}, {"text": "These n-best reorderings are then used to train the perceptron model as described in Section 2.4.", "labels": [], "entities": []}, {"text": "Note that Visweswariah et al.", "labels": [], "entities": []}, {"text": "(2011) used only manually aligned data for training the TSP model.", "labels": [], "entities": [{"text": "TSP", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.7508135437965393}]}, {"text": "However, we use machine aligned data in addition to manually aligned data for training the TSP model as it leads to better performance.", "labels": [], "entities": []}, {"text": "We used this improvised TSP model as the state of the art baseline (rows 2 and 3 in respectively) for comparing with our approach.", "labels": [], "entities": []}, {"text": "We observed that the perceptron algorithm converges after 5 iterations beyond which there is very little (<1%) improvement in the bigram precision on 1 http://www.ldc.upenn.edu 2 http://centralasiaonline.com the training data itself (bigram precision is the fraction of word pairs which are correctly put next to each other).", "labels": [], "entities": []}, {"text": "Hence, for all the numbers reported in this paper, we used 5 iterations of perceptron training.", "labels": [], "entities": []}, {"text": "Similarly, while generating the n-best reorderings, we experimented with following values of n : 10, 25, 50, 100 and 200.", "labels": [], "entities": []}, {"text": "We observed that, by restricting the search space to the top-50 reorderings we get the best reordering performance (mBLEU) on a development set.", "labels": [], "entities": [{"text": "mBLEU)", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9188755452632904}]}, {"text": "Hence, we used n=50 for our MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9854093194007874}]}, {"text": "For intrinsic evaluation we use a development set of 8017 Urdu tokens reordered manually.", "labels": [], "entities": []}, {"text": "compares the performance of the top-1 reordering output by our algorithm with the top-1 reordering generated by the improved TSP model in terms of mBLEU.", "labels": [], "entities": []}, {"text": "We see again of 1.8 mBLEU points with our approach.", "labels": [], "entities": []}, {"text": "Next, we seethe impact of the better reorderings produced by our system on the performance of a state-of-the-art MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9850329756736755}]}, {"text": "For this, we used a standard phrase based system) with a lexicalized distortion model with a window size of +/-4 words (.", "labels": [], "entities": []}, {"text": "As mentioned earlier, our training data consisted of 3.9M words including the NIST MT-08 training data.", "labels": [], "entities": [{"text": "NIST MT-08 training data", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.8703356981277466}]}, {"text": "We use HMM alignments along with higher quality alignments from a supervised aligner).", "labels": [], "entities": []}, {"text": "The Gigaword English corpus was used for building the English language model.", "labels": [], "entities": [{"text": "Gigaword English corpus", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.9169233838717142}]}, {"text": "We report results on the NIST MT-08 evaluation set, averaging BLEU scores from the News and Web conditions to provide a single BLEU score.", "labels": [], "entities": [{"text": "NIST MT-08 evaluation set", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.9202604442834854}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9990118741989136}, {"text": "News and Web conditions", "start_pos": 83, "end_pos": 106, "type": "DATASET", "confidence": 0.9347268640995026}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9980764389038086}]}, {"text": "compares the MT performance obtained by reordering the training and test data using the following approaches: 1.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9855016469955444}]}, {"text": "No pre-ordering: A baseline system which does not use any source side reordering as a preprocessing step 2.", "labels": [], "entities": []}, {"text": "HIERO : A state of the art hierarchical phrase based translation system 3.", "labels": [], "entities": [{"text": "HIERO", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7775747776031494}, {"text": "phrase based translation", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6708370745182037}]}, {"text": "TSP: A system which uses the 1-best reordering produced by the TSP model   which reranks n-best reorderings produced by TSP using higher order and structural features 5.", "labels": [], "entities": [{"text": "TSP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9272117614746094}]}, {"text": "Interpolating with MT score : A system which interpolates the score assigned to a reordering by our model with the score assigned by a MT system We used Joshua 4.0 ( which provides an open source implementation of HIERO.", "labels": [], "entities": []}, {"text": "For training, tuning and testing HIERO we used the same experimental setup as described above.", "labels": [], "entities": [{"text": "HIERO", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.899260938167572}]}, {"text": "As seen in, we get an overall gain of 6.2 BLEU points with our approach as compared to a baseline system which does not use any reordering.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9994658827781677}]}, {"text": "More importantly, we outperform (i) a PBSMT system which uses the TSP model by 1.3 BLEU points and (ii) a state of the art hierarchical phrase based translation system by 3 points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9980782270431519}]}], "tableCaptions": [{"text": " Table 3: mBLEU scores for Urdu to English reordering  using different models.", "labels": [], "entities": [{"text": "mBLEU", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.5479870438575745}, {"text": "Urdu to English reordering", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.5773844718933105}]}, {"text": " Table 4: MT performance for Urdu to English without re- ordering and with reordering using different approaches.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9739623069763184}]}, {"text": " Table 5: mBLEU improvements on sentences of different  lengths", "labels": [], "entities": [{"text": "mBLEU", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.6931102871894836}]}, {"text": " Table 6: Ablation test indicating the contribution of each  feature to the reordering performance.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9979925155639648}]}]}