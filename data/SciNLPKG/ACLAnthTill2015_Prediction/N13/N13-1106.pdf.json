{"title": [{"text": "Answer Extraction as Sequence Tagging with Tree Edit Distance", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9462874531745911}, {"text": "Sequence Tagging", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.876941978931427}]}], "abstractContent": [{"text": "Our goal is to extract answers from pre-retrieved sentences for Question Answering (QA).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.8395713627338409}]}, {"text": "We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types.", "labels": [], "entities": []}, {"text": "This casts answer extraction as an answer sequence tagging problem for the first time, where knowledge of shared structure between question and source sentence is incorporated through features based on Tree Edit Distance (TED).", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.907701849937439}, {"text": "answer sequence tagging", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.6399766405423483}]}, {"text": "Our model is free of manually created question and answer templates, fast to run (processing 200 QA pairs per second excluding parsing time), and yields an F1 of 63.3% on anew public dataset based on prior TREC QA evaluations.", "labels": [], "entities": [{"text": "F1", "start_pos": 156, "end_pos": 158, "type": "METRIC", "confidence": 0.99957674741745}]}, {"text": "The developed system is open-source, and includes an implementation of the TED model that is state of the art in the task of ranking QA pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "The success of IBM's Watson system for Question Answering (QA) has illustrated a continued public interest in this topic.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8219073951244354}]}, {"text": "Watson is a sophisticated piece of software engineering consisting of many components tied together in a large parallel architecture.", "labels": [], "entities": [{"text": "Watson", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8743273019790649}]}, {"text": "It took many researchers working full time for years to construct.", "labels": [], "entities": []}, {"text": "Such resources are not available to individual academic researchers.", "labels": [], "entities": []}, {"text": "If they are interested in evaluating new ideas on some aspect of QA, they must either construct a full system, or create a focused subtask * Performed while faculty at Johns Hopkins University.", "labels": [], "entities": [{"text": "QA", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.8681354522705078}]}, {"text": "paired with a representative dataset.", "labels": [], "entities": []}, {"text": "We follow the latter approach and focus on the task of answer extraction, i.e., producing the exact answer strings fora question.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8139772117137909}]}, {"text": "We propose the use of a linear-chain Conditional Random Field (CRF) () in order to cast the problem as one of sequence tagging by labeling each token in a candidate sentence as either Beginning, Inside or Outside (BIO) of an answer.", "labels": [], "entities": []}, {"text": "This is to our knowledge the first time a CRF has been used to extract answers.", "labels": [], "entities": []}, {"text": "We utilize not only traditional contextual features based on POS tagging, dependency parsing and Named Entity Recognition (NER), but most importantly, features extracted from a Tree Edit Distance (TED) model for aligning an answer sentence tree with the question tree.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.6587695777416229}, {"text": "dependency parsing", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7324264496564865}, {"text": "Named Entity Recognition (NER)", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.6942461828390757}]}, {"text": "The linear-chain CRF, when trained to learn the associations between question and answer types, is a robust approach against error propagation introduced in the NLP pipeline.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.72687928378582}]}, {"text": "For instance, given an NER tool that always (i.e., in both training and test data) recognizes the pesticide DDT as an ORG, our model realizes, when a question is asked about the type of chemicals, the correct answer might be incorrectly but consistently recognized as ORG by NER.", "labels": [], "entities": []}, {"text": "This helps reduce errors introduced by wrong answer types, which were estimated as the most significant contributer (36.4%) of errors in the then state-of-the-art QA system of.", "labels": [], "entities": []}, {"text": "The features based on TED allow us to draw the connection between the question and answer sentences before answer extraction, whereas traditionally the exercise of answer validation) has been performed after as a remedy to ensure the answer is really \"about\" the question.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7122377008199692}]}, {"text": "Motivated by a desire fora fast runtime, 2 we base our TED implementation on the dynamicprogramming approach of, which helps our final system process 200 QA pairs per second on standard desktop hardware, when input is syntactically pre-parsed.", "labels": [], "entities": []}, {"text": "In the following we first provide background on the TED model, going onto evaluate our implementation against prior work in the context of question answer sentence ranking (QASR), achieving state of the art in that task.", "labels": [], "entities": [{"text": "TED", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.7567101120948792}, {"text": "question answer sentence ranking (QASR)", "start_pos": 139, "end_pos": 178, "type": "TASK", "confidence": 0.7083687313965389}]}, {"text": "We then describe how we couple TED features to a linear-chain CRF for answer extraction, providing the set of features used, and finally experimental results on an extraction dataset we make public (together with the software) to the community.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8938703238964081}]}, {"text": "Related prior work is interspersed throughout the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained and tested on the dataset from, which spans QA pairs from TREC QA 8-13 (see).", "labels": [], "entities": [{"text": "TREC QA 8-13", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.7767893671989441}]}, {"text": "Per question, sentences with non-stopword overlap were first retrieved from the task collection, which were then compared against the TREC answer pattern (in the form of Perl regular expressions).", "labels": [], "entities": [{"text": "TREC", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.8258535861968994}]}, {"text": "If a sentence matched, then it was deemed a (noisy) positive example.", "labels": [], "entities": []}, {"text": "Finally, TRAIN, DEV and TEST were manually corrected for errors.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9714086055755615}, {"text": "DEV", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.7954397797584534}, {"text": "TEST", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9824509024620056}]}, {"text": "Those authors decided to limit candidate source senSystem MAP MRR 0.6029 0.6852 0.6091 0.6917 0.5951 0.6951 this paper  tences to be no longer than 40 words.", "labels": [], "entities": [{"text": "senSystem MAP MRR 0.6029 0.6852 0.6091 0.6917 0.5951 0.6951", "start_pos": 48, "end_pos": 107, "type": "DATASET", "confidence": 0.7346918632586797}]}, {"text": "Keeping with prior work, those questions with only positive or negative examples were removed, leaving 94 of the original 100 questions for evaluation.", "labels": [], "entities": []}, {"text": "The data was processed by with the following tool chain: POS tags via MX-POST; parse trees via MSTParser () with 12 coarsegrained dependency relation labels; and named entities via Identifinder ().", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 95, "end_pos": 104, "type": "DATASET", "confidence": 0.9186705946922302}]}, {"text": "Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported in.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.9397435188293457}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 33, "end_pos": 59, "type": "METRIC", "confidence": 0.9732778370380402}]}, {"text": "Our implementation gives state of the art performance, and is furthered improved by our inclusion of semantic features drawn from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.9562186598777771}]}], "tableCaptions": [{"text": " Table 2: Distribution of data, with imbalance towards", "labels": [], "entities": []}, {"text": " Table 3: Results on the QA Sentence Ranking task.", "labels": [], "entities": [{"text": "QA Sentence Ranking task", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.5987849980592728}]}, {"text": " Table 5: Performance on TEST. \"CRF\" only takes  votes from candidates tagged by the sequence tag- ger. \"CRF forced\" (described in  \u00a73.3) further col- lects answer candidates from sentences that CRF  does not tag an answer by detecting outliers.", "labels": [], "entities": []}, {"text": " Table 6: F1 based on feature ablation tests.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9964291453361511}]}]}