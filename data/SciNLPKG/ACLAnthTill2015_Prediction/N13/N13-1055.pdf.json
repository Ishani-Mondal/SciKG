{"title": [{"text": "Robust Systems for Preposition Error Correction Using Wikipedia Revisions", "labels": [], "entities": [{"text": "Preposition Error Correction", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.8885161876678467}]}], "abstractContent": [{"text": "We show that existing methods for training preposition error correction systems, whether using well-edited text or error-annotated corpora , do not generalize across very different test sets.", "labels": [], "entities": [{"text": "preposition error correction", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6622404058774313}]}, {"text": "We present anew, large error-annotated corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics.", "labels": [], "entities": []}, {"text": "This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections.", "labels": [], "entities": [{"text": "preposition corrections", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.722413182258606}]}], "introductionContent": [{"text": "One of the main themes that has defined the field of automatic grammatical error correction has been the availability of error-annotated learner data to train and test a system.", "labels": [], "entities": [{"text": "automatic grammatical error correction", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.6588380336761475}]}, {"text": "Some errors, such as determinernoun number agreement, are easily corrected using rules and regular expressions ().", "labels": [], "entities": []}, {"text": "On the other hand, errors involving the usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics.", "labels": [], "entities": []}, {"text": "These errors are better handled by statistical models which potentially require millions of training examples.", "labels": [], "entities": []}, {"text": "Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage); 2) training on examples of correct usage and artificially generated errors (; and 3) training on examples of correct usage and real learner errors ().", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6699346403280894}]}, {"text": "The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor.", "labels": [], "entities": []}, {"text": "In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history.", "labels": [], "entities": [{"text": "Wikipedia revision history", "start_pos": 158, "end_pos": 184, "type": "DATASET", "confidence": 0.7960602243741354}]}, {"text": "While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation and spelling correction, this resource has not been used for the task of grammatical error correction.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.8571470379829407}, {"text": "spelling correction", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8960752785205841}, {"text": "grammatical error correction", "start_pos": 176, "end_pos": 204, "type": "TASK", "confidence": 0.6097517808278402}]}, {"text": "To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one).", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6490417718887329}, {"text": "preposition selection", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.6858925819396973}]}, {"text": "We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data.", "labels": [], "entities": [{"text": "Wikipedia revision data", "start_pos": 107, "end_pos": 130, "type": "DATASET", "confidence": 0.9317809542020162}]}, {"text": "We also generate artificial errors using the confusion distributions derived from this data.", "labels": [], "entities": []}, {"text": "We compare both of these approaches to models trained on well-edited text and evaluate each on three test sets with a range of different characteristics.", "labels": [], "entities": []}, {"text": "Each training paradigm is applied to multiple data sources for comparison.", "labels": [], "entities": []}, {"text": "With these multiple evaluations, we address the following research questions: is more useful for correcting preposition errors: a large amount of well-edited text, a large amount of potentially noisy error-annotated data (either artificially generated or automatically extracted) or a smaller amount of higher quality error-annotated data?", "labels": [], "entities": [{"text": "correcting preposition", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.8413538634777069}]}, {"text": "2. Given error-annotated data, is it better to train on the corrections directly or to use the confusion distributions derived from these corrections for generating artificial errors in welledited text?", "labels": [], "entities": []}, {"text": "3. What is the impact of having a mismatch in the error distributions of the training and test sets?", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the preposition error correction model described in to evaluate the many ways of using Wikipedia error corrections as described in the Section 4.", "labels": [], "entities": [{"text": "preposition error correction", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.520277718702952}]}, {"text": "We use this system since it has been recreated for other work ( and is similar in methodology to 7 Tajiri et al.", "labels": [], "entities": []}, {"text": "(2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8.", "labels": [], "entities": [{"text": "Lang-8", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.9579429626464844}]}, {"text": "They kindly provided us with their scripts to carryout the scraping of Lang-8.", "labels": [], "entities": [{"text": "scraping", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.9699277281761169}, {"text": "Lang-8", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.6428481936454773}]}, {"text": "The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results.", "labels": [], "entities": [{"text": "HOO 2011 shared task", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.5238166004419327}, {"text": "preposition selection error", "start_pos": 70, "end_pos": 97, "type": "METRIC", "confidence": 0.5952431758244833}]}, {"text": "Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. and De.", "labels": [], "entities": [{"text": "preposition error detection", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.571339746316274}]}, {"text": "In short, the method models the problem of preposition error correction (for replacement errors) as a 36-way classification problem using a multinomial logistic regression model.", "labels": [], "entities": [{"text": "preposition error correction", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6010726392269135}]}, {"text": "The system uses 25 lexical, syntactic and n-gram features derived from the contexts of each preposition training instance.", "labels": [], "entities": []}, {"text": "We modified the training paradigm of so that a model could be trained on examples of correct usage as well as actual errors.", "labels": [], "entities": []}, {"text": "We did this by adding anew feature specifying the writer's original preposition (as in and).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Preposition selection error correction results (f-score). The systems with scores in bold are statistically  significantly better than all systems marked with an asterisk (p < 0.01). Confidence intervals were obtained using  bootstrap resampling with 50,000 replicates.", "labels": [], "entities": []}]}