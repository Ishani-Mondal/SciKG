{"title": [{"text": "Systematic Comparison of Professional and Crowdsourced Reference Translations for Machine Translation", "labels": [], "entities": [{"text": "Systematic Comparison of Professional and Crowdsourced Reference Translations", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.5414247587323189}, {"text": "Machine Translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7144857048988342}]}], "abstractContent": [{"text": "We present a systematic study of the effect of crowdsourced translations on Machine Translation performance.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8526599705219269}]}, {"text": "We compare Machine Translation systems trained on the same data but with translations obtained using Amazon's Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7984570562839508}]}, {"text": "We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online crowdsourcing services have been shown to be a cheap and effective data annotation resource for various Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "The resulting quality of annotations is high enough to be used for training statistical NLP models, with a saving in cost and time of up to an order of magnitude.", "labels": [], "entities": []}, {"text": "Statistical Machine Translation (SMT) is one of the NLP tasks that can benefit from crowdsourced annotations.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8561627964178721}]}, {"text": "With appropriate quality control mechanisms, reference translations collected by crowdsourcing have been successfully used for training and evaluating SMT systems (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 151, "end_pos": 154, "type": "TASK", "confidence": 0.9549195170402527}]}, {"text": "In this work, we used Amazon's Mechanical Turk (MTurk) to obtain alternative reference translations of four Arabic-English parallel corpora previously released by the Linguistic Data Consortium (LDC) for the DARPA BOLT program.", "labels": [], "entities": []}, {"text": "This data, totaling over 500K Arabic tokens, was originally collected from web discussion forums and translated professionally to English.", "labels": [], "entities": []}, {"text": "We used alternative MTurk translations of the same data to train and evaluation MT systems; and conducted the first systematic study that quantifies the effect of the reference translation process on MT output.", "labels": [], "entities": [{"text": "MTurk translations", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.8809154033660889}, {"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.970061719417572}, {"text": "MT output", "start_pos": 200, "end_pos": 209, "type": "TASK", "confidence": 0.92729052901268}]}, {"text": "We found that: \u2022 Mechanical Turk can be used to translate enough data for training an MT system at 1/10th the price of professional translation, and at a much faster rate.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9673924446105957}]}, {"text": "\u2022 Training MT systems on MTurk reference translations gives the same performance as training with professional translations at 20% of the cost.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9578379988670349}, {"text": "MTurk reference translations", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.7331387599309286}]}, {"text": "\u2022 A second translation of the development set obtained via MTurk improves parameter tuning and output evaluation.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8023003935813904}]}], "datasetContent": [{"text": "The MT system used is based on a string-todependency-tree hierarchical model of.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9444416165351868}]}, {"text": "Sentence alignment was done using GIZA++.", "labels": [], "entities": [{"text": "Sentence alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9559750556945801}, {"text": "GIZA++", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8397637009620667}]}, {"text": "Decoder features include translation probabilities, smoothed lexical probabilities, and a dependency tree language model.", "labels": [], "entities": [{"text": "translation probabilities", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.8998914062976837}]}, {"text": "Additionally, we used 50,000 sparse, binaryvalued source and target features based on.", "labels": [], "entities": []}, {"text": "The English language model was trained on 7 billion words from the LDC Gigaword corpus and from a web crawl.", "labels": [], "entities": [{"text": "LDC Gigaword corpus", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.9292328357696533}]}, {"text": "We used expected BLEU maximization to tune feature weights.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9987133741378784}]}, {"text": "We defined a tuning set (3581 segments, 43.3K tokens) and a test set (4166 segments, 47.7K tokens) using LDC2012E30, the corpus designated as a development set by the LDC, augmented with around 50K Words held out from LDC2012E15 and LDC2012E19, to make a development set large enough to tune the large number of feature weights 2 . The remaining data was used for training.", "labels": [], "entities": [{"text": "LDC", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.9132899045944214}]}, {"text": "We two versions of each set: one with the professional reference translations for the target, and the other with the same source data, but the MTurk translations.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.8602598309516907}]}, {"text": "We defined two versions of the test and tuning sets similarly.", "labels": [], "entities": []}, {"text": "We report translation results in terms of lower-case BLEU scores ().", "labels": [], "entities": [{"text": "translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.95905601978302}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9985852241516113}]}], "tableCaptions": [{"text": " Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference transla- tions. All results use professional references for the tuning and test sets.", "labels": [], "entities": [{"text": "MTurk reference transla- tions", "start_pos": 90, "end_pos": 120, "type": "DATASET", "confidence": 0.7967463374137879}]}, {"text": " Table 2: Effect of Tuning and Scoring References on MT.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 20, "end_pos": 26, "type": "TASK", "confidence": 0.9548065066337585}, {"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9716169834136963}]}]}