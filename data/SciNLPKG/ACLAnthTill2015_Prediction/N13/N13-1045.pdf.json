{"title": [{"text": "Using a Supertagged Dependency Language Model to Select a Good Translation in System Combination", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel, structured language model-Supertagged Dependency Language Model to model the syntactic dependencies between words.", "labels": [], "entities": []}, {"text": "The goal is to identify ungrammatical hypotheses from a set of candidate translations in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features.", "labels": [], "entities": [{"text": "MT system combination", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7920993566513062}]}, {"text": "We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.6953206360340118}]}, {"text": "Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, there has been a burgeoning interest in incorporating syntactic structure into Statistical machine translation (SMT) models (e..g,).", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.8090903709332148}]}, {"text": "In addition to modeling syntactic structure in the decoding process, a methodology for candidate translation selection has also emerged.", "labels": [], "entities": [{"text": "candidate translation selection", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.7734073797861735}]}, {"text": "This methodology first generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation.", "labels": [], "entities": []}, {"text": "The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process.", "labels": [], "entities": []}, {"text": "The methodology is usually applied in two scenarios: one is as part of an n-best reranking), where n-best candidate translations are generated through a decoding process.", "labels": [], "entities": []}, {"text": "The other is translation selection or reranking, where candidate translations are generated by different decoding processes or different decoders.", "labels": [], "entities": [{"text": "translation selection", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.9745018780231476}]}, {"text": "This paper belongs to the latter; the goal is to identify ungrammatical hypotheses from given candidate translations using grammatical knowledge in the target language that expresses syntactic dependencies between words.", "labels": [], "entities": []}, {"text": "To achieve that, we propose a novel Structured Language Model (SLM) -Supertagged Dependency Language Model (SDLM) to model the syntactic dependencies between words.", "labels": [], "entities": []}, {"text": "Supertag () is an elementary syntactic structure based on Lexicalized Tree Adjoining Grammar (LTAG).", "labels": [], "entities": [{"text": "Lexicalized Tree Adjoining Grammar (LTAG)", "start_pos": 58, "end_pos": 99, "type": "TASK", "confidence": 0.5844440502779824}]}, {"text": "Traditional supertagged n-gram LM predicts the next supertag based on the immediate words to the left with supertags, so it cannot explicitly model long-distance dependency relations.", "labels": [], "entities": []}, {"text": "In contrast, SDLM predicts the next supertag using the words with supertags on which it syntactically depend, and these words could be anywhere and arbitrarily far apart in a sentence.", "labels": [], "entities": []}, {"text": "A candidate translation's grammatical degree or \"fluency\" can be measured by simply calculating the SDLM likelihood of the supertagged dependency structure that spans the entire sentence.", "labels": [], "entities": []}, {"text": "To obtain the supertagged dependency structure, the most intuitive way is through a LTAG parser (.", "labels": [], "entities": []}, {"text": "However, this could be very slow as it has time complexity of O(n 6 ).", "labels": [], "entities": [{"text": "O", "start_pos": 62, "end_pos": 63, "type": "METRIC", "confidence": 0.9614980220794678}]}, {"text": "Instead we propose an alternative mechanism in this paper: first we use a constituent parser 1 of O(n 3 ) ~ O(n 5 ) to obtain the parse of a sentence, and then we extract elementary trees with dependencies from the parse in linear time.", "labels": [], "entities": []}, {"text": "Aside from the consideration of time complexity, another motivation of this two-step mechanism is that compared with LTAG parsing, the mechanism is more flexible for defining syntactic structures of elementary trees for our needs.", "labels": [], "entities": [{"text": "LTAG parsing", "start_pos": 117, "end_pos": 129, "type": "TASK", "confidence": 0.6577837020158768}]}, {"text": "Because those structures are defined only within the elementary tree extractor, we can easily adjust the definition of those structures within the extractor and avoid redesigning or retraining our constituent parser.", "labels": [], "entities": []}, {"text": "We experiment with sentence-level translation combination of five different translation systems; the goal is for the system to select the best translation for each input source sentence among the translations provided by the five systems.", "labels": [], "entities": [{"text": "sentence-level translation", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6590112745761871}]}, {"text": "The results show a significant improvement of 1.45 Bleu score over the best single MT system and 0.72 Bleu score over a baseline sentence-level combination system of using consensus and ngram LM.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9760042726993561}, {"text": "Bleu score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9817965626716614}]}], "datasetContent": [{"text": "Our experiments are conducted and reported on the Chinese-English dataset from NIST 2008 (LDC2010T01).", "labels": [], "entities": [{"text": "Chinese-English dataset from NIST 2008 (LDC2010T01)", "start_pos": 50, "end_pos": 101, "type": "DATASET", "confidence": 0.9031591564416885}]}, {"text": "It consists of four human reference translations and corresponding machine translations for the NIST Open MT08 test set, which consists of newswire and web data.", "labels": [], "entities": [{"text": "NIST Open MT08 test set", "start_pos": 96, "end_pos": 119, "type": "DATASET", "confidence": 0.9342493653297425}]}, {"text": "The test set contains 105 documents with 1312 sentences and output from 23 machine translation systems.", "labels": [], "entities": []}, {"text": "Each system provides the top one translation hypothesis for every sentence.", "labels": [], "entities": []}, {"text": "We further divide the NIST Open MT08 test set into the tuning set and test set for our experiment of sentence-level translation combination.", "labels": [], "entities": [{"text": "NIST Open MT08 test set", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.9231894612312317}, {"text": "sentence-level translation combination", "start_pos": 101, "end_pos": 139, "type": "TASK", "confidence": 0.7730891009171804}]}, {"text": "We divided the 1312 sentences into tuning data of 524 sentences and the test set of 788 sentences.", "labels": [], "entities": []}, {"text": "Out of 23 MT systems, we manually select the top five MT systems as our MT systems for our combination experiment.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9396077990531921}]}, {"text": "In terms of SDLM training, since the size of TreeBank-extracted elementary trees is much smaller compared to most practical n-gram LMs trained from the Gigaword corpus, we also extract elementary trees from automatically-generated parses of part of the Gigaword corpus (around oneyear newswire of \"afp_eng\" in Gigaword 4) in addition to TreeBank-extracted elementary trees.", "labels": [], "entities": [{"text": "SDLM", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9778743386268616}, {"text": "Gigaword corpus", "start_pos": 152, "end_pos": 167, "type": "DATASET", "confidence": 0.9304232597351074}, {"text": "Gigaword corpus", "start_pos": 253, "end_pos": 268, "type": "DATASET", "confidence": 0.9263498187065125}]}], "tableCaptions": []}