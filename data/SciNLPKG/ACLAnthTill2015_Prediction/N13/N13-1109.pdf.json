{"title": [{"text": "A Just-In-Time Keyword Extraction from Meeting Transcripts", "labels": [], "entities": []}], "abstractContent": [{"text": "Ina meeting, it is often desirable to extract keywords from each utterance as soon as it is spoken.", "labels": [], "entities": []}, {"text": "Thus, this paper proposes a just-in-time keyword extraction from meeting transcripts.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.761556088924408}]}, {"text": "The proposed method considers two major factors that make it different from keyword extraction from normal texts.", "labels": [], "entities": [{"text": "keyword extraction from normal texts", "start_pos": 76, "end_pos": 112, "type": "TASK", "confidence": 0.8411188244819641}]}, {"text": "The first factor is the temporal history of preceding utterances that grants higher importance to recent utterances than old ones, and the second is topic relevance that forces only the preceding utterances relevant to the current utterance to be considered in keyword extraction.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 261, "end_pos": 279, "type": "TASK", "confidence": 0.7533915340900421}]}, {"text": "Our experiments on two data sets in English and Korean show that the consideration of the factors results in performance improvement in keyword extraction from meeting transcripts.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.781210720539093}]}], "introductionContent": [{"text": "A meeting is generally accomplished by a number of participants and a wide range of subjects are discussed.", "labels": [], "entities": []}, {"text": "Therefore, it would be helpful to meeting participants to provide them with some additional information related to the current subject.", "labels": [], "entities": []}, {"text": "For instance, assume that a participant is discussing a specific topic with other participants at a meeting.", "labels": [], "entities": []}, {"text": "The summary of previous meetings on the topic is then one of the most important resources for her discussion.", "labels": [], "entities": []}, {"text": "In order to provide information on a topic to participants, keywords should be first generated for the topic since keywords are often representatives of a topic.", "labels": [], "entities": []}, {"text": "A number of techniques have been proposed for automatic keyword extraction, and they are designed to extract keywords from a written document.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.6979431807994843}]}, {"text": "However, they are not suitable for meeting transcripts.", "labels": [], "entities": []}, {"text": "Ina meeting, it is often desirable to extract keywords at the time at which anew utterance is made for just-in-time service of additional information.", "labels": [], "entities": []}, {"text": "Otherwise, the extracted keywords become just the important words at the end of the meeting.", "labels": [], "entities": []}, {"text": "Two key factors for just-in-time keyword extraction from meeting transcripts are time of preceding utterances and topic of current utterance.", "labels": [], "entities": [{"text": "just-in-time keyword extraction from meeting transcripts", "start_pos": 20, "end_pos": 76, "type": "TASK", "confidence": 0.7978676358858744}]}, {"text": "First, current utterance is affected by temporal history of preceding utterances.", "labels": [], "entities": []}, {"text": "That is, when anew utterance is made it is likely to be related more closely with latest utterances than old ones.", "labels": [], "entities": []}, {"text": "Second, the preceding utterances which carry similar topics to current utterance are more important than irrelevant utterances.", "labels": [], "entities": []}, {"text": "Since a meeting consists of several topics, the utterances that have nothing to do with current utterance are inappropriate as a history of the current utterance.", "labels": [], "entities": []}, {"text": "This paper proposes a graph-based keyword extraction to reflect these factors.", "labels": [], "entities": [{"text": "graph-based keyword extraction", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.7015298207600912}]}, {"text": "The proposed method represents an utterance as a graph of which nodes are candidate keywords.", "labels": [], "entities": []}, {"text": "The preceding utterances are also expressed as a history graph in which the weight of an edge is the temporal importance of the keywords connected by the edge.", "labels": [], "entities": []}, {"text": "To reflect the temporal history of utterances, forgetting curve) is adopted in updating the weights of edges in the history graph.", "labels": [], "entities": [{"text": "forgetting curve", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.9687711894512177}]}, {"text": "It expresses effectively not only the reciprocal relation between memory re-tention and time, but also active recall that makes frequent words more consequential in keyword extraction.", "labels": [], "entities": [{"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9433282613754272}, {"text": "keyword extraction", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.7345171868801117}]}, {"text": "Then, a subgraph that is relevant to the current utterance is derived from the history graph, and used as an actual history of the current utterance.", "labels": [], "entities": []}, {"text": "The keywords of the current utterance are extracted by TextRank () from the merged graph of the current utterance and the history graphs.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.8997347950935364}]}, {"text": "The proposed method is evaluated with two kinds of data sets: the National Assembly transcripts in Korean and the ICSI meeting corpus () in English.", "labels": [], "entities": [{"text": "National Assembly transcripts", "start_pos": 66, "end_pos": 95, "type": "DATASET", "confidence": 0.9564902583758036}, {"text": "ICSI meeting corpus", "start_pos": 114, "end_pos": 133, "type": "DATASET", "confidence": 0.8911085923512777}]}, {"text": "The experimental results show that it outperforms both the TFIDF framework () and the PageRank-based graph model (.", "labels": [], "entities": []}, {"text": "One thing to note is that the proposed method improves even the supervised methods that do not reflect utterance time and topic relevance for the ICSI corpus.", "labels": [], "entities": [{"text": "ICSI corpus", "start_pos": 146, "end_pos": 157, "type": "DATASET", "confidence": 0.7865148484706879}]}, {"text": "This proves that it is critical to consider time and content of utterances simultaneously in keyword extraction from meeting transcripts.", "labels": [], "entities": [{"text": "keyword extraction from meeting transcripts", "start_pos": 93, "end_pos": 136, "type": "TASK", "confidence": 0.8214443802833558}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the related studies on keyword extraction.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8739388585090637}]}, {"text": "Section 3 explains the overall process of the proposed method, and Section 4 addresses its detailed description how to reflect meeting characteristics.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally, Section 6 draws some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed method is evaluated with two kinds of data sets: the National Assembly transcripts in Korean and the ICSI meeting corpus in English.", "labels": [], "entities": [{"text": "National Assembly transcripts", "start_pos": 66, "end_pos": 95, "type": "DATASET", "confidence": 0.9529162446657816}, {"text": "ICSI meeting corpus", "start_pos": 114, "end_pos": 133, "type": "DATASET", "confidence": 0.9184430440266927}]}, {"text": "Both data sets are the records of meetings that are manually dictated by human transcribers., the damping factor d of Equation, and the threshold \u03b8.", "labels": [], "entities": [{"text": "Equation", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9942535758018494}]}, {"text": "For all experiments below, dis set 0.85, Wis 10, and \u03b8 is 0.28.", "labels": [], "entities": [{"text": "Wis 10", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9580350816249847}]}, {"text": "The remaining first meeting transcript is used as a data set to extract keywords since this transcript contains more utterances.", "labels": [], "entities": []}, {"text": "Only nouns are considered as potential keywords.", "labels": [], "entities": []}, {"text": "That is, only the words whose POS tag is NNG (common noun) or NNP (proper noun) can be a keyword.", "labels": [], "entities": []}, {"text": "Three annotators are engaged to extract keywords manually for each utterance in the first meeting transcript, since the Knowledge Management System does not provide the keywords 3 . The average number of keywords per utterance is 2.58.", "labels": [], "entities": []}, {"text": "To seethe inter-judge agreement among the annotators, the Kappa coefficient was investigated.", "labels": [], "entities": []}, {"text": "The kappa agreement of the National Assembly transcript is 0.31 that falls under the category of 'Fair'.", "labels": [], "entities": [{"text": "kappa agreement", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8060404658317566}, {"text": "National Assembly transcript", "start_pos": 27, "end_pos": 55, "type": "DATASET", "confidence": 0.9342613220214844}]}, {"text": "Even though all congressmen in the transcript belong to the same committee, they discussed various topics at the meeting.", "labels": [], "entities": []}, {"text": "As a result, the keywords are difficult to be agreed unanimously by all three annotators.", "labels": [], "entities": []}, {"text": "Therefore, in this paper the words that are recommended by more than two annotators are chosen as keywords.", "labels": [], "entities": []}, {"text": "The evaluation is done with two metics: Fmeasure and the weighted relative score (WRS).", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9985376596450806}, {"text": "weighted relative score (WRS)", "start_pos": 57, "end_pos": 86, "type": "METRIC", "confidence": 0.866012821594874}]}, {"text": "Since the previous work by reported only F-measure and WRS, F-measure instead of precision/recall are used for the comparison with their method.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9608001708984375}, {"text": "WRS", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.986737072467804}, {"text": "F-measure", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.986850380897522}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9992721676826477}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9718600511550903}]}, {"text": "The weighted relative score is derived from Pyramid metric ().", "labels": [], "entities": []}, {"text": "When a keyword extraction system generates keywords which many annotators agree, a higher score is given to it.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7727213501930237}]}, {"text": "On the other hand, a lower score is given if fewer annotators agree.", "labels": [], "entities": []}, {"text": "The proposed method is compared with two baseline models to see its relative performance.", "labels": [], "entities": []}, {"text": "One is the frequency-based keyword extraction with TFIDF weighting) and the other is TextRank in which the weight of edges is mutual information between vertices (.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7573725283145905}]}, {"text": "In TFIDF, each utterance is considered as a document, and thus all utterances including the current one are regarded as whole documents.", "labels": [], "entities": []}, {"text": "The frequencybased TFIDF chooses top-K words according to their TFIDF value from the set of words appearing in the meeting transcript.", "labels": [], "entities": []}, {"text": "Since the human annotators are restricted to extract up to five keywords, the keyword extraction systems including our method are also requested to select top-5 keywords when more than five keywords are produced.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7498364448547363}]}, {"text": "In order to seethe effect of preceding utterances in baseline models, the performances are measured according to the number of preceding utterances used.", "labels": [], "entities": []}, {"text": "The X-axis of this figure is the number of preceding utterances and the Yaxis represents F-measures.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9723927974700928}]}, {"text": "As shown in this figure, the performance of the baseline models improves monotonically at first as the number of preceding utterances increases.", "labels": [], "entities": []}, {"text": "However, the performance improvement stops when many preceding utterances are involved, and the performance begins to drop  when too many utterances are considered.", "labels": [], "entities": []}, {"text": "The performance of TextRank model drops from 20 preceding utterances, while that of TFIDF model begins to drops at 50 utterances.", "labels": [], "entities": []}, {"text": "When too many preceding utterances are taken into account, it is highly possible that some of their topics are irrelevant to the current utterance, which leads to performance drop.", "labels": [], "entities": []}, {"text": "compares our method with the baseline models on the National Assembly transcripts.", "labels": [], "entities": [{"text": "National Assembly transcripts", "start_pos": 52, "end_pos": 81, "type": "DATASET", "confidence": 0.9576691190401713}]}, {"text": "The performances of baseline models are obtained when they show the best performance for various number of preceding utterances.", "labels": [], "entities": []}, {"text": "TextRank model achieves F-measure of 0.478 and weighted relative score of 0.387, while TFIDF reports its best F-measure of 0.481 and weighted relative score of 0.394.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9400002360343933}, {"text": "F-measure", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9989854693412781}, {"text": "TFIDF", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.7901701927185059}, {"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9856699109077454}]}, {"text": "Thus, the difference between TFIDF and TextRank is not significant.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9146338105201721}]}, {"text": "However, F-measure and weighted relative score of the proposed method are 0.533 and 0.421 respectively, and they are much higher than those of baseline models.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.998380184173584}, {"text": "weighted relative score", "start_pos": 23, "end_pos": 46, "type": "METRIC", "confidence": 0.7631304462750753}]}, {"text": "In addition, our method achieves precision of 0.543 and recall of 0.523 and this is much higher performance than TextRank whose precision is just 0.510.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9992608428001404}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9997475743293762}, {"text": "TextRank", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.9071066975593567}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9814888834953308}]}, {"text": "Since the proposed method uses, as history, the preceding utterances relevant to the current utterance, its performance is kept high even if whole utterances are used.", "labels": [], "entities": []}, {"text": "Therefore, it could be inferred that it is important to adopt only the relevant history in keyword extraction from meeting transcripts.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7659499645233154}]}, {"text": "One of the key factors of our method is the temporal history.", "labels": [], "entities": []}, {"text": "Its importance is given in.", "labels": [], "entities": []}, {"text": "As explained above, the temporal history is achieved by Equation (2).", "labels": [], "entities": [{"text": "Equation", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9949108958244324}]}, {"text": "Thus, the proposed model does not reflect the temporal importance of preceding utterances if w 2 ij = 1 always.", "labels": [], "entities": []}, {"text": "That is, under w 2 ij = 1, old utterances are regarded as important as recent utterances.", "labels": [], "entities": []}, {"text": "Without temporal history, F-measure and weighted relative score are just 0.518 and 0.413 respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9986118078231812}, {"text": "weighted relative score", "start_pos": 40, "end_pos": 63, "type": "METRIC", "confidence": 0.7799785335858663}]}, {"text": "These poor performances prove the importance of the temporal history in keyword extraction from meeting transcripts.", "labels": [], "entities": [{"text": "keyword extraction from meeting transcripts", "start_pos": 72, "end_pos": 115, "type": "TASK", "confidence": 0.8715964317321777}]}], "tableCaptions": [{"text": " Table 1: Simple statistics of the National Assembly transcripts", "labels": [], "entities": [{"text": "National Assembly transcripts", "start_pos": 35, "end_pos": 64, "type": "DATASET", "confidence": 0.8319266438484192}]}, {"text": " Table 2: The experimental results on the National Assem- bly transcripts", "labels": [], "entities": [{"text": "National Assem- bly transcripts", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.7695823311805725}]}, {"text": " Table 3: The importance of temporal history", "labels": [], "entities": []}, {"text": " Table 4: Simple statistics of the ICSI meeting data  Information  Value  # of meetings  26  # of topic segments  201  # of topic segments used actually  140  Average # of utterances per topic segment  260  Average # of words per utterance  7.22", "labels": [], "entities": [{"text": "ICSI meeting data", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.7619831760724386}]}, {"text": " Table 5: The experimental results on the ICSI corpus", "labels": [], "entities": [{"text": "ICSI", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6799022555351257}]}, {"text": " Table 6: The effect of considering topic relevance", "labels": [], "entities": []}]}