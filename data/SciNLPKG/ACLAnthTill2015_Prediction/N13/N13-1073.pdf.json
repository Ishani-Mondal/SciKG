{"title": [{"text": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9421548644701639}]}], "abstractContent": [{"text": "We present a simple log-linear reparame-terization of IBM Model 2 that overcomes problems arising from Model 1's strong assumptions and Model 2's overparame-terization.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9500545660654703}]}, {"text": "Efficient inference, likelihood evaluation, and parameter estimation algorithms are provided.", "labels": [], "entities": [{"text": "likelihood evaluation", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.724941074848175}]}, {"text": "Training the model is consistently ten times faster than Model 4.", "labels": [], "entities": []}, {"text": "On three large-scale translation tasks, systems built using our alignment model outperform IBM Model 4.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9053213596343994}]}, {"text": "An open-source implementation of the alignment model described in this paper is available from http://github.com/clab/fast align .", "labels": [], "entities": [{"text": "alignment", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.9657263159751892}]}], "introductionContent": [{"text": "Word alignment is a fundamental problem in statistical machine translation.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7449884116649628}, {"text": "statistical machine translation", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.712830513715744}]}, {"text": "While the search for more sophisticated models that provide more nuanced explanations of parallel corpora is a key research activity, simple and effective models that scale well are also important.", "labels": [], "entities": []}, {"text": "These play a crucial role in many scenarios such as parallel data mining and rapid large scale experimentation, and as subcomponents of other models or training and inference algorithms.", "labels": [], "entities": [{"text": "parallel data mining", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.658161054054896}]}, {"text": "For these reasons, IBM Models 1 and 2, which support exact inference in time \u0398(|f| \u00b7 |e|), continue to be widely used.", "labels": [], "entities": []}, {"text": "This paper argues that both of these models are suboptimal, even in the space of models that permit such computationally cheap inference.", "labels": [], "entities": []}, {"text": "Model 1 assumes all alignment structures are uniformly likely (a problematic assumption, particularly for frequent word types), and Model 2 is vastly overparameterized, making it prone to degenerate behavior on account of overfitting.", "labels": [], "entities": []}, {"text": "We present a simple log-linear reparameterization of Model 2 that avoids both problems ( \u00a72).", "labels": [], "entities": []}, {"text": "While inference in log-linear models is generally computationally more expensive than in their multinomial counterparts, we show how the quantities needed for alignment inference, likelihood evaluation, and parameter estimation using EM and related methods can be computed using two simple algebraic identities ( \u00a73), thereby defusing this objection.", "labels": [], "entities": [{"text": "alignment inference", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.9688090980052948}, {"text": "likelihood evaluation", "start_pos": 180, "end_pos": 201, "type": "TASK", "confidence": 0.6748528033494949}, {"text": "parameter estimation", "start_pos": 207, "end_pos": 227, "type": "TASK", "confidence": 0.7032275795936584}]}, {"text": "We provide results showing our model is an order of magnitude faster to train than Model 4, that it requires no staged initialization, and that it produces alignments that lead to significantly better translation quality on downstream translation tasks ( \u00a74).", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating and maximizing the data likelihood under log-linear models can be computationally expensive since this requires evaluation of normalizing partition functions.", "labels": [], "entities": []}, {"text": "In our case, exp \u03bbh(i, j , m, n).", "labels": [], "entities": []}, {"text": "While computing this sum is obviously possible in \u0398(|f|) operations, our formulation permits exact computation in \u0398(1), meaning our model can be applied even in applications where computational efficiency is paramount (e.g., MCMC simulations).", "labels": [], "entities": []}, {"text": "The key insight is that the partition function is the (partial) sum of two geometric series of unnormalized probabilities that extend up and down from the probability-maximizing diagonal.", "labels": [], "entities": []}, {"text": "The closest point on or above the diagonal j \u2191 , and the next point down j \u2193 (see the right side of for an illustration), is computed as follows: Starting at j \u2191 and moving up the alignment column, as well as starting at j \u2193 and moving down, the unnormalized probabilities decrease by a factor of r = exp \u2212\u03bb n per step.", "labels": [], "entities": []}, {"text": "To compute the value of the partition, we only need to evaluate the unnormalized probabilities at j \u2191 and j \u2193 and then use the following identity, which gives the sum of the first terms of a geometric series: Using this identity, Z \u03bb (i, m, n) can be computed as  Fortunately, like the partition function, the derivative of the log-partition function (i.e., the second term in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 377, "end_pos": 379, "type": "DATASET", "confidence": 0.9249475002288818}]}, {"text": "2) can be computed inconstant time using an algebraic identity.", "labels": [], "entities": []}, {"text": "To derive this, we observe that the values of h(i, j , m, n) form an arithmetic sequence about the diagonal, with common difference d = \u22121/n.", "labels": [], "entities": []}, {"text": "Thus, the quantity we seek is the sum of a series whose elements are the products of terms from an arithmetic sequence and those of the geometric sequence above, divided by the partition function value.", "labels": [], "entities": []}, {"text": "This construction is referred to as an arithmetico-geometric series, and its sum maybe computed as follows (): In this expression r, the g 1 's and the 's have the same values as above, d = \u22121/n and the a 1 's are equal to the value of h evaluated at the starting indices, j \u2191 and j \u2193 ; thus, the derivative we seek at each optimization iteration inside the M-step is:  In this section we evaluate the performance of our proposed model empirically.", "labels": [], "entities": []}, {"text": "Experiments are conducted on three datasets representing different language typologies and dataset sizes: the FBIS Chinese-English corpus (LDC2003E14); a FrenchEnglish corpus consisting of version 7 of the Europarl and news-commentary corpora; 5 and a large Arabic-English corpus consisting of all parallel data made available for the NIST 2012 Open MT evaluation.", "labels": [], "entities": [{"text": "FBIS Chinese-English corpus (LDC2003E14)", "start_pos": 110, "end_pos": 150, "type": "DATASET", "confidence": 0.8874612847963969}, {"text": "FrenchEnglish corpus", "start_pos": 154, "end_pos": 174, "type": "DATASET", "confidence": 0.8731856942176819}, {"text": "Europarl", "start_pos": 206, "end_pos": 214, "type": "DATASET", "confidence": 0.9610030651092529}, {"text": "NIST 2012 Open MT evaluation", "start_pos": 335, "end_pos": 363, "type": "DATASET", "confidence": 0.8397401332855224}]}, {"text": "We begin with several preliminary results.", "labels": [], "entities": []}, {"text": "First, we quantify the benefit of using the geometric series trick ( \u00a73.2) for computing the partition function relative to na\u00a8\u0131vena\u00a8\u0131ve summation.", "labels": [], "entities": []}, {"text": "Our method requires only 0.62 seconds to compute all partition function values for 0 < i, m, n < 150, whereas the na\u00a8\u0131vena\u00a8\u0131ve algorithm requires 6.49 seconds for the same.", "labels": [], "entities": []}, {"text": "Second, using a 10k sample of the French-English data set (only 0.5% of the corpus), we determined 1) whether p 0 should be optimized; 2) what the optimal Dirichlet parameters \u00b5 i are; and 3) whether the commonly used \"staged initialization\" procedure (in which Model 1 parameters are used to initialize Model 2, etc.) is necessary for our model.", "labels": [], "entities": [{"text": "French-English data set", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.8294715881347656}]}, {"text": "First, like who explored this issue for training Model 3, we found that EM tended to find poor values for p 0 , producing alignments that were overly sparse.", "labels": [], "entities": [{"text": "EM", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.8532356023788452}]}, {"text": "By fixing the value at p 0 = 0.08, we obtained minimal AER.", "labels": [], "entities": [{"text": "AER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9982830286026001}]}, {"text": "Second, like, we found that small values of \u03b1 improved the alignment error rate, although the impact was not particularly strong overlarge ranges of   \u03b1.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 59, "end_pos": 79, "type": "METRIC", "confidence": 0.7979023853937784}]}, {"text": "Finally, we (perhaps surprisingly) found that the standard staged initialization procedure was less effective in terms of AER than simply initializing our model with uniform translation probabilities and a small value of \u03bb and running EM.", "labels": [], "entities": [{"text": "AER", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9985200762748718}]}, {"text": "Based on these observations, we fixed p 0 = 0.08, \u00b5 i = 0.01, and set the initial value of \u03bb to 4 for the remaining experiments.", "labels": [], "entities": []}, {"text": "We next compare the alignments produced by our model to the Giza++ implementation of the standard IBM models using the default training procedure and parameters reported in.", "labels": [], "entities": []}, {"text": "Our model is trained for 5 iterations using the procedure described above ( \u00a73.3).", "labels": [], "entities": []}, {"text": "The algorithms are compared in terms of (1) time required for training; (2) alignment error rate (AER, lower is better); 8 and (3) translation quality (BLEU, higher is better) of hierarchical phrase-based translation system that used the alignments.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 76, "end_pos": 96, "type": "METRIC", "confidence": 0.969267725944519}, {"text": "AER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9184983968734741}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9286302924156189}]}, {"text": "shows the CPU time in hours required for training (one direction, English is generated).", "labels": [], "entities": []}, {"text": "Our model is at least 10\u00d7 faster to train than Model 4.", "labels": [], "entities": []}, {"text": "reports the differences in BLEU on a held-out test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9976246953010559}]}, {"text": "Our model's alignments lead to consistently better scores than Model 4's do.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CPU time (hours) required to train alignment  models in one direction.", "labels": [], "entities": []}, {"text": " Table 2: Alignment quality (AER) on the WMT 2012  French-English and FBIS Chinese-English. Rows with  EM use expectation maximization to estimate the \u03b8 f , and  \u223cDir use variational Bayes.", "labels": [], "entities": [{"text": "Alignment quality (AER)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8999396920204162}, {"text": "WMT 2012  French-English and FBIS Chinese-English", "start_pos": 41, "end_pos": 90, "type": "DATASET", "confidence": 0.913813441991806}, {"text": "variational Bayes", "start_pos": 171, "end_pos": 188, "type": "METRIC", "confidence": 0.9552437365055084}]}, {"text": " Table 3: Translation quality (BLEU) as a function of  alignment type.", "labels": [], "entities": [{"text": "Translation quality (BLEU)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8216719508171082}]}]}