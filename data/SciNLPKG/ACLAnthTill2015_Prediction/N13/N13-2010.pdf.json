{"title": [], "abstractContent": [{"text": "Automatically describing visual content is an extremely difficult task, with hard AI problems in Computer Vision (CV) and Natural Language Processing (NLP) at its core.", "labels": [], "entities": [{"text": "Automatically describing visual content", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7443448007106781}]}, {"text": "Previous work relies on supervised visual recognition systems to determine the content of images.", "labels": [], "entities": []}, {"text": "These systems require massive amounts of hand-labeled data for training, so the number of visual classes that can be recognized is typically very small.", "labels": [], "entities": []}, {"text": "We argue that these approaches place unrealistic limits on the kinds of images that can be captioned, and are unlikely to produce captions which reflect human interpretations.", "labels": [], "entities": []}, {"text": "We present a framework for image caption generation that does not rely on visual recognition systems, which we have implemented on a dataset of online shopping images and product descriptions.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8600029150644938}]}, {"text": "We propose future work to improve this method, and extensions for other domains of images and natural text.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the number of images on the web continues to increase, the task of automatically describing images becomes especially important.", "labels": [], "entities": []}, {"text": "Image captions can provide background information about what is seen in the image, can improve accessibility of websites for visually-impaired users, and can improve image retrieval by providing text to search user queries against.", "labels": [], "entities": [{"text": "Image captions", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6248900294303894}, {"text": "image retrieval", "start_pos": 166, "end_pos": 181, "type": "TASK", "confidence": 0.7166033685207367}]}, {"text": "Typically, online search engines rely on collocated textual information to resolve queries, rather than analyzing visual content directly.", "labels": [], "entities": []}, {"text": "Likewise, earlier image captioning research from the Natural Language Processing (NLP) community use collocated information such as news articles or GPS coordinates, to decide what information to include in the generated caption.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7410392761230469}]}, {"text": "However, in some instances visual recognition is necessary because collocated information is missing, irrelevant, or unreliable.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7670932710170746}]}, {"text": "Recognition is a classic Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered).", "labels": [], "entities": [{"text": "Recognition", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9719868302345276}, {"text": "recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered)", "start_pos": 78, "end_pos": 277, "type": "Description", "confidence": 0.6878232997517253}]}, {"text": "Recent works in image caption generation represent visual content via the output of trained recognition systems fora pre-defined set of visual classes.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.8360102574030558}]}, {"text": "They then use linguistic models to correct noisy initial detections, and generate more naturalsounding text.", "labels": [], "entities": []}, {"text": "A key problem with this approach is that it assumes that image captioning is a grounding problem, with language acting only as labels for visual meaning.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7429355084896088}]}, {"text": "One good reason to challenge this assumption is that it imposes unrealistic constraints on the kinds of images that can be automatically described.", "labels": [], "entities": []}, {"text": "Previous work only recognizes a limited number of visual classes -typically no more than a few dozen in total -because training CV systems requires a huge amount of hand-annotated data.", "labels": [], "entities": []}, {"text": "For example, the PASCAL VOC dataset 1 has 11,530 training im-ages with 27,450 labeled objects, in order to learn only 20 object classes.", "labels": [], "entities": [{"text": "PASCAL VOC dataset 1", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.8219679594039917}]}, {"text": "Since training visual recognition systems is such a burden, \"general-domain\" image captioning datasets are limited by the current technology.", "labels": [], "entities": [{"text": "training visual recognition", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.616925577322642}, {"text": "general-domain\" image captioning", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.6116684824228287}]}, {"text": "For example, the SBU-Flickr dataset (), which contains 1 million images and captions, is built by first querying Flickr using a pre-defined set of queries, then further filtering to remove instances where the caption does not contain at least two words belonging to their term list.", "labels": [], "entities": [{"text": "SBU-Flickr dataset", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.9195656478404999}]}, {"text": "Furthermore, detections are too noisy to generate a good caption for the majority of images.", "labels": [], "entities": []}, {"text": "For example, select their test set according to which images receive the most confident visual object detection scores.", "labels": [], "entities": [{"text": "visual object detection", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7452246944109598}]}, {"text": "We instead direct our attention to the domainspecific image captioning task, assuming that we know a general objector scene category for the query image, and that we have access to a dataset of images and captions from the same domain.", "labels": [], "entities": [{"text": "image captioning task", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7819550434748331}]}, {"text": "While some techniques maybe unrealistic in assuming that high-quality collocated text is always available, assuming that there is no collocated information at all is equally unrealistic.", "labels": [], "entities": []}, {"text": "Data sources such as file names, website text, Facebook likes, and web searches all provide clues to the content of an image.", "labels": [], "entities": []}, {"text": "Even an image file by itself carries metadata on where and when it was taken, and the camera settings used to take it.", "labels": [], "entities": []}, {"text": "Since visual recognition is much easier for domain-specific tasks, there is more potential for natural language researchers to do research that will impact the greater community.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7269587963819504}]}, {"text": "Finally, labeling visual content is often not enough to provide an adequate caption.", "labels": [], "entities": [{"text": "labeling visual content", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.9078298608462015}]}, {"text": "The meaning of an image to a user is more than just listing the objects in the image, and can even change for different users.", "labels": [], "entities": []}, {"text": "This problem is commonly known as \"bridging the semantic gap\": \"The semantic gap is the lack of coincidence between the information that one can extract from the visual data and the interpretation that the same data have fora user in a given situation.", "labels": [], "entities": []}, {"text": "A linguistic description is almost always contextual, whereas an image may live by itself.\"", "labels": [], "entities": []}, {"text": "( challenges/VOC/ General-domain models of caption generation fail to capture context because they assume that all the relevant information has been provided in the image.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9499317705631256}]}, {"text": "However, training models on data from the same domain gives implicit context about what information should be provided in the generated text.", "labels": [], "entities": []}, {"text": "This thesis proposes a framework for image captioning that does not require supervision in the form of hand-labeled examples.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7454651594161987}]}, {"text": "We train a topic model on a corpus of images and captions in the same domain, in order to jointly learn image features and natural language descriptions.", "labels": [], "entities": []}, {"text": "The trained topic model is used to estimate the likelihood of words appearing in a caption, given an unseen query image.", "labels": [], "entities": []}, {"text": "We then use these likelihoods to rewrite an extracted humanwritten caption to accurately describe the query image.", "labels": [], "entities": []}, {"text": "We have implemented our framework using a dataset of online shopping images and captions, and propose to extend this model to other domains, including natural images.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}