{"title": [], "abstractContent": [{"text": "We propose anew method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora.", "labels": [], "entities": [{"text": "translation acquisition", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.9849706292152405}]}, {"text": "The motivation is that, given a certain query term, it is often possible fora user to specify one or more synonyms.", "labels": [], "entities": []}, {"text": "Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term's context vector does not always reliably represent a terms meaning due to the context vector's sparsity.", "labels": [], "entities": []}, {"text": "Our proposed method uses a weighted average of the synonyms' context vectors, that is derived by inferring the mean vector of the von Mises-Fisher distribution.", "labels": [], "entities": []}, {"text": "We evaluate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet.", "labels": [], "entities": []}, {"text": "The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors.", "labels": [], "entities": [{"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.9380599856376648}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.8874038457870483}]}], "introductionContent": [{"text": "Automatic translation acquisition is an important task for various applications.", "labels": [], "entities": [{"text": "Automatic translation acquisition", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8542763392130533}]}, {"text": "For example, finding term translations can be used to automatically update existing bilingual dictionaries, which are an indispensable resource for tasks such as cross-lingual information retrieval and text mining.", "labels": [], "entities": [{"text": "finding term translations", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.7231101592381796}, {"text": "cross-lingual information retrieval", "start_pos": 162, "end_pos": 197, "type": "TASK", "confidence": 0.6549111704031626}, {"text": "text mining", "start_pos": 202, "end_pos": 213, "type": "TASK", "confidence": 0.7958382964134216}]}, {"text": "Various previous research like) has shown that it is possible to acquire word translations from comparable corpora.", "labels": [], "entities": [{"text": "acquire word translations", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6524220108985901}]}, {"text": "We suggest here an extension of this approach which uses several query terms instead of a single query term.", "labels": [], "entities": []}, {"text": "A user who searches a translation fora query term that is not listed in an existing bilingual dictionary, might first try to find a synonym of that term.", "labels": [], "entities": []}, {"text": "For example, the user might lookup a synonym in a thesaurus 1 or might use methods for automatic synonym acquisition like described in.", "labels": [], "entities": [{"text": "synonym acquisition", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7965730130672455}]}, {"text": "If the synonym is listed in the bilingual dictionary, we can consider the synonym's translations as the translations of the query term.", "labels": [], "entities": []}, {"text": "Otherwise, if the synonym is not listed in the dictionary either, we use the synonym together with the original query term to find a translation.", "labels": [], "entities": []}, {"text": "We claim that using a set of synonymous query terms to find a translation is better than using a single query term.", "labels": [], "entities": []}, {"text": "The reason is that a single query term's context vector is, in general, unreliable due to sparsity.", "labels": [], "entities": []}, {"text": "For example, a low frequent query term tends to have many zero entries in its context vector.", "labels": [], "entities": []}, {"text": "To mitigate this problem it has been proposed to smooth a query's context vector by its nearest neighbors ().", "labels": [], "entities": []}, {"text": "However, nearest neighbors, which context vectors are close the query's context vector, can have different meanings and therefore might introduce noise.", "labels": [], "entities": []}, {"text": "The contributions of this paper are two-fold.", "labels": [], "entities": []}, {"text": "First, we confirm experimentally that smoothing a query's context vector with its synonyms leads indeed to higher translation accuracy, compared to smoothing with nearest neighbors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9449394941329956}]}, {"text": "Second, we propose a simple method to combine a set of context vectors that performs in this setting better than a method previously proposed by).", "labels": [], "entities": []}, {"text": "Our approach to combine a set of context vec-tors is derived by learning the mean vector of a von Mises-Fisher distribution.", "labels": [], "entities": []}, {"text": "The combined context vector is a weighted-average of the original contextvectors, where the weights are determined by the word occurrence frequencies.", "labels": [], "entities": []}, {"text": "In the following section we briefly show the relation to other previous work.", "labels": [], "entities": []}, {"text": "In Section 3, we explain our method in detail, followed by an empirical evaluation in Section 4.", "labels": [], "entities": []}, {"text": "We summarize our results in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT) and the USA National Highway Traffic Safety Administration (NHTSA) , respectively.", "labels": [], "entities": [{"text": "USA National Highway Traffic Safety Administration (NHTSA)", "start_pos": 215, "end_pos": 273, "type": "DATASET", "confidence": 0.7743465039465163}]}, {"text": "The Japanese corpus contains 24090 sentences that were POS tagged using MeCab ().", "labels": [], "entities": [{"text": "Japanese corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7708878815174103}, {"text": "POS tagged", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.6011863350868225}, {"text": "MeCab", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.9430648684501648}]}, {"text": "The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (), and use the Lemmatizer () to extract and stem content words (nouns, verbs, adjectives, adverbs).", "labels": [], "entities": [{"text": "English corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7828981876373291}]}, {"text": "For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio ().", "labels": [], "entities": []}, {"text": "It was shown in) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMI 5 and LLR 6 . For comparing two context vectors we use the cosine similarity.", "labels": [], "entities": [{"text": "PMI 5", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.7453744113445282}]}, {"text": "To transform the Japanese and English context vectors into the same vector space, we use a bilingual dictionary with around 1.6 million entries.", "labels": [], "entities": []}, {"text": "To express all context vectors in the same vector space, we map the context vectors in English to context vectors in Japanese.", "labels": [], "entities": []}, {"text": "8 First, for all the words which are listed in the bilingual dictionary we calculate word translation probabilities.", "labels": [], "entities": [{"text": "word translation", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7017295360565186}]}, {"text": "These translation probabilities are calculated using the EM-algorithm described in).", "labels": [], "entities": []}, {"text": "We then create a translation matrix T which contains in each column the translation probabilities fora word in English into any word in Japanese.", "labels": [], "entities": []}, {"text": "Each context vector in English is then mapped into Japanese using the linear transformation described by the translation matrix T . For word x with context vector x in English, let x \u2032 be its context vector after transformation into Japanese, i.e. The gold-standard was created by considering all nouns in the Japanese and English WordNet where synsets are aligned cross-lingually.", "labels": [], "entities": []}, {"text": "This way we were able to create a gold-standard with 215 Japanese nouns, and their respective English translations that occur in our comparable corpora.", "labels": [], "entities": []}, {"text": "Note that the cross-lingual alignment is needed only for evaluation.", "labels": [], "entities": []}, {"text": "For evaluation, we consider only the translations that occur in the corresponding English synset as correct.", "labels": [], "entities": []}, {"text": "Because all methods return a ranked list of translation candidates, the accuracy is measured using the rank of the translation listed in the gold-standard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9995145797729492}]}, {"text": "The inverse rank is the sum of the inverse ranks of each translation in the gold-standard.", "labels": [], "entities": [{"text": "inverse rank", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.960927277803421}]}, {"text": "In, the first row shows the results when using no smoothing.", "labels": [], "entities": []}, {"text": "Next, we smooth the query's context vector by using Equation (1) and (2).", "labels": [], "entities": [{"text": "Equation", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9964978098869324}]}, {"text": "The set of neighbors K is defined as the k-terms in the source language that are closest to the query word, with respect to the cosine similarity (sim).", "labels": [], "entities": [{"text": "cosine similarity (sim)", "start_pos": 128, "end_pos": 151, "type": "METRIC", "confidence": 0.8037549495697022}]}, {"text": "The weight w x fora neighbor x is set tow x = 10 0.13\u00b7sim(x,q) in accordance to ().", "labels": [], "entities": []}, {"text": "For k we tried values between 1 and 100, and got the best inverse rank when using k=19.", "labels": [], "entities": [{"text": "inverse rank", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9703558385372162}]}, {"text": "The resulting method (Topk Smoothing) performs consistently better than the method using no smoothing, see, second row.", "labels": [], "entities": []}, {"text": "Next, instead of smoothing the query word with its nearest neighbors, we use as the set K the set of synonyms of the query word (Syn Smoothing).", "labels": [], "entities": []}, {"text": "Table 1 shows a clear improvement over the method that uses nearest neighbor-smoothing.", "labels": [], "entities": []}, {"text": "This confirms our claim that using synonyms for smoothing can lead to better translation accuracy than using nearest neighbors.", "labels": [], "entities": [{"text": "translation", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9349623322486877}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8409280776977539}]}, {"text": "In the last row of, we compare our proposed method to combine context vectors of synonyms (Syn Mises-Combination), with the pre-vious method (Syn Smoothing).", "labels": [], "entities": [{"text": "Syn Mises-Combination", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.6296188980340958}]}, {"text": "A pair-wise comparison of our proposed method with Syn Smoothing shows a statistically significant improvement (p < 0.01).", "labels": [], "entities": []}, {"text": "Finally, we also show the result when simply adding each synonym vector to the query's context vector to form anew combined context vector (Syn Sum).", "labels": [], "entities": []}, {"text": "11 Even though, this approach does not use the frequency information of a word, it performs better than Syn Smoothing.", "labels": [], "entities": []}, {"text": "We suppose that this is due to the fact that it actually indirectly uses frequency information, since the log-odds-ratio tends to be higher for words which occur with high frequency in the corpus.: Shows Top-n accuracy and mean inverse rank (MIR) for baseline methods which use no synonyms (No Smoothing, Top-k Smoothing), the proposed method (Syn Mises-Combination) which uses synonyms, and alternative methods that also use synonyms (Syn Smoothing, Syn Sum).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9550136923789978}, {"text": "mean inverse rank (MIR)", "start_pos": 223, "end_pos": 246, "type": "METRIC", "confidence": 0.8651957909266154}, {"text": "Syn Mises-Combination)", "start_pos": 344, "end_pos": 366, "type": "TASK", "confidence": 0.8021586736043295}]}], "tableCaptions": [{"text": " Table 1: Shows Top-n accuracy and mean inverse rank  (MIR) for baseline methods which use no synonyms  (No Smoothing, Top-k Smoothing), the proposed method  (Syn Mises-Combination) which uses synonyms, and al- ternative methods that also use synonyms (Syn Smooth- ing, Syn Sum).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.978576123714447}, {"text": "mean inverse rank  (MIR)", "start_pos": 35, "end_pos": 59, "type": "METRIC", "confidence": 0.893986165523529}]}]}