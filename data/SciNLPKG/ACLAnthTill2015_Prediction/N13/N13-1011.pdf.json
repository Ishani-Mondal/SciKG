{"title": [{"text": "Cross-Lingual Semantic Similarity of Words as the Similarity of Their Semantic Word Responses", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose anew approach to identifying semantically similar words across languages.", "labels": [], "entities": [{"text": "identifying semantically similar words across languages", "start_pos": 28, "end_pos": 83, "type": "TASK", "confidence": 0.8313083449999491}]}, {"text": "The approach is based on an idea that two words in different languages are similar if they are likely to generate similar words (which includes both source and target language words) as their top semantic word responses.", "labels": [], "entities": []}, {"text": "Semantic word responding is a concept from cognitive science which addresses detecting most likely words that humans output as free word associations given some cue word.", "labels": [], "entities": [{"text": "Semantic word responding", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7356605728467306}]}, {"text": "The method consists of two main steps: (1) it utilizes a probabilistic multilingual topic model trained on comparable data to learn and quantify the semantic word responses, (2) it provides ranked lists of similar words according to the similarity of their semantic word response vectors.", "labels": [], "entities": []}, {"text": "We evaluate our approach in the task of bilingual lexicon extraction (BLE) fora variety of language pairs.", "labels": [], "entities": [{"text": "bilingual lexicon extraction (BLE)", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.7690058251221975}]}, {"text": "We show that in the cross-lingual settings without any language pair dependent knowledge the response-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-lingual semantic word similarity addresses the task of detecting words that refer to similar semantic concepts and convey similar meanings across languages.", "labels": [], "entities": [{"text": "Cross-lingual semantic word similarity", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6295818090438843}]}, {"text": "It ultimately boils down to the automatic identification of translation pairs, that is, bilingual lexicon extraction (BLE).", "labels": [], "entities": [{"text": "bilingual lexicon extraction (BLE)", "start_pos": 88, "end_pos": 122, "type": "TASK", "confidence": 0.6706892549991608}]}, {"text": "Such lexicons and semantically similar words serve as important resources in cross-lingual knowledge induction (e.g.,), statistical machine translation) and cross-lingual information retrieval).", "labels": [], "entities": [{"text": "cross-lingual knowledge induction", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.7185644706090292}, {"text": "statistical machine translation", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.7281065185864767}, {"text": "cross-lingual information retrieval", "start_pos": 157, "end_pos": 192, "type": "TASK", "confidence": 0.7177358170350393}]}, {"text": "From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models ().", "labels": [], "entities": []}, {"text": "However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) ().", "labels": [], "entities": []}, {"text": "Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis, which states that words with similar meanings are likely to appear in similar contexts.", "labels": [], "entities": [{"text": "detecting semantic word similarity", "start_pos": 14, "end_pos": 48, "type": "TASK", "confidence": 0.743415504693985}]}, {"text": "Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features.", "labels": [], "entities": []}, {"text": "The semantic similarity of two words, w S 1 given in the source language L S with vocabulary V Sand w T 2 in the target language L T with vocabulary VT is then: Sim(w S 1 , w T 2 ) = SF (cv(w S 1 ), cv(w T 2 )) (1) cv(w S 1 ) = [sc S 1 (c 1 ), . .", "labels": [], "entities": []}, {"text": ", sc S 1 (c N )] denotes a context vector for w S 1 with N context features ck , where sc S 1 (c k ) denotes the score for w S 1 associated with context feature ck (similar for w T 2 ).", "labels": [], "entities": []}, {"text": "SF is a similarity function (e.g., cosine, the Kullback-Leibler divergence, the Jaccard index) operating on the context vectors.", "labels": [], "entities": []}, {"text": "In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space.", "labels": [], "entities": [{"text": "cross-lingual semantic word similarity", "start_pos": 20, "end_pos": 58, "type": "TASK", "confidence": 0.6349577531218529}]}, {"text": "Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (), or more recently by a generative probabilistic model ().", "labels": [], "entities": []}, {"text": "Context vectors cv(w S 1 ) and cv(w T 2 ) for both source and target words are then compared in the semantic space independently of their respective languages.", "labels": [], "entities": []}, {"text": "In this work, we propose anew approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word association.", "labels": [], "entities": [{"text": "semantic word responding", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.6543785234292349}]}, {"text": "We borrow that concept from the psychology/cognitive science literature.", "labels": [], "entities": []}, {"text": "Semantic word responding addresses a task that requires participants to produce first words that come to their mind that are related to a presented cue word ().", "labels": [], "entities": [{"text": "Semantic word responding", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7831697861353556}]}, {"text": "The new cross-lingual semantic space is spanned by all vocabulary words in the source and the target language.", "labels": [], "entities": []}, {"text": "Each axis in the space denotes a semantic word response.", "labels": [], "entities": []}, {"text": "The similarity between two words is then computed as the similarity between the vectors comprising their semantic word responses using any of existing SF -s.", "labels": [], "entities": []}, {"text": "Two words are considered semantically similar if they are likely to generate similar semantic word responses and assign similar importance to them.", "labels": [], "entities": []}, {"text": "We utilize a shared semantic space of latent crosslingual topics learned by a multilingual probabilistic topic model to obtain semantic word responses and quantify the strength of association between any cue word and its responses monolingually and across languages, and, consequently, to build semantic response vectors.", "labels": [], "entities": []}, {"text": "That effectively translates the task of word similarity from the semantic space spanned by latent cross-lingual topics to the semantic space spanned by all vocabulary words in both languages.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7155361920595169}]}, {"text": "The main contributions of this article are: \u2022 We propose anew approach to modeling crosslingual semantic similarity of words based on the similarity of their semantic word responses.", "labels": [], "entities": []}, {"text": "\u2022 We present how to estimate and quantify semantic word responses by means of a multilingual probabilistic topic model.", "labels": [], "entities": []}, {"text": "\u2022 We demonstrate how to employ our novel paradigm that relies on semantic word responding in the task of bilingual lexicon extraction (BLE) from comparable data.", "labels": [], "entities": [{"text": "bilingual lexicon extraction (BLE)", "start_pos": 105, "end_pos": 139, "type": "TASK", "confidence": 0.7230376203854879}]}, {"text": "\u2022 We show that the response-based model of similarity is more robust and obtains better results for BLE than the models that operate in the semantic space spanned by latent semantic concepts, i.e., cross-lingual topics directly.", "labels": [], "entities": [{"text": "BLE", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9893118739128113}]}, {"text": "The following sections first review relevant prior work and provide a very short introduction to multilingual probabilistic topic modeling, then describe our response-based approach to modeling crosslingual semantic word similarity, and finally present our evaluation and results on the BLE task fora variety of language pairs.", "labels": [], "entities": [{"text": "multilingual probabilistic topic modeling", "start_pos": 97, "end_pos": 138, "type": "TASK", "confidence": 0.6200789958238602}, {"text": "crosslingual semantic word similarity", "start_pos": 194, "end_pos": 231, "type": "TASK", "confidence": 0.5947705805301666}, {"text": "BLE", "start_pos": 287, "end_pos": 290, "type": "METRIC", "confidence": 0.5799115300178528}]}], "datasetContent": [{"text": "4 Since our task is bilingual lexicon extraction, we designed a set of ground truth one-to-one translation pairs for all 3 language pairs as follows.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.682686467965444}]}, {"text": "For Dutch-English and Spanish-English, we randomly sampled a set of Dutch (Spanish) nouns from our Wikipedia corpora.", "labels": [], "entities": []}, {"text": "Following that, we used the Google Translate tool plus an additional annotator to translate those words to English.", "labels": [], "entities": []}, {"text": "The annotator manually revised the lists and retained only words that have The values are set empirically.", "labels": [], "entities": []}, {"text": "Calculating similarity Sim(w S 1 , w T 2 ) maybe interpreted as: \"Given word w S 1 detect how similar word w T 2 is to the word w S 1 .\" Therefore, when calculating Sim(w S 1 , w T 2 ), even when dealing with symmetric similarity functions such as BC, we always consider only the scores P (\u00b7|w S 1 ) for truncating.", "labels": [], "entities": []}, {"text": "Available online: http://people.cs.kuleuven.be /\u223civan.vulic/software/ their corresponding translation in the English vocabulary.", "labels": [], "entities": []}, {"text": "Additionally, only one possible translation was annotated as correct.", "labels": [], "entities": []}, {"text": "When more than 1 translation is possible, the annotator marked as correct the translation that occurs more frequently in the English Wikipedia data.", "labels": [], "entities": [{"text": "English Wikipedia data", "start_pos": 125, "end_pos": 147, "type": "DATASET", "confidence": 0.7940994699796041}]}, {"text": "Finally, we built a set of 1000 one-to-one translation pairs for Dutch-English and Spanish-English.", "labels": [], "entities": []}, {"text": "The same procedure was followed for Italian-English, but there we obtained the ground truth one-to-one translation pairs for 1000 most frequent Italian nouns in order to test the effect of word frequency on the quality of semantic word responses and the overall lexicon quality.", "labels": [], "entities": []}, {"text": "All the methods under consideration actually retrieve ranked lists of semantically similar words that could be observed as potential translation candidates.", "labels": [], "entities": []}, {"text": "We measure the performance on BLE as Top M accuracy (Acc M ).", "labels": [], "entities": [{"text": "BLE", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9439581632614136}, {"text": "Top M accuracy", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.7729014158248901}, {"text": "Acc M )", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9698698321978251}]}, {"text": "It denotes the number of source words from ground truth translation pairs whose top M semantically similar words contain the correct translation according to our ground truth over the total number of ground truth translation pairs (=1000) ().", "labels": [], "entities": []}, {"text": "Additionally, we compute the mean reciprocal rank (MRR) scores (Voorhees, 1999)..", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR) scores", "start_pos": 29, "end_pos": 62, "type": "METRIC", "confidence": 0.8665701832090106}]}, {"text": "Based on these results, we are able to derive several conclusions: (i) Response-BC performs consistently better than the other 3 methods overall corpora and all language pairs.", "labels": [], "entities": []}, {"text": "It is more robust and is able to find some cross-lingual similarities omitted by the other meth-   ods (see).", "labels": [], "entities": []}, {"text": "The overall quality of the crosslingual word similarities and lexicons extracted by the method is dependent on the quality of estimated semantic response vectors.", "labels": [], "entities": []}, {"text": "The quality of these vectors is of course further dependent on the quality of multilingual training data.", "labels": [], "entities": []}, {"text": "For instance, for Dutch-English, we may observe a rather spectacular increase in overall scores (the tests are performed over the same set of 1000 words) when we augment Wikipedia data with Europarl data (compare the scores for NL-EN-W and NL-EN-W+EP).", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 170, "end_pos": 184, "type": "DATASET", "confidence": 0.862192839384079}, {"text": "Europarl data", "start_pos": 190, "end_pos": 203, "type": "DATASET", "confidence": 0.9624645113945007}]}], "tableCaptions": [{"text": " Table 1: An example of top 10 semantic word responses and the final response-based similarity for some Spanish and  English words. The responses are estimated from Spanish-English Wikipedia data by bilingual LDA. We can observe  several interesting phenomena: (1) High-frequency words tend to appear higher in the lists of semantic responses  (e.g., play and obra for all 3 words), (2) Due to the modeling properties that give preference to high-frequency words  (Sect. 3.3), a word might not generate itself as the top semantic response (e.g., playwright-play), (3) Both source  and target language words occur as the top responses in the lists, (4) Although play is the top semantic response in  English for both dramaturgo and playwright, its list of top semantic responses is less similar to the lists of those two  words, (5) Although the English word playwright does not appear in the top 10 semantic responses to dramaturgo,  and dramaturgo does not appear in the top 10 responses to playwright, the more robust response-based similarity  method detects that the two words are actually very similar based on their lists of responses, (6) dramaturgo and  playwright have very similar lists of semantic responses which ultimately leads to detecting that playwright is the  most semantically similar word to dramaturgo across the two languages (the last column), i.e., they are direct one-to- one translations of each other, (7) Another English word dramatist very similar to Spanish dramaturgo is also pushed  higher in the final list, although it is not found in the list of top semantic responses to dramaturgo.", "labels": [], "entities": []}, {"text": " Table 2: BLE performance of all the methods for Italian-English, Spanish-English and Dutch-English (with 2 different  corpora utilized for the training of bilingual LDA and the estimation of semantic word responses for Dutch-English).", "labels": [], "entities": [{"text": "BLE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9987616539001465}]}]}