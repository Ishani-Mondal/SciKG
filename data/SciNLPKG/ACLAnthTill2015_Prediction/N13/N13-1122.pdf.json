{"title": [{"text": "To Link or Not to Link? A Study on End-to-End Tweet Entity Linking", "labels": [], "entities": [{"text": "End-to-End Tweet Entity Linking", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6394263356924057}]}], "abstractContent": [{"text": "Information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world.", "labels": [], "entities": [{"text": "Information extraction from microblog posts", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8418076157569885}]}, {"text": "As the core component of information extraction , we consider the task of Twitter entity linking in this paper.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.8371068835258484}, {"text": "Twitter entity linking", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.5625043412049612}]}, {"text": "In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7300167381763458}, {"text": "mention detection", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7390072494745255}, {"text": "entity disambiguation", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7594559490680695}]}, {"text": "However, in our task, we find that mention detection is often the performance bottleneck.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9476847052574158}]}, {"text": "The reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings.", "labels": [], "entities": []}, {"text": "To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disam-biguation as a single end-to-end task.", "labels": [], "entities": [{"text": "Twitter entity linking", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.594130257765452}, {"text": "entity linking", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7273652106523514}, {"text": "mention detection", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7192351371049881}]}, {"text": "By combining structural learning and a variety of first-order, second-order, and context-sensitive features , our system is able to outperform existing state-of-the art entity linking systems by 15% F 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Microblogging services, such as Twitter and Facebook, are today capturing the largest volume ever recorded of fine-grained discussions spanning a huge breadth of topics, from the mundane to the historic.", "labels": [], "entities": []}, {"text": "The micro-blogging service Twitter reports that it alone captures over 340M short messages, or tweets, per day.", "labels": [], "entities": []}, {"text": "From such micro-blogging services' data streams, researchers have reported mining insights about a variety of domains, from election results) and democracy movements) to health issues and disease spreading (, as well as tracking product feedback and sentiment.", "labels": [], "entities": [{"text": "disease spreading", "start_pos": 188, "end_pos": 205, "type": "TASK", "confidence": 0.8154033422470093}]}, {"text": "A critical step in mining information from a micro-blogging service, such as Twitter, is the identification of entities in tweets.", "labels": [], "entities": [{"text": "identification of entities in tweets", "start_pos": 93, "end_pos": 129, "type": "TASK", "confidence": 0.8099238276481628}]}, {"text": "In order to mine the relationship between drugs, symptoms and sideeffects, or track the popularity of politicians or sentiment about social issues, we must first be able to identify the topics and specific entities being discussed.", "labels": [], "entities": []}, {"text": "The challenge is that messages on microblogs are short, noisy, and informal texts with little context, and often contain phrases with ambiguous meanings.", "labels": [], "entities": []}, {"text": "For example, \"one day\" maybe either a set phrase or a reference to a movie.", "labels": [], "entities": []}, {"text": "Given such difficulties, current mining and analysis of microblogs lists limits its application to certain domains with easy-to-recognize, unambiguous entities in order to avoid noise in the extraction results.", "labels": [], "entities": []}, {"text": "We begin this paper with a thorough investigation of mention detection and entity disambiguation for social media, focused on the Twitter micro-blogging service.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7783228158950806}, {"text": "entity disambiguation", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.702511191368103}]}, {"text": "Mention detection is the task of extraction surface form candidates that can link to an entity in the domain of interest.", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8456748127937317}]}, {"text": "Entity disambiguation is the task of linking an extracted mention to a specific definition or instance of an entity in a knowledge base.", "labels": [], "entities": [{"text": "Entity disambiguation is the task of linking an extracted mention to a specific definition or instance of an entity in a knowledge base", "start_pos": 0, "end_pos": 135, "type": "Description", "confidence": 0.7718972343465557}]}, {"text": "While mention detection and entity disambiguation are frequently cast as equally important but distinct and separate problems, we find that mention detection is where today's systems and our baseline techniques incur the most failures.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7259493321180344}, {"text": "entity disambiguation", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7415828704833984}, {"text": "mention detection", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.7276837825775146}]}, {"text": "Detecting the correct entity mention is a significant challenge given mis-capitalizations, incorrect grammar, and ambiguous phrases.", "labels": [], "entities": [{"text": "Detecting the correct entity mention", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8582274794578553}]}, {"text": "In (), the authors report their system achieves 0.64 to 0.67 F 1 on named entity segmentation results with 34K tokens of labeled examples.", "labels": [], "entities": [{"text": "F 1", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9918674528598785}, {"text": "named entity segmentation", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6372709770997366}]}, {"text": "On the other hand, once the correct entity mention is detected, a trivial disambiguation that maps to the most popular entity 2 will achieve 85% accuracy in our set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9991886019706726}]}, {"text": "Our primary contribution in this paper is a recasting and merging of the tasks of mention detection and entity disambiguation into a single endto-end entity linking task.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7721463739871979}, {"text": "entity disambiguation", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.7094809114933014}]}, {"text": "We achieve significant improvements by applying structural learning techniques to jointly optimize the detection and disambiguation of entities.", "labels": [], "entities": [{"text": "detection and disambiguation of entities", "start_pos": 103, "end_pos": 143, "type": "TASK", "confidence": 0.7327402830123901}]}, {"text": "Treating detection and disambiguation as a single task also enables us to apply a large set of new features, conventionally used only for disambiguation, to the initial detection of mentions.", "labels": [], "entities": [{"text": "Treating detection and disambiguation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6031321138143539}]}, {"text": "These features, derived from external knowledge bases, include entity popularity and inter-entity relations from external knowledge bases, and are not well utilized in current mention detection systems.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7298050969839096}]}, {"text": "For example, consider the following partial tweet: The town is so, so good.", "labels": [], "entities": []}, {"text": "And don't worry Ben, we already forgave you for Gigli.", "labels": [], "entities": []}, {"text": "Determining whether or not \"The town\" is a mention of a location or other specific entity based solely on lexical and syntactic features is challenging.", "labels": [], "entities": []}, {"text": "Knowing \"The Town\" is the name of a recent movie helps, and we can we be more confident if we know that Ben Affleck is an actor in the movie, and Gigli is another of his movies.", "labels": [], "entities": [{"text": "The Town\"", "start_pos": 9, "end_pos": 18, "type": "DATASET", "confidence": 0.8308083613713583}]}, {"text": "To train and evaluate our system, we created three separate annotated data sets of approximately 500 tweets each.", "labels": [], "entities": []}, {"text": "These data sets are hand annotated with entity links to Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.9807096719741821}]}, {"text": "We evaluate our system by comparing its performance at detecting en-tities to the performance of two state-of-the-art entity linking systems, Cucerzan and, and find that our system outperforms them significantly by 15% in absolute F 1 . The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results.", "labels": [], "entities": [{"text": "F 1", "start_pos": 231, "end_pos": 234, "type": "METRIC", "confidence": 0.8930088579654694}, {"text": "entity linking", "start_pos": 320, "end_pos": 334, "type": "TASK", "confidence": 0.7336197644472122}]}], "datasetContent": [{"text": "We collected unlabeled Twitter data from two resources and then asked human annotators to label each tweet with a set of entities present.", "labels": [], "entities": []}, {"text": "Our annotators ignored the following: duplicate entities per tweet, ambiguous entity mentions, and entities not present in Wikipedia.", "labels": [], "entities": []}, {"text": "We next describe the two sets of Twitter data used as our training data and testing data.", "labels": [], "entities": []}, {"text": "In addition to these two datasets, we also randomly sampled another 200 tweets as our development set.", "labels": [], "entities": []}, {"text": "Ritter We sampled 473 and 500 tweets 8 from the data used in () to be our training data and test data, respectively.", "labels": [], "entities": []}, {"text": "We did not use any labels generated by); our annotators completely re-annotated each tweets with its set of entities.", "labels": [], "entities": []}, {"text": "We refer to the first set as Train and the second set as Test 1.", "labels": [], "entities": [{"text": "Train", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.8630875945091248}]}, {"text": "Entertainment To check if our system has the ability to generalize across different domains, we sampled another 488 tweets related to entertainment entities.", "labels": [], "entities": []}, {"text": "Our main focus was to extract tweets that contained TV shows, Movies, and Books/Magazines.", "labels": [], "entities": []}, {"text": "Identifying tweets from a specific domain is a research topic on its own, so we followed (, and used a keyword matching method.", "labels": [], "entities": [{"text": "Identifying tweets from a specific domain", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8299568891525269}]}, {"text": "After sampling this set of tweets, we asked our annotators to label the data in the same way as before (all entities are labeled, not just entertainment entities).", "labels": [], "entities": []}, {"text": "We refer to this tweet set as Test 2.", "labels": [], "entities": []}, {"text": "After sampling, all tweets were then normalized in the following way.", "labels": [], "entities": []}, {"text": "First, we removed all retweet symbols (RT) and special symbols, as these are tokens that may easily confuse NER systems.", "labels": [], "entities": []}, {"text": "We treated punctuation as separate tokens.", "labels": [], "entities": []}, {"text": "Hashtags (#) play a very important role in tweets as they often carry critical information.", "labels": [], "entities": []}, {"text": "We used the following web service 10 to break the hashtags into tokens (e.g., the service will break \"#TheCloneWars\" into \"the clone wars\") ().", "labels": [], "entities": []}, {"text": "The statistics of our labeled examples are presented in.", "labels": [], "entities": []}, {"text": "First, note that the average number of mentions per tweet is well below 1.", "labels": [], "entities": []}, {"text": "In fact, many tweets are personal conversations and do not carry any entities that can be linked to Wikipedia.", "labels": [], "entities": []}, {"text": "Still, many candidates are generated (such as \"really\") for those tweets, given that those candidates can still potentially link to an entity (\"really\" could be a TV channel).", "labels": [], "entities": []}, {"text": "Therefore, it is very important to include tweets without entities in the training set because we do not want our system to create unnecessary links to entities.", "labels": [], "entities": []}, {"text": "Another interesting thing to note is the percentage of entity mentions that disambiguate directly to their most often linked entities in Wikipedia.", "labels": [], "entities": []}, {"text": "If we simply disambiguate each entity mention to its most linked entity in Wikipedia, we can already achieve 85% to 90% accuracy, if mention detection is perfectly accurate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9985093474388123}, {"text": "mention detection", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.6927457600831985}]}, {"text": "However, mention detection is a difficult problem as only about 3% of candidates are valid entity mentions.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8121971786022186}]}, {"text": "It is worthwhile to mention that, as per, for computational efficiency, we apply several preprocessing steps before running our entity linking system.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.7013571113348007}]}, {"text": "First, for each anchor in Wikipedia, we gather all entities it can disambiguate to and remove from that anchor's entity set all entities that are linked less than 2% of the time.", "labels": [], "entities": []}, {"text": "Second, we apply a modified filtering procedure similar to that proposed in to filter the set of candidates per tweet.", "labels": [], "entities": []}, {"text": "Evaluation Our annotated datasets contain entities from many Wikipedia categories.", "labels": [], "entities": []}, {"text": "For evaluation, we primarily focus on entities belonging to a set of six core categories (Person, Location, Organization, TV Show, Book/Magazine, Movie).", "labels": [], "entities": []}, {"text": "We believe it is necessary to focus upon core entities, rather than considering all possible entities in Wikipedia.", "labels": [], "entities": []}, {"text": "Most common words in the English language have their own Wikpedia page, but most words are not important enough to be considered entities.", "labels": [], "entities": []}, {"text": "In general, there is a large degree of subjectivity when comparing different entity linking datasets; different researchers have their own interpretation of what constitutes an entity.", "labels": [], "entities": []}, {"text": "For example, we examined the annotation used in () and found it to be extremely lenient, when compared to our own beliefs of what is an entity.", "labels": [], "entities": []}, {"text": "Therefore, we believe evaluating performance on restricted entity types is the only fairway to compare different endto-end entity linking systems.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our system on a per-tweet basis, by comparing the set of annotated \"gold\" entities with the set of entities predicted by our system, and computing performance metrics (precision, recall, F 1 ).", "labels": [], "entities": [{"text": "precision", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.9995402097702026}, {"text": "recall", "start_pos": 210, "end_pos": 216, "type": "METRIC", "confidence": 0.9984502792358398}, {"text": "F 1", "start_pos": 218, "end_pos": 221, "type": "METRIC", "confidence": 0.9891731441020966}]}, {"text": "We choose to evaluate our system on a per-tweet basis, as opposed to a perentity basis, because we wish to avoid the issue of matching segmentations.", "labels": [], "entities": []}, {"text": "For example, it is quite common to observe multiple overlapping phrases in a tweet that should be linked to the same entity (e.g., \"President Obama\" and \"Obama\").", "labels": [], "entities": []}, {"text": "When evaluating our system, we compute performance metrics for both all entities and core entities.", "labels": [], "entities": []}, {"text": "11 Parameters In our implementation, we fixed the regularization parameter C = 10.", "labels": [], "entities": []}, {"text": "When beam-  search is used, the beam size is set to be 50, and we only consider the top 10 candidates for each candidate to speed the inference process.", "labels": [], "entities": [{"text": "beam-  search", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.7930357853571574}]}, {"text": "In the context word mining algorithm, r = 0.5% and z = 1000.", "labels": [], "entities": [{"text": "word mining", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.770196408033371}]}], "tableCaptions": [{"text": " Table 2: Labeled example statistics. \"#Cand\" represents  the total number of candidates we found in this dataset.  \"#Men.\" is the total number of mentions that disam- biguate to an entity. The top-1 rate (P@1) represents the  proportion of the mentions that disambiguate to the most  linked entity in Wikipedia.", "labels": [], "entities": [{"text": "top-1 rate (P@1)", "start_pos": 194, "end_pos": 210, "type": "METRIC", "confidence": 0.7337192893028259}]}, {"text": " Table 3: Comparisons between different end-to-end en- tity linking systems. We evaluate performance on core  entities, as it is the only fair way to compare different  systems.", "labels": [], "entities": []}, {"text": " Table 4: Feature Study: F 1 for entity linking perfor- mance. \"All\" means evaluation on all annotated entities.  \"Core\" means evaluation only on our six entity types.  Each row contains all additional features of the row above  it.", "labels": [], "entities": [{"text": "F", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.9726584553718567}]}, {"text": " Table 5: An example of context words that are automati- cally extracted from 20 million unlabeled tweets. For the  sake of brevity, we only display context words for two  categories. Note that there are misspelled words (such  as \"watchn\") and abbreviations (such as nw) that do not  appear in well-written documents.", "labels": [], "entities": []}, {"text": " Table 6: Evaluation results (F 1 ) of the advanced models.  \"+ Context\" is the model that uses additional context fea- tures extracted from 20 millions unlabeled tweets. \"+ Co- hesiveness\" is the model with both additional context and  cohesiveness features. \"+2nd order\" is our final model  (which incorporates context, cohesiveness, and second- order features).", "labels": [], "entities": []}]}