{"title": [{"text": "Semi-Supervised Discriminative Language Modeling with Out-of-Domain Text Data", "labels": [], "entities": [{"text": "Semi-Supervised Discriminative Language Modeling", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.5625823810696602}]}], "abstractContent": [{"text": "One way to improve the accuracy of automatic speech recognition (ASR) is to use dis-criminative language modeling (DLM), which enhances discrimination by learning where the ASR hypotheses deviate from the uttered sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.998702883720398}, {"text": "automatic speech recognition (ASR)", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7791858861843745}, {"text": "dis-criminative language modeling (DLM)", "start_pos": 80, "end_pos": 119, "type": "TASK", "confidence": 0.7567871908346812}]}, {"text": "However, DLM requires large amounts of ASR output to train.", "labels": [], "entities": [{"text": "ASR", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8519041538238525}]}, {"text": "Instead, we can simulate the output of an ASR system , in which case the training becomes semi-supervised.", "labels": [], "entities": [{"text": "ASR", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9672451019287109}]}, {"text": "The advantage of using simulated hypotheses is that we can generate as many hypotheses as we want provided that we have enough text material.", "labels": [], "entities": []}, {"text": "In typical scenarios , transcribed in-domain data is limited but large amounts of out-of-domain (OOD) data is available.", "labels": [], "entities": []}, {"text": "In this study, we investigate how semi-supervised training performs with OOD data.", "labels": [], "entities": [{"text": "OOD data", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.7165176421403885}]}, {"text": "We find out that OOD data can yield improvements comparable to in-domain data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discriminative language modeling (DLM) helps ASR systems to discriminate between acoustically similar word sequences in the process of choosing the most accurate transcription of an utterance.", "labels": [], "entities": [{"text": "Discriminative language modeling (DLM", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7496481657028198}, {"text": "ASR", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9896695017814636}]}, {"text": "DLM characterizes and learns from ASR errors by comparing the reference transcription of the utterance and the candidate hypotheses generated by the ASR system.", "labels": [], "entities": [{"text": "ASR errors", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.8624040484428406}]}, {"text": "Although previous studies based on this supervised setting have been successful, they require large amounts of transcribed speech data and a well-trained in-domain ASR system, both of which are hard to obtain.", "labels": [], "entities": [{"text": "ASR", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.967985987663269}]}, {"text": "To overcome this difficulty, instead of training with the real ASR output, we can use simulated output, in which case the training becomes semi-supervised.", "labels": [], "entities": []}, {"text": "Semi-supervised training for discriminative language modeling has been shown to achieve as good word error rate (WER) reduction as the training done with real ASR output (.", "labels": [], "entities": [{"text": "discriminative language modeling", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7186482151349386}, {"text": "word error rate (WER) reduction", "start_pos": 96, "end_pos": 127, "type": "METRIC", "confidence": 0.8842295748846871}]}, {"text": "In this approach, first a confusion model (CM) is estimated from supervised data.", "labels": [], "entities": []}, {"text": "This CM contains all seen confusions and their occurrence probabilities in hypotheses generated by an ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.8886258006095886}]}, {"text": "Then, the CM is used to generate a number of alternative-but-incorrect hypotheses, or simulated hypotheses, fora given sentence.", "labels": [], "entities": []}, {"text": "Since the CM characterizes the errors that the ASR system makes, simulated hypotheses carry these characteristics.", "labels": [], "entities": [{"text": "ASR", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9735060334205627}]}, {"text": "At the end, the DLM is trained on the reference sentences and their simulated hypotheses.", "labels": [], "entities": []}, {"text": "Although being able to simulate the output of the ASR system allows us to generate as much output as we need for the DLM training, there is not always enough text data that is in the same domain as the ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9243703484535217}]}, {"text": "Yet, it is easier to find large amounts of out-ofdomain (OOD) text data.", "labels": [], "entities": []}, {"text": "In this study, we extend the previous studies where in-domain text data was used for hypothesis simulation.", "labels": [], "entities": [{"text": "hypothesis simulation", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8933144509792328}]}, {"text": "Instead of using limited in-domain data, we experiment with larger amounts of OOD data for hypothesis simulation.", "labels": [], "entities": [{"text": "hypothesis simulation", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.8360853493213654}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we summarize the related work.", "labels": [], "entities": []}, {"text": "In Section 3, we explain the methods to simulate the hypotheses and to train the DLM.", "labels": [], "entities": []}, {"text": "We give the experimental results in Section 4 before concluding with Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employ DLM on a Turkish broadcast news transcription data set (, which comprises disjoint training (105356 sentences), held-out (1947 sentences) and test (1784 sentences) subsets consisting of ASR outputs represented as N -best lists.", "labels": [], "entities": [{"text": "Turkish broadcast news transcription data set", "start_pos": 19, "end_pos": 64, "type": "DATASET", "confidence": 0.675569216410319}]}, {"text": "We use Morfessor ( to obtain the morph level word segmentations from which we build the LMs.", "labels": [], "entities": []}, {"text": "For semi-supervised experiments, we use the first half of the training subset (t 1 : 53992 sentences, 965K morphs) to learn the confusion models, and the reference transcriptions of the second half (t 2 : 51364 sentences, 935K morphs) to generate in-domain simulated n-best lists to be compared against OOD simulated ones.", "labels": [], "entities": []}, {"text": "For this setup, the generative baseline WER and oracle WER on the held-out set are 22.9% and 14.2% and on the test set are 22.4% and 13.9%, respectively.", "labels": [], "entities": [{"text": "WER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.714260995388031}]}, {"text": "When we use ASR 50-best from t 1 for DLM training, WERs drop to 22.2% and 21.8% on the held-out and the test sets, respectively.", "labels": [], "entities": [{"text": "ASR 50-best", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.921378880739212}, {"text": "DLM training", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.8887768983840942}, {"text": "WERs", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9996181726455688}]}, {"text": "For OOD data, we use a data set of 10.8M sentences (140M morphs) from newspaper articles downloaded from the Internet.", "labels": [], "entities": [{"text": "OOD", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7783543467521667}]}, {"text": "To calculate the perplexity of OOD sentences for selection, we use a language model trained over the reference transcripts and 50-best lists oft 1 and t 2 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 shows all the results obtained from  four 500K OOD data sets.", "labels": [], "entities": [{"text": "OOD data sets", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.8170287410418192}]}, {"text": " Table 2: KL distance, KL(M || U), between uniform dis- tribution (U) and unigram morph distribution (M); num- ber of unique morphs and tokens.", "labels": [], "entities": []}, {"text": " Table 3. In order to see how the size of OOD  data set affects the WER reduction, we start with  50K sentences and increase the size gradually up  to 500K. The first row of", "labels": [], "entities": [{"text": "OOD  data set", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8752428690592448}, {"text": "WER reduction", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.783970057964325}]}, {"text": " Table 3: WER (%) on held-out set for in-domain  (Syllable+ASR-LM+ASRdist-50) and four OOD data  sets in increasing sizes", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9969243407249451}, {"text": "OOD data  sets", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.7738729814688364}]}]}