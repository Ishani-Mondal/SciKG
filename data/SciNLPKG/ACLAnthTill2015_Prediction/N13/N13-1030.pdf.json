{"title": [{"text": "Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression", "labels": [], "entities": [{"text": "Keyphrase Extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6406427472829819}]}], "abstractContent": [{"text": "Multi-Sentence Compression (MSC) is the task of generating a short single sentence summary from a cluster of related sentences.", "labels": [], "entities": [{"text": "Multi-Sentence Compression (MSC)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7852401196956634}]}, {"text": "This paper presents an N-best reranking method based on keyphrase extraction.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7417823374271393}]}, {"text": "Compression candidates generated by a word graph-based MSC approach are reranked according to the number and relevance of keyphrases they contain.", "labels": [], "entities": []}, {"text": "Both manual and automatic evaluations were performed using a dataset made of clusters of newswire sentences.", "labels": [], "entities": []}, {"text": "Results show that the proposed method significantly improves the informativity of the generated compressions .", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-Sentence Compression (MSC) can be broadly described as the task of generating a short single sentence summary from a cluster of related sentences.", "labels": [], "entities": [{"text": "Multi-Sentence Compression (MSC)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8241653800010681}]}, {"text": "It has recently attracted much attention, mostly because of its relevance to single or multi-document extractive summarization.", "labels": [], "entities": [{"text": "single or multi-document extractive summarization", "start_pos": 77, "end_pos": 126, "type": "TASK", "confidence": 0.6588533699512482}]}, {"text": "A standard way to generate summaries consists in ranking sentences by importance, cluster them by similarity and select a sentence from the top ranked clusters (.", "labels": [], "entities": []}, {"text": "One difficulty is then to generate concise, non-redundant summaries.", "labels": [], "entities": []}, {"text": "Selected sentences almost always contain additional information specific to the documents from which they came, leading to readability issues in the summary.", "labels": [], "entities": []}, {"text": "Sentence Compression (SC), i.e. the task of summarizing a sentence while retaining most of the informational content and remaining grammatical, is a straightforward solution to this problem.", "labels": [], "entities": [{"text": "Sentence Compression (SC)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8495575547218323}, {"text": "summarizing a sentence", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.9028792579968771}]}, {"text": "Another solution would be to create, for each cluster of related sentences, a concise and fluent fusion of information, reflecting facts common to all sentences.", "labels": [], "entities": []}, {"text": "Originally defined as sentence fusion (), MSC is a textto-text generation process in which a novel sentence is produced as a result of summarizing common information across a set of similar sentences.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7334998697042465}, {"text": "textto-text generation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7225103676319122}]}, {"text": "Most of the previous MSC approaches rely on syntactic parsers for producing grammatical compressions, e.g. (. Recently, proposed a word graph-based approach which only requires a Part-Of-Speech (POS) tagger and a list of stopwords.", "labels": [], "entities": []}, {"text": "The key assumption behind her approach is that redundancy within the set of related sentences provides a reliable way of generating informative and grammatical sentences.", "labels": [], "entities": []}, {"text": "Although this approach seemingly works well, 48% to 60% of the generated sentences are missing important information about the set of related sentences.", "labels": [], "entities": []}, {"text": "In this study, we aim at producing more informative sentences by maximizing the range of topics they cover.", "labels": [], "entities": []}, {"text": "Keyphrases are words that capture the main topics of a document.", "labels": [], "entities": []}, {"text": "Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and questionanswering (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.9871501922607422}, {"text": "information retrieval", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7916966378688812}]}, {"text": "In summarization, keyphrases provide semantic metadata that represent the content of a document.", "labels": [], "entities": [{"text": "summarization", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9767246842384338}]}, {"text": "Sentences containing the most relevant keyphrases are used to generate the summary).", "labels": [], "entities": []}, {"text": "In the same way, we hypothesize that keyphrases can be used to better generate sentences that convey the gist of the set of related sentences.", "labels": [], "entities": []}, {"text": "In this paper, we present a reranking method of N-best multi-sentence compressions based on keyphrase extraction and describe a series of experiments conducted on a manually constructed evaluation corpus.", "labels": [], "entities": [{"text": "N-best multi-sentence compressions", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.6277251938978831}, {"text": "keyphrase extraction", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.7182832807302475}]}, {"text": "More precisely, the main contributions of our work are as follows: \u2022 We extend Filippova (2010)'s word graphbased MSC approach to produce wellpunctuated and more informative compressions.", "labels": [], "entities": []}, {"text": "\u2022 We investigate the use of automatic Machine Translation (MT) and summarization evaluation metrics to evaluate MSC performance.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8607802033424378}, {"text": "summarization evaluation", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.8409605324268341}, {"text": "MSC", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9720991253852844}]}, {"text": "\u2022 We introduce a French evaluation dataset made of 40 sets of related sentences along with reference compressions composed by humans.", "labels": [], "entities": [{"text": "French evaluation dataset", "start_pos": 17, "end_pos": 42, "type": "DATASET", "confidence": 0.8066115776697794}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first briefly review the previous work, followed by a description of the method we propose.", "labels": [], "entities": []}, {"text": "Next, we give the details of the evaluation dataset we have constructed and present our experiments and results.", "labels": [], "entities": []}, {"text": "Lastly, we conclude with a discussion and directions for further work.", "labels": [], "entities": []}, {"text": "2 Related work 2.1 Multi-sentence compression MSC have received much attention recently and many different approaches have been proposed.", "labels": [], "entities": [{"text": "Multi-sentence compression MSC", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.8796864549318949}]}, {"text": "The pioneering work of () introduced the framework used by many subsequent works: input sentences are represented by dependency trees, some words are aligned to merge the trees into a lattice, and the lattice is linearized using tree traversal to produce fusion sentences.) cast MSC as an integer linear program, and show promising results for German.", "labels": [], "entities": []}, {"text": "Later, () proposed a supervised approach trained on examples of manually fused sentences.", "labels": [], "entities": []}, {"text": "Previously described approaches require the use of a syntactic parser to control the grammaticality of the output.", "labels": [], "entities": []}, {"text": "As an alternative, several word graph-based approaches that only require a POS tagger were proposed.", "labels": [], "entities": []}, {"text": "The key assumption is that redundancy provides a reliable way of generating grammatical sentences.", "labels": [], "entities": []}, {"text": "First, a directed word graph is constructed from the set of input sentences in which nodes represent unique words, defined as word and POS tuples, and edges express the original structure of sentences (i.e. word ordering).", "labels": [], "entities": []}, {"text": "Sentence compressions are obtained by finding commonly used paths in the graph.", "labels": [], "entities": [{"text": "Sentence compressions", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.933563232421875}]}, {"text": "Word graphbased MSC approaches were used in different tasks, such as guided microblog summarization (, opinion summarization () and newswire summarization (Filippova, 2010).", "labels": [], "entities": [{"text": "guided microblog summarization", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.6168851256370544}, {"text": "opinion summarization", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.7224126160144806}, {"text": "newswire summarization", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.626626506447792}]}], "datasetContent": [{"text": "To our knowledge, there is no dataset available to evaluate MSC in an automatic way.", "labels": [], "entities": [{"text": "MSC", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9203193783760071}]}, {"text": "The performance of the previously described approaches was assessed by human judges.", "labels": [], "entities": []}, {"text": "In this work, we introduce anew evaluation dataset made of 40 sets of related sentences along with reference compressions composed by human assessors.", "labels": [], "entities": []}, {"text": "The purpose of this dataset is to investigate the use of existing automatic evaluation metrics for the MSC task.", "labels": [], "entities": [{"text": "MSC task", "start_pos": 103, "end_pos": 111, "type": "TASK", "confidence": 0.9340674579143524}]}, {"text": "Similar to, we collected news articles presented in clusters on the French edition of Google News 2 over a period of three months.", "labels": [], "entities": [{"text": "French edition of Google News 2", "start_pos": 68, "end_pos": 99, "type": "DATASET", "confidence": 0.9378451903661092}]}, {"text": "Clusters composed of at least 20 news articles and containing one single prevailing event were manually selected.", "labels": [], "entities": []}, {"text": "To obtain the sets of related sentences, we extracted the first sentences from each article in the cluster, removing duplicates.", "labels": [], "entities": []}, {"text": "Leading sentences in news articles are known to provide a good summary of the article content and are used as a baseline in summarization (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.976394772529602}]}, {"text": "The resulting dataset contains 618 sentences (33 tokens on average) spread over 40 clusters.", "labels": [], "entities": []}, {"text": "The number of sentences within each cluster is on average 15, with a minimum of 7 and a maximum of 36.", "labels": [], "entities": []}, {"text": "The word redundancy rate within the dataset, computed as the number of unique words over the number of words for each cluster, is 38.8%.", "labels": [], "entities": []}, {"text": "Three reference compressions were manually composed for each set of sentences.", "labels": [], "entities": []}, {"text": "Human annotators, all native French speakers, were asked to carefully read the set of sentences, extract the most salient facts and generate a sentence (compression) that summarize the set of sentences.", "labels": [], "entities": []}, {"text": "Annotators were also told to introduce as little new vocabulary as possible in their compressions.", "labels": [], "entities": []}, {"text": "The purpose of this guideline is to reduce the number of possible mismatches, as existing evaluation metrics are based on n-gram comparison.", "labels": [], "entities": []}, {"text": "Reference compressions have a compression rate of 60%.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.9761003255844116}]}, {"text": "The use of automatic methods for evaluating machine-generated text has gradually become the mainstream in Computational Linguistics.", "labels": [], "entities": [{"text": "Computational Linguistics", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.8800740838050842}]}, {"text": "Well known examples are the ROUGE) and BLEU () evaluation metrics used in the summarization and MT communities.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9798668622970581}, {"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9934084415435791}, {"text": "summarization", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.9823765754699707}, {"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.7676690816879272}]}, {"text": "These metrics assess the quality of a system output by computing its similarity to one or more human-generated references.", "labels": [], "entities": []}, {"text": "Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.7685571908950806}, {"text": "F1 measure", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9837019145488739}]}, {"text": "It was shown to correlate significantly with human judgments () and behave similarly to BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9982935786247253}]}, {"text": "However, this metric is not entirely reliable as it depends on parser accuracy and the type of dependency relations used).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9353142380714417}]}, {"text": "In this work, the following evaluation measures are considered relevant: BLEU 3 , ROUGE-1 (unigrams), ROUGE-2 (bigrams) and ROUGE-SU4 (bigrams with skip distance up to 4 words) . ROUGE measures are computed using stopword removal and French stemming 5 .  The quality of the generated compressions was assessed in an experiment with human raters.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9984059929847717}, {"text": "ROUGE-1", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.967401921749115}, {"text": "ROUGE-2", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9805343747138977}, {"text": "ROUGE-SU4", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9534536004066467}]}, {"text": "Two aspects were considered: grammaticality and informativity.", "labels": [], "entities": []}, {"text": "Following previous work), we asked raters to assess grammaticality on a 3-points scale: perfect (2 pts), if the compression is a complete grammatical sentence; almost (1 pt), if it requires minor editing, e.g. one mistake in articles, agreement or punctuation; ungrammatical (0 pts), if it is none of the above.", "labels": [], "entities": []}, {"text": "Raters were explicitly asked to ignore lack of capitalization while evaluating grammaticality.", "labels": [], "entities": []}, {"text": "Informativity is evaluated according to the 3-points scale defined in: perfect (2 pts), if the compression conveys the gist of the main event and is more or less like the summary the person would produce himself; related (1 pt), if it is related to the the main theme but misses something important; unrelated (0 pts), if the compression is not related to the main theme.", "labels": [], "entities": []}, {"text": "Three raters, all native French speakers, were hired to assess the generated compressions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average ratings over all clusters and raters along  with average compression length (in tokens), standard de- viation and corresponding compression rate ( \u2020 indicates  significance at the 0.01 level using Student's t-test).", "labels": [], "entities": [{"text": "standard de- viation", "start_pos": 107, "end_pos": 127, "type": "METRIC", "confidence": 0.8889080882072449}, {"text": "compression rate", "start_pos": 146, "end_pos": 162, "type": "METRIC", "confidence": 0.926460862159729}, {"text": "Student's t-test", "start_pos": 215, "end_pos": 231, "type": "DATASET", "confidence": 0.8322230776151022}]}, {"text": " Table 2: Distribution over possible manual ratings for  grammaticality and informativity. Ratings are expressed  on a scale of 0 to 2.", "labels": [], "entities": []}, {"text": " Table 3: Automatic evaluation scores ( \u2020 and  \u2021 indicate  significance at the 0.01 and 0.001 levels respectively us- ing Student's t-test)", "labels": [], "entities": [{"text": "Automatic evaluation scores", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.9104138811429342}]}, {"text": " Table 4: Pearson correlation coefficients for automatic  metrics vs. average human ratings.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8840576708316803}]}]}