{"title": [{"text": "A Tensor-based Factorization Model of Semantic Compositionality", "labels": [], "entities": [{"text": "Semantic Compositionality", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.702458530664444}]}], "abstractContent": [{"text": "In this paper, we present a novel method for the computation of compositionality within a distri-butional framework.", "labels": [], "entities": [{"text": "computation of compositionality", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.6798367301623026}]}, {"text": "The key idea is that com-positionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data.", "labels": [], "entities": []}, {"text": "We use our method to model the composition of subject verb object triples.", "labels": [], "entities": []}, {"text": "The method consists of two steps.", "labels": [], "entities": []}, {"text": "First, we compute a latent factor model for nouns from standard co-occurrence data.", "labels": [], "entities": []}, {"text": "Next, the latent factors are used to induce a latent model of three-way subject verb object interactions.", "labels": [], "entities": []}, {"text": "Our model has been evaluated on a similarity task for transitive phrases, in which it exceeds the state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the course of the last two decades, significant progress has been made with regard to the automatic extraction of lexical semantic knowledge from largescale text corpora.", "labels": [], "entities": [{"text": "automatic extraction of lexical semantic knowledge from largescale text corpora", "start_pos": 93, "end_pos": 172, "type": "TASK", "confidence": 0.8485493957996368}]}, {"text": "Most work relies on the distributional hypothesis of meaning, which states that words that appear within the same contexts tend to be semantically similar.", "labels": [], "entities": []}, {"text": "A large number of researchers have taken this dictum to heart, giving rise to a plethora of algorithms that try to capture the semantics of words by looking at their distribution in text.", "labels": [], "entities": []}, {"text": "Up till now, however, most work on the automatic acquisition of semantics only deals with individual words.", "labels": [], "entities": [{"text": "automatic acquisition of semantics", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.8077924400568008}]}, {"text": "The modeling of meaning beyond the level of individual words -i.e. the combination of words into larger units -is to a large degree left unexplored.", "labels": [], "entities": []}, {"text": "The principle of compositionality, often attributed to Frege, is the principle that states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined.", "labels": [], "entities": []}, {"text": "It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words.", "labels": [], "entities": []}, {"text": "Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality ().", "labels": [], "entities": []}, {"text": "However, the absolute gains of the systems remain a bit unclear, and a simple method of composition -vector multiplicationoften seems to produce the best results.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel method for the joint composition of a verb with its subject and direct object.", "labels": [], "entities": []}, {"text": "The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data.", "labels": [], "entities": []}, {"text": "In order to adequately model the multiway interaction between a verb and its subject and objects, a significant part of our method relies on tensor algebra.", "labels": [], "entities": []}, {"text": "Additionally, our method makes use of a factorization model appropriate for tensors.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2, we give an overview of previous work that is relevant to the task of computing compositionality within a distributional framework.", "labels": [], "entities": []}, {"text": "Section 3 presents a detailed description of our method, including an overview of the necessary mathematical machinery.", "labels": [], "entities": []}, {"text": "Section 4 illustrates our method with a number of detailed examples.", "labels": [], "entities": []}, {"text": "Section 5 presents a quantitative evaluation, and compares our method to other models of distributional compositionality.", "labels": [], "entities": []}, {"text": "Section 6, then, concludes and lays out a number of directions for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Factor pairs with highest value for matrix Y athlete,race", "labels": [], "entities": []}, {"text": " Table 2: Factor pairs with highest value for matrix Y user,command", "labels": [], "entities": []}, {"text": " Table 5: Results of the different compositionality models  on the phrase similarity task", "labels": [], "entities": [{"text": "phrase similarity", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7955165207386017}]}]}