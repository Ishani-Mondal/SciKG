{"title": [{"text": "Three Knowledge-Free Methods for Automatic Lexical Chain Extraction", "labels": [], "entities": [{"text": "Automatic Lexical Chain Extraction", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.6905015930533409}]}], "abstractContent": [{"text": "We present three approaches to lexical chaining based on the LDA topic model and evaluate them intrinsically on a manually annotated set of German documents.", "labels": [], "entities": [{"text": "LDA topic model", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.8678150375684103}]}, {"text": "After motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains, we describe our new two-level chain annotation scheme, which rooted in the concept of cohesive harmony.", "labels": [], "entities": []}, {"text": "Also, we propose anew measure for direct evaluation of lexical chains.", "labels": [], "entities": []}, {"text": "Our three LDA-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin, which can be attributed to lacking coverage of the knowledge resource.", "labels": [], "entities": []}, {"text": "Subsequent analysis shows that the three methods yield a different chaining behavior, which could be utilized in tasks that use lexical chaining as a component within NLP applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "A text that is understandable by its nature exhibits an underlying structure which makes the text coherent; that is, the structure is responsible for making the text \"hang\" together (.", "labels": [], "entities": []}, {"text": "The theoretic foundation of this structure is defined as coherence and cohesion.", "labels": [], "entities": []}, {"text": "While the former is concerned with the meaning of a text, the latter can be seen as a collection of devices for creating it.", "labels": [], "entities": []}, {"text": "Cohesion and coherence build the basis for most of the current natural language processing problems that deal with text understanding.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7415366172790527}]}, {"text": "Lexical cohesion ties together words or phrases that are semantically related.", "labels": [], "entities": []}, {"text": "Once all the cohesive ties are identified the involved items can be grouped together to form so-called lexical chains, which form a theoretically well-founded building block in various natural language processing applications, such as word sense disambiguation, summarization (, malapropism detection and correction), document hyperlinking), text segmentation (), topic tracking, and others.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 235, "end_pos": 260, "type": "TASK", "confidence": 0.6944744984308878}, {"text": "summarization", "start_pos": 262, "end_pos": 275, "type": "TASK", "confidence": 0.9869138598442078}, {"text": "malapropism detection and correction", "start_pos": 279, "end_pos": 315, "type": "TASK", "confidence": 0.8125406801700592}, {"text": "text segmentation", "start_pos": 342, "end_pos": 359, "type": "TASK", "confidence": 0.7679193615913391}, {"text": "topic tracking", "start_pos": 364, "end_pos": 378, "type": "TASK", "confidence": 0.8402877449989319}]}, {"text": "The performance of the individual task heavily depends on the quality of the identified lexical chains.", "labels": [], "entities": []}], "datasetContent": [{"text": "For comparison, we implemented three baselines, which we describe below.", "labels": [], "entities": []}, {"text": "One baseline is trivial, two baselines are state-of-the art knowledge-based systems adapted to German.", "labels": [], "entities": []}, {"text": "Random: Candidate lexical items are randomly tied together to form sets of lexical chains.", "labels": [], "entities": []}, {"text": "Level two links are created analogously.", "labels": [], "entities": []}, {"text": "We regulate the process to yield the same average number of chains and links as in the development and test data.", "labels": [], "entities": []}, {"text": "S&M GermaNet: Algorithm by with GermaNet as its knowledge resource.", "labels": [], "entities": []}, {"text": "G&M GermaNet: Algorithm by, also using GermaNet.", "labels": [], "entities": []}, {"text": "GermaNet) is a large WordNet-like resource for German, containing almost 100,000 lexical units and over 87,000 conceptual relations between synsets.", "labels": [], "entities": []}, {"text": "While its size is only about half of WordNet, it is one of the largest nonEnglish lexical semantic resources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9685220718383789}]}, {"text": "For evaluation purposes, terms that consist of multiple words are mapped to its rightmost term which is assumed to be the head, e.g. \"dirty money\" is mapped to \"money\".", "labels": [], "entities": []}, {"text": "Additionally, singleton chains, i.e. chains that contain only a single lexical item are omitted unless the respective lexical item is not linked by a level two link.", "labels": [], "entities": []}, {"text": "Dense Chains Comparative results of the approaches in terms of lccm for both annotators are summarized in (upper half).", "labels": [], "entities": []}, {"text": "We observe that all our new methods beat the random baseline and the two knowledge-based baselines by a large margin.", "labels": [], "entities": []}, {"text": "The knowledge-based baselines, both using  GermaNet, produce very similar lccm scores, which highlights the important role of the knowledge resource.", "labels": [], "entities": [{"text": "GermaNet", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8823962807655334}]}, {"text": "Data analysis revealed that while chains produced by knowledge-based baselines are sensible, the main problem is alack of coverage in terms of vocabulary and relations in GermaNet.", "labels": [], "entities": [{"text": "GermaNet", "start_pos": 171, "end_pos": 179, "type": "DATASET", "confidence": 0.8989999294281006}]}, {"text": "Comparing the statistical methods, the LDA-GM method excels over the others.", "labels": [], "entities": []}, {"text": "(lower half) summarizes the evaluation results of the merged chains via level two links.", "labels": [], "entities": []}, {"text": "Because of merging, a text now contains fewer chains with more lexical items each.", "labels": [], "entities": []}, {"text": "Note that knowledge-based baselines do not construct level two links, which is why they are heavily penalized in this setup.", "labels": [], "entities": []}, {"text": "Again, the statistical methods beat the baselines by a substantial amount.", "labels": [], "entities": []}, {"text": "In this evaluation, the random baseline performs above the knowledge-based methods, which is rooted in the fact that lccm penalizes small, correct chains, whereas the random baseline with linking often produces very large chains containing most of the terms -something that we also observe for many manually annotated documents.", "labels": [], "entities": []}, {"text": "The large overlap in the biggest chain then leads to the comparatively high random baseline score.", "labels": [], "entities": [{"text": "random baseline score", "start_pos": 76, "end_pos": 97, "type": "METRIC", "confidence": 0.8539226651191711}]}, {"text": "In this evaluation, the LDA-MM is the clear winner, with LDA-GM being clearly inferior this time.: Quantitiative characteristics of automatic and manual lexical chains.", "labels": [], "entities": [{"text": "Quantitiative", "start_pos": 99, "end_pos": 112, "type": "METRIC", "confidence": 0.9413141012191772}]}, {"text": "In average, a document contains 51.58 candidate terms as extracted by our noun phrase patterns", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of the evaluation based on dense chains  (upper half) and merged chains (lower half). The annota- tor agreement on the test set's chains = 0.585; on merged  chains = 0.553", "labels": [], "entities": [{"text": "annota- tor agreement", "start_pos": 108, "end_pos": 129, "type": "METRIC", "confidence": 0.9436721354722977}]}, {"text": " Table 3: Quantitiative characteristics of automatic and manual lexical chains. In average, a document contains 51.58  candidate terms as extracted by our noun phrase patterns", "labels": [], "entities": []}]}