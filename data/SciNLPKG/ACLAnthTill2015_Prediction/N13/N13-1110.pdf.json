{"title": [{"text": "Same Referent, Different Words: Unsupervised Mining of Opaque Coreferent Mentions", "labels": [], "entities": [{"text": "Unsupervised Mining of Opaque Coreferent Mentions", "start_pos": 32, "end_pos": 81, "type": "TASK", "confidence": 0.7339942504962286}]}], "abstractContent": [{"text": "Coreference resolution systems rely heavily on string overlap (e.g., Google Inc. and Google), performing badly on mentions with very different words (opaque mentions) like Google and the search giant.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9263949990272522}]}, {"text": "Yet prior attempts to resolve opaque pairs using ontolo-gies or distributional semantics hurt precision more than improved recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9992653727531433}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9958015084266663}]}, {"text": "We present anew unsupervised method for mining opaque pairs.", "labels": [], "entities": []}, {"text": "Our intuition is to restrict distributional semantics to articles about the same event, thus promoting referential match.", "labels": [], "entities": []}, {"text": "Using an En-glish comparable corpus of tech news, we built a dictionary of opaque coreferent mentions (only 3% are in WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.9778874516487122}]}, {"text": "Our dictionary can be integrated into any coreference system (it increases the performance of a state-of-the-art system by 1% F1 on all measures) and is easily extendable by using news aggregators.", "labels": [], "entities": [{"text": "F1", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9982913136482239}]}], "introductionContent": [{"text": "Repetition is one of the most common coreferential devices in written text, making string-match features important to all coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.8237041234970093}]}, {"text": "In fact, the scores achieved by just head match and a rudimentary form of pronominal resolution are not far from that of state-of-the-art systems.", "labels": [], "entities": [{"text": "pronominal resolution", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7265306711196899}]}, {"text": "This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate area serious problem for modern systems: they comprise 65% of the non-pronominal Closest NP with the same gender and number.", "labels": [], "entities": [{"text": "Cupertino slate", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.8027148246765137}]}, {"text": "errors made by the Stanford system on the CoNLL-2011 data.", "labels": [], "entities": [{"text": "CoNLL-2011 data", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9635556042194366}]}, {"text": "Solving this problem is critical for overcoming the recall gap of state-of-the-art systems.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9871125817298889}]}, {"text": "Previous systems have turned either to ontologies) or distributional semantics) to help solve these errors.", "labels": [], "entities": []}, {"text": "But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists.", "labels": [], "entities": []}, {"text": "Thus ontologies lead to precision problems, and to recall problems like missing NE descriptions (e.g., Apple and the iPhone maker) and metonymies (e.g., agreement and wording), while distributional systems lead to precision problems like coreferring Microsoft and the Mountain View giant because of their similar vector representation (release, software, update).", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9966028928756714}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9702326059341431}, {"text": "precision", "start_pos": 214, "end_pos": 223, "type": "METRIC", "confidence": 0.9768823385238647}, {"text": "Mountain View giant", "start_pos": 268, "end_pos": 287, "type": "DATASET", "confidence": 0.950231651465098}]}, {"text": "We increase precision by drawing on the intuition that referents that are both similar and participate in the same event are likely to corefer.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9990000128746033}]}, {"text": "We restrict distributional similarity to collections of articles that discuss the same event.", "labels": [], "entities": []}, {"text": "In the following two documents on the Nexus One from different sources, we take the subjects of the identical verb releaseGoogle and the Mountain View giant-as coreferent.", "labels": [], "entities": [{"text": "Mountain View giant-as coreferent", "start_pos": 137, "end_pos": 170, "type": "DATASET", "confidence": 0.9696359932422638}]}, {"text": "Based on this idea, we introduce anew unsupervised method that uses verbs in comparable corpora as pivots for extracting the hard cases of coreference resolution, and build a dictionary of opaque coreferent mentions (i.e., the dictionary entries are pairs of mentions).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.9262239038944244}]}, {"text": "This dictionary is then integrated into the Stanford coreference system), resulting in an average 1% improvement in the F1 score of all the evaluation measures.", "labels": [], "entities": [{"text": "Stanford coreference system", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.9124936461448669}, {"text": "F1 score", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9871150553226471}]}, {"text": "Our work points out the importance of context to decide whether a specific mention pair is coreferent.", "labels": [], "entities": []}, {"text": "On the one hand, we need to know what semantic relations are potentially coreferent (e.g., content and video).", "labels": [], "entities": []}, {"text": "On the other, we need to distinguish contexts that are compatible for coreference-(1) and (2-a)-from those that are not-(1) and (2-b).", "labels": [], "entities": []}, {"text": "(1) Elemental helps those big media entities process content across a full slate of mobile devices.", "labels": [], "entities": []}, {"text": "Elemental provides the picks and shovels to make video work across multiple devices. b. Elemental is powering the video for HBO Go.", "labels": [], "entities": [{"text": "HBO Go", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.950392872095108}]}, {"text": "Our dictionary of opaque coreferent pairs is our solution to the first problem, and we report on some preliminary work on context compatibility to address the second problem.", "labels": [], "entities": [{"text": "context compatibility", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.7297098636627197}]}], "datasetContent": [{"text": "We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard.", "labels": [], "entities": []}, {"text": "We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011).", "labels": [], "entities": [{"text": "CoNLL-2011 Shared Task", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.7890104651451111}]}, {"text": "Link-based metric that measures how many links the true and system partitions have in common.", "labels": [], "entities": []}, {"text": "\u2022 B 3 (Bagga and Baldwin, 1998).", "labels": [], "entities": [{"text": "Bagga and Baldwin, 1998)", "start_pos": 7, "end_pos": 31, "type": "DATASET", "confidence": 0.7135572185118993}]}, {"text": "Mention-based metric that measures the proportion of mention overlap between gold and predicted entities.", "labels": [], "entities": []}, {"text": "\u2022 CEAF-\u03c6 3 (Luo, 2005).", "labels": [], "entities": [{"text": "CEAF-\u03c6 3 (Luo, 2005)", "start_pos": 2, "end_pos": 22, "type": "DATASET", "confidence": 0.8807658808571952}]}, {"text": "Mention-based metric that, unlike B 3 , enforces a one-to-one alignment between gold and predicted entities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Dataset statistics: development (dev) and test.", "labels": [], "entities": []}, {"text": " Table 5: Incremental results for the four sieves using our dictionary on the development set. Baseline is the Stanford  system without the WordNet sieves. Scores are on gold mentions.", "labels": [], "entities": [{"text": "WordNet sieves", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.9515455067157745}]}, {"text": " Table 6: Performance on the test set. Scores are on gold mentions. Stars indicate a statistically significant difference  with respect to the baseline.", "labels": [], "entities": []}]}