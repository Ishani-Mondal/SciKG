{"title": [{"text": "Simultaneous Word-Morpheme Alignment for Statistical Machine Translation", "labels": [], "entities": [{"text": "Simultaneous Word-Morpheme Alignment", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6717131634553274}, {"text": "Statistical Machine Translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.8196161389350891}]}], "abstractContent": [{"text": "Current word alignment models for statistical machine translation do not address morphology beyond merely splitting words.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.7609336972236633}, {"text": "statistical machine translation", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.6937138140201569}]}, {"text": "We present a two-level alignment model that distinguishes between words and morphemes, in which we embed an IBM Model 1 inside an HMM based word alignment model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 140, "end_pos": 154, "type": "TASK", "confidence": 0.696648582816124}]}, {"text": "The model jointly induces word and morpheme alignments using an EM algorithm.", "labels": [], "entities": [{"text": "word and morpheme alignments", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6292650476098061}]}, {"text": "We evaluated our model on Turkish-English parallel data.", "labels": [], "entities": [{"text": "Turkish-English parallel data", "start_pos": 26, "end_pos": 55, "type": "DATASET", "confidence": 0.6221964458624522}]}, {"text": "We obtained significant improvement of BLEU scores over IBM Model 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9989548921585083}]}, {"text": "Our results indicate that utilizing information from morphology improves the quality of word alignments .", "labels": [], "entities": [{"text": "word alignments", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.7086309641599655}]}], "introductionContent": [{"text": "All current state-of-the-art approaches to SMT rely on an automatically word-aligned corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9952048063278198}]}, {"text": "However, current alignment models do not take into account the morpheme, the smallest unit of syntax, beyond merely splitting words.", "labels": [], "entities": []}, {"text": "Since morphology has not been addressed explicitly in word alignment models, researchers have resorted to tweaking SMT systems by manipulating the content and the form of what should be the so-called \"word\".", "labels": [], "entities": [{"text": "word alignment", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7390968501567841}, {"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9846184849739075}]}, {"text": "Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. and use unsupervised methods to find word segmentations that create a one-to-one mapping of words in both languages., \u02c7, and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.7280816435813904}, {"text": "machine translation.", "start_pos": 289, "end_pos": 309, "type": "TASK", "confidence": 0.7177727818489075}]}, {"text": "attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation.", "labels": [], "entities": []}, {"text": "split German compound nouns, and merge German phrases that correspond to a single English word.", "labels": [], "entities": []}, {"text": "Alternatively, manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation.", "labels": [], "entities": []}, {"text": "We present an alignment model that assumes internal structure for words, and we can legitimately talk about words and their morphemes inline with the linguistic conception of these terms.", "labels": [], "entities": []}, {"text": "Our model avoids the problem of collapsing words and morphemes into one single category.", "labels": [], "entities": []}, {"text": "We adopt a twolevel representation of alignment: the first level involves word alignment, the second level involves morpheme alignment in the scope of a given word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.7047300636768341}]}, {"text": "The model jointly induces word and morpheme alignments using an EM algorithm.", "labels": [], "entities": [{"text": "word and morpheme alignments", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6292650476098061}]}, {"text": "We develop our model in two stages.", "labels": [], "entities": []}, {"text": "Our initial model is analogous to IBM Model 1: the first level is a bag of words in a pair of sentences, and the second level is a bag of morphemes.", "labels": [], "entities": []}, {"text": "In this manner, we embed one IBM Model 1 in the scope of another IBM Model 1.", "labels": [], "entities": []}, {"text": "At the second stage, by introducing distortion probabilities at the word level, we develop an HMM extension of the initial model.", "labels": [], "entities": []}, {"text": "We evaluated the performance of our model on the Turkish-English pair both on hand-aligned data and by running end-to-end machine translation experiments.", "labels": [], "entities": []}, {"text": "To evaluate our results, we created gold word alignments for 75 Turkish-English sentences.", "labels": [], "entities": []}, {"text": "We obtain significant improvement of AER and BLEU scores over IBM Model 4.", "labels": [], "entities": [{"text": "AER", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9993802309036255}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9979487061500549}]}, {"text": "Section 2.1 introduces the concept of morpheme alignment in terms of its relation to word alignment.", "labels": [], "entities": [{"text": "morpheme alignment", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7807532846927643}, {"text": "word alignment", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7500452101230621}]}, {"text": "Section 2.2 presents the derivation of the EM algorithm and Section 3 presents the results of our experiments.", "labels": [], "entities": []}, {"text": "2 Two-level Alignment Model (TAM)", "labels": [], "entities": []}], "datasetContent": [{"text": "We initialized our baseline word-only model with 5 iterations of IBM Model 1, and further trained the HMM extension () for 5 iterations.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.9458445111910502}]}, {"text": "We call this model 'baseline HMM' in the discussions.", "labels": [], "entities": [{"text": "HMM", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8692581653594971}]}, {"text": "Similarly, we initialized the two versions of TAM with 5 iterations of the model explained in Section 2.2, and then trained the HMM extension of it as explained in Section 2.3 for 5 iterations.", "labels": [], "entities": []}, {"text": "To obtain BLEU scores for TAM models and our implementation of the word-only model, i.e. baseline-HMM, we bypassed GIZA++ in the Moses toolkit.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985536932945251}]}, {"text": "We also ran GIZA++ (IBM Model 1-4) on the data.", "labels": [], "entities": []}, {"text": "We translated 1000 sentence test sets.", "labels": [], "entities": []}], "tableCaptions": []}