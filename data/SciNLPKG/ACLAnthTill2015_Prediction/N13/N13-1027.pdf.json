{"title": [{"text": "Unsupervised Learning Summarization Templates from Concise Summaries", "labels": [], "entities": [{"text": "Unsupervised Learning Summarization Templates from Concise Summaries", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.7424804525715964}]}], "abstractContent": [{"text": "We here present and compare two unsuper-vised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages.", "labels": [], "entities": []}, {"text": "We evaluate the two approaches in two different information extraction settings: mono-lingual and cross-lingual information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7873825430870056}, {"text": "cross-lingual information extraction", "start_pos": 98, "end_pos": 134, "type": "TASK", "confidence": 0.5964745382467905}]}, {"text": "The extraction systems are trained on auto-annotated summaries (containing the induced concepts) and evaluated on human-annotated documents.", "labels": [], "entities": []}, {"text": "Extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries.", "labels": [], "entities": [{"text": "Extraction", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7514758706092834}]}], "introductionContent": [{"text": "Information Extraction ( and Automatic Text Summarization (Saggion and Poibeau, 2013) are two Natural Language Processing tasks which require domain and language adaptation.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8344857692718506}, {"text": "Automatic Text Summarization (Saggion and Poibeau, 2013)", "start_pos": 29, "end_pos": 85, "type": "TASK", "confidence": 0.7737263917922974}, {"text": "language adaptation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7224515974521637}]}, {"text": "For over two decades) the natural language processing community has been interested in automatic or semiautomatic methods which could be used to port systems from one domain or task to another, aiming at reducing at least in part the cost associated with the creation of human annotated datasets.", "labels": [], "entities": []}, {"text": "Automatic system adaptation can take different forms: if high * This work is partially supported by Ministerio de Econom\u00eda y Competitividad, Secretar\u00eda de Estado de Investigaci\u00f3n, Desarrollo e Innovaci\u00f3n, Spain under project number TIN2012-38584-C06-03 and Advanced Research Fellowship RYC-2009-04291.", "labels": [], "entities": [{"text": "system adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6947227567434311}, {"text": "Advanced Research Fellowship RYC-2009-04291", "start_pos": 257, "end_pos": 300, "type": "DATASET", "confidence": 0.6198213323950768}]}, {"text": "We thank Biljana Drndarevi\u00b4cDrndarevi\u00b4c for proofreading the paper.", "labels": [], "entities": []}, {"text": "quality human annotated data is available, then rulebased or statistical systems can be trained on this data, reducing the efforts of writing rules and handcrafting dictionaries.", "labels": [], "entities": []}, {"text": "If high quality human annotated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data.", "labels": [], "entities": []}, {"text": "Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications.", "labels": [], "entities": [{"text": "training information extraction or text summarization", "start_pos": 185, "end_pos": 238, "type": "TASK", "confidence": 0.6040560553471247}]}, {"text": "However, it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (.", "labels": [], "entities": []}, {"text": "We therefore here concentrate on learning template-like representations from concise event summaries which should contain the key information of an event.", "labels": [], "entities": []}, {"text": "18 de julio Un atentado contra la sede de la Asociaci\u00f3n Mutual Israelita Argentina Target de Buenos Aires PlaceOfAttack causa la muerte de 86 NumberOfVictims personas.", "labels": [], "entities": [{"text": "Target de Buenos Aires PlaceOfAttack causa", "start_pos": 83, "end_pos": 125, "type": "DATASET", "confidence": 0.8657883207003275}]}, {"text": "(18th July 1994. An attack against the headquarters of the Jewish Mutual Association in Buenos Aires, Argentina, kills 86 people.)", "labels": [], "entities": []}, {"text": "An example of the summaries we want to learn from is presented in.", "labels": [], "entities": []}, {"text": "It is a summary in the terrorist attack domain in Spanish.", "labels": [], "entities": []}, {"text": "It has been manually annotated with concepts such as DateOfAttack, Target, PlaceOfAttack, and NumberOfVictims, which are key in the domain.", "labels": [], "entities": []}, {"text": "Our task is to discover from this kind of summary what the concepts are and how to recognise them automatically.", "labels": [], "entities": []}, {"text": "As will be shown in this paper and unlike current approaches (), the methods to be presented here do not require parsing or semantic dictionaries to work or specification of the underlying number of concepts in the domain to be learn.", "labels": [], "entities": []}, {"text": "The approach we take learns concepts in the set of domain summaries, relying on noun phrase contextual information.", "labels": [], "entities": []}, {"text": "They are able to generate reasonable domain conceptualizations from relatively small datasets and in different languages.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: In Section 2 we overview related work in the area of concept induction from text.", "labels": [], "entities": [{"text": "concept induction", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7281099259853363}]}, {"text": "Next, in Section 3 we describe the dataset used and how we have processed it while in Section 4 we outline the two unsupervised learning algorithms we compare in this paper for template induction from text.", "labels": [], "entities": [{"text": "template induction from text", "start_pos": 177, "end_pos": 205, "type": "TASK", "confidence": 0.8164743632078171}]}, {"text": "Then, in Section 5, we describe the experiments on template induction indicating how we have instantiated the algorithms and in Section 6 we explain how we have extrinsically evaluated the induction process.", "labels": [], "entities": [{"text": "template induction", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7469644844532013}]}, {"text": "In Section 7 we discuss the obtained results and in Section 8 we summarize our findings and close the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments reported here we rely on the CONCISUS corpus 1 which is distributed free of charge.", "labels": [], "entities": [{"text": "CONCISUS corpus 1", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9209981759389242}]}, {"text": "It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish).", "labels": [], "entities": [{"text": "Terrorist Attacks", "start_pos": 223, "end_pos": 240, "type": "TASK", "confidence": 0.7534178793430328}]}, {"text": "The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into English, automatic translation of English summaries into Spanish, and associated original full documents in Spanish and English for two of the domains (Aviation Accidents and Earthquakes).", "labels": [], "entities": []}, {"text": "The dataset comes with human annotations representing the key information in each domain.", "labels": [], "entities": []}, {"text": "In we detail the concepts used in each of the domains.", "labels": [], "entities": []}, {"text": "Note that not all concepts are represented in each of the summaries.", "labels": [], "entities": []}, {"text": "Creation of such a dataset can take up to 500 hours fora human annotator, considering data collection, cleansing, and annotation proper.", "labels": [], "entities": [{"text": "data collection", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.775293231010437}]}, {"text": "Only one human annotator and one curator were responsible for the annotation process.", "labels": [], "entities": []}, {"text": "In this section we detail the different parameters used by the algorithms and report the performance of the induction process with different inputs.", "labels": [], "entities": []}, {"text": "We carryout a number of experiments per domain where we run the algorithms using as input the summaries annotated with a different chunk type each time.", "labels": [], "entities": []}, {"text": "After each experiment all concepts induced are   In we report baseline performance on the entire dataset.", "labels": [], "entities": []}, {"text": "As can be appreciated by the obtained numbers, directly mapping named entity types onto concepts does not provide a very good performance, especially for Spanish; we expected the learning procedures to produce better results.", "labels": [], "entities": []}, {"text": "In we present the results of inducing concepts from the gold chunks by the two algorithms.", "labels": [], "entities": []}, {"text": "In almost all cases, using gold chunks improves over the baseline procedure, except for the Terrorist Attack domain in English, where the iterative learning procedure underperforms the baseline.", "labels": [], "entities": []}, {"text": "In all tested domains, the clustering-based induction procedure has a very competitive performance.", "labels": [], "entities": [{"text": "clustering-based induction", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.851191520690918}]}, {"text": "A t-test is run to verify differences in performance between the two systems in terms of f-score.", "labels": [], "entities": []}, {"text": "In all tested domains in Spanish, except the Train Accident domain, there are sta-   tistically significant differences between the clustering procedure and the iterative learning procedure (p = 0.01).", "labels": [], "entities": [{"text": "Train Accident domain", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.7968862454096476}]}, {"text": "In all tested domains in English, except for the Earthquake domain, there are statistically significant differences between the performance of clustering and iterative learning (p = 0.01).", "labels": [], "entities": [{"text": "Earthquake domain", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9399314820766449}]}, {"text": "Now we turn to the results of both algorithms when automatic chunks are used, that is, when no human annotation is provided to the learners.", "labels": [], "entities": []}, {"text": "Results are reported in Tables 4 (Spanish) and 5 (English).", "labels": [], "entities": []}, {"text": "The results are presented by the chunk type used during the learning procedure.", "labels": [], "entities": []}, {"text": "In addition to the chunk types specified above, we include a type all, which represents the use of all automat-   ically computed chunks (i.e. nc, ne, wiki).", "labels": [], "entities": []}, {"text": "We observe that, in general, when presented with automatic chuks, the iterative learning procedure is able to induce concepts with a better f-score than the clustering-based algorithm.", "labels": [], "entities": []}, {"text": "A t-test is run to verify differences between the two induction procedures within each chunk condition (differences shown with a \u2020 in the tables).", "labels": [], "entities": []}, {"text": "In 11 out of 16 cases in Spanish and in 12 out of 16 cases in English, statistically significant differences are observed.", "labels": [], "entities": []}, {"text": "In three out of four domains the combination of automatic chunks outperforms the use of individual chunk types.", "labels": [], "entities": []}, {"text": "Generally, named entity chunks and wiki chunks have the lowest performance.", "labels": [], "entities": []}, {"text": "This is   not an unexpected result since named entities, for example, cover much fewer strings which may form part of a concept extension.", "labels": [], "entities": []}, {"text": "Additionally, off-theshelf entity recogizers only identify a limited number of entity types.", "labels": [], "entities": []}, {"text": "The numbers above are interesting because they provide intrinsic evaluation of the concept induction procedure, but they do not tell us much about their usability.", "labels": [], "entities": []}, {"text": "Therefore, and in order to better assess the value of the discovered concepts, we decided to carryout two extrinsic evaluations using an information extraction task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.7502070367336273}]}, {"text": "Once the conceps are induced and, as a result, the summaries are auto-annotated with domain specific concepts, we decide to train an off-the-shelf SVM token classification procedure and apply it to unseen human annotated documents.", "labels": [], "entities": [{"text": "SVM token classification", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.8315088550249735}]}, {"text": "The SVM classifier uses the same linguistic information as the induction procedures: token level information and a window size of 5 around each token to be classified.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Baseline Induction Performance", "labels": [], "entities": []}, {"text": " Table 3: Conceptual induction (Spanish and English) Using Gold", "labels": [], "entities": [{"text": "Conceptual induction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.760151594877243}]}, {"text": " Table 4: Comparison of conceptual induction in Spanish", "labels": [], "entities": []}, {"text": " Table 5: Comparison of conceptual induction in English", "labels": [], "entities": [{"text": "Comparison of conceptual induction", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6518547311425209}]}, {"text": " Table 6: Cross-lingual Information Extraction. System Trained with", "labels": [], "entities": [{"text": "Cross-lingual Information Extraction", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7466383576393127}]}, {"text": " Table 7: Cross-lingual Information Extraction Results in Spanish", "labels": [], "entities": [{"text": "Cross-lingual Information Extraction", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7516963879267374}]}, {"text": " Table 8: Cross-lingual Information Extraction Results in English", "labels": [], "entities": [{"text": "Cross-lingual Information Extraction", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7633219758669535}]}, {"text": " Table 9: Extraction from Full Documents. System Trained on Gold", "labels": [], "entities": []}, {"text": " Table 10: Full-text Information Extraction Results in Spanish. Sys-", "labels": [], "entities": [{"text": "Full-text Information Extraction", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.5625056624412537}]}, {"text": " Table 11: Full-text Information Extraction Results in English. Sys-", "labels": [], "entities": [{"text": "Full-text Information Extraction", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.5920683145523071}]}]}