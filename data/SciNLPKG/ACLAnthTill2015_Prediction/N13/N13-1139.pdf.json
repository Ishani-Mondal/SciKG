{"title": [{"text": "Optimal Data Set Selection: An Application to Grapheme-to-Phoneme Conversion", "labels": [], "entities": [{"text": "Grapheme-to-Phoneme Conversion", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.693998247385025}]}], "abstractContent": [{"text": "In this paper we introduce the task of unla-beled, optimal, data set selection.", "labels": [], "entities": []}, {"text": "Given a large pool of unlabeled examples, our goal is to select a small subset to label, which will yield a high performance supervised model over the entire data set.", "labels": [], "entities": []}, {"text": "Our first proposed method, based on the rank-revealing QR matrix factorization, selects a subset of words which span the entire word-space effectively.", "labels": [], "entities": []}, {"text": "For our second method, we develop the concept of feature coverage which we optimize with a greedy algorithm.", "labels": [], "entities": [{"text": "feature coverage", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.6935151666402817}]}, {"text": "We apply these methods to the task of grapheme-to-phoneme prediction.", "labels": [], "entities": [{"text": "grapheme-to-phoneme prediction", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6926963925361633}]}, {"text": "Experiments over a data-set of 8 languages show that in all scenarios, our selection methods are effective at yielding a small, but optimal set of labelled examples.", "labels": [], "entities": []}, {"text": "When fed into a state-of-the-art supervised model for grapheme-to-phoneme prediction, our methods yield average error reductions of 20% over randomly selected examples.", "labels": [], "entities": [{"text": "grapheme-to-phoneme prediction", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.7105738371610641}]}], "introductionContent": [{"text": "Over the last 15 years, supervised statistical learning has become the dominant paradigm for building natural language technologies.", "labels": [], "entities": []}, {"text": "While the accuracy of supervised models can be high, expertly annotated data sets exist fora small fraction of possible tasks, genres, and languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991897940635681}]}, {"text": "The would-be tool builder is thus often faced with the prospect of annotating data, using crowd-sourcing or domain experts.", "labels": [], "entities": []}, {"text": "With limited time and budget, the amount of data to be annotated might be small, especially in the prototyping stage, when the exact specification of the prediction task may still be in flux, and rapid prototypes are desired.", "labels": [], "entities": []}, {"text": "In this paper, we propose the problem of unsupervised, optimal data set selection.", "labels": [], "entities": []}, {"text": "Formally, given a large set X of n unlabeled examples, we must select a subset S \u2282 X of size kn to label.", "labels": [], "entities": []}, {"text": "Our goal is to select such a subset which, when labeled, will yield a high performance supervised model over the entire data set X . This task can bethought of as a zero-stage version of active learning: we must choose a single batch of examples to label, without the benefit of any prior labelled data points.", "labels": [], "entities": []}, {"text": "This problem definition avoids the practical complexity of the active learning set-up (many iterations of learning and labeling), and ensures that the labeled examples are not tied to one particular model class or task, a well-known danger of active learning.", "labels": [], "entities": []}, {"text": "Alternatively, our methods maybe used to create the initial seed set for the active learner.", "labels": [], "entities": []}, {"text": "Our initial testbed for optimal data set selection is the task of grapheme-to-phoneme conversion.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.7252888381481171}]}, {"text": "In this task, we are given an out-of-vocabulary word, with the goal of predicting a sequence of phonemes corresponding to its pronunciation.", "labels": [], "entities": []}, {"text": "As training data, we are given a pronunciation dictionary listing words alongside corresponding sequences of phones, representing canonical pronunciations of those words.", "labels": [], "entities": []}, {"text": "Such dictionaries are used as the final bridge between written and spoken language for technologies that span this divide, such as speech recognition, text-to-speech generation, and speech-to-speech language translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.7266279757022858}, {"text": "text-to-speech generation", "start_pos": 151, "end_pos": 176, "type": "TASK", "confidence": 0.7073117643594742}, {"text": "speech-to-speech language translation", "start_pos": 182, "end_pos": 219, "type": "TASK", "confidence": 0.676835964123408}]}, {"text": "These dictionaries are necessary: the pronunciation of words continues to evolve after their written form has been fixed, leading to a large number of rules and irregularities.", "labels": [], "entities": []}, {"text": "While large pronunciation dictionaries of over 100,000 words exist for several major languages, these resources are entirely lacking for the majority of the world's languages.", "labels": [], "entities": []}, {"text": "Our goal is to automatically select a small but optimal subset of words to be annotated with pronunciation data.", "labels": [], "entities": []}, {"text": "The main intuition behind our approach is that the subset of selected data points should efficiently cover the range of phenomena most commonly observed across the pool of unlabeled examples.", "labels": [], "entities": []}, {"text": "The first comes from a line of research initiated by the numerical linear algebra community and taken up by computer science theoreticians (, with the name COLUMN SUBSET SELECTION PROBLEM (CSSP).", "labels": [], "entities": []}, {"text": "Given a matrix A, the goal of CSSP is to select a subset of k columns whose span most closely captures the range of the full matrix.", "labels": [], "entities": []}, {"text": "In particular, the matrix\u02dcAmatrix\u02dc matrix\u02dcA formed by orthogonally projecting A onto the k-dimensional space spanned by the selected columns should be a good approximation to A.", "labels": [], "entities": []}, {"text": "By defining A T to be our data matrix, whose rows correspond to words and whose columns correspond to features (character 4-grams), we can apply the CSSP randomized algorithm of () on A to obtain a subset of k words which best span the entire space of words.", "labels": [], "entities": []}, {"text": "Our second approach is based on a notion of feature coverage.", "labels": [], "entities": [{"text": "feature coverage", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7136234939098358}]}, {"text": "We assume that the benefit of seeing a feature fin a selected word bears some positive relationship to the frequency off in the unlabeled pool.", "labels": [], "entities": []}, {"text": "However, we further assume that the lion's share of benefit accrues the first few times that we label a word with feature f , with the marginal utility quickly tapering off as more such examples have been labeled.", "labels": [], "entities": []}, {"text": "We formalize this notion and provide an exact greedy algorithm for selecting the k data points with maximal feature coverage.", "labels": [], "entities": []}, {"text": "To assess the benefit of these methods, we apply them to a suite of 8 languages with pronunciation dictionaries.", "labels": [], "entities": []}, {"text": "We consider ranges from 500 to 2000 selected words and train a start-of-the-art grapheme-to-phoneme prediction model.", "labels": [], "entities": []}, {"text": "Our experiments show that both methods produce significant improvements in prediction quality over randomly selected words, with our feature coverage method consistently outperforming the randomized CSSP algorithm.", "labels": [], "entities": []}, {"text": "Over the 8 languages, our method produces average reductions in error of 20%.", "labels": [], "entities": [{"text": "error", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.7815460562705994}]}], "datasetContent": [{"text": "To test the effectiveness of the two proposed data set selection methods, we conduct grapheme-tophoneme prediction experiments across a test suite of 8 languages: Dutch, English, French, Frisian, German, Italian, Norwegian, and Spanish.", "labels": [], "entities": [{"text": "grapheme-tophoneme prediction", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.6928484737873077}]}, {"text": "The data was obtained from the PASCAL Letter-to-Phoneme Conversion Challenge, and was processed to match the setup of.", "labels": [], "entities": [{"text": "PASCAL Letter-to-Phoneme Conversion Challenge", "start_pos": 31, "end_pos": 76, "type": "TASK", "confidence": 0.7195913344621658}]}, {"text": "The data comes from a range of sources, including CELEX for Dutch and German (, BRULEX for French (, CMUDict for English, 11 the Italian Festival Dictionary (), as well as pronunciation dictionaries for Spanish, Norwegian, and Frisian (original provenance not clear).", "labels": [], "entities": [{"text": "CELEX", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9285480380058289}, {"text": "BRULEX", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9982311129570007}, {"text": "Italian Festival Dictionary", "start_pos": 129, "end_pos": 156, "type": "DATASET", "confidence": 0.7088350057601929}]}, {"text": "As shows, the size of the dictionaries ranges from 31,491 words (Spanish) up to 116,211 words (Dutch).", "labels": [], "entities": []}, {"text": "We follow the PASCAL challenge training and test folds, treating the training set as our pool of words to be selected for labeling.", "labels": [], "entities": [{"text": "PASCAL challenge training", "start_pos": 14, "end_pos": 39, "type": "DATASET", "confidence": 0.5808068017164866}]}, {"text": "Results We consider training subsets of sizes 500, 1000, 1500, and 2000.", "labels": [], "entities": []}, {"text": "For our baseline, we train the G2P model) on randomly selected words of each size, and average the results over 10 runs.", "labels": [], "entities": []}, {"text": "We follow the same procedure for our two data set selection methods.", "labels": [], "entities": []}, {"text": "plots the word prediction accuracy for all three methods across the eight languages with varying training sizes, while provides corresponding numerical results.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7583618462085724}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.876188337802887}]}, {"text": "We see that in all scenarios the two data set selection strategies fare better than random subsets of words.", "labels": [], "entities": []}, {"text": "In all but one case, the feature coverage method yields the best performance (with the exception of Spanish trained with 500 words, where the CSSP yields the best results).", "labels": [], "entities": [{"text": "feature coverage", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.6254395842552185}]}, {"text": "Feature coverage achieves average error reduction of 20% over the randomly selected training words across the different languages and training set sizes.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 34, "end_pos": 49, "type": "METRIC", "confidence": 0.9532178938388824}]}, {"text": "Coverage variants We also experimented with the other versions of the feature coverage function discussed in Section 3.2 (see).", "labels": [], "entities": []}, {"text": "While cov 1 tended to perform quite poorly (usually worse than random), cov 2 -which gives full credit for each feature the first time it is seen -yields results just slightly worse than the CSSP matrix method on average, and always better than random.", "labels": [], "entities": []}, {"text": "In the 2000 word scenario, for example, cov 2 achieves average accuracy of 74.0, just a bit below the 74.4 accuracy of the CSSP method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.933809757232666}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9408513307571411}]}, {"text": "It is also possible that more    pare our three methods -random selections, CSSP, and feature coverage -all of which select k examples as a basis, against the lower bound given by SVD.", "labels": [], "entities": []}, {"text": "shows the result of this analysis fork = 2000 (Note that we were unable to compute the projection matrices for English and Dutch due to the size of the data and memory limitations).", "labels": [], "entities": []}, {"text": "As expected, SVD fares the best, with CSSP as a somewhat distant second.", "labels": [], "entities": [{"text": "SVD", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.8552243113517761}, {"text": "CSSP", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.875195324420929}]}, {"text": "On average, feature coverage seems to do a bit better than random.", "labels": [], "entities": [{"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.6232869029045105}]}, {"text": "A similar analysis for the feature coverage objective function is shown in.", "labels": [], "entities": []}, {"text": "Unsurprisingly, this objective is best optimized by the feature coverage method.", "labels": [], "entities": [{"text": "feature coverage", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.6350495666265488}]}, {"text": "Interestingly though, CSSP seems to perform about halfway between random and the feature coverage method.", "labels": [], "entities": [{"text": "feature coverage", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.6575574725866318}]}, {"text": "This makes some sense, as good basis data points will tend to have frequent features, while at the same time being maximally spread out from one another.", "labels": [], "entities": []}, {"text": "We also note that the poor coverage result for English in mirrors its overall poor performance in the G2P prediction task -not only are the phoneme labels unpredictable, but the input data itself is wild and hard to compress.", "labels": [], "entities": [{"text": "coverage", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9656297564506531}, {"text": "G2P prediction task", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7969489097595215}]}, {"text": "Stratified length sampling As shows, the top 10 words selected by the feature coverage method are mostly long and unusual, averaging 13.3 characters in length.", "labels": [], "entities": []}, {"text": "In light of the potential annotation burden, we developed a stratified sampling strategy to ensure typical word lengths.", "labels": [], "entities": []}, {"text": "Before selecting each new word, we first sample a word length according to the empirical word length distribution.", "labels": [], "entities": []}, {"text": "We then choose among words of the sampled length according to the feature coverage criterion.", "labels": [], "entities": []}, {"text": "This results in more typical words of average length, with only a very small drop in performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pronunciation dictionary size for each of the lan- guages.", "labels": [], "entities": [{"text": "Pronunciation dictionary size", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.9051193992296854}]}, {"text": " Table 2: Test word accuracy across the 8 languages for  randomly selected words (RAND), CSSP matrix subset  selection (CSSP), and Feature Coverage Maximization  (FEAT). We show results for 500 and 2000 word train- ing sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.914579451084137}, {"text": "CSSP matrix subset  selection (CSSP)", "start_pos": 89, "end_pos": 125, "type": "TASK", "confidence": 0.6350881116730827}, {"text": "Feature Coverage Maximization  (FEAT)", "start_pos": 131, "end_pos": 168, "type": "TASK", "confidence": 0.6682052314281464}]}, {"text": " Table 3: Residual matrix norm across 6 languages for  randomly selected words (RAND), CSSP matrix subset  selection (CSSP), feature coverage maximization (FEAT),  and the rank k SVD (SVD). Lower is better.", "labels": [], "entities": [{"text": "CSSP matrix subset  selection", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.5886890441179276}, {"text": "feature coverage maximization (FEAT)", "start_pos": 125, "end_pos": 161, "type": "METRIC", "confidence": 0.5503652542829514}]}, {"text": " Table 4: Feature coverage across the 8 languages for ran- domly selected words (RAND), CSSP matrix subset selec- tion (CSSP), and feature coverage maximization (FEAT).  Higher is better.", "labels": [], "entities": [{"text": "FEAT", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.6277784109115601}]}]}