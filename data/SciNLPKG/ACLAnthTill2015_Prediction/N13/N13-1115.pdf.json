{"title": [], "abstractContent": [{"text": "This paper examines tuning for statistical machine translation (SMT) with respect to multiple evaluation metrics.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.8088380098342896}]}, {"text": "We propose several novel methods for tuning towards multiple objectives , including some based on ensemble decoding methods.", "labels": [], "entities": []}, {"text": "Pareto-optimality is a natural way to think about multi-metric optimization (MMO) and our methods can effectively combine several Pareto-optimal solutions, obviating the need to choose one.", "labels": [], "entities": [{"text": "multi-metric optimization (MMO)", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8402177095413208}]}, {"text": "Our best performing ensemble tuning method is anew algorithm for multi-metric optimization that searches for Pareto-optimal ensemble models.", "labels": [], "entities": []}, {"text": "We study the effectiveness of our methods through experiments on multiple as well as single reference(s) datasets.", "labels": [], "entities": []}, {"text": "Our experiments show simultaneous gains across several met-rics (BLEU, RIBES), without any significant reduction in other metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9972780346870422}, {"text": "RIBES", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9713601469993591}]}, {"text": "This contrasts the traditional tuning where gains are usually limited to a single metric.", "labels": [], "entities": []}, {"text": "Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9922516942024231}]}], "introductionContent": [{"text": "Tuning algorithms are used to find the weights fora statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.7693354388078054}]}, {"text": "The tuning process improves the performance of an SMT system as measured by this metric; with BLEU () being the most popular choice.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.994929850101471}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9991494417190552}]}, {"text": "Minimum error-rate training (MERT) was the first approach in MT to directly optimize an evaluation metric.", "labels": [], "entities": [{"text": "Minimum error-rate training (MERT)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6443424125512441}, {"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9957013726234436}]}, {"text": "Several alternatives now exist: MIRA (,), linear regression ( and ORO (Watanabe, 2012) among others.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9123554229736328}, {"text": "ORO", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9974314570426941}]}, {"text": "However these approaches optimize towards the best score as reported by a single evaluation metric.", "labels": [], "entities": []}, {"text": "MT system developers typically use BLEU and ignore all the other metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9453461766242981}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9982059001922607}]}, {"text": "This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference), to capturing lexical choices in translation as in METEOR) to modelling semantic similarity through textual entailment to RIBES, an evaluation metric that pays attention to long-distance reordering (.", "labels": [], "entities": [{"text": "translation edit rate (TER)", "start_pos": 111, "end_pos": 138, "type": "METRIC", "confidence": 0.7928432077169418}]}, {"text": "While some of these metrics such as TER, ME-TEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics).", "labels": [], "entities": [{"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9964144229888916}, {"text": "ME-TEOR", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9009791612625122}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.997142493724823}, {"text": "BLEU", "start_pos": 208, "end_pos": 212, "type": "METRIC", "confidence": 0.9655919671058655}]}, {"text": "The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics, suggesting that over-fitting to a specific metric might happen without improvements in translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.962525486946106}]}, {"text": "In this paper we propose anew tuning framework for jointly optimizing multiple evaluation metrics.", "labels": [], "entities": []}, {"text": "Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach.", "labels": [], "entities": [{"text": "multi-metric optimization (MMO)", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.7732451677322387}]}, {"text": "PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics.", "labels": [], "entities": [{"text": "PMO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8407312631607056}, {"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9403945207595825}]}, {"text": "In () the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (.", "labels": [], "entities": []}, {"text": "In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding ( ).", "labels": [], "entities": []}, {"text": "We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combine this with the PMO ap-proach ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.8940424919128418}, {"text": "PMO ap-proach", "start_pos": 136, "end_pos": 149, "type": "METRIC", "confidence": 0.7025355994701385}]}, {"text": "We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning.", "labels": [], "entities": []}, {"text": "Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now.", "labels": [], "entities": []}, {"text": "Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task.", "labels": [], "entities": []}, {"text": "HTER) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9905210733413696}]}], "datasetContent": [{"text": "We evaluate the different methods on ArabicEnglish translation in single as well as multiple references scenario.", "labels": [], "entities": [{"text": "ArabicEnglish translation", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7611102759838104}]}, {"text": "Corpus statistics are shown in.", "labels": [], "entities": [{"text": "Corpus", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7955927848815918}]}, {"text": "For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased) (Hiero) system, and integrated the required changes for ensemble decoding.", "labels": [], "entities": []}, {"text": "Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets . We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO () for optimizing meta weights, wherever applicable.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6820107102394104}]}, {"text": "In both cases, we use SVMRank) as the optimizer.", "labels": [], "entities": []}, {"text": "We used the default parameter settings for different MT tuning metrics.", "labels": [], "entities": [{"text": "MT tuning", "start_pos": 53, "end_pos": 62, "type": "TASK", "confidence": 0.8946188986301422}]}, {"text": "For METEOR, we tried both METEOR-tune and METEOR-hter settings and found the latter to perform better in BLEU and TER scores, even though the former was marginally better in METEOR and RIBES scores.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.6947740912437439}, {"text": "METEOR-tune", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.5303537845611572}, {"text": "METEOR-hter", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.748071551322937}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9992998838424683}, {"text": "TER scores", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9651688933372498}, {"text": "METEOR", "start_pos": 174, "end_pos": 180, "type": "DATASET", "confidence": 0.5557959079742432}, {"text": "RIBES", "start_pos": 185, "end_pos": 190, "type": "METRIC", "confidence": 0.8632346391677856}]}, {"text": "We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization and evaluation of all our experiments.", "labels": [], "entities": [{"text": "margin of loss", "start_pos": 16, "end_pos": 30, "type": "METRIC", "confidence": 0.969903826713562}, {"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9924647808074951}, {"text": "TER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9925399422645569}, {"text": "METEOR", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9120676517486572}, {"text": "RIBES", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.975050687789917}, {"text": "METEOR-hter", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.499761164188385}]}, {"text": "Unlike conventional tuning methods, PMO) was originally evaluated on the tuning set to avoid potential mismatch with the test set.", "labels": [], "entities": [{"text": "PMO", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.6619627475738525}]}, {"text": "In order to ensure robustness of evaluation, they redecode the devset using the optimal weights from the last tuning iteration and report the scores on 1-best candidates.", "labels": [], "entities": []}, {"text": "This section contains multi-metric optimization results on the unseen test sets, one test set has multiple references and the other has a single-reference.", "labels": [], "entities": []}, {"text": "We plot BLEU scores against other metrics (RIBES, METEOR and TER) and this allows us to compare the performance of each metric relative to the defacto standard BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.995564877986908}, {"text": "RIBES", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9520530700683594}, {"text": "METEOR", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9896339178085327}, {"text": "TER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.994407057762146}, {"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.969710111618042}]}, {"text": "Baseline points are identified by single letters B for BLEU, T for TER, etc. and the baseline (singlemetric optimized) score for each metric is indicated by a dashed line on the corresponding axis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9973276853561401}, {"text": "TER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9924191236495972}]}, {"text": "MMO points use a series of single letters referring to the metrics used, e.g. BT for BLEU-TER.", "labels": [], "entities": [{"text": "MMO", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8381460309028625}, {"text": "BT", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.7094197273254395}, {"text": "BLEU-TER", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9787870049476624}]}, {"text": "The union of metrics method is identified with the suffix 'J' and lateen method with suffix 'L' (thus BT-L refers to the lateen tuning with BLEU-TER).", "labels": [], "entities": [{"text": "lateen", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9825387001037598}, {"text": "BLEU-TER", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9966631531715393}]}, {"text": "MMO points without any suffix use the ensemble tuning approach.", "labels": [], "entities": []}, {"text": "plot the scores for the MTA test set with 4-references.", "labels": [], "entities": [{"text": "MTA test set", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.8225070039431254}]}, {"text": "We see noticeable and some statistically significant improvements in BLEU and RIBES (see for BLEU improvements).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9967315196990967}, {"text": "RIBES", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9881502389907837}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9962660670280457}]}, {"text": "All our MMO approaches, except for the union method, show gains on both BLEU and RIBES axes.", "labels": [], "entities": [{"text": "MMO", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9793627262115479}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.998815655708313}, {"text": "RIBES", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9405409693717957}]}, {"text": "show that none of the proposed methods managed to improve the baseline scores for METEOR and TER.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9019774794578552}, {"text": "TER", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9881321787834167}]}, {"text": "However, several of our ensemble tuning combinations work well for both ME-TEOR (BR, BMRTB3, etc.) and TER (BMRT and BRT) in that they improved or were close to the baseline scores in either dimension.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9498342275619507}, {"text": "BR", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.90898597240448}, {"text": "BMRTB3", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.6161327958106995}, {"text": "TER", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9959813356399536}]}, {"text": "We again see in these figures that the MMO approaches can improve the BLEU-only tuning by 0.3 BLEU points, without much drop in other metrics.", "labels": [], "entities": [{"text": "MMO", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9343117475509644}, {"text": "BLEU-only tuning", "start_pos": 70, "end_pos": 86, "type": "METRIC", "confidence": 0.9581774175167084}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9969642758369446}]}, {"text": "This is in tune with the finding that BLEU could be tuned easily) and also explains why it remains  a popular choice for optimizing SMT systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9952232241630554}, {"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9552148580551147}]}, {"text": "Among the different MMO methods the ensemble tuning performs better than lateen or union approaches.", "labels": [], "entities": [{"text": "MMO", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9764952659606934}]}, {"text": "In terms of the number of metrics being optimized jointly, we see substantial gains when using a small number (typically 2 or 3) of metrics.", "labels": [], "entities": []}, {"text": "Results seem to suffer beyond this number; probably because there might not be a space that contain solution(s) optimal for all the metrics that are jointly optimized.", "labels": [], "entities": []}, {"text": "We hypothesize that each metric correlates well (in a looser sense) with few others, but not all.", "labels": [], "entities": []}, {"text": "For example, union optimizations BR-J and BMT-J perform close to or better than RIBES and TER baselines, but get very poor score in METEOR.", "labels": [], "entities": [{"text": "BR-J", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.7713026404380798}, {"text": "BMT-J", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.6519225835800171}, {"text": "RIBES", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.678613543510437}, {"text": "TER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9532879590988159}, {"text": "METEOR", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.6590089797973633}]}, {"text": "On the other hand BM-J is close to the METEOR baseline, while doing poorly on the RIBES and TER.", "labels": [], "entities": [{"text": "BM-J", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.6134093999862671}, {"text": "METEOR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9669180512428284}, {"text": "RIBES", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.7220447063446045}, {"text": "TER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9532198309898376}]}, {"text": "This behaviour is also evident from the single-metric baselines, where Rand T-only settings are clearly distinguished from the M-only system.", "labels": [], "entities": []}, {"text": "It is not clear if such distinct classes of metrics could be bridged by some optimal solution and the metric dichotomy requires further study as this is key to practical multimetric tuning in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 192, "end_pos": 195, "type": "TASK", "confidence": 0.9913860559463501}]}, {"text": "The lateen and union approaches appear to be very sensitive to the number of metrics and they generally perform well for two metrics case and show degradation for more metrics.", "labels": [], "entities": []}, {"text": "Unlike other approaches, the union approach failed to improve over the baseline BLEU and this could be attributed to the conflict of interest among the metrics, while choosing example points for the optimization step.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9949648380279541}]}, {"text": "The positive example preferred by a particular metric could be a negative example for the other metric.", "labels": [], "entities": []}, {"text": "This would only confuse the optimizer resulting in poor solutions.", "labels": [], "entities": []}, {"text": "Our future line of work would be to study the effect of avoiding such of conflicting examples in the union approach.", "labels": [], "entities": []}, {"text": "For the single-reference (ISI) dataset, we only plot the BLEU-TER case in(b) due to lack of space.", "labels": [], "entities": [{"text": "ISI) dataset", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.5908701916535696}, {"text": "BLEU-TER", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.999099612236023}]}, {"text": "The results are similar to the multiple references set indicating that MMO approaches are equally effective for single references .  shows the BLEU scores for our ensemble tuning method (for various combinations) and we again see improvements over the baseline BLEU-only tuning.", "labels": [], "entities": [{"text": "MMO", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9449582695960999}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9991075396537781}, {"text": "BLEU-only", "start_pos": 261, "end_pos": 270, "type": "METRIC", "confidence": 0.9865567088127136}]}, {"text": "So far we have shown that multi-metric optimization can improve over single-metric tuning on a single metric like BLEU and we have shown that our methods find a tuned model that performs well with respect to multiple metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9942712783813477}]}, {"text": "Is the output that scores higher on multiple metrics actually a better translation?", "labels": [], "entities": []}, {"text": "To verify this, we conducted a post-editing human evaluation experiment.", "labels": [], "entities": []}, {"text": "We compared our ensemble tuning approach involving BLEU, METEOR and RIBES (B-M-R) with systems optimized for BLEU (B-only) and METEOR (M-only).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9905200004577637}, {"text": "METEOR", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.48298266530036926}, {"text": "RIBES", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9776700139045715}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9953444600105286}]}, {"text": "We selected 100 random sentences (that are at least 15 words long) from the Arabic-English MTA (4 references) test set and translated them using the three systems (two single metric systems and BMR ensemble tuning).", "labels": [], "entities": [{"text": "MTA (4 references) test set", "start_pos": 91, "end_pos": 118, "type": "DATASET", "confidence": 0.67493907894407}]}, {"text": "We shuffled the resulting translations and split them into 3 sets such that each set has equal number of the translations from three systems.", "labels": [], "entities": []}, {"text": "The translations were edited by three human annotators in a post-editing setup, where the goal was to edit the translations to make them as close to the references as possible, using the Post-Editing Tool: PET ().", "labels": [], "entities": []}, {"text": "The annotators were not Arabic-literate and relied only on the reference translations during post-editing.", "labels": [], "entities": []}, {"text": "The identifiers that link each translation to the system that generated it are removed to avoid annotator bias.", "labels": [], "entities": []}, {"text": "In the end we collated post-edited translations for each system and then computed the system-level erence sentence.", "labels": [], "entities": []}, {"text": "Our experiment shows that even with a single reference MMO methods can work.", "labels": [], "entities": [{"text": "MMO", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9594241380691528}]}, {"text": "human-targeted (HBLEU, HMETEOR, HTER) scores, by using respective post-edited translations as the reference.", "labels": [], "entities": []}, {"text": "First comparing the HTER () scores shown in, we see that the single-metric system optimized for ME-TEOR performs slightly worse than the one optimized for BLEU, despite using METEOR-hter version).", "labels": [], "entities": [{"text": "HTER", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9408720135688782}, {"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9829727411270142}]}, {"text": "Ensemble tuning-based system optimized for three metrics (B-M-R) improves HTER by 4% and 6.3% over BLEU and METEOR optimized systems respectively.", "labels": [], "entities": [{"text": "B-M-R", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9564704298973083}, {"text": "HTER", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9854560494422913}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9938867688179016}]}, {"text": "The single-metric system tuned with M-only setting scores high on HBLEU, closely followed by the ensemble system.", "labels": [], "entities": [{"text": "HBLEU", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8400627374649048}]}, {"text": "We believe this to be caused by chance rather than any systematic gains by the Monly tuning; the ensemble system scores high on HMETEOR compared to the M-only system.", "labels": [], "entities": [{"text": "HMETEOR", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.5786114931106567}]}, {"text": "While HTER captures the edit distance to the targeted reference, HMETEOR and HBLEU metrics capture missing content words or synonyms by exploiting n-grams and paraphrase matching.", "labels": [], "entities": [{"text": "paraphrase matching", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7302192747592926}]}, {"text": "We also computed the regular variants (BLEU, METEOR and TER), which are scored against original references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9991231560707092}, {"text": "METEOR", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.995360791683197}, {"text": "TER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.995425283908844}]}, {"text": "The ensemble system outperformed the single-metric systems in all the three metrics.", "labels": [], "entities": []}, {"text": "The improvements were also statistically significant at p-value of 0.05 for BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9967247843742371}, {"text": "TER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9479023814201355}]}], "tableCaptions": [{"text": " Table 2: BLEU Scores on MTA (4 refs) and ISI (1 ref) test sets  using the standard mteval script. Boldface scores indicate scores  that are comparable to or better than the baseline BLEU-only  tuning. Italicized scores indicate statistically significant differ- ences at p-value 0.05 computed with bootstrap significance test.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9969192743301392}, {"text": "MTA", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.8321796655654907}, {"text": "BLEU-only", "start_pos": 183, "end_pos": 192, "type": "METRIC", "confidence": 0.9867103695869446}]}, {"text": " Table 3: Post-editing Human Evaluation: Regular (untargeted)  and human-targeted scores. Human targeted scores are com- puted against the post-edited reference and regular scores are  computed with the original references. Best scores are in bold- face and statistically significant ones (at p = 0.05) are italicized.", "labels": [], "entities": []}]}