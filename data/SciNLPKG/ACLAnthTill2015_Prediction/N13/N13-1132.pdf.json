{"title": [{"text": "Learning Whom to Trust with MACE", "labels": [], "entities": [{"text": "MACE", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.5589640140533447}]}], "abstractContent": [{"text": "Non-expert annotation services like Amazon's Mechanical Turk (AMT) are cheap and fast ways to evaluate systems and provide categorical annotations for training data.", "labels": [], "entities": []}, {"text": "Unfortunately , some annotators choose bad labels in order to maximize their pay.", "labels": [], "entities": []}, {"text": "Manual identification is tedious, so we experiment with an item-response model.", "labels": [], "entities": [{"text": "Manual identification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.698132336139679}]}, {"text": "It learns in an un-supervised fashion to a) identify which an-notators are trustworthy and b) predict the correct underlying labels.", "labels": [], "entities": []}, {"text": "We match performance of more complex state-of-the-art systems and perform well even under adversarial conditions.", "labels": [], "entities": []}, {"text": "We show considerable improvements over standard baselines, both for predicted label accuracy and trustworthiness estimates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9488223791122437}]}, {"text": "The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference.", "labels": [], "entities": []}, {"text": "Additionally , we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9989896416664124}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9808651804924011}]}, {"text": "Our system, MACE (Multi-Annotator Competence Estimation), is available for download 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Amazon's MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP).", "labels": [], "entities": [{"text": "Amazon's MechanicalTurk (AMT)", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.8759364287058512}]}, {"text": "However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label.", "labels": [], "entities": []}, {"text": "We refer to Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer.", "labels": [], "entities": []}, {"text": "In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label.", "labels": [], "entities": []}, {"text": "The simplest approach is majority voting, which weights all answers equally.", "labels": [], "entities": []}, {"text": "Unfortunately, it is easy for majority voting to go wrong.", "labels": [], "entities": []}, {"text": "A common and simple spammer strategy for categorical labeling tasks is to always choose the same (often the first) label.", "labels": [], "entities": []}, {"text": "When multiple spammers follow this strategy, the majority can be incorrect.", "labels": [], "entities": []}, {"text": "While this specific scenario might seem simple to correct for (remove annotators that always produce the same label), the situation grows more tricky when spammers do not annotate consistently, but instead choose labels at random.", "labels": [], "entities": []}, {"text": "A more sophisticated approach than simple majority voting is required.", "labels": [], "entities": []}, {"text": "If we knew whom to trust, and when, we could reconstruct the correct labels.", "labels": [], "entities": []}, {"text": "Yet, the only way to be sure we know whom to trust is if we knew the correct labels ahead of time.", "labels": [], "entities": []}, {"text": "To address this circular problem, we build a generative model of the annotation process that treats the correct labels as latent variables.", "labels": [], "entities": []}, {"text": "We then use unsupervised learning to estimate parameters directly from redundant annotations.", "labels": [], "entities": []}, {"text": "This is a common approach in the class of unsupervised models called item-response models (.", "labels": [], "entities": []}, {"text": "While such models have been implemented in other fields (e.g., vision), we are not aware of their availability for NLP tasks (see also Section 6).", "labels": [], "entities": []}, {"text": "Our model includes a binary latent variable that explicitly encodes if and when each annotator is spamming, as well as parameters that model the annotator's specific spamming \"strategy\".", "labels": [], "entities": []}, {"text": "Impor-tantly, the model assumes that labels produced by an annotator when spamming are independent of the true label (though, a spammer can still produce the correct label by chance).", "labels": [], "entities": []}, {"text": "In experiments, our model effectively differentiates dutiful annotators from spammers (Section 4), and is able to reconstruct the correct label with high accuracy (Section 5), even under extremely adversarial conditions (Section 5.2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9959099292755127}]}, {"text": "It does not require any annotated instances, but is capable of including varying levels of supervision via token constraints (Section 5.2).", "labels": [], "entities": []}, {"text": "We consistently outperform majority voting, and achieve performance equal to that of more complex state-of-the-art models.", "labels": [], "entities": []}, {"text": "Additionally, we find that thresholding based on the posterior label entropy can be used to trade off coverage for accuracy in label reconstruction, giving considerable gains (Section 5.1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9981940388679504}, {"text": "label reconstruction", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.7842491865158081}]}, {"text": "In tasks where correct answers are more important than answering every instance, e.g., when constructing anew annotated corpus, this feature is extremely valuable.", "labels": [], "entities": []}, {"text": "Our contributions are: \u2022 We demonstrate the effectiveness of our model on real world AMT datasets, matching the accuracy of more complex state-of-the-art systems \u2022 We show how posterior entropy can be used to trade some coverage for considerable gains inaccuracy \u2022 We study how various factors affect performance, including number of annotators, annotator strategy, and available supervision \u2022 We provide MACE (Multi-Annotator Competence Estimation), a Java-based implementation of a simple and scalable unsupervised model that identifies malicious annotators and predicts labels with high accuracy", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9881866574287415}]}], "datasetContent": [{"text": "We evaluate our method on existing annotated datasets from various AMT tasks.", "labels": [], "entities": [{"text": "AMT tasks", "start_pos": 67, "end_pos": 76, "type": "TASK", "confidence": 0.9330610632896423}]}, {"text": "However, we also want to ensure that our model can handle adversarial conditions.", "labels": [], "entities": []}, {"text": "Since we have no control over the factors in existing datasets, we create synthetic data for this purpose.", "labels": [], "entities": []}, {"text": "First, we want to know which annotators to trust.", "labels": [], "entities": []}, {"text": "We evaluate whether our model's learned trustworthiness parameters \u03b8 j can be used to identify these individuals (Section 4).", "labels": [], "entities": []}, {"text": "We then compare the label predicted by our model and by majority voting to the correct label.", "labels": [], "entities": []}, {"text": "The results are reported as accuracy (Section 5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9998315572738647}]}, {"text": "Since our model computes posterior entropies for each instance, we can use this as an approximation for the model's confidence in the prediction.", "labels": [], "entities": []}, {"text": "If we focus on predictions with high confidence (i.e., low entropy), we hope to see better accuracy, even at the price of leaving some items unanswered.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9987552165985107}]}, {"text": "We evaluate this trade-off in Section 5.1.", "labels": [], "entities": []}, {"text": "In addition, we investigate the influence of the number of spammers and their strategy on the accuracy of our model (Section 5.2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9992187023162842}]}], "tableCaptions": [{"text": " Table 2: Accuracy of different methods on data sets  from (Snow et al., 2008). MACE-VB uses Varia- tional Bayes training. Results @n use the n% items  the model is most confident in (Section 5.1). Results  below double line trade coverage for accuracy and  are thus not comparable to upper half.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9764007925987244}, {"text": "accuracy", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.9993533492088318}]}]}