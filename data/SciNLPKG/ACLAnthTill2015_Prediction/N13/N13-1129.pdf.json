{"title": [{"text": "A method for the approximation of incremental understanding of explicit utterance meaning using predictive models infinite domains", "labels": [], "entities": [{"text": "approximation of incremental understanding of explicit utterance meaning", "start_pos": 17, "end_pos": 89, "type": "TASK", "confidence": 0.7842127755284309}]}], "abstractContent": [{"text": "This paper explores the relationship between explicit and predictive models of incremental speech understanding in a dialogue system that supports a finite set of user utterance meanings.", "labels": [], "entities": []}, {"text": "We present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain.", "labels": [], "entities": [{"text": "approximation of explicit understanding", "start_pos": 37, "end_pos": 76, "type": "TASK", "confidence": 0.6749227344989777}]}, {"text": "We show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches .", "labels": [], "entities": [{"text": "corpus evaluation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.6825159788131714}]}], "introductionContent": [{"text": "In recent years, there has been a growing interest among researchers in methods for incremental natural language understanding (NLU) for spoken dialogue systems; see e.g. (. This work has generally been motivated by a desire to make dialogue systems more efficient and more natural, by enabling them to provide lower latency responses, human-like feedback such as backchannels that indicate how well the system is understanding user speech, and more interactive response capabilities such as collaborative completions of user utterances), more adaptive handling of interruptions (, and others.", "labels": [], "entities": [{"text": "incremental natural language understanding (NLU)", "start_pos": 84, "end_pos": 132, "type": "TASK", "confidence": 0.82614723273686}]}, {"text": "This paper builds on techniques developed in previous work that has adopted a predictive approach to incremental NLU).", "labels": [], "entities": []}, {"text": "On this approach, at specific moments while a user's speech is in progress, an attempt is made to predict what the full meaning of the complete user utterance will be.", "labels": [], "entities": []}, {"text": "Predictive models can be contrasted with explicit approaches to incremental NLU.", "labels": [], "entities": []}, {"text": "We use the term explicit understanding to refer to approaches that attempt to determine the meaning that has been expressed explicitly in the user's partial utterance so far (without predicting further aspects of meaning to come).", "labels": [], "entities": []}, {"text": "Explicit understanding of partial utterances can be implemented using statistical classification or sequential tagging models (.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.6997297257184982}, {"text": "sequential tagging", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.6726944446563721}]}, {"text": "Both predictive and explicit incremental NLU capabilities can be valuable in a dialogue system.", "labels": [], "entities": []}, {"text": "Prediction can support specific response capabilities, such as system completion of user utterances and reduced response latency.", "labels": [], "entities": []}, {"text": "However, explicit models support additional and complementary capabilities.", "labels": [], "entities": []}, {"text": "For instance, depending on the application domain () and on the individual utterance, it maybe difficult fora system to predict a user's impending meaning with confidence.", "labels": [], "entities": [{"text": "predict a user's impending meaning", "start_pos": 120, "end_pos": 154, "type": "TASK", "confidence": 0.6280635992685953}]}, {"text": "Nevertheless, it may often be possible for systems to determine the meaning of what a user has said so far, and to take action based on this partial understanding.", "labels": [], "entities": []}, {"text": "As one example, items in a user interface could be highlighted when mentioned by a user ().", "labels": [], "entities": []}, {"text": "Another capability would be to provide grounding feedback, such as verbal back-channels or head nods (in embodied systems), to indicate when the system is understanding the user's meaning.", "labels": [], "entities": []}, {"text": "Explicit utterance meanings also allow a system to distinguish between meaning that has been expressed and meaning that is merely implied or inferred, which maybe less reliable.", "labels": [], "entities": []}, {"text": "In the near future, as incremental processing capabilities in dialogue systems grow, it may prove valuable for dialogue systems to combine both predictive and explicit incremental understanding capabilities.", "labels": [], "entities": []}, {"text": "In this paper, we present a technique for approximating a user's explicit meaning using an existing predictive understanding framework).", "labels": [], "entities": []}, {"text": "The specific new contributions in this paper are (1) to show that an estimate of a user's explicit utterance meaning can be derived from this kind of predictive understanding model (Section 2); (2) to quantify the performance of this new method in a corpus evaluation (Section 3); (3) to provide concrete examples and discussion of the annotation costs associated with implementing this technique, in relation to some alternative approaches to explicit understanding (Section 4).", "labels": [], "entities": []}, {"text": "Our results and discussion show that the proposed method offers promising performance, has relatively low annotation costs, and enables explicit and predictive understanding to be easily combined within a dialogue system.", "labels": [], "entities": []}, {"text": "It may therefore be a useful incremental understanding technique for some dialogue systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate this technique, we constructed subsets of frame elements or \"explicit subframes\" using Equation (2) and various minimum probability thresholds \u03c4 for partial ASR results in our test set.", "labels": [], "entities": [{"text": "Equation", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9587555527687073}, {"text": "ASR", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.9498151540756226}]}, {"text": "We then compared the resulting subframes both to the final complete frame G u for each utterance u, and also to manually annotated sub- Figure 2: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of hello elder.", "labels": [], "entities": [{"text": "ASR", "start_pos": 211, "end_pos": 214, "type": "TASK", "confidence": 0.9129467010498047}]}, {"text": "frames that represent human judgments of explicit incremental utterance meaning.", "labels": [], "entities": []}, {"text": "To collect these judgments, we hand-annotated a wordmeaning alignment for 50 random utterances in our test set.", "labels": [], "entities": []}, {"text": "To perform this annotation, successively larger prefixes of each utterance transcript were mapped to successively larger subframes of the full frame for the complete utterance.", "labels": [], "entities": []}, {"text": "The annotated subframes for each utterance prefix were selected to be explicit; they include only those frame elements that are explicitly expressed in the corresponding prefix of the user's utterance.", "labels": [], "entities": []}, {"text": "(We discuss the cost of this annotation in Section 4.)", "labels": [], "entities": []}, {"text": "We provide a simple concrete example in.", "labels": [], "entities": []}, {"text": "This example shows two partial ASR results during an utterance of hello elder by a user.", "labels": [], "entities": [{"text": "ASR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9822434782981873}]}, {"text": "For each partial ASR result, three frames are indicated horizontally.", "labels": [], "entities": [{"text": "ASR", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9558611512184143}]}, {"text": "At the right, labeled \"Annotated subframe\", we show the human judgment of explicit incremental utterance meaning for this partial utterance.", "labels": [], "entities": []}, {"text": "Our human judge has indicated that the word hello corresponds to the frame element <S>.sem.speechact.type greeting, and that the words hello elder correspond to an expanded frame that includes the frame element <S>.addressee elder-al-hassan.", "labels": [], "entities": []}, {"text": "At the left, labeled \"Explicit subframe\", we show the subframe selected by Equation (2) for each partial ASR result, with threshold \u03c4 = 0.5.", "labels": [], "entities": [{"text": "Equation (2)", "start_pos": 75, "end_pos": 87, "type": "METRIC", "confidence": 0.9472558051347733}, {"text": "ASR", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.890453040599823}]}, {"text": "A relevant background fact for this example is that in this scenario, the user can generally address either of two virtual humans who are present, Doctor Perez or Elder Al-Hassan.", "labels": [], "entities": []}, {"text": "After the user has said hello, the frame element <S>.sem.speechact.type greeting is assigned probability 0.813 by Equation (1), and only this frame element appears in the explicit subframe.", "labels": [], "entities": []}, {"text": "In the middle, labeled \"Predicted complete frame\", the figure also shows the full predicted frame from mxNLU at each point.", "labels": [], "entities": [{"text": "Predicted complete frame", "start_pos": 24, "end_pos": 48, "type": "METRIC", "confidence": 0.9195995330810547}]}, {"text": "After the user has said hello, the full predicted output includes an additional frame element, <S>.addressee doctor-perez, indicating a prediction that the addressee of this user utterance will be Doctor Perez rather than Elder al-Hassan.", "labels": [], "entities": []}, {"text": "However, the 2 Note that no utterances in our training set were annotated.", "labels": [], "entities": []}, {"text": "probability assigned to this prediction by Equation (1) is less than 0.5, and so this predicted frame element is excluded from the explicit subframe.", "labels": [], "entities": [{"text": "Equation", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9794763922691345}]}, {"text": "And indeed, this is the correct explicit representation of the meaning of hello in this system.", "labels": [], "entities": []}, {"text": "This simple example illustrates how our proposed technique can enable a dialogue system to have access to both explicit and predicted utterance meaning as a user's utterance progresses.", "labels": [], "entities": []}, {"text": "An excerpt from a more complex utterance is given in.", "labels": [], "entities": []}, {"text": "This example shows incremental outputs for two partial ASR results during a user utterance of we will provide transportation at no cost.", "labels": [], "entities": [{"text": "ASR", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9792192578315735}]}, {"text": "In this example, the explicit subframe for we will includes frame elements that convey that the captain (i.e. the user) is promising to do something.", "labels": [], "entities": []}, {"text": "This subframe does not exactly match the human judgment of explicit meaning at the right, which does not include at this point the <S>.sem.agent captain-kirk and <S>.sem.type event frame elements.", "labels": [], "entities": []}, {"text": "However, the explicit subframe more closely matches the human judgment than does the predicted complete frame from mxNLU (middle column), which includes an incorrect prediction that the captain is promising to deliver medical supplies (represented by the key values <S>.sem.event deliver and <S>.sem.theme medical-supplies).", "labels": [], "entities": []}, {"text": "For the next partial ASR result shown in the figure, the explicit subframe correctly adds several additional frame elements which formalize the meaning of the phrase provide transportation in this scenario as having the army move the clinic out of the market area.", "labels": [], "entities": [{"text": "ASR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9495314359664917}]}, {"text": "To understand more quantitatively how well this technique works, we evaluated this technique in the SASO-EN test corpus, using different probability thresholds in the range [0.5,1.0).", "labels": [], "entities": [{"text": "SASO-EN test corpus", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.8026849031448364}]}, {"text": "We present the results in.", "labels": [], "entities": []}, {"text": "To understand the effect of the threshold \u03c4 , note that, in general, the effect of selecting a higher threshold should be to \"cherry pick\" those frame elements which are most likely to appear in the complete frame G u , thereby increasing precision while decreasing recall of the frame elements in S SUB j in relation to G u . In the figure, we can see that this is indeed the case.", "labels": [], "entities": [{"text": "precision", "start_pos": 239, "end_pos": 248, "type": "METRIC", "confidence": 0.9990386962890625}, {"text": "recall", "start_pos": 266, "end_pos": 272, "type": "METRIC", "confidence": 0.9967467784881592}]}, {"text": "The lines marked \"(complete frame)\"  in the figure evaluate the returned subframes in relation to the complete frame G u associated with the user's complete utterance.", "labels": [], "entities": []}, {"text": "We see that this method enables us to select subsets of frame elements that are most likely to appear in G u : by increasing the threshold, it is possible to return subframes which are of increasingly higher precision in relation to the final frame G u , but that also have lower recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.9770269989967346}, {"text": "recall", "start_pos": 280, "end_pos": 286, "type": "METRIC", "confidence": 0.9981768131256104}]}, {"text": "We also evaluated the returned subframes in relation to the hand-annotated subframes, to assess its performance at identifying the user's explicit meaning.", "labels": [], "entities": []}, {"text": "For an utterance u that generates partial ASR results r 1 , ..., rm , we denote the hand-annotated subframe corresponding to partial ASR result r j by G SUB j . In the lines marked \"(annotated subframe)\", we show the precision, recall, and F-score of the explicit subframe for each ASR result r j in relation to the annotated subframe G SUB j . As a first observation, note that at any threshold level, the explicit subframes do better at recalling the handannotated subframe elements than they do at recalling the complete frame elements.", "labels": [], "entities": [{"text": "ASR", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9838634729385376}, {"text": "ASR result r j", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.8473126739263535}, {"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9996281862258911}, {"text": "recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9985679388046265}, {"text": "F-score", "start_pos": 240, "end_pos": 247, "type": "METRIC", "confidence": 0.999403715133667}]}, {"text": "This means our new method is better at recalling what has been said already by the user than it is at predicting what will be said, as intended.", "labels": [], "entities": [{"text": "recalling what has been said already", "start_pos": 39, "end_pos": 75, "type": "TASK", "confidence": 0.7823523084322611}, {"text": "predicting what will be said", "start_pos": 102, "end_pos": 130, "type": "TASK", "confidence": 0.8400670170783997}]}, {"text": "We have seen two examples of this already, for the partial ASR result hello in, and for the partial ASR result we will in.", "labels": [], "entities": []}, {"text": "A second observation in is that precision remains better against the complete utterance frame than against the hand-annotated subframe (at all threshold levels).", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9995049238204956}]}, {"text": "This indicates that the explicit subframes are often still predicting some aspects of the full frame.", "labels": [], "entities": []}, {"text": "An example of this is given in, where the user's partial utterance we need to is assigned an explicit subframe that includes frame elements describing an event of moving the clinic, which the user has not said explicitly.", "labels": [], "entities": []}, {"text": "This happens because, in the SASO-EN domain, in fact there is nothing else that the interlocutors need to do besides move the clinic.", "labels": [], "entities": [{"text": "SASO-EN domain", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.7947985827922821}]}, {"text": "So based on the NLU training data, the data-driven probabilities assigned by Equation (1) describe the additional frame elements as about as probable as the ones capturing the we need to part of the semantics (given at the right).", "labels": [], "entities": [{"text": "NLU training data", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.8937448859214783}, {"text": "Equation", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.92476487159729}]}, {"text": "Finally, a third observation is that overall, the precision, recall, and F-score results against the annotated subframes using our method are surprisingly strong.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.999695897102356}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9990298748016357}, {"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9995232820510864}]}, {"text": "For example, when evaluating the explicit subframes overall partial ASR results, an F-score of 0.75 is attained at thresholds in the range 0.5-0.55.", "labels": [], "entities": [{"text": "ASR", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9253806471824646}, {"text": "F-score", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9995313882827759}]}, {"text": "This F-score is substantially better than the F-score of our predictive NLU in relation to the final full frames, which is 0.56 when evaluated overall partial ASR results.", "labels": [], "entities": [{"text": "F-score", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.9990505576133728}, {"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9987891316413879}, {"text": "ASR", "start_pos": 159, "end_pos": 162, "type": "TASK", "confidence": 0.5789660811424255}]}, {"text": "This means that our proposed model works better as an explicit incremental NLU than mxNLU works as a predictive incremental NLU.", "labels": [], "entities": []}, {"text": "Further, we observe that this F-score of 0.75 against hand-annotated subframes is approximately as good as the F-score of 0.76 that is achieved when mxNLU is used to interpret complete utterances.", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9987320303916931}, {"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9988754391670227}]}, {"text": "We therefore conclude that the proposed model is a promising and viable approach to explicit incremental NLU in SASO-EN.", "labels": [], "entities": [{"text": "SASO-EN", "start_pos": 112, "end_pos": 119, "type": "TASK", "confidence": 0.5971186757087708}]}], "tableCaptions": []}