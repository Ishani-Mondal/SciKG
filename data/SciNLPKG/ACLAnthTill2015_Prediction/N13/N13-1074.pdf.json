{"title": [{"text": "Phrase Training Based Adaptation for Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase Training Based Adaptation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8131023347377777}, {"text": "Statistical Machine Translation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.8450495998064677}]}], "abstractContent": [{"text": "We present a novel approach for translation model (TM) adaptation using phrase training.", "labels": [], "entities": [{"text": "translation model (TM) adaptation", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.9247951805591583}]}, {"text": "The proposed adaptation procedure is initialized with a standard general-domain TM, which is then used to perform phrase training on a smaller in-domain set.", "labels": [], "entities": [{"text": "phrase training", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.8924674093723297}]}, {"text": "This way, we bias the probabilities of the general TM towards the in-domain distribution.", "labels": [], "entities": []}, {"text": "Experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones.", "labels": [], "entities": [{"text": "lectures translation tasks", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7309866746266683}]}, {"text": "Additionally, we compare our results to mixture modeling, where we report gains when using the suggested phrase training adaptation method.", "labels": [], "entities": [{"text": "phrase training adaptation", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.765183687210083}]}], "introductionContent": [{"text": "The task of domain-adaptation attempts to exploit data mainly drawn from one domain (e.g. news, parliamentary discussion) to maximize the performance on the test domain (e.g. lectures, web forums).", "labels": [], "entities": []}, {"text": "In this work, we focus on translation model (TM) adaptation.", "labels": [], "entities": [{"text": "translation model (TM) adaptation", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.9185287157694498}]}, {"text": "A prominent approach in recent work is weighting at different levels of granularity.", "labels": [], "entities": [{"text": "weighting", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.9804394841194153}]}, {"text": "perform weighting at the corpus level, where different corpora receive different weights and are then combined using mixture modeling.", "labels": [], "entities": []}, {"text": "A finer grained weighting is that of, who weight each sentence in the bitexts using features of meta-information and optimize a mapping from the feature vectors to weights using a translation quality measure.", "labels": [], "entities": []}, {"text": "In this work, we propose to perform TM adaptation using phrase training.", "labels": [], "entities": [{"text": "TM adaptation", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9940280914306641}]}, {"text": "We start from a generaldomain phrase table and adapt the probabilities by training on an in-domain data.", "labels": [], "entities": []}, {"text": "Thus, we achieve direct phrase probabilities adaptation as opposed to weighting.", "labels": [], "entities": [{"text": "phrase probabilities adaptation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6463457345962524}]}, {"text": "perform weighting at the phrase level, assigning each phrase pair a weight according to its relevance to the test domain.", "labels": [], "entities": []}, {"text": "They compare phrase weighting to a \"flat\" model, where the weight directly approximates the phrase probability.", "labels": [], "entities": [{"text": "phrase weighting", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.8170827627182007}]}, {"text": "In their experiments, the weighting method performs better than the flat model, therefore, they conclude that retaining the original relative frequency probabilities of the TM is important for good performance.", "labels": [], "entities": []}, {"text": "The \"flat\" model of is similar to our work.", "labels": [], "entities": []}, {"text": "We differ in the following points: (i) we use the same procedure to perform the phrase training based adaptation and the search thus avoiding inconsistencies between the two; (ii) we do not directly interpolate the original statistics with the new ones, but use a training procedure to manipulate the original statistics.", "labels": [], "entities": [{"text": "phrase training based adaptation", "start_pos": 80, "end_pos": 112, "type": "TASK", "confidence": 0.5792133584618568}]}, {"text": "We perform experiments on the publicly available IWSLT TED task, on both Arabic-to-English and Germanto-English lectures translation tracks.", "labels": [], "entities": [{"text": "IWSLT TED task", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.5424180825551351}]}, {"text": "We compare our suggested phrase training adaptation method to a variety of baselines and show its effectiveness.", "labels": [], "entities": [{"text": "phrase training adaptation", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.8756353457768759}]}, {"text": "Finally, we experiment with mixture modeling based adaptation.", "labels": [], "entities": []}, {"text": "We compare mixture modeling to our adaptation method, and apply our method within a mixture modeling framework.", "labels": [], "entities": [{"text": "mixture modeling", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.814652681350708}]}, {"text": "In Section 2, we present the phrase training method and explain how it is utilized for adaptation.", "labels": [], "entities": []}, {"text": "Experimental setup including corpora statistics and the SMT system are described in Section 3.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9917759895324707}]}, {"text": "Section 4 summarizes the phrase training adaptation results ending with a comparison to mixture modeling.", "labels": [], "entities": [{"text": "phrase training adaptation", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.5761883656183878}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: IWSLT 2011 TED bilingual corpora statistics:  the number of tokens is given for the source side. OOV/X  denotes the number of OOV words in relation to corpus  X (the percentage is given in parentheses). IN is the TED  in-domain data, OD denotes other-domain data, ALL de- notes the concatenation of IN and OD.", "labels": [], "entities": [{"text": "IWSLT 2011 TED bilingual corpora statistics", "start_pos": 10, "end_pos": 53, "type": "DATASET", "confidence": 0.8701849977175394}]}, {"text": " Table 2: TED 2011 translation results. BLEU and TER are given in percentages. IN denotes the TED lectures in- domain corpus, OD denotes the other-domain corpus, ALL is the concatenation of IN and OD. FA 0 denotes forced  alignment training without leaving-one-out (otherwise, leaving-one-out is used).", "labels": [], "entities": [{"text": "TED 2011 translation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5923658013343811}, {"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9993346333503723}, {"text": "TER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9960722923278809}, {"text": "TED lectures in- domain corpus", "start_pos": 94, "end_pos": 124, "type": "DATASET", "confidence": 0.775218000014623}, {"text": "FA", "start_pos": 201, "end_pos": 203, "type": "METRIC", "confidence": 0.9878270030021667}, {"text": "forced  alignment training", "start_pos": 214, "end_pos": 240, "type": "TASK", "confidence": 0.6156782607237498}]}, {"text": " Table 3:  TED 2011 mixture modeling results.  Heuristics best is the best heuristics based system, IN for  Arabic-English and ALL for German-English. X,Y de- notes linear interpolation between X and Y phrase tables.", "labels": [], "entities": [{"text": "TED 2011 mixture modeling", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7756751775741577}, {"text": "IN", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9937830567359924}]}]}