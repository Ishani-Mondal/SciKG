{"title": [{"text": "Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods", "labels": [], "entities": [{"text": "Labeling the Languages of Words in Mixed-Language Documents", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.8783366233110428}]}], "abstractContent": [{"text": "In this paper we consider the problem of labeling the languages of words in mixed-language documents.", "labels": [], "entities": [{"text": "labeling the languages of words in mixed-language documents", "start_pos": 41, "end_pos": 100, "type": "TASK", "confidence": 0.8378841429948807}]}, {"text": "This problem is approached in a weakly supervised fashion, as a sequence labeling problem with monolingual text samples for training data.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6451629549264908}]}, {"text": "Among the approaches evaluated, a conditional random field model trained with generalized expectation criteria was the most accurate and performed consistently as the amount of training data was varied .", "labels": [], "entities": []}], "introductionContent": [{"text": "Language identification is a well-studied problem), but it is typically only studied in its canonical text-classification formulation, identifying a document's language given sample texts from a few different languages.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.700451672077179}]}, {"text": "But there are several other interesting and useful formulations of the problem that have received relatively little attention.", "labels": [], "entities": []}, {"text": "Here, we focus on the problem of labeling the languages of individual words within a multilingual document.", "labels": [], "entities": [{"text": "labeling the languages of individual words within a multilingual document", "start_pos": 33, "end_pos": 106, "type": "TASK", "confidence": 0.7946696162223816}]}, {"text": "To our knowledge, this is the first paper to specifically address this problem.", "labels": [], "entities": []}, {"text": "Our own motivation for studying this problem stems from issues encountered while attempting to build language resources for minority languages.", "labels": [], "entities": []}, {"text": "In trying to extend parts of Kevin Scannell's Cr\u00fabad\u00e1n project, which automatically builds minority language corpora from the Web, we found that the majority of webpages that contain text in a minority language also contain text in other languages.", "labels": [], "entities": []}, {"text": "Since Scannell's method builds these corpora by bootstrapping from the pages that were retrieved, the corpus-building process can go disastrously wrong without accounting for this problem.", "labels": [], "entities": []}, {"text": "And any resources, such as a lexicon, created from the corpus will also be incorrect.", "labels": [], "entities": []}, {"text": "In this paper, we explore techniques for performing language identification at the word level in mixed language documents.", "labels": [], "entities": [{"text": "language identification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7254379838705063}]}, {"text": "Our results show that one can do better than independent word language classification, as there are clues in a word's context: words of one language are frequently surrounded by words in the same language, and many documents have patterns that maybe marked by the presence of certain words or punctuation.", "labels": [], "entities": [{"text": "independent word language classification", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.6196906864643097}]}, {"text": "The methods in this paper also outperform sentence-level language identification, which is too coarse to capture most of the shifts between language.", "labels": [], "entities": [{"text": "sentence-level language identification", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.6043062607447306}]}, {"text": "To evaluate our methods, we collected and manually annotated a corpus of over 250,000 words of bilingual (though mostly non-parallel) text from the web.", "labels": [], "entities": []}, {"text": "After running several different weaklysupervised learning methods, we found that a conditional random field model trained with generalized expectation criteria is the most accurate and performs quite consistently as the amount of training data is varied.", "labels": [], "entities": []}, {"text": "In section 2, we review the related work.", "labels": [], "entities": []}, {"text": "In section 3, we define the task and describe the data and its annotation.", "labels": [], "entities": []}, {"text": "Because the task of language identification for individual words has not been explicitly studied in the literature, and because of its importance to the overall task, we examine the features and methods that work best for independent word language identification in section 4.", "labels": [], "entities": [{"text": "language identification for individual words", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.8095598459243775}, {"text": "independent word language identification", "start_pos": 222, "end_pos": 262, "type": "TASK", "confidence": 0.6378571838140488}]}, {"text": "We begin to ex-amine the larger problem of labeling the language of words in context in section 5 by describing our methods.", "labels": [], "entities": [{"text": "labeling the language of words in context", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.8575760040964399}]}, {"text": "In section 6, we describe the evaluation and present the results.", "labels": [], "entities": []}, {"text": "We present our error analysis in section 7 and conclude in section 8.", "labels": [], "entities": [{"text": "error", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.974651038646698}]}], "datasetContent": [{"text": "To build a corpus of mixed language documents, we used the BootCat tool () seeded with words from a minority language.", "labels": [], "entities": [{"text": "BootCat", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9209694862365723}]}, {"text": "BootCat is designed to automatically collect webpages on a specific topic by repeatedly searching for keywords from a topic-specific set of seed words.", "labels": [], "entities": [{"text": "BootCat", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8894020318984985}]}, {"text": "We found that this method works equally well for languages as for topics, when seeded with words from a specific language.", "labels": [], "entities": []}, {"text": "Once BootCat returned a collection of documents, we manually identified documents from the set that contained text in both the target language and in English, but did not contain text in any other languages.", "labels": [], "entities": []}, {"text": "Since the problem becomes trivial when the languages do not share a character set, we limited ourselves to languages with a Latin orthography.", "labels": [], "entities": []}, {"text": "We found that there was an important balance to be struck concerning the popularity of a language.", "labels": [], "entities": []}, {"text": "If a language is not spoken widely enough, then there is little chance of finding any text in that language on the Web.", "labels": [], "entities": []}, {"text": "Conversely if a language is too widely spoken, then it is difficult to find mixed-language pages for it.", "labels": [], "entities": []}, {"text": "The list of languages present in the corpus and the number of words in each language reflects this balance as seen in.", "labels": [], "entities": []}, {"text": "For researchers who wish to make use this data, the set of annotations used in this paper is available from the first author's website 1 .  We evaluated each method using simple token-level accuracy, i.e. whether the correct label was assigned to a word in the document.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.8413766622543335}]}, {"text": "Word boundaries were defined by punctuation or whitespace, and no tokens containing a digit were included.", "labels": [], "entities": []}, {"text": "displays the accuracy for each method as the number of sampled words from each language example is varied from 10 to 1000.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.999679684638977}]}, {"text": "In all the cases we tested, CRF trained with GE is clearly the most accurate option among the methods examined, though the EM-trained HMM seemed to be approaching a similar accuracy with large amounts of training data.", "labels": [], "entities": [{"text": "GE", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.903164267539978}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9989842772483826}]}, {"text": "With a slight edge in efficiency also in its favor, we think the GE+CRF approach, rather than EM+HMM, is the best approach for this problem because of its consistent performance across a wide range of training data sizes.", "labels": [], "entities": [{"text": "GE+CRF", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.842890997727712}]}, {"text": "In its favor, the EM+HMM approach has a slightly lower variance in its performance across different files, though not at a statistically significant level.", "labels": [], "entities": []}, {"text": "Contrary to most of the results in (), a logistic regression classifier trained with GE did not outperform a standard supervised na\u00a8\u0131vena\u00a8\u0131ve Bayes classifier.", "labels": [], "entities": []}, {"text": "We suspect that this is due to the different nature of this problem as compared to most other sequence labeling problems, with the classifier bootstrapping over a single document only.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.6597903966903687}]}, {"text": "In the problems studied by Mann and McCallum, the GE-trained classifier was able to train over the entire training set, which was on average about 50,000 instances, far more than the number of words in the average document in this set (2,500).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Number of total words of training data for  each language.", "labels": [], "entities": []}, {"text": " Table 4: Logistic regression accuracy when trained  using varying features.", "labels": [], "entities": [{"text": "Logistic regression", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8419126570224762}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9789832830429077}]}]}