{"title": [{"text": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters", "labels": [], "entities": [{"text": "Improved Part-of-Speech Tagging", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8276598652203878}]}], "abstractContent": [{"text": "We consider the problem of part-of-speech tagging for informal, online conversational text.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7663763761520386}]}, {"text": "We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7332458794116974}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.8525512218475342}]}, {"text": "With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute).", "labels": [], "entities": [{"text": "IRC POS tagging", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.7597113251686096}, {"text": "Twitter tagging", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.6260760128498077}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9965909719467163}]}, {"text": "Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre.", "labels": [], "entities": []}, {"text": "Additionally, we contribute the first POS annotation guidelines for such text and release anew dataset of English language tweets annotated using these guidelines.", "labels": [], "entities": []}, {"text": "Tagging software, annotation guidelines, and large-scale word clusters are available at:", "labels": [], "entities": []}], "introductionContent": [{"text": "Online conversational text, typified by microblogs, chat, and text messages, 1 is a challenge for natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.6588843663533529}]}, {"text": "Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns.", "labels": [], "entities": []}, {"text": "These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate.", "labels": [], "entities": []}, {"text": "There Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging), named entity recognition (), and parsing), but accuracy rates are still significantly lower than traditional well-edited genres like newswire.", "labels": [], "entities": [{"text": "social media part-of-speech (POS) tagging", "start_pos": 82, "end_pos": 123, "type": "TASK", "confidence": 0.6024778613022396}, {"text": "named entity recognition", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.6318639715512594}, {"text": "parsing", "start_pos": 159, "end_pos": 166, "type": "TASK", "confidence": 0.9739164113998413}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9989107847213745}]}, {"text": "Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text, as does speech transcript parsing.", "labels": [], "entities": [{"text": "web text parsing", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.5927514433860779}, {"text": "speech transcript parsing", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.7762914299964905}]}, {"text": "To tackle the challenge of novel words and constructions, we create anew Twitter part-of-speech tagger-building on previous work by-that includes new large-scale distributional features.", "labels": [], "entities": []}, {"text": "This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8032468557357788}]}, {"text": "We also annotated anew dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets.", "labels": [], "entities": [{"text": "POS tagging of tweets", "start_pos": 172, "end_pos": 193, "type": "TASK", "confidence": 0.8669210970401764}]}, {"text": "We release all of these resources to the research community: \u2022 an open-source part-of-speech tagger for online conversational text ( \u00a72); \u2022 unsupervised Twitter word clusters ( \u00a73); \u2022 an improved emoticon detector for conversational text ( \u00a74); \u2022 POS annotation guidelines ( \u00a75.1); and \u2022 anew dataset of 547 manually POS-annotated tweets ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "(2011) provided a dataset of POStagged tweets consisting almost entirely of tweets sampled from one particular day (.", "labels": [], "entities": []}, {"text": "We were concerned about overfitting to timespecific phenomena; for example, a substantial fraction of the messages are about a basketball game happening that day.", "labels": [], "entities": []}, {"text": "We created anew test set of 547 tweets for evaluation.", "labels": [], "entities": []}, {"text": "The test set consists of one random English tweet from everyday between January 1, 2011 and June 30, 2012.", "labels": [], "entities": []}, {"text": "In order fora tweet to be considered English, it had to contain at least one English word other than a URL, emoticon, or at-mention.", "labels": [], "entities": []}, {"text": "We noticed biases in the outputs of langid.py, so we instead selected these messages completely manu-ally (going through a random sample of one day's messages until an English message was found).", "labels": [], "entities": []}, {"text": "We are primarily concerned with performance on our annotated datasets described in \u00a75 (OCT27, DAILY547), though for comparison to previous work we also test on other corpora (RITTERTW in \u00a76.2, NPSCHAT in \u00a76.3).", "labels": [], "entities": [{"text": "DAILY547", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.6253183484077454}, {"text": "RITTERTW", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.8351667523384094}, {"text": "NPSCHAT", "start_pos": 193, "end_pos": 200, "type": "DATASET", "confidence": 0.7815974354743958}]}, {"text": "The annotated datasets are listed in.", "labels": [], "entities": []}, {"text": "We use OCT27 to refer to the entire dataset described in it is split into training, development, and test portions (OCT27TRAIN, OCT27DEV, OCT27TEST).", "labels": [], "entities": []}, {"text": "We use DAILY547 as an additional test set.", "labels": [], "entities": [{"text": "DAILY547", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.6126358509063721}]}, {"text": "Neither OCT27TEST nor DAILY547 were extensively evaluated against until final ablation testing when writing this paper.", "labels": [], "entities": [{"text": "OCT27TEST", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.4631498456001282}, {"text": "DAILY547", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.7975957989692688}]}, {"text": "The total number of features is 3.7 million, all of which are used under pure L 2 regularization; but only 60,000 are selected by elastic net regularization with (\u03bb 1 , \u03bb 2 ) = (0.25, 2), which achieves nearly the same (but no better) accuracy as pure L 2 , and we use it for all experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.9983769655227661}]}, {"text": "We observed that it was We conducted a grid search for the regularizer values on part of DAILY547, and many regularizer values give the best or nearly the best results.", "labels": [], "entities": [{"text": "DAILY547", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9305887222290039}]}, {"text": "We suspect a different setup would have yielded similar results.", "labels": [], "entities": []}, {"text": "possible to get radically smaller models with only a slight degradation in performance: (4, 0.06) has 0.5% worse accuracy but uses only 1,632 features, a small enough number to browse through manually.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9987754225730896}]}, {"text": "First, we evaluate on the new test set, training on all of OCT27.", "labels": [], "entities": [{"text": "OCT27", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.830298900604248}]}, {"text": "Due to DAILY547's statistical representativeness, we believe this gives the best view of the tagger's accuracy on English Twitter text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9989715814590454}, {"text": "English Twitter text", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.785900890827179}]}, {"text": "The full tagger attains 93.2% accuracy (final row of Table 2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9991543292999268}]}, {"text": "To facilitate comparisons with previous work, we ran a series of experiments training only on OCT27's training and development sets, then report test results on both OCT27TEST and all of DAILY547, shown in.", "labels": [], "entities": [{"text": "OCT27's training and development sets", "start_pos": 94, "end_pos": 131, "type": "DATASET", "confidence": 0.9059251050154368}, {"text": "OCT27TEST", "start_pos": 166, "end_pos": 175, "type": "DATASET", "confidence": 0.9401736259460449}, {"text": "DAILY547", "start_pos": 187, "end_pos": 195, "type": "DATASET", "confidence": 0.7822226285934448}]}, {"text": "Our tagger achieves substantially higher accuracy than.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9994695782661438}]}, {"text": "A number of ablation tests indicate the word clusters area very strong source of lexical knowledge.", "labels": [], "entities": []}, {"text": "When dropping the tag dictionaries and name lists, the word clusters maintain most of the accuracy (row 2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9995049238204956}]}, {"text": "If we drop the clusters and rely only on tag dictionaries and namelists, accuracy decreases significantly (row 3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9994779229164124}]}, {"text": "In fact, we can remove all observation features except for word clusters-no word features, orthographic fea-  tures, affix n-grams, capitalization, emoticon patterns, etc.-and the accuracy is in fact still better than the previous work (row 4).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9997584223747253}]}, {"text": "We also wanted to know whether to keep the tag dictionary and name list features, but the splits reported in did not show statistically significant differences; so to better discriminate between ablations, we created a lopsided train/test split of all data with a much larger test portion (26,974 tokens), having greater statistical power (tighter confidence intervals of \u00b1 0.3%).", "labels": [], "entities": []}, {"text": "The full system got 90.8% while the no-tag dictionary, no-namelists ablation had 90.0%, a statistically significant difference.", "labels": [], "entities": []}, {"text": "Therefore we retain these features.", "labels": [], "entities": []}, {"text": "Compared to the tagger in Gimpel et al., most of our feature changes are in the new lexical features described in \u00a73.5.", "labels": [], "entities": []}, {"text": "We do not reuse the other lexical features from the previous work, including a phonetic normalizer (Metaphone), a name list consisting of words that are frequently capitalized, and distributional features trained on a much smaller unlabeled corpus; they are all worse than our new lexical features described here.", "labels": [], "entities": []}, {"text": "(We did include, however, a variant of the tag dictionary feature that uses phonetic normalization for lookup; it seemed to yield a small improvement.)", "labels": [], "entities": []}, {"text": "Furthermore, when evaluating the clusters as unsupervised (hard) POS tags, we obtain a many-to-one accuracy of 89.2% on DAILY547.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9906372427940369}, {"text": "DAILY547", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.8566131591796875}]}, {"text": "Before computing this, we lowercased the text to match the clusters and removed tokens tagged as URLs and at-mentions.", "labels": [], "entities": []}, {"text": "Reported confidence intervals in this paper are 95% binomial normal approximation intervals for the proportion of correctly tagged tokens: \u00b11.96 p(1 \u2212 p)/n tokens 1/ \u221a n.", "labels": [], "entities": [{"text": "binomial normal approximation intervals", "start_pos": 52, "end_pos": 91, "type": "METRIC", "confidence": 0.7709917277097702}]}, {"text": "20 Details on the exact feature set are available in a technical report, also available on the website.", "labels": [], "entities": []}, {"text": "The word clusters are especially helpful with words that do not appear in traditional dictionaries.", "labels": [], "entities": []}, {"text": "We constructed a dictionary by lowercasing the union of the ispell 'American', 'British', and 'English' dictionaries, plus the standard Unix words file from Webster's Second International dictionary, totalling 260,985 word types.", "labels": [], "entities": []}, {"text": "After excluding tokens defined by the gold standard as punctuation, URLs, at-mentions, or emoticons, 21 22% of DAILY547's tokens do not appear in this dictionary.", "labels": [], "entities": [{"text": "DAILY547's tokens", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.872868816057841}]}, {"text": "Without clusters, they are very difficult to classify (only 79.2% accuracy), but adding clusters generates a 5.7 point improvement-much larger than the effect on in-dictionary tokens (Table 3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9989018440246582}]}, {"text": "Varying the amount of unlabeled data.", "labels": [], "entities": []}, {"text": "A tagger that only uses word clusters achieves an accuracy of 88.6% on the OCT27 development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9995489716529846}, {"text": "OCT27 development set", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.943570077419281}]}, {"text": "We created several clusterings with different numbers of unlabeled tweets, keeping the number of clusters constant at 800.", "labels": [], "entities": []}, {"text": "As shown in, there was initially a logarithmic relationship between number of tweets and accuracy, but accuracy (and lexical coverage) levels out after 750,000 tweets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9991845488548279}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9991631507873535}]}, {"text": "We use the largest clustering (56 million tweets and 1,000 clusters) as the default for the released tagger.", "labels": [], "entities": []}, {"text": "(2011) annotated a corpus of 787 tweets with a single annotator, using the PTB We retain hashtags since by our guidelines a #-prefixed token is ambiguous between a hashtag and a normal word, e.g. #1 or going #home.", "labels": [], "entities": [{"text": "PTB", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.9461247324943542}]}, {"text": "The only observation features are the word clusters of a token and its immediate neighbors.", "labels": [], "entities": []}, {"text": "tagset plus several Twitter-specific tags, referred to in as RITTERTW.", "labels": [], "entities": [{"text": "RITTERTW", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9463754892349243}]}, {"text": "Linguistic concerns notwithstanding ( \u00a75.2), fora controlled comparison, we train and test our system on this data with the same 4-fold cross-validation setup they used, attaining 90.0% (\u00b10.5%) accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9981484413146973}]}, {"text": "Ritter et al.'s CRFbased tagger had 85.3% accuracy, and their best tagger, trained on a concatenation of PTB, IRC, and Twitter, achieved 88.3%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9996127486228943}, {"text": "PTB", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.9548385739326477}]}, {"text": "IRC is another medium of online conversational text, with similar emoticons, misspellings, abbreviations and acronyms as Twitter data.", "labels": [], "entities": [{"text": "IRC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8603358864784241}]}, {"text": "We evaluate our tagger on the NPS Chat Corpus, a PTB-part-of-speech annotated dataset of Internet Relay Chat (IRC) room messages from 2006.", "labels": [], "entities": [{"text": "NPS Chat Corpus, a PTB-part-of-speech annotated dataset of Internet Relay Chat (IRC) room messages from 2006", "start_pos": 30, "end_pos": 138, "type": "DATASET", "confidence": 0.9255405852669164}]}, {"text": "First, we compare to a tagger in the same setup as experiments on this data in, training on 90% of the data and testing on 10%; we average results across 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "The full tagger model achieved 93.4% (\u00b10.3%) accuracy, significantly improving over the best result they report, 90.8% accuracy with a tagger trained on a mix of several POS-annotated corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993594288825989}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9994082450866699}]}, {"text": "We also perform the ablation experiments on this corpus, with a slightly different experimental setup: we first filter out system messages then split data", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotated datasets: number of messages, to- kens, tagset, and date range. More information in  \u00a75,   \u00a76.3, and  \u00a76.2.", "labels": [], "entities": []}, {"text": " Table 3: DAILY547 accuracies (%) for tokens in and out  of a traditional dictionary, for models reported in rows 1  and 3 of Table 2.", "labels": [], "entities": [{"text": "DAILY547", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9852572083473206}, {"text": "accuracies", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.6132503747940063}]}, {"text": " Table 2: Tagging accuracies (%) in ablation experiments. OCT27TEST and DAILY547 95% confidence intervals are  roughly \u00b10.7%. Our final tagger uses all features and also trains on OCT27TEST, achieving 93.2% on DAILY547.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.5536707043647766}, {"text": "OCT27TEST", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8543067574501038}, {"text": "DAILY547 95% confidence intervals", "start_pos": 72, "end_pos": 105, "type": "METRIC", "confidence": 0.7720280408859252}, {"text": "OCT27TEST", "start_pos": 180, "end_pos": 189, "type": "DATASET", "confidence": 0.91275954246521}, {"text": "DAILY547", "start_pos": 210, "end_pos": 218, "type": "DATASET", "confidence": 0.975807249546051}]}]}