{"title": [{"text": "Using Out-of-Domain Data for Lexical Addressee Detection in Human-Human-Computer Dialog", "labels": [], "entities": [{"text": "Lexical Addressee Detection in Human-Human-Computer Dialog", "start_pos": 29, "end_pos": 87, "type": "TASK", "confidence": 0.7115475535392761}]}], "abstractContent": [{"text": "Addressee detection (AD) is an important problem for dialog systems in human-human-computer scenarios (contexts involving multiple people and a system) because system-directed speech must be distinguished from human-directed speech.", "labels": [], "entities": [{"text": "Addressee detection (AD)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9284341692924499}]}, {"text": "Recent work on AD (Shriberg et al., 2012) showed good results using prosodic and lexical features trained on in-domain data.", "labels": [], "entities": [{"text": "AD", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9427434206008911}]}, {"text": "In-domain data, however, is expensive to collect for each new domain.", "labels": [], "entities": []}, {"text": "In this study we focus on lexical models and investigate how well out-of-domain data (either outside the domain, or from single-user scenarios) can fill in for matched in-domain data.", "labels": [], "entities": []}, {"text": "We find that human-addressed speech can be modeled using out-of-domain conversational speech transcripts, and that human-computer utterances can be modeled using single-user data: the resulting AD system outperforms a system trained only on matched in-domain data.", "labels": [], "entities": []}, {"text": "Further gains (up to a 4% reduction in equal error rate) are obtained when in-domain and out-of-domain models are interpolated.", "labels": [], "entities": [{"text": "equal error rate", "start_pos": 39, "end_pos": 55, "type": "METRIC", "confidence": 0.7441774010658264}]}, {"text": "Finally, we examine which parts of an utterance are most useful.", "labels": [], "entities": []}, {"text": "We find that the first 1.5 seconds of an utterance contain most of the lexical information for AD, and analyze", "labels": [], "entities": [{"text": "AD", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.4789276719093323}]}], "introductionContent": [{"text": "Before a spoken dialog system can recognize and interpret a user's speech, it should ideally determine if speech was even meant to be interpreted by the system.", "labels": [], "entities": []}, {"text": "We refer to this task as addressee detection (AD).", "labels": [], "entities": [{"text": "addressee detection (AD)", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8198539614677429}]}, {"text": "AD is often overlooked, especially in traditional single-user scenarios, because with the exception of self-talk, side-talk or background speech, the majority of speech is usually system-directed.", "labels": [], "entities": [{"text": "AD", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7430309057235718}]}, {"text": "As dialog systems expand to more natural contexts and multiperson environments, however, AD can become a crucial part of the system's operational requirements.", "labels": [], "entities": []}, {"text": "This is particularly true for systems in which explicit system addressing (e.g., push-to-talk or required keyword addressing) is undesirable.", "labels": [], "entities": []}, {"text": "Past research on addressee detection has focused on human-human (H-H) settings, such as meetings, sometimes with multimodal cues (op den.", "labels": [], "entities": [{"text": "addressee detection", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7313379347324371}]}, {"text": "Early systems relied primarily on rejection of H-H utterances either because they could not be interpreted (), or because they yielded low speech recognition confidence).", "labels": [], "entities": []}, {"text": "Some systems combine gaze with lexical and syntactic cues to detect H-H speech).", "labels": [], "entities": []}, {"text": "Others use relatively simple prosodic features based on pitch and energy in addition to those derived from automatic speech recognition (ASR)).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR))", "start_pos": 107, "end_pos": 142, "type": "TASK", "confidence": 0.7902453243732452}]}, {"text": "With some exceptions (), relatively little work has looked at the human-human-computer (H-H-C) scenario, i.e. at contexts involving two or more people who interact both with a system and with each other.", "labels": [], "entities": []}, {"text": "found that novel prosodic features were more accurate than lexical or semantic features based on speech recognition for the addressee task.", "labels": [], "entities": []}, {"text": "The corpus, also used herein, is comprised of H-H-C dialog in which roughly half of the computer-addressed speech consisted of a small set of fixed commands.", "labels": [], "entities": []}, {"text": "While the word-based features map directly to the commands, they had trouble distinguishing all other (noncommand) computerdirected speech from human-directed speech.", "labels": [], "entities": []}, {"text": "This is because addressee detection in the H-H-C scenario becomes even more challenging when the system is designed for natural speech, i.e., utterances that are conversational inform and not limited to command phrases with restricted syntax.", "labels": [], "entities": [{"text": "addressee detection", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7453460395336151}]}, {"text": "Furthermore, H-H utterances can be about the domain of the system (e.g., discussing the dialog task), making AD based on language content more difficult.", "labels": [], "entities": []}, {"text": "The prosodic features were good at both types of distinctions-even improving performance significantly when combined with true-word (cheating) lexical features that have 100% accuracy on the commands.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9946175217628479}]}, {"text": "Nevertheless, the prior work showed that lexical n-grams are useful for addressee detection in the H-H-C scenario.", "labels": [], "entities": [{"text": "addressee detection", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8187173008918762}]}, {"text": "A problem with lexical features is that they are highly task-and domain-dependent.", "labels": [], "entities": []}, {"text": "As with other language modeling tasks, one usually has to collect matched training data in significant quantities.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7247614115476608}]}, {"text": "Data collection is made more cumbersome and expensive by the multi-user aspect of the scenario.", "labels": [], "entities": [{"text": "Data collection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6265135258436203}]}, {"text": "Thus, for practical reasons alone, it would be much better if the language models for AD could be trained on out-of-domain data, and if whatever in-domain data is needed could be limited to single-user interaction.", "labels": [], "entities": []}, {"text": "We show in this paper that precisely this training scenario is feasible and achieves results that are comparable or better than using completely matched H-H-C training data.", "labels": [], "entities": []}, {"text": "In addition to studying the role of out-of-domain data for lexical AD models, we also examine which words are useful, and how soon in elapsed time they are available.", "labels": [], "entities": []}, {"text": "Whereas most prior work in AD has looked at processing of entire utterances, we consider an online processing version where AD decisions are to be made as soon as possible after an utterance was initiated.", "labels": [], "entities": []}, {"text": "We find that most of the addressee-relevant lexical information can be found", "labels": [], "entities": []}], "datasetContent": [{"text": "Typically, an application-dependent threshold would be applied to the decision score to convert it into a binary decision.", "labels": [], "entities": []}, {"text": "The optimal threshold is a function of prior class probabilities and error costs.", "labels": [], "entities": []}, {"text": "As in, we used equal error rate (EER) to compare systems, since we are interested in the discriminative power of the decision score independent of priors and costs.", "labels": [], "entities": [{"text": "equal error rate (EER)", "start_pos": 15, "end_pos": 37, "type": "METRIC", "confidence": 0.9065018991629282}]}, {"text": "EER is the probability of false detections and misses at the operating point at which the two types of errors are equally probable.", "labels": [], "entities": [{"text": "EER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9864988327026367}]}, {"text": "A prior-free metric such as EER is more meaningful than classification accuracy because the utterance type distribution is heavily skewed, and because the rate of human-versus computerdirected speech can vary widely depending on the particular people, domain, and context.", "labels": [], "entities": [{"text": "EER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9901970624923706}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.7436870336532593}]}, {"text": "We also use classification accuracy (based on data priors) in one analysis below, because EERs are not comparable for different test data subdivisions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8240995407104492}, {"text": "EERs", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9248307347297668}]}], "tableCaptions": [{"text": " Table 3: Addressee detection performance (EER) with different training sets", "labels": [], "entities": [{"text": "Addressee detection performance (EER)", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.7993184725443522}]}, {"text": " Table 3. Thin lines at the top right  corner use ASR output", "labels": [], "entities": [{"text": "ASR", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.708987832069397}]}, {"text": " Table 3:  1,5 = in-domain only, 2,6 = out-of-domain only, 4,7  = both-all, 3,8 = both-small.", "labels": [], "entities": []}, {"text": " Table 4: Perplexities (computed on dev set ASR  words) by utterance type, for different training cor- pora. Interpolation refers to the combination of the  three models listed in each case.", "labels": [], "entities": []}, {"text": " Table 6: The top 15 first words in utterances", "labels": [], "entities": []}, {"text": " Table 5: Performance of POS-based model with various top-N word lists (EER)", "labels": [], "entities": []}]}