{"title": [{"text": "Purpose and Polarity of Citation: Towards NLP-based Bibliometrics", "labels": [], "entities": [{"text": "Purpose", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9570896029472351}]}], "abstractContent": [{"text": "Bibliometric measures are commonly used to estimate the popularity and the impact of published research.", "labels": [], "entities": []}, {"text": "Existing bibliometric measures provide \"quantitative\" indicators of how good a published paper is.", "labels": [], "entities": []}, {"text": "This does not necessarily reflect the \"quality\" of the work presented in the paper.", "labels": [], "entities": []}, {"text": "For example, when h-index is computed fora researcher, all incoming citations are treated equally, ignoring the fact that some of these citations might be negative.", "labels": [], "entities": []}, {"text": "In this paper, we propose using NLP to add a \"qualitative\" aspect to biblometrics.", "labels": [], "entities": []}, {"text": "We analyze the text that accompanies citations in scientific articles (which we term citation context).", "labels": [], "entities": []}, {"text": "We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation.", "labels": [], "entities": []}], "introductionContent": [{"text": "An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment.", "labels": [], "entities": []}, {"text": "Existing bibliometric measures such as H-Index (, G-index), and Impact Factor focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect.", "labels": [], "entities": [{"text": "Impact Factor", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.932601273059845}]}, {"text": "For example, the number of papers published by a researcher only tells how productive she or he is.", "labels": [], "entities": []}, {"text": "It does not say anything about the quality or the impact of the work.", "labels": [], "entities": []}, {"text": "Similarly, the number of citations that a paper receives should not be used to gauge the quality of the work as it really only measures the popularity of the work and the interest of other researchers in it.", "labels": [], "entities": []}, {"text": "Controversial papers or those based on fabricated data or experiments may receive a large number of citations.", "labels": [], "entities": []}, {"text": "A popular example of fraudulent research that deceived many researchers and caught media attention was the case of a South Korean research scientist, Hwang Woosuk, who was found to have faked his research results in the area of human stem cell cloning.", "labels": [], "entities": [{"text": "human stem cell cloning", "start_pos": 228, "end_pos": 251, "type": "TASK", "confidence": 0.6325419917702675}]}, {"text": "His research was published in Science and received close to 200 citations after the fraud was discovered.", "labels": [], "entities": []}, {"text": "The vast majority of those citations were negative.", "labels": [], "entities": []}, {"text": "This suggests that the purpose of citation should betaken into consideration when biblometric measures are computed.", "labels": [], "entities": []}, {"text": "Negative citations should be weighted less than positive or neutral citations.", "labels": [], "entities": [{"text": "Negative citations", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.8051193952560425}]}, {"text": "This motivates the need to automatically distinguish between positive, negative, and neutral citations and to identify the purpose of a citation; i.e. the author's intention behind choosing a published article and citing it.", "labels": [], "entities": []}, {"text": "This analysis of citation purpose and polarity can be useful for many applications.", "labels": [], "entities": []}, {"text": "For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers' work more accurately.", "labels": [], "entities": []}, {"text": "It can also be used as a preprocessing step in systems that process scholarly data.", "labels": [], "entities": []}, {"text": "For example, citation-based summarization systems; AbuJbara and Radev, 2011) and survey generation systems ( can benefit from citation purpose and polarity analysis to improve paper and content selection.", "labels": [], "entities": [{"text": "paper and content selection", "start_pos": 176, "end_pos": 203, "type": "TASK", "confidence": 0.6173260807991028}]}, {"text": "In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation.", "labels": [], "entities": []}, {"text": "We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it.", "labels": [], "entities": []}, {"text": "We use the term citation context to refer to this text.", "labels": [], "entities": []}, {"text": "Next, we use supervised classification techniques to analyze this text and identify the purpose and polarity of citation.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the related work.", "labels": [], "entities": []}, {"text": "We present our approach in Section 3.", "labels": [], "entities": []}, {"text": "We then describe the data and experiments in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the paper and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the data that we used for evaluation and the experiments that we conducted.", "labels": [], "entities": []}, {"text": "We use the CRF++ 1 toolkit for CRF training and testing.", "labels": [], "entities": [{"text": "CRF++ 1 toolkit", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.8858331739902496}]}, {"text": "We use the Stanford parser to parse the citation text and generate the dependency parse trees of sentences.", "labels": [], "entities": []}, {"text": "We use Weka for classification experiments.", "labels": [], "entities": [{"text": "Weka", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.9538732171058655}]}, {"text": "We experimented with several classifiers including: SVM, Logistic Regression (LR), and Naive Bayes.", "labels": [], "entities": []}, {"text": "All the experiments that we conducted used the training/testing dataset in a 10-fold cross validation mode.", "labels": [], "entities": []}, {"text": "All the results have been tested for statistical significance using a 2-tailed paired t-test.", "labels": [], "entities": []}, {"text": "We compare the CRF approach to three baselines.", "labels": [], "entities": []}, {"text": "The first baseline (ALL) labels all the sentences in the citation window of size 4 as INCLUDED in the citation context.", "labels": [], "entities": [{"text": "INCLUDED", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9918000102043152}]}, {"text": "The second baseline (CS-ONLY) labels the citing sentence only as INCLUDED in the citation context.", "labels": [], "entities": [{"text": "INCLUDED", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9960476756095886}]}, {"text": "In the third baseline, we use a supervised classification method instead of sequence labeling.", "labels": [], "entities": []}, {"text": "We use Support Vector Machines (SVM) to train a model using the same set of features as in the CRF approach.", "labels": [], "entities": []}, {"text": "shows the precision, recall, and F1 score of the CRF approach and the baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9998154044151306}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.99931800365448}, {"text": "F1 score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9892387092113495}]}, {"text": "The results show that our CRF approach outperforms all the baselines.", "labels": [], "entities": [{"text": "CRF", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8444874286651611}]}, {"text": "It also asserts our expectation that addressing this problem as a sequence labeling problem leads to better performance than individual sen-: Results of citation context identification tence classification, which is also clear from the nature of the task.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.6730076372623444}, {"text": "citation context identification tence classification", "start_pos": 153, "end_pos": 205, "type": "TASK", "confidence": 0.8334012866020203}]}, {"text": "Feature Analysis: We evaluated the importance of the features listed in by computing the chi-squared statistic for every feature with respect to the class.", "labels": [], "entities": [{"text": "Feature Analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8114402294158936}]}, {"text": "We found that the lexical features (such as determiners and conjunction adverbs) are generally more important than the structural features (such as position and reference count).", "labels": [], "entities": []}, {"text": "The features shown in are listed in the order of their importance based on this analysis.", "labels": [], "entities": []}, {"text": "Our experiments with several classification algorithms showed that the SVM classifier outperforms Logistic Regression and Naive Bayes classifiers.", "labels": [], "entities": []}, {"text": "Due to space limitations, we only show the results for SVM.", "labels": [], "entities": [{"text": "SVM", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.5893504619598389}]}, {"text": "shows the precision, recall, and F1 for each of the six categories.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9998083710670471}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.999679684638977}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9998537302017212}]}, {"text": "It also shows the overall accuracy and the Macro-F measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9969547986984253}, {"text": "Macro-F measure", "start_pos": 43, "end_pos": 58, "type": "METRIC", "confidence": 0.6672456562519073}]}, {"text": "Feature Analysis: The chi-squared evaluation of the features listed in shows that both lexical and structural features are important.", "labels": [], "entities": [{"text": "Feature Analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7273710370063782}]}, {"text": "It also shows that among lexical features, the ones that are limited to the existence of a direct relation to the target reference (such as closest verb, adjective, adverb, subjective cue, etc.) are most useful.", "labels": [], "entities": []}, {"text": "This can be explained by the fact that the restricting the features to having direct dependency relation introduces much less noise than other features (such as Dependency Triplets).", "labels": [], "entities": []}, {"text": "Among the structural features, the number of references in the citation context showed to be more useful.", "labels": [], "entities": []}, {"text": "Similar to the case of citation purpose classification, our experiments showed that the SVM classifier outperforms the other classifiers that we experimented with.", "labels": [], "entities": [{"text": "citation purpose classification", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.8648761709531149}]}, {"text": "each of the three categories.", "labels": [], "entities": []}, {"text": "It also shows the overall accuracy and the Macro-F measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9969547986984253}, {"text": "Macro-F measure", "start_pos": 43, "end_pos": 58, "type": "METRIC", "confidence": 0.6672456562519073}]}, {"text": "The analysis of the features used to train this classifier using chisquared analysis leads to the same conclusions about the relative importance of the features as described in the previous subsection.", "labels": [], "entities": []}, {"text": "However, we noticed that features that are related to subjectivity (Subjectivity Cues, Negation, Speculation) are ranked higher which makes sense in the case of polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 161, "end_pos": 184, "type": "TASK", "confidence": 0.7238168716430664}]}], "tableCaptions": [{"text": " Table 4: Results of citation context identification", "labels": [], "entities": [{"text": "citation context identification", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.8072940905888876}]}, {"text": " Table 5: Summary of Citation Purpose Classification Results (10-fold cross validation, SVM: Linear Kernel, c = 1.0)", "labels": [], "entities": []}, {"text": " Table 6: Summary of Citation Polarity Classification Re- sults (10-fold cross validation, SVM: Linear Kernel, c =  1.0). Numbers between rounded parentheses are when  only the explicit citing sentence is used (i.e. no context).  Numbers in square brackets are when the gold standard  context is used.", "labels": [], "entities": []}]}