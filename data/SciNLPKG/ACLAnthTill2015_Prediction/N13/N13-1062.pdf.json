{"title": [{"text": "Embracing Ambiguity: A Comparison of Annotation Methodologies for Crowdsourcing Word Sense Labels", "labels": [], "entities": []}], "abstractContent": [{"text": "Word sense disambiguation aims to identify which meaning of a word is present in a given usage.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7130018572012583}]}, {"text": "Gathering word sense annotations is a laborious and difficult task.", "labels": [], "entities": []}, {"text": "Several methods have been proposed to gather sense annotations using large numbers of untrained anno-tators, with mixed results.", "labels": [], "entities": []}, {"text": "We propose three new annotation methodologies for gathering word senses where untrained annotators are allowed to use multiple labels and weight the senses.", "labels": [], "entities": []}, {"text": "Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as anno-tators in a controlled setting, and in aggregate generate equally as good of a sense labeling.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense annotation is regarded as one of the most difficult annotation tasks and building manually-annotated corpora with highquality sense labels is often a time-and resourceconsuming task.", "labels": [], "entities": [{"text": "Word sense annotation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.5920814673105875}]}, {"text": "As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (;, which results in a knowledge acquisition bottleneck for training systems that require sense labels (.", "labels": [], "entities": []}, {"text": "In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing.", "labels": [], "entities": []}, {"text": "Recently, several works have proposed gathering sense annotations using crowdsourcing ().", "labels": [], "entities": []}, {"text": "However, these methods produce sense labels that are different from the commonly used sense inventories such as WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9621031284332275}]}, {"text": "Furthermore, while did use WordNet sense labels, they found the quality was well below that of trained experts.", "labels": [], "entities": [{"text": "WordNet sense labels", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.8303171197573344}]}, {"text": "We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment.", "labels": [], "entities": []}, {"text": "First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.9671916365623474}]}, {"text": "However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage).", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 114, "end_pos": 145, "type": "METRIC", "confidence": 0.8287720680236816}]}, {"text": "Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree?", "labels": [], "entities": []}, {"text": "Furthermore, we adopt the goal of, which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity.", "labels": [], "entities": []}, {"text": "This paper provides the following contributions.", "labels": [], "entities": []}, {"text": "First, we demonstrate that the choice in annotation setup can significantly improve IAA and that the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate.", "labels": [], "entities": [{"text": "IAA", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9276609420776367}]}, {"text": "Second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "For measuring the difference in methodologies, we propose three experiments based on different analyses of comparing Turker and non-Turker annotations on the same dataset, the latter of which we refer to as the reference labeling.", "labels": [], "entities": []}, {"text": "First, we measure the ability of the Turkers individually by evaluating their IAA with the reference labeling.", "labels": [], "entities": [{"text": "IAA", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9737259149551392}]}, {"text": "Second, many studies using crowdsourcing combine the results into a single answer, thereby leveraging the wisdom of the crowds) to smooth over inconsistencies in the data.", "labels": [], "entities": []}, {"text": "Therefore, in the second experiment, we evaluate different methods of combining Turker responses into a single sense labeling, referred to as an aggregate labeling, and comparing that with the reference labeling.", "labels": [], "entities": []}, {"text": "Third, we measure the replicability of the Turker annotations) using a sampling methodology.", "labels": [], "entities": []}, {"text": "Two equally-sized sets of Turker annotations are created by randomly sampling without replacement from the full set of annotations for each item.", "labels": [], "entities": []}, {"text": "IAA is calculated between the aggregate labelings computed from each set.", "labels": [], "entities": [{"text": "IAA", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9801604151725769}]}, {"text": "This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers.", "labels": [], "entities": [{"text": "IAA", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9597812294960022}]}, {"text": "For the reference sense labeling, we use a subset of the GWS dataset of, where three annotators rated 50 instances each for eight words.", "labels": [], "entities": [{"text": "reference sense labeling", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.7184450229008993}, {"text": "GWS dataset", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9897738993167877}]}, {"text": "For clarity, we refer to these individuals as the GWS annotators.", "labels": [], "entities": [{"text": "GWS annotators", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9503178000450134}]}, {"text": "Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3.", "labels": [], "entities": [{"text": "GWS annotators", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.906289130449295}]}, {"text": "Contexts were drawn evenly from the SemCor ( and SENSEVAL-3 lexical substitution () corpora.", "labels": [], "entities": []}, {"text": "GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels.", "labels": [], "entities": [{"text": "GWS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8830174207687378}]}, {"text": "For each annotation task, Turkers were presented with an identical set of annotation guidelines, followed by methodology-specific instructions.", "labels": [], "entities": []}, {"text": "1 To increase the familiarity with the task, four instances were shown per task, with all instances using the same target word.", "labels": [], "entities": []}, {"text": "(2012b), we did not require a Turker to annotate all contexts fora single word; however many Turkers did complete the majority of instances.", "labels": [], "entities": []}, {"text": "Both the Likert, Select, and Rate tasks used ten Turkers each.", "labels": [], "entities": [{"text": "Select", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.8758875727653503}, {"text": "Rate", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9199864864349365}]}, {"text": "Senses were passed from Select to Rate if they received at least three votes.", "labels": [], "entities": []}, {"text": "For MaxDiff, we gathered at least 3n annotations per context where n is the number of senses of the target word, ensuring that each sense appeared at least once.", "labels": [], "entities": []}, {"text": "Due to resource limitations, we omitted the evaluation of argument.n for MaxDiff.", "labels": [], "entities": []}, {"text": "Following the recommendation of, Turkers were paid $0.05USD for each Likert, Select, and Rate task.", "labels": [], "entities": [{"text": "Select", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.7981517910957336}]}, {"text": "For MaxDiff, due to their shorter nature and comparably high volume, Turkers were paid $0.03USD per task.", "labels": [], "entities": []}, {"text": "To ensure fluency in English as well as reduce the potential for low-quality results, we prefaced each task with a simple test question that asked the Turker to pick out a definition of the target word from a list of four options.", "labels": [], "entities": []}, {"text": "The incorrect options were selected so that they would be nonsensical for anyone familiar with the target word.", "labels": [], "entities": []}, {"text": "Additionally, we rejected all Turker responses where more than one option was missing a rating.", "labels": [], "entities": []}, {"text": "In the case of missing ratings, we infer a rating of 1.", "labels": [], "entities": []}, {"text": "Approximately 20-30% of the submissions were rejected by these criteria, underscoring the importance of filtering.", "labels": [], "entities": []}, {"text": "For measuring IAA, we selected Krippendorff's \u03b1, which is an agreement coefficient that handles missing data, as well as different levels of measurement, e.g., nominal data (Select and MaxDiff) and interval data (Likert and Rate).", "labels": [], "entities": [{"text": "IAA", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.823722779750824}, {"text": "Likert and Rate)", "start_pos": 213, "end_pos": 229, "type": "METRIC", "confidence": 0.8434819430112839}]}, {"text": "Krippendorff's \u03b1 adjusts for chance, ranging between [\u22121, 1] for nominal data and (\u22121, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; random labels would have an expected \u03b1 of zero.", "labels": [], "entities": []}, {"text": "We treat each sense and instance combination as a separate item to rate.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: IAA per word (top) and IAA between aggregate labelings and the GWS annotators (bottom)", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9991306662559509}, {"text": "IAA", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9991706609725952}, {"text": "GWS", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9531092643737793}]}]}