{"title": [{"text": "What to do about bad language on the internet", "labels": [], "entities": []}], "abstractContent": [{"text": "The rise of social media has brought computational linguistics in ever-closer contact with bad language: text that defies our expectations about vocabulary, spelling, and syntax.", "labels": [], "entities": []}, {"text": "This paper surveys the landscape of bad language , and offers a critical review of the NLP community's response, which has largely followed two paths: normalization and domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.7095022946596146}]}, {"text": "Each approach is evaluated in the context of theoretical and empirical work on computer-mediated communication.", "labels": [], "entities": []}, {"text": "In addition , the paper presents a quantitative analysis of the lexical diversity of social media text, and its relationship to other corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "As social media becomes an increasingly important application domain for natural language processing, we encounter language that is substantially different from many benchmark corpora.", "labels": [], "entities": []}, {"text": "The following examples are all from the social media service Twitter: \u2022 Work on farm Fri.", "labels": [], "entities": [{"text": "farm Fri", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.6833775639533997}]}, {"text": "Burning piles of brush WindyFire got out of control.", "labels": [], "entities": [{"text": "WindyFire", "start_pos": 23, "end_pos": 32, "type": "DATASET", "confidence": 0.9722878932952881}]}, {"text": "Thank God for good naber He help get undr control PantsBurnLegWound.", "labels": [], "entities": [{"text": "PantsBurnLegWound", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.890151858329773}]}, {"text": "(Senator Charles Grassley) \u2022 Boom!", "labels": [], "entities": []}, {"text": "Ya ur website suxx bro (Sarah Silverman) \u2022 ...dats why pluto is pluto it can neva b a star (Shaquille O'Neil) \u2022 michelle obama great. job. and.", "labels": [], "entities": []}, {"text": "whit all my. respect she. look. great. congrats. to. her.", "labels": [], "entities": []}, {"text": "(Ozzie Guillen) These examples are selected from celebrities (for privacy reasons), but they contain linguistic challenges that are endemic to the medium, including non-standard punctuation, capitalization, spelling, vocabulary, and syntax.", "labels": [], "entities": []}, {"text": "The consequences for language technology are dire: a series of papers has detailed how state-of-the-art natural language processing (NLP) systems perform significantly worse on social media text.", "labels": [], "entities": []}, {"text": "In part-of-speech tagging, the accuracy of the Stanford tagger () falls from 97% on Wall Street Journal text to 85% accuracy on Twitter ().", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.7649311423301697}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995948672294617}, {"text": "Wall Street Journal text", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.9502701312303543}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9972655773162842}]}, {"text": "In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure (), down from 86% on the CoNLL test set ().", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6713286141554514}, {"text": "CoNLL-trained Stanford recognizer", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.6011448999245962}, {"text": "F-measure", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9990130662918091}, {"text": "CoNLL test set", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.9328409830729166}]}, {"text": "In parsing, report double-digit decreases inaccuracy for four different state-of-the-art parsers when applied to social media text.", "labels": [], "entities": [{"text": "parsing", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9765195846557617}]}, {"text": "The application of language technology to social media is potentially transformative, leveraging the knowledge and perspectives of millions of people.", "labels": [], "entities": []}, {"text": "But to deliver on this potential, the problems at the core of the NLP pipeline must be addressed.", "labels": [], "entities": []}, {"text": "A growing thread of research takes up this challenge, including a shared task and workshop on \"parsing the web,\" with new corpora which appear to sit somewhere between the Wall Street Journal and Twitter on the spectrum of bad language.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 172, "end_pos": 191, "type": "DATASET", "confidence": 0.950370728969574}]}, {"text": "But perhaps surprisingly, very little of this research has considered why social media language is so different.", "labels": [], "entities": []}, {"text": "This review paper attempts to shed some light on this question, surveying a strong tradition of empirical and theoreti-cal research on computer-mediated communication (CMC).", "labels": [], "entities": [{"text": "computer-mediated communication (CMC)", "start_pos": 135, "end_pos": 172, "type": "TASK", "confidence": 0.6384089648723602}]}, {"text": "I argue that the two main computational approaches to dealing with bad language -normalization and domain adaptation -are based on theories of social media language that are not descriptively accurate.", "labels": [], "entities": [{"text": "dealing with bad language -normalization", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.5967115412155787}, {"text": "domain adaptation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.698009803891182}]}, {"text": "I have worked and continue to work in both of these areas, so I make this argument not as a criticism of others, but in a spirit of self-reflection.", "labels": [], "entities": []}, {"text": "It is hoped that a greater engagement with sociolinguistic and CMC research will lead to new, nuanced approaches to the challenge of bad language.", "labels": [], "entities": []}, {"text": "Most of the examples in this paper will focus on Twitter, a microblogging service.", "labels": [], "entities": []}, {"text": "argue that Twitter has unfairly dominated recent research, at the expense of email and SMS text messages, which they found to be both linguistically distinct from Twitter and significantly more prevalent.", "labels": [], "entities": []}, {"text": "This matches earlier research arguing that email contained relatively little \"neography,\" compared with text messages and chat.", "labels": [], "entities": []}, {"text": "A crucial advantage for Twitter is that it is public by default, while SMS and email are private.", "labels": [], "entities": []}, {"text": "This makes Twitter data less problematic from a privacy standpoint, 1 far easier to obtain, and more amenable to target applications such as large-scale mining of events () and opinions).", "labels": [], "entities": []}, {"text": "Similar argument could be made on behalf of other public social media, such as blog comments, forums, and chatrooms).", "labels": [], "entities": []}, {"text": "The main advantage of Twitter over these media is convenience in gathering large datasets through a single streaming interface.", "labels": [], "entities": []}, {"text": "More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}