{"title": [{"text": "Grouping Language Model Boundary Words to Speed K-Best Extraction from Hypergraphs", "labels": [], "entities": [{"text": "Grouping Language Model Boundary Words to Speed K-Best Extraction from Hypergraphs", "start_pos": 0, "end_pos": 82, "type": "TASK", "confidence": 0.7749740427190607}]}], "abstractContent": [{"text": "We propose anew algorithm to approximately extract top-scoring hypotheses from a hyper-graph when the score includes an N-gram language model.", "labels": [], "entities": []}, {"text": "In the popular cube pruning algorithm, every hypothesis is annotated with boundary words and permitted to recom-bine only if all boundary words are equal.", "labels": [], "entities": []}, {"text": "However, many hypotheses share some, but not all, boundary words.", "labels": [], "entities": []}, {"text": "We use these common boundary words to group hypotheses and do so recursively, resulting in a tree of hypotheses.", "labels": [], "entities": []}, {"text": "This tree forms the basis for our new search algorithm that iteratively refines groups of boundary words on demand.", "labels": [], "entities": []}, {"text": "Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7049109041690826}]}], "introductionContent": [{"text": "This work presents anew algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N -gram language model.", "labels": [], "entities": []}, {"text": "Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition.", "labels": [], "entities": [{"text": "hypergraph search", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7558776140213013}, {"text": "hierarchical and syntactic machine translation", "start_pos": 93, "end_pos": 139, "type": "TASK", "confidence": 0.6608621597290039}, {"text": "lattice rescoring in speech recognition", "start_pos": 144, "end_pos": 183, "type": "TASK", "confidence": 0.6638892531394959}, {"text": "confusion network decoding", "start_pos": 189, "end_pos": 215, "type": "TASK", "confidence": 0.6696638464927673}, {"text": "optical character recognition", "start_pos": 219, "end_pos": 248, "type": "TASK", "confidence": 0.5905767182509104}]}, {"text": "Large language models have been shown to improve quality, especially in machine translation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8124450147151947}]}, {"text": "However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space.", "labels": [], "entities": []}, {"text": "Prior work, including cube pruning, has largely treated the language model as a black box.", "labels": [], "entities": []}, {"text": "Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning.", "labels": [], "entities": []}, {"text": "The search spaces mentioned in the previous paragraph are special cases of a directed acyclic hypergraph.", "labels": [], "entities": []}, {"text": "As used here, the difference from a normal graph is that an edge can go from one vertex to any number of vertices; this number is the arity of the edge.", "labels": [], "entities": []}, {"text": "Lattices and confusion networks are hypergraphs in which every edge happens to have arity one.", "labels": [], "entities": []}, {"text": "We experiment with parsing-based machine translation, where edges represent grammar rules that may have any number of non-terminals, including zero.", "labels": [], "entities": [{"text": "parsing-based machine translation", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.8860177795092264}]}, {"text": "Hypotheses are paths in the hypergraph scored by a linear combination of features.", "labels": [], "entities": []}, {"text": "Many features are additive: they can be expressed as weights on edges that sum to form hypothesis features.", "labels": [], "entities": []}, {"text": "However, log probability from an N -gram language model is non-additive because it examines surface strings across edge and vertex boundaries.", "labels": [], "entities": []}, {"text": "Non-additivity makes search difficult because locally optimal hypotheses may not be globally optimal.", "labels": [], "entities": []}, {"text": "In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (.", "labels": [], "entities": []}, {"text": "Hypotheses with equal state maybe recombined, so a straightforward dynamic programming approach () simply treats state as an additional dimension in the dynamic programming table.", "labels": [], "entities": []}, {"text": "However, this approach quickly becomes intractable for large language models where the number of states is too large.", "labels": [], "entities": []}, {"text": "Beam search) approximates the straightforward algorithm by remembering abeam of up to k hypotheses 1 in each vertex.", "labels": [], "entities": [{"text": "Beam search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6705894768238068}]}, {"text": "It visits each vertex in bottom-up order, each time calling abeam filling algorithm to select k hypotheses.", "labels": [], "entities": []}, {"text": "The parameter k is a time-accuracy trade-off: larger k increases both CPU time and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9962393045425415}]}, {"text": "We contribute anew beam filling algorithm that improves the time-accuracy trade-off over the popular cube pruning algorithm) discussed in \u00a72.3.", "labels": [], "entities": [{"text": "beam filling", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.8113816976547241}]}, {"text": "The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar.", "labels": [], "entities": []}, {"text": "Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once.", "labels": [], "entities": []}, {"text": "The algorithm is fully described in \u00a73.", "labels": [], "entities": []}], "datasetContent": [{"text": "Performance is measured by translating the 3003-sentence German-English test set from the 2011 Workshop on Machine Translation.", "labels": [], "entities": [{"text": "German-English test set from the 2011 Workshop on", "start_pos": 57, "end_pos": 106, "type": "DATASET", "confidence": 0.880313478410244}, {"text": "Machine Translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.770323783159256}]}, {"text": "Two translation models were built, one hierarchical and one with target syntax.", "labels": [], "entities": []}, {"text": "The target-syntax system is based on English parses from the Collins (1999) parser.", "labels": [], "entities": [{"text": "Collins (1999) parser", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.886456561088562}]}, {"text": "Both were trained on Europarl ().", "labels": [], "entities": [{"text": "Europarl", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9929174184799194}]}, {"text": "The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9866839051246643}]}, {"text": "Interpolation weights were tuned on the 2010 test set.", "labels": [], "entities": [{"text": "Interpolation", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9378588199615479}, {"text": "2010 test set", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9111066659291586}]}, {"text": "Language models were built with SRILM), modified Kneser-Ney smoothing, default pruning, and order 5.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.8976329565048218}]}, {"text": "Feature weights were tuned with MERT, beam size 1000, 100-best output, and cube pruning.", "labels": [], "entities": [{"text": "MERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9975428581237793}]}, {"text": "Systems were built with the Moses pipeline.", "labels": [], "entities": [{"text": "Moses pipeline", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9606970846652985}]}, {"text": "Measurements were collected by running the decoder on all 3003 sentences.", "labels": [], "entities": []}, {"text": "For consistency, all relevant files were forced into the operating system disk cache before each run.", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.8520116209983826}]}, {"text": "CPU time is the total user and system time taken by the decoder minus loading time.", "labels": [], "entities": [{"text": "CPU time", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8865016996860504}]}, {"text": "Loading time was measured by running the decoder with empty input.", "labels": [], "entities": []}, {"text": "In particular, CPU time includes the cost of parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9813693761825562}]}, {"text": "Our test system has 32 cores and 64 GB of RAM; no run came close to running out of memory.", "labels": [], "entities": [{"text": "RAM", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9800649285316467}]}, {"text": "While multi-threaded experiments showed improvements as well, we only report single-threaded results to reduce noise and to compare with cdec (.", "labels": [], "entities": []}, {"text": "Decoders were compiled with the optimization settings suggested in their documentation.", "labels": [], "entities": []}, {"text": "Search accuracy is measured by average model score; higher is better.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9876416921615601}]}, {"text": "Only relative comparisons are meaningful because model scores have arbitrary scale and include constant factors.", "labels": [], "entities": []}, {"text": "Beam sizes start at 5 and rise until a time limit determined by running the slowest algorithm with beam size 1000.", "labels": [], "entities": []}, {"text": "Performance with additive scores is roughly the same as using full scores with half the beam size.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Numerical results from the hierarchical system  for select beam sizes k comparing our best result with the  best baseline, both in Moses with rest costs enabled. To  conserve space, model scores are shown with 100 added.", "labels": [], "entities": []}]}