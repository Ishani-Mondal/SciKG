{"title": [{"text": "Massively Parallel Suffix Array Queries and On-Demand Phrase Extraction for Statistical Machine Translation Using GPUs", "labels": [], "entities": [{"text": "Phrase Extraction", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6929566711187363}, {"text": "Statistical Machine Translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.7900534272193909}]}], "abstractContent": [{"text": "Translation models in statistical machine translation can be scaled to large corpora and arbitrarily-long phrases by looking up translations of source phrases \"on the fly\" in an indexed parallel corpus using suffix arrays.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6325567464033762}]}, {"text": "However, this can be slow because on-demand extraction of phrase tables is computationally expensive.", "labels": [], "entities": []}, {"text": "We address this problem by developing novel algorithms for general purpose graphics processing units (GPUs), which enable suffix array queries for phrase lookup and phrase extraction to be massively parallelized.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.7944652736186981}]}, {"text": "Compared to a highly-optimized, state-of-the-art serial CPU-based implementation, our techniques achieve at least an order of magnitude improvement in terms of throughput.", "labels": [], "entities": []}, {"text": "This work demonstrates the promise of massively parallel architectures and the potential of GPUs for tackling computationally-demanding problems in statistical machine translation and language processing.", "labels": [], "entities": [{"text": "statistical machine translation and language processing", "start_pos": 148, "end_pos": 203, "type": "TASK", "confidence": 0.6532829850912094}]}], "introductionContent": [{"text": "Efficiently handling large translation models is a perennial problem in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.6946461896101633}]}, {"text": "One particularly promising solution ( \u00a72) is to use the parallel text itself as an implicit representation of the translation model and extract translation units \"on the fly\" when they are needed to decode new input.", "labels": [], "entities": []}, {"text": "This idea has been applied to phrase-based), hierarchical, and syntax-based (Cromieres and Kurohashi, 2011) models.", "labels": [], "entities": []}, {"text": "A benefit of this technique is that it scales to arbitrarily large models with very little pre-processing.", "labels": [], "entities": []}, {"text": "For instance, showed that a translation model trained on a large corpus with sparse word alignments and loose extraction heuristics substantially improved Chinese-English translation.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 155, "end_pos": 182, "type": "TASK", "confidence": 0.6912309229373932}]}, {"text": "An explicit representation of the model would have required nearly a terabyte of memory, but its implicit representation using the parallel text required only a few gigabytes.", "labels": [], "entities": []}, {"text": "Unfortunately, there is substantial computational cost in searching a parallel corpus for source phrases, extracting their translations, and scoring them on the fly.", "labels": [], "entities": []}, {"text": "Since the number of possible translation units maybe quite large (for example, all substrings of a source sentence) and their translations are numerous, both phrase lookup and extraction are performance bottlenecks.", "labels": [], "entities": []}, {"text": "Despite considerable research and the use of efficient indexes like suffix arrays, this problem remains not fully solved.", "labels": [], "entities": []}, {"text": "We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with \"on the fly\" phrase extraction.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 186, "end_pos": 203, "type": "TASK", "confidence": 0.7302650213241577}]}, {"text": "GPUs have previously been applied to DNA sequence matching using suffix trees () and suffix arrays).", "labels": [], "entities": [{"text": "DNA sequence matching", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6284651954968771}]}, {"text": "Building on this work, we present two novel contributions: First, we describe improved GPU algorithms for suffix array queries that achieve greater parallelism ( \u00a73).", "labels": [], "entities": []}, {"text": "Second, we propose novel data structures and algorithms for phrase extraction ( \u00a74) and scoring ( \u00a75) that are amenable to GPU par-allelization.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8342917859554291}]}, {"text": "The resulting implementation achieves at least an order of magnitude higher throughput than a state-of-the-art single-threaded CPU implementation ( \u00a76).", "labels": [], "entities": []}, {"text": "Since our experiments verify that the GPU implementation produces exactly the same results as a CPU reference implementation on a full extraction, we can simply replace that component and reap significant performance advantages with no impact on translation quality.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first reported application of GPU acceleration techniques for statistical machine translation.", "labels": [], "entities": [{"text": "GPU acceleration", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.701704278588295}, {"text": "statistical machine translation", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.7540473143259684}]}, {"text": "We believe these results reveal a promising yet unexplored future direction in exploiting parallelism to tackle perennial performance bottlenecks in state-of-the-art translation models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our GPU-based grammar extraction implementation under the conditions in which it would be used fora Chinese-to-English machine translation task, in particular, replicating the data conditions of.", "labels": [], "entities": [{"text": "GPU-based grammar extraction", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6818819840749105}, {"text": "Chinese-to-English machine translation task", "start_pos": 110, "end_pos": 153, "type": "TASK", "confidence": 0.7081832066178322}]}, {"text": "Experiments were performed on two data sets.", "labels": [], "entities": []}, {"text": "First, we used the source (Chinese) side of news articles collected from the Xinhua Agency, with around 27 million words of Chinese in around one million sentences (totaling 137 MB).", "labels": [], "entities": [{"text": "Chinese) side of news articles collected from the Xinhua Agency", "start_pos": 27, "end_pos": 90, "type": "DATASET", "confidence": 0.7581346739422191}]}, {"text": "Second, we added source-side parallel text from the United Nations, with around 81 million words of Chinese in around four million sentences (totaling 561 MB).", "labels": [], "entities": []}, {"text": "Ina pre-processing phase, we mapped every word to a unique integer, with two special integers representing end-of-sentence and end-of-corpus, respectively.", "labels": [], "entities": []}, {"text": "Input query data consisted of all sentences from the NIST 2002-2006 translation campaigns, tokenized and integerized identically to the training data.", "labels": [], "entities": [{"text": "NIST 2002-2006 translation campaigns", "start_pos": 53, "end_pos": 89, "type": "DATASET", "confidence": 0.7641312330961227}]}, {"text": "On average, sentences contained around 29 words.", "labels": [], "entities": []}, {"text": "In order to fully stress our GPU algorithms, we ran tests on batches of 2,000, 4,000, 6,000, 8,000, and 16,000 sentences.", "labels": [], "entities": []}, {"text": "Since there are only around 8,000 test sentences in the NIST data, we simply duplicated the test data as necessary.", "labels": [], "entities": [{"text": "NIST data", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.9902300238609314}]}, {"text": "Our experiments used NVIDIA's Tesla C2050 GPU (Fermi Generation), which has 448 CUDA cores with a peak memory bandwidth 144 GB/s.", "labels": [], "entities": []}, {"text": "Note that the GPU was released in early 2010 and represents previous generation technology.", "labels": [], "entities": [{"text": "GPU", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.9401066303253174}]}, {"text": "NVIDIA's current GPUs (Kepler) boasts raw processing power in the 1.3 TFlops (double precision) range, which is approximately three times the GPU we used.", "labels": [], "entities": [{"text": "NVIDIA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9006219506263733}, {"text": "double precision)", "start_pos": 78, "end_pos": 95, "type": "METRIC", "confidence": 0.6864818334579468}]}, {"text": "Our CPU is a 3.33 GHz Intel Xeon X5260 processor, which has two cores.", "labels": [], "entities": []}, {"text": "As a baseline, we compared against the publicly available implementation of the CPU-based algorithms described by found in the pycdec () extension of the cdec machine translation system (.", "labels": [], "entities": []}, {"text": "Note that we only tested grammar extraction for continuous pairs of phrases, and we did not test the slower and more complex queries for hierarchical  (gappy) patterns described by.", "labels": [], "entities": [{"text": "grammar extraction", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7210958302021027}]}, {"text": "Both our implementation and the baseline are written primarily in C/C++.", "labels": [], "entities": []}, {"text": "Our source corpora and test data are the same as that presented in, and using the CPU implementation as a reference enabled us to confirm that our extracted grammars and features are identical (modulo sampling).", "labels": [], "entities": []}, {"text": "We timed our GPU implementation as follows: from the loading of query sentences, extractions of substrings and grammar rules, until all grammars for all sentences are generated in memory.", "labels": [], "entities": []}, {"text": "Timing does not include offline preparations such as the construction of the suffix array on source texts and the I/O costs for writing the per-sentence grammar files to disk.", "labels": [], "entities": []}, {"text": "This timing procedure is exactly the same for the CPU baseline.", "labels": [], "entities": [{"text": "timing", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9828945398330688}]}, {"text": "We are confident that our results represent a fair comparison between the GPU and CPU, and are not attributed to misconfigurations or other flaws in experimental procedures.", "labels": [], "entities": []}, {"text": "Note that the CPU implementation runs in a single thread, on the same machine that hosts the GPU (described above).", "labels": [], "entities": []}, {"text": "shows performance results comparing our GPU implementation against the reference CPU implementation for phrase extraction.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.8607107996940613}]}, {"text": "In one experimental condition, the sampling parameter for frequently-matching phrases is set to 300, per, denoted s 300 . The experimental condition without sampling is denoted s \u221e . Following standard settings, the maximum length of the source phrase is set to 5 and the maximum length of the target phrase is set to 15 (same for both GPU and CPU implementations  of input sentences.", "labels": [], "entities": []}, {"text": "Performance is reported in terms of throughput: the number of processed words per second on average (i.e., total time divided by the batch size in words).", "labels": [], "entities": []}, {"text": "The results are averaged over five trials, with 95% confidence intervals shown in parentheses.", "labels": [], "entities": []}, {"text": "Note that as the batch size increases, we achieve higher throughput on the GPU since we are better saturating its full processing power.", "labels": [], "entities": [{"text": "GPU", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.9653061628341675}]}, {"text": "In contrast, performance is constant on the CPU regardless of the number of sentences processed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example of how large numbers of suffix array queries can be factored across two highly parallel passes on a  GPU with a total of 26 threads to perform all queries for this sample input sentence.", "labels": [], "entities": []}, {"text": " Table 1: all threads during a  pass execute in parallel, and each thread performs a  binary search which takes no more than O(|Q| +  log |T |) time. While spawning so many threads  may seem wasteful, this degree of parallelization  still under-utilizes the GPU; the hardware we use  ( \u00a76) can manage up to 21,504 concurrent threads  in its resident occupancy. To fully take advantage  of the processing power, we process multiple input  sentences in parallel. Compared with previous  algorithms, our two-pass approach and our strategy  of thread assignment to increase the amount of  parallelism represent novel contributions.", "labels": [], "entities": [{"text": "O", "start_pos": 125, "end_pos": 126, "type": "METRIC", "confidence": 0.9909975528717041}, {"text": "thread assignment", "start_pos": 540, "end_pos": 557, "type": "TASK", "confidence": 0.7320632338523865}]}, {"text": " Table 2: Comparing the GPU and CPU implementations for phrase extraction on two different corpora. Throughput  is measured in words per second under different test set sizes; the 95% confidence intervals across five trials are given  in parentheses, along with relative speedups comparing the two implementations.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.8534184992313385}]}, {"text": " Table 3: Comparing no sampling on the GPU with sam- pling on the CPU in terms of performance improvements  (GPU over CPU) and increases in the number of phrase  pairs extracted (GPU over CPU).", "labels": [], "entities": []}, {"text": " Table 4: End-to-end machine translation performance:  time to process the NIST05 test set in seconds, broken  down in terms of the three processing stages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7126327753067017}, {"text": "NIST05 test set", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9270568291346232}]}]}