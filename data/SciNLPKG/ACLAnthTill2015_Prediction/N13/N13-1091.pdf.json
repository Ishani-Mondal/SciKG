{"title": [], "abstractContent": [{"text": "We propose a novel semantic annotation type of assigning truth values to predicate occurrences, and present TruthTeller, a standalone publicly-available tool that produces such annotations.", "labels": [], "entities": []}, {"text": "TruthTeller integrates a range of semantic phenomena, such as negation , modality, presupposition, implicativ-ity, and more, which were dealt only partly in previous works.", "labels": [], "entities": []}, {"text": "Empirical evaluations against human annotations show satisfactory results and suggest the usefulness of this new type of tool for NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina text, the action or relation denoted by every predicate can be seen as being either positively or negatively inferred from its sentence, or otherwise having an unknown truth status.", "labels": [], "entities": []}, {"text": "Only in (3) below can we infer that Gal sold her shop, hence the positive truth value of the predicate sell, while according to and (4) Gal did not sell it, hence the negative truth values, and in (1) we do not know if she sold it or not (the notations pt+, pt-and pt? denote truth states, defined in Subsection 2.3).", "labels": [], "entities": []}, {"text": "Identifying these predicate truth values is an important subtask within many semantic processing scenarios, including various applications such as Question Answering (QA), Information Extraction (IE), paraphrasing and summarization.", "labels": [], "entities": [{"text": "Identifying these predicate truth values", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8326497673988342}, {"text": "Question Answering (QA)", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.870007848739624}, {"text": "Information Extraction (IE)", "start_pos": 172, "end_pos": 199, "type": "TASK", "confidence": 0.834047120809555}, {"text": "paraphrasing", "start_pos": 201, "end_pos": 213, "type": "TASK", "confidence": 0.9565214514732361}, {"text": "summarization", "start_pos": 218, "end_pos": 231, "type": "TASK", "confidence": 0.9829239249229431}]}, {"text": "The following examples illustrate the phenomenon: (1) Gal made an attempt pt+ to sell pt?", "labels": [], "entities": []}, {"text": "(2) Gal did not try pt\u2212 to sell pt\u2212 her shop after hearing pt+ the offers.", "labels": [], "entities": []}, {"text": "(3) Maybe Gal wasn't smart pt?", "labels": [], "entities": []}, {"text": "to sell pt+ her shop.", "labels": [], "entities": []}, {"text": "(4) Gal wasn't smart pt\u2212 enough to sell pt\u2212 the shop that she had bought pt+ . Previous works addressed specific aspects of the truth detection problem:, and later, were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and \"natural logic\", a recursive truth value calculus (sell in (1-3)); recently, built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and and laid the groundwork for factive and implicative entailment calculus (sell in), as well as many generic constructions of presupposition (hearing in (2) is presupposed because it heads an adverbial clause and bought in (4) heads a finite relative clause), which, to our knowledge, have not yet been implemented computationally.", "labels": [], "entities": [{"text": "truth detection", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.7771728336811066}]}, {"text": "Notice in the examples that presuppositions persist under negation, in questions and if-clauses, while entailments do not.", "labels": [], "entities": []}, {"text": "In addition, there is a growing research line of negation and modality detection.", "labels": [], "entities": [{"text": "negation and modality detection", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7957205325365067}]}, {"text": "See, for example,.", "labels": [], "entities": []}, {"text": "We present TruthTeller 1 , a novel algorithm and system that identifies the truth value of each predicate in a given sentence.", "labels": [], "entities": []}, {"text": "It annotates nodes in the text's dependency parse-tree via a combination of pattern-based annotation rules and a recursive algorithm based on natural logic.", "labels": [], "entities": []}, {"text": "In the course of computing truth value, it also computes the implicativity/factivity signature of predicates, and their negation and modality to a basic degree, both of which are made available in the system output.", "labels": [], "entities": []}, {"text": "It addresses and combines the aforementioned phenomena (see Section 2), many of which weren't dealt in previous systems.", "labels": [], "entities": []}, {"text": "TruthTeller is an open source and publicly available annotation tool, offers a relatively simple algebra for truth value computation, and is accompanied by a publicly available lexicon of over 1,700 implicative and factive predicates.", "labels": [], "entities": [{"text": "TruthTeller", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8840833306312561}, {"text": "truth value computation", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.6540077229340872}]}, {"text": "Also, we provide an intuitive GUI for viewing and modifying the algorithm's annotation rules.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate TruthTeller's accuracy, we sampled 25 sentences from each of the RTE5 and RTE6 Test datasets (, widely used for textual inference benchmarks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9956758618354797}, {"text": "RTE5", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9636138081550598}, {"text": "RTE6 Test datasets", "start_pos": 86, "end_pos": 104, "type": "DATASET", "confidence": 0.9258577028910319}]}, {"text": "In these 50 sentences, we manually annotated each predicate, 153 in total, forming a gold standard.", "labels": [], "entities": []}, {"text": "As baseline, we report the most frequent value for each annotation.", "labels": [], "entities": []}, {"text": "The results, in, show high accuracy for all types, reducing the baseline CT and PT errors by half.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9995068311691284}, {"text": "CT", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.930618166923523}, {"text": "PT errors", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9481041431427002}]}, {"text": "Furthermore, most of the remaining errors were due to parser errors, according to a manual error analysis we conducted.", "labels": [], "entities": []}, {"text": "The baseline for NU annotations shows that negations are scarce in these RTE datasets, which was also the case for ct-and pt-annotations.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.7666398286819458}]}, {"text": "Thus, mostly indicates TruthTeller's performance in distinguishing positive CT and PT annotations from unknown ones, the latter constituting 20% of the gold standard.", "labels": [], "entities": [{"text": "PT", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.922956109046936}]}, {"text": "To further assess ct-and pt-annotations we performed two targeted measurements.", "labels": [], "entities": []}, {"text": "Precision for ct-and pt-was measured by manually judging the correctness of such annotations by TruthTeller, on a sample from RTE6 Test including 50 ct-and 124 pt-annotations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9953476786613464}, {"text": "TruthTeller", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.96670001745224}, {"text": "RTE6 Test", "start_pos": 126, "end_pos": 135, "type": "DATASET", "confidence": 0.9681051075458527}]}, {"text": "This test yielded 78% and 83% precision, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9998385906219482}]}, {"text": "pt-is more frequent as it is typically triggered by ct-, as well as by other constructions involving negation.", "labels": [], "entities": []}, {"text": "Recall was estimated by employing a human annotator to go through the dataset and look for ct-and pt-gold standard annotations.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6533045768737793}]}, {"text": "The annotator identified 40 \"ct-\"s and 50 \"pt-\"s, out of which TruthTeller found 47.5% of the \"ct-\"s and 74% of the \"pt-\"s.", "labels": [], "entities": [{"text": "TruthTeller", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.959147036075592}]}, {"text": "In summary, TruthTeller's performance on our target PT annotations is quite satisfactory with 89% accuracy overall, having 83% precision and 74% recall estimates specifically for pt-.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9994219541549683}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.997933030128479}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9992597699165344}]}], "tableCaptions": [{"text": " Table  2:  The  accuracy  measures  for  TruthTeller's 4 annotations. The right col- umn gives the accuracy for the corresponding  most-frequent baseline: {+/?, nu+, ct+, pt+}.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9996077418327332}, {"text": "TruthTeller's 4 annotations", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.9432502835988998}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9994292855262756}]}]}