{"title": [], "abstractContent": [{"text": "Most NLP tools are applied to text that is different from the kind of text they were evaluated on.", "labels": [], "entities": []}, {"text": "Common evaluation practice prescribes significance testing across data points in available test data, but typically we only have a single test sample.", "labels": [], "entities": []}, {"text": "This short paper argues that in order to assess the robustness of NLP tools we need to evaluate them on diverse samples, and we consider the problem of finding the most appropriate way to estimate the true effect size across datasets of our systems over their baselines.", "labels": [], "entities": []}, {"text": "We apply meta-analysis and show experimentally-by comparing estimated error reduction over observed error reduction on held-out datasets-that this method is significantly more predic-tive of success than the usual practice of using macro-or micro-averages.", "labels": [], "entities": [{"text": "estimated error reduction", "start_pos": 60, "end_pos": 85, "type": "METRIC", "confidence": 0.6776595513025919}]}, {"text": "Finally, we present anew parametric meta-analysis based on non-standard assumptions that seems superior to standard parametric meta-analysis.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP tools and online services such as the Stanford Parser or Google Translate are used fora wide variety of purposes and therefore also on very different kinds of data.", "labels": [], "entities": []}, {"text": "Some use the Stanford Parser to parse literature), while others use it for processing social media content).", "labels": [], "entities": []}, {"text": "The parser, however, was not necessarily evaluated on literature or social media content during development.", "labels": [], "entities": []}, {"text": "Still, users typically expect reasonable performance on any natural language input.", "labels": [], "entities": []}, {"text": "This paper asks what we as developers can do to estimate the effect of a change to our system -not on the labeled test data that happens to be available to us, but on future, still unseen datasets provided by our end users.", "labels": [], "entities": []}, {"text": "The usual practice in NLP is to evaluate a system on a small sample of held-out labeled data.", "labels": [], "entities": []}, {"text": "The observed effect size on this sample is then validated by significance testing across data points, testing whether the observed difference in performance means is likely to be due to mere chance.", "labels": [], "entities": []}, {"text": "The preferred significance testis probably the nonparametric paired bootstrap), but many researchers also resort to Student's t-test for dependent means relying on the assumption that their metric scores are normally distributed.", "labels": [], "entities": []}, {"text": "Such significance tests tell us nothing about how likely our change to our system is to lead to improvements on new datasets.", "labels": [], "entities": []}, {"text": "The significance tests all rely on the assumption that our datapoints are sampled i.i.d. at random.", "labels": [], "entities": []}, {"text": "The significance tests only tell us how likely it is that the observed difference in performance means would change if we sampled a bigger test sample the same way we sampled the one we have available to us right now.", "labels": [], "entities": []}, {"text": "In standard machine learning papers a similar situation arises.", "labels": [], "entities": []}, {"text": "If we are developing anew perceptron learning algorithm, for example, we are interested in how likely the new learning algorithm is to perform better than other perceptron learning algorithms across datasets, and we may for that reason evaluate it on a large set of repository datasets.", "labels": [], "entities": []}, {"text": "Demsar (2006) presents motivation for using nonparametric methods such as the Wilcoxon signed rank test to estimate significance across datasets.", "labels": [], "entities": [{"text": "Wilcoxon signed rank test", "start_pos": 78, "end_pos": 103, "type": "METRIC", "confidence": 0.5261379480361938}]}, {"text": "The t-test is based on means, and typically results across datasets are not commensurable.", "labels": [], "entities": []}, {"text": "The ttest is also extremely sentitive to outliers.", "labels": [], "entities": [{"text": "ttest", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9949151277542114}]}, {"text": "Notice also that typically we do not have enough datasets to do paired bootstrapping (van den.", "labels": [], "entities": []}, {"text": "In this paper we will assume that the Wilcoxon signed rank test provides a reasonable estimate of the significance of an observed difference in performance means across datasets, or of the significance of observed error reductions, but note that this still depends on the assumption that datasets are sampled i.i.d. at random.", "labels": [], "entities": []}, {"text": "More importantly, a non-parametric test across data sets does not provide an actual estimate of the effect size.", "labels": [], "entities": []}, {"text": "Estimating effect size is important, e.g. when there is a trade-off between performance gains and computational efficiency.", "labels": [], "entities": []}, {"text": "In evaluations across datasets in NLP we typically use the macro-average as an estimate of effect size, but in other fields such as psychology or medicine it is more common to use a weighted mean obtained using what is known as the fixed effects model or the random effects model for meta-analysis.", "labels": [], "entities": []}, {"text": "The experiments reported on in this paper focus on estimating error reduction and show that meta-analysis is generally superior to macro-and micro-average in terms of predicting future error reductions.", "labels": [], "entities": [{"text": "estimating error reduction", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.7618345022201538}]}, {"text": "Parametric meta-analysis, however, overparameterizes the distribution of error reductions, leading to some instability.", "labels": [], "entities": []}, {"text": "While meta-analysis is generally superior to macro-average, it is sometimes off by a large margin.", "labels": [], "entities": []}, {"text": "We therefore introduce anew parametric meta-analysis that seems better suited to predicting error reductions.", "labels": [], "entities": [{"text": "predicting error reductions", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.8279025554656982}]}, {"text": "In our experiments test set sizes are balanced, so micro-averages will be near-identical to macro-averages.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first experiment makes use of the 20 Newsgroups document classification dataset.", "labels": [], "entities": [{"text": "20 Newsgroups document classification dataset", "start_pos": 38, "end_pos": 83, "type": "DATASET", "confidence": 0.7056242763996124}]}, {"text": "The topics in 20 Newsgroups are hierarchically structured, which enables us to extract a large set of binary classification problems with considerable bias between source and target data.", "labels": [], "entities": []}, {"text": "We extract 20 high-level binary classification problems by considering all pairs of top-level categories, e.g. COMPUTERS-RECREATIVE (comp-rec).", "labels": [], "entities": []}, {"text": "For each of these 20 problems, we have different possible datasets, e.g. IBM-BASEBALL, MAC-MOTORCYCLES, etc.", "labels": [], "entities": [{"text": "IBM-BASEBALL", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.8250997066497803}]}, {"text": "A problem instance takes training and test data from two different datasets belong to the same high-level problem.", "labels": [], "entities": []}, {"text": "For example a problem instance could be learning to distinguish articles about Macintosh and motorcycles MAC-MOTORCYCLES (evaluated on the 20 Newsgroups test section) using labeled data from IBM-BASEBALL (the training section).", "labels": [], "entities": [{"text": "20 Newsgroups test section", "start_pos": 139, "end_pos": 165, "type": "DATASET", "confidence": 0.7994456216692924}]}, {"text": "In total we have 288 available problem instances in the 20 Newsgroups dataset.", "labels": [], "entities": [{"text": "Newsgroups dataset", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.7994508743286133}]}, {"text": "In our first experiment we are interested in predicting the error reductions of a naive Bayes learner 2 http://people.csail.mit.edu/jrennie/20Newsgroups/ over a perceptron model.", "labels": [], "entities": []}, {"text": "We use publicly available implementations with default parameters.", "labels": [], "entities": []}, {"text": "In each experiment we randomly select k datasets and estimate the true effect size using macro-average, a fixed effects model, a random effects model, and a corrected random effects model.", "labels": [], "entities": []}, {"text": "In order to estimate the within-study variance we take 50 paired bootstrap samples of the system outputs.", "labels": [], "entities": []}, {"text": "We evaluate our estimates against the observed average effect across 5 new randomly extracted datasets.", "labels": [], "entities": []}, {"text": "For each k we repeat the experiment 20 times and report average error.", "labels": [], "entities": [{"text": "average error", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.8765042722225189}]}, {"text": "We vary k to see how many observations are needed for our estimates to be reliable.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "We note that meta-analysis provides much better estimates than macro-averages across the board.", "labels": [], "entities": []}, {"text": "Our parametric meta-analysis based on the assumption that error reductions are Gumbel distributed performs best with more observations.", "labels": [], "entities": []}, {"text": "Our second experiment repeats the same procedure using available data from cross-lingual dependency parsing.", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.6249103049437205}]}, {"text": "We use the submitted results by participants in the CoNLL-X shared task) and try to predict the error reduction of one system over another given k many observations.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 96, "end_pos": 111, "type": "METRIC", "confidence": 0.9181629121303558}]}, {"text": "Given that we only have 12 submissions per system we use k \u2208 {5, 7, 9} randomly extracted datasets for observations and test on another five randomly extracted datasets.", "labels": [], "entities": []}, {"text": "While results) are only statistically significant with k = 7, we see that metaanalysis estimates effect size across data sets better than macro-average in all cases.", "labels": [], "entities": []}], "tableCaptions": []}