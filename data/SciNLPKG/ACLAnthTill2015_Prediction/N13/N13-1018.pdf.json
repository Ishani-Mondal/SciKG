{"title": [{"text": "Towards Topic Labeling with Phrase Entailment and Aggregation", "labels": [], "entities": [{"text": "Topic Labeling", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.8473306000232697}, {"text": "Phrase Entailment", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.85586217045784}]}], "abstractContent": [{"text": "We propose a novel framework for topic labeling that assigns the most representative phrases fora given set of sentences covering the same topic.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.8229584693908691}]}, {"text": "We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases.", "labels": [], "entities": []}, {"text": "We then aggregate those selected phrases by means of phrase generalization and merging.", "labels": [], "entities": [{"text": "phrase generalization", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7083292752504349}]}, {"text": "We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given text segments about the same topic written in different ways (i.e., language variability), topic labeling deals with the problem of automatically generating semantically meaningful labels for those text segments.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7110031247138977}]}, {"text": "The potential of integrating topic labeling as a prerequisite for higher-level analysis has been reported in several areas, such as summarization (, information extraction) and conversation visualization ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.9867942929267883}, {"text": "information extraction", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.7466034442186356}]}, {"text": "Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters.", "labels": [], "entities": []}, {"text": "shows an example of two human written topic labels fora topic cluster collected from a blog 1 , http://slashdot.org Text: a: Where do you think the term \"Horse laugh\" comes from?", "labels": [], "entities": []}, {"text": "b: And that rats also giggled when tickled.", "labels": [], "entities": []}, {"text": "c: My hypothesis-if an animal can play, it can \"laugh\" or at least it is familiar with the concept of \"laughing\".", "labels": [], "entities": []}, {"text": "There are various sorts of humour though.", "labels": [], "entities": []}, {"text": "Some involve you laughing because your brain suddenly made a lots of unexpected connections.", "labels": [], "entities": []}, {"text": "Possible extracted phrases: animals play, rats have, laugh, Horse laugh, rats also giggle, rats Human-authored topic labels: animals which laugh, animal laughter: Topic labeling example. and possible phrases that can be extracted from the topic cluster using different approaches.", "labels": [], "entities": []}, {"text": "This example demonstrates that although most approaches ( advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic.", "labels": [], "entities": []}, {"text": "As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g.,).", "labels": [], "entities": []}, {"text": "The same problem can be observed with many other examples.", "labels": [], "entities": []}, {"text": "This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research.", "labels": [], "entities": []}, {"text": "Moreover, to generate a label fora topic we have to be able to capture the overall meaning of a topic.", "labels": [], "entities": []}, {"text": "However, most current methods disregard semantic relations, in favor of statistical models of word distributions and frequencies.", "labels": [], "entities": []}, {"text": "This calls for the integra-tion of semantic models for topic labeling.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7224870920181274}]}, {"text": "Towards the solution of the mentioned problems, in this paper we focus on two novel contributions: 1.", "labels": [], "entities": []}, {"text": "We propose to generate topic labels using the extracted information by producing the most representative phrases for each text segment.", "labels": [], "entities": []}, {"text": "We perform this task in two steps.", "labels": [], "entities": []}, {"text": "First, we generalize some lexically diverse concepts in the extracted phrases.", "labels": [], "entities": []}, {"text": "Second, we aggregate and generate new phrases that can semantically imply more than one original extracted phrase.", "labels": [], "entities": []}, {"text": "For example, the phrase \"rats also giggle\" and \"horse laugh\" should be merged into anew phrase \"animals laugh\".", "labels": [], "entities": []}, {"text": "Although our method is still relying on extracting phrases, we move beyond current extractive approaches, by generating new phrases through generalization and aggregation of the extracted ones.", "labels": [], "entities": []}, {"text": "2. Building a multidirectional entailment graph over the extracted phrases to identify and select the relevant information.", "labels": [], "entities": []}, {"text": "We set such problem as an application-oriented variant of the Textual Entailment (TE) recognition task (), to identify the information that are semantically equivalent, novel, or more informative with respect to the content of the others.", "labels": [], "entities": [{"text": "Textual Entailment (TE) recognition task", "start_pos": 62, "end_pos": 102, "type": "TASK", "confidence": 0.7604808977672032}]}, {"text": "In this way, we prune the redundant and less informative text portions (e.g., phrases), and produce semantically informed phrases for the generation phase.", "labels": [], "entities": []}, {"text": "In the case of the example in, we eliminate phrases such as \"rats have\", \"rats\" and \"laugh\" while keeping \"animal play\", \"Horse laugh\" and \"rats also giggle\".", "labels": [], "entities": []}, {"text": "The experimental results over conversational data sets show that, in all cases, our approach outperforms other models significantly.", "labels": [], "entities": []}, {"text": "Although conversational data are known to be challenging, we choose to test our method on conversations because this is a genre in which topic modeling is critically needed, as conversations lack the structure and organization of, for instance, edited monologues.", "labels": [], "entities": []}, {"text": "The results indicate that our framework is sufficiently robust to deal with topic labeling in less structured, informal genres (when compared with edited monologues).", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.713917076587677}]}, {"text": "As an additional result of our experiments, we show that the identification and selection phase using semantic relations (entailment graph) is a necessary step to perform the final step (i.e., the phrase aggregation).", "labels": [], "entities": [{"text": "phrase aggregation", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.7046201080083847}]}], "datasetContent": [{"text": "To verify the effectiveness of our approach, we experiment with two different conversational datasets.", "labels": [], "entities": []}, {"text": "Our interest in dealing with conversational texts derives from two reasons.", "labels": [], "entities": []}, {"text": "First, the huge amount of textual data generated everyday in these conversations validates the need of text analysis frameworks to process such conversational texts effectively.", "labels": [], "entities": []}, {"text": "Second, conversational texts pose challenges to the traditional techniques, including redundancies, disfluencies, higher language variabilities and ill-formed sentence structure (.", "labels": [], "entities": []}, {"text": "Our conversational datasets are from two different asynchronous media: email and blog.", "labels": [], "entities": []}, {"text": "For email, we use the dataset presented in (, where three individuals annotated the publicly available BC3 email corpus () with topics.", "labels": [], "entities": [{"text": "BC3 email corpus", "start_pos": 103, "end_pos": 119, "type": "DATASET", "confidence": 0.8726074695587158}]}, {"text": "The corpus contains 40 email threads (or conversations) at an average of 5 emails per thread.", "labels": [], "entities": []}, {"text": "On average it has 26.3 sentences and 2.5 topics per thread.", "labels": [], "entities": []}, {"text": "A topic has an average length of 12.6 sentences.", "labels": [], "entities": []}, {"text": "In total, the three annotators found 269 topics in a corpus of 1,024 sentences.", "labels": [], "entities": []}, {"text": "There are no publicly available blog corpora annotated with topics.", "labels": [], "entities": []}, {"text": "For this study, we build our own blog corpus containing 20 blog conversations of various lengths from Slashdot, each annotated with topics by three human annotators.", "labels": [], "entities": []}, {"text": "The number of comments per conversation varies from 30 to 101 with an average of 60.3 and the number of sentences per conversation varies from 105 to 430 with an average of 220.6.", "labels": [], "entities": []}, {"text": "The annotators first read a conversation and list the topics discussed in the conversation by a short description (e.g., Game contents or size, Bugs or faults) which provides a high-level overview of the topic.", "labels": [], "entities": [{"text": "Bugs", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9485849738121033}]}, {"text": "Then, they assign the most appropriate topic to each sentence in the conversation.", "labels": [], "entities": []}, {"text": "The short high-level descriptions of the topics serve as reference (or gold) topic labels in our experiments.", "labels": [], "entities": []}, {"text": "The target number of topics was not given in advance and the annotators were instructed to find as many topics as needed to convey the overall content structure of the conversation.", "labels": [], "entities": []}, {"text": "The annotators found 5 to 23 topics per conversation with an average of 10.77.", "labels": [], "entities": []}, {"text": "The number of sentences per topic varies from 11.7 to 61.2 with an average of 27.16.", "labels": [], "entities": []}, {"text": "In total, the three annotators found 512 topics in our blog corpus containing 4,411 sentences overall.", "labels": [], "entities": []}, {"text": "Note that our annotators performed topic segmentation and labeling independently.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7560110688209534}]}, {"text": "In the email corpus, the three annotators found 100, 77 and 92 topics respectively (269 in total), and in the blog corpus, they found 251, 119 and 192 topics respectively (562 in total).", "labels": [], "entities": []}, {"text": "For the evaluation, there is a single gold standard per topic written by each annotator.", "labels": [], "entities": []}, {"text": "shows a casein which two annotators selected the same topical cluster and so we have two labels for the same cluster.", "labels": [], "entities": []}, {"text": "Traditionally, key phrase extraction is evaluated using precision, recall and f-measure based on exact matches on all the extracted key phrases with gold standards fora given text.", "labels": [], "entities": [{"text": "key phrase extraction", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7326739629109701}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9995717406272888}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9993042945861816}, {"text": "f-measure", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9946957230567932}]}, {"text": "However, as claimed by), this approach is not flexible enough as it ignores the near-misses.", "labels": [], "entities": []}, {"text": "Moreover, in the case of topic labeling, most of the human written topic labels cannot be found in the text.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.8419492840766907}]}, {"text": "Recently, () evaluated the utility of different n-gram-based metrics for key phrase extraction and showed that the metric R-precision correlates most with human judgments.", "labels": [], "entities": [{"text": "key phrase extraction", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.6037844121456146}]}, {"text": "R-precision normalizes the approximate matching score by the maximum number of words in the reference and candidate phrases.", "labels": [], "entities": []}, {"text": "Since this penalize our aggregation phase, where the phrases tend to be longer than original extracted phrase, we decide to use R-f1 as our evaluation metric which considers length of both reference and candidate phrases.", "labels": [], "entities": []}, {"text": "The metric described above only considers word overlap and ignores other semantic relations (e.g., synonymy, hypernymy) between words.", "labels": [], "entities": []}, {"text": "However, annotators write labels of their own and may use words that are not directly from the conversation but are semantically related.", "labels": [], "entities": []}, {"text": "Therefore, we propose to also use another variant of R-f1 that incorporates semantic relation between words.", "labels": [], "entities": []}, {"text": "To calculate the Semantic R-f1, we count the number of overlaps not only when they have the same form, but also when they are connected in WordNet with a synonymy, hypernymy, hyponymy and entailment relation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 139, "end_pos": 146, "type": "DATASET", "confidence": 0.9585185050964355}]}, {"text": "Its worth noting that the generalizations phase and the evaluation method are completely independent.", "labels": [], "entities": []}, {"text": "In the generalization step, we try to generalize the phrases which are automatically extracted from the text segments.", "labels": [], "entities": []}, {"text": "While, in the evaluation, we compare the human written gold standards with the system output.", "labels": [], "entities": []}, {"text": "Therefore, using WordNet in the generalization step does not bias the results in the evaluation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.9436347484588623}]}, {"text": "We conduct our experiments over the blog and email datasets described in Section 3.1, after eliminating the development set from the test datasets.", "labels": [], "entities": []}, {"text": "In our experiments, the development set was used for the pattern extraction and the shortest path threshold connecting the words in Wordnet in the generalization phase.", "labels": [], "entities": [{"text": "pattern extraction", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7112971097230911}, {"text": "Wordnet", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.9689247012138367}]}, {"text": "Our test dataset consists of 461 topics (i.e., clusters and their associated topic labels) from 20 blog conversations and 242 topics from 40 email conversations.", "labels": [], "entities": []}, {"text": "For preprocessing our dataset we use OpenNLP 6 for tokenization, part-of-speech tagging and chuncking.", "labels": [], "entities": [{"text": "OpenNLP 6", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9237163960933685}, {"text": "tokenization", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9617660641670227}, {"text": "part-of-speech tagging", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7199673503637314}]}, {"text": "For sense disambiguation, we use the extended gloss overlap measure with the window size of 5, developed by).", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6406262069940567}, {"text": "extended gloss overlap measure", "start_pos": 37, "end_pos": 67, "type": "METRIC", "confidence": 0.6741959601640701}]}, {"text": "We also apply Snowball algorithm) for stemming.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9107962846755981}, {"text": "stemming", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.9666413068771362}]}, {"text": "We compare our approach with two strong baselines.", "labels": [], "entities": []}, {"text": "The first baseline Freq-BL ranks the words according to their frequencies and select the top 5 candidates applying Maximum Marginal Relevance algorithm) using the same pre-and post-processing as the work by).", "labels": [], "entities": [{"text": "Maximum Marginal Relevance algorithm", "start_pos": 115, "end_pos": 151, "type": "METRIC", "confidence": 0.7568279206752777}]}, {"text": "The second baseline Lead-BL, ranks the words based on their relevance to the leading sentences.", "labels": [], "entities": []}, {"text": "The ranking criteria is log(tf w,Lt + 1) \u00d7 log(tf w,t + 1), where tf w,Lt and tf w,t are the number of times word w appears in a set of leading sentences Lt and topic cluster t, respectively).", "labels": [], "entities": []}, {"text": "The log expressions, as the ranking criterion, assign more weights to the words in the topic segment, that also appear in the leading sentences.", "labels": [], "entities": []}, {"text": "This is because topics tend to be introduced in the first few sentences of a topical cluster.", "labels": [], "entities": []}, {"text": "We also measure the performance of our framework at each step in order to compare the effectiveness of each phase independently or in combination.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Phrase merging patterns.", "labels": [], "entities": [{"text": "Phrase merging", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9583362638950348}]}, {"text": " Table 3: Successful examples of human-authored and system generated labels for blog and email datasets. The number  near some examples refers to the aggregation patterns in Table 2.", "labels": [], "entities": []}, {"text": " Table 4: Results for candidate topic labels on blog and  email corpora.", "labels": [], "entities": []}]}