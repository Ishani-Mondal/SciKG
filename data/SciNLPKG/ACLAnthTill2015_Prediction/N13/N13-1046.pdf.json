{"title": [{"text": "Dudley North visits North London: Learning When to Transliterate to Arabic", "labels": [], "entities": [{"text": "Dudley North visits North London", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.9622912406921387}, {"text": "Learning When to Transliterate to Arabic", "start_pos": 34, "end_pos": 74, "type": "TASK", "confidence": 0.6432457168896993}]}], "abstractContent": [{"text": "We report the results of our work on automating the transliteration decision of named entities for English to Arabic machine translation.", "labels": [], "entities": [{"text": "English to Arabic machine translation", "start_pos": 99, "end_pos": 136, "type": "TASK", "confidence": 0.6075038850307465}]}, {"text": "We construct a classification-based framework to automate this decision, evaluate our classifier both in the limited news and the diverse Wikipedia domains, and achieve promising accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9981318116188049}]}, {"text": "Moreover, we demonstrate a reduction of translation error and an improvement in the performance of an English-to-Arabic machine translation system .", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9452835917472839}, {"text": "error", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.43986615538597107}, {"text": "English-to-Arabic machine translation", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.6897422870000204}]}], "introductionContent": [{"text": "Translation of named entities (NEs) is important for NLP applications such as Machine Translation (MT) and Cross-lingual Information Retrieval.", "labels": [], "entities": [{"text": "Translation of named entities (NEs)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9127561279705593}, {"text": "Machine Translation (MT)", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8401498794555664}, {"text": "Cross-lingual Information Retrieval", "start_pos": 107, "end_pos": 142, "type": "TASK", "confidence": 0.7709614038467407}]}, {"text": "For MT, NEs are major subset of the out-of-vocabulary terms (OOVs).", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9847500324249268}]}, {"text": "Due to their diversity, they cannot always be found in parallel corpora, dictionaries or gazetteers.", "labels": [], "entities": []}, {"text": "Thus, state-of-the-art of MT needs to handle NEs in specific ways.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9767317771911621}]}, {"text": "For instance, in the English-Arabic automatic translation example given in, the noun \"North\" has been erroneously translated to \" /Al$mAlyp \" (indicating the north direction in English) instead of being transliterated to \" / nwrv\".", "labels": [], "entities": []}, {"text": "As shown in, direct translation of invocabulary terms could degrade translation quality.", "labels": [], "entities": []}, {"text": "Also blind transliteration of OOVs does not necessarily contribute to translation adequacy and may actually create noisy contexts for the language model and the decoder.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train two classifiers and tune their parameters using a held out development set of 500 NEs drawn randomly from the news parallel corpus.", "labels": [], "entities": [{"text": "news parallel corpus", "start_pos": 119, "end_pos": 139, "type": "DATASET", "confidence": 0.8500740130742391}]}, {"text": "We use 55k NEs from the same corpus to train the C news classifier.", "labels": [], "entities": []}, {"text": "Furthermore, we train the C diverse classifier cumulatively with the 55K news NEs and another 4600 NEs from Wikipedia titles.", "labels": [], "entities": []}, {"text": "The classifiers are evaluated on three different datasets: Test N ews which consists of 2K of NEs selected randomly from the news corpus, Test W iki consisting of 1K NEs extracted from the Wikipedia and Test Combination , an aggregation of the two previous sets.", "labels": [], "entities": []}, {"text": "We manually reviewed the labels of these test sets and fixed any incorrect labels.", "labels": [], "entities": []}, {"text": "compares the accuracy of the two classifiers under different training and test data settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9992908239364624}]}, {"text": "Starting with a majority class baseline, our classifiers achieve a promising performance inmost settings.", "labels": [], "entities": []}, {"text": "The majority class for both classifiers is the translation which performs as a baseline approach with an accuracy equal to the distribution of the two classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9989663362503052}]}, {"text": "We also  observe that the addition of a small diverse training set in C diverse provides a relatively large improvement (about 2%) when tested on Wikipedia.", "labels": [], "entities": []}, {"text": "Finally, illustrates the contribution of different classes of features on our diverse classifier (evaluated on Test W iki ).", "labels": [], "entities": []}, {"text": "We observe a fairly linear relationship between the size of the training data and the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9991649389266968}]}, {"text": "Furthermore, we observe that the features describing the category of the NE are more important than the token's local context.", "labels": [], "entities": []}, {"text": "For example, in the case of \"Dudley North\" and \"North London\", the most effective feature for the decision is the category of the named entities.", "labels": [], "entities": [{"text": "Dudley North\" and \"North London\"", "start_pos": 29, "end_pos": 61, "type": "DATASET", "confidence": 0.7868848331272602}]}, {"text": "We evaluate the effects of the classifier on an English to Arabic statistical MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.8627156615257263}]}, {"text": "Our first evaluation focuses on the utility of our classifier in preventing erroneous translation of NEs which need to be transliterated.", "labels": [], "entities": [{"text": "preventing erroneous translation of NEs", "start_pos": 65, "end_pos": 104, "type": "TASK", "confidence": 0.7169851541519165}]}, {"text": "In the following experiments we use C news classifier.", "labels": [], "entities": [{"text": "C news classifier", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.5670490066210429}]}, {"text": "In order to experiment with a diverse set of NEs, we conducted a study on a small corpus of Wikipedia articles from a diverse set of topics.", "labels": [], "entities": []}, {"text": "We use 10 Wikipedia articles describing: Anarchism, Artemis, Buddhism, Isfahan, Shawn Michaels, Turkey, etc.", "labels": [], "entities": []}, {"text": "We first use our classifier to locate the subset of NEs which should be transliterated.", "labels": [], "entities": []}, {"text": "An annotator validates the decision and examines the phrase table on the default MT decision on those NEs.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9177129864692688}]}, {"text": "We observe that out of 1031 NE tokens, 624 tokens (60.5%) which would have been translated incorrectly, are directed to the transliteration module.", "labels": [], "entities": []}, {"text": "Finally, we deploy the transliteration classifier as a pre-translation component to the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9498581886291504}]}, {"text": "Our MT test set is the MEDAR corpus ().", "labels": [], "entities": [{"text": "MT test set", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8200381994247437}, {"text": "MEDAR corpus", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.8975596129894257}]}, {"text": "The MEDAR corpus consists of about 10,000 words English texts on news related to the climate change with four Arabic reference translations.", "labels": [], "entities": [{"text": "MEDAR corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9060745239257812}]}, {"text": "Due to the lack of non-news English-Arabic corpus, we have to limit this experiment only to the news domain.", "labels": [], "entities": []}, {"text": "However, we expect that many of the NEs may already exist in the training corpus and the effects of the classifier is more limited than using a diverse domain like Wikipedia.", "labels": [], "entities": []}, {"text": "We automatically locate the NEs in the source language sentences and use the classifier to find those which should be transliterated.", "labels": [], "entities": []}, {"text": "For such terms, we offer the transliterated form as an option to the decoder aiming to improve the decoding process.", "labels": [], "entities": []}, {"text": "For that a human annotator selected the transliterations from the suggested list that is provided by the automatic transliterator (Maren) without any knowledge of the reference transliterations.", "labels": [], "entities": []}, {"text": "shows the impact of adding the classifier to the SMT pipeline with a modest improvement.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9746914505958557}]}, {"text": "Moreover, a bilingual annotator examined the automatically tagged NEs in the MT test set and labeled them with the translation vs. transliteration The baseline MT system is the MOSES phrase-based decoder () trained on a standard English-Arabic parallel corpus.", "labels": [], "entities": [{"text": "MT test set", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.8688202897707621}, {"text": "MT", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.9434192776679993}]}, {"text": "The 18 million parallel corpus consists of the non-UN parts of the NIST corpus distributed by the Linguistic Data Consortium.", "labels": [], "entities": [{"text": "NIST corpus distributed by the Linguistic Data Consortium", "start_pos": 67, "end_pos": 124, "type": "DATASET", "confidence": 0.8071693181991577}]}, {"text": "We perform the standard preprocessing and tokenization on the English side.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9683237671852112}]}, {"text": "We also use MADA+TOKAN () to preprocess and tokenize the Arabic side of the corpus.", "labels": [], "entities": [{"text": "TOKAN", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.5068990588188171}]}, {"text": "We use the standard setting of GIZA++ and the grow-diagonal-final heuristic of MOSES to get the word alignments.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.6738682985305786}, {"text": "word alignments", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.6594171673059464}]}, {"text": "We use a set of 500 sentences to tune the decoder parameters using the MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.615917444229126}]}, {"text": "We use El Kholy and Habash (2010) detokenization framework for the Arabic decoding.", "labels": [], "entities": []}, {"text": "We evaluate the MT system with the BLEU metric ().", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9500700235366821}, {"text": "BLEU metric", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9823925793170929}]}], "tableCaptions": [{"text": " Table 3: Accuracy results for the two classifiers and the  baseline on the three test datasets", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993808269500732}]}]}