{"title": [{"text": "A Cross-language Study on Automatic Speech Disfluency Detection", "labels": [], "entities": [{"text": "Automatic Speech Disfluency Detection", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7311494573950768}]}], "abstractContent": [{"text": "We investigate two systems for automatic dis-fluency detection on English and Mandarin conversational speech data.", "labels": [], "entities": [{"text": "dis-fluency detection", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.6773045808076859}]}, {"text": "The first system combines various lexical and prosodic features in a Conditional Random Field model for detecting edit disfluencies.", "labels": [], "entities": [{"text": "detecting edit disfluencies", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.811280886332194}]}, {"text": "The second system combines acoustic and language model scores for detecting filled pauses through constrained speech recognition.", "labels": [], "entities": [{"text": "detecting filled pauses through constrained speech recognition", "start_pos": 66, "end_pos": 128, "type": "TASK", "confidence": 0.7532663728509631}]}, {"text": "We compare the contributions of different knowledge sources to detection performance between these two languages .", "labels": [], "entities": [{"text": "detection", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.9570755958557129}]}], "introductionContent": [{"text": "Speech disfluencies are common phenomena in spontaneous speech.", "labels": [], "entities": [{"text": "Speech disfluencies", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6284169256687164}]}, {"text": "They consist of spoken words and phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance.", "labels": [], "entities": []}, {"text": "The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts.", "labels": [], "entities": [{"text": "parsing and other natural language processing", "start_pos": 110, "end_pos": 155, "type": "TASK", "confidence": 0.6088542143503824}]}, {"text": "There has been much research effort on automatic disfluency detection in recent years;;, particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN).", "labels": [], "entities": [{"text": "automatic disfluency detection", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6320837239424387}, {"text": "DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003)", "start_pos": 111, "end_pos": 250, "type": "TASK", "confidence": 0.6345404783884684}, {"text": "conversational telephone speech (CTS) and broadcast news (BN)", "start_pos": 397, "end_pos": 458, "type": "TASK", "confidence": 0.6728306909402212}]}, {"text": "However, the EARS MDE effort was focused on English only, and there hasn't been much research on the effectiveness of similar automatic disfluency detection approaches for multiple languages.", "labels": [], "entities": [{"text": "EARS MDE", "start_pos": 13, "end_pos": 21, "type": "TASK", "confidence": 0.582023024559021}, {"text": "automatic disfluency detection", "start_pos": 126, "end_pos": 156, "type": "TASK", "confidence": 0.7266592383384705}]}, {"text": "This paper presents three main innovations.", "labels": [], "entities": []}, {"text": "First, we extend the EARS MDE-style disfluency detection approach combining lexical and prosodic features using a Conditional Random Field (CRF) model, which was employed for detecting disfluency on English conversational speech data (), to Mandarin conversational speech, as presented in Section 2.", "labels": [], "entities": [{"text": "EARS MDE-style disfluency detection", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.6270832046866417}]}, {"text": "Second, we implement an automatic filled pause detection approach through constrained speech recognition, as presented in Section 3.", "labels": [], "entities": [{"text": "filled pause detection", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.6744625568389893}, {"text": "speech recognition", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7107749730348587}]}, {"text": "Third, for both disfluency detection systems, we compare side-by-side contributions of different knowledge sources to detection performance for two languages, English and Mandarin, as presented in Section 4.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7618652582168579}]}, {"text": "Conclusions appear in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Scoring of EARS MDE-style automatic disfluency detection output is done using the NIST tools 1 , computing the error rate as the average number of misclassified words per reference event word.", "labels": [], "entities": [{"text": "EARS MDE-style automatic disfluency detection output", "start_pos": 11, "end_pos": 63, "type": "TASK", "confidence": 0.6899848828713099}, {"text": "NIST tools 1", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9485494494438171}, {"text": "error rate", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9536315500736237}]}, {"text": "For English, the training and evaluation data were from the 40 hours CTS data in the NIST RT-04F MDE training data including speech, their transcriptions and disfluency annotations by LDC.", "labels": [], "entities": [{"text": "CTS", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9304597973823547}, {"text": "NIST RT-04F MDE training data", "start_pos": 85, "end_pos": 114, "type": "DATASET", "confidence": 0.9363649725914002}]}, {"text": "We randomly held out two 3-hour subsets from this training data set for evaluation and parameter tuning respectively, and used the remaining data for training.", "labels": [], "entities": [{"text": "training data set", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.8007224003473917}]}, {"text": "Note that for Mandarin, there is no LDC released Mandarin MDE training data.", "labels": [], "entities": [{"text": "LDC released Mandarin MDE training data", "start_pos": 36, "end_pos": 75, "type": "DATASET", "confidence": 0.8089328110218048}]}, {"text": "We adapted the English MDE annotation guidelines for Mandarin and manually annotated the manual transcripts of 92 Mandarin broadcast conversation (BC) shows released by LDC under the DARPA GALE program, for edit disfluencies and filler words.", "labels": [], "entities": []}, {"text": "We randomly held out two 3-hour subsets from the 92 shows for evaluation and parameter tuning respectively, and manually corrected disfluency annotation errors on the evaluation set.", "labels": [], "entities": []}, {"text": "shows the results in NIST error rate (%) for edit word, IP, and filler word detection.", "labels": [], "entities": [{"text": "NIST error rate", "start_pos": 21, "end_pos": 36, "type": "METRIC", "confidence": 0.6451130906740824}, {"text": "filler word detection", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.6011058588822683}]}, {"text": "We observe that adding POS features improves edit word, edit IP, and filler word detection for both languages, and adding a prosody model produced further improvement (note that filler word detection systems did not employ prosodic features).", "labels": [], "entities": [{"text": "filler word detection", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7186352809270223}, {"text": "filler word detection", "start_pos": 178, "end_pos": 199, "type": "TASK", "confidence": 0.7240786751111349}]}, {"text": "The gains from combining the word, POS, and prosody model over the word n-gram baseline are statistically significant for both languages (confidence level p < 0:05 using matched pair test).", "labels": [], "entities": []}, {"text": "Also, adding the prosody model over word+POS yielded a larger relative gain in edit word+IP detection performance for Mandarin than for English data.", "labels": [], "entities": [{"text": "edit word+IP detection", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.4841781973838806}]}, {"text": "A preliminary study of these results has shown that the prosody model contributes differently for different types of disfluencies for English and Mandarin conversational speech and we will continue this study in future work.", "labels": [], "entities": []}, {"text": "We also plan 1 www.itl.nist.gov/iad/mig/tests/rt/2004-fall/index.html to investigate the prosodic features considering the special characteristics of edited disfluencies in Mandarin studied in ().", "labels": [], "entities": []}, {"text": "For evaluating constrained speech recognition for FP detection, the English test set of conversational speech data and word transcripts is derived from the CTS subset of the NIST 2002 Rich Transcription evaluation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7609204053878784}, {"text": "FP detection", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.9958039820194244}, {"text": "English test set of conversational speech data", "start_pos": 68, "end_pos": 114, "type": "DATASET", "confidence": 0.789269323859896}, {"text": "NIST 2002 Rich Transcription evaluation", "start_pos": 174, "end_pos": 213, "type": "DATASET", "confidence": 0.9265825510025024}]}, {"text": "The waveforms were segmented according to utterance boundaries given by the humangenerated transcripts, resulting in 6554 utterance segments with a total duration of 6.8 hours.", "labels": [], "entities": []}, {"text": "We then excluded turns that have fewer than five tokens or have two or more FPs in a row (such as 'uh um' and 'uh, uh'), resulting in 3359 segments.", "labels": [], "entities": [{"text": "FPs", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9514994025230408}]}, {"text": "This yields the test set from which we computed English FP detection scores.", "labels": [], "entities": [{"text": "FP detection", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.6476162075996399}]}, {"text": "The transcripts of this test set contain 54511 non-FP words and 1394 FPs, transcribed as either uh or um.", "labels": [], "entities": []}, {"text": "When evaluating FP detection performance, these two orthographical forms were mapped to a single token type, so recognizing one form as the other is not penalized.", "labels": [], "entities": [{"text": "FP detection", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9933512806892395}]}, {"text": "The Mandarin test set is the DARPA GALE 2008 Mandarin speechto-text development test set of 1 hour duration.", "labels": [], "entities": [{"text": "Mandarin test set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7892379462718964}, {"text": "DARPA GALE 2008 Mandarin speechto-text development test set", "start_pos": 29, "end_pos": 88, "type": "DATASET", "confidence": 0.8734442889690399}]}, {"text": "The transcripts of this test set contain 9820 non-FP words and 370 FP words, transcribed as uh, mm, zhege, and nage.", "labels": [], "entities": []}, {"text": "We collapsed them to a single token type for FP scoring.", "labels": [], "entities": [{"text": "FP scoring", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.9093074202537537}]}, {"text": "We evaluated FP detection performance in terms of both false alarm (incorrect detection) and miss (failed detection) rates, shown in Table 2.", "labels": [], "entities": [{"text": "FP detection", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9529747068881989}, {"text": "miss (failed detection) rates", "start_pos": 93, "end_pos": 122, "type": "METRIC", "confidence": 0.9125198920567831}]}, {"text": "We observed that adding pronunciation scores didn't change the P fa and P miss . On the English test set, adding LM scores degraded P miss but improved P fa . However, on the Mandarin test set, increasing LM weight improved both P miss and P fa , suggesting that for the Mandarin LVCSR system in this study, the LM could provide complementary information to the AM to discriminate FP and non-FP words.", "labels": [], "entities": [{"text": "English test set", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.8374194502830505}, {"text": "Mandarin test set", "start_pos": 175, "end_pos": 192, "type": "DATASET", "confidence": 0.9366135994593302}]}], "tableCaptions": [{"text": " Table 1: NIST error rate (%) for edit word, IP, and filler  word detection on the English and Mandarin test set,  using word n-gram features, POS n-gram features, and  prosody model.", "labels": [], "entities": [{"text": "NIST error rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.6849715809027354}, {"text": "IP, and filler  word detection", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6359277764956156}, {"text": "English and Mandarin test set", "start_pos": 83, "end_pos": 112, "type": "DATASET", "confidence": 0.6630128741264343}]}, {"text": " Table 2: Probabilities of false alarms (FAs) and misses in  FP detection on the English and Mandarin test set w.r.t.", "labels": [], "entities": [{"text": "Probabilities of false alarms (FAs", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.757144033908844}, {"text": "misses", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.6067782640457153}, {"text": "FP detection", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.9745000600814819}, {"text": "English and Mandarin test set w.r.t", "start_pos": 81, "end_pos": 116, "type": "DATASET", "confidence": 0.7205899407466253}]}]}