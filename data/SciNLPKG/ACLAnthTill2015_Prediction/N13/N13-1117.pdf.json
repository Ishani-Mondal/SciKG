{"title": [{"text": "A Systematic Bayesian Treatment of the IBM Alignment Models", "labels": [], "entities": [{"text": "IBM Alignment", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.8677647411823273}]}], "abstractContent": [{"text": "The dominant yet ageing IBM and HMM word alignment models underpin most popular Statistical Machine Translation implementations in use today.", "labels": [], "entities": [{"text": "HMM word alignment", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.6412297189235687}, {"text": "Statistical Machine Translation", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.8271980881690979}]}, {"text": "Though beset by the limitations of implausible independence assumptions, intractable optimisation problems, and an excess of tunable parameters, these models provide a scalable and reliable starting point for inducing translation systems.", "labels": [], "entities": []}, {"text": "In this paper we build upon this venerable base by recasting these models in the non-parametric Bayesian framework.", "labels": [], "entities": []}, {"text": "By replacing the categorical distributions at their core with hierarchical Pitman-Yor processes, and through the use of collapsed Gibbs sampling, we provide a more flexible formulation and sidestep the original heuristic optimisation techniques.", "labels": [], "entities": []}, {"text": "The resulting models are highly extendible, naturally permitting the introduction of phrasal dependencies.", "labels": [], "entities": []}, {"text": "We present extensive experimental results showing improvements in both AER and BLEU when benchmarked against Giza++, including significant improvements over IBM model 4.", "labels": [], "entities": [{"text": "AER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9971824884414673}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9958406090736389}]}], "introductionContent": [{"text": "The IBM and HMM word alignment models) have underpinned the majority of statistical machine translation systems for almost twenty years.", "labels": [], "entities": [{"text": "HMM word alignment", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.6182816922664642}, {"text": "statistical machine translation", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.655476709206899}]}, {"text": "The key attraction of these models is their principled probabilistic formulation, and the existence of (mostly) tractable algorithms for their training.", "labels": [], "entities": []}, {"text": "The dominant Giza++ implementation of the IBM models) employs a variety of exact and approximate EM algorithms to optimise categorical alignment distributions.", "labels": [], "entities": []}, {"text": "While effective, this parametric approach results in a significant number of parameters to be tuned and intractable summations over the space of alignments for models 3 and 4.", "labels": [], "entities": []}, {"text": "Giza++ hides the hyperparameters with defaults and approximates the intractable expectations using restricted alignment neighbourhoods.", "labels": [], "entities": []}, {"text": "However this approach was shown to often return alignments with probabilities well below the true maxima.", "labels": [], "entities": []}, {"text": "To overcome perceived limitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.7930863499641418}]}, {"text": "While interesting results have been reported, these alternatives have failed to dislodge the IBM approach.", "labels": [], "entities": []}, {"text": "In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes -a mathematical tool which has been successfully applied to a range of language tasks).", "labels": [], "entities": []}, {"text": "In the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney ().", "labels": [], "entities": [{"text": "language modelling", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7334482371807098}]}, {"text": "The key contribution of the hierarchical PY formulation is that it provides a principle probabilistic framework that easily extends to latent variable models, such as those used for alignment, for which a Kneser-Ney formulation is unclear.", "labels": [], "entities": [{"text": "alignment", "start_pos": 182, "end_pos": 191, "type": "TASK", "confidence": 0.9608819484710693}]}, {"text": "While Bayesian priors have previously been applied to IBM model 1 (, in this work we go considerably further by implementing non-parametric priors for the full Giza++ training pipeline.", "labels": [], "entities": []}, {"text": "Inference for the proposed models and their hyper-parameters is done with Gibbs sampling.", "labels": [], "entities": []}, {"text": "This eliminates the intractable summations over alignments and the need for tuning hyperparameters.", "labels": [], "entities": []}, {"text": "Further, we exploit the highly extendible nature of the hierarchical PY process to implement improvements to the original models such as the introduction of phrasal dependencies.", "labels": [], "entities": []}, {"text": "We present extensive experimental results showing improvements in both BLEU scores and AER when compared to Giza++.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9812506437301636}, {"text": "AER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9993577599525452}]}, {"text": "The demonstrated improvements over IBM model 4 suggest that the heuristics used in the implementation of the EM algorithm for this model were suboptimal.", "labels": [], "entities": []}, {"text": "We begin with a formal presentation of the hierarchical PY process used in our Bayesian approach to replace the original categorical distributions.", "labels": [], "entities": []}, {"text": "Section 3 introduces our Bayesian formulation of the word alignment models, while its inference scheme is presented in the following section.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7598653137683868}]}, {"text": "Finally, the experimental results evaluating our models against the originals are given in section 5, demonstrating the superiority of the non-parametric approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++).", "labels": [], "entities": [{"text": "PY process alignment", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.6290810902913412}]}, {"text": "We evaluated the BLEU scores () of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9994934797286987}, {"text": "alignment error rates (AER)", "start_pos": 120, "end_pos": 147, "type": "METRIC", "confidence": 0.9291961987813314}]}, {"text": "Moses () was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT ( used for tuning of the weights, and SRILM to build the language model (5-grams based).", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9919666647911072}, {"text": "MERT", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9912866950035095}]}, {"text": "The corpus used for training and evaluation was the Chinese FBIS corpus.", "labels": [], "entities": [{"text": "Chinese FBIS corpus", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.9237594803174337}]}, {"text": "MT02 was used for tuning, and MT03 was used for evaluation.", "labels": [], "entities": [{"text": "MT02", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.936427652835846}, {"text": "tuning", "start_pos": 18, "end_pos": 24, "type": "TASK", "confidence": 0.9417219758033752}, {"text": "MT03", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8258523344993591}]}, {"text": "In each case we used one reference sentence in Chinese and 4 reference sentences in English.", "labels": [], "entities": []}, {"text": "Most translation systems rely on the Giza++ package in which the implementation of the original models is done by combining them in a pipeline.", "labels": [], "entities": []}, {"text": "Model 1 and the HMM alignment model are run sequentially each for 5 iterations; then models 3 and 4 are run sequentially for 3 iterations each.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7443764507770538}]}, {"text": "This follows the observation of that bootstrapping from previous results assists the fertility algorithms find the best alignment neighbourhood in order to estimate the expectations.", "labels": [], "entities": []}, {"text": "We assessed the proposed models against the original models in a pipeline experiment where both systems were trained on a corpus starting at model 1, and used the results of the previous run to initialise the next one -noting the BLEU scores and AER at each step.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 230, "end_pos": 241, "type": "METRIC", "confidence": 0.9833369553089142}, {"text": "AER", "start_pos": 246, "end_pos": 249, "type": "METRIC", "confidence": 0.9906131029129028}]}, {"text": "The Gibbs samplers for the pipelined PY-IBM models were run for 50 iterations for each model and started accumulating samples after a burn-in period of 10 iterations, each experiment was repeated three times and the results averaged.", "labels": [], "entities": []}, {"text": "As can be seen in figures 1 to 3, the pipelined PY-IBM models achieved higher BLEU scores across all steps, with the highest improvement of 1.6 percentage points in the pipelined HMM alignment models when translating    The alignment disagreement (the number of changed alignment positions between subsequent iterations) of the Chinese to English pipelined PY-IBM models (1 to 4) can be seen in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9995642304420471}]}, {"text": "This graph shows that each model in the pipeline reaches an alignment disagreement equilibrium after about 20 iterations, and that earlier models have greater initial deviation from their equilibrium than later models -which have an overall lower disagreement.", "labels": [], "entities": []}, {"text": "In order to assess the dependence of the fertility based models on the initialisation step another set of experiments was carried out.", "labels": [], "entities": []}, {"text": "The models were trained with a randomly initialised set of alignments and assessed after a set number of iterations for the Giza++ models (5 and 10 for the Giza++ HMM alignment model, and 3 and 10 for the Giza++ IBM model 4), or after 100 iterations with a burnin period of 10 iterations for the PY-IBM ones (we report the average of three runs for both models).", "labels": [], "entities": [{"text": "PY-IBM", "start_pos": 296, "end_pos": 302, "type": "DATASET", "confidence": 0.868876576423645}]}, {"text": "The results, reported in figures 4 to 6, show again that the PY-IBM model outperformed the Giza++ implementations, and to a large extent in the case of IBM model 4.", "labels": [], "entities": []}, {"text": "This provides further evidence that the supposition underlying the neighbourhood approximation for training models 3 and 4 -that there exists a small set of alignments on which most of the probability mass concentrates -is poor.", "labels": [], "entities": []}, {"text": "An interesting observation to note is that the BLEU score of the non-pipelined PY-IBM model 4 is the same as the PY-IBM HMM model translating in both directions, as opposed to an improvement in the pipelined case.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.980311393737793}]}, {"text": "This suggests that the sampler might not have fully converged after 100 iterations for model 4 (the number of alignment disagreements for this experiment can be seen in).", "labels": [], "entities": []}, {"text": "Further confirmation for this comes from the higher standard deviation of 0.54 observed for the PY-IBM model 4, as opposed to a standard deviation for the PY-IBM HMM model of 0.21 (which is still more significant than that of the pipelined PY-IBM model 4, whose standard deviation was 0.13).", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 52, "end_pos": 70, "type": "METRIC", "confidence": 0.9613401293754578}, {"text": "PY-IBM model 4", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9228948752085367}, {"text": "PY-IBM HMM model", "start_pos": 155, "end_pos": 171, "type": "DATASET", "confidence": 0.8741001486778259}]}, {"text": "Both the PY-IBM and the Giza++ trained models run in a linear time in the number of sentences, where due to the nature of MCMC sampling techniques, more iterations are required for its convergence.", "labels": [], "entities": [{"text": "PY-IBM", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.8859894871711731}]}, {"text": "In our experiments, the running time of the unoptimised Gibbs sampler was 50 times slower than the optimised EM.", "labels": [], "entities": []}], "tableCaptions": []}