{"title": [], "abstractContent": [{"text": "Measuring term informativeness is a fundamental NLP task.", "labels": [], "entities": []}, {"text": "Existing methods, mostly based on statistical information in corpora, do not actually measure informativeness of a term with regard to its semantic context.", "labels": [], "entities": []}, {"text": "This paper proposes anew lightweight feature-free approach to encode term informativeness in context by leveraging web knowledge.", "labels": [], "entities": []}, {"text": "Given a term and its context, we model context-aware term informativeness based on semantic similarity between the context and the term's most featured context in a knowledge base, Wikipedia.", "labels": [], "entities": []}, {"text": "We apply our method to three applications: core term extraction from snippets (text segment), scientific keywords extraction (paper), and back-of-the-book index generation (book).", "labels": [], "entities": [{"text": "core term extraction from snippets (text segment)", "start_pos": 43, "end_pos": 92, "type": "TASK", "confidence": 0.8053851591216193}, {"text": "scientific keywords extraction (paper)", "start_pos": 94, "end_pos": 132, "type": "TASK", "confidence": 0.7529357274373373}, {"text": "back-of-the-book index generation (book)", "start_pos": 138, "end_pos": 178, "type": "TASK", "confidence": 0.7277419517437617}]}, {"text": "The performance is state-of-the-art or close to it for each application, demonstrating its effectiveness and generality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computationally measuring importance of a word in text, or \"term informativeness\", is fundamental to many NLP tasks such as keyword extraction, text categorization, clustering, and summarization, etc.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7273510098457336}, {"text": "summarization", "start_pos": 181, "end_pos": 194, "type": "TASK", "confidence": 0.94487065076828}]}, {"text": "Various features derived from statistical and linguistic information can be helpful in encoding term informativeness, whereas practical feature definition and selection are usually ad hoc, data-driven and application dependent.", "labels": [], "entities": [{"text": "encoding term informativeness", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.8647756179173788}]}, {"text": "Statistical information based on term frequency (TF) and document frequency (DF) tend to be more effective in finding keywords in large corpora, but can have issues with small amounts of text or small corpora.", "labels": [], "entities": [{"text": "term frequency (TF)", "start_pos": 33, "end_pos": 52, "type": "METRIC", "confidence": 0.8690152645111084}, {"text": "document frequency (DF)", "start_pos": 57, "end_pos": 80, "type": "METRIC", "confidence": 0.8468937695026397}]}, {"text": "Linguistic information such as POS tag patterns often require manual selection based on prior applications.", "labels": [], "entities": []}, {"text": "We contend that few methods actually measure the informativeness of a term to the discourse unit it contains.", "labels": [], "entities": []}, {"text": "For example, given a context such as \"A graph comprises nodes (also called vertices) connected by links (also known as edges or arcs)\", it is difficult to measure the term informativeness of \"graph\", \"nodes\", or \"links\" based on any statistical or linguistic information.", "labels": [], "entities": []}, {"text": "Is there a fundamental and less ad hoc way to measure the term informativeness of a word within a discourse unit?", "labels": [], "entities": []}, {"text": "Can we actually find a general approach based on comprehensive and high-level \"knowledge\" and not have to nitpick over features?", "labels": [], "entities": []}, {"text": "Can this new metric be effectively applied to real world applications?", "labels": [], "entities": []}, {"text": "To answer these questions, we develop anew term informativeness metric, motivated by query-document relevance in information retrieval.", "labels": [], "entities": []}, {"text": "The higher the relevance score a query-document pair is, the more informative the query is to the document.", "labels": [], "entities": [{"text": "relevance", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9469885230064392}]}, {"text": "If a similar principle also exists between word and context and there is an effective search engine returning ranked contexts fora given word, then we contend that word is more informative in the higher rank contexts.", "labels": [], "entities": []}, {"text": "To seethe term informativeness of three words \"graph\", \"nodes\" and \"links\" in context, we manually check the search results from Wikipedia, Google, and Bing.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.9487585425376892}]}, {"text": "We found that very similar contexts are among the top 5 ranked results of \"graph\" while no such contexts appear in that of the other two words.", "labels": [], "entities": []}, {"text": "Thus, we define a context-aware term informativeness based on the semantic relatedness between the context and the term's featured contexts (or the top important contexts that cover most of a term's semantics).", "labels": [], "entities": []}, {"text": "We apply the context-aware term informativeness (CTI) to three typical NLP applications: core term extraction in snippets, keyword extraction and backof-the-book index generation.", "labels": [], "entities": [{"text": "core term extraction in snippets", "start_pos": 89, "end_pos": 121, "type": "TASK", "confidence": 0.7230904757976532}, {"text": "keyword extraction", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7954528629779816}, {"text": "backof-the-book index generation", "start_pos": 146, "end_pos": 178, "type": "TASK", "confidence": 0.5230078796545664}]}, {"text": "Experiments show that the method is effective and efficient.", "labels": [], "entities": []}, {"text": "Moreover, the metric can be easily combined with other methods, or as a feature for learning algorithms.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the literature of term informativeness measurements.", "labels": [], "entities": []}, {"text": "Section 3 proposes the formal definition of the context-aware term informativeness as well as its practical implementation using Web knowledge.", "labels": [], "entities": []}, {"text": "Section 4 studies the three applications.", "labels": [], "entities": []}, {"text": "Finally, we conclude with discussion and future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on computer science term extrac- tion from descriptive snippets", "labels": [], "entities": []}, {"text": " Table 3: Results on Wiki20 and citeulike180", "labels": [], "entities": [{"text": "Wiki20", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.9108936190605164}]}, {"text": " Table 4: Results on SemEval2010", "labels": [], "entities": [{"text": "SemEval2010", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9428508281707764}]}, {"text": " Table 6: Average recall(%) comparisons as the output index size increases", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9588840007781982}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.8447691202163696}]}]}