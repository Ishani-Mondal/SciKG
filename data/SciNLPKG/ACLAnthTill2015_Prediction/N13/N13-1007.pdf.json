{"title": [{"text": "Minimally Supervised Method for Multilingual Paraphrase Extraction from Definition Sentences on the Web", "labels": [], "entities": [{"text": "Multilingual Paraphrase Extraction from Definition Sentences on the Web", "start_pos": 32, "end_pos": 103, "type": "TASK", "confidence": 0.8234427041477628}]}], "abstractContent": [{"text": "We propose a minimally supervised method for multilingual paraphrase extraction from definition sentences on the Web.", "labels": [], "entities": [{"text": "paraphrase extraction from definition sentences", "start_pos": 58, "end_pos": 105, "type": "TASK", "confidence": 0.8374578833580018}]}, {"text": "(2011) extracted paraphrases from Japanese definition sentences on the Web, assuming that definition sentences defining the same concept tend to contain paraphrases.", "labels": [], "entities": []}, {"text": "However, their method requires manually annotated data and is language dependent.", "labels": [], "entities": []}, {"text": "We extend their framework and develop a minimally supervised method applicable to multiple languages.", "labels": [], "entities": []}, {"text": "Our experiments show that our method is comparable to Hashimoto et al.'s for Japanese and outperforms previous unsu-pervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese.", "labels": [], "entities": [{"text": "precision", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.9982703924179077}, {"text": "precision", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.9984649419784546}, {"text": "precision", "start_pos": 287, "end_pos": 296, "type": "METRIC", "confidence": 0.9988980293273926}]}], "introductionContent": [{"text": "Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (;;).", "labels": [], "entities": []}, {"text": "We propose a minimally supervised method for multilingual paraphrase extraction.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.74532650411129}]}, {"text": "developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases.", "labels": [], "entities": []}, {"text": "Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal a.", "labels": [], "entities": []}, {"text": "Paraphrasing is the use of your own words to express the author's ideas without changing the meaning. b. Paraphrasing is defined as a process of transforming an expression into another while keeping its meaning intact.", "labels": [], "entities": []}, {"text": "(Paraphrasing refers to the replacement of an expression into another without changing the semantic content.) b. (Paraphrasing is a process of transforming an expression into another of the same language while preserving the meaning and content as much as possible.) paraphrases from the definition sentences.", "labels": [], "entities": []}, {"text": "Both steps require supervised classifiers trained by manually annotated data, and heavily depend on their target language.", "labels": [], "entities": []}, {"text": "However, the basic idea is actually language-independent.", "labels": [], "entities": []}, {"text": "gives examples of definition sentences on the Web that define the same concept in English, Japanese, and Chinese (with English translation).", "labels": [], "entities": []}, {"text": "As indicated by underlines, each definition pair has a phrasal paraphrase.", "labels": [], "entities": []}, {"text": "We aim at extending Hashimoto et al.'s method to a minimally supervised method, thereby enabling acquisition of phrasal paraphrases within one language, but in different languages without manually annotated data.", "labels": [], "entities": []}, {"text": "The first contribution of our work is to develop a minimally supervised method for multilingual definition extraction that uses a classifier distinguishing definition from non-definition.", "labels": [], "entities": [{"text": "multilingual definition extraction", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.6807040770848592}]}, {"text": "The classifier is learnt from the first sentences in  Wikipedia articles, which can be regarded as the definition of the title of Wikipedia article and hence can be used as positive examples.", "labels": [], "entities": []}, {"text": "Our method relies on a POS tagger, a dependency parser, a NER tool, noun phrase chunking rules, and frequency thresholds for each language, in addition to Wikipedia articles, which can be seen as a manually annotated knowledge base.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.694156214594841}, {"text": "noun phrase chunking", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.6533529261747996}]}, {"text": "However, our method needs no additional manual annotation particularly for this task and thus we categorize our method as a minimally supervised method.", "labels": [], "entities": []}, {"text": "On the other hand, Hashimoto et al.'s method heavily depends on the properties of Japanese like the assumption that characteristic expressions of definition sentences tend to appear at the end of sentence in Japanese.", "labels": [], "entities": []}, {"text": "We show that our method is applicable to English, Japanese, and Chinese, and that its performance is comparable to state-of-the-art supervised methods . Since the three languages are very different we believe that our definition extraction method is applicable to any language as long as Wikipedia articles of the language exist.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 218, "end_pos": 239, "type": "TASK", "confidence": 0.8035299479961395}]}, {"text": "The second contribution of our work is to develop a minimally supervised method for multilingual paraphrase extraction from definition sentences.", "labels": [], "entities": [{"text": "multilingual paraphrase extraction from definition sentences", "start_pos": 84, "end_pos": 144, "type": "TASK", "confidence": 0.7632323205471039}]}, {"text": "Again, Hashimoto et al.'s method utilizes a supervised classifier trained with annotated data particularly prepared for this task.", "labels": [], "entities": []}, {"text": "We eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts.", "labels": [], "entities": []}, {"text": "Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods and is comparable to Hashimoto et al.'s supervised method for Japanese.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7807201445102692}]}, {"text": "Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach () and a parallel corpus based approach (;).", "labels": [], "entities": [{"text": "paraphrase (and entailment) extraction", "start_pos": 21, "end_pos": 59, "type": "TASK", "confidence": 0.8596989909807841}]}, {"text": "The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs ().", "labels": [], "entities": []}, {"text": "The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive.", "labels": [], "entities": []}, {"text": "As with, our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus.", "labels": [], "entities": []}, {"text": "However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automatically collect definition pairs from the Web on a large scale.", "labels": [], "entities": []}, {"text": "The difference between ours and Hashimoto et al.'s is that our method requires no manual labeling of data and is mostly language-independent.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted definitions from 10% of the Web corpus.", "labels": [], "entities": [{"text": "Web corpus", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.903779923915863}]}, {"text": "We applied Proposed def to the corpus of each language, and the state-of-the-art supervised method for Japanese (Hashimoto et al., 2011) (Hashi def , hereafter) to the Japanese corpus.", "labels": [], "entities": []}, {"text": "Hashi def was trained on their training data that consisted of 2,911 sentences, 61.1% of which were definitions.", "labels": [], "entities": []}, {"text": "Note that we removed sentences in TrDat from 10% of the Web corpus in advance, while we did not remove Hashimoto et al.'s training data from the corpus.", "labels": [], "entities": [{"text": "Web corpus", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.8330550789833069}]}, {"text": "This means that, for Hashi def , the training data is included in the test data.", "labels": [], "entities": [{"text": "Hashi def", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.8128088712692261}]}, {"text": "For each method, we filtered out its positive outputs whose defined term appeared more than 1,000 times in 10% of the Web corpus, since those terms tend to be too vague to be a defined term or refer to an entity outside the definition sentence.", "labels": [], "entities": [{"text": "Web corpus", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.8284983336925507}]}, {"text": "For example, if \"the college\" appears more than 1,000 times in 10% of the corpus, we filter out sentences like \"The college is one of three colleges in the Coast Community College District and was founded in 1947.\"", "labels": [], "entities": [{"text": "Coast Community College District", "start_pos": 156, "end_pos": 188, "type": "DATASET", "confidence": 0.933999702334404}]}, {"text": "For Proposed def , the number of remaining positive outputs is 3,216,121 for English, 651,293 for Japanese, and 682,661 for Chinese.", "labels": [], "entities": []}, {"text": "For Hashi def , the number of positive outputs is 523,882.", "labels": [], "entities": [{"text": "Hashi def", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.81305330991745}]}, {"text": "For Proposed def of each language, we randomly sampled 200 sentences from the remaining positive outputs.", "labels": [], "entities": []}, {"text": "For Hashi def , we first sorted its output by the SVM score in descending order and then randomly sampled 200 from the top 651,293, i.e., the same number as the remaining positive outputs of Proposed def of Japanese, out of all the remaining sentences of Hashi def . For each language, after shuffling all the samples, two human annotators evaluated each sample.", "labels": [], "entities": []}, {"text": "The annotators for English and Japanese were not the authors, while one of the Chinese annotators was one of the authors.", "labels": [], "entities": []}, {"text": "We regarded a sample as a definition if it was regarded as a definition by both annotators.", "labels": [], "entities": []}, {"text": "Cohen's kappa) was 0.55 for English (moderate agreement), 0.73 for Japanese (substantial agreement), and 0.69 for Chinese (substantial agreement).", "labels": [], "entities": [{"text": "kappa)", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9548870921134949}]}, {"text": "For English, Proposed def achieved 70% precision for the 200 samples.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9995056390762329}]}, {"text": "For Japanese, Proposed def achieved 62.5% precision for the 200 samples, while Hashi def achieved 70% precision for the 200 samples.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9992613196372986}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9989816546440125}]}, {"text": "For Chinese, Proposed def achieved 67% precision for the 200 samples.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9995574355125427}]}, {"text": "From these results, we conclude that Proposed def can extract a large number of definition sentences from the Web moderately well for the three languages.", "labels": [], "entities": []}, {"text": "Although the precision is not very high, our experiments in the next section show that we can still extract a large number of paraphrases with high precision from these definition sentences, due mainly to our similarity measures, localSim and globalSim.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9993574023246765}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9502833485603333}]}, {"text": "We show (1) that our paraphrase extraction method outperforms unsupervised methods for the three languages, (2) that globalSim is effective, and (3) that our method is comparable to the state-of-the-art su-.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7499623596668243}]}, {"text": "We assume that Moses should extract a set of two phrases that are paraphrases of each other, if we input monolingual parallel sentence pairs like our definition pairs.", "labels": [], "entities": []}, {"text": "We used default values for all the parameters.", "labels": [], "entities": []}, {"text": "Outputs are ranked by the product of two phrase translation probabilities of both directions., was set to 2 since our target was phrasal paraphrases.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7023653239011765}]}, {"text": "We extracted paraphrases from definition sentences in Pos and those extracted by Proposed def in Section 3.1.3.", "labels": [], "entities": []}, {"text": "First we coupled two definition sentences whose defined term was the same.", "labels": [], "entities": []}, {"text": "The number of definition pairs was 3,208,086 for English, 742,306 for Japanese, and 457,233 for Chinese.", "labels": [], "entities": []}, {"text": "Then we evaluated six methods in.", "labels": [], "entities": []}, {"text": "All the methods except P&D took the same definition pairs as input, while P&D's input was 10% of the Web corpus.", "labels": [], "entities": [{"text": "Web corpus", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.8904552459716797}]}, {"text": "The input can be seen as the same for all the methods, since the definition pairs were derived from that 10% of the Web corpus.", "labels": [], "entities": [{"text": "Web corpus", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.8006983995437622}]}, {"text": "In our experiments Exp1 and Exp2 below, all evaluation samples were shuffled so that human annotators could not know which sample was from which method.", "labels": [], "entities": []}, {"text": "Annotators were the same as those who conducted the evaluation in Section 3.1.3.", "labels": [], "entities": [{"text": "Annotators", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9465904831886292}]}, {"text": "Cohen's kappa) was 0.83 for English, 0.88 for Japanese, We filtered out phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore, Proposed local , SMT, and P&D, since most of them were not paraphrases.", "labels": [], "entities": []}, {"text": "We used Stanford NER () for English named entity recognition (NER), KNP for Japanese NER, and BaseNER ( for Chinese NER.", "labels": [], "entities": [{"text": "Stanford NER", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.8719151020050049}, {"text": "English named entity recognition (NER)", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.7027353942394257}, {"text": "KNP", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9521138668060303}, {"text": "BaseNER", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9316290616989136}]}, {"text": "Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of), and thus we did not apply the filter to them any further. and 0.85 for Chinese, all of which indicated reasonably good.", "labels": [], "entities": []}, {"text": "We regarded a candidate phrase pair as a paraphrase if both annotators regarded it as a paraphrase.", "labels": [], "entities": []}, {"text": "Exp1 We compared the methods that take definition pairs as input, i.e. Proposed Score , Proposed local , Hashi sup , Hashi uns , and SMT.", "labels": [], "entities": []}, {"text": "We randomly sampled 200 phrase pairs from the top 10,000 for each method for evaluation.", "labels": [], "entities": []}, {"text": "The evaluation of each candidate phrase pair (p 1 , p 2 ) was based on bidirectional checking of entailment relation, p 1 \u2192 p 2 and p 2 \u2192 p 1 , with p 1 and p 2 embedded in contexts, as did.", "labels": [], "entities": []}, {"text": "Entailment relation of both directions hold if (p 1 , p 2 ) is a paraphrase.", "labels": [], "entities": [{"text": "Entailment", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9782692790031433}]}, {"text": "We used definition pairs from which candidate phrase pairs were extracted as contexts.", "labels": [], "entities": []}, {"text": "Exp2 We compared Proposed Score and P&D.", "labels": [], "entities": [{"text": "Proposed Score", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.8018822371959686}, {"text": "P&D", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.7467660506566366}]}, {"text": "Since P&D restricted its output to phrase pairs in which each phrase consists of two to four words, we restricted the output of Proposed Score to 2-to-4-words phrase pairs, too.", "labels": [], "entities": []}, {"text": "We randomly sampled 200 from the top 3,000 phrase pairs from each method for evaluation, and the annotators checked entailment relation of both directions between two phrases using Web sentence pairs that contained the two phrases as contexts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Values of frequency threshold.", "labels": [], "entities": []}, {"text": " Table 5: Definition classification results on T rDat wcl .", "labels": [], "entities": [{"text": "Definition classification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7793313562870026}, {"text": "T rDat wcl", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.8720468680063883}]}]}