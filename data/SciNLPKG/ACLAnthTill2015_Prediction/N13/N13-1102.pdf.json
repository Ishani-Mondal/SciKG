{"title": [{"text": "Disfluency Detection Using Multi-step Stacked Learning", "labels": [], "entities": [{"text": "Disfluency Detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9640301167964935}]}], "abstractContent": [{"text": "In this paper, we propose a multi-step stacked learning model for disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.8801442682743073}]}, {"text": "Our method incorporates refined n-gram features step by step from different word sequences.", "labels": [], "entities": []}, {"text": "First, we detect filler words.", "labels": [], "entities": []}, {"text": "Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text.", "labels": [], "entities": []}, {"text": "In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection.", "labels": [], "entities": [{"text": "word detection", "start_pos": 154, "end_pos": 168, "type": "TASK", "confidence": 0.7289861589670181}]}, {"text": "We use Max-Margin Markov Networks (M 3 Ns) as the classifier with the weighted hamming loss to balance precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.999392032623291}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9976609945297241}]}, {"text": "Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and M 3 Ns with weighted hamming loss can significantly improve the performance.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.8585788011550903}]}, {"text": "Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9659824967384338}, {"text": "F-score", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.997817873954773}]}], "introductionContent": [{"text": "Detecting disfluencies in spontaneous speech can be used to cleanup speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules.", "labels": [], "entities": []}, {"text": "There are two types of disfluencies: filler words including filled pauses (e.g., 'uh', 'um') and discourse markers (e.g., 'I mean', 'you know'), and edited words that are repeated, discarded, or corrected by  Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., \"uh\", \"you know\", \"I mean\"), hence our work focuses on edited word detection.", "labels": [], "entities": [{"text": "Automatic filler word detection", "start_pos": 209, "end_pos": 240, "type": "TASK", "confidence": 0.6021283492445946}, {"text": "edit detection", "start_pos": 268, "end_pos": 282, "type": "TASK", "confidence": 0.747319221496582}, {"text": "edited word detection", "start_pos": 377, "end_pos": 398, "type": "TASK", "confidence": 0.7022172411282858}]}, {"text": "Many models have been evaluated for this task.", "labels": [], "entities": []}, {"text": "used Conditional Random Fields (CRFs) for sentence boundary and edited word detection.", "labels": [], "entities": [{"text": "word detection", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.7293856292963028}]}, {"text": "They showed that CRFs significantly outperformed Maximum Entropy models and HMMs.", "labels": [], "entities": []}, {"text": "proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier).", "labels": [], "entities": []}, {"text": "extended this model using minimal expected F-loss oriented n-best reranking.", "labels": [], "entities": [{"text": "F-loss", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9553449749946594}]}, {"text": "They obtained the best reported F-score of 83.8% on the Switchboard corpus.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9993120431900024}, {"text": "Switchboard corpus", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.9541845917701721}]}, {"text": "Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints.", "labels": [], "entities": []}, {"text": "From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work;).", "labels": [], "entities": []}, {"text": "trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage.", "labels": [], "entities": []}, {"text": "They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset.", "labels": [], "entities": [{"text": "F-score", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9995346069335938}, {"text": "edited word detection", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.6837826172510783}, {"text": "Switchboard development dataset", "start_pos": 104, "end_pos": 135, "type": "DATASET", "confidence": 0.8715527852376302}]}, {"text": "In this paper, we propose a multi-step stacked learning approach for disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.9013461172580719}]}, {"text": "In our method, we first perform filler word detection, then edited word detection.", "labels": [], "entities": [{"text": "filler word detection", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7039376099904379}, {"text": "edited word detection", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.6460733215014139}]}, {"text": "In every step, we generate new refined n-gram features based on the processed text (remove the detected filler or edited words from the previous step), and use these in the next step.", "labels": [], "entities": []}, {"text": "We also include anew type of features, called inbetween features, and incorporate them into the last step.", "labels": [], "entities": []}, {"text": "For edited word detection, we use Max-Margin Markov Networks (M 3 Ns) with weighted hamming loss as the classifier, as it can well balance the precision and recall to achieve high performance.", "labels": [], "entities": [{"text": "word detection", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7394792586565018}, {"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9988934397697449}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9980008006095886}]}, {"text": "On the commonly used Switchboard corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.8557574152946472}, {"text": "edit disfluency detection", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.8523208896319071}]}], "datasetContent": [{"text": "We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 11, "end_pos": 29, "type": "DATASET", "confidence": 0.7807866930961609}]}, {"text": "We also remove the partial words and punctuation from the training and test data for the reason to simulate the situation when speech recognizers are used and such kind of information is not available).", "labels": [], "entities": []}, {"text": "We tuned the weight matrix for hamming loss on the development dataset using simple grid search.", "labels": [], "entities": [{"text": "hamming", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9131227731704712}]}, {"text": "The diagonal elements are fixed at 0; for false positive errors, O \u2192 \u00b7E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, \u00b7E \u2192 O, we tried the weight from 1 to 3, and increased the weight 0.5 each time.", "labels": [], "entities": []}, {"text": "The optimal weight matrix is shown in.", "labels": [], "entities": []}, {"text": "Note that we use five labels in the sequence labeling task; however, for edited word detection evaluation, it is only a binary task, that is, all of the words labeled with \u00b7E will be mapped to the class of edited words.", "labels": [], "entities": [{"text": "edited word detection evaluation", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.7472821325063705}]}], "tableCaptions": [{"text": " Table 5: Effect of training strategy and recovered features  for stacked learning. F scores are reported. AP = Aver- aged Perceptron, PA = online Passive Aggresive, M 3 N =  un-weighted M 3 Ns, w. M 3 N = weighted M 3 Ns.", "labels": [], "entities": [{"text": "F", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9943945407867432}, {"text": "AP", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9927240014076233}]}, {"text": " Table 6. We  achieved the best F-score. The most competitive  system is (", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9981222748756409}]}, {"text": " Table 6: Comparison with other systems.  \u2020 they used  the re-segmented Switchboard corpus, which is not ex- actly the same as ours. \u22c6 they reported the F-score of  BE tag (beginning of the edited sequences). + they used  language model learned from 3 additional corpora.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 72, "end_pos": 90, "type": "DATASET", "confidence": 0.7781120240688324}, {"text": "F-score", "start_pos": 153, "end_pos": 160, "type": "METRIC", "confidence": 0.986646294593811}, {"text": "BE tag", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.8892618119716644}]}]}