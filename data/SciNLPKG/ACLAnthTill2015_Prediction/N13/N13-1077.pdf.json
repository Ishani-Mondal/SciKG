{"title": [{"text": "Zipfian corruptions for robust POS tagging", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.7689163088798523}]}], "abstractContent": [{"text": "Inspired by robust generalization and adver-sarial learning we describe a novel approach to learning structured perceptrons for part-of-speech (POS) tagging that is less sensitive to domain shifts.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 128, "end_pos": 156, "type": "TASK", "confidence": 0.6900456190109253}]}, {"text": "The objective of our method is to minimize average loss under random distribution shifts.", "labels": [], "entities": []}, {"text": "We restrict the possible target distributions to mixtures of the source distribution and random Zipfian distributions.", "labels": [], "entities": []}, {"text": "Our algorithm is used for POS tagging and evaluated on the English Web Treebank and the Danish Dependency Treebank with an average 4.4% error reduction in tagging accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.7962827980518341}, {"text": "English Web Treebank", "start_pos": 59, "end_pos": 79, "type": "DATASET", "confidence": 0.9501243233680725}, {"text": "Danish Dependency Treebank", "start_pos": 88, "end_pos": 114, "type": "DATASET", "confidence": 0.9093331495920817}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9571367502212524}]}], "introductionContent": [{"text": "Supervised learning approaches have advanced the state of the art on a variety of tasks in natural language processing, often resulting in systems approaching the level of inter-annotator agreement on in-domain data, e.g. in POS tagging, where report a tagging accuracy of 97.3%.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 225, "end_pos": 236, "type": "TASK", "confidence": 0.666104719042778}, {"text": "accuracy", "start_pos": 261, "end_pos": 269, "type": "METRIC", "confidence": 0.9845271110534668}]}, {"text": "However, performance of state-of-the-art supervised systems is known to drop considerably on out-ofdomain data.", "labels": [], "entities": []}, {"text": "State-of-the-art POS taggers trained on the Penn Treebank () mapped to Google's universal tag set () achieve tagging accuracies in the range of 89-91% on Web 2.0 data . To bridge this gap we may consider using semisupervised or transfer learning methods to adjust to new target domains;, pooling unlabeled data from those domains.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.7297520935535431}, {"text": "Penn Treebank", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9955829977989197}]}, {"text": "However, in many applications this is not possible.", "labels": [], "entities": []}, {"text": "If we want to provide an online service or design apiece of software with many potential users covering a wide range of use cases, we do not know the target domain in advance.", "labels": [], "entities": []}, {"text": "This is the usual problem of robust learning, but in this paper we describe a novel learning algorithm that goes beyond robust learning by making various assumptions about the difference between the source domain and the (unknown) target domain.", "labels": [], "entities": []}, {"text": "Under these assumptions we can minimize average loss under (all possible or a representative sample of) domain shifts.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two recently introduced cross-domain POS tagging datasets.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.7612968385219574}]}, {"text": "Our approach is inspired by work in robust generalization and adversarial learning.", "labels": [], "entities": []}, {"text": "Our approach also bears similarities to feature bagging ().", "labels": [], "entities": []}, {"text": "noted that in learning of linear models useful features are often swamped by correlating, but more indicative features.", "labels": [], "entities": []}, {"text": "If the more indicative features are absent in the target domain due to out-of-vocabulary (OOV) effects, we are left with the swamped features which were not updated properly.", "labels": [], "entities": []}, {"text": "This is, indirectly, the problem solved in adversarial learning with corrupted data points.", "labels": [], "entities": []}, {"text": "Adversarial learning can also be seen as away of averaging exponentially many models ().", "labels": [], "entities": []}, {"text": "Adversarial learning techniques have been developed for security-related learning tasks, e.g. where systems need to be robust to failing sensors.", "labels": [], "entities": []}, {"text": "We also show how we can do better than straight-forward ap-plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features.", "labels": [], "entities": []}, {"text": "Similar assumptions have been made before in computational linguistics, e.g. by.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our tagger on Sections 2-21 of the WSJ sections of the English Treebank, in the Ontotes 4.0 release.", "labels": [], "entities": [{"text": "WSJ sections of the English Treebank", "start_pos": 44, "end_pos": 80, "type": "DATASET", "confidence": 0.8908954858779907}, {"text": "Ontotes 4.0 release", "start_pos": 89, "end_pos": 108, "type": "DATASET", "confidence": 0.8582606514294943}]}, {"text": "This was also the training data used in the experiments in the Parsing the Web (PTW) shared task at NAACL 2012.", "labels": [], "entities": [{"text": "Parsing the Web (PTW) shared task at NAACL 2012", "start_pos": 63, "end_pos": 110, "type": "DATASET", "confidence": 0.5828111117536371}]}, {"text": "In the shared task they used the coarse-grained Google tagset).", "labels": [], "entities": [{"text": "Google tagset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8583158552646637}]}, {"text": "We believe this tagset is too coarsegrained for most purposes and do experiments with the original PTB tagset instead.", "labels": [], "entities": [{"text": "PTB tagset", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.9827336668968201}]}, {"text": "Our evaluation data comes from the English Web Treebank (EWT), which was also used in the PTW shared task.", "labels": [], "entities": [{"text": "English Web Treebank (EWT)", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.9188789824644724}, {"text": "PTW shared task", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.7859880129496256}]}, {"text": "The EWT contains development and evaluation data for five domains: answers (from Yahoo!), emails (from the Enron corpus), BBC newsgroups, Amazon reviews, and weblogs.", "labels": [], "entities": [{"text": "EWT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7899808883666992}]}, {"text": "In order not to optimize on in-domain data, we tune on the Email development data and evaluate on the remaining domains (the test sections).", "labels": [], "entities": [{"text": "Email development data", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.8532947500546774}]}, {"text": "The Web 2.0 data used for evaluation contains a lot of non-canonical language use.", "labels": [], "entities": []}, {"text": "An example is the sentence your retarded. from the Email section.", "labels": [], "entities": [{"text": "Email section", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9280048906803131}]}, {"text": "The POS tagger finds no support for r as a verb in the training data, but needs to infer this from the context.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.5517293214797974}]}, {"text": "We also include experiments on the Danish Dependency Treebank (DDT) magazines.", "labels": [], "entities": [{"text": "Danish Dependency Treebank (DDT) magazines", "start_pos": 35, "end_pos": 77, "type": "DATASET", "confidence": 0.9317204185894558}]}, {"text": "We train our tagger on the newspaper data and evaluate on the remaining three sections.", "labels": [], "entities": [{"text": "newspaper data", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.967072606086731}]}], "tableCaptions": [{"text": " Table 1: Results. BSP samples binary vectors with prob- abilities {0 : 0.1, 1 : 0.9}", "labels": [], "entities": [{"text": "BSP", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.7932149171829224}]}]}