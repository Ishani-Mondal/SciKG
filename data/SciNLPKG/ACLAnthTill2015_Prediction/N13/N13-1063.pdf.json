{"title": [{"text": "Compound Embedding Features for Semi-supervised Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "To solve data sparsity problem, recently there has been a trend in discriminative methods of NLP to use representations of lexical items learned from unlabeled data as features.", "labels": [], "entities": []}, {"text": "In this paper, we investigated the usage of word representations learned by neural language models, i.e. word embeddings.", "labels": [], "entities": []}, {"text": "The direct usage has disadvantages such as large amount of computation, inadequacy with dealing word ambiguity and rare-words, and the problem of linear non-separability.", "labels": [], "entities": []}, {"text": "To overcome these problems, we instead built compound features from continuous word embeddings based on clustering.", "labels": [], "entities": []}, {"text": "Experiments showed that the compound features not only improved the performances on several NLP tasks, but also ran faster, suggesting the potential of embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised learning methods have achieved great successes in the field of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.758118083079656}]}, {"text": "However, in practice most methods are usually limited by the problem of data sparsity, since it is impossible to obtain sufficient labeled data for all NLP tasks.", "labels": [], "entities": []}, {"text": "In these situations semisupervised learning can help to make use of both labeled data and easy-to-obtain unlabeled data.", "labels": [], "entities": []}, {"text": "The semi-supervised framework that is widely applied to NLP is to first learn word representations, which are feature vectors of lexical items, from unlabeled data and then plug them into a supervised system.", "labels": [], "entities": []}, {"text": "These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (.", "labels": [], "entities": []}, {"text": "With the development of neural language models (NLM), recently researchers become interested in word representations (also called word embeddings) learned by these models.", "labels": [], "entities": []}, {"text": "Word embeddings are dense low dimensional real-valued vectors.", "labels": [], "entities": []}, {"text": "They are composed of some latent features, which are expected to capture useful syntactic and semantic properties.", "labels": [], "entities": []}, {"text": "Word embeddings are usually served as the first layer in deep learning systems for NLP) and help these systems perform comparably with the state-of-the-art models based on handcrafted features.", "labels": [], "entities": []}, {"text": "They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements (.", "labels": [], "entities": [{"text": "chunking", "start_pos": 81, "end_pos": 89, "type": "TASK", "confidence": 0.9669830799102783}]}, {"text": "Although the direct usage of continuous embeddings has been proved to bean effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features () and made them computationally complicated.", "labels": [], "entities": []}, {"text": "Firstly, embeddings of rare words are insufficiently trained since they are only updated few times and are close to their random initial values.", "labels": [], "entities": []}, {"text": "As shown in, this is the main reason that models with embedding features made more errors than those with Brown cluster features.", "labels": [], "entities": []}, {"text": "Secondly, in NLMs, each word has its unique representation, so it is difficult to represent different senses for ambiguous words.", "labels": [], "entities": []}, {"text": "Thirdly, word embeddings are unsuitable for linear models in some tasks as will be proved in Section 4.2.", "labels": [], "entities": []}, {"text": "This is possibly because in these tasks, either the target labels are correlated with combinations of different dimensions of word embeddings, or discriminative information maybe coded in different intervals in the same dimension.", "labels": [], "entities": []}, {"text": "So treating embeddings directly as inputs to a linear model could not fully utilize them.", "labels": [], "entities": []}, {"text": "Moreover, since embeddings are dense vectors, it will introduce large amount of computations when they are directly used as inputs, making the method impractical.", "labels": [], "entities": []}, {"text": "In this paper, we first introduced the idea of clustering embeddings to overcome the last two disadvantages discussed above.", "labels": [], "entities": []}, {"text": "The highdimensional cluster features make samples from different classes better separated by linear models.", "labels": [], "entities": []}, {"text": "And models with these features can still run fast because the clusters are sparse and discrete.", "labels": [], "entities": []}, {"text": "Second, we proposed the compound features based on clustering.", "labels": [], "entities": []}, {"text": "Compound features, which are conjunctive features of neighboring words, have been widely used in NLP models for improving the performances because they are more discriminative.", "labels": [], "entities": []}, {"text": "Compound features of embeddings can also help a model to better predict labels associated with rarewords and ambiguous words, because compound features composed of embeddings of nearby words can help to better describe the property of these words.", "labels": [], "entities": []}, {"text": "Compound features are difficult to build on dense embeddings.", "labels": [], "entities": []}, {"text": "However they are easy to induce from the sparse embedding clusters proposed in this paper.", "labels": [], "entities": []}, {"text": "Experiments on chunking and NER showed that based on the same embeddings, the compound features managed to achieve better performances.", "labels": [], "entities": [{"text": "chunking", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9587949514389038}, {"text": "NER", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.6824235916137695}]}, {"text": "Moreover, we proposed analyses to reveal the reasons for the improvements of embedding-clusters and compound features.", "labels": [], "entities": []}, {"text": "They suggest that these features can better deal with rare-words and word ambiguity, and are more suitable for linear models.", "labels": [], "entities": []}, {"text": "In addition, although Brown clustering was considered better in (Turian et al 2010), our experiment results and comparisons showed that our compound features from embedding clustering is at least comparable with those from Brown clustering.", "labels": [], "entities": []}, {"text": "Since embeddings can greatly benefit from the improvement and developing of deep learning in the future, we believe that our proposed method has a large space of performance growth and will benefit more applications in NLP.", "labels": [], "entities": []}, {"text": "In the rest of the paper, Section 2 introduces how compound embedding features were obtained.", "labels": [], "entities": []}, {"text": "Section 3 gives experimental results.", "labels": [], "entities": []}, {"text": "In Section 4, we give analysis about the advantages of compound features.", "labels": [], "entities": []}, {"text": "Section 5 gives the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our compound features on the same chunking (CoNLL2000) and NER (CoNLL2003) tasks in ().", "labels": [], "entities": []}, {"text": "The Brown cluster features were used for comparison, which shared the same feature template used by clusters of embeddings.", "labels": [], "entities": []}, {"text": "To compare with the work of (Turian et al, 2010), which aimed to solve the same problem but using embedding directly, we used the same word embeddings (CW 50) and Brown clusters (1000 clusters) they provided.", "labels": [], "entities": []}, {"text": "The embeddings in) are trained on RCV corpus, while the CoNLL2000 data is apart of the WSJ corpus.", "labels": [], "entities": [{"text": "RCV corpus", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.9621347486972809}, {"text": "CoNLL2000 data", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9727522432804108}, {"text": "WSJ corpus", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.9750139117240906}]}, {"text": "Since we believe that word representations trained on similar domain may better help to improve the results, we also used embeddings and Brown clusters trained on unlabeled WSJ data from) for comparison.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 173, "end_pos": 181, "type": "DATASET", "confidence": 0.8863345086574554}]}, {"text": "Moreover, we wish to find out whether our method extends well to languages other than English.", "labels": [], "entities": []}, {"text": "So we conducted experiments on Chinese NER, where large amount of training data exists, which makes improving accuracies more difficult.", "labels": [], "entities": [{"text": "Chinese NER", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.8186207413673401}, {"text": "accuracies", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9833749532699585}]}, {"text": "We used data from People's Daily (Jan.-Jun. 1998) and converted them following the style of Penn CTB).", "labels": [], "entities": [{"text": "People's Daily (Jan.-Jun. 1998)", "start_pos": 18, "end_pos": 49, "type": "DATASET", "confidence": 0.9592116475105286}, {"text": "Penn CTB", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.9842562675476074}]}, {"text": "Data from April was chosen as test set (1,309,616 words in 55,177 sentences), others for training (6,119,063 words in 255,951 sentences).", "labels": [], "entities": []}, {"text": "The Chinese word representations were trained on Chinese Wikipedia until March 2011.", "labels": [], "entities": []}, {"text": "The features used in Chinese NER are similar to those in English, except for the orthography, pre/suffixes, and chunking features.", "labels": [], "entities": []}, {"text": "We did little pre-processing work for the training of word representations on WSJ data.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.7878925800323486}]}, {"text": "The datasets were tokenized and capital words were kept.", "labels": [], "entities": []}, {"text": "For training of Chinese Wikipedia, we retained the bodies of all articles and replaced words with frequencies lower than 10 as an \"UK_WORD\" token.", "labels": [], "entities": [{"text": "UK_WORD\" token", "start_pos": 131, "end_pos": 145, "type": "DATASET", "confidence": 0.8665464043617248}]}, {"text": "On each dataset, we induced embeddings with 64 dimensions based on 7-gram models and 1000 Brown clusters.", "labels": [], "entities": []}, {"text": "The method in was used to accelerate the training processes of NLMs.", "labels": [], "entities": []}, {"text": "All the NLMs were trained for 5 epochs.", "labels": [], "entities": []}, {"text": "For clustering of embeddings we choose k=500 and 2500 since such combination performed best on development set as shown in the next section.", "labels": [], "entities": []}, {"text": "We chose the Sofia-ml toolkit for clustering of embeddings in order to save time.", "labels": [], "entities": [{"text": "Sofia-ml toolkit", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.9421128630638123}]}, {"text": "In the experiments CRF models were used and were optimized by ASGD (implemented by L\u00e9on Bottou).", "labels": [], "entities": [{"text": "ASGD", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.6040578484535217}]}, {"text": "For comparison we re-implemented the direct usage of embeddings in) with CRFsuite (Okazaki, 2007) since their features contain continuous values.", "labels": [], "entities": []}, {"text": "In the experiments of NER, first we evaluated how the numbers of clusters k will affect the performances on development set.", "labels": [], "entities": [{"text": "NER", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8451265692710876}]}, {"text": "The results showed that both the cluster features (excluding all compound embedding features) and compound features could achieve better results than direct usage of the same embeddings.", "labels": [], "entities": []}, {"text": "It also showed that the performances did not vary much when k was between 500 and 3000.", "labels": [], "entities": []}, {"text": "When k=2500, the result was a little higher than others.", "labels": [], "entities": []}, {"text": "We finally chose combination of k=500 and 2500, which achieved best results on development set.", "labels": [], "entities": []}, {"text": "The performances of NER on test set are shown in.", "labels": [], "entities": [{"text": "NER", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.627413272857666}]}, {"text": "Our baseline is slightly lower than that in  Performances on Chinese NER are shown in.", "labels": [], "entities": [{"text": "baseline", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9790403842926025}, {"text": "Chinese NER", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.8978486359119415}]}, {"text": "Similar results were observed as in English NER, showing that our method extends to other languages as well.", "labels": [], "entities": [{"text": "English NER", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.48682133853435516}]}, {"text": "Above results gave evidences that although clustering embeddings may lose some information, the derived compound features did have better performances.", "labels": [], "entities": []}, {"text": "The compound features can also improve the performances of Brown clusters, but not as much as they did on embeddings.", "labels": [], "entities": []}, {"text": "And the combination of embedding-clusters and Brown-clusters could further improve the performances, since they made use of different type of context information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Chunking features. Cluster features are suitable  for both Brown clusters and embedding clusters. Sym- bol i  y is the tag predicted on word i  w .", "labels": [], "entities": []}, {"text": " Table 2: NER features. Hyp indicates if word contains  hyphen and Cap indicates if first letter is capitalized.", "labels": [], "entities": [{"text": "Hyp", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9721423983573914}, {"text": "Cap", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9761048555374146}]}, {"text": " Table 3: F1-scores of chunking", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9980733394622803}, {"text": "chunking", "start_pos": 23, "end_pos": 31, "type": "TASK", "confidence": 0.9130285978317261}]}, {"text": " Table 4. Our baseline is slightly lower than that  in", "labels": [], "entities": [{"text": "baseline", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9592646360397339}]}, {"text": " Table 4: F1-scores of English NER on test data", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985490441322327}]}, {"text": " Table 5. Similar results were observed as in Eng- lish NER, showing that our method extends to oth- er languages as well.", "labels": [], "entities": []}, {"text": " Table 5: F1-scores of Chinese NER on test data", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9986854195594788}, {"text": "NER", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.4561663866043091}]}]}