{"title": [{"text": "Using Document Summarization Techniques for Speech Data Subset Selection", "labels": [], "entities": [{"text": "Document Summarization", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.8415282368659973}]}], "abstractContent": [{"text": "In this paper we leverage methods from sub-modular function optimization developed for document summarization and apply them to the problem of subselecting acoustic data.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6676794588565826}]}, {"text": "We evaluate our results on data subset selection fora phone recognition task.", "labels": [], "entities": [{"text": "phone recognition task", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.8445232709248861}]}, {"text": "Our framework shows significant improvements over random selection and previously proposed methods using a similar amount of resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Present-day applications in spoken language technology (speech recognizers, keyword spotters, etc.) can draw on an unprecedented amount of training data.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7019736468791962}, {"text": "keyword spotters", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7080313861370087}]}, {"text": "However, larger data sets come with increased demands on computational resources; moreover, they tend to include redundant information as their size increases.", "labels": [], "entities": []}, {"text": "Therefore, the performance gain curves of large-scale systems with respect to the amount of training data often show \"diminishing returns\": new data is often less valuable (in terms of performance gain) when added to a larger pre-existing data set than when added to a smaller pre-existing set (e.g.,).", "labels": [], "entities": []}, {"text": "Therefore it is of prime importance to develop methods for data subset selection.", "labels": [], "entities": [{"text": "data subset selection", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.6992196341355642}]}, {"text": "We distinguish two data subselection scenarios: (a) a priori selection of a data set before (re-)training a system; in this case the goal is to subselect the existing data set as well as possible, eliminating redundant information; (b) selection for adaptation, where the goal * These authors are joint first authors with equal contributions. is to tune a system to a known development or test set.", "labels": [], "entities": []}, {"text": "While many studies have addressed the second scenario, this paper investigates the first: our goal is to select a smaller subset of the data that fits a given 'budget' (e.g. maximum number of hours of data) but provides, to the extent possible, as much information as the complete data set.", "labels": [], "entities": []}, {"text": "Additionally, our selection method should be a low-resource method that does not require an already-trained complex system such as an existing word recognizer.", "labels": [], "entities": [{"text": "word recognizer", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.6932151168584824}]}, {"text": "This problem is akin to unsupervised data 'summarization'.", "labels": [], "entities": []}, {"text": "In () a novel class of summarization techniques based on submodular function optimization were proposed for extractive document summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9861949682235718}, {"text": "extractive document summarization", "start_pos": 108, "end_pos": 141, "type": "TASK", "confidence": 0.7672215898831686}]}, {"text": "Interestingly, these methods can also be applied to speech data 'summarization' with only small modifications.", "labels": [], "entities": []}, {"text": "In the following sections we develop a submodular framework for speech data summarization and evaluate it on a proofof-concept phone recognition task.", "labels": [], "entities": [{"text": "speech data summarization", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.6281110743681589}, {"text": "proofof-concept phone recognition task", "start_pos": 111, "end_pos": 149, "type": "TASK", "confidence": 0.7794532775878906}]}], "datasetContent": [{"text": "We tested the three different similarity measures described above in combination with the submodular functions in Equations 2 and 3.", "labels": [], "entities": []}, {"text": "The parameters of the gapped string kernel (i.e. the kernel order (k), the gap penalty (\u03bb), and the contiguous substring length l) were optimized on the development set.", "labels": [], "entities": [{"text": "gap penalty (\u03bb)", "start_pos": 75, "end_pos": 90, "type": "METRIC", "confidence": 0.8714868068695069}]}, {"text": "The best values were \u03bb = 0.1, k = 4, l = 3.", "labels": [], "entities": []}, {"text": "We found that facility location was superior to saturated cover function across the board.", "labels": [], "entities": []}, {"text": "shows the performance of the random and entropy-based baselines as well as the performance of the facility location function with different similarity measures.", "labels": [], "entities": []}, {"text": "The entropy-based baseline beats the random baseline for most percentage cases but is otherwise the lowest-performing method overall.", "labels": [], "entities": []}, {"text": "Note that this baseline uses the true transcriptions inline with () rather than the hypothesized phone labels output by our recognizer.", "labels": [], "entities": []}, {"text": "The low performance and the fact that it is even outperformed by the random baseline in the 2.5% and 70% cases maybe because the selection method encourages highly diverse but not very representative subsets.", "labels": [], "entities": []}, {"text": "Furthermore, the entropy-based baseline utilizes a non-submodular objective function with a heuristic greedy search method.", "labels": [], "entities": []}, {"text": "No theoretical guarantee of optimality can be made for the subset found by this method.", "labels": [], "entities": []}, {"text": "Among the different similarity measures the Fisher kernel outperforms the baseline methods but has lower performance than the TF-IDF kernel and the string kernel.", "labels": [], "entities": []}, {"text": "The best performance is obtained with the string kernel, especially when using small training data sets (2.5%-10%).", "labels": [], "entities": []}, {"text": "The submodular selection methods yield significant improvements (p < 0.05) over both the random baseline and over the entropybased method.", "labels": [], "entities": []}, {"text": "We also investigated using different submodular functions, i.e. the facility location function and the saturated coverage function.", "labels": [], "entities": []}, {"text": "shows the performance of the facility location (f f ac ) and saturated coverage (f SC ) functions in combination with the string kernel similarity measure.", "labels": [], "entities": [{"text": "saturated coverage (f SC )", "start_pos": 61, "end_pos": 87, "type": "METRIC", "confidence": 0.8219878276189169}]}, {"text": "The reason ff ac outperforms f SC is that f SC primarily controls for over-coverage of any element not in the subset via the \u03b1 saturation hyper-parameter.", "labels": [], "entities": []}, {"text": "However, it does not ensure that every non-selected element has good representation in the subset.", "labels": [], "entities": []}, {"text": "f SC measures the quality of the subset by how well each individual element outside the subset has a surrogate within the subset (via the max function) and hence tends to model complete coverage better, leading to better results.", "labels": [], "entities": []}, {"text": "Finally we examined whether using hypothesized phone sequences vs. the true transcriptions has negative effects.", "labels": [], "entities": []}, {"text": "shows that this is not the case: interestingly, the hypothesized labels even result in slightly better results.", "labels": [], "entities": []}, {"text": "This maybe because the recognized phone sequences area function of both the underlying phonetic sequences that were spoken and the acoustic signal characteristics, such as the speaker and channel.", "labels": [], "entities": []}, {"text": "The true transcriptions, on the other hand, are able to provide information only about phonetic as opposed to acoustic characteristics.", "labels": [], "entities": []}], "tableCaptions": []}