{"title": [], "abstractContent": [{"text": "This paper describes an approach to improve summaries fora collection of Twitter posts created using the Phrase Reinforcement (PR) Algorithm (Sharifi et al., 2010a).", "labels": [], "entities": []}, {"text": "The PR algorithm often generates summaries with excess text and noisy speech.", "labels": [], "entities": [{"text": "PR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9639935493469238}]}, {"text": "We parse these summaries using a dependency parser and use the dependencies to eliminate some of the excess text and build better-formed summaries.", "labels": [], "entities": []}, {"text": "We compare the results to those obtained using the PR Algorithm.", "labels": [], "entities": [{"text": "PR Algorithm", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.7547745704650879}]}], "introductionContent": [{"text": "Millions of people use the Web to express themselves and share ideas.", "labels": [], "entities": []}, {"text": "Twitter is a very popular micro blogging site.", "labels": [], "entities": []}, {"text": "According to a recent study approximately 340 million Tweets are sent out everyday . People mostly upload daily routines, fun activities and other words of wisdom for readers.", "labels": [], "entities": []}, {"text": "There is also plenty of serious information beyond the personal; according to a study approximately 4% of posts on Twitter have relevant news data 2 . Topics that maybe covered by reputable new sources like CNN (Cable News Network) were considered relevant.", "labels": [], "entities": [{"text": "CNN (Cable News Network)", "start_pos": 207, "end_pos": 231, "type": "DATASET", "confidence": 0.785418689250946}]}, {"text": "A topic is simply a keyword or key phrase that one may use to search for Twitter posts containing it.", "labels": [], "entities": []}, {"text": "It is possible to gather large amounts of posts from Twitter on many different topics in short amounts of time.", "labels": [], "entities": []}, {"text": "Obviously, processing all this information by human hands is impossible.", "labels": [], "entities": [{"text": "processing all this information", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.881645530462265}]}, {"text": "One way to extract information from Twitter posts on a certain topic is to automatically summarize them.", "labels": [], "entities": [{"text": "summarize them", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.8901788294315338}]}, {"text": "() present an algorithm called the Phrase Reinforcement Algorithm to produces summaries of a set of Twitter posts on a certain topic.", "labels": [], "entities": []}, {"text": "The PR algorithm produces good summaries for many topics, but for sets of posts on certain topics, the summaries become syntactically malformed or too wordy.", "labels": [], "entities": []}, {"text": "This is because the PR Algorithm does not pay much attention to syntactic well-formedness as it constructs a summary sentence from phrases that occur frequently in the posts it summarizes.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to improve Twitter summaries produced by the PR algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "To begin, the Twitter posts were collected manually and stored in text files.", "labels": [], "entities": []}, {"text": "The topics we chose to focus on important current events and some pop culture.", "labels": [], "entities": []}, {"text": "Approximately 100 posts were collected on ten different topics.", "labels": [], "entities": []}, {"text": "These topics are \"The Avengers,\" \"Avril Lavigne,\" \"Christmas,\" \"the election,\" \"Election Day,\" \"Iron Man 3,\" \"president 2012,\" \"Hurricane Sandy,\" \"Thanksgiving,\" and \"vote.\"", "labels": [], "entities": [{"text": "vote", "start_pos": 167, "end_pos": 171, "type": "TASK", "confidence": 0.8247668743133545}]}, {"text": "The collections of posts were passed onto three volunteers to produce short accurate summaries that capture the main idea from the posts.", "labels": [], "entities": []}, {"text": "The collections of posts were also first run through the PR Algorithm and then through the process described in this paper to try and refine the summaries output by the PR Algorithm.", "labels": [], "entities": [{"text": "PR Algorithm", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9454821348190308}, {"text": "PR Algorithm", "start_pos": 169, "end_pos": 181, "type": "DATASET", "confidence": 0.8882715702056885}]}, {"text": "The Stanford CoreNLP parser 3 was used to build the lists of governor and dependent words.", "labels": [], "entities": [{"text": "Stanford CoreNLP parser 3", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.9057620912790298}]}, {"text": "We use ROUGE evaluation metrics (Lin 2004) just like (), who evaluated summaries obtained with the PR Algorithm.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.8660780787467957}, {"text": "PR Algorithm", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8433952629566193}]}, {"text": "Specifically, we use ROUGE-L, which uses the longest common subsequence (LCS) to compare summaries.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9801022410392761}, {"text": "longest common subsequence (LCS)", "start_pos": 45, "end_pos": 77, "type": "METRIC", "confidence": 0.6531590421994528}]}, {"text": "As the LCS of the two summaries in comparison increases in length, so does the similarity of the two summaries.", "labels": [], "entities": [{"text": "LCS", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.977387011051178}, {"text": "length", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9906634092330933}, {"text": "similarity", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9935439825057983}]}, {"text": "We now discuss results using ROUGE-L on the summaries we produce.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9898406863212585}]}, {"text": "show the results of four different ROUGE-L evaluations, comparing them to the results found using the PR Algorithm, and shows the comparisons of the averaged scores to the scores (Sharifi et al., 2010a) obtained using the PR Algorithm.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.8711101412773132}]}, {"text": "shows the regular ROUGE-L scores, meaning the recall, precision and F-scores for each task and the average overall scores, for the collection of posts before using the dependency parser to refine the summaries.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9883801937103271}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9994494318962097}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9906558394432068}, {"text": "F-scores", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9793460369110107}]}, {"text": "Table 3 displays the results after using the dependency parser on the summaries formed by the PR Algorithm.", "labels": [], "entities": [{"text": "PR Algorithm", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.7747727334499359}]}, {"text": "One of the options in ROUGE is to show the \"best\" result, for each task. has this result for the PR Algorithm results.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.6191375851631165}]}, {"text": "shows the results of the \"best\" scores, after running it through the dependency parser., using the dependency parser, compared to Sharifi et al.'s results using the PR Algorithm.", "labels": [], "entities": []}, {"text": "Stopwords were not removed in our experiments.", "labels": [], "entities": []}, {"text": "As one can see, the use of our algorithm on the summaries produced by the PR Algorithm improves the F-score values, at least in the example cases we tried.", "labels": [], "entities": [{"text": "PR Algorithm", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.7807942926883698}, {"text": "F-score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9888336658477783}]}, {"text": "In almost every case, there is substantial rise in the F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9817808866500854}]}, {"text": "As previously mentioned, some col-  lections of Tweets do not produce good summaries.", "labels": [], "entities": []}, {"text": "Task 3 had some poor scores in all cases, so one can deduce that the posts on that topic (Christmas) were widely spread, or they did not have a central theme.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: ROUGE-L without Stopwords, Before", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9859955906867981}]}, {"text": " Table 3: ROUGE-L without Stopwords, After", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9849773645401001}]}, {"text": " Table 4: ROUGE-L Best without Stopwords, Before", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9867089986801147}]}, {"text": " Table 5: ROUGE-L Best without Stopwords, After", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9868648648262024}]}, {"text": " Table 6: ROUGE-L Averages after applying our algo- rithm vs. Sharifi et al.", "labels": [], "entities": [{"text": "ROUGE-L Averages", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.965592086315155}]}]}