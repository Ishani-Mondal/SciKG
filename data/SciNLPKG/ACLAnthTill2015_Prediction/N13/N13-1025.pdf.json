{"title": [{"text": "Large-Scale Discriminative Training for Statistical Machine Translation Using Held-Out Line Search", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.8687292536099752}]}], "abstractContent": [{"text": "We introduce anew large-scale discrimina-tive learning algorithm for machine translation that is capable of learning parameters in models with extremely sparse features.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8077082335948944}]}, {"text": "To ensure their reliable estimation and to prevent over-fitting, we use a two-phase learning algorithm.", "labels": [], "entities": []}, {"text": "First, the contribution of individual sparse features is estimated using large amounts of parallel data.", "labels": [], "entities": []}, {"text": "Second, a small development corpus is used to determine the relative contributions of the sparse features and standard dense features.", "labels": [], "entities": []}, {"text": "Not only does this two-phase learning approach prevent overfitting, the second pass optimizes corpus-level BLEU of the Viterbi translation of the decoder.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9798863530158997}]}, {"text": "We demonstrate significant improvements using sparse rule indicator features in three different translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.896938145160675}]}, {"text": "To our knowledge, this is the first large-scale discriminative training algorithm capable of showing improvements over the MERT baseline with only rule indicator features in addition to the standard MERT features .", "labels": [], "entities": [{"text": "MERT baseline", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.791919469833374}]}], "introductionContent": [{"text": "This paper is about large scale discriminative training of machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7859644591808319}]}, {"text": "Like MERT, our procedure directly optimizes the cost of the Viterbi output on corpus-level metrics, but does so while scaling to millions of features.", "labels": [], "entities": []}, {"text": "The training procedure, which we call the Held-Out Line Search algorithm (HOLS), is a two-phase iterative batch optimization procedure consisting of (1) a gradient calculation on a differentiable approximation to the loss on a large amount of parallel training data and (2) a line search (using the standard MERT algorithm) to search in a subspace defined by the gradient for the weights that minimize the true cost.", "labels": [], "entities": []}, {"text": "While sparse features are successfully used in many NLP systems, such parameterizations pose a number of learning challenges.", "labels": [], "entities": []}, {"text": "First, since anyone feature is likely to occur infrequently, a large amount of training data is necessary to reliably estimate their weights.", "labels": [], "entities": []}, {"text": "Therefore, we use the full parallel training data (rather than a small development set) to estimate the contribution of the sparse features in phase 1.", "labels": [], "entities": []}, {"text": "Second, sparse features can lead to overfitting.", "labels": [], "entities": []}, {"text": "To prevent this from hurting our model's ability to generalize to new data, we do two things.", "labels": [], "entities": []}, {"text": "First, we use \"grammar and language model folds\" (translation grammars and language models built from other portions of the training data than are being used for discriminative training), and second, we run the phase 2 line search on a held-out development set.", "labels": [], "entities": []}, {"text": "Finally, since our algorithm requires decoding the entire training corpus, it is desirable (on computational grounds) to only require one or two passes through the training data.", "labels": [], "entities": []}, {"text": "To get the most out of these passes, we rescale features by their inverse frequency which improves the scaling of the optimization problem.", "labels": [], "entities": []}, {"text": "In addition to learning with few passes through the training data, the HOLS algorithm has the advantage that it is easily parallelizable.", "labels": [], "entities": []}, {"text": "After reviewing related work in the next section, we analyze two obstacles to effective discriminative learning for machine translation: overfitting (since both rules and their weights must be learned, if they are learned together degenerate solutions that fail to generalize are possible) and poor scaling (since MT decoding is so expensive, it is not feasible to make many passes through large amounts of training data, so optimization must be efficient).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7884759902954102}, {"text": "MT decoding", "start_pos": 314, "end_pos": 325, "type": "TASK", "confidence": 0.8978673219680786}]}, {"text": "We then present the details of our algorithm that addresses these issues, give results on three language pairs, and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate and analyze the performance of our training method with three sets of experiments.", "labels": [], "entities": []}, {"text": "The first set of experiments compares HOLS to other tuning algorithms used in machine translation in a medium-scale discriminative setting.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7542733550071716}]}, {"text": "The second set looks in detail at HOLS for large scale discriminative training fora Chinese-English task.", "labels": [], "entities": [{"text": "HOLS", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6790637373924255}]}, {"text": "The third set looks at two other languages.", "labels": [], "entities": []}, {"text": "All the experiments use a Hiero MT system with rule indicator features for the sparse features and the following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule.", "labels": [], "entities": []}, {"text": "The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En).", "labels": [], "entities": [{"text": "Ar-En", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9912153482437134}]}, {"text": "The same features are used for all tuning methods, except MERT baseline which uses only dense features.", "labels": [], "entities": [{"text": "MERT baseline", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.6781159341335297}]}, {"text": "Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7630564570426941}]}, {"text": "We use GIZA++ for word alignments,) to extract the grammars, our decoder is cdec () which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7373348027467728}, {"text": "SRILM", "start_pos": 155, "end_pos": 160, "type": "DATASET", "confidence": 0.7855522632598877}]}, {"text": "Our optimizer uses code implemented in the pycdec python interface to cdec ().", "labels": [], "entities": []}, {"text": "To speedup decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e | f ).", "labels": [], "entities": []}, {"text": "Statistics about the datasets we used are listed in.", "labels": [], "entities": []}, {"text": "We use the \"soft ramp 3\" loss function as the surrogate loss function for calculating the gradient in HOLS.", "labels": [], "entities": [{"text": "HOLS", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.8393930792808533}]}, {"text": "It is defined as\u02dcL where the sum over i ranges over training examples, Gen(x) is the space of possible outputs and derivations for the input x, and cost(y i , y) is add one smoothing sentence level BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9907997250556946}]}, {"text": "Except where noted, all experiments are repeated 5 times and results are averaged, initial weights for the dense features are drawn from a standard normal, and initial weights for the sparse features are set to zero.", "labels": [], "entities": []}, {"text": "We evaluate using MultEval) and report standard deviations across optimizer runs and significance at p = .05 using MultEval's built-in permutation test.", "labels": [], "entities": [{"text": "MultEval", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.8776775598526001}, {"text": "MultEval", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9253343343734741}]}, {"text": "In the large-scale experiments for HOLS, we only run the full optimizer once, and report standard deviations using multiple runs of the last MERT run (i.e. the last line search on the dev data).", "labels": [], "entities": []}, {"text": "Our first set of experiments compares the performance of the proposed HOLS algorithm to learning algorithms popularly used in machine translation on a Chinese-English task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7614589631557465}]}, {"text": "We also compare to a close relative of the HOLS algorithm: optimizing the soft ramp 3 loss directly with online stochastic gradient descent and with conditioning.", "labels": [], "entities": []}, {"text": "As we will see, SGD SOFTRAMP3 performs significantly worse than HOLS, despite both algorithms optimizing similar loss functions.", "labels": [], "entities": []}, {"text": "In the experiments in this section, we do not use the full version of the training setup described in \u00a75 since we wish to compare to algorithms that do not necessarily scale to large amounts of training data.", "labels": [], "entities": []}, {"text": "We therefore use only one fifth of the training data for learning the weights for both the dense and sparse features.", "labels": [], "entities": []}, {"text": "In this section we refer to the subset of the training data used to learn the weights as the tuning set (Tune).", "labels": [], "entities": []}, {"text": "The grammar and LM are built using the training data that is not in the tuning set (the LM also includes the English monolingual corpus), and the weights for the features are tuned using the tuning set.", "labels": [], "entities": []}, {"text": "This is similar to the typical train-dev-test split commonly used to tune machine translation systems, except that the tuning set is much larger (60k sentence pairs versus the usual 1k-2k) and comes from a random subset of the training data rather than a, SOFTRAMP3, HOLS, and a variant of HOLS which we call HILS (discussed below).", "labels": [], "entities": [{"text": "tune machine translation", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6349588334560394}]}, {"text": "For HOLS, we used 10k of the 60k tuning set for the line search, and the rest of the tuning set was used for calculating the gradient.", "labels": [], "entities": [{"text": "HOLS", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.562501072883606}]}, {"text": "For HILS (\"Held-In\" Line Search), the full 60k tuning set was used to calculate the gradient, but the line search was on a 10k subset of that set.", "labels": [], "entities": [{"text": "HILS", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.7647744417190552}, {"text": "Held-In\" Line Search)", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6582147300243377}]}, {"text": "For MERT, we used a 10k subset of the tuning data because it takes along time to run on large datasets, and it only has the eight dense features and so does not need the entire 60k tuning set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.7867656350135803}]}, {"text": "All the subsets are drawn randomly.", "labels": [], "entities": []}, {"text": "Conditioning was performed only for HOLS, HILS, and SOFTRAMP3 because conditioning would affect the regularizer for PRO and require modifications to the MIRA algorithm.", "labels": [], "entities": [{"text": "HILS", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.735567033290863}, {"text": "MIRA", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.6067490577697754}]}, {"text": "To do the conditioning for we used rule count during extraction of the grammar and not the frequency in the n-best lists because the online nature of SOFT-RAMP3 prevents us from knowing how frequent a rule will be (and the dense features are conditioned using the corpus size).", "labels": [], "entities": []}, {"text": "We chose MIRA's best learning rate (\u03b7 = .001) from {.1, .01, .001}, used default settings for PRO in cdec, and for SOFTRAMP3 we used the same loss function as HOLS but included an L 2 regularizer of strength .001 and used a stepsize of 1 (which was scaled because of conditioning).", "labels": [], "entities": []}, {"text": "To remedy problems of length bias for sentence level BLEU, we used brevity penalty smoothed and grounded BLEU+1 for sentence level scores (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9177446365356445}, {"text": "BLEU+1", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9666011134783427}]}, {"text": "Tuning was repeated four times with different initial weights, except for PRO which we only ran three times (due to training costs).", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9671889543533325}, {"text": "PRO", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9187102317810059}]}, {"text": "The initial weights for MERT were drawn from a standard normal distribution, and final MERT weights were used as the initial weights for the dense features for the other algorithms.", "labels": [], "entities": [{"text": "MERT", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.7438446879386902}]}, {"text": "Initial weights for the sparse features were set to zero.", "labels": [], "entities": []}, {"text": "For HOLS, and HILS, tuning set BLEU scores were evaluated on the set that the line search was run on.", "labels": [], "entities": [{"text": "HOLS", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.5785970687866211}, {"text": "HILS", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.8184847831726074}, {"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7548680901527405}]}, {"text": "We also report run times for 8 threads on an Opteron 6220 processor.", "labels": [], "entities": [{"text": "Opteron 6220 processor", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.9615224003791809}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "PRO and HOLS area statistically significant improvement upon the MERT baseline on the MT08 test data, but MIRA, SOFTRAMP3, and HILS are not.", "labels": [], "entities": [{"text": "PRO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.621332049369812}, {"text": "HOLS", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9649465084075928}, {"text": "MERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.7493405938148499}, {"text": "MT08 test data", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.9563087423642477}, {"text": "MIRA", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.6885724067687988}, {"text": "HILS", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.6496633887290955}]}, {"text": "HILS dramatically overfits the tuning set, while HOLS does not, justifying the use of a held-out dataset for the line search.", "labels": [], "entities": [{"text": "HILS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6923782825469971}, {"text": "HOLS", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.5445640683174133}]}, {"text": "SOFTRAMP3 performs significantly worse than HOLS on the test set.", "labels": [], "entities": [{"text": "HOLS", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.7968316078186035}]}, {"text": "PRO is a promising training algorithm, but does not scale to the full FBIS corpus because it requires many iterations.", "labels": [], "entities": [{"text": "PRO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6765831708908081}, {"text": "FBIS corpus", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.762941837310791}]}, {"text": "This set of experiments evaluates the performance of the full HOLS algorithm described in \u00a75 for large-scale discriminative training on the full FBIS Chinese-English dataset.", "labels": [], "entities": [{"text": "FBIS Chinese-English dataset", "start_pos": 145, "end_pos": 173, "type": "DATASET", "confidence": 0.9137840072313944}]}, {"text": "Since this is a relatively small and widely studied dataset, we also investigate what happens if different aspects of the procedure are omitted.", "labels": [], "entities": []}, {"text": "The number of updates is the number of times the HOLS line search optimizer is run (gradient updates).", "labels": [], "entities": []}, {"text": "For 2 passes, 4 updates, a line search is performed after a half pass through the training data, which is repeated four times fora total of two passes.", "labels": [], "entities": []}, {"text": "Using just one pass through the training data and 8 Standard MIRA and SGD SOFTRAMP3 are not parallelizable and only use a single thread.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.966444730758667}]}, {"text": "All of these algorithms were run for one iteration, except for MERT which ran for at least seven iterations, and PRO which we stopped after 20 iterations.", "labels": [], "entities": []}, {"text": "Is folding and conditioning necessary?", "labels": [], "entities": [{"text": "folding", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9421396851539612}]}, {"text": "We experiment with what happens if grammar and LM folds are not used and if conditioning is not done.", "labels": [], "entities": []}, {"text": "\u2212Folds denotes 1 pass 1 update without folds, and \u2212Conditioning denotes 1 pass 1 update without conditioning.", "labels": [], "entities": [{"text": "Folds", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9923914074897766}]}, {"text": "We can see that both these steps are important for the training procedure to work well.", "labels": [], "entities": []}, {"text": "The decrease in performance of the training procedure without folds or conditioning is dramatic but not too surprising.", "labels": [], "entities": []}, {"text": "With just one gradient update, one would expect conditioning to be very important.", "labels": [], "entities": []}, {"text": "And from the lessons learned in section 3.1, one would also expect the procedure to perform poorly or even worse than the MERT baseline without grammar or LM folds.", "labels": [], "entities": []}, {"text": "But because HOLS runs MERT on the dev data for the last line search, it is almost impossible for HOLS to be worse than the MERT baseline.", "labels": [], "entities": []}, {"text": "(This, in fact, was part of our motivation when we originally attempted the HOLS algorithm.)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MERT on Zh-En FBIS", "labels": [], "entities": [{"text": "MERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9949851036071777}, {"text": "Zh-En FBIS", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.818403959274292}]}, {"text": " Table 3: Comparison Experiments for Zh-En", "labels": [], "entities": []}, {"text": " Table 4: Full-scale Chinese-English and Ablation  Experiments", "labels": [], "entities": [{"text": "Ablation", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9403297901153564}]}]}