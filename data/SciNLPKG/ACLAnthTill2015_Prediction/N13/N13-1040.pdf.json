{"title": [], "abstractContent": [{"text": "We describe anew self-learning framework for parser lexicalisation that requires only a plain-text corpus of in-domain text.", "labels": [], "entities": []}, {"text": "The method first creates augmented versions of dependency graphs by applying a series of modifications designed to directly capture higher-order lexical path dependencies.", "labels": [], "entities": []}, {"text": "Scores are assigned to each edge in the graph using statistics from an automatically parsed background corpus.", "labels": [], "entities": []}, {"text": "As bilexical dependencies are sparse, a novel directed distributional word similarity measure is used to smooth edge score estimates.", "labels": [], "entities": []}, {"text": "Edge scores are then combined into graph scores and used for reranking the top-n analyses found by the unlexicalised parser.", "labels": [], "entities": []}, {"text": "The approach achieves significant improvements on WSJ and biomedical text over the unlexicalised baseline parser, which is originally trained on a subset of the Brown corpus.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.491977721452713}, {"text": "Brown corpus", "start_pos": 161, "end_pos": 173, "type": "DATASET", "confidence": 0.9010412395000458}]}], "introductionContent": [{"text": "Most parsers exploit supervised machine learning methods and a syntactically annotated dataset (i.e. treebank), incorporating a wide range of features in the training process to deliver competitive performance.", "labels": [], "entities": []}, {"text": "The use of lexically-conditioned features, such as relations between lemmas or word forms, is often critical when choosing the correct syntactic analysis in ambiguous contexts.", "labels": [], "entities": []}, {"text": "However, utilising such features leads the parser to learn information that is often specific to the domain and/or genre of the training data.", "labels": [], "entities": []}, {"text": "Several experiments have demonstrated that many lexical features learnt in one domain provide little if any benefit when parsing text from different domains and genres).", "labels": [], "entities": []}, {"text": "Furthermore, manual creation of in-domain treebanks is an expensive and timeconsuming process, which can only be performed by experts with sufficient linguistic and domain knowledge.", "labels": [], "entities": []}, {"text": "In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags.", "labels": [], "entities": []}, {"text": "While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts ().", "labels": [], "entities": []}, {"text": "In this work, instead of trying to adapt a lexicalised parser to new domains, we explore how bilexical features can be integrated effectively with any unlexicalised parser.", "labels": [], "entities": []}, {"text": "As our novel self-learning framework requires only a large unannotated corpus, lexical features can be easily tuned to a specific domain or genre by selecting a suitable dataset.", "labels": [], "entities": []}, {"text": "In addition, we describe a graph expansion process that captures selected bilexical relations which improve performance but would otherwise require sparse higherorder dependency path feature types inmost approaches to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.7621451318264008}]}, {"text": "As many bilexical features will still be sparse, we also develop an approach to estimating confidence scores for dependency relations using a directional distributional word similarity measure.", "labels": [], "entities": []}, {"text": "The final framework integrates easily with any unlexicalised (and therefore potentially less domain/genre-biased) parser capable of returning ranked dependency analyses.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate how much the reranker improves the highest-ranked dependency graph, we calculate the microaveraged precision, recall and F-score overall dependencies from the top-ranking parses for the test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.8606539964675903}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9980295300483704}, {"text": "F-score", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.9889931082725525}]}, {"text": "Following the official RASP evaluation ( ) we employ the hierarchical edge matching scheme which aggregates counts up the dependency relation subsumption hierarchy and thus rewards the parser for making more finegrained distinctions.", "labels": [], "entities": [{"text": "RASP", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.8253381848335266}]}, {"text": "Statistical significance of the change in F-score is calculated by using the Approximate Randomisation Test) with 10 6 iterations.", "labels": [], "entities": [{"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9970766305923462}, {"text": "Approximate Randomisation Test", "start_pos": 77, "end_pos": 107, "type": "METRIC", "confidence": 0.9123401244481405}]}, {"text": "We also wish to measure how well the reranker does at the overall task of ordering dependency graphs.", "labels": [], "entities": []}, {"text": "For this we make use of an oracle that creates the perfect ranking fora set of graphs by calculating their individual F-scores; this ideal ranking is then compared to the output of our system.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9300328493118286}]}, {"text": "Spearman's rank correlation coefficient between the two rankings is calculated for each sentence and then averaged overall sentences.", "labels": [], "entities": [{"text": "rank correlation coefficient", "start_pos": 11, "end_pos": 39, "type": "METRIC", "confidence": 0.8605717619260153}]}, {"text": "If the scores for all of the returned analyses are equal, this coefficient cannot be calculated and is set to 0.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of different edge scoring methods on the test data. For each measure we report precision,  recall, F-score, and average Spearman's correlation (\u03c1). The highest results for each measure are marked in bold. The  underlined F-scores are significantly better compared to the baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9997472167015076}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9996839761734009}, {"text": "F-score", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9984514713287354}, {"text": "Spearman's correlation (\u03c1)", "start_pos": 142, "end_pos": 168, "type": "METRIC", "confidence": 0.968807319800059}, {"text": "F-scores", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.967333972454071}]}]}