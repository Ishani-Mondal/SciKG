{"title": [{"text": "Extracting the Native Language Signal for Second Language Acquisition", "labels": [], "entities": [{"text": "Extracting", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9497739672660828}, {"text": "Second Language Acquisition", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6388591527938843}]}], "abstractContent": [{"text": "We develop a method for effective extraction of linguistic patterns that are differentially expressed based on the native language of the author.", "labels": [], "entities": []}, {"text": "This method uses multiple corpora to allow for the removal of data set specific patterns, and addresses both feature relevancy and redundancy.", "labels": [], "entities": []}, {"text": "We evaluate different rel-evancy ranking metrics and show that common measures of relevancy can be inappropriate for data with many rare features.", "labels": [], "entities": []}, {"text": "Our feature set is abroad class of syntactic patterns , and to better capture the signal we extend the Bayesian Tree Substitution Grammar induction algorithm to a supervised mixture of latent grammars.", "labels": [], "entities": []}, {"text": "We show that this extension can be used to extract a larger set of relevant features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Native Language Identification (NLI) is a classification task in which a statistical signal is exploited to determine an author's native language (L1) from their writing in a second language (L2).", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.770095114906629}]}, {"text": "This academic exercise is often motivated not only by fraud detection or authorship attribution for which L1 can bean informative feature, but also by its potential to assist in Second Language Acquisition (SLA).", "labels": [], "entities": [{"text": "fraud detection", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.6795381605625153}, {"text": "Second Language Acquisition (SLA)", "start_pos": 178, "end_pos": 211, "type": "TASK", "confidence": 0.8119848668575287}]}, {"text": "Our work focuses on the latter application and on the observation that the actual ability to automatically determine L1 from text is of limited utility in the SLA domain, where the native language of a student is either known or easily solicited.", "labels": [], "entities": []}, {"text": "Instead, the likely role of NLP in the context of SLA is to provide a set of linguistic patterns that students with certain L1 backgrounds use with a markedly unusual frequency.", "labels": [], "entities": [{"text": "SLA", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9684451818466187}]}, {"text": "Experiments have shown that such L1 specific information can be incorporated into lesson plans that improve student performance.", "labels": [], "entities": []}, {"text": "This is essentially a feature selection task with the additional caveat that features should be individually discriminative between native languages in order to facilitate the construction of focused educational excersizes.", "labels": [], "entities": []}, {"text": "With this goal, we consider metrics for data set dependence, relevancy, and redundancy.", "labels": [], "entities": []}, {"text": "We show that measures of relevancy based on mutual information can be inappropriate in problems such as ours where rare features are important.", "labels": [], "entities": []}, {"text": "While the majority of the methods that we consider generalize to any of the various feature sets employed in NLI, we focus on the use of Tree Substitution Grammar rules as features.", "labels": [], "entities": []}, {"text": "Obtaining a compact feature set is possible with the well known Bayesian grammar induction algorithm , but its rich get richer dynamics can make it difficult to find rare features.", "labels": [], "entities": [{"text": "Bayesian grammar induction", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.6154391368230184}]}, {"text": "We extend the induction model to a supervised mixture of latent grammars and show how it can be used to incorporate linguistic knowledge and extract discriminative features more effectively.", "labels": [], "entities": []}, {"text": "The end result of this technique is a filtered list of patterns along with their usage statistics.", "labels": [], "entities": []}, {"text": "This provides an enhanced resource for SLA research such as which tackles the manual connection of highly discriminative features with plausible linguistic transfer explanations.", "labels": [], "entities": [{"text": "SLA", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9843239188194275}]}, {"text": "We output a compact list of language patterns that are empirically associated with native language labels, avoid-ing redundancy and artifacts from the corpus creation process.", "labels": [], "entities": []}, {"text": "We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "The first step in our L1 signal extraction pipeline controls for patterns that occur too frequently in certain combinations of native language and data set.", "labels": [], "entities": [{"text": "L1 signal extraction", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.6091718177000681}]}, {"text": "Such patterns arise primarily from the reuse of essay prompts in the creation of certain corpora, and we construct a hard filter to exclude features of this type.", "labels": [], "entities": []}, {"text": "A simple first choice would be to rank the rules in order of dependence on the corpus, as we expect an irregularly represented topic to be confined to a single data set.", "labels": [], "entities": []}, {"text": "However, this misses the subtle but important point that corpora have different qualities such as register and author proficiency.", "labels": [], "entities": [{"text": "register", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9774098992347717}]}, {"text": "Instead we treat the set of sentences containing an arbitrary feature X as a set of observations of a pair of categorical random variables Land D, representing native language and data set respectively.", "labels": [], "entities": []}, {"text": "To see why this treatment is superior, consider the outcomes for the two hypothetical features shown Figure 2: Two hypothetical feature profiles that illustrate the problems with filtering only on data set independence, which prefers the right profile over the left.", "labels": [], "entities": []}, {"text": "Our method has the opposite preference. in.", "labels": [], "entities": []}, {"text": "The left table has a high data set dependence but exhibits a clean twofold preference for L 1 in both data sets, making it a desirable feature to retain.", "labels": [], "entities": []}, {"text": "Conversely, the right table shows a feature where the distribution is uniform over data sets, but has language preference in only one.", "labels": [], "entities": []}, {"text": "This is a sign of either a large variance in usage or some data set specific tendency, and in either case we cannot make confident claims as to this feature's association with any native language.", "labels": [], "entities": []}, {"text": "The L-D dependence can be measured with Pearson's \u03c7 2 test, although the specifics of its use as a filter deserve some discussion.", "labels": [], "entities": []}, {"text": "As we eliminate the features for which the null hypothesis of independence is rejected, our noisy data will cause us to overzealously reject.", "labels": [], "entities": []}, {"text": "In order to prevent the unneccesary removal of interesting patterns, we use a very small p value as a cutoff point for rejection.", "labels": [], "entities": []}, {"text": "In all of our experiments the \u03c7 2 value corresponding top < .001 is in the twenties; we use \u03c7 2 > 100 as our criteria for rejection.", "labels": [], "entities": []}, {"text": "Another possible source of error is the sparsity of some features in our data.", "labels": [], "entities": []}, {"text": "To avoid making predictions of rules for which we have not observed a sufficient number of examples, we automatically exclude any rule with a countless than five for any L-D combination \u03b7.", "labels": [], "entities": []}, {"text": "This also satisfies the common requirements for validity of the \u03c7 2 test that require a minimum number of 5 expected counts for every outcome.", "labels": [], "entities": [{"text": "validity", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9848159551620483}]}], "tableCaptions": []}