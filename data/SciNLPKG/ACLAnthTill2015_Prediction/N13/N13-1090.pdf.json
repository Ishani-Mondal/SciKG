{"title": [{"text": "Linguistic Regularities in Continuous Space Word Representations", "labels": [], "entities": [{"text": "Linguistic Regularities in Continuous Space Word Representations", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.7898226422922952}]}], "abstractContent": [{"text": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks.", "labels": [], "entities": []}, {"text": "In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights.", "labels": [], "entities": []}, {"text": "We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.", "labels": [], "entities": []}, {"text": "This allows vector-oriented reasoning based on the offsets between words.", "labels": [], "entities": []}, {"text": "For example, the male/female relationship is automatically learned, and with the induced vector representations, \"King-Man + Woman\" results in a vector very close to \"Queen.\"", "labels": [], "entities": []}, {"text": "We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.", "labels": [], "entities": []}, {"text": "We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions.", "labels": [], "entities": [{"text": "SemEval-2012 Task 2", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.6123149991035461}]}, {"text": "Remarkably , this method outperforms the best previous systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "A defining feature of neural network language models is their representation of words as high dimensional real valued vectors.", "labels": [], "entities": []}, {"text": "In these models, words are converted via a learned lookuptable into real valued vectors which are used as the * Currently at inputs to a neural network.", "labels": [], "entities": []}, {"text": "As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors.", "labels": [], "entities": []}, {"text": "Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carryover to occurrences of similar words and sequences.", "labels": [], "entities": []}, {"text": "By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which maybe used for other, potentially unrelated, tasks.", "labels": [], "entities": []}, {"text": "This has been used to good effect, for example in) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks.", "labels": [], "entities": []}, {"text": "In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way.", "labels": [], "entities": []}, {"text": "Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship.", "labels": [], "entities": []}, {"text": "For example, if we denote the vector for word i as xi , and focus on the singular/plural relation, we observe that x apple \u2212x apples \u2248 x car \u2212x cars , x family \u2212x f amilies \u2248 x car \u2212x cars , and soon.", "labels": [], "entities": []}, {"text": "Perhaps more surprisingly, we find that this is also the case fora variety of semantic relations, as measured by the SemEval 2012 task of measuring relation similarity.", "labels": [], "entities": [{"text": "SemEval 2012 task", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.6009753545125326}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012).", "labels": [], "entities": [{"text": "vector offset", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.6923672407865524}, {"text": "RNN toolkit of Mikolov (2012)", "start_pos": 71, "end_pos": 100, "type": "DATASET", "confidence": 0.9291160362107413}]}, {"text": "Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600.", "labels": [], "entities": []}, {"text": "The systems were trained with 320M words of Broadcast News data as described in (, and had an 82k vocabulary.", "labels": [], "entities": [{"text": "Broadcast News data", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.9478260278701782}]}, {"text": "shows results for both RNNLM and LSA vectors on the syntactic task.", "labels": [], "entities": []}, {"text": "LSA was trained on the same data as the RNN.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8982890844345093}, {"text": "RNN", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.926240861415863}]}, {"text": "We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly.", "labels": [], "entities": []}, {"text": "In we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and, as implemented by) and available online Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those.", "labels": [], "entities": []}, {"text": "This resulted in a 36k word vocabulary, and a test set with 6632 Guessing gets a small fraction of a percent.    questions.", "labels": [], "entities": []}, {"text": "Turian's Collobert and Weston based vectors do poorly on this task, whereas the Hierarchical Log-Bilinear Model vectors of) do essentially as well as the RNN vectors.", "labels": [], "entities": []}, {"text": "These representations were trained on 37M words of data and this may indicate a greater robustness of the HLBL method.", "labels": [], "entities": []}, {"text": "We conducted similar experiments with the semantic test set.", "labels": [], "entities": []}, {"text": "For each target word pair in a relation category, the model measures its relational similarity to each of the prototypical word pairs, and then uses the average as the final score.", "labels": [], "entities": []}, {"text": "The results are evaluated using the two standard metrics defined in the task, Spearman's rank correlation coefficient \u03c1 and MaxDiff accuracy.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient \u03c1", "start_pos": 78, "end_pos": 119, "type": "METRIC", "confidence": 0.8164398769537607}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.5292032957077026}]}, {"text": "In both cases, larger values are better.", "labels": [], "entities": []}, {"text": "To compare to previous systems, we report the average overall 69 relations in the test set.", "labels": [], "entities": []}, {"text": "From, we see that as with the syntactic regularity study, the RNN-based representations perform best.", "labels": [], "entities": []}, {"text": "In this case, however, Turian's CW vectors are comparable in performance to the HLBL vectors.", "labels": [], "entities": []}, {"text": "With the RNN vectors, the performance improves as the number of dimensions increases.", "labels": [], "entities": []}, {"text": "Surprisingly, we found that even though the RNN vec-: Results in measuring relation similarity tors are not trained or tuned specifically for this task, the model achieves better results (RNN-320, RNN-640 & RNN-1600) than the previously best performing system, UTD-NB (Rink and Harabagiu, 2012).", "labels": [], "entities": [{"text": "UTD-NB (Rink and Harabagiu, 2012)", "start_pos": 261, "end_pos": 294, "type": "DATASET", "confidence": 0.782343327999115}]}], "tableCaptions": [{"text": " Table 2: Results for identifying syntactic regularities for  different word representations. Percent correct.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of RNN vectors with Turian's Col- lobert and Weston based vectors and the Hierarchical  Log-Bilinear model of Mnih and Hinton. Percent correct.", "labels": [], "entities": [{"text": "Percent", "start_pos": 148, "end_pos": 155, "type": "METRIC", "confidence": 0.9736233353614807}]}, {"text": " Table 4: Results in measuring relation similarity", "labels": [], "entities": [{"text": "relation similarity", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7777625620365143}]}]}