{"title": [{"text": "Reversing Morphological Tokenization in English-to-Arabic SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8107316493988037}]}], "abstractContent": [{"text": "Morphological tokenization has been used in machine translation for morphologically complex languages to reduce lexical sparsity.", "labels": [], "entities": [{"text": "Morphological tokenization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7414233684539795}, {"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7540406882762909}]}, {"text": "Unfortunately, when translating into a morphologically complex language, recombining segmented tokens to generate original word forms is not a trivial task, due to morphological , phonological and orthographic adjustments that occur during tokenization.", "labels": [], "entities": []}, {"text": "We review a number of detokenization schemes for Arabic, such as rule-based and table-based approaches and show their limitations.", "labels": [], "entities": []}, {"text": "We then propose a novel detokenization scheme that uses a character-level discriminative string transducer to predict the original form of a segmented word.", "labels": [], "entities": []}, {"text": "Ina comparison to a state-of-the-art approach, we demonstrate slightly better detokenization error rates, without the need for any hand-crafted rules.", "labels": [], "entities": []}, {"text": "We also demonstrate the effectiveness of our approach in an English-to-Arabic translation task.", "labels": [], "entities": [{"text": "English-to-Arabic translation task", "start_pos": 60, "end_pos": 94, "type": "TASK", "confidence": 0.7370130320390066}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) relies on tokenization to split sentences into meaningful units for easy processing.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8273040056228638}]}, {"text": "For morphologically complex languages, such as Arabic or Turkish, this may involve splitting words into morphemes.", "labels": [], "entities": []}, {"text": "Throughout this paper, we adopt the definition of tokenization proposed by, which incorporates both morphological segmentation as well as orthographic character transformations.", "labels": [], "entities": []}, {"text": "To use an English example, the word tries would be morphologically tokenized as \"try + s\", which involves orthographic changes at morpheme boundaries to match the lexical form of each token.", "labels": [], "entities": []}, {"text": "When translating into a tokenized language, the tokenization must be reversed to make the generated text readable and evaluable.", "labels": [], "entities": []}, {"text": "Detokenization is the process of converting tokenized words into their original orthographically and morphologically correct surface form.", "labels": [], "entities": []}, {"text": "This includes concatenating tokens into complete words and reversing any character transformations that may have taken place.", "labels": [], "entities": []}, {"text": "For languages like Arabic, tokenization can facilitate SMT by reducing lexical sparsity.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9957505464553833}]}, {"text": "shows how the morphological tokenization of the Arabic word \"and he will prevent them\" simplifies the correspondence between Arabic and English tokens, which in turn can improve the quality of word alignment, rule extraction and decoding.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 193, "end_pos": 207, "type": "TASK", "confidence": 0.7494445145130157}, {"text": "rule extraction", "start_pos": 209, "end_pos": 224, "type": "TASK", "confidence": 0.7899156808853149}]}, {"text": "When translating from Arabic into English, the tokenization is a form of preprocessing, and the output translation is readable, space-separated English.", "labels": [], "entities": []}, {"text": "However, when translating from English to Arabic, the output will be in a tokenized form, which cannot be compared to the original reference without detokenization.", "labels": [], "entities": []}, {"text": "Simply concatenating the tokenized morphemes cannot fully reverse this process, because of character transformations that occurred during tokenization.", "labels": [], "entities": []}, {"text": "The techniques that have been proposed for the detokenization task fall into three categories ().", "labels": [], "entities": [{"text": "detokenization task", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.931768923997879}]}, {"text": "The simplest detokenization approach concatenates morphemes based on token markers without any adjustment. by observing the tokenizer's in- put and output on large amounts of text.", "labels": [], "entities": []}, {"text": "Rule-based detokenization relies on hand-built rules or regular expressions to convert the segmented form into the original surface form.", "labels": [], "entities": []}, {"text": "Other techniques use combinations of these approaches.", "labels": [], "entities": []}, {"text": "Each approach has its limitations: rule-based approaches are language specific and brittle, while table-based approaches fail to deal with sequences outside of their tables.", "labels": [], "entities": []}, {"text": "We present anew detokenization approach that applies a discriminative sequence model to predict the original form of the tokenized word.", "labels": [], "entities": []}, {"text": "Like table-based approaches, our sequence model requires large amounts of tokenizer input-output pairs; but instead of building a table, we use these pairs as training data.", "labels": [], "entities": []}, {"text": "By using features that consider large windows of within-word input context, we are able to intelligently transition between rule-like and table-like behavior.", "labels": [], "entities": []}, {"text": "Our experimental results on Arabic text demonstrate an improvement in terms of sentence error rate 1 of 11.9 points over a rule-based approach, and 1.1 points over a table-based approach that backs off to rules.", "labels": [], "entities": [{"text": "sentence error rate", "start_pos": 79, "end_pos": 98, "type": "METRIC", "confidence": 0.6762808859348297}]}, {"text": "More importantly, we achieve a slight improvement over the state-of-the-art approach of El, which combines rules and tables, using a 5-gram language model to disambiguate conflicting table entries.", "labels": [], "entities": []}, {"text": "In addition, our detokenization method results in a small BLEU improvement over a rule-based approach when applied to English-to-Arabic SMT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9995219707489014}, {"text": "SMT", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.6897173523902893}]}, {"text": "Compared to English, Arabic has rich and complex morphology.", "labels": [], "entities": []}, {"text": "Arabic base words inflect to eight features.", "labels": [], "entities": []}, {"text": "Verbs inflect for aspect, mood, person and voice.", "labels": [], "entities": []}, {"text": "Nouns and adjectives inflect for case and state.", "labels": [], "entities": []}, {"text": "Verbs, nouns and adjectives inflect for both gender and number.", "labels": [], "entities": []}, {"text": "Furthermore, inflected base words can attract various optional clitics.", "labels": [], "entities": []}, {"text": "Clitical prefixes include determiners, particle proclitics, conjunctions and question particles in strict order.", "labels": [], "entities": []}, {"text": "Clitical suffixes include pronominal modifiers.", "labels": [], "entities": []}, {"text": "As a result of clitic attachment, morpho-syntactic interactions sometimes cause changes in spelling or pronunciations.", "labels": [], "entities": []}, {"text": "Several tokenization schemes can be defined for Arabic, depending on the clitical level that the tokenization is applied to.", "labels": [], "entities": []}, {"text": "In this paper, we use Penn Arabic Treebank (PATB) tokenization scheme, which El Kholy and Habash (2012) report as producing the best results for Arabic SMT.", "labels": [], "entities": [{"text": "Penn Arabic Treebank (PATB) tokenization", "start_pos": 22, "end_pos": 62, "type": "DATASET", "confidence": 0.8707956075668335}, {"text": "Arabic SMT", "start_pos": 145, "end_pos": 155, "type": "TASK", "confidence": 0.4842734783887863}]}, {"text": "The PATB scheme detaches all clitics except for the definite article Al . Multiple prefix clitics are treated as one token.", "labels": [], "entities": []}, {"text": "Some Arabic letters present further ambiguity in text.", "labels": [], "entities": []}, {"text": "For example, the different forms of Hamzated Alif \" \" are usually written without the Hamza \"\".", "labels": [], "entities": []}, {"text": "Likewise, when the letter Ya 'Y' is present at the end of the word, it is sometimes written in the form of \"Alif Maqsura\" letter '\u00fd' . Also, short vowels in Arabic are represented using diacritics, which are usually absent in written text.", "labels": [], "entities": []}, {"text": "In order to deal with these ambiguities in SMT, normalization is often performed as a preprocessing step, which usually involves converting different forms of Alif and Ya to a single form.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.99349045753479}]}, {"text": "This decreases Arabic's lexical sparsity and improves SMT performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9941490888595581}]}], "datasetContent": [{"text": "This section presents two experiments that evaluate the effect of the detokenization schemes on both naturally occurring Arabic and SMT output.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 132, "end_pos": 142, "type": "TASK", "confidence": 0.885103702545166}]}, {"text": "To train the detokenization systems, we generate a table of mappings from tokenized forms to surface forms based on the Arabic part of our 4 parallel datasets, giving us complete coverage of the output vocabulary of our SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 220, "end_pos": 223, "type": "TASK", "confidence": 0.9830955862998962}]}, {"text": "In the tablebased approaches, if a tokenized form is mapped to more than one surface form, we use the most frequent surface form.", "labels": [], "entities": []}, {"text": "For out-of-table words, we fallback on concatenation (in T) or rules (in T+R).", "labels": [], "entities": []}, {"text": "For SRILM-Disambig detokenization, we maintain ambiguous table entries along with their frequencies, and we introduce a 5-gram language model to disambiguate detokenization choices in context.", "labels": [], "entities": [{"text": "SRILM-Disambig detokenization", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.7650156021118164}]}, {"text": "Like the table-based approaches, the Disambig approach can back off to either simple concatenation (T+LM) or rules (T+R+LM) for missing entries.", "labels": [], "entities": []}, {"text": "The latter is a re-implementation of the state-of-the-art system presented by El.", "labels": [], "entities": []}, {"text": "We train our discriminative string transducer using word types from the 4 LDC catalogs.", "labels": [], "entities": []}, {"text": "We use M2M-ALIGNER to generate a 2-to-1 character alignments between tokenized forms and surface forms.", "labels": [], "entities": []}, {"text": "For the decoder, we set Markov order to one, joint n-gram features to 5, n-gram size to 11, and context size to 5.", "labels": [], "entities": []}, {"text": "This means the decoder can utilize contexts up to 11 characters long, allowing it to  effectively memorize many words.", "labels": [], "entities": []}, {"text": "We found these settings using grid search on the development set, NIST MT04.", "labels": [], "entities": [{"text": "NIST MT04", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.8627757430076599}]}, {"text": "For the SMT experiment, we use GIZA++ for the alignment between English and tokenized Arabic, and perform the translation using Moses phrasebased SMT system (, with a maximum phrase length of 5.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9954324960708618}]}, {"text": "We apply each detokenization scheme on the SMT tokenized Arabic output test set, and evaluate using the BLEU score).", "labels": [], "entities": [{"text": "SMT tokenized Arabic output test set", "start_pos": 43, "end_pos": 79, "type": "DATASET", "confidence": 0.8830673396587372}, {"text": "BLEU score", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9674803614616394}]}, {"text": "shows the performance of several detokenization schemes.", "labels": [], "entities": []}, {"text": "For evaluation, we use the sentence and word error rates on naturally occurring Arabic text, and BLEU score on tokenized Arabic output of the SMT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9997829794883728}, {"text": "SMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9805609583854675}]}, {"text": "The baseline scheme, which is a simple concatenation of morphemes, introduces errors in over a third of all sentences.", "labels": [], "entities": []}, {"text": "The table-based approach outperforms the rule-based approach, indicating that there are frequent exceptions to the rules in that require memorization.", "labels": [], "entities": []}, {"text": "Their combination (T+R) fares better, leveraging the strengths of both approaches.", "labels": [], "entities": []}, {"text": "The addition of SRILM-Disambig produces further improvements as it uses a language model context to disambiguate the correct detokenized word form.", "labels": [], "entities": [{"text": "SRILM-Disambig", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.6712222695350647}]}, {"text": "Our system outperforms SRILM-Disambig by a very slight margin, indicating that the two systems are roughly equal.", "labels": [], "entities": [{"text": "SRILM-Disambig", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.5000036954879761}]}, {"text": "This is interesting, as it is able to do so by using only features derived from the tokenized word itself; unlike SRILM-Disambig, it has no access to the surrounding words to inform its decisions.", "labels": [], "entities": [{"text": "SRILM-Disambig", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.814988374710083}]}, {"text": "In ad-dition, it is able to achieve this level of performance without any manually constructed rules.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Type counts before and after tokenization.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9698034524917603}]}, {"text": " Table 3: Word and sentence error rate of detokenization  schemes on the Arabic reference text of NIST MT05.  BLEU score refers to English-Arabic SMT output.", "labels": [], "entities": [{"text": "Arabic reference text of NIST MT05", "start_pos": 73, "end_pos": 107, "type": "DATASET", "confidence": 0.7420685589313507}, {"text": "BLEU score", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9798438847064972}, {"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9217852354049683}]}]}