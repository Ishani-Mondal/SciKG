{"title": [{"text": "A Machine Learning Approach to Automatic Term Extraction using a Rich Feature Set *", "labels": [], "entities": [{"text": "Automatic Term Extraction", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7350332935651144}]}], "abstractContent": [{"text": "In this paper we propose an automatic term extraction approach that uses machine learning incorporating varied and rich features of candidate terms.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7291657477617264}]}, {"text": "In our preliminary experiments , we also tested different attribute selection methods to verify which features are more relevant for automatic term extraction.", "labels": [], "entities": [{"text": "automatic term extraction", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.594540148973465}]}, {"text": "We achieved state of the art results for uni-gram extraction in Brazilian Portuguese.", "labels": [], "entities": [{"text": "uni-gram extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.6978960931301117}]}], "introductionContent": [{"text": "Terms are terminological units from specialised texts).", "labels": [], "entities": []}, {"text": "A term may be: (i) simple 1 (a single element), such as \"biodiversity\", or (ii) complex (more than one element), such as \"aquatic ecosystem\" and \"natural resource management\".", "labels": [], "entities": []}, {"text": "Automatic term extraction (ATE) methods aim to identify terminological units in specific domain corpora).", "labels": [], "entities": [{"text": "Automatic term extraction (ATE)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7281156380971273}]}, {"text": "Such information is extremely useful for several tasks, from the linguistic perspective of building dictionaries, taxonomies and ontologies, to computational applications as information retrieval, extraction, and summarisation.", "labels": [], "entities": [{"text": "information retrieval, extraction", "start_pos": 174, "end_pos": 207, "type": "TASK", "confidence": 0.7522453367710114}, {"text": "summarisation", "start_pos": 213, "end_pos": 226, "type": "TASK", "confidence": 0.9920827150344849}]}, {"text": "Although ATE has been researched for more than 20 years, there is still room for improvement.", "labels": [], "entities": [{"text": "ATE", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.4936884939670563}]}, {"text": "There are four major ATE problems.", "labels": [], "entities": [{"text": "ATE", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9740619659423828}]}, {"text": "The first one is that the ATE approaches may extract terms that are not actual terms (\"noise\") or do not extract actual terms (\"silence\").", "labels": [], "entities": []}, {"text": "Considering the ecology domain, an example of silence is when a term (e.g., pollination), * This research was supported by FAPESP, Brazil.", "labels": [], "entities": [{"text": "FAPESP", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.9342278242111206}]}, {"text": "1 When we refer to unigrams, we mean simple terms. with low frequency, is not considered a candidate term (CT), and, therefore, it will not appear in the extracted term list if we consider its frequency.", "labels": [], "entities": [{"text": "candidate term (CT)", "start_pos": 91, "end_pos": 110, "type": "METRIC", "confidence": 0.7000591635704041}]}, {"text": "Regarding noise, if we consider that nouns maybe terms and that adjectives may not, if an adjective (e.g., ecological) is mistakenly tagged as a noun, it will be wrongly extracted as a term.", "labels": [], "entities": []}, {"text": "The second problem is the difficulty in dealing with extremely high number of candidates (called the high dimensionality of candidate representation) that requires time to process them.", "labels": [], "entities": []}, {"text": "Since the ATE approaches generate large lists of TCs, we have the third problem that is the time and human effort spent for validating the TCs, which usually is manually performed.", "labels": [], "entities": []}, {"text": "The fourth problem is that the results are still not satisfactory and there is a natural ATE challenge since the difficulty in obtaining a consensus among the experts about which words are terms of a specific domain (.", "labels": [], "entities": [{"text": "ATE", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.7255610823631287}]}, {"text": "Our proposed ATE approach uses machine learning (ML), since it has been achieving high precision values (.", "labels": [], "entities": [{"text": "ATE", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8832276463508606}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9723957180976868}]}, {"text": "Although ML may also generate noise and silence, it facilitates the use of a large number of TCs and their features, since ML techniques learn by themselves how to recognize a term and then they save time extracting them.", "labels": [], "entities": []}, {"text": "Our approach differs from others because we adopt a rich feature set using varied knowledge levels.", "labels": [], "entities": []}, {"text": "With this, it is possible to decrease the silence and noise and, consequently, to improve the ATE results.", "labels": [], "entities": [{"text": "silence", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9838046431541443}, {"text": "ATE", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9756106734275818}]}, {"text": "Our features range from simple statistical (e.g., term frequency) and linguistic (e.g., part of speech -POS) knowledge to more sophisticated hybrid knowledge, such as the analysis of the term context.", "labels": [], "entities": []}, {"text": "As far as we know, the combined use of this specific knowledge has not been applied before.", "labels": [], "entities": []}, {"text": "Another difference is that we apply 3 statistical features (Term Variance (), Term Variance Quality (, and Term Contribution () that to date have only been used for attribute selection and not for term extraction.", "labels": [], "entities": [{"text": "Term Variance Quality", "start_pos": 78, "end_pos": 99, "type": "METRIC", "confidence": 0.7092720667521158}, {"text": "term extraction", "start_pos": 197, "end_pos": 212, "type": "TASK", "confidence": 0.690752312541008}]}, {"text": "As far as we know, the combined use of this specific knowledge and feature feedback has not been applied before.", "labels": [], "entities": []}, {"text": "We also propose 4 new linguistic features for ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.6666812300682068}]}, {"text": "All these features are detailed in Section 4.", "labels": [], "entities": []}, {"text": "Finally, for the first time, ML is being applied in the task of ATE in Brazilian Portuguese (BP) corpora.", "labels": [], "entities": [{"text": "ML", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9261903762817383}, {"text": "ATE", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.7949492335319519}, {"text": "Brazilian Portuguese (BP) corpora", "start_pos": 71, "end_pos": 104, "type": "DATASET", "confidence": 0.5075893302758535}]}, {"text": "Our approach may also be easily adapted to other languages.", "labels": [], "entities": []}, {"text": "We focus on extracting only unigram terms, since this is already a complex task.", "labels": [], "entities": []}, {"text": "We run our experiments on 3 different corpora.", "labels": [], "entities": []}, {"text": "Our main contribution is the improvement of precision (in the best case, we improve the results 11 times) and F-measure (in the best case, we improve 2 times).", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9996753931045532}, {"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9992497563362122}]}, {"text": "Section 2 presents the main related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our ATE approach.", "labels": [], "entities": [{"text": "ATE", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.7023234367370605}]}, {"text": "Section 4 details the experiments, and Section 5 reports the results.", "labels": [], "entities": []}, {"text": "Conclusions and future work are presented in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "At this point, for obtaining the knowledge in order to extract terms, we tested 17 features that do not depend on general or contrastive corpora and 2 features that depend on these corpora.", "labels": [], "entities": []}, {"text": "We intend to explore more features (and we will possibly propose new measures) that use contrastive or general corpora or any taxonomic structure.", "labels": [], "entities": []}, {"text": "The experiments that expand the number of features are ongoing now.", "labels": [], "entities": []}, {"text": "We used 3 corpora of different domains in the Portuguese language.", "labels": [], "entities": []}, {"text": "The EaD corpus) has 347 texts about distance education and has a gold standard with 118 terms 5 (Gi-5) stated that the EaD unigram gold standard has 59 terms, but in this paper we used 118 unigrams that the authors provided us prior to their work.", "labels": [], "entities": [{"text": "EaD corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.953028678894043}]}, {"text": "The second one is the ECO 6 corpus (.", "labels": [], "entities": [{"text": "ECO 6 corpus", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.9511593580245972}]}, {"text": "It contains 390 texts of ecology domain and its gold standard has 322 unigrams.", "labels": [], "entities": []}, {"text": "The latter is the Nanoscience and Nanotechnology (N&N) corpus) that contains 1,057 texts.", "labels": [], "entities": [{"text": "Nanoscience and Nanotechnology (N&N) corpus", "start_pos": 18, "end_pos": 61, "type": "DATASET", "confidence": 0.5978897247049544}]}, {"text": "Its gold standard has 1,794 unigrams (. In order to preprocess these corpora, we POS tagged them using the PALAVRAS parser) and normalized their words using a stemming 7 technique.", "labels": [], "entities": []}, {"text": "Stemming was chosen because of its capacity to group similar word meanings, and its use decreases representation dimensionality of candidate terms, which minimizes the second and third ATE problems.", "labels": [], "entities": [{"text": "ATE", "start_pos": 185, "end_pos": 188, "type": "METRIC", "confidence": 0.8282889127731323}]}, {"text": "Afterwards, we removed the stopwords 8 , the conjugation of the verb \"to be\", punctuation, numbers, accents, and the words composed of only one character are removed.", "labels": [], "entities": []}, {"text": "We identify and calculate 19 features in which 11 features are used for ATE in the literature, 3 features are normally applied to the attribute selection tasks (identified by *), 1 normally used for Named Entity Recognition (identified by **), and we created 4 new features (identified by \u2206 ).", "labels": [], "entities": [{"text": "ATE", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.6466487050056458}, {"text": "Named Entity Recognition", "start_pos": 199, "end_pos": 223, "type": "TASK", "confidence": 0.6305606365203857}]}, {"text": "These features are shown in, accompanied by the hypotheses that underlie their use.", "labels": [], "entities": []}, {"text": "They are also divided into 3 levels of knowledge: statistical, linguistic, and hybrid.", "labels": [], "entities": []}, {"text": "For the S feature, we removed stopwords at the beginning and at the end of these phrases.", "labels": [], "entities": []}, {"text": "For POS, we assumed that terms may also be adjectives, besides nouns and verbs.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8428748250007629}]}, {"text": "For GC and Freq GC, we used the NILC Corpus 9 as a general corpus, which contains 40 million words.", "labels": [], "entities": [{"text": "NILC Corpus 9", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9791841506958008}]}, {"text": "We created and used 40 indicative phrases (NPs).", "labels": [], "entities": []}, {"text": "For example, considering are composed of as an IP in All organisms are composed of one or more cells, we would consider organisms and cells as TCs.", "labels": [], "entities": []}, {"text": "For features related to CT stem, we analyzed, e.g., the words educative, educators, education and educate that came from the stem educ.", "labels": [], "entities": []}, {"text": "Therefore, educ may have as features N Noun = 2 (educators and education), N Adj = 1 educative, N Verb = 1 (educate), and N PO = 4 (total number of words).", "labels": [], "entities": [{"text": "Verb", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.8018604516983032}]}, {"text": "Our hypothesis is that stemmed candidates that were originated from a higher number of nouns than adjectives or verbs will be terms.", "labels": [], "entities": []}, {"text": "Finally, we used NC-Value adapted to unigrams).", "labels": [], "entities": []}, {"text": "After calculating the features for each unigram (candidate term), the CT representation has high dimensionality (it is the second ATE problem) and, hence, the experiments may take a considerable amount of time to be executed.", "labels": [], "entities": [{"text": "ATE", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.8953152894973755}]}, {"text": "To decrease this dimensionality and, consequently, the number of TCs (which corresponds to the second and third ATE problems, respectively), we tested two different cutoffs, which preserve only TCs that occur in at least two documents in the corpus.", "labels": [], "entities": []}, {"text": "The first cut-off is called C1.", "labels": [], "entities": []}, {"text": "In the second one (called C2), the candidates must be noun and prepositional phrases and also follow some of these POS: nouns, proper nouns, verbs, and adjectives.", "labels": [], "entities": []}, {"text": "The number of obtained candidates (stems) for the ECO, EaD, and N&N corpora, respectively.", "labels": [], "entities": [{"text": "ECO", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.9057530164718628}]}, {"text": "When using the C1 cut-off, we decreased to 55,15%, 45,82%, and 57,04%, and C2 decreased 63.10%, 63.18%, 66.94% in relation to the number of all the obtained candidates (without cutt-offs).", "labels": [], "entities": []}, {"text": "The first evaluation aimed to identify which features must be used for ATE (see Section 3).", "labels": [], "entities": [{"text": "ATE", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.93571537733078}]}, {"text": "For that, we applied 2 methods that select attributes by evaluating the attribute subsets.", "labels": [], "entities": []}, {"text": "Their evaluation is based on consistency (CBF) and correlation (CFS).", "labels": [], "entities": [{"text": "consistency (CBF)", "start_pos": 29, "end_pos": 46, "type": "METRIC", "confidence": 0.9032320529222488}, {"text": "correlation (CFS)", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.9592712074518204}]}, {"text": "We also tested search methods.", "labels": [], "entities": []}, {"text": "The combination of these methods, available in WEKA (, is: CFS SubsetEval using the RankSearch Filter as search method (CFS R), CFS SubsetEval using the BestFirst as search method (CFS BF), CBF SubsetEval using the Ranking Filter (C R), and CBF SubsetEval using the Greedy Stepwise (C G).", "labels": [], "entities": [{"text": "WEKA", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.929002583026886}]}, {"text": "These methods return feature sets that are considered the most representative for the term classification).", "labels": [], "entities": []}, {"text": "For the EaD corpus, the CG attribute selection method did not select any feature.", "labels": [], "entities": [{"text": "EaD corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9041382968425751}, {"text": "CG attribute selection", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6110903720060984}]}, {"text": "For our experiments, we also considered all the features (referred by All).", "labels": [], "entities": []}, {"text": "Additionally, we compared the use of two cut-off types for each feature set, C1 and C2, detailed in Section 4.", "labels": [], "entities": []}, {"text": "For both evaluations 8 , we chose largely known inductors in the machine learning area.", "labels": [], "entities": []}, {"text": "They represent different learning paradigms: JRip (Rule Induction), Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (Probabilistic), J48 (Decision Tree) with confidence factor of 25%, and SMO (Statistical Learning).", "labels": [], "entities": [{"text": "confidence factor", "start_pos": 129, "end_pos": 146, "type": "METRIC", "confidence": 0.952984631061554}, {"text": "SMO", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.7389675974845886}]}, {"text": "All of these algorithms are avail-: Features chosen by the attribute selection methable in WEKA and described in).", "labels": [], "entities": [{"text": "WEKA", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.9225753545761108}]}, {"text": "We run the experiments on a 10 fold crossvalidation and calculated the precision, recall, and F-measure scores of term classification according to the gold standard of unigrams of each corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9997126460075378}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9985163807868958}, {"text": "F-measure scores", "start_pos": 94, "end_pos": 110, "type": "METRIC", "confidence": 0.9736105501651764}, {"text": "term classification", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7121231108903885}]}, {"text": "Using default parameter values for SMO, the results were lower than the other inductors.", "labels": [], "entities": [{"text": "SMO", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.895159125328064}]}, {"text": "Due to this fact and the lack of space in the paper, we do not present the SMO results here.", "labels": [], "entities": [{"text": "SMO", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9186232089996338}]}, {"text": "The best precision obtained for the EaD corpus using the term classification, 66.66%, was achieved by the C R attribute selection method with the C2 cut-off (C R-C2) using the JRIP inductor.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9985155463218689}, {"text": "EaD corpus", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.8673617541790009}]}, {"text": "The best recall score, 20.96%, was obtained using Na\u00a8\u0131veNa\u00a8\u0131ve Bayes with the CFS R-C1 method.", "labels": [], "entities": [{"text": "recall score", "start_pos": 9, "end_pos": 21, "type": "METRIC", "confidence": 0.9805492758750916}, {"text": "CFS", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.6689410209655762}]}, {"text": "The best Fmeasure was 17.58% using the J48 inductor with C R-C2.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995369911193848}]}, {"text": "For the ECO corpus, the best precision was 60% obtained with the J48 inductor with confidence factor of 25% and the C R-C1 method.", "labels": [], "entities": [{"text": "ECO corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9160705804824829}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.99722820520401}]}, {"text": "The best recall was 21.40% with JRIP and the C G-C1 method.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.999677300453186}, {"text": "JRIP", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.708318829536438}]}, {"text": "Our best F-measure was 24.26% obtained with Na\u00a8\u0131veNa\u00a8\u0131ve Bayes using the CFS R-C1 method.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9977331161499023}, {"text": "CFS", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.7742145657539368}]}, {"text": "For the N&N corpus, the best precision score was 61.03% using JRIP.", "labels": [], "entities": [{"text": "N&N corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.6150519400835037}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9995676875114441}, {"text": "JRIP", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9071183800697327}]}, {"text": "The best recall was 52.53% and the best F-measure score was 54.04%, both using J48 inductor with confidence factor of 25%.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9996911287307739}, {"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.993748664855957}]}, {"text": "The three results used the All-C2 method.", "labels": [], "entities": []}, {"text": "shows the comparison of our best results with 2 baselines, which are the well-known term frequency and TFIDF, using our stoplist.", "labels": [], "entities": [{"text": "term frequency", "start_pos": 84, "end_pos": 98, "type": "METRIC", "confidence": 0.773606926202774}, {"text": "TFIDF", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9924913048744202}]}, {"text": "We also considered all the stemmed words of these corpora as CT, except the stopwords, and we calculated the precision, recall, and F-measure scores for these words as well.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9996904134750366}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9954675436019897}, {"text": "F-measure", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9992170333862305}]}, {"text": "Finally, we compared our results with the third baseline, which is the only previous work that uniquely extracts unigrams (, described in Section 2.", "labels": [], "entities": []}, {"text": "Therefore, this is the state of the art for unigrams extraction for Portuguese.", "labels": [], "entities": [{"text": "unigrams extraction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8176086246967316}]}, {"text": "In order to compare this work with our results of the EaD and N&N corpora, we implemented the ATE method of Zavaglia et al.", "labels": [], "entities": [{"text": "ATE", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9773059487342834}]}, {"text": "We have to mention that this method uses the normalization technique called lemmatization instead of stemming, which we used in our method.", "labels": [], "entities": []}, {"text": "The only difference between our implementation descriptions and the original method is that we POS tagged and lemmatizated the texts using the same parser) used in our experiments instead of the MXPOST tagger.", "labels": [], "entities": []}, {"text": "For all used corpora, we obtained better results of precision and F-measure comparing with the baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9997814297676086}, {"text": "F-measure", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9985608458518982}]}, {"text": "In general, we improve the ATE precision scores, for the EaD corpus, eleven times (from 6.1% to 66.66%) and, for the N&N corpus, one and a half times (from 35.4% to 61.03%), both comparing our results with the use of TFIDF.", "labels": [], "entities": [{"text": "ATE", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9900460839271545}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.7135882377624512}, {"text": "EaD corpus", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9306423962116241}, {"text": "N&N corpus", "start_pos": 117, "end_pos": 127, "type": "DATASET", "confidence": 0.6386027708649635}]}, {"text": "For the ECO corpus, we improve four and a half times (from 12.9% to 60%), by comparing with the use of frequency.", "labels": [], "entities": [{"text": "ECO corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8951841592788696}]}, {"text": "We improve the ATE F-measure scores, for the EaD corpus, one and a half times (from 10.93% to 17.58%); for the ECO corpus, we slightly improve the results (from 20.64% to 24.26%); and, for the N&N corpus, two times (from 28.12% to 54.04%).", "labels": [], "entities": [{"text": "ATE F-measure scores", "start_pos": 15, "end_pos": 35, "type": "METRIC", "confidence": 0.8223321239153544}, {"text": "EaD corpus", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9153782427310944}, {"text": "ECO corpus", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.9526196718215942}, {"text": "N&N corpus", "start_pos": 193, "end_pos": 203, "type": "DATASET", "confidence": 0.8770913332700729}]}, {"text": "The last three cases are based on the best F-measure values obtained using TFIDF.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9827647805213928}, {"text": "TFIDF", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.5883273482322693}]}, {"text": "Regarding recall, on the one hand, the linguistic ExPorTer method (detailed in Section 2), to which we also compare our results, achieved better recall for all used corpora, about 89%.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9924097061157227}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9988252520561218}]}, {"text": "On the other hand, its precision (about 2%) and F-measure (about 4%) were significantly lower than our results.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9997116923332214}, {"text": "F-measure", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9992515444755554}]}, {"text": "Finally, if we compare our results with the results of all stemmed words, with the exception of the stopwords, the recall values of the latter are high (about 76%) for all used corpora.", "labels": [], "entities": [{"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9990857839584351}]}, {"text": "However, the precision scores are extremely low (about 1.26%), because it used almost all words of the texts.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996776580810547}]}], "tableCaptions": [{"text": " Table 3: Comparison with baselines.", "labels": [], "entities": []}]}