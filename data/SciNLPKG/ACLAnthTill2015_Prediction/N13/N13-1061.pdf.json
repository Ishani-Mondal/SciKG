{"title": [{"text": "Using Derivation Trees for Informative Treebank Inter-Annotator Agreement Evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper discusses the extension of a system developed for automatic discovery of tree-bank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter-annotator agreement.", "labels": [], "entities": []}, {"text": "This system makes fora more informative IAA evaluation than other systems because it pinpoints the inconsistencies and groups them by their structural types.", "labels": [], "entities": [{"text": "IAA evaluation", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.8403221666812897}]}, {"text": "We evaluate the system on two corpora-(1) a corpus of English web text, and (2) a corpus of Modern British English.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper discusses the extension of a system developed for automatic discovery of treebank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter-annotator agreement (IAA).", "labels": [], "entities": [{"text": "evaluation of inter-annotator agreement (IAA)", "start_pos": 168, "end_pos": 213, "type": "TASK", "confidence": 0.5960808566638401}]}, {"text": "In IAA, two or more annotators annotate the same sentences, and a comparison identifies areas in which the annotators might need more training, or the annotation guidelines some refinement.", "labels": [], "entities": [{"text": "IAA", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.8385968208312988}]}, {"text": "Unlike other IAA evaluation systems, this system application results in a precise pinpointing of inconsistencies and the grouping of inconsistencies by their structural types, making fora more informative IAA evaluation.", "labels": [], "entities": [{"text": "IAA evaluation", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.8630580902099609}, {"text": "IAA evaluation", "start_pos": 205, "end_pos": 219, "type": "TASK", "confidence": 0.8441813886165619}]}, {"text": "Treebank annotation, consisting of syntactic structure with words as the terminals, is by its nature more complex and therefore more prone to error than many other annotation tasks.", "labels": [], "entities": []}, {"text": "However, high annotation consistency is crucial to providing reliable training and testing data for parsers and linguistic research.", "labels": [], "entities": [{"text": "consistency", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.8480609655380249}]}, {"text": "Error detection is therefore an important area of research, and the importance of work such as is that errors and annotation inconsistencies might be automatically discovered, and once discovered, be targeted for subsequent quality control.", "labels": [], "entities": [{"text": "Error detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7702475488185883}]}, {"text": "A recent approach to this problem () (which we will call the KBM system) improves upon by decomposing the full syntactic tree into smaller units, using ideas from Tree Adjoining Grammar (TAG).", "labels": [], "entities": []}, {"text": "This allows the comparison to be based on small syntactic units instead of string n-grams, improving the detection of inconsistent annotation.", "labels": [], "entities": []}, {"text": "The KBM system, like that of before it, is based on the notion of comparing identical strings.", "labels": [], "entities": []}, {"text": "In the general case, this is a problematic assumption, since annotation inconsistencies are missed because of superficial word differences between strings which one would want to compare.", "labels": [], "entities": []}, {"text": "1 However, this limitation is not present for IAA evaluation, since the strings to compare are, by definition, identical.", "labels": [], "entities": [{"text": "IAA evaluation", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.77822145819664}]}, {"text": "The same is also true of parser evaluation, since the parser output and the gold standard are based on the same sentences.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9188264310359955}]}, {"text": "We therefore take the logical step of applying the KBM system developed for automatic discovery of annotation inconsistency to the special case of IAA.", "labels": [], "entities": [{"text": "IAA", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.6465953588485718}]}, {"text": "To our knowledge, this work is the first to utilize such a general system for this special case.", "labels": [], "entities": []}, {"text": "The advantages of the KBM system play out somewhat differently in the context of IAA evaluation than in the more general case.", "labels": [], "entities": [{"text": "IAA evaluation", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7034625709056854}]}, {"text": "In this context, the comparison of word sequences based on syntactic units allows fora precise pinpointing of differences.", "labels": [], "entities": []}, {"text": "The system also retains the ability to group inconsistencies together by their structural type, which we have found to be useful for the more general case.", "labels": [], "entities": []}, {"text": "Together, these two properties make fora useful and informative system for IAA evaluation.", "labels": [], "entities": [{"text": "IAA evaluation", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.9159026741981506}]}, {"text": "In Section 2 we describe the basic working of our system.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss in more detail the advantages of this approach.", "labels": [], "entities": []}, {"text": "In Section 4 we evaluate the system on two treebanks, a corpus of English web text and a corpus of Modern British English.", "labels": [], "entities": [{"text": "Modern British English", "start_pos": 99, "end_pos": 121, "type": "DATASET", "confidence": 0.7555081049601237}]}, {"text": "Section 5 discusses future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Inconsistency types found for system evaluation", "labels": [], "entities": []}]}