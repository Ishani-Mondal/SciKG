{"title": [{"text": "Overcoming the Memory Bottleneck in Distributed Training of Latent Variable Models of Text", "labels": [], "entities": []}], "abstractContent": [{"text": "Large unsupervised latent variable models (LVMs) of text, such as Latent Dirichlet Allocation models or Hidden Markov Models (HMMs), are constructed using parallel training algorithms on computational clusters.", "labels": [], "entities": []}, {"text": "The memory required to hold LVM parameters forms a bottleneck in training more powerful models.", "labels": [], "entities": []}, {"text": "In this paper, we show how the memory required for parallel LVM training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node.", "labels": [], "entities": []}, {"text": "We present a greedy document partitioning technique for the task.", "labels": [], "entities": []}, {"text": "For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing ().", "labels": [], "entities": []}, {"text": "LVM techniques include Latent Dirichlet Allocation (LDA) (, Hidden Markov Models (HMMs), and Probabilistic Latent Semantic Analysis), among others.", "labels": [], "entities": []}, {"text": "LVMs become more predictive as they are trained on more text.", "labels": [], "entities": []}, {"text": "However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity.", "labels": [], "entities": []}, {"text": "The time complexity of LVM training has been addressed through parallel training algorithms (), which reduce LVM training time through the use of large computational clusters.", "labels": [], "entities": []}, {"text": "However, the memory cost for training LVMs remains a bottleneck.", "labels": [], "entities": []}, {"text": "While LVM training makes sequential scans of the corpus (which can be stored on disk), it requires consistent random access to model parameters.", "labels": [], "entities": []}, {"text": "Thus, the model parameters must be stored in memory on each node.", "labels": [], "entities": []}, {"text": "Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times the vocabulary size.", "labels": [], "entities": []}, {"text": "For large models (i.e., with many latent variable values) and large corpora (with large vocabularies), the memory required for training can exceed the limits of the commodity servers comprising modern computational clusters.", "labels": [], "entities": []}, {"text": "Because model accuracy tends to increase with both corpus size and model size, training accurate language models requires that we overcome the memory bottleneck.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9902487397193909}]}, {"text": "We present a simple technique for mitigating the memory bottleneck in parallel LVM training.", "labels": [], "entities": []}, {"text": "Existing parallelization schemes begin by partitioning the training corpus arbitrarily across computational nodes.", "labels": [], "entities": []}, {"text": "In this paper, we show how to reduce memory footprint by instead partitioning the corpus to minimize the number of unique words on each node (and thereby minimize the number of parameters the node must store).", "labels": [], "entities": []}, {"text": "Because corpus partitioning is a pre-processing step in parallel LVM training, our technique can be applied to reduce the memory footprint of essentially any existing LVM or training approach.", "labels": [], "entities": [{"text": "corpus partitioning", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.705697700381279}]}, {"text": "The accuracy of LVM training fora fixed model size and corpus remains unchanged, but intelligent corpus partitioning allows us to train larger and typically more accurate models using the same memory capacity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993007183074951}]}, {"text": "While the general minimization problem we encounter is NP-hard, we develop greedy approximations that work well.", "labels": [], "entities": []}, {"text": "In experiments with both HMM and LDA models, we show that our technique offers large advantages over existing approaches in terms of both memory footprint and execution time.", "labels": [], "entities": []}, {"text": "On a large corpus using 50 nodes in parallel, our best partitioning method can reduce the memory required per node to less than 1/10th that when training without corpus partitioning, and to half that of a random partitioning.", "labels": [], "entities": []}, {"text": "Further, our approach reduces the training time of an existing parallel HMM codebase by 3x.", "labels": [], "entities": []}, {"text": "Our work includes the release of our partitioning codebase, and an associated codebase for the parallel training of HMMs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our partitioning method against the baseline and Z&I, the best performing scalable method from previous work, which uses random document selection and MINIMUM node selection ().", "labels": [], "entities": []}, {"text": "We evaluate on three corpora: the Brown corpus of newswire text, the Reuters Corpus Volume1 (RCV1) (), and a larger WebSent corpus of sentences gathered from the Web (: Characteristics of the three corpora.", "labels": [], "entities": [{"text": "Brown corpus of newswire text", "start_pos": 34, "end_pos": 63, "type": "DATASET", "confidence": 0.9499142050743103}, {"text": "Reuters Corpus Volume1 (RCV1)", "start_pos": 69, "end_pos": 98, "type": "DATASET", "confidence": 0.9436185459295908}]}, {"text": "N = # of documents, V = # of word types, Z = # of tokens.", "labels": [], "entities": []}, {"text": "We treat each sentence as a document in the Brown and Web-Sent corpora.", "labels": [], "entities": []}, {"text": "shows how the maximum word type size V max varies for each method and corpus, for T = 50 nodes.", "labels": [], "entities": []}, {"text": "BJAC significantly decreases V max over the: Maximum word type size V max for each partitioning method, for each corpus.", "labels": [], "entities": [{"text": "BJAC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.49600908160209656}, {"text": "V max", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9685653448104858}, {"text": "word type size V max", "start_pos": 53, "end_pos": 73, "type": "METRIC", "confidence": 0.6421594202518464}]}, {"text": "For the larger corpora, BJAC reduces V max by over 50% compared to the baseline, and by 23% compared to Z&I. random partitioning baseline typically employed in practice.", "labels": [], "entities": [{"text": "BJAC", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9697889089584351}, {"text": "V max", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9774667918682098}]}, {"text": "Furthermore, the advantage of BJAC over the baseline is maintained as more computational nodes are utilized, as illustrated in.", "labels": [], "entities": [{"text": "BJAC", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.5861795544624329}]}, {"text": "BJac reduces V max by a larger factor over the baseline as more computational nodes are employed.", "labels": [], "entities": [{"text": "BJac", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8705016374588013}, {"text": "V max", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9649865627288818}]}, {"text": "We now turn to an evaluation of our corpus partitioning within parallel LVM training systems.", "labels": [], "entities": []}, {"text": "shows the memory footprint required for HMM and LDA training for three different partitioning methods.", "labels": [], "entities": []}, {"text": "We compare BJAC with the random partitioning baseline, Zhu's method, and with allwords, the straightforward approach of simply storing parameters for the entire corpus vocabulary on every node).", "labels": [], "entities": []}, {"text": "All-words has the same memory footprint as when training on a single node.", "labels": [], "entities": []}, {"text": "For large corpora, BJAC reduces memory size per node by approximately a factor of two over the random baseline, and by a factor of 8-11 over all-: Memory footprint of computational nodes in megabytes(MB), using 50 computational nodes.", "labels": [], "entities": [{"text": "BJAC", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.8238150477409363}]}, {"text": "Both models utilize 1000 latent variable values. words.", "labels": [], "entities": []}, {"text": "The results demonstrate that in addition to the well-known savings in computation time offered by parallel LVM training, distributed computation also significantly reduces the memory footprint on each node.", "labels": [], "entities": []}, {"text": "In fact, for the RCV1 corpus, BJAC reduces memory footprint to less than 1/10th that of training with all words on each computational node.", "labels": [], "entities": [{"text": "RCV1 corpus", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.9213200807571411}, {"text": "BJAC", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9853943586349487}]}, {"text": "We next evaluate the execution time for an iteration of model training.", "labels": [], "entities": []}, {"text": "Here, we use a parallel implementation of HMMs, and measure iteration time for training on the Web-sent corpus with 50 hidden states as the number of computational nodes varies.", "labels": [], "entities": []}, {"text": "We compare against the random baseline and against the all-words approach utilized in an existing parallel HMM codebase.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Moving beyond the allwords method to exploit corpus partitioning reduces training iteration time, by a factor of two to three.", "labels": [], "entities": [{"text": "corpus partitioning", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.700671911239624}]}, {"text": "However, differences in partitioning methods have only small effects in iteration time: BJAC has essentially the same iteration time as the random baseline in this experiment.", "labels": [], "entities": [{"text": "BJAC", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.7729865312576294}]}, {"text": "It is also important to consider the additional time required to execute the partitioning methods themselves.", "labels": [], "entities": []}, {"text": "However, in practice this additional time is negligible.", "labels": [], "entities": []}, {"text": "For example, BJAC can partition the Web-sent corpus in 368 seconds, using a single computational node.", "labels": [], "entities": [{"text": "BJAC", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.862830400466919}]}, {"text": "By contrast, training a 200-state HMM on the same corpus requires over a hundred CPU-days.", "labels": [], "entities": []}, {"text": "Thus, BJAC's time to partition has a negligible impact on total training time.", "labels": [], "entities": [{"text": "BJAC", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.4827101230621338}]}], "tableCaptions": [{"text": " Table 1: Characteristics of the three corpora. N = #  of documents, V = # of word types, Z = # of tokens.  We treat each sentence as a document in the Brown  and Web-Sent corpora.", "labels": [], "entities": []}, {"text": " Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.", "labels": [], "entities": [{"text": "BJAC", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.8234905004501343}, {"text": "V max", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.9383077621459961}]}, {"text": " Table 3: Memory footprint of computational nodes  in megabytes(MB), using 50 computational nodes.  Both models utilize 1000 latent variable values.", "labels": [], "entities": []}, {"text": " Table 4: Average iteration time(sec) for training an  HMM with 50 hidden states on Web-Sent. Partition- ing with BJAC outperforms all-words, which stores  parameters for all word types on each node.", "labels": [], "entities": [{"text": "Average iteration time(sec)", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.8124465594689051}, {"text": "BJAC", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.7954360246658325}]}]}