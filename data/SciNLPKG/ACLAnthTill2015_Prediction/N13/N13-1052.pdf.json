{"title": [{"text": "Approximate PCFG Parsing Using Tensor Decomposition", "labels": [], "entities": [{"text": "Approximate", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9788206815719604}, {"text": "PCFG Parsing", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.5488736480474472}]}], "abstractContent": [{"text": "We provide an approximation algorithm for PCFG parsing, which asymptotically improves time complexity with respect to the input grammar size, and prove upper bounds on the approximation quality.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.7324885129928589}]}, {"text": "We test our algorithm on two treebanks, and get significant improvements in parsing speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9718483090400696}]}], "introductionContent": [{"text": "The problem of speeding-up parsing algorithms based on probabilistic context-free grammars (PCFGs) has received considerable attention in recent years.", "labels": [], "entities": [{"text": "speeding-up parsing algorithms based on probabilistic context-free grammars (PCFGs)", "start_pos": 15, "end_pos": 98, "type": "TASK", "confidence": 0.5425142537463795}]}, {"text": "Several strategies have been proposed, including beam-search, best-first and A * . In this paper we focus on the standard approach of approximating the source PCFG in such away that parsing accuracy is traded for efficiency.", "labels": [], "entities": [{"text": "A", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9966804385185242}, {"text": "parsing", "start_pos": 182, "end_pos": 189, "type": "TASK", "confidence": 0.9582769870758057}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9226226806640625}]}, {"text": "gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures.", "labels": [], "entities": []}, {"text": "In the probabilistic domain, approximation by means of regular grammars is also exploited by, who filter long-distance dependencies on-the-fly.", "labels": [], "entities": []}, {"text": "Beyond finite automata approximation, propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence.", "labels": [], "entities": []}, {"text": "Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar.", "labels": [], "entities": [{"text": "filter parsing", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.6681384742259979}]}, {"text": "The approach can also be iterated at several levels.", "labels": [], "entities": []}, {"text": "In the non-probabilistic setting, a similar filtering approach was also proposed by, called \"guided parsing.\"", "labels": [], "entities": []}, {"text": "In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in . We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing.", "labels": [], "entities": [{"text": "tensor decomposition", "start_pos": 212, "end_pos": 232, "type": "TASK", "confidence": 0.7101049423217773}, {"text": "approximate PCFG parsing", "start_pos": 299, "end_pos": 323, "type": "TASK", "confidence": 0.6278870105743408}]}, {"text": "We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.7937171161174774}, {"text": "error upper", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9684452414512634}]}], "datasetContent": [{"text": "In this section, we describe experiments that demonstrate the trade-off between the accuracy of the tensor approximation (and as a consequence, the accuracy of the approximate parsing algorithm) and parsing time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.99945467710495}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9991338849067688}, {"text": "parsing", "start_pos": 199, "end_pos": 206, "type": "TASK", "confidence": 0.9780150055885315}]}, {"text": "Experimental Setting We compare the tensor approximation parsing algorithm versus the vanilla Goodman algorithm.", "labels": [], "entities": [{"text": "tensor approximation parsing", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.7371124227841696}]}, {"text": "Both algorithms were implemented in Java, and the code for both is almost identical, except for the set of instructions which computes the dynamic programming equation for propagating the beliefs up in the tree.", "labels": [], "entities": []}, {"text": "This makes the clocktime comparison reliable for drawing conclusions about the speed of the algorithms.", "labels": [], "entities": []}, {"text": "Our implementation of the vanilla parsing algorithm is linear in the size of the grammar (and not cubic in the number of nonterminals, which would give a worse running time).", "labels": [], "entities": [{"text": "vanilla parsing", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7583582699298859}]}, {"text": "In our experiments, we use the method described in Chi and Kolda (2011) for tensor decomposition.", "labels": [], "entities": [{"text": "tensor decomposition", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.9130087494850159}]}, {"text": "This method is fast, even for large tensors, as long as they are sparse.", "labels": [], "entities": []}, {"text": "Such is the case with the tensors for our grammars.", "labels": [], "entities": []}, {"text": "We use two treebanks for our comparison: the Penn treebank) and the Arabic treebank ().", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9903745055198669}, {"text": "Arabic treebank", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9261848628520966}]}, {"text": "With the Penn treebank, we use sections 2-21 for training a maximum likelihood model and section 22 for parsing, while for the Arabic treebank we divide the data into two sets, of size 80% and 20%, one is used for training a maximum likelihood model and the other is used for parsing.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 9, "end_pos": 22, "type": "DATASET", "confidence": 0.9931863844394684}, {"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9911171197891235}, {"text": "Arabic treebank", "start_pos": 127, "end_pos": 142, "type": "DATASET", "confidence": 0.9572392702102661}, {"text": "parsing", "start_pos": 276, "end_pos": 283, "type": "TASK", "confidence": 0.9857454299926758}]}, {"text": "The number of binary rules in the treebank grammar is 7,240.", "labels": [], "entities": []}, {"text": "Results describes the results of comparing the tensor decomposition algorithm to the vanilla PCFG parsing algorithm.", "labels": [], "entities": [{"text": "tensor decomposition", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7919879257678986}, {"text": "vanilla PCFG parsing", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.6807548006375631}]}, {"text": "The first thing to note is that the running time of the parsing algorithm is linear in r.", "labels": [], "entities": [{"text": "parsing algorithm", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.8970577716827393}]}, {"text": "This indeed validates the asymptotic complexity of the insideoutside component in Goodman's algorithm with the approximate tensors.", "labels": [], "entities": []}, {"text": "It also shows that most of the time during parsing is spent on the inside-outside algorithm, and not on the dynamic programming algorithm which follows it.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9676065444946289}]}, {"text": "In addition, compared to the baseline which uses a vanilla CKY algorithm (linear in the number of rules), we get a speedup of a factor of 4.75 for Arabic (r = 140) and 6.5 for English (r = 260) while retaining similar performance.", "labels": [], "entities": [{"text": "speedup", "start_pos": 115, "end_pos": 122, "type": "METRIC", "confidence": 0.9568869471549988}]}, {"text": "Perhaps more surprising is that using the tensor approximation actually improves performance in several cases.", "labels": [], "entities": []}, {"text": "We hypothesize that the cause of this is that the tensor decomposition requires less parameters to express the rule probabilities in the grammar, and therefore leads to better generalization than a vanilla maximum likelihood estimate.", "labels": [], "entities": []}, {"text": "We include results fora more complex model for Arabic, which uses horizontal Markovization of order 1 and vertical Markovization of order 2 (.", "labels": [], "entities": []}, {"text": "This grammar includes 2,188 binary rules.", "labels": [], "entities": []}, {"text": "Parsing exhaustively using this grammar takes 1.30 seconds per sentence (on average) with an F 1 measure of 64.43.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9844617247581482}]}, {"text": "Parsing with tensor decomposition for r = 280 takes 0.62 seconds per sentence (on average) with an F 1 measure of 64.05.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9706805944442749}, {"text": "F 1 measure", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.9817118446032206}]}], "tableCaptions": [{"text": " Table 1: Results for the Arabic and English treebank of parsing using a vanilla PCFG with and without tensor decom- position. Speed is given in seconds per sentence.", "labels": [], "entities": [{"text": "Speed", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9599331617355347}]}]}