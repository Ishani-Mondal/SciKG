{"title": [{"text": "Knowledge-Rich Morphological Priors for Bayesian Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a morphology-aware nonparamet-ric Bayesian model of language whose prior distribution uses manually constructed finite-state transducers to capture the word formation processes of particular languages.", "labels": [], "entities": []}, {"text": "This relaxes the word independence assumption and enables sharing of statistical strength across, for example, stems or inflectional paradigms in different contexts.", "labels": [], "entities": [{"text": "word independence", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7385706007480621}]}, {"text": "Our model can be used in virtually any scenario where multinomial distributions over words would be used.", "labels": [], "entities": []}, {"text": "We obtain state-of-the-art results in language modeling, word alignment, and un-supervised morphological disambiguation fora variety of morphologically rich languages.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6946957260370255}, {"text": "word alignment", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.8108201622962952}]}], "introductionContent": [{"text": "Despite morphological phenomena's salience inmost human languages, many NLP systems treat fully inflected forms as the atomic units of language.", "labels": [], "entities": []}, {"text": "By assuming independence of lexical stems' various surface forms, this avoidance approach exacerbates the problem of data sparseness.", "labels": [], "entities": []}, {"text": "If it is employed at all, morphological analysis of text tends to be treated as a preprocessing step to other NLP modules.", "labels": [], "entities": []}, {"text": "While this latter disambiguation approach helps address data sparsity concerns, it has substantial drawbacks: it requires supervised learning from expert-annotated corpora, and determining the optimal morphological granularity is labor-intensive ().", "labels": [], "entities": []}, {"text": "Neither approach fully exploits the finite-state transducer (FST) technology that has been so successful for modeling the mapping between surface forms and their morphological analyses, and the mature collections of high quality transducers that already exist for many languages (e.g., Turkish, Russian, Arabic).", "labels": [], "entities": []}, {"text": "Much linguistic knowledge is encoded in such FSTs.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 45, "end_pos": 49, "type": "TASK", "confidence": 0.7806940078735352}]}, {"text": "In this paper, we develop morphology-aware nonparametric Bayesian language models that bring together hand-written FSTs with statistical modeling and require no token-level annotation.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 115, "end_pos": 119, "type": "TASK", "confidence": 0.8742281198501587}]}, {"text": "The sparsity issue discussed above is addressed by hierarchical priors that share statistical strength across different inflections of the same stem by backing off to word formation models that piece together morphemes using FSTs.", "labels": [], "entities": [{"text": "word formation", "start_pos": 167, "end_pos": 181, "type": "TASK", "confidence": 0.7063395380973816}]}, {"text": "Furthermore, because of the nonparametric formulation of our models, the regular morphological patterns found in the long tail of word types will rely more heavily on deeper analysis, while frequent and idiosyncratically behaved forms are modeled opaquely.", "labels": [], "entities": []}, {"text": "Our prior can be used in virtually any generative model of language as a replacement for multinomial distributions over words, bringing morphological awareness to numerous applications.", "labels": [], "entities": []}, {"text": "For various morphologically rich languages, we show that: \u2022 our model can provide rudimentary unsupervised disambiguation fora highly ambiguous analyzer; \u2022 integrating morphology into n-gram language models allows better generalization to unseen words and can improve the performance of applications that are truly open vocabulary; and \u2022 bilingual word alignment models also benefit greatly from sharing translation information across stems.", "labels": [], "entities": [{"text": "bilingual word alignment", "start_pos": 338, "end_pos": 362, "type": "TASK", "confidence": 0.6628026266892751}]}, {"text": "We are particularly interested in low-resource scenarios, where one has to make the most of the small quantity of available data, and overcoming data sparseness is crucial.", "labels": [], "entities": []}, {"text": "If analyzers exist in such settings, they tend to be highly ambiguous, and annotated data for learning to disambiguate are also likely to be scarce or non-existent.", "labels": [], "entities": []}, {"text": "Therefore, in our experiments with Russian, we compare two analyzers: a rapidly-developed guesser, which models regular inflectional paradigms but contains no lexicon or irregular forms, and a high-quality analyzer.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train the unigram model on a 1.7M-word corpus of TED talks transcriptions translated into Russian () and evaluate our analyzer against a test set consisting of 1,500 goldstandard analyses obtained from the morphology disambiguation task of the DIALOG 2010 conference (.", "labels": [], "entities": [{"text": "morphology disambiguation task of the DIALOG 2010 conference", "start_pos": 209, "end_pos": 269, "type": "TASK", "confidence": 0.785006944090128}]}, {"text": "Each analysis is composed of a lemma (\u0438\u0432\u0440\u0438\u0442), apart of speech (Noun), and a sequence of additional functional morphemes (Masc,Prep,Sg).", "labels": [], "entities": []}, {"text": "We consider only open-class categories: nouns, ad-jectives, adverbs and verbs, and evaluate the output of our model with three metrics: the lemma accuracy, the part-of-speech accuracy, and the morphology F -measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.8154990077018738}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.7149243354797363}, {"text": "F -measure", "start_pos": 204, "end_pos": 214, "type": "METRIC", "confidence": 0.948955774307251}]}, {"text": "As a baseline, we consider picking a random analysis from output of the analyzer or choosing the most frequent lemma and the most frequent morphological pattern.", "labels": [], "entities": []}, {"text": "8 Then, we use our model with each of the three versions of the pattern model described in \u00a72.2.", "labels": [], "entities": []}, {"text": "Finally, as an upper bound, we use the gold standard to select one of the analyses produced by the guesser.", "labels": [], "entities": []}, {"text": "Since our evaluation is not directly comparable to the standard for this task, we use for reference a high-quality analyzer from Xerox 9 disambiguated with the MP 0 model (all of the models have very close accuracy in this case).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.9892541170120239}]}, {"text": "We train several trigram models on the Russian TED talks corpus used in the previous section.", "labels": [], "entities": [{"text": "Russian TED talks corpus", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.651222325861454}]}, {"text": "Our baseline is a hierarchical PY trigram model with a trigram character model as the base word distribution.", "labels": [], "entities": []}, {"text": "We compare it with our model using the same character model for the base stem distribution.", "labels": [], "entities": []}, {"text": "Both of the morphological analyzers described in the previous section help obtaining perplexity reductions (Table 2).", "labels": [], "entities": []}, {"text": "We ran a similar experiment on the Turkish version of this corpus (1.6M words) with a highquality analyzer and obtain even larger gains   These results can partly be attributed to the high OOV rate in these conditions: 4% for the Russian corpus and 6% for the Turkish corpus.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9844967126846313}, {"text": "Turkish corpus", "start_pos": 260, "end_pos": 274, "type": "DATASET", "confidence": 0.7781257927417755}]}, {"text": "We evaluate the alignment error rate of our models for two language pairs with rich morphology on the target side.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 16, "end_pos": 36, "type": "METRIC", "confidence": 0.7249930202960968}]}, {"text": "We compare to alignments inferred using IBM Model 4 trained with EM (, 10 aversion of our baseline model (described above) without PY priors (learned using EM), and the PY-based baseline.", "labels": [], "entities": []}, {"text": "We consider two language pairs.", "labels": [], "entities": []}, {"text": "English-Turkish We use a 2.8M word cleaned version of the South-East European Times corpus and gold-standard alignments from.", "labels": [], "entities": [{"text": "South-East European Times corpus", "start_pos": 58, "end_pos": 90, "type": "DATASET", "confidence": 0.830491378903389}]}, {"text": "Our morphological analyzer is identical to the one used in the previous sections.", "labels": [], "entities": []}, {"text": "English-Czech We use the 1.3M word News Commentary corpus and gold-standard alignments We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. from.", "labels": [], "entities": [{"text": "News Commentary corpus", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.6031430860360464}]}, {"text": "The morphological analyzer is provided by Xerox.", "labels": [], "entities": [{"text": "morphological analyzer", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8653003871440887}, {"text": "Xerox", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.9590993523597717}]}, {"text": "Results Results are shown in.", "labels": [], "entities": []}, {"text": "Our lightly parameterized model performs much better than IBM Model 4 in these small-data conditions.", "labels": [], "entities": []}, {"text": "With an identical model, we find PY priors outperform traditional multinomial distributions.", "labels": [], "entities": []}, {"text": "Adding morphology further reduced the alignment error rate, for both languages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 38, "end_pos": 58, "type": "METRIC", "confidence": 0.7724315126736959}]}, {"text": "As an example of how our model generalizes better, consider the sentence pair in, taken from the evaluation data.", "labels": [], "entities": []}, {"text": "The two words composing the Turkish sentence are not found elsewhere in the corpus, but several related inflections occur.", "labels": [], "entities": []}, {"text": "11 It is therefore trivial for the stem-base model to find the correct alignment (marked in black), while all the other models have no evidence for it and choose an arbitrary alignment (gray points).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Russian morphological disambiguation.", "labels": [], "entities": [{"text": "Russian morphological disambiguation", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6978504459063212}]}, {"text": " Table 2: Evaluation of the Russian n-gram model.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of Turkish predictive text input.", "labels": [], "entities": [{"text": "Turkish predictive text input", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6444417163729668}]}, {"text": " Table 5. Our lightly  parameterized model performs much better than  IBM Model 4 in these small-data conditions. With  an identical model, we find PY priors outperform  traditional multinomial distributions. Adding mor- phology further reduced the alignment error rate, for  both languages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 249, "end_pos": 269, "type": "METRIC", "confidence": 0.8070317904154459}]}, {"text": " Table 5: Word alignment experiments on English-Turkish  (en-tr) and English-Czech (en-cs) data.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7668370604515076}]}]}