{"title": [], "abstractContent": [{"text": "We consider the problem of training a statistical parser in the situation when there are multiple treebanks available, and these tree-banks are annotated according to different linguistic conventions.", "labels": [], "entities": []}, {"text": "To address this problem, we present two simple adaptation methods: the first method is based on the idea of using a shared feature representation when parsing multiple treebanks, and the second method on guided parsing where the output of one parser provides features fora second one.", "labels": [], "entities": []}, {"text": "To evaluate and analyze the adaptation methods , we train parsers on treebank pairs in four languages: German, Swedish, Italian, and En-glish.", "labels": [], "entities": []}, {"text": "We see significant improvements for all eight treebanks when training on the full training sets.", "labels": [], "entities": []}, {"text": "However, the clearest benefits are seen when we consider smaller training sets.", "labels": [], "entities": []}, {"text": "Our experiments were carried outwith unlabeled dependency parsers, but the methods can easily be generalized to other feature-based parsers.", "labels": [], "entities": []}], "introductionContent": [{"text": "When developing a data-driven syntactic parser, we need to fit the parameters of its statistical model on a collection of syntactically annotated sentences -a treebank.", "labels": [], "entities": []}, {"text": "Generally speaking, a larger collection of examples in the training treebank will give a higher quality of the resulting parser, but the cost in time and effort of annotating training sentences is fairly high.", "labels": [], "entities": []}, {"text": "Most existing treebanks are in the range of a few thousand sentences.", "labels": [], "entities": []}, {"text": "However, there is an abundance of theoretical models of syntax and there is no consensus on how treebanks should be annotated.", "labels": [], "entities": []}, {"text": "For some languages, there exist multiple treebanks annotated according to different syntactic theories.", "labels": [], "entities": []}, {"text": "Apart from German, Swedish, and Italian, which will be considered in this paper, there are important examples among the world's major languages, such as Arabic and Chinese.", "labels": [], "entities": []}, {"text": "To exemplify how syntactic annotation conventions may differ in even such a simple case as unlabeled dependency annotation, consider the Italian sentence fragment la sospensione o l'interruzione ('the suspension or the interruption') in.", "labels": [], "entities": []}, {"text": "As we will see in detail in \u00a73.1.3, there are two Italian treebanks: the ISST and TUT.", "labels": [], "entities": [{"text": "ISST", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.7402058839797974}, {"text": "TUT", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.6191504001617432}]}, {"text": "If annotating as in the ISST treebank (drawn above the sentence) determiners (la, l') are annotated as dependents of the following nouns (sospensione, interruzione); in TUT (drawn below the sentence), we have the reverse situation.", "labels": [], "entities": [{"text": "ISST treebank", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9244098365306854}, {"text": "TUT", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.6636976599693298}]}, {"text": "There are also differences in how coordinate structures are represented: in ISST, the two conjuncts are directly conjoined and the conjunction attached to the first of them, while in TUT the conjunction acts as a link between the conjuncts.", "labels": [], "entities": [{"text": "ISST", "start_pos": 76, "end_pos": 80, "type": "TASK", "confidence": 0.7972652316093445}]}, {"text": "o sospensione la interruzione l' Given the high cost of treebank annotation and the importance of a proper amount of data for parser development, this situation is frustrating.", "labels": [], "entities": [{"text": "parser development", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.9502069354057312}]}, {"text": "How could we then make use of multiple treebanks when training a parser?", "labels": [], "entities": []}, {"text": "A na\u00a8\u0131vena\u00a8\u0131ve way would be simply to concatenate them, but as we will see this results in a parser that performs badly on all the treebanks.", "labels": [], "entities": []}, {"text": "In this paper, we investigate two simple adaptation methods to bridge the gap between differing syntactic annotation styles, allowing us to use more data for parser training.", "labels": [], "entities": []}, {"text": "The first approach treats the problem of parsing with multiple syntactic annotation styles as a multiview learning problem and addresses it by using feature representation that is partly shared between the views.", "labels": [], "entities": [{"text": "parsing with multiple syntactic annotation styles", "start_pos": 41, "end_pos": 90, "type": "TASK", "confidence": 0.8110522131125132}]}, {"text": "In the second one we use a parser trained on one treebank to guide anew parser trained on another treebank.", "labels": [], "entities": []}, {"text": "We evaluate these methods as well as their combination on four languages: German, Swedish, Italian, and English.", "labels": [], "entities": []}, {"text": "In all four languages, we see a similar picture: the shared features approach is generally better when one of the treebanks is very small, while the guided parsing approach is better when the treebanks are more similar in size.", "labels": [], "entities": []}, {"text": "However, for most training set sizes the combination of the the two methods achieves a higher performance than either of them individually.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out experiments to evaluate the crossframework adaptation methods.", "labels": [], "entities": [{"text": "crossframework adaptation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.8476310074329376}]}, {"text": "The evaluations were carried out using the official CoNLL-X evaluation script using the default parameters.", "labels": [], "entities": [{"text": "CoNLL-X evaluation script", "start_pos": 52, "end_pos": 77, "type": "DATASET", "confidence": 0.8594784736633301}]}, {"text": "Since our parsers do not predict edge labels, we report unlabeled attachment scores in all tables and plots.", "labels": [], "entities": []}, {"text": "In our experiments, we used four languages: German, Swedish, Italian, and English.", "labels": [], "entities": []}, {"text": "For each language, we had two treebanks.", "labels": [], "entities": []}, {"text": "Our approaches currently require that the treebanks use the same tokenization conventions, so for Italian and Swedish we automatically retokenized the treebanks.", "labels": [], "entities": []}, {"text": "We also made sure that the two treebanks for one language used the same part-of-speech tag sets, by applying an automatic tagger when necessary.", "labels": [], "entities": []}, {"text": "We trained new parsers using the shared features and guided parsing adaptation methods described in \u00a72.", "labels": [], "entities": []}, {"text": "Additionally, we trained parsers using both methods at the same time; we refer to these parsers as combined.", "labels": [], "entities": []}, {"text": "Including the baseline parsers, this gave us 24 parsers to evaluate on their respective test sets.", "labels": [], "entities": []}, {"text": "The results for German are given in.", "labels": [], "entities": [{"text": "German", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.9319567680358887}]}, {"text": "Here, we see that all three adaptation methods give statistically significant 4 improvements over the baseline when parsing the Tiger treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 116, "end_pos": 123, "type": "TASK", "confidence": 0.9722264409065247}, {"text": "Tiger treebank", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.9846947193145752}]}, {"text": "In particular, the combined method gives a strong 0.7-point improvement, a 6% error reduction.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 78, "end_pos": 93, "type": "METRIC", "confidence": 0.9785967469215393}]}, {"text": "For T\u00fcBa-D/Z, the improvements are smaller, although still significant except for the guided parsing method.", "labels": [], "entities": [{"text": "guided parsing", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.44345586001873016}]}], "tableCaptions": [{"text": " Table 1: Baseline performance figures.", "labels": [], "entities": []}, {"text": " Table 2. Here,  we see that all three adaptation methods give statis- tically significant 4 improvements over the baseline  when parsing the Tiger treebank. In particular, the  combined method gives a strong 0.7-point improve- ment, a 6% error reduction. For T\u00fcBa-D/Z, the im- provements are smaller, although still significant ex- cept for the guided parsing method.", "labels": [], "entities": [{"text": "Tiger treebank", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.9653129875659943}, {"text": "error", "start_pos": 239, "end_pos": 244, "type": "METRIC", "confidence": 0.9498246312141418}, {"text": "guided parsing", "start_pos": 346, "end_pos": 360, "type": "TASK", "confidence": 0.6513516008853912}]}, {"text": " Table 2: Performance figures for the German adapted  parsers. Results that are significantly different from the  baseline performances are written in boldface.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the Swedish adapted parsers.", "labels": [], "entities": []}, {"text": " Table 4: Performance of the Italian adapted parsers.", "labels": [], "entities": []}, {"text": " Table 5.  The shared features and combined methods gave sta- tistically significant improvements for the WSJ Part  1 parser, and the guided parsing method an improve- ment that is nearly significant. However the most  dramatic change is the 1.2-point improvement of the  WSJ Part 2 parser, given by the guided parsing and  combined methods. It is possible that this result  partly can be explained by the fact that this exper- iment is a bit cleaner: in particular, as outlined in   \u00a73.1.4, there are no domain differences.", "labels": [], "entities": [{"text": "WSJ Part  1 parser", "start_pos": 106, "end_pos": 124, "type": "DATASET", "confidence": 0.9059537053108215}, {"text": "WSJ Part 2 parser", "start_pos": 272, "end_pos": 289, "type": "DATASET", "confidence": 0.91995769739151}]}, {"text": " Table 5: Performance of the English adapted parsers.", "labels": [], "entities": []}]}