{"title": [], "abstractContent": [{"text": "LDA-frames is an unsupervised approach for identifying semantic frames from semantically unlabeled text corpora, and seems to be a useful competitor for manually created databases of selectional preferences.", "labels": [], "entities": [{"text": "identifying semantic frames from semantically unlabeled text corpora", "start_pos": 43, "end_pos": 111, "type": "TASK", "confidence": 0.7379050403833389}]}, {"text": "The most limiting property of the algorithm is such that the number of frames and roles must be pre-defined.", "labels": [], "entities": []}, {"text": "In this paper we present a modification of the LDA-frames algorithm allowing the number of frames and roles to be determined automatically, based on the character and size of training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic frames and valency lexicons are useful lexical sources capturing semantic roles valid fora set of lexical units.", "labels": [], "entities": []}, {"text": "The structures of linked semantic roles are called semantic frames.", "labels": [], "entities": []}, {"text": "Linguists are using them for their ability to describe an interface between syntax and semantics.", "labels": [], "entities": []}, {"text": "In practical natural language processing applications, they can be used, for instance, for the word sense disambiguation task or in order to resolve ambiguities in syntactic analysis of natural languages.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 95, "end_pos": 125, "type": "TASK", "confidence": 0.7998814135789871}, {"text": "resolve ambiguities in syntactic analysis of natural languages", "start_pos": 141, "end_pos": 203, "type": "TASK", "confidence": 0.732840359210968}]}, {"text": "The lexicons of semantic frames or verb valencies are mainly created manually or semi-automatically by highly trained linguists.", "labels": [], "entities": []}, {"text": "Manually created lexicons involve, for instance, a well-known lexicon of semantic frames FrameNet () or a lexicon of verb valencies VerbNet ().", "labels": [], "entities": []}, {"text": "These and other similar lexical resources have many promising applications, but suffer from several disadvantages: \u2022 Creation of them requires manual work of trained linguists which is very time-consuming and expensive.", "labels": [], "entities": []}, {"text": "\u2022 Coverage of the resources is usually small or limited to some specific domain.", "labels": [], "entities": []}, {"text": "\u2022 Most of the resources do not provide any information about relative frequency of usage in corpora.", "labels": [], "entities": []}, {"text": "For instance, both patterns and acquire reflect correct usage of verb acquire, but the former is much more frequent in English.", "labels": [], "entities": []}, {"text": "\u2022 Notion of semantic classes and frames is subjectively biased when the frames are created manually without corpus evidence.", "labels": [], "entities": [{"text": "Notion of semantic classes and frames", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.8576603730519613}]}, {"text": "In order to avoid those problems we proposed a method for creating probabilistic semantic frames called LDA-frames.", "labels": [], "entities": []}, {"text": "The main idea of LDA-frames is to generate the set of semantic frames and roles automatically by maximizing posterior probability of a probabilistic model on a syntactically annotated training corpus.", "labels": [], "entities": []}, {"text": "A semantic role is represented as probability distribution overall its realizations in the corpus, a semantic frame as a tuple of semantic roles, each of them connected with some grammatical relation.", "labels": [], "entities": []}, {"text": "For every lexical unit (a verb in case of computing verb valencies), a probability distribution overall semantic frames is generated, where the probability of a frame corresponds to the relative frequency of usage in the corpus fora given lexical unit.", "labels": [], "entities": []}, {"text": "An example of LDA-frames computed on the British National Corpus is available at the LDA-frames website . The original LDA-frames algorithm has two parameters that must be predefined -number of frames and number of roles -which is the most limiting property of the algorithm.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.9556372761726379}]}, {"text": "A simple cross-validation approach can be used in case of very small data.", "labels": [], "entities": []}, {"text": "However, real data is much bigger and it is not recommended to use such techniques.", "labels": [], "entities": []}, {"text": "For example, the inference on the British National Corpus using a single core 2.4 GHz CPU takes several days to compute one reasonable combination of parameters.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.9393497308095297}]}, {"text": "In this paper we present a non-parametric modification of the LDA-frames algorithm allowing to determine the parameters automatically, based on the character and size of training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "The non-parametric algorithm was evaluated by an experiment on a synthetic data set consisting of 155 subject-object tuples.", "labels": [], "entities": []}, {"text": "The training data was generated randomly from a predefined set of 7 frames and 4 roles for 16 verbs using the following algorithm.", "labels": [], "entities": []}, {"text": "For every lexical unit u: 1.", "labels": [], "entities": []}, {"text": "Choose a number of corpus realizations N u \u2208 {5, . .", "labels": [], "entities": []}, {"text": ", 15} from the uniform distribution.", "labels": [], "entities": []}, {"text": "2. For each realization nu \u2208 {1, . .", "labels": [], "entities": []}, {"text": ", N u }, among all permitted frames for lexical unit u, choose a semantic frame f nu from the uniform distribution.", "labels": [], "entities": []}, {"text": "3. For each frame f nu , generate a realization of all its roles from the uniform distribution.", "labels": [], "entities": []}, {"text": "Each semantic role had 6 possible realizations on average, some of them assigned to more than one semantic role to reflect the character of real languages.", "labels": [], "entities": []}, {"text": "Since the data was generated artificially, we knew the number of frames and roles, how the frames were defined, and which frame and which role was responsible for generating each realization in the data.", "labels": [], "entities": []}, {"text": "We ran the non-parametric algorithm with hyperparameters \u03b1 = 5, \u03b2 = \u03b3 = 0.1, \u03b4 = 1.5.", "labels": [], "entities": []}, {"text": "It has been shown that the selection of hyperparameters has little impact on the resulting frames when they are in some reasonable range, thus, the hyperparameters were chosen empirically by hand.", "labels": [], "entities": []}, {"text": "The experiment led to correct assignments off u,t and r f,s after 56 iterations on average (based on 10 independent runs of the algorithm).", "labels": [], "entities": []}, {"text": "In order to compare the non-parametric algorithm with the original, we ran the original algorithm with the same data that had the number of frames and roles set to R \u2208 {1 . .", "labels": [], "entities": []}, {"text": "10}, F \u2208 {1 . .", "labels": [], "entities": []}, {"text": "20}, and measured the perplexity of the data given to the model after convergence.", "labels": [], "entities": []}, {"text": "The perplexities for all settings are shown in.", "labels": [], "entities": []}, {"text": "The lowest perplexity was reached with F = 7, R = 4 and had the same value as the case of the non-parametric algorithm.", "labels": [], "entities": [{"text": "F", "start_pos": 39, "end_pos": 40, "type": "METRIC", "confidence": 0.9988865256309509}, {"text": "R", "start_pos": 46, "end_pos": 47, "type": "METRIC", "confidence": 0.9800717830657959}]}, {"text": "The f u,t and r f,s assignments were correct as well.", "labels": [], "entities": []}, {"text": "We also ran the non-parametric algorithm with the same hyperparameters on real data (1.4 millions of subject-object tuples) acquired from the British National Corpus 2 using the Stanford Parser ().", "labels": [], "entities": [{"text": "British National Corpus 2", "start_pos": 142, "end_pos": 167, "type": "DATASET", "confidence": 0.9579322189092636}, {"text": "Stanford Parser", "start_pos": 178, "end_pos": 193, "type": "DATASET", "confidence": 0.8464335501194}]}, {"text": "The algorithm reached the optimal perplexity with 427 frames and 144 roles.", "labels": [], "entities": []}, {"text": "This experiment has been performed only for illustrating the algorithm on real data.", "labels": [], "entities": []}, {"text": "Because of long running time of the algorithm on such huge data set, we did not perform the same experiments as with the case of the small synthetic data.", "labels": [], "entities": []}], "tableCaptions": []}