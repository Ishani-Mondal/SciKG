{"title": [{"text": "A Beam-Search Decoder for Normalization of Social Media Text with Application to Machine Translation", "labels": [], "entities": [{"text": "Normalization of Social Media Text", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.8931118249893188}, {"text": "Machine Translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7709378302097321}]}], "abstractContent": [{"text": "Social media texts are written in an informal style, which hinders other natural language processing (NLP) applications such as machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7596868872642517}]}, {"text": "Text normalization is thus important for processing of social media text.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7674242258071899}, {"text": "processing of social media text", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.851934814453125}]}, {"text": "Previous work mostly focused on normalizing words by replacing an informal word with its formal form.", "labels": [], "entities": []}, {"text": "In this paper, to further improve other downstream NLP applications , we argue that other normalization operations should also be performed, e.g., missing word recovery and punctuation correction.", "labels": [], "entities": [{"text": "missing word recovery", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.6621632476647695}, {"text": "punctuation correction", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.7982958555221558}]}, {"text": "A novel beam-search decoder is proposed to effectively integrate various normalization operations.", "labels": [], "entities": []}, {"text": "Empirical results show that our system obtains statistically significant improvements over two strong baselines in both normaliza-tion and translation tasks, for both Chinese and English.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.8994100689888}]}], "introductionContent": [{"text": "Social media texts include SMS (Short Message Service) messages, Twitter messages, Facebook updates, etc.", "labels": [], "entities": []}, {"text": "They are different from formal texts due to their significant informal characteristics, so they always pose difficulties for applications such as machine translation (MT) () and named entity recognition ( ), because of alack of training data containing informal texts.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.8334544837474823}, {"text": "named entity recognition", "start_pos": 178, "end_pos": 202, "type": "TASK", "confidence": 0.6174575686454773}]}, {"text": "Thus, the applications always suffer from a substantial performance drop when evaluated on social media texts.", "labels": [], "entities": []}, {"text": "For example, reported a drop from 90% to 76% on part-of-speech tagging, and found a drop of 20% in dependency parsing.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6867437958717346}, {"text": "dependency parsing", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8822892010211945}]}, {"text": "Creating training data of social media texts specifically fora text processing task is time-consuming.", "labels": [], "entities": []}, {"text": "For example, to create parallel Chinese-English training texts for translation of social media texts, it takes three minutes on average to translate an informally written social media text of eleven words from Chinese into English.", "labels": [], "entities": [{"text": "translation of social media texts", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.8530019640922546}]}, {"text": "On the other hand, it takes thirty seconds to normalize the same message, a six-fold increase in speed.", "labels": [], "entities": []}, {"text": "After training a text normalization system to normalize social media texts, we can use an existing statistical machine translation (SMT) system trained on normal texts (non-social media texts) to carryout translation.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7137772589921951}, {"text": "statistical machine translation (SMT)", "start_pos": 99, "end_pos": 136, "type": "TASK", "confidence": 0.7998082836469015}]}, {"text": "So we argue that normalization followed by regular translation is a more practical approach.", "labels": [], "entities": [{"text": "normalization", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.9774213433265686}, {"text": "regular translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.6993804574012756}]}, {"text": "Thus, text normalization is important for social media text processing.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8205878436565399}, {"text": "social media text processing", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6298602297902107}]}, {"text": "Most previous work on normalization of social media text focused on word substitution).", "labels": [], "entities": [{"text": "normalization of social media text", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.8935670137405396}, {"text": "word substitution", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7582955062389374}]}, {"text": "However, we argue that some other normalization operations besides word substitution are also critical for subsequent natural language processing (NLP) applications, such as missing word recovery (e.g., zero pronouns) and punctuation correction.", "labels": [], "entities": [{"text": "word substitution", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7664154469966888}, {"text": "missing word recovery", "start_pos": 174, "end_pos": 195, "type": "TASK", "confidence": 0.6479027271270752}, {"text": "punctuation correction", "start_pos": 222, "end_pos": 244, "type": "TASK", "confidence": 0.8243750333786011}]}, {"text": "In this paper, we propose a novel beam-search decoder for normalization of social media text for MT.", "labels": [], "entities": [{"text": "normalization of social media text", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.8672939419746399}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9817028045654297}]}, {"text": "Our decoder can effectively integrate different normalization operations together.", "labels": [], "entities": []}, {"text": "In contrast to previous work, some of our normalization operations are specifically designed for MT, e.g., missing word recovery based on conditional random fields (CRF) () and punctuation correction based on dynamic conditional random fields (DCRF) ().", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9929117560386658}, {"text": "missing word recovery", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.6314816474914551}, {"text": "punctuation correction", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.7324798256158829}]}, {"text": "To the best of our knowledge, our work is the first to perform missing word recovery and punctuation correction for normalization of social media text, and also the first to perform message-level normalization of Chinese social media text.", "labels": [], "entities": [{"text": "missing word recovery", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.6504777371883392}, {"text": "punctuation correction", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.6387193948030472}, {"text": "normalization of social media text", "start_pos": 116, "end_pos": 150, "type": "TASK", "confidence": 0.8540680646896363}, {"text": "message-level normalization of Chinese social media text", "start_pos": 182, "end_pos": 238, "type": "TASK", "confidence": 0.7731022111007145}]}, {"text": "We investigate the effects on translating social media text after addressing various characteristics of informal social media text through normalization.", "labels": [], "entities": [{"text": "translating social media text", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.8752629905939102}]}, {"text": "To show the applicability of our normalization approach for different languages, we experiment with two languages, Chinese and English.", "labels": [], "entities": []}, {"text": "We achieved statistically significant improvements over two strong baselines: an improvement of 9.98%/7.35% in BLEU scores for normalization of Chinese/English social media text, and an improvement of 1.38%/1.35% in BLEU scores for translation of Chinese/English social media text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9963251948356628}, {"text": "normalization of Chinese/English social media text", "start_pos": 127, "end_pos": 177, "type": "TASK", "confidence": 0.8990672677755356}, {"text": "BLEU", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9931193590164185}, {"text": "translation of Chinese/English social media text", "start_pos": 232, "end_pos": 280, "type": "TASK", "confidence": 0.8617484346032143}]}, {"text": "We created two corpora: a Chinese corpus containing 1,000 Weibo 1 messages with their normalizations and English translations; and another similar English corpus containing 2,000 SMS messages from the NUS SMS corpus).", "labels": [], "entities": [{"text": "NUS SMS corpus", "start_pos": 201, "end_pos": 215, "type": "DATASET", "confidence": 0.9354244073232015}]}, {"text": "As far as we know, our corpora are the first publicly available Chinese/English corpora for normalization and translation of social media text 2 . performed text normalization of informally written email messages using CRF).", "labels": [], "entities": [{"text": "normalization and translation of social media text", "start_pos": 92, "end_pos": 142, "type": "TASK", "confidence": 0.7530524560383388}, {"text": "text normalization of informally written email messages", "start_pos": 157, "end_pos": 212, "type": "TASK", "confidence": 0.8588675601141793}]}, {"text": "Due to its importance, normalization of social media text has been extensively studied recently.", "labels": [], "entities": [{"text": "normalization of social media text", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.8737755179405212}]}, {"text": "proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns.", "labels": [], "entities": []}, {"text": "used hidden Markov model to perform word-level normalization.", "labels": [], "entities": [{"text": "word-level normalization", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7504957616329193}]}, {"text": "combined MT and automatic speech recognition (ASR) to better normalize French SMS message.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.782288134098053}, {"text": "automatic speech recognition (ASR", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.6468063831329346}, {"text": "normalize French SMS message", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.5755350813269615}]}, {"text": "used an unsupervised noisy channel model considering different word formation processes.", "labels": [], "entities": [{"text": "word formation", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.7453455030918121}]}, {"text": "Han and Baldwin (2011) normalized informal words using morphophonemic similarity.", "labels": [], "entities": []}, {"text": "only dealt with SMS abbreviations.", "labels": [], "entities": []}, {"text": "normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors.", "labels": [], "entities": []}, {"text": "designed a system combining different human perspectives to perform word-level normalization.", "labels": [], "entities": [{"text": "word-level normalization", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.7425137758255005}]}, {"text": "normalized Spanish SMS messages using a normalization and a phonetic dictionary.", "labels": [], "entities": []}, {"text": "For normalization of Chinese social media text, investigated informal phrase detection, and mined informal-formal phrase pairs from Web corpora.", "labels": [], "entities": [{"text": "normalization of Chinese social media text", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.8671708504358927}, {"text": "informal phrase detection", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.653674046198527}]}], "datasetContent": [{"text": "As previous work) mostly focused on word normalization, no data is available with corrected punctuation and recovered missing words.", "labels": [], "entities": [{"text": "word normalization", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7842367589473724}]}, {"text": "We thus create the following two corpora: Chinese-English corpus We crawled 1,000 messages from Weibo which were first normalized into formal Chinese and then translated into formal English.", "labels": [], "entities": []}, {"text": "The first half of the corpus serves as our development set to tune our text normalization decoder for Chinese, while the second half serves as the test set to evaluate text normalization for Chinese and Chinese-English MT.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7003294229507446}, {"text": "text normalization", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.7222457826137543}, {"text": "MT", "start_pos": 219, "end_pos": 221, "type": "TASK", "confidence": 0.7133987545967102}]}, {"text": "English-Chinese corpus From the NUS English SMS corpus), we randomly selected 2,000 messages.", "labels": [], "entities": [{"text": "NUS English SMS corpus", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.9570380449295044}]}, {"text": "The messages were first normalized into formal English and then translated into formal Chinese.", "labels": [], "entities": []}, {"text": "Similar to the Chinese-English corpus, the first half of the corpus serves as our development set while the second half serves as the test set.", "labels": [], "entities": []}, {"text": "The formal corpus used (as described in Section 4) is the concatenation of two Chinese-English spoken parallel corpora: the IWSLT 2009 corpus and another spoken text corpus collected at the Harbin Institute of Technology 3 . The language model used for Chinese (English) text normalization is the Chinese (English) side of the formal corpus and the LDC Chinese (English) Gigaword corpus.", "labels": [], "entities": [{"text": "IWSLT 2009 corpus", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.9737070202827454}, {"text": "Chinese (English) text normalization", "start_pos": 253, "end_pos": 289, "type": "TASK", "confidence": 0.6431363423665365}, {"text": "LDC Chinese (English) Gigaword corpus", "start_pos": 349, "end_pos": 386, "type": "DATASET", "confidence": 0.7748111060687474}]}, {"text": "To evaluate the effect of text normalization on MT, we build phrase-based MT systems using Moses (, with word alignments generated by GIZA++ (.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7185605317354202}, {"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9898343086242676}]}, {"text": "The MT training data contains the above formal corpus and some LDC 4 parallel corpora LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03).", "labels": [], "entities": [{"text": "MT training", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8678111135959625}]}, {"text": "In total, 214M/192M English/Chinese tokens are used to train our MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9667429327964783}]}, {"text": "The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.8281956911087036}, {"text": "FBIS corpus (LDC2003E14)", "start_pos": 107, "end_pos": 131, "type": "DATASET", "confidence": 0.8896444082260132}, {"text": "English (Chinese) Gigaword corpus", "start_pos": 140, "end_pos": 173, "type": "DATASET", "confidence": 0.5766298323869705}]}, {"text": "Our MT systems are tuned on the manually normalized messages of our development sets.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9405243396759033}]}, {"text": "Following), we use BLEU scores () to evaluate text normalization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9990298748016357}, {"text": "text normalization", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7507151067256927}]}, {"text": "We also use BLEU scores to evaluate MT quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9990247488021851}, {"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9910853505134583}]}, {"text": "We use the sign test to determine statistical significance, for both text normalization and translation.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7935373485088348}, {"text": "translation", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.9177954196929932}]}, {"text": "The Chinese-English normalization and translation results are shown in.", "labels": [], "entities": [{"text": "Chinese-English normalization", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.5868938267230988}]}, {"text": "The first group of experiments is the three baselines, and the second group is an oracle experiment using manually normalized messages as the output of text normaliza-  tion which indicates the theoretical upper bounds of perfect normalization.", "labels": [], "entities": []}, {"text": "In the normalization experiments, the ORIGINAL baseline gets a BLEU score of 61.01%, and the LATTICE baseline greatly improves the ORIGINAL baseline by 13.51%, which shows that the dictionary collected from the Internet is highly effective in text normalization.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.982883632183075}, {"text": "LATTICE", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9848527312278748}, {"text": "text normalization", "start_pos": 243, "end_pos": 261, "type": "TASK", "confidence": 0.8025018870830536}]}, {"text": "The PBMT baseline further improves the BLEU score by 2.25%.", "labels": [], "entities": [{"text": "PBMT baseline", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.6155454814434052}, {"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9775095582008362}]}, {"text": "In the corresponding MT experiments, as the normalization BLEU scores increase, the MT BLEU scores also increase.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9442729949951172}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.8449966311454773}, {"text": "MT", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.44385483860969543}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.5116163492202759}]}, {"text": "The third group is the isolated experiments, i.e., each experiment only uses one hypothesis producer.", "labels": [], "entities": []}, {"text": "As expected, the individual hypothesis producers alone do notwork well except the Dictionary hypothesis producer.", "labels": [], "entities": [{"text": "Dictionary hypothesis producer", "start_pos": 82, "end_pos": 112, "type": "DATASET", "confidence": 0.9402468005816141}]}, {"text": "One interesting discovery is that the Dictionary hypothesis producer outperforms the LATTICE baseline, which shows that our normalization decoder can utilize the dictionary more effectively, probably because of the additional features used in our normalization decoder such as the informal word penalty.", "labels": [], "entities": [{"text": "LATTICE baseline", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.7973665297031403}]}, {"text": "The Resegmentation hypothesis producer alone worsens the BLEU scores, since it can only split informal words, and is designed to work together with other hypothesis producers to normalize words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9989532232284546}]}, {"text": "The last group is the combined experiments.", "labels": [], "entities": []}, {"text": "We add each hypothesis producer in the order of its normalization effectiveness in the isolated experiments.", "labels": [], "entities": []}, {"text": "Adding the Punctuation hypothesis producer greatly improves the BLEU scores of both normalization and translation, which confirms the importance of punctuation correction.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9994463324546814}, {"text": "punctuation correction", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.6929891109466553}]}, {"text": "The Pronoun and Interjection hypothesis producers also contribute some improvements.", "labels": [], "entities": []}, {"text": "Finally, Resegmentation significantly improves the normalization/translation BLEU scores by 1.42%/0.35%.", "labels": [], "entities": [{"text": "Resegmentation", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.6912783980369568}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.8198041915893555}]}, {"text": "Compared with the isolated experiments, the combined experiments show that our normalization decoder can effectively integrate different hypothesis producers to achieve better performance for both text normalization and translation.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.7610913217067719}]}, {"text": "Overall, in the Chinese text normalization experiments, our normalization decoder outperforms the best baseline PBMT by 9.98% in BLEU score.", "labels": [], "entities": [{"text": "Chinese text normalization", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.5776806573073069}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9985878467559814}]}, {"text": "In the Chinese-English MT experiments, the normalized texts output by our normalization decoder lead to improved translation quality compared to normalization by the PBMT baseline, by 1.38% in BLEU score.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.8720129132270813}, {"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9989385008811951}]}, {"text": "The English-Chinese normalization and translation results are shown in, with the same experimental setup as in the Chinese-English experiments.", "labels": [], "entities": []}, {"text": "The text normalization BLEU score of the ORIG-INAL baseline is much lower in English compared to Chinese, since the English texts contain more informal words.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6328233182430267}, {"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9718601703643799}, {"text": "ORIG-INAL", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9157487154006958}]}, {"text": "Again, the individual hypothesis producers alone do notwork well, except the Dictionary hypothesis producer.", "labels": [], "entities": [{"text": "Dictionary hypothesis producer", "start_pos": 77, "end_pos": 107, "type": "DATASET", "confidence": 0.938839872678121}]}, {"text": "The Retokenization hypothesis producer greatly improves the normalization/translation BLEU scores by 2.37%/0.86%.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.7165588736534119}]}, {"text": "The Punctuation hypothesis producer helps less for English compared to Chinese, suggesting that our Chinese texts contain noisier punctuation.", "labels": [], "entities": []}, {"text": "Overall, we achieved similar improvements in English text normalization and English-Chinese translation, and the improvements in BLEU scores are 7.35% and 1.35% respectively.", "labels": [], "entities": [{"text": "English text normalization", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.5774035851160685}, {"text": "English-Chinese translation", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.7113905400037766}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9988884329795837}]}], "tableCaptions": [{"text": " Table 1: Occurrence frequency of various informal char- acteristics in 200 Chinese/English social media texts.", "labels": [], "entities": []}, {"text": " Table 3: Statistics of the corpora. CN2EN-dev/CN2EN- test is the development/test set in our Chinese- English experiments. EN2CN-dev/EN2CN-test is the  development/test set in our English-Chinese experi- ments. NEN/NCN denotes manually normalized En- glish/Chinese texts.", "labels": [], "entities": []}, {"text": " Table 4: Chinese-English experimental results.", "labels": [], "entities": []}, {"text": " Table 5: English-Chinese experimental results.", "labels": [], "entities": []}]}