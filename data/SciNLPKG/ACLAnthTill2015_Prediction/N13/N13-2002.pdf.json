{"title": [{"text": "Reducing Annotation Effort on Unbalanced Corpus based on Cost Matrix", "labels": [], "entities": []}], "abstractContent": [{"text": "Annotated corpora play a significant role in many NLP applications.", "labels": [], "entities": []}, {"text": "However, annotation by humans is time-consuming and costly.", "labels": [], "entities": []}, {"text": "In this paper, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes.", "labels": [], "entities": [{"text": "recall predictor", "start_pos": 22, "end_pos": 38, "type": "METRIC", "confidence": 0.9470952451229095}]}, {"text": "We demonstrate the effectiveness of our approach in the context of one form of unbalanced task: annotation of transcribed human-human dialogues for pres-ence/absence of uncertainty.", "labels": [], "entities": []}, {"text": "In two data sets, our cost-matrix based method of uncertainty annotation achieved high levels of recall while maintaining acceptable levels of accuracy.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9995560050010681}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.996955156326294}]}, {"text": "The method is able to reduce human annotation effort by about 80% without a significant loss in data quality, as demonstrated by an extrinsic evaluation showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Annotated corpora are crucial for the development of statistical-based NLP tools.", "labels": [], "entities": []}, {"text": "However, the annotation of corpora is most commonly done by humans, which is time-consuming and costly.", "labels": [], "entities": []}, {"text": "To obtain a higher quality annotated corpus, it is necessary to spend more time and money on data annotation.", "labels": [], "entities": []}, {"text": "For this reason, one often has to accept some tradeoff between data quality and human effort.", "labels": [], "entities": []}, {"text": "A significant proportion of corpora are unbalanced, where the distribution of class categories are heavily skewed towards one or a few categories.", "labels": [], "entities": []}, {"text": "Unbalanced corpora are common in a number of different tasks, such as emotion detection), sentiment classification (), polarity of opinion), uncertainty and correctness of student answers in tutoring dialogue systems), text classification, information extraction (, and soon . In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7437938898801804}, {"text": "sentiment classification", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.9058063626289368}, {"text": "text classification", "start_pos": 219, "end_pos": 238, "type": "TASK", "confidence": 0.8259916603565216}, {"text": "information extraction", "start_pos": 240, "end_pos": 262, "type": "TASK", "confidence": 0.8560594022274017}]}, {"text": "Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9959768652915955}]}, {"text": "The second step is to apply this classifier to the rest of the unlabeled data, where the data are then classified with positive or negative labels.", "labels": [], "entities": []}, {"text": "The last step is to manually check every positive label and correct it if it is wrong.", "labels": [], "entities": [{"text": "correct", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9546757340431213}]}, {"text": "To apply this method to work in practice, two research questions have to be addressed.", "labels": [], "entities": []}, {"text": "The first one is how to get a high-recall classifier.", "labels": [], "entities": []}, {"text": "High recall means only a low proportion of true positives are misclassified (false negatives).", "labels": [], "entities": [{"text": "recall", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9994727969169617}]}, {"text": "This property allows for only positive labels to be corrected by human annotators in the third step, so that annotation effort maybe reduced.", "labels": [], "entities": []}, {"text": "A related and separate research question concerns the overall quality of data when false negatives are not corrected: will a dataset annotated with this method produce the same results as a fully manually annotated version of the same dataset when analyzed for substantive research questions?", "labels": [], "entities": []}, {"text": "In this paper, we will answer the two research questions in the context of one form of binary unbalanced task : annotation of transcribed humanhuman dialogue for presence/absence of uncertainty.", "labels": [], "entities": []}, {"text": "The contribution of this paper is twofold.", "labels": [], "entities": []}, {"text": "First, an extrinsic evaluation demonstrates the utility of our approach, by showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations.", "labels": [], "entities": []}, {"text": "Second, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes such as uncertainty.", "labels": [], "entities": [{"text": "recall predictor", "start_pos": 15, "end_pos": 31, "type": "METRIC", "confidence": 0.9210835695266724}]}], "datasetContent": [{"text": "Even with a high recall classifier, some of the true positive data are labeled incorrectly in the final an- Only Cf n = 1, 2, 3, 5, 10, 15, 20 are reported here due to page limits notated corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9966621398925781}]}, {"text": "In addition, it also changes the distribution of class labels.", "labels": [], "entities": []}, {"text": "To test whether it hurts the overall data quality, we performed an analysis, which demonstrates that this annotation scheme is sufficient to produce quality data.", "labels": [], "entities": []}, {"text": "We attempted to replicate an analysis on the Eng data set, which examines the use of analogy, a cognitive strategy where a source and target knowledge structure are compared in terms of structural correspondences as a strategy for solving problems under uncertainty.", "labels": [], "entities": [{"text": "Eng data set", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9395661552747091}]}, {"text": "The analysis we attempt to replicate here focuses on examining how uncertainty levels change relative to baseline before, during, and after the use of analogies.", "labels": [], "entities": []}, {"text": "The overall Eng transcripts were segmented into one of 5 block types: 1) pre-analogy (Lag -1) blocks, 10 utterances just prior to an analogy episode, 2) during-analogy (Lag 0) blocks, utterances from the beginning to end of an analogy episode, 3) postanalogy (Lag 1) blocks, 10 utterances immediately following an analogy episode, 4) post-post-analogy (Lag 2) blocks, 10 utterances immediately following post-analogy utterances, and 5) baseline blocks, each block of 10 utterances at least 25 utterances away from the other block types.", "labels": [], "entities": []}, {"text": "The measure of uncertainty in each block was the proportion of uncertain utterances.", "labels": [], "entities": []}, {"text": "The sampling strategy for the baseline blocks was designed to provide an estimate of uncertainty levels when the speakers were engaged in pre-analogy, during-analogy, or post-analogy conversation, with the logic being that a certain amount of lag or spillover of uncertainty was assumed to take place surrounding analogy episodes.", "labels": [], "entities": []}, {"text": "shows the relationship of block type to mean levels of uncertainty, comparing the pattern with human vs. classifier-supported uncertainty labels.", "labels": [], "entities": []}, {"text": "The classifier-generated labels were first preprocessed such that all FPs were removed, but FNs remain.", "labels": [], "entities": []}, {"text": "This re-analysis comparison thus provides a test of whether the recall rate is high enough that known statistical effects are not substantially altered or removed.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9872278273105621}]}, {"text": "To examine how different settings of Cf n might impact overall performance, we used labels (corrected for false positives) for 4 different levels of Cf n (1, 5, 10, 20) from the.", "labels": [], "entities": []}, {"text": "In the Eng data analyses, the main findings were that analogy was triggered by local spikes in uncertainty levels (Lag -1 > baseline), replicating re- sults from prior work with the MER dataset (; in contrast to the findings in MER, uncertainty did not reduce to baseline levels following analogy (Lags 1 and 2 > baseline).", "labels": [], "entities": [{"text": "Eng data analyses", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.8031205733617147}, {"text": "MER dataset", "start_pos": 182, "end_pos": 193, "type": "DATASET", "confidence": 0.9242731928825378}]}, {"text": "plots the relationship of block type to mean levels of uncertainty in this data set, comparing the pattern with human vs. classifier-generated uncertainty labels.", "labels": [], "entities": []}, {"text": "shows the standardized mean difference (Cohen's d) from baseline by block type and label source.", "labels": [], "entities": [{"text": "standardized mean difference", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8836472431818644}]}, {"text": "The pattern of effects (Lag -1 > baseline, Lags 1 and 2 > baseline) remains substantially unchanged with the exception of the Lag 2 vs. baseline comparison falling short of statistical significance (although note that the standardized mean difference remains very similar) for Cf n ranging from 20 to 5, although we can observe a noticeable attenuation of effect sizes from Cf n of 5 and below, and a loss of statistical significance for the main effect of uncertainty being significantly higher than baseline for Lag -1 blocks when Cf n = 1.", "labels": [], "entities": [{"text": "standardized mean difference", "start_pos": 222, "end_pos": 250, "type": "METRIC", "confidence": 0.924185315767924}]}, {"text": "The re-analysis clearly demonstrates that the recall rate of the classifier is sufficient to not substantially alter or miss known statistical effects.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9867771565914154}]}, {"text": "We can reasonably extrapolate that using this classifier for uncertainty annotation in other datasets should be satisfactory.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Naive Bayes classifier performance on the MER  (train set) and Eng (test set) with only the words/phrases", "labels": [], "entities": [{"text": "Naive Bayes classifier", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9185144305229187}, {"text": "MER  (train set", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.6834964528679848}, {"text": "Eng (test set", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.755595937371254}]}, {"text": " Table 5. This time, we  compare different classifiers including Naive Bayes  (NB), Decision Tree (DT) and Support Vector Ma- chine (SVM). All of them are implemented using the  open source platform Weka (Hall et al., 2009) with  default parameters.  As we can see, test recall is worse than train recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 271, "end_pos": 277, "type": "METRIC", "confidence": 0.978939414024353}, {"text": "recall", "start_pos": 298, "end_pos": 304, "type": "METRIC", "confidence": 0.7444268465042114}]}, {"text": " Table 5:  Performance with original and new  words/phrases as a feature set: train on the MER  and test on the Eng data for class '1'. TP is true positive;  FP is false positive", "labels": [], "entities": [{"text": "MER", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.8112027645111084}, {"text": "Eng data", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.8039699792861938}, {"text": "TP", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9608362913131714}, {"text": "FP", "start_pos": 158, "end_pos": 160, "type": "METRIC", "confidence": 0.9945230484008789}]}, {"text": " Table 6: Test performance with cost matrix", "labels": [], "entities": []}, {"text": " Table 7: Standardized mean difference (Cohen's d) from  baseline by block type and label source (the Eng data set)  (Note: '*' denotes p < .05, '**' denotes p < .01)", "labels": [], "entities": [{"text": "Standardized mean difference (Cohen's d)", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.7438223659992218}, {"text": "Eng data set", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.9201956192652384}]}]}