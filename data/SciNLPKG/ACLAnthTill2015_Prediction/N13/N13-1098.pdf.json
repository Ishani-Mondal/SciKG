{"title": [{"text": "Differences in User Responses to a Wizard-of-Oz versus Automated System", "labels": [], "entities": []}], "abstractContent": [{"text": "Wizard-of-Oz experimental setup in a dialogue system is commonly used to gather data for informing an automated version of that system.", "labels": [], "entities": []}, {"text": "Previous work has exposed dependencies between user behavior towards systems and user belief about whether the system is automated or human-controlled.", "labels": [], "entities": []}, {"text": "This work examines whether user behavior changes when user belief is held constant and the sys-tem's operator is varied.", "labels": [], "entities": []}, {"text": "We perform a post-hoc experiment using generalizable prosodic and lexical features of user responses to a dialogue system backed with and without a human wizard.", "labels": [], "entities": []}, {"text": "Our results suggest that user responses are different when communicating with a wizarded and an automated system, indicating that wizard data maybe less reliable for informing automated systems than generally assumed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina Wizard-of-Oz (WOZ) experimental setup, some or all of the automated portions of a dialogue system are replaced with a hidden, human evaluator.", "labels": [], "entities": []}, {"text": "This setup is often used to gather data from users who believe they are interacting with an automated system ().", "labels": [], "entities": []}, {"text": "This data can inform a downstream, real automated system.", "labels": [], "entities": []}, {"text": "A WOZ experimental protocol calls for holding \"all other input and output . .", "labels": [], "entities": []}, {"text": "constant so that the only unknown variable is who does the internal processing\").", "labels": [], "entities": []}, {"text": "Thus, hiding the human wizard's input by layers of system interface can render that system believably automated.", "labels": [], "entities": []}, {"text": "An assumption of this WOZ data-gathering strategy is that user behavior will not vary substantially between the WOZ and automated (AUT) experimental setups.", "labels": [], "entities": []}, {"text": "However, it was shown in a dialogue system that training with a small set of data from an automated system gave rise to better performance than training with a large set of data from an analogous wizarded system ().", "labels": [], "entities": []}, {"text": "There, it was suggested that differences in system automation maybe responsible for the performance gap.", "labels": [], "entities": []}, {"text": "It is possible that user responses to these dialogue systems differed substantially.", "labels": [], "entities": []}, {"text": "This paper aims to investigate this possibility by comparing data between a wizarded and automated version of a tutoring dialogue system.", "labels": [], "entities": []}, {"text": "We hypothesize that what users say and how they say it will differ when the only change is whether the system's speech recognition and correctness evaluation components are wizarded or automated.", "labels": [], "entities": [{"text": "speech recognition and correctness evaluation", "start_pos": 112, "end_pos": 157, "type": "TASK", "confidence": 0.792867773771286}]}], "datasetContent": [{"text": "Using both lexical and prosodic features, we aimed to determine whether there exist significant differences in users' turn-level responses to the WOZ and AUT systems.", "labels": [], "entities": [{"text": "WOZ", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.7861161828041077}]}, {"text": "It was suspected that the imperfect accuracy 2 (87%) of the AUT system's evaluations of the (in)correctness of user responses may have led to remedial sub-dialogues being accessed by the AUT system more often, since false-negatives accounted The average word-error rate for these AUT responses was 19%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9917579293251038}, {"text": "AUT system", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.9158539474010468}, {"text": "AUT", "start_pos": 187, "end_pos": 190, "type": "DATASET", "confidence": 0.8947589993476868}]}, {"text": "2 Agreement of \u03ba = 0.7 between the system and human.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9693866968154907}]}, {"text": "for 72% of inaccurate evaluations.", "labels": [], "entities": []}, {"text": "To correct for this imbalance, rather than comparing user responses to all questions, we compared the features of user responses (turns) to each question individually.", "labels": [], "entities": []}, {"text": "We omitted questions which were presented in only one setup 3 as well as turns for which a human transcriber found no user speech.", "labels": [], "entities": []}, {"text": "gives the numbers of users, number of unique questions asked, and total number of user responses contained in the remaining data and used in our investigations.", "labels": [], "entities": []}, {"text": "For prosodic features, we considered duration, pitch, and energy (RMS), each extracted using openSMILE.", "labels": [], "entities": [{"text": "duration", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9957846999168396}, {"text": "pitch, and energy (RMS)", "start_pos": 47, "end_pos": 70, "type": "METRIC", "confidence": 0.7599071349416461}]}, {"text": "From pitch and energy, which contain many samples during a single turn, we extracted features for maximum, minimum, mean, and standard deviation of these readings.", "labels": [], "entities": []}, {"text": "We also considered speech duration and the length of the pause before speech began.", "labels": [], "entities": [{"text": "duration", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.909709632396698}]}, {"text": "This gave us a total of 10 prosodic features.", "labels": [], "entities": []}, {"text": "To account for any differences in recording environment and users' voices, we normalized each prosodic feature by dividing its value on each turn by its value in the first turn of the current problem dialogue for that user.", "labels": [], "entities": []}, {"text": "This normal-ization scheme was chosen for our analysis because it is used in the live system, though we note that alternative methods considering more user responses could be explored in the future.", "labels": [], "entities": []}, {"text": "For lexical features, we used the Linguistic Inquiry and Word Count (LIWC).", "labels": [], "entities": [{"text": "Word Count (LIWC)", "start_pos": 57, "end_pos": 74, "type": "METRIC", "confidence": 0.8137237310409546}]}, {"text": "LIWC (), a word-count dictionary, provides features representing the percentage of words in an utterance falling under particular categories.", "labels": [], "entities": []}, {"text": "Though still a counting strategy, these categories capture higher-level concepts than would simple unigrams.", "labels": [], "entities": []}, {"text": "For example, one category is Tentative(T), which includes words such as \"maybe\", \"perhaps\", and \"guess\".", "labels": [], "entities": []}, {"text": "Less abstract categories, such as Prepositions(P), with words such as \"to\", \"with\", and \"above\", are also generated by LIWC.", "labels": [], "entities": [{"text": "LIWC", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.9078986048698425}]}, {"text": "Using these example categories, the utterance \"Maybe above\" would receive feature vector: 0, . .", "labels": [], "entities": []}, {"text": ", 0, T = 50, 0, . .", "labels": [], "entities": [{"text": "T", "start_pos": 5, "end_pos": 6, "type": "METRIC", "confidence": 0.99610835313797}]}, {"text": ", 0, P = 50, 0, . .", "labels": [], "entities": []}, {"text": ", 0 (1) Human transcriptions of users' speech were made available post-hoc for both system versions.", "labels": [], "entities": []}, {"text": "We extracted 69 LIWC categories as lexical features from these human transcriptions of each user turn.", "labels": [], "entities": []}, {"text": "Between the WOZ and AUT setups, we looked for user response feature differences in two ways.", "labels": [], "entities": [{"text": "AUT", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.4808219373226166}]}, {"text": "First, a Welch's two-tailed t-test was used to compare the distributions of each feature's values between WOZ and AUT user responses per question.", "labels": [], "entities": []}, {"text": "We noted the features found to be significantly different.", "labels": [], "entities": []}, {"text": "Second, we built classification models to distinguish between user responses per question from the WOZ and AUT experiments.", "labels": [], "entities": [{"text": "WOZ and AUT experiments", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.7257411330938339}]}, {"text": "For each question, a J48 4 decision tree model was trained and tested using 10-fold cross validation via the Weka 5 toolkit.", "labels": [], "entities": [{"text": "Weka 5 toolkit", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.9526890913645426}]}, {"text": "Only questions with at least 10 responses between both setups were considered.", "labels": [], "entities": []}, {"text": "Each model was compared against majority-class baseline for its respective question by checking for statistically significant differences in the model's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9962649941444397}]}, {"text": "We tried Logistic Regression and Support Vector classifiers but these were consistently outperformed by J48.", "labels": [], "entities": [{"text": "J48", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.9400638341903687}]}, {"text": "5 http://www.cs.waikato.ac.nz/ml/weka 4 Results  After removing questions with less than 10 responses between the two setups, there remained 97 questions totaling 2980 turns.", "labels": [], "entities": []}, {"text": "Of the J48 models built and tested on each question, 21 of 97 outperformed the majority-class baseline accuracies for those questions with significance p < 0.05.", "labels": [], "entities": []}, {"text": "These 21 questions represented 32.79% of the corpus by turns.", "labels": [], "entities": []}, {"text": "We present in detail the two of these 21 questions with the most turns.", "labels": [], "entities": []}, {"text": "The question \"Would you like to do another problem?\" represented 6.11% of the corpus by turns and the J48 model built for it, shown in, outperformed the baseline accuracy with p < 0.001.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9959352016448975}]}, {"text": "While the Duration feature was the root node, a bigger decision was made by Word Count \u2264 1, for which most responses were from AUT data.", "labels": [], "entities": [{"text": "Word Count \u2264 1", "start_pos": 76, "end_pos": 90, "type": "METRIC", "confidence": 0.7988883256912231}, {"text": "AUT data", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.9213933944702148}]}, {"text": "This result is consistent with literature () that suggests that users interacting with automated systems will be more curt.", "labels": [], "entities": []}, {"text": "The question \"Now let's find the forces exerted on the car in the vertical direction during the collision.", "labels": [], "entities": []}, {"text": "First, what vertical force is always exerted on an object near the surface of the earth?\" represented 1.54% of the corpus by turns and the J48 model built for it, shown in, outperformed the baseline accuracy with p < 0.01.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9989987015724182}]}, {"text": "Again, Duration emerged as the tree root, but here the biggest decision fell to RMS mean.", "labels": [], "entities": [{"text": "Duration", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9121450781822205}, {"text": "RMS", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.4341031312942505}]}, {"text": "Student responses approximately louder than the initial response to the tutor in this question dialogue were marked, almost entirely accurately, as AUT.", "labels": [], "entities": [{"text": "AUT", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.8343838453292847}]}, {"text": "Since both trees were rooted at Duration, we sam- pled common responses from each experiment for both problems.", "labels": [], "entities": [{"text": "Duration", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8960217833518982}]}, {"text": "We noticed that hyper-articulation (speaking slowly, loudly, and enunciating each syllable) was more common in the AUT responses.", "labels": [], "entities": [{"text": "AUT responses", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.5631802082061768}]}, {"text": "For example, one user answering \"Would you like to do another problem?\" took almost 4 seconds to clearly and slowly pronounce the word \"yes\".", "labels": [], "entities": []}, {"text": "We suspect that these hyper-articulations may have contributed to the classifiers' ability to detect WOZ responses based on their brevity.", "labels": [], "entities": []}, {"text": "The performance of the per-question J48 models shows, fora non-trivial portion of the turns, that the experiment-of-origin can be classified based on generalizable prosodic and lexical features alone.", "labels": [], "entities": []}, {"text": "The two trees discussed above demonstrate the simplicity of the models needed to perform this separation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Counts for users, unique questions, and user  turns in each data set.", "labels": [], "entities": []}, {"text": " Table 2: Number of questions for which at least one fea- ture from the feature set was found to differ with signif- icance p < 0.05 between WOZ and AUT responses and  the percentage the corpus represented by those questions,  weighted by the speech turns they comprise.", "labels": [], "entities": [{"text": "signif- icance p", "start_pos": 109, "end_pos": 125, "type": "METRIC", "confidence": 0.8017343282699585}]}, {"text": " Table 3: Features shown to differ with significance p <  0.05 between WOZ and AUT responses in questions  comprising at least 10% of the corpus by turns (CbT).  The numbers of questions these turns comprised and of  questions with greater (W)OZ than (A)UT mean are also  given.", "labels": [], "entities": [{"text": "AUT", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.44447076320648193}]}]}