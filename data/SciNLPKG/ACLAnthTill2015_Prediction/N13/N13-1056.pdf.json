{"title": [{"text": "Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals", "labels": [], "entities": [{"text": "Bilingual Lexicon Induction", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.7671311696370443}]}], "abstractContent": [{"text": "Prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem.", "labels": [], "entities": [{"text": "learning translations from source and target language monolingual texts", "start_pos": 20, "end_pos": 91, "type": "TASK", "confidence": 0.8053666551907858}]}, {"text": "Although many techniques take advantage of a seed bilingual lexicon, this work is the first to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model.", "labels": [], "entities": []}, {"text": "Even in a low resource machine translation setting, where induced translations have the potential to improve performance substantially, it is reasonable to assume access to some amount of data to perform this kind of optimization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.743953675031662}]}, {"text": "Our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task, and our approach yields an average relative gain inaccuracy of nearly 50% over an unsupervised baseline.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.7070938274264336}]}, {"text": "Large gains inaccuracy hold for all 22 languages (low and high resource) that we investigate.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual lexicon induction is the task of identifying word translation pairs using source and target monolingual corpora, which are often comparable.", "labels": [], "entities": [{"text": "Bilingual lexicon induction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8452776273091634}, {"text": "identifying word translation pairs", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.6791858971118927}]}, {"text": "Most approaches to the task are based on the idea that words that are translations of one another have similar distributional properties across languages.", "labels": [], "entities": []}, {"text": "Prior research has shown that contextual similarity, temporal similarity (, and topical information ( \u21e4 Performed while faculty at Johns Hopkins University are all good signals for learning translations from monolingual texts.", "labels": [], "entities": [{"text": "learning translations from monolingual texts", "start_pos": 181, "end_pos": 225, "type": "TASK", "confidence": 0.7111907601356506}]}, {"text": "Most prior work either makes use of only one or two monolingual signals or uses unsupervised methods (like rank combination) to aggregate orthogonal signals ().", "labels": [], "entities": []}, {"text": "Surprisingly, no past research has employed supervised approaches to combine diverse monolingually-derived signals for bilingual lexicon induction.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.675786038239797}]}, {"text": "The field of machine learning has shown decisively that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 163, "end_pos": 194, "type": "TASK", "confidence": 0.6400727331638336}]}, {"text": "For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary, and that same dictionary maybe used for estimating the parameters of a model to combine monolingual signals.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.7389458417892456}]}, {"text": "Alternatively, in a low resource machine translation (MT) setting, it is reasonable to assume a small amount of parallel data from which a bilingual dictionary can be extracted for supervision.", "labels": [], "entities": [{"text": "low resource machine translation (MT)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.8208496911185128}]}, {"text": "In this setting, bilingual lexicon induction is critical for translating source words which do not appear in the parallel data or dictionary.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.752768357594808}]}, {"text": "We frame bilingual lexicon induction as a binary classification problem; fora pair of source and target language words, we predict whether the two are translations of one another or not.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.699361264705658}]}, {"text": "For a given source language word, we score all target language candidates separately and then rerank them.", "labels": [], "entities": []}, {"text": "We use a variety of signals derived from source and target monolingual corpora as features and use supervision to estimate the strength of each.", "labels": [], "entities": []}, {"text": "In this work we: \u2022 Use the following similarity metrics derived from monolingual corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency.", "labels": [], "entities": []}, {"text": "\u2022 For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations.", "labels": [], "entities": [{"text": "predicting translations", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.9241752624511719}]}, {"text": "\u2022 Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9985961318016052}]}], "datasetContent": [{"text": "After training initial classifiers, we use our development data to choose the most informative subset of features.", "labels": [], "entities": []}, {"text": "shows the top-10 accuracy on the development data when we use individual features  to predict translations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.998794436454773}]}, {"text": "Top-10 accuracy refers to the percent of source language words for which a correct English translation appears in the top-10 ranked English candidates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9943985342979431}]}, {"text": "Each box-and-whisker plot summarizes performance over the 22 languages.", "labels": [], "entities": []}, {"text": "We don't display reciprocal rank features, as their performance is very similar to that of the corresponding raw similarity score.", "labels": [], "entities": []}, {"text": "It's easy to see that features based on the Wikipedia topic signal are the most informative.", "labels": [], "entities": [{"text": "Wikipedia topic signal", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9155708948771158}]}, {"text": "It is also clear that training a supervised model to combine all of the features (the last plot) yields performance that is dramatically higher than using any individual feature alone., from left to right, shows a greedy search for the best subset of features among those listed above.", "labels": [], "entities": []}, {"text": "Again, the Wikipedia topic score is the most informative stand-alone feature, and the Wikipedia context score is the most informative second feature.", "labels": [], "entities": [{"text": "Wikipedia topic score", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.7115415533383688}, {"text": "Wikipedia context score", "start_pos": 86, "end_pos": 109, "type": "METRIC", "confidence": 0.6672565937042236}]}, {"text": "Adding features to the model beyond the six shown in the figure does not yield additional performance gains over our set of languages.", "labels": [], "entities": []}, {"text": "shows learning curves over the number of positive training instances.", "labels": [], "entities": []}, {"text": "In all cases, the number of randomly generated negative training instances is three times the number of positive.", "labels": [], "entities": []}, {"text": "For all languages, performance is stable after about 300 correct translations are used for training.", "labels": [], "entities": []}, {"text": "This shows that our supervised method for combining signals requires only a small training dictionary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Millions of monolingual web crawl and", "labels": [], "entities": []}, {"text": " Table 2: Top-10 Accuracy on test set. Performance", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9868804216384888}]}]}