{"title": [{"text": "Supersense Tagging for Arabic: the MT-in-the-Middle Attack", "labels": [], "entities": [{"text": "Supersense Tagging", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7640731930732727}, {"text": "MT-in-the-Middle Attack", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.8640666306018829}]}], "abstractContent": [{"text": "We consider the task of tagging Arabic nouns with WordNet supersenses.", "labels": [], "entities": []}, {"text": "The first uses an expert-crafted but limited-coverage lexicon, Arabic WordNet, and heuristics.", "labels": [], "entities": []}, {"text": "The second uses un-supervised sequence modeling.", "labels": [], "entities": []}, {"text": "The third and most successful approach uses machine translation to translate the Arabic into English, which is automatically tagged with English supersenses, the results of which are then projected back into Arabic.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7177563011646271}]}, {"text": "Analysis shows gains and remaining obstacles in four Wikipedia topical domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "A taxonomic view of lexical semantics groups word senses/usages into categories of varying granularities.", "labels": [], "entities": []}, {"text": "WordNet supersense tags denote coarse semantic classes, including person and artifact (for nouns) and motion and weather (for verbs); these categories can betaken as the top level of a taxonomy.", "labels": [], "entities": []}, {"text": "Nominal supersense tagging is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories.", "labels": [], "entities": [{"text": "Nominal supersense tagging", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.834143062432607}]}, {"text": "illustrates two such labelings of an Arabic sentence.", "labels": [], "entities": []}, {"text": "Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as away of inferring representations that move toward language independence.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.6801863312721252}, {"text": "supersense tagging of text", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.8089644461870193}]}, {"text": "Here we consider the problem of nominal supersense tagging for Arabic, a language with ca.", "labels": [], "entities": [{"text": "nominal supersense tagging", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6259619692961375}]}, {"text": "300 million speakers and moderate linguistic resources, including a WordNet (), annotated datasets (), monolingual corpora, and large amounts of Arabic-English parallel data.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9484100341796875}]}, {"text": "The supervised learning approach that is used in state-of-the-art English supersense taggers (Cia- ramita and) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by , versus the 360,000 words that are annotated for English).", "labels": [], "entities": []}, {"text": "Here, we reserve that corpus for development and evaluation, not training.", "labels": [], "entities": []}, {"text": "We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments.", "labels": [], "entities": []}, {"text": "This \"MT-in-themiddle\" approach has also been successfully used for mention detection and coreference resolution.", "labels": [], "entities": [{"text": "MT-in-themiddle", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.8871995806694031}, {"text": "mention detection", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.9835283756256104}, {"text": "coreference resolution", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.973152756690979}]}, {"text": "We first discuss the task and relevant resources ( \u00a72), then the approaches we explored ( \u00a73), and finally present experimental results and analysis in \u00a74.", "labels": [], "entities": []}], "datasetContent": [{"text": "With heuristic lexicon lookup, 18% of the tokens are marked as part of a nominal supersense mention.", "labels": [], "entities": []}, {"text": "Both labeled and unlabeled mention recall with this method are below 30%; labeled precision is about 30%, and unlabeled mention precision is above 50%.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.880425214767456}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.6432982683181763}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.5796277523040771}]}, {"text": "From this we conclude that the biggest problems are (a) out-of-vocabulary items and (b) poor semantic disambiguation of in-vocabulary items.", "labels": [], "entities": []}, {"text": "The unsupervised sequence tagger does even worse on the labeled evaluation.", "labels": [], "entities": []}, {"text": "It has some success at detecting supersense mentions-unlabeled recall is substantially improved, and unlabeled precision is The unsupervised evaluation greedily maps clusters to tags, separately for each version of the test set; coverage numbers thus differ and are not shown here.", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9417411088943481}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9787378907203674}]}, {"text": "Unlabeled tagging refers to noun chunk detection only.", "labels": [], "entities": [{"text": "Unlabeled tagging", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6196238398551941}, {"text": "noun chunk detection", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8176425298055013}]}, {"text": "It was produced in part using the chunkeval.py script: see https://github.com/nschneid/pyutil slightly improved.", "labels": [], "entities": []}, {"text": "But it seems to be much worse at assigning semantic categories; the number of labeled true positive mentions is actually lower than with the lexicon-based approach.", "labels": [], "entities": [{"text": "assigning semantic categories", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.8503460884094238}]}, {"text": "MT-in-the-middle is by far the most successful single approach: both systems outperform the lexicon-only baseline by about 10 F 1 points, despite many errors in the automatic translation, English tagging, and projection, as well as underlying linguistic differences between English and Arabic.", "labels": [], "entities": [{"text": "MT-in-the-middle", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7871939539909363}, {"text": "English tagging", "start_pos": 188, "end_pos": 203, "type": "TASK", "confidence": 0.6768620610237122}]}, {"text": "The baseline's unlabeled recall is doubled, indicating substantially more nominal expressions are detected, in addition to the improved labeled scores.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9875876307487488}]}, {"text": "We further tested simple hybrids combining the lexicon-based and MT-based approaches.", "labels": [], "entities": []}, {"text": "Applying MT-in-the-middle first, then expanding token coverage with the lexicon improves recall at a small cost to precision (table 1, last row).", "labels": [], "entities": [{"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9988711476325989}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9989511966705322}]}, {"text": "Combining the techniques in the reverse order is slightly worse than MTbased projection without consulting the lexicon.", "labels": [], "entities": [{"text": "MTbased projection", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.6752513647079468}]}, {"text": "MT-in-the middle improves upon the lexicon-only baseline, yet performance is still dwarfed by the supervised English tagger (at least in the SemCor evaluation; see \u00a72), and also well below the 70% interannotator F 1 reported by . We therefore examine the weaknesses of our approach for Arabic.", "labels": [], "entities": [{"text": "interannotator F 1", "start_pos": 197, "end_pos": 215, "type": "METRIC", "confidence": 0.8121161460876465}]}], "tableCaptions": [{"text": " Table 1: Supersense tagging results on the test set: coverage measures 5 and gold-standard evaluation-exact la- beled/unlabeled 6 mention precision, recall, and F-score against each annotator. The last row is a hybrid: MT-in-the- middle followed by lexicon heuristics to improve recall. Best single-technique and best hybrid results are bolded.", "labels": [], "entities": [{"text": "Supersense tagging", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6742876172065735}, {"text": "coverage", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9960058331489563}, {"text": "gold-standard evaluation-exact la- beled/unlabeled 6 mention precision", "start_pos": 78, "end_pos": 148, "type": "METRIC", "confidence": 0.6122480988502502}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.997786283493042}, {"text": "F-score", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9991418123245239}, {"text": "recall", "start_pos": 280, "end_pos": 286, "type": "METRIC", "confidence": 0.996221661567688}]}]}