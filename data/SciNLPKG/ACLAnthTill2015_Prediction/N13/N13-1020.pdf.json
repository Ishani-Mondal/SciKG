{"title": [{"text": "Text Alignment for Real-Time Crowd Captioning", "labels": [], "entities": [{"text": "Text Alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6463905572891235}, {"text": "Real-Time Crowd Captioning", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6313465933005015}]}], "abstractContent": [{"text": "The primary way of providing real-time cap-tioning for deaf and hard of hearing people is to employ expensive professional stenogra-phers who can type as fast as natural speaking rates.", "labels": [], "entities": []}, {"text": "Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear.", "labels": [], "entities": []}, {"text": "In this paper, we describe an improved method for combining partial captions into a final output based on weighted A * search and multiple sequence alignment (MSA).", "labels": [], "entities": []}, {"text": "In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9992493987083435}, {"text": "speed", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9514747858047485}]}, {"text": "Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%).", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 55, "end_pos": 76, "type": "METRIC", "confidence": 0.7983816564083099}, {"text": "BLEU Score", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9850665032863617}, {"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9953652620315552}]}, {"text": "The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "Real-time captioning provides deaf or hard of hearing people access to speech in mainstream classrooms, at public events, and on live television.", "labels": [], "entities": [{"text": "Real-time captioning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8190539181232452}]}, {"text": "To maintain consistency between the captions being read and other visual cues, the latency between when a word was said and when it is displayed must be under five seconds.", "labels": [], "entities": [{"text": "consistency", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9714092016220093}]}, {"text": "The most common approach to real-time captioning is to recruit a trained stenographer with a special purpose phonetic keyboard, who transcribes the speech to text within approximately 5 seconds.", "labels": [], "entities": [{"text": "real-time captioning", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7054649889469147}]}, {"text": "Unfortunately, professional captionists are quite expensive ($150 per hour), must be recruited in blocks of an hour or more, and are difficult to schedule on short notice.", "labels": [], "entities": []}, {"text": "Automatic speech recognition (ASR) () attempts to solve this problem by converting speech to text completely automatically.", "labels": [], "entities": [{"text": "Automatic speech recognition (ASR)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7824805577596029}]}, {"text": "However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker's voice, in anew environment, or in the absence of a high quality microphone).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9997518658638}, {"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9617710113525391}]}, {"text": "An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type) (.", "labels": [], "entities": []}, {"text": "In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time, and their partial input is combined to produce a final transcript (see).", "labels": [], "entities": []}, {"text": "This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "ASR", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9571720361709595}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9994783997535706}, {"text": "Word Error Rate (WER)", "start_pos": 90, "end_pos": 111, "type": "METRIC", "confidence": 0.926964541276296}]}, {"text": "Furthermore, recall approached and even exceeded that of a trained expert stenographer with seven workers contributing, suggesting that the information is present to meet the performance of a stenographer.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.99955815076828}]}, {"text": "However, combining these captions involves real-time alignment of partial captions that maybe incomplete and that often have spelling errors and inconsistent timestamps.", "labels": [], "entities": []}, {"text": "In this paper, we present a more accurate combiner that leverages Multiple Sequence Alignment (MSA) and Natural Language Processing to improve performance.", "labels": [], "entities": [{"text": "Multiple Sequence Alignment (MSA)", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.7290630290905634}]}, {"text": "Gauging the quality of captions is not easy.", "labels": [], "entities": []}, {"text": "Although word error rate (WER) is commonly used in speech recognition, it considers accuracy and completeness, not readability.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 9, "end_pos": 30, "type": "METRIC", "confidence": 0.8285909593105316}, {"text": "speech recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7364528328180313}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9986679553985596}]}, {"text": "As a result, a lower WER does not always result in better understanding (.", "labels": [], "entities": [{"text": "WER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9957985281944275}]}, {"text": "We compare WER with two other commonly used metrics: BLEU () and F-measure (, and report their correlation with that of 50 human evaluators.", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9575156569480896}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9993038177490234}, {"text": "F-measure", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9955562949180603}]}, {"text": "The key contributions of this paper are as follows: \u2022 We have implemented an A * -search based Multiple Sequence Alignment algorithm) that can trade-off speed and accuracy by varying the heuristic weight and chunk-size parameters.", "labels": [], "entities": [{"text": "Multiple Sequence Alignment", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.5881456335385641}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9984299540519714}]}, {"text": "We show that it outperforms previous approaches in terms of WER, BLEU score, and F-measure.", "labels": [], "entities": [{"text": "WER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9992668032646179}, {"text": "BLEU score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9882459044456482}, {"text": "F-measure", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9957271814346313}]}, {"text": "\u2022 We propose a beam-search based technique using the timing information of the captions that helps to restrict the search space and scales effectively to align longer sequences efficiently.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (), implying it maybe a more useful metric overall when evaluating captions.", "labels": [], "entities": [{"text": "WER", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9417353272438049}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9967718720436096}, {"text": "F-measure", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9972060322761536}, {"text": "WER", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9942209720611572}, {"text": "BLEU score", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9824862480163574}]}], "datasetContent": [{"text": "Automated evaluation of speech to text captioning is known to be a challenging task (.", "labels": [], "entities": [{"text": "evaluation of speech to text captioning", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.5850863605737686}]}, {"text": "Word Error Rate (WER) is the most commonly used metric that finds the best pairwise alignment between the candidate caption and the ground truth reference sentence.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.7999922682841619}]}, {"text": "WER is estimated as S+I+D N , where S, I, and Dis the number of incorrect word substitutions, insertions, and deletions required to match the candidate sentence with reference, and N is the total number of words in the reference.", "labels": [], "entities": [{"text": "WER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7123501896858215}]}, {"text": "WER has several nice properties such as: 1) it is easy to estimate, and 2) it tries to preserve word ordering.", "labels": [], "entities": [{"text": "WER", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6513873934745789}, {"text": "word ordering", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.6920947581529617}]}, {"text": "However, WER does not account for the overall 'readability' of text and thus does not always correlate well with human evaluation ().", "labels": [], "entities": [{"text": "WER", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8789308071136475}]}, {"text": "The widely-used BLEU metric has been shown to agree well with human judgment for evaluating translation quality ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9730446934700012}]}, {"text": "However, unlike WER, BLEU imposes no explicit constraints on the word ordering.", "labels": [], "entities": [{"text": "WER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.4639931917190552}, {"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9964097142219543}]}, {"text": "BLEU has been criticized as an 'under-constrained' measure) for allowing too much variation in word ordering.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.975366473197937}, {"text": "word ordering", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.6976806670427322}]}, {"text": "Moreover, BLEU does not directly estimate recall, and instead relies on the brevity penalty.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9976746439933777}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.997757613658905}]}, {"text": "suggest that a better approach is to explicitly measure both precision and recall and combine them via F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9987989664077759}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9929550290107727}, {"text": "F-measure", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9758559465408325}]}, {"text": "Our application is similar to automatic speech recognition in that there is a single correct output, as opposed to machine translation where many outputs can be equally correct.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.6681754986445109}, {"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7239337712526321}]}, {"text": "On the other hand, unlike with ASR, out-of-order output is frequently produced by our alignment system when there is not enough overlap between the partial captions to derive the correct ordering for all words.", "labels": [], "entities": [{"text": "ASR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9503616094589233}]}, {"text": "It maybe the case that even such out-of-order output can be of value to the user, and should receive some sort of partial credit that is not possible using WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.791266143321991}]}, {"text": "For this reason, we wished to systematically compare BLEU, F-measure, and WER as metrics for our task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9993143081665039}, {"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9915168285369873}, {"text": "WER", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9974556565284729}]}, {"text": "We performed a study to evaluate the agreement of the three metrics with human judgment.", "labels": [], "entities": []}, {"text": "We ran- domly extracted one-minute long audio clips from four MIT OpenCourseWare lectures.", "labels": [], "entities": []}, {"text": "Each clip was transcribed by 7 human workers, and then aligned and combined using four different systems: the graph-based system, and three different versions of our weighted A * algorithm with different values of tuning parameters.", "labels": [], "entities": []}, {"text": "Fifty people participated in the study and were split in two equal sized groups.", "labels": [], "entities": []}, {"text": "Each group was assigned two of the four audio clips, and each person evaluated all four captions for both clips.", "labels": [], "entities": []}, {"text": "Each participant assigned a score between 1 to 10 to these captions, based on two criteria: 1) the overall estimated agreement of the captions with the ground truth text, and 2) the readability and understandability of the captions.", "labels": [], "entities": []}, {"text": "Finally, we estimated the correlation coefficients (both Spearman and Pearson) for the three metrics discussed above with respect to the average score assigned by the human participants.", "labels": [], "entities": [{"text": "correlation", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9708138108253479}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "Among the three metrics, WER had the highest agreement with the human participants.", "labels": [], "entities": [{"text": "WER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.8176201581954956}]}, {"text": "This indicates that reconstructing the correct word order is in fact important to the users, and that, in this aspect, our task has more of the flavor of speech recognition than of machine translation.", "labels": [], "entities": [{"text": "reconstructing the correct word order", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.6699082791805268}, {"text": "speech recognition", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7366300672292709}, {"text": "machine translation", "start_pos": 181, "end_pos": 200, "type": "TASK", "confidence": 0.7541707456111908}]}, {"text": "We experiment with the MSA-A * algorithm for captioning different audio clips, and compare the results with two existing techniques.", "labels": [], "entities": []}, {"text": "Our experimental setup is similar to the experiments by . Our dataset consists of four 5-minute long audio clips extracted from lectures available on MIT OpenCourseWare.", "labels": [], "entities": []}, {"text": "The audio clips contain speech from electrical engineering and chemistry lectures.", "labels": [], "entities": []}, {"text": "Each audio clip is transcribed by ten non-expert human workers in real-time.", "labels": [], "entities": []}, {"text": "We then combine these inputs using our MSA-A * algorithm, and also compare with the existing graph-based system and mul- tiple sequence alignment using MUSCLE.", "labels": [], "entities": [{"text": "mul- tiple sequence alignment", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.47596354484558107}, {"text": "MUSCLE", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.8228306770324707}]}, {"text": "As explained earlier, we vary the four key parameters of the algorithm: the chunk size (c), the heuristic weight (w), the voting threshold (t v ), and the beam size (b).", "labels": [], "entities": [{"text": "heuristic weight (w)", "start_pos": 96, "end_pos": 116, "type": "METRIC", "confidence": 0.8519538283348084}]}, {"text": "The heuristic weight and chunk size parameters help us to trade-off between speed versus accuracy; the voting threshold t v helps improve precision by pruning words having less than t v votes, and beam size reduces the search space by restricting states to be inside a time window/beam.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9982163310050964}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9987402558326721}]}, {"text": "We use affine gap penalty) with different gap opening and gap extension penalty.", "labels": [], "entities": []}, {"text": "We set gap opening penalty to 0.125 and gap extension penalty to 0.05.", "labels": [], "entities": [{"text": "gap opening", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.7019882351160049}, {"text": "gap extension penalty", "start_pos": 40, "end_pos": 61, "type": "METRIC", "confidence": 0.7306390603383383}]}, {"text": "We evaluate the performance using the three standard metrics: Word Error Rate (WER), BLEU, and F-measure.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 62, "end_pos": 83, "type": "METRIC", "confidence": 0.9098857740561167}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9970070719718933}, {"text": "F-measure", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9947060942649841}]}, {"text": "The performance in terms of these metrics using different systems is presented in.", "labels": [], "entities": []}, {"text": "Out of the five systems in, the first three are different versions of our A * search based MSA algorithm with different parameter settings: 1) A * -10-t system (c = 10 seconds, t v = 2), 2) A * -15-t (c = 15 seconds, t v = 2), and 3) A * -15 (c = 15 seconds, t v = 1 i.e. no pruning while voting).", "labels": [], "entities": [{"text": "A * -15", "start_pos": 234, "end_pos": 241, "type": "METRIC", "confidence": 0.8893181532621384}]}, {"text": "For all three systems, the heuristic weight parameter w is set to 2.5 and beam size b = 20 seconds.", "labels": [], "entities": [{"text": "heuristic weight parameter w", "start_pos": 27, "end_pos": 55, "type": "METRIC", "confidence": 0.915435254573822}, {"text": "beam size b", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.7675756017367045}]}, {"text": "The other two systems are the existing graph-based system and multiple sequence alignment using MUSCLE.", "labels": [], "entities": [{"text": "multiple sequence alignment", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.6432449916998545}, {"text": "MUSCLE", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.8288252949714661}]}, {"text": "Among the three A * based algorithms, both A * -15-t and A * -10-t produce better quality transcripts and outperform the existing algorithms.", "labels": [], "entities": [{"text": "A", "start_pos": 43, "end_pos": 44, "type": "METRIC", "confidence": 0.9595963358879089}, {"text": "A * -10-t", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.8446447402238846}]}, {"text": "Both systems apply the voting threshold that improves precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9991036057472229}]}, {"text": "The system A * -15 applies no threshold and ends up producing many spurious words having poor agreement among the workers, and hence it scores worse in all the three metrics.", "labels": [], "entities": [{"text": "A * -15", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9238189607858658}]}, {"text": "The A * -15-t achieves 57.4% average accuracy in terms of (1-WER), providing 29.6% improvement with respect to the graph-based system (average accuracy 42.6%), and 35.4% improvement with respect to the MUSCLE-based MSA system (average accuracy 41.9%).", "labels": [], "entities": [{"text": "A", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.8761731386184692}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9692057967185974}, {"text": "MUSCLE-based MSA system", "start_pos": 202, "end_pos": 225, "type": "DATASET", "confidence": 0.7426662047704061}, {"text": "accuracy", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.6593411564826965}]}, {"text": "On the same set of audio clips,  reported 36.6% accuracy using ASR (Dragon Naturally Speaking, version 11.5 for Windows), which is worse than all the crowd-based based systems used in this experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993770718574524}, {"text": "ASR", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.7877410054206848}]}, {"text": "To measure the statistical significance of this improvement, we performed a t-test at both the dataset level (n = 4 clips) and the word level (n = 2862 words).", "labels": [], "entities": []}, {"text": "The improvement over the graph-based model was statistically significant with dataset level p-value 0.001 and word level p-value smaller than 0.0001.", "labels": [], "entities": []}, {"text": "The average time to align each 15 second chunk with 10 input captions is \u223c400 milliseconds.", "labels": [], "entities": []}, {"text": "We have also experimented with a trigram language model, trained on the British National Corpus having \u223c122 million words.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.9452588359514872}]}, {"text": "The language-model-integrated A * search provided a negligible 0.21% improvement in WER over the A * -15-t system on average.", "labels": [], "entities": [{"text": "WER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9781026244163513}]}, {"text": "The task of combining captions does not require recognizing words; it only requires aligning them in the correct order.", "labels": [], "entities": [{"text": "combining captions", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8540791869163513}]}, {"text": "This could explain why language model did not improve accuracy, as it does for speech recognition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9991669654846191}, {"text": "speech recognition", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7610810697078705}]}, {"text": "Since the standard MSA-A * algorithm (without language model) produced comparable accuracy and faster running time, we used that version in the rest of the experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.999168872833252}]}, {"text": "Next, we look at the critical speed versus accuracy trade-off for different values of the heuristic weight (w) and the chunk size (c) parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9860942363739014}, {"text": "heuristic weight (w)", "start_pos": 90, "end_pos": 110, "type": "METRIC", "confidence": 0.8204725623130799}]}, {"text": "Since WER has been shown to correlate most with human judgment, we show the next results only with respect to WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.7144485116004944}, {"text": "WER", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.5684599876403809}]}, {"text": "First, we fix the chunk size at different values, and then vary the heuristic weight parameter: w = 1.8, 2, 2.5, 3, 4, 6, and 8.", "labels": [], "entities": []}, {"text": "The results are shown in, where each curve represents how time and accuracy changed over the range of values of wand a fixed value of c.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9991840720176697}]}, {"text": "We observe that for smaller values of w, the algorithm is more accurate, but comparatively slower.", "labels": [], "entities": []}, {"text": "As w increases, the search reaches the goal faster, but the quality of the solution degrades as well.", "labels": [], "entities": []}, {"text": "Next, we fix wand vary chunk size c = 5, 10, 15, 20, 40, 60 second.", "labels": [], "entities": []}, {"text": "We repeat this experiment fora range of values of wand the results are shown in(b).", "labels": [], "entities": []}, {"text": "We can see that the accuracy improves steeply up to c = 20 seconds, and does not improve much beyond c = 40 seconds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9997745156288147}]}, {"text": "For all these benchmarks, we set the beam size (b) to 20 seconds and voting threshold (t v ) to 2.", "labels": [], "entities": [{"text": "beam size (b)", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.8584361433982849}, {"text": "voting threshold (t v )", "start_pos": 69, "end_pos": 92, "type": "METRIC", "confidence": 0.9043532411257426}]}, {"text": "In our tests, the beam size parameter (b) did not play a significant role in performance, and setting it to any reasonably large value (usually \u2265 15 seconds) resulted in similar accuracy and running time.", "labels": [], "entities": [{"text": "beam size parameter (b)", "start_pos": 18, "end_pos": 41, "type": "METRIC", "confidence": 0.789370079835256}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9993430972099304}]}, {"text": "This is because the A * search with h pair heuristic already reduces the the search space significantly, and usually reaches the goal in a number of steps smaller than the state space size after the beam restriction.", "labels": [], "entities": []}, {"text": "Finally, we investigate how the accuracy of our algorithm varies with the number of inputs/workers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9994662404060364}]}, {"text": "We start with a pool of 10 input captions for one of the audio clips.", "labels": [], "entities": []}, {"text": "We vary the number of input captions (K) to the MSA-A * algorithm from 2 up to 10.", "labels": [], "entities": []}, {"text": "The quality of input captions differs greatly among the workers.", "labels": [], "entities": []}, {"text": "Therefore, for each value of K, we repeat the experiment min 20, 10 K times; each time we randomly select K input captions out of the total pool of 10.", "labels": [], "entities": []}, {"text": "shows that accuracy steeply increases as the number of inputs increases to 7, and after that adding more workers does not provide much improvement inaccuracy, but increases running time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.999488115310669}]}], "tableCaptions": [{"text": " Table 1: The correlation of average human judgment with  three automated metrics: 1-WER, BLEU, and F-measure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9990822076797485}, {"text": "F-measure", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9963487386703491}]}]}