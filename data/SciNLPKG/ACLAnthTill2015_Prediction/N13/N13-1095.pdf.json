{"title": [{"text": "Distant Supervision for Relation Extraction with an Incomplete Knowledge Base", "labels": [], "entities": [{"text": "Distant Supervision", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9553645849227905}, {"text": "Relation Extraction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.9887655079364777}]}], "abstractContent": [{"text": "Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation ex-tractors.", "labels": [], "entities": []}, {"text": "In this paper, we show that a significant number of \"negative\" examples generated by the labeling process are false negatives because the knowledge base is incomplete.", "labels": [], "entities": []}, {"text": "Therefore the heuristic for generating negative examples has a serious flaw.", "labels": [], "entities": []}, {"text": "Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate its advantage over existing algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation Extraction is a well-studied problem (. Recently, Distant Supervision (DS)) has emerged to be a popular choice for training relation extractors without using manually labeled data.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9330337941646576}, {"text": "Distant Supervision (DS))", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6611022472381591}, {"text": "relation extractors", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.6670210957527161}]}, {"text": "It automatically generates training examples by labeling relation mentions 1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB).", "labels": [], "entities": []}, {"text": "This method significantly reduces human efforts for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.9370368421077728}]}, {"text": "The labeling heuristic has a serious flaw.", "labels": [], "entities": [{"text": "labeling heuristic", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8893641531467438}]}, {"text": "Knowledge bases are usually highly incomplete.", "labels": [], "entities": []}, {"text": "For exam- An occurrence of a pair of entities with the source sentence.", "labels": [], "entities": []}, {"text": "ple, 93.8% of persons from Freebase 2 have no place of birth, and 78.5% have no nationality (section 3).", "labels": [], "entities": [{"text": "Freebase 2", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.957792192697525}]}, {"text": "Previous work typically assumes that if the argument entity pair is not listed in the KB as having a relation, all the corresponding relation mentions are considered negative examples.", "labels": [], "entities": []}, {"text": "This crude assumption labeled many entity pairs as negative when in fact some of their mentions express a relation.", "labels": [], "entities": []}, {"text": "The number of such false negative matches even exceeds the number of positive pairs, by 3 to 10 times, leading to a significant problem for training.", "labels": [], "entities": []}, {"text": "Previous approaches ( bypassed this problem by heavily under-sampling the \"negative\" class.", "labels": [], "entities": []}, {"text": "We instead deal with a learning scenario where we only have entity-pair level labels that are either positive or unlabeled.", "labels": [], "entities": []}, {"text": "We proposed an extension to that can train on this dataset.", "labels": [], "entities": []}, {"text": "Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9477059841156006}, {"text": "false negative match rate", "start_pos": 85, "end_pos": 110, "type": "METRIC", "confidence": 0.7613027095794678}]}, {"text": "Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over existing solutions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data set: We use the KBP (Ji et al., 2011) dataset 9 prepared and publicly released by for our experiment since it is 1) large and realistic, 2) publicly available, 3) most importantly, it is the only dataset that has associated human-labeled ground truth.", "labels": [], "entities": [{"text": "KBP (Ji et al., 2011) dataset", "start_pos": 21, "end_pos": 50, "type": "DATASET", "confidence": 0.8341846532291837}]}, {"text": "Any KB held-out evaluation without manual assessment will be significantly affected by KB incompleteness.", "labels": [], "entities": [{"text": "KB incompleteness", "start_pos": 87, "end_pos": 104, "type": "METRIC", "confidence": 0.6893461346626282}]}, {"text": "In KBP dataset, the training bags are generated by mapping Wikipedia (http://en.wikipedia.org) infoboxes (after merging similar types following the KBP 2011 task definition) into a large unlabeled corpus (consisting of 1.5M documents from the KBP source corpus and a complete snapshot of Wikipedia).", "labels": [], "entities": [{"text": "KBP dataset", "start_pos": 3, "end_pos": 14, "type": "DATASET", "confidence": 0.9388157725334167}, {"text": "KBP 2011 task definition", "start_pos": 148, "end_pos": 172, "type": "DATASET", "confidence": 0.8147618621587753}, {"text": "KBP source corpus", "start_pos": 243, "end_pos": 260, "type": "DATASET", "confidence": 0.9388446807861328}]}, {"text": "The KBP shared task provided 200 query named entities with their associated slot values (in total several thousand pairs).", "labels": [], "entities": []}, {"text": "We use 40 queries as development dataset (dev), and the rest (160 queries) as evaluation dataset.", "labels": [], "entities": []}, {"text": "We set \u03b8 = 0.25 by tuning on the dev set and use it in the experiments.", "labels": [], "entities": []}, {"text": "For a fair comparison, we follow Surdeanu et al.", "labels": [], "entities": []}, {"text": "(2012) and begin by downsampling the \"negative\" class to 5%.", "labels": [], "entities": []}, {"text": "We also set T=8 and use the following noisy-or (for ith bag) of mention-level probability to rank predicted types (r) of pairs and plot the precision-recall curves for all experiments.", "labels": [], "entities": [{"text": "T", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9936341047286987}, {"text": "precision-recall", "start_pos": 140, "end_pos": 156, "type": "METRIC", "confidence": 0.9875308871269226}]}, {"text": "Evaluation: We compare our algorithm (MIMLsemi) to three algorithms: 1) MIML (Surdeanu et al., 2012), the Multiple-Instance Multiple Label algorithm which labels the bags directly with the KB (y = \u2113).", "labels": [], "entities": []}, {"text": "2) MultiR (denoted as Hoffmann)), a Multiple-Instance algorithm that supports overlapping relations.", "labels": [], "entities": []}, {"text": "It also imposes y = \u2113.", "labels": [], "entities": []}, {"text": "3) Mintz++ (), a variant of the single-instance learning algorithm (section 3).", "labels": [], "entities": []}, {"text": "The first two are stat-of-the-art Multi-Instance Multi-Label algorithms.", "labels": [], "entities": []}, {"text": "Mintz++ is a strong baseline () and an improved version of. shows that our algorithm consistently outperforms all three algorithms at almost all recall levels (with the exception of a very small region in the PR-curve).", "labels": [], "entities": [{"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9962440729141235}]}, {"text": "This demonstrates that by treating unla-beled data set differently and leveraging the missing positive bags, MIML-semi is able to learn a more accurate model for extraction.", "labels": [], "entities": []}, {"text": "Although the proposed solution is a specific algorithm, we believe the idea of treating unlabeled data differently can be incorporated into any of these algorithms that only use unlabeled data as negative examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The incompleteness of Freebase (* are must- have attributes for a person).", "labels": [], "entities": [{"text": "Freebase", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9378535747528076}]}, {"text": " Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All  numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and  5% in Riedel and KBP dataset, respectively.", "labels": [], "entities": [{"text": "KBP dataset", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.9709728062152863}, {"text": "BD", "start_pos": 161, "end_pos": 163, "type": "METRIC", "confidence": 0.9966423511505127}, {"text": "KBP dataset", "start_pos": 247, "end_pos": 258, "type": "DATASET", "confidence": 0.9659760296344757}]}]}