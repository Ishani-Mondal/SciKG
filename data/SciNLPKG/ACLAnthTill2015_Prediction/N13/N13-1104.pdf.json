{"title": [], "abstractContent": [{"text": "In natural-language discourse, related events tend to appear near each other to describe a larger scenario.", "labels": [], "entities": []}, {"text": "Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions.", "labels": [], "entities": []}, {"text": "Identifying frames is a prerequisite for information extraction and natural language generation , and is usually done manually.", "labels": [], "entities": [{"text": "Identifying frames", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8947312235832214}, {"text": "information extraction", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.8211541175842285}, {"text": "natural language generation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6950803399085999}]}, {"text": "Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend.", "labels": [], "entities": []}, {"text": "In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, and participants as latent topics and learns those frame and event transitions that best explain the text.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7712706327438354}]}, {"text": "The number of frame components is inferred by a novel application of a split-merge method from syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7010622322559357}]}, {"text": "In end-to-end evaluations from text to induced frames and extracted facts, our method produces state-of-the-art results while substantially reducing engineering effort.", "labels": [], "entities": []}], "introductionContent": [{"text": "Events with causal or temporal relations tend to occur near each other in text.", "labels": [], "entities": []}, {"text": "For example, a BOMB-ING scenario in an article on terrorism might begin with a DETONATION event, in which terrorists set off a bomb.", "labels": [], "entities": [{"text": "BOMB-ING", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8069843649864197}, {"text": "DETONATION", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.8508865237236023}]}, {"text": "Then, a DAMAGE event might ensue to describe the resulting destruction and any casualties, followed by an INVESTIGATION event * This research was undertaken during the author's internship at Microsoft Research.", "labels": [], "entities": [{"text": "DAMAGE", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.84977787733078}, {"text": "INVESTIGATION", "start_pos": 106, "end_pos": 119, "type": "METRIC", "confidence": 0.997028648853302}]}, {"text": "Afterwards, the BOMBING scenario may transition into a CRIMINAL-PROCESSING scenario, which begins with police catching the terrorists, and proceeds to atrial, sentencing, etc.", "labels": [], "entities": [{"text": "BOMBING", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.8913947939872742}]}, {"text": "A common set of participants serves as the event arguments; e.g., the agent (or subject) of DETONATION is often the same as the theme (or object) of INVESTIGATION and corresponds to a PERPETRATOR.", "labels": [], "entities": [{"text": "PERPETRATOR", "start_pos": 184, "end_pos": 195, "type": "METRIC", "confidence": 0.8789853453636169}]}, {"text": "Such structures can be formally captured by the notion of a frame (a.k.a. template, scenario), which consists of a set of events with prototypical transitions, as well as a set of slots representing the common participants.", "labels": [], "entities": []}, {"text": "Identifying frames is an explicit or implicit prerequisite for many NLP tasks.", "labels": [], "entities": []}, {"text": "Information extraction, for example, stipulates the types of events and slots that are extracted fora frame or template.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.800684928894043}]}, {"text": "Online applications such as dialogue systems and personal-assistant applications also model users' goals and subgoals using frame-like representations.", "labels": [], "entities": []}, {"text": "In natural-language generation, frames are often used to represent contents to be expressed as well as to support surface realization.", "labels": [], "entities": [{"text": "natural-language generation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7607690989971161}]}, {"text": "Until recently, frames and related representations have been manually constructed, which has limited their applicability to a relatively small number of domains and a few slots within a domain.", "labels": [], "entities": []}, {"text": "Furthermore, additional manual effort is needed after the frames are defined in order to extract frame components from text (e.g., in annotating examples and designing features to train a supervised learning model).", "labels": [], "entities": []}, {"text": "This paradigm makes generalizing across tasks difficult, and might suffer from annotator bias.", "labels": [], "entities": [{"text": "generalizing", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.9727546572685242}]}, {"text": "Recently, there has been increasing interest in au-tomatically inducing frames from text.", "labels": [], "entities": []}, {"text": "A notable example is Chambers and Jurafsky (2011), which first clusters related verbs to form frames, and then clusters the verbs' syntactic arguments to identify slots.", "labels": [], "entities": []}, {"text": "While Chambers and Jurafsky (2011) represents a major step forward in frame induction, it is also limited in several aspects.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.8958130776882172}]}, {"text": "The clustering used ad hoc steps and customized similarity metrics, as well as an additional retrieval step from a large external text corpus for slot generation.", "labels": [], "entities": [{"text": "slot generation", "start_pos": 146, "end_pos": 161, "type": "TASK", "confidence": 0.8478776216506958}]}, {"text": "This makes it hard to replicate their approach or adapt it to new domains.", "labels": [], "entities": []}, {"text": "Lacking a coherent model, it is also difficult to incorporate additional linguistic insights and prior knowledge.", "labels": [], "entities": []}, {"text": "In this paper, we present PROFINDER (PRObabilistic Frame INDucER), the first probabilistic approach to frame induction.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.8036006391048431}]}, {"text": "PROFINDER defines a joint distribution over the words in a document and their frame assignments by modeling frame and event transitions, correlations among events and slots, and their surface realizations.", "labels": [], "entities": []}, {"text": "Given a set of documents, PROFINDER outputs a set of induced frames with learned parameters, as well as the most probable frame assignments that can be used for event and entity extraction.", "labels": [], "entities": [{"text": "event and entity extraction", "start_pos": 161, "end_pos": 188, "type": "TASK", "confidence": 0.6110808253288269}]}, {"text": "The numbers of events and slots are dynamically determined by a novel application of the split-merge approach from syntactic parsing ().", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.7305286526679993}]}, {"text": "In end-to-end evaluations from text to entity extraction using standard MUC and TAC datasets, PROFINDER achieved state-of-the-art results while significantly reducing engineering effort and requiring no external data.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7179314196109772}, {"text": "MUC and TAC datasets", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.695784404873848}]}], "datasetContent": [{"text": "We first evaluate our model on a standard entity extraction task, using the evaluation settings from Chambers and Jurafsky (2011) (henceforth, C&J) to enable a head-to-head comparison.", "labels": [], "entities": [{"text": "entity extraction task", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7987597187360128}]}, {"text": "Specifically, we use the MUC-4 data set (1992) , which contains 1300 training and development documents on terrorism in South America, with 200 additional documents for testing.", "labels": [], "entities": [{"text": "MUC-4 data set (1992)", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.9729733765125275}]}, {"text": "MUC-4 contains four templates: ATTACK, KIDNAPPING, BOMBING, and ARSON.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9257346987724304}, {"text": "ATTACK", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9680363535881042}, {"text": "KIDNAPPING", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9841310381889343}, {"text": "BOMBING", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9942061901092529}, {"text": "ARSON", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9432328939437866}]}, {"text": "3 All templates share the same set of predefined slots, with the evaluation focusing on the following four: PERPETRATOR, PHYSICAL TARGET, HUMAN TAR-GET, and INSTRUMENT.", "labels": [], "entities": [{"text": "PERPETRATOR", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9906858801841736}, {"text": "PHYSICAL", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8550431728363037}, {"text": "TARGET", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.5204789042472839}, {"text": "HUMAN TAR-GET", "start_pos": 138, "end_pos": 151, "type": "METRIC", "confidence": 0.5259244740009308}, {"text": "INSTRUMENT", "start_pos": 157, "end_pos": 167, "type": "METRIC", "confidence": 0.6863951683044434}]}, {"text": "For each slot in a MUC template, the system first identifies an induced slot that best maps to it by F 1 on the development set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9558187425136566}]}, {"text": "As in C&J, tem-plate is ignored in final evaluation, so all the clusters that belong to the same slot are then merged across the templates; e.g., the PERPETRATOR clusters for KIDNAPPING and BOMBING are merged.", "labels": [], "entities": [{"text": "BOMBING", "start_pos": 190, "end_pos": 197, "type": "METRIC", "confidence": 0.8004589676856995}]}, {"text": "The final precision, recall, and F 1 are computed based on these merged clusters.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9986265897750854}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.999531626701355}, {"text": "F 1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9907855689525604}]}, {"text": "Correctness is determined by matching head words, and slots marked as optional in MUC are ignored when computing recall.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9577202796936035}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.942044734954834}]}, {"text": "All hyperparameters are tuned on the development set (see Appendix A for their values).", "labels": [], "entities": []}, {"text": "Named entity type Named entity type is a useful feature to filter out entities for particular slots; e.g. a location cannot bean INSTRUMENT.", "labels": [], "entities": []}, {"text": "We thus divide each induced cluster into four clusters by named entity type before performing the mapping, following C&J's heuristic and using a named entity recognizer and word lists derived from WordNet: PER-SON/ORGANIZATION, PHYSICAL OBJECT, LOCA-TION, and OTHER.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 197, "end_pos": 204, "type": "DATASET", "confidence": 0.9481513500213623}, {"text": "OTHER", "start_pos": 260, "end_pos": 265, "type": "DATASET", "confidence": 0.6104980707168579}]}], "tableCaptions": [{"text": " Table 2: Results on TAC 2010 entity extraction with N - best mapping for N = 1 and N = 5. Intermediate values  of N produce intermediate results, and are not shown for  brevity.", "labels": [], "entities": [{"text": "TAC 2010 entity extraction", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.7336213290691376}]}]}