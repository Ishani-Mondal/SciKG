{"title": [{"text": "Adaptation of Reordering Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.8622235854466757}]}], "abstractContent": [{"text": "Previous research on domain adaptation (DA) for statistical machine translation (SMT) has mainly focused on the translation model (TM) and the language model (LM).", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8648130297660828}, {"text": "statistical machine translation (SMT)", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.7969365765651067}]}, {"text": "To the best of our knowledge, there is no previous work on reordering model (RM) adaptation for phrase-based SMT.", "labels": [], "entities": [{"text": "reordering model (RM) adaptation", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.7044167071580887}, {"text": "phrase-based SMT", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.6079045832157135}]}, {"text": "In this paper, we demonstrate that mixture model adaptation of a lexical-ized RM can significantly improve SMT performance , even when the system already contains a domain-adapted TM and LM.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9962337613105774}]}, {"text": "We find that, surprisingly, different training corpora can vary widely in their reordering characteristics for particular phrase pairs.", "labels": [], "entities": []}, {"text": "Furthermore, particular training corpora maybe highly suitable for training the TM or the LM, but unsuitable for training the RM, or vice versa, so mixture weights for these models should be estimated separately.", "labels": [], "entities": []}, {"text": "An additional contribution of the paper is to propose two improvements to mixture model adaptation: smoothing the in-domain sample, and weighting instances by document frequency.", "labels": [], "entities": [{"text": "mixture model adaptation", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.7011879086494446}]}, {"text": "Applied to mixture RMs in our experiments, these techniques (es-pecially smoothing) yield significant performance improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "A phrase-based statistical machine translation (SMT) system typically has three main components: a translation model (TM) that contains information about how to translate word sequences (phrases) from the source language to the target language, a language model (LM) that contains information about probable word sequences in the target language, and a reordering model (RM) that indicates how the order of words in the source sentence is likely to influence the order of words in the target sentence.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 2, "end_pos": 52, "type": "TASK", "confidence": 0.7489336090428489}]}, {"text": "The TM and the RM are trained on parallel data, and the LM is trained on target-language data.", "labels": [], "entities": []}, {"text": "Usage of language and therefore the best translation practice differs widely across genres, topics, and dialects, and even depends on a particular author's or publication's style; the word \"domain\" is often used to indicate a particular combination of all these factors.", "labels": [], "entities": []}, {"text": "Unless there is a perfect match between the training data domain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9827541708946228}]}, {"text": "In offline domain adaptation, the system is provided with a sample of translated sentences from the test domain prior to deployment.", "labels": [], "entities": [{"text": "offline domain adaptation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.6529596745967865}]}, {"text": "Ina popular variant of offline adaptation, linear mixture model adaptation, each training corpus is used to generate a separate model component that forms part of a linear combination, and the sample is used to assign a weight to each component.", "labels": [], "entities": [{"text": "offline adaptation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7227111905813217}, {"text": "linear mixture model adaptation", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.660212904214859}]}, {"text": "If the sample resembles some of the corpora more than others, those corpora will receive higher weights in the combination.", "labels": [], "entities": []}, {"text": "Previous research on domain adaptation for SMT has focused on the TM and the LM.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7350747138261795}, {"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9814697504043579}]}, {"text": "Such research is easily motivated: translations across domains are unreliable.", "labels": [], "entities": []}, {"text": "For example, the Chinese translation of the English word \"mouse\" would most likely be \"laoshu \u8001\u9f20\" if the topic is the animal; if the topic is computer hardware, its translation would most likely be \"shubiao \u9f20\u6807\".", "labels": [], "entities": []}, {"text": "However, when the translation is for people in Taiwan, even when the topic is computer hardware, its translation would more likely be \"huashu \u6ed1\u9f20\".", "labels": [], "entities": []}, {"text": "It is intuitively obvious why TM and LM adaptation would be helpful here.", "labels": [], "entities": []}, {"text": "By contrast, it is not at all obvious that RM model adaptation will improve SMT performace.", "labels": [], "entities": [{"text": "RM model adaptation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.9070464173952738}, {"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9955050349235535}]}, {"text": "One would expect reordering behaviour to be characteristic of a particular language pair, but not of particular domains.", "labels": [], "entities": []}, {"text": "At most, one might think that reordering is lexicalized-perhaps, (for instance) in translating from Chinese to English, or from Arabic to English, there are certain words whose English translations tend to undergo long-distance movement from their original positions, while others stay close to their original positions.", "labels": [], "entities": []}, {"text": "However, one would not expect a particular Chinese adverb or a particular Arabic noun to undergo long-distance movement when being translated into English in one domain, but not in others.", "labels": [], "entities": []}, {"text": "Nevertheless, that is what we observe: see section 5 below.", "labels": [], "entities": []}, {"text": "This paper shows that RM adaptation improves the performance of our phrase-based SMT system.", "labels": [], "entities": [{"text": "RM adaptation", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9880892932415009}, {"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.7767028212547302}]}, {"text": "In our implementation, the RM is adapted by means of a linear mixture model, but it is likely that other forms of RM adaptation would also work.", "labels": [], "entities": [{"text": "RM adaptation", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9578410387039185}]}, {"text": "We obtain even more effective RM adaptation by smoothing the in-domain sample and by weighting orientation counts by the document frequency of the phrase pair.", "labels": [], "entities": [{"text": "RM adaptation", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.9940416812896729}]}, {"text": "Both improvements could be applied to the TM or the LM as well, though we have not done so.", "labels": [], "entities": []}, {"text": "Finally, the paper analyzes reordering to see why RM adaptation works.", "labels": [], "entities": [{"text": "RM adaptation", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9897778928279877}]}, {"text": "There seem to be two factors at work.", "labels": [], "entities": []}, {"text": "First, the reordering behaviour of words and phrases often differs dramatically from one bilingual corpus to another.", "labels": [], "entities": []}, {"text": "Second, there are corpora (for instance, comparable corpora and bilingual lexicons) which may contain very valuable information for the TM, but which are poor sources of RM information; RM adaptation downweights information from these corpora significantly, and thus improves the overall quality of the RM.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: NIST Chinese-English data. In the gen- res column: nw=newswire, bc=broadcast conversa- tion, bn=broadcast news, wl=weblog, ng=newsgroup,  un=United Nations proceedings.", "labels": [], "entities": [{"text": "NIST Chinese-English data", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9430208206176758}]}, {"text": " Table 2: NIST Arabic-English data. In the gen- res column: nw=newswire, bc=broadcast conversation,  bn=broadcase news, ng=newsgroup, wl=weblog.", "labels": [], "entities": [{"text": "NIST Arabic-English data", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.8700442711512247}]}, {"text": " Table 3: Results for variants of RM adaptation.", "labels": [], "entities": [{"text": "RM adaptation", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9881593585014343}]}, {"text": " Table 4: RM adaptation improves over a baseline con- taining adapted LMs and TMs.", "labels": [], "entities": [{"text": "RM adaptation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9814658164978027}]}, {"text": " Table 8: Orientation frequencies for the phrase pair \"\u7acb  \u5373 immediately\", with respect to the previous phrase.", "labels": [], "entities": []}, {"text": " Table 9: Orientation frequencies for the phrase pair  \"work AlEml\" with respect to the previous phrase.", "labels": [], "entities": []}]}