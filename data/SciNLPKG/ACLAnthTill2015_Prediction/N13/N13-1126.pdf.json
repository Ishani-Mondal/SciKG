{"title": [{"text": "Target Language Adaptation of Discriminative Transfer Parsers", "labels": [], "entities": [{"text": "Target Language Adaptation of Discriminative Transfer Parsers", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.7067493370601109}]}], "abstractContent": [{"text": "We study multi-source transfer parsing for resource-poor target languages; specifically methods for target language adaptation of delexicalized discriminative graph-based dependency parsers.", "labels": [], "entities": [{"text": "multi-source transfer parsing", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.7258219122886658}, {"text": "target language adaptation of delexicalized discriminative graph-based dependency parsers", "start_pos": 100, "end_pos": 189, "type": "TASK", "confidence": 0.6413019895553589}]}, {"text": "We first show how recent insights on selective parameter sharing, based on typological and language-family features, can be applied to a discriminative parser by carefully decomposing its model features.", "labels": [], "entities": [{"text": "selective parameter sharing", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6597865025202433}]}, {"text": "We then show how the parser can be relexicalized and adapted using unlabeled target language data and a learning method that can incorporate diverse knowledge sources through ambiguous labelings.", "labels": [], "entities": []}, {"text": "In the latter scenario, we exploit two sources of knowledge: arc marginals derived from the base parser in a self-training algorithm, and arc predictions from multiple transfer parsers in an ensemble-training algorithm.", "labels": [], "entities": []}, {"text": "Our final model outperforms the state of the art in multi-source transfer parsing on 15 out of 16 evaluated languages.", "labels": [], "entities": [{"text": "multi-source transfer parsing", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.7498701612154642}]}], "introductionContent": [{"text": "Many languages still lack access to core NLP tools, such as part-of-speech taggers and syntactic parsers.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7281074821949005}]}, {"text": "This is largely due to the reliance on fully supervised learning methods, which require large quantities of manually annotated training data.", "labels": [], "entities": []}, {"text": "Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging () and syntactic dependency parsing (, * Work primarily carried out while at Google, NY. 2012).", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7820066511631012}, {"text": "part-of-speech tagging", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.734308660030365}, {"text": "syntactic dependency parsing", "start_pos": 147, "end_pos": 175, "type": "TASK", "confidence": 0.742130974928538}]}, {"text": "While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods () and weakly supervised methods (.", "labels": [], "entities": []}, {"text": "A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages.", "labels": [], "entities": [{"text": "cross-lingual transfer of syntactic dependency parsers", "start_pos": 24, "end_pos": 78, "type": "TASK", "confidence": 0.8246538937091827}]}, {"text": "This idea was first explored for weakly supervised learning and recently by for multisource cross-lingual transfer.", "labels": [], "entities": [{"text": "multisource cross-lingual transfer", "start_pos": 80, "end_pos": 114, "type": "TASK", "confidence": 0.6618901391824087}]}, {"text": "In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages.", "labels": [], "entities": []}, {"text": "However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9982561469078064}]}, {"text": "In this paper, we improve upon the state of the art in cross-lingual transfer of dependency parsers from multiple source languages by adapting feature-rich discriminatively trained parsers to a specific target language.", "labels": [], "entities": [{"text": "cross-lingual transfer of dependency parsers", "start_pos": 55, "end_pos": 99, "type": "TASK", "confidence": 0.8176329255104064}]}, {"text": "First, in \u00a74 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph-based parsing model.", "labels": [], "entities": []}, {"text": "This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts.", "labels": [], "entities": []}, {"text": "The resulting parser outperforms the method of on 12 out of 16 evaluated languages.", "labels": [], "entities": []}, {"text": "Second, in \u00a75 we introduce a train-ing method that can incorporate diverse knowledge sources through ambiguously predicted labelings of unlabeled target language data.", "labels": [], "entities": []}, {"text": "This permits effective relexicalization and target language adaptation of the transfer parser.", "labels": [], "entities": []}, {"text": "Here, we experiment with two different knowledge sources: arc sets, which are filtered by marginal probabilities from the cross-lingual transfer parser, are used in an ambiguity-aware self-training algorithm ( \u00a75.2); these arc sets are then combined with the predictions of a different transfer parser in an ambiguity-aware ensemble-training algorithm ( \u00a75.3).", "labels": [], "entities": []}, {"text": "The resulting parser provides significant improvements over a strong baseline parser and achieves a 13% relative error reduction on average with respect to the best model of, outperforming it on 15 out of the 16 evaluated languages.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 104, "end_pos": 128, "type": "METRIC", "confidence": 0.8235087593396505}]}], "datasetContent": [{"text": "Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of on selective parameter sharing can be incorporated into such models in the transfer scenario.", "labels": [], "entities": []}, {"text": "We first review the basic graph-based parser framework and the experimental setup that we will use throughout.", "labels": [], "entities": []}, {"text": "We then delve into details on how to incorporate selective sharing in this model in \u00a74.", "labels": [], "entities": []}, {"text": "In \u00a75, we show how learning with ambiguous labelings in this parser can be used for further target language adaptation, both through self-training and through ensemble-training.", "labels": [], "entities": [{"text": "target language adaptation", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.6987198988596598}]}, {"text": "To facilitate comparison with the state of the art, we use the same treebanks and experimental setup as.", "labels": [], "entities": []}, {"text": "Notably, we use the mapping proposed by to map from fine-grained treebank specific part-of-speech tags to coarse-grained \"universal\" tags, rather than the more recent mapping proposed by.", "labels": [], "entities": []}, {"text": "For  We now study the different approaches to target language adaptation empirically.", "labels": [], "entities": [{"text": "target language adaptation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6263768176237742}]}, {"text": "As in, we use the CoNLL training sets, stripped of all dependency information, as the unlabeled target language data in our experiments.", "labels": [], "entities": [{"text": "CoNLL training sets", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.9281749526659647}]}, {"text": "We use the Family model as the base parser, which is used to label the unlabeled target data with the Viterbi parses as well as with the ambiguous labelings.", "labels": [], "entities": []}, {"text": "The final model is then trained on this data using standard lexicalized features).", "labels": [], "entities": []}, {"text": "Since labeled training data is unavailable in the target language, we cannot tune any hyper-parameters and simply set \u03bb = 1 and \u03c3 = 0.95 throughout.", "labels": [], "entities": []}, {"text": "Although the latter may suggest that\u02dcythat\u02dc that\u02dcy(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language.", "labels": [], "entities": []}, {"text": "The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems.", "labels": [], "entities": []}, {"text": "First, NBG+EM is the generative model of trained with expectation-maximization on additional unlabeled target language text.", "labels": [], "entities": []}, {"text": "Second, Family is the best discriminative model from the previous section.", "labels": [], "entities": []}, {"text": "Third, Viterbi is the basic Viterbi self-training model.", "labels": [], "entities": []}, {"text": "The results of each of these models are shown in.", "labels": [], "entities": []}, {"text": "There area number of things that can be observed.", "labels": [], "entities": []}, {"text": "First, Viterbi self-training helps slightly on average, but the gains are not consistent and there are even drops inaccuracy for some languages.", "labels": [], "entities": []}, {"text": "Second, AAST outperforms the Viterbi variant on all languages and  nearly always improves on the base parser, although it sees a slight drop for Italian.", "labels": [], "entities": [{"text": "AAST", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.7949672341346741}]}, {"text": "AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish.", "labels": [], "entities": [{"text": "AAST", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7617426514625549}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9996070265769958}, {"text": "absolute", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9727590680122375}]}, {"text": "Comparing this model to the NBG+EM baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages.", "labels": [], "entities": [{"text": "NBG+EM baseline", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8732362240552902}]}, {"text": "Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models.", "labels": [], "entities": []}, {"text": "Naseem et al. observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages.", "labels": [], "entities": []}, {"text": "AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average.", "labels": [], "entities": [{"text": "AAST", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.38233649730682373}]}, {"text": "Third, as shown in the rightmost column of, ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages.", "labels": [], "entities": [{"text": "AAET", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.6673411726951599}]}, {"text": "The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to NBG+EM is 13%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9723854064941406}, {"text": "NBG+EM", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.5263834297657013}]}, {"text": "Before concluding, two additional points are worth making.", "labels": [], "entities": []}, {"text": "First, further gains may potentially be achievable with feature-rich discriminative models.", "labels": [], "entities": []}, {"text": "While the best generative transfer model of approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%).", "labels": [], "entities": [{"text": "generative transfer", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.9694042801856995}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.7469148635864258}]}, {"text": "One promising statistic along these lines is that the oracle accuracy for the ambiguous labelings of AAST is 75.7%, averaged across languages, which suggests that other training algorithms, priors or constraints could improve the accuracy substantially.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9829849600791931}, {"text": "accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.9985933899879456}]}, {"text": "Second, relexicalization is a key component of self-training.", "labels": [], "entities": []}, {"text": "If we use delexicalized features during self-training, we only observe a small average improvement from 62.0% to 62.1%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Unlabeled attachment scores of the multi-source  transfer models. Boldface numbers indicate the best result  per language. Underlined numbers indicate languages  whose group is not represented in the training data (these  default to Share under Similarity and Family). NBG is the  \"D-,T o \" model in Table 2 from Naseem et al. (2012).", "labels": [], "entities": []}, {"text": " Table 3: Target language adaptation using unlabeled tar- get data. AAST: ambiguity-aware self-training. AAET:  ambiguity-aware ensemble-training. Boldface numbers  indicate the best result per language. Underlined numbers  indicate the best result, excluding AAET. NBG+EM is the  \"D+,T o \" model from Naseem et al. (2012).", "labels": [], "entities": [{"text": "language adaptation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7396216094493866}, {"text": "AAST", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.7255141139030457}, {"text": "AAET", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.8696840405464172}]}]}