{"title": [], "abstractContent": [{"text": "Crowdsourcing, while ideally reducing both costs and the need for domain experts , is no all-purpose tool.", "labels": [], "entities": []}, {"text": "We review how paraphrase recognition has benefited from crowdsourcing in the past and identify two problems in paraphrase acquisition and semantic similarity evaluation that can be solved by employing a smart crowdsourcing strategy.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.9384832084178925}, {"text": "paraphrase acquisition", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.8655640780925751}, {"text": "semantic similarity evaluation", "start_pos": 138, "end_pos": 168, "type": "TASK", "confidence": 0.68452321489652}]}, {"text": "First, we employ the CrowdFlower platform to conduct an experiment on sub-sentential paraphrase acquisition with early exclusion of low-accuracy crowdworkers.", "labels": [], "entities": [{"text": "sub-sentential paraphrase acquisition", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.6502300401528677}]}, {"text": "Second, we compare two human intelligence task designs for evaluating phrase pairs on a semantic similarity scale.", "labels": [], "entities": []}, {"text": "While the first experiment confirms our strategy successful at tackling the problem of missing gold in paraphrase generation, the results of the second experiment suggest that, for both semantic similarity evaluation on a continuous and a binary scale, querying crowd-workers fora semantic similarity value on a multi-grade scale yields better results than directly asking fora binary classification .", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.8465665876865387}, {"text": "semantic similarity evaluation", "start_pos": 186, "end_pos": 216, "type": "TASK", "confidence": 0.7143564025561014}]}], "introductionContent": [{"text": "Paraphrase recognition 1 means to analyse whether two texts are paraphrastic, i.e. \"a pair of units of text deemed to be interchangeable\".", "labels": [], "entities": [{"text": "Paraphrase recognition 1", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9095583955446879}]}, {"text": "It has numerous applications in information retrieval, information extraction, machine translation and plagiarism detection.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8533825576305389}, {"text": "information extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8705110549926758}, {"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.836299329996109}, {"text": "plagiarism detection", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.7745290100574493}]}, {"text": "For instance, an internet search provider could recognize \"murder of the 35th U.S. president\" and \"assassination of John F. Kennedy\" to be 1 the terms paraphrase detection and paraphrase identification might be used instead paraphrases of each other and thus yield the same result.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 151, "end_pos": 171, "type": "TASK", "confidence": 0.7298592329025269}, {"text": "paraphrase identification", "start_pos": 176, "end_pos": 201, "type": "TASK", "confidence": 0.6709430515766144}]}, {"text": "Paraphrase recognition is an open research problem and, even though having progressed immensely in recent years), state of the art performance is still below the human reference.", "labels": [], "entities": [{"text": "Paraphrase recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9745166599750519}]}, {"text": "In this research, we analyse how crowdsourcing can contribute to paraphrase recognition.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9837246835231781}]}, {"text": "Crowdsourcing is the process of outsourcing avast number of small, simple tasks, so called HITs 2 , to a distributed group of unskilled workers, so called crowdworkers 3 . Reviewing current literature on the topic, we identify two problems in paraphrase acquisition and semantic similarity evaluation that can be solved by employing a smart crowdsourcing strategy.", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 243, "end_pos": 265, "type": "TASK", "confidence": 0.9429051876068115}, {"text": "semantic similarity evaluation", "start_pos": 270, "end_pos": 300, "type": "TASK", "confidence": 0.7131475210189819}]}, {"text": "First, we propose how to reduce paraphrase generation costs by early exclusion of low-accuracy crowdworkers.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.9675078094005585}]}, {"text": "Second, we compare two HIT designs for evaluating phrase pairs on a continuous semantic similarity scale.", "labels": [], "entities": []}, {"text": "In order to evaluate our crowdsourcing strategies, we conduct our own experiments via the CROWDFLOWER 4 platform.", "labels": [], "entities": [{"text": "CROWDFLOWER 4 platform", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.7542803088823954}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 first gives an overview of related work and lines out current approaches.", "labels": [], "entities": []}, {"text": "We then proceed to our own experiments on crowdsourcing paraphrase acquisition (3.3) and semantic similarity evaluation.", "labels": [], "entities": [{"text": "crowdsourcing paraphrase acquisition", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.7576857209205627}, {"text": "semantic similarity evaluation", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.7788619796435038}]}, {"text": "Section 4 and 5 conclude the study and propose future work in the area of paraphrase recognition and crowdsourcing.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.9391975402832031}]}], "datasetContent": [{"text": "Paraphrase verification can be said to be a manual semantic similarity evaluation done by experts or trusted crowdworkers, most often on a binary scale.", "labels": [], "entities": [{"text": "Paraphrase verification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.923511266708374}]}, {"text": "However, believe that \"binary indicators of semantic equivalence are not ideal and a continuous value indicating the degree to which two pairs are paraphrastic is more suitable for most approaches\".", "labels": [], "entities": []}, {"text": "They propose averaging a large number of binary crowdworker judgements or, alternatively, a smaller number of judgements on an ordinal scale as in the SEMEVAL-2012 Semantic Textual Similarity (STS) task.", "labels": [], "entities": [{"text": "SEMEVAL-2012 Semantic Textual Similarity (STS) task", "start_pos": 151, "end_pos": 202, "type": "TASK", "confidence": 0.7456674613058567}]}, {"text": "A continuous semantic similarity score is also used to weigh the influence of sub-sentential paraphrases used by the TERP metric.", "labels": [], "entities": []}, {"text": "While only 28% of the collected pairs were validated after the traditional two-staged paraphrase generation, this percentage increased to 80% in the second validation stage belonging to the multistage approach.", "labels": [], "entities": []}, {"text": "Although the experiment was conducted on a small number of phrases, this result is a good indicator that our hypothesis is correct and that a combined generation and verification stage with gold items can reduce costs by early exclusion of low-accuracy workers.", "labels": [], "entities": []}, {"text": "Lexical divergence measures (TERP) decline, but this is expected after filtering out possibly highly divergent non-paraphrastic pairs.", "labels": [], "entities": [{"text": "Lexical divergence", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.748199075460434}]}, {"text": "While our generation costs per non-validated subsentential paraphrase were around the same as those reported by (0.024$), the costs for validated sub-sentential paraphrases were not much higher (0.06$).", "labels": [], "entities": []}, {"text": "report costs of 0.27$ per sentential paraphrase, however these costs are difficult to compare, also because we did not optimize for lexical divergence.", "labels": [], "entities": []}, {"text": "We conducted an experiment in order to determine how to optimally query continuous semantic similarity scores from crowdworkers.", "labels": [], "entities": []}, {"text": "The two different examined methods originally proposed by are binary and senary 8 semantic similarity evaluation.", "labels": [], "entities": [{"text": "senary 8 semantic similarity evaluation", "start_pos": 73, "end_pos": 112, "type": "TASK", "confidence": 0.5657510876655578}]}, {"text": "Paraphrases were taken from the MSRPC.", "labels": [], "entities": [{"text": "MSRPC", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.928019106388092}]}, {"text": "Optimality was defined by two different criteria: First, we analysed how well the (binary) paraphrase classification by domain experts on the MSRPC can be reproduced 8 senary: {0, 1, 2, 3, 4, 5} as opposed to binary {0, 1}. from our collected judgements.", "labels": [], "entities": [{"text": "Optimality", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8821067214012146}, {"text": "MSRPC", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.956184446811676}]}, {"text": "Second, we analysed how consistent our collected judgements are.", "labels": [], "entities": []}, {"text": "Since we could not find any reference corpus for semantic similarity evaluation apart from the SEMEVAL-2012 STS gold that was also acquired via crowdsourcing, we resorted to training a machine learning classifier and comparing relative performance on the collected training data.", "labels": [], "entities": [{"text": "semantic similarity evaluation", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.8161793351173401}, {"text": "SEMEVAL-2012 STS gold", "start_pos": 95, "end_pos": 116, "type": "DATASET", "confidence": 0.7414137919743856}]}, {"text": "We trained the UKP machine learning classifier originally developed for the Semantic Textual Similarity (STS) task at SemEval-2012 () on the averaged binary and senary judgements for 207 identical phrase pairs.", "labels": [], "entities": [{"text": "UKP machine learning classifier", "start_pos": 15, "end_pos": 46, "type": "DATASET", "confidence": 0.8637082278728485}, {"text": "Semantic Textual Similarity (STS) task at SemEval-2012", "start_pos": 76, "end_pos": 130, "type": "TASK", "confidence": 0.7697309719191657}]}, {"text": "Since we were not interested in the performance of the machine learning classifier but in the quality of the collected data, we measured the relative performance of the learned model on the training data.", "labels": [], "entities": []}, {"text": "The number of training examples remained constant.", "labels": [], "entities": []}, {"text": "This was repeated multiple times while varying the number of judgements used in the aggregation of the semantic similarity values.", "labels": [], "entities": []}, {"text": "We observed that with increasing number of judgements, the correlation coefficient converges seemingly against an upper bound (binary: 0.68 for 20 judgements, senary: 0.741 for 8 judgements).", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 59, "end_pos": 82, "type": "METRIC", "confidence": 0.978158563375473}]}, {"text": "The Ina second step, we compared the performance while employing different input normalization techniques on the whole set of 667 phrase pairs with senary judgements.", "labels": [], "entities": []}, {"text": "While all techniques increased the trained classifier's performance, weighted voting performed best (2).", "labels": [], "entities": []}, {"text": "In addition to the machine learning evaluation, we compared our results to the binary semantic similarity classification given by the MSRPC expert annotators.", "labels": [], "entities": [{"text": "MSRPC expert annotators", "start_pos": 134, "end_pos": 157, "type": "DATASET", "confidence": 0.8207602103551229}]}, {"text": "In order to do so, we had to find an optimal threshold in splitting our semantic similarity range in two, dividing paraphras-  tic from non-paraphrastic phrase pairs.", "labels": [], "entities": []}, {"text": "Again, this was repeated multiple times while varying the number of judgements used in the aggregation of the semantic similarity values.", "labels": [], "entities": []}, {"text": "However, this time we did not simply take the first n judgements each, but averaged over different possible sampling combinations.", "labels": [], "entities": []}, {"text": "We measured percentage agreement with MSRPC and the optimal threshold for non-weighted and weighted judgements, since weighted voting performed best in the machine learning evaluation (5c).", "labels": [], "entities": []}, {"text": "Surprisingly, even for binary paraphrastic-nonparaphrastic classification, querying a senary semantic similarity value from crowdworkers yields better results than directly asking fora binary classification.", "labels": [], "entities": []}, {"text": "However, the results also indicate that in both cases, input normalization plays an important role and agreement could be improved by more sophisticated or combined input normalization techniques as well as by collecting additional judgements.", "labels": [], "entities": [{"text": "agreement", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.960440993309021}]}, {"text": "A semantic similarity of 3.1 (senary) (5a) respectively 3.5 (binary) (5b) corresponds optimally to the paraphrastic-non-paraphrastic threshold chosen by the MSRPC expert annotators.", "labels": [], "entities": [{"text": "MSRPC expert annotators", "start_pos": 157, "end_pos": 180, "type": "DATASET", "confidence": 0.7941242059071859}]}, {"text": "Costs per evaluated phrase pair were at 0.16$", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Two-staged (1. -2.) and multi-staged (1. -4.) paraphrase generation results. Percentage values  denote the amount of validated pairs relative to the preceding generation stage.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7305713593959808}]}, {"text": " Table 2: Input normalization results", "labels": [], "entities": [{"text": "Input normalization", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8115482926368713}]}]}