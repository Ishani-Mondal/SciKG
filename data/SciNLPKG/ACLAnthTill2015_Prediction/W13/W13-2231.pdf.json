{"title": [{"text": "Coping with the Subjectivity of Human Judgements in MT Quality Estimation", "labels": [], "entities": [{"text": "Subjectivity of Human Judgements", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.6643442660570145}, {"text": "MT Quality Estimation", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.8728253444035848}]}], "abstractContent": [{"text": "Supervised approaches to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.9174136817455292}]}, {"text": "For some tasks, however , the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications.", "labels": [], "entities": []}, {"text": "In Machine Translation (MT) Quality Estimation (QE), for instance, using human-annotated data to train a binary classifier that discriminates between good (useful fora post-editor) and bad translations is not trivial.", "labels": [], "entities": [{"text": "Machine Translation (MT) Quality Estimation (QE)", "start_pos": 3, "end_pos": 51, "type": "TASK", "confidence": 0.8694120824337006}]}, {"text": "Focusing on this binary task, we show that subjective human judgements can be effectively replaced with an automatic annotation procedure.", "labels": [], "entities": []}, {"text": "To this aim, we compare binary classifiers trained on different data: the human-annotated dataset from the 7 th Workshop on Statistical Machine Translation (WMT-12), and an automatically labelled version of the same corpus.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT-12)", "start_pos": 124, "end_pos": 164, "type": "TASK", "confidence": 0.7268240749835968}]}, {"text": "Our results show that human labels are less suitable for the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the steady progress in the field of Statistical Machine Translation (SMT), the translation industry is now faced with the possibility of significant productivity increases (i.e. amount of publishable output per unit of time).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 41, "end_pos": 78, "type": "TASK", "confidence": 0.8716786007086436}]}, {"text": "One way to achieve this goal, in Computer Assisted Translation (CAT) environments, is the integration of (precise, but often partial) suggestions obtained through \"fuzzy matches\" from a Translation Memory (TM), with (complete, but potentially less precise) translations produced by an MT system.", "labels": [], "entities": [{"text": "Computer Assisted Translation (CAT)", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.8292857011159261}]}, {"text": "Such integration can loosely consist in presenting translators with unranked suggestions obtained from the MT and the TM, or rely on tighter combination strategies.", "labels": [], "entities": [{"text": "MT", "start_pos": 107, "end_pos": 109, "type": "DATASET", "confidence": 0.5995075106620789}]}, {"text": "For instance, MT and TM translations can be automatically ranked to ease the selection of the most suitable one for post-editing (, or the TM can be used to constrain and improve MT suggestions).", "labels": [], "entities": [{"text": "MT and TM translations", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7505116909742355}, {"text": "MT", "start_pos": 179, "end_pos": 181, "type": "TASK", "confidence": 0.9687747359275818}]}, {"text": "In all cases, the effectiveness of the integration is conditioned by: i) the quality of MT, and ii) the accuracy in automatically predicting such quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.8391704559326172}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9993606209754944}]}, {"text": "Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable \"as is\", and the capability to automatically identify and present to human translators only such suggestions.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9738932251930237}]}, {"text": "Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations ().", "labels": [], "entities": [{"text": "Recognizing good translations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8422874410947164}, {"text": "MT Quality Estimation (QE)", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.8331163624922434}]}, {"text": "In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation, the definition of features showing good correlation with human judgements , and the release of open-source software.", "labels": [], "entities": [{"text": "QE", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.796196460723877}, {"text": "MT community", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.9105409979820251}]}, {"text": "The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data.", "labels": [], "entities": [{"text": "QE problem", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.8714320957660675}]}, {"text": "While early works () exploited annotations obtained with automatic MT evaluation metrics like BLEU (), the current trend is to rely on human annotations, which seem to lead to more accurate models.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.8911100924015045}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9969136714935303}]}, {"text": "Along this direction, the QE task consists in predicting scores that reflect human quality judgements, by learning from manually annotated datasets (e.g. collections of source-target pairs la-belled according to an n-point Likert scale or with real numbers in a given interval).", "labels": [], "entities": [{"text": "QE task", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.8735502064228058}, {"text": "predicting", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.9653699398040771}]}, {"text": "Within this dominant supervised framework, we explore different ways to obtain labelled data for training a binary QE classifier suitable for integration in a CAT tool.", "labels": [], "entities": []}, {"text": "Since, to the best of our knowledge, labelled data with binary judgements are currently not available, we consider two alternative options.", "labels": [], "entities": []}, {"text": "The first option is to adapt an existing dataset, checking whether it can be partitioned in away that reflects the distinction between good (useful for the translator, suitable for post editing) and bad translations (that need complete rewriting).", "labels": [], "entities": []}, {"text": "To this aim we experiment with the QE data released within the 7 th Workshop on Machine Translation (WMT-12).", "labels": [], "entities": [{"text": "QE data released within the 7 th Workshop on", "start_pos": 35, "end_pos": 79, "type": "DATASET", "confidence": 0.8965704076819949}, {"text": "Machine Translation (WMT-12)", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.6548519611358643}]}, {"text": "The corpus consists of source-target pairs annotated with manual QE labels (1-5 scores) indicating the post-editing needed to correct the translations.", "labels": [], "entities": [{"text": "QE labels", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9143439531326294}]}, {"text": "Besides explicit human judgements, the availability of post-edited translations makes also possible to calculate the actual HTER values, indicating the minimum edit distance between the machine translation and its manually post-edited version in the The second option is to automatically reannotate the same dataset, trying to produce labels that reflect an objective and more reliable binary distinction based on empirical observations.", "labels": [], "entities": [{"text": "HTER", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.873421847820282}]}, {"text": "Our analysis aims to answer the following questions: 1.", "labels": [], "entities": []}, {"text": "Are human labels reliable and coherent enough to train accurate binary models?", "labels": [], "entities": []}, {"text": "2. Are arbitrarily-set thresholds useful to partition QE data for this task?", "labels": [], "entities": []}, {"text": "3. Is it possible to obtain reliable binary annotations from an automatic procedure?", "labels": [], "entities": []}, {"text": "Negative answers to the first two questions would respectively call into question: i) the intuitive idea that human labels are the most reliable fora supervised approach to binary QE, and ii) the possibility that thresholds on a single metric (e.g. the HTER) can beset to capture the subtle differences separating useful from useless translations.", "labels": [], "entities": []}, {"text": "A positive answer to the third question would open to the possibility to create training datasets in a more coherent and replicable way compared to current data annotation methods.", "labels": [], "entities": []}, {"text": "By answering these questions, this paper provides the following main contributions: \u2022 We show that training a binary classifier on arbitrary partitions of an existing dataset is difficult.", "labels": [], "entities": []}, {"text": "Our experiments with the WMT-12 corpus demonstrate that neither following standard indications (e.g. \"if more than 70% of the MT output needs to be edited, a translation from scratch is necessary\") 3 , nor considering arbitrary HTER thresholds, it is possible to obtain accurate binary classifiers suitable for integration in a CAT environment; \u2022 We propose a replicable automatic (hence non subjective) method to re-annotate an existing dataset in away that the resulting binary classifier outperforms those trained with human labels.", "labels": [], "entities": [{"text": "WMT-12 corpus", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9656093716621399}]}, {"text": "\u2022 We show that, with our method, a smaller amount of training data is sufficient to obtain similar or better performance compared to that of the human-annotated dataset used for comparison.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to the lack of datasets annotated with explicit binary (good, bad) judgements about translation quality, the most intuitive way to obtain training data for our QE classifier is to adapt existing manually-labelled data.", "labels": [], "entities": []}, {"text": "The reasonable size of the WMT-12 dataset makes it a good candidate for our purposes.", "labels": [], "entities": [{"text": "WMT-12 dataset", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.9657461643218994}]}, {"text": "The corpus consists of 2,254 English-Spanish news sentences (1,832 for training, 422 for test) produced by the Moses phrasebased SMT system () trained on Europarl ( and News Commentaries corpora, along with their source sentences, reference translations and post-edited translations.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.986883819103241}]}, {"text": "Training and test instances have been annotated by professional translators with scores (1 to 5) indicating the estimated post-editing effort (percentage of MT output that has to be corrected).", "labels": [], "entities": []}, {"text": "According to the proposed scheme, the highest score indicates lowest effort (MT output requires little or no editing), while the lowest score indicates that the MT output needs to be translated from scratch.", "labels": [], "entities": [{"text": "MT output", "start_pos": 77, "end_pos": 86, "type": "TASK", "confidence": 0.8098059296607971}, {"text": "MT output", "start_pos": 161, "end_pos": 170, "type": "TASK", "confidence": 0.846555083990097}]}, {"text": "To cope with systematic biases among the annotators, 5 the judgements were combined in a final score obtained from their weighted average, resulting in a labelled dataset with real numbers in the interval as effort scores.", "labels": [], "entities": []}, {"text": "In order to obtain suitable data for binary QE, the WMT-12 training set (1,832 instances) has been partitioned in different ways, leaving the test set for evaluation (see Section 5).", "labels": [], "entities": [{"text": "WMT-12 training set", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.8581216732660929}]}, {"text": "The goal, for each partition strategy, was to label as bad (the assigned label is -1) only the translations that need complete rewriting, keeping all the other translations as good instances (labelled with +1).", "labels": [], "entities": []}, {"text": "Considering the averaged effort scores, the actual human judgements, and the HTER values calculated between the translations and the corresponding postedited version, we experimented with the following three partition criteria.", "labels": [], "entities": [{"text": "HTER", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.986748218536377}]}, {"text": "Average effort scores (AES).", "labels": [], "entities": [{"text": "Average effort scores (AES)", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.8747154672940572}]}, {"text": "Three partitions have been generated based on the effort scores of 2, 2.5, and 3, labelling the WMT-12 training instances with scores below or equal to each threshold as negative examples (-1), and the instances with scores above the threshold as positive examples (+1).", "labels": [], "entities": [{"text": "WMT-12", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.820986270904541}]}, {"text": "Partitions with thresholds below 2 were also considered, including the most intuitive partition with cut-off set to 1.", "labels": [], "entities": []}, {"text": "However, the resulting number of negative instances, if any, was too scarce, and the overall dataset too unbalanced, to make standard supervised learning methods effective The creation of highly unbalanced data is a recurring issue for all the partition meth-ods we applied to the WMT-12 corpus.", "labels": [], "entities": [{"text": "WMT-12 corpus", "start_pos": 281, "end_pos": 294, "type": "DATASET", "confidence": 0.9661069214344025}]}, {"text": "Together with the low homogeneity of human labels (even for very poor translations the three judges do not agree in assigning the lowest score), inmost of the cases the small number of low-quality translations in the dataset makes the negative class considerably smaller than the positive one.", "labels": [], "entities": []}, {"text": "This can be observed in, which provides the total number of positive and negative instances for each partition method.", "labels": [], "entities": []}, {"text": "For instance, with our lowest AES threshold (2) the total number of negative instances is 113, while the positive ones are 1,719.", "labels": [], "entities": [{"text": "AES threshold", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.970740795135498}]}, {"text": "Although considering different cut-off criteria aims to make our investigation more complete, it's also worth remarking that the higher the threshold, the higher the distance of the resulting experimental setting from our target scenario.", "labels": [], "entities": []}, {"text": "While 2, as an effort score threshold, is likely to reflect a reasonable separation between useless and post-editable translations, higher values are in principle more appropriate for \"soft\" separations into worse versus better translations.", "labels": [], "entities": []}, {"text": "Human scores (HS).", "labels": [], "entities": [{"text": "Human scores (HS)", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.6473251402378082}]}, {"text": "Five partitions have been generated using the actual labels assigned by the three annotators to each translation instead of the average effort scores.", "labels": [], "entities": []}, {"text": "In particular, we considered the following score combinations (\"X\" stands for any integer between 1 and 5): 1-X-X, 2-2-2, 2-2-X, 2-3-3, 3-3-3.", "labels": [], "entities": []}, {"text": "Also in this case, as shown in, partitions based on lower scores lead to highly unbalanced datasets of limited usability, while those based on higher scores are increasingly more distant to our application scenario.", "labels": [], "entities": []}, {"text": "HTER scores (HTER).", "labels": [], "entities": [{"text": "HTER scores (HTER)", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.5253939270973206}]}, {"text": "Seven partitions have been generated considering the following HTER thresholds: 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45.", "labels": [], "entities": []}, {"text": "In this case, being the HTER an error measure, training instances with scores above or equal to the threshold were labelled as negative examples (-1), while instances with lower scores were labelled as positive examples (+1).", "labels": [], "entities": [{"text": "HTER", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.4350024461746216}]}, {"text": "Similar to the other partition criteria, some of our threshold values reflect our task more closely than others, but result in more unbalanced datasets.", "labels": [], "entities": []}, {"text": "In particular, thresholds around 0.7 substantially adhere to the WMT-12 annotation guidelines (as far as translations that need complete rewriting are concerned) and produce training data with fewer negative instances.", "labels": [], "entities": [{"text": "WMT-12", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.7644857168197632}]}, {"text": "Other thresholds, which is still worth exploring since we do not know the optimal cut-off value, are in principle less suitable to our task but produce more balanced training data.", "labels": [], "entities": []}, {"text": "As an alternative to partitioning methods, we investigated the possibility to re-annotate the WMT-12 training set with an automatic procedure.", "labels": [], "entities": [{"text": "WMT-12 training set", "start_pos": 94, "end_pos": 113, "type": "DATASET", "confidence": 0.901565154393514}]}, {"text": "At this point, the question is: are the automatically labelled data more suitable than partitions based on human labels to train a binary QE classifier?", "labels": [], "entities": []}, {"text": "To answer this question, all the proposed separations of the WMT-12 training set have been evaluated on different test sets.", "labels": [], "entities": [{"text": "WMT-12 training set", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.7867668668429056}]}, {"text": "For each separation we trained a binary classifier able to assign a label (good or bad) to unseen source-target pairs.", "labels": [], "entities": []}, {"text": "Since the classifiers use the same algorithm and feature set, differences in performance will mainly depend on the quality of the training data on which they are built.", "labels": [], "entities": []}, {"text": "Using task-oriented metrics sensitive to the number of false positives, results highlighting such differences will indicate the best separation.", "labels": [], "entities": []}, {"text": "Each separation of the WMT-12 training data was used to train a binary SVM classifier.", "labels": [], "entities": [{"text": "WMT-12 training data", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.8634200493494669}]}, {"text": "Different kernels and parameters were optimized through a grid search in 5-fold cross-validation on each training set.", "labels": [], "entities": []}, {"text": "Being the number of positive and negative training instances highly unbalanced, the best models were selected optimizing a metric that takes into account the number of true and false positives (see below).", "labels": [], "entities": []}, {"text": "Seventeen features proposed in ( were extracted from each source-target pair.", "labels": [], "entities": []}, {"text": "This feature set, fully described in, mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities).", "labels": [], "entities": []}, {"text": "Results of the WMT 2012 QE task shown that these \"baseline\" features are particularly competitive in the regression task, with only few systems able to beat them.", "labels": [], "entities": [{"text": "WMT 2012 QE task", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.7402123361825943}, {"text": "regression task", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.8976692259311676}]}, {"text": "All the features are extracted using the Quest software and the model files released by the organizers of the WMT 2013 workshop.", "labels": [], "entities": [{"text": "WMT 2013 workshop", "start_pos": 110, "end_pos": 127, "type": "DATASET", "confidence": 0.759740432103475}]}, {"text": "To obtain different separations between good and bad translations, artificial test sets have been created using arbitrary thresholds on the HTER (the same used to partition the training set on a HTER basis) and the post-editing time (PET).", "labels": [], "entities": [{"text": "post-editing time (PET)", "start_pos": 215, "end_pos": 238, "type": "METRIC", "confidence": 0.7187230467796326}]}, {"text": "Two different datasets were split: i) the WMT-12 test (422 source, target, post-edited and reference sentences); ii) the WMT-13 training set for Task 1.3 (800 source, target and post-edited sentences labelled with PET).", "labels": [], "entities": [{"text": "WMT-12 test", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.8813506960868835}, {"text": "WMT-13 training set", "start_pos": 121, "end_pos": 140, "type": "DATASET", "confidence": 0.8668768604596456}, {"text": "PET", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.8359577059745789}]}, {"text": "The first dataset, the most similar to the WMT-12 training set, should better reflect (and reward) the HTER-based partitions proposed in Section 3.", "labels": [], "entities": [{"text": "WMT-12 training set", "start_pos": 43, "end_pos": 62, "type": "DATASET", "confidence": 0.8925267259279887}]}, {"text": "The WMT-13 dataset contains sentences translated with a different configuration (data and parameters) of the SMT engine.", "labels": [], "entities": [{"text": "WMT-13 dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9532157480716705}, {"text": "SMT engine", "start_pos": 109, "end_pos": 119, "type": "TASK", "confidence": 0.9221364557743073}]}, {"text": "This can result in different HTER-based partitions in good and bad, useful to test the portability of our automatic re-annotation method across different datasets.", "labels": [], "entities": []}, {"text": "Finally, testing on data partitions based on PET allows us to check the stability of the automatic re-annotation method when evaluated on a test set divided according to a different concept of translation quality.", "labels": [], "entities": []}, {"text": "In the end, the combination of different partition methods, thresholds and datasets results in 21 different test sets (see).", "labels": [], "entities": []}, {"text": "F-score and accuracy are the classic evaluation metrics used in classification.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9776605367660522}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9991342425346375}]}, {"text": "In our evaluation, however, they would always result in high uninformative values due to the unbalanced nature of the test sets (positive instances negative instances).", "labels": [], "entities": []}, {"text": "In order to bet-: Number of positive and negative instances for each partition of the WMT-12 test set and WMT-13 training set.", "labels": [], "entities": [{"text": "WMT-12 test set", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9632280866305033}, {"text": "WMT-13 training set", "start_pos": 106, "end_pos": 125, "type": "DATASET", "confidence": 0.9320548971494039}]}, {"text": "\"*\": Average PET computed on all the instances in the WMT-13 dataset.", "labels": [], "entities": [{"text": "PET", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.788809061050415}, {"text": "WMT-13 dataset", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9814563095569611}]}, {"text": "ter understand the real quality of the classification, we hence opted for two task-oriented evaluation metrics sensitive to the number of false positives (the main issue in a CAT environment, where false positives and true positives should be respectively minimized and maximized).", "labels": [], "entities": []}, {"text": "These are: i) the weighted combination of the false positive rate (FPR) and false discovery rate (FDR), and ii) the weighed average of sensitivity and specificity (also called balanced/weighted accuracy).", "labels": [], "entities": [{"text": "false positive rate (FPR)", "start_pos": 46, "end_pos": 71, "type": "METRIC", "confidence": 0.7816596329212189}, {"text": "false discovery rate (FDR)", "start_pos": 76, "end_pos": 102, "type": "METRIC", "confidence": 0.799655536810557}, {"text": "sensitivity", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.9827073216438293}, {"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.771049976348877}]}, {"text": "FPR measures the level of false positives, but does not provide information about the number of true positives.", "labels": [], "entities": [{"text": "FPR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7297919392585754}]}, {"text": "For this reason, we combined it with FDR (1-precision), which indirectly controls the level of true positives.", "labels": [], "entities": [{"text": "FDR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9983772039413452}]}, {"text": "FPR and FDR were equally weighted in the average; lower values indicate good performance.", "labels": [], "entities": [{"text": "FPR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6151766180992126}, {"text": "FDR", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.8086345195770264}]}, {"text": "Furthermore, in our scenario it is desirable to have a classifier with high prediction accuracy over the minority class (specificity), while maintaining reasonable accuracy for the majority class (sensitivity).", "labels": [], "entities": [{"text": "prediction accuracy", "start_pos": 76, "end_pos": 95, "type": "METRIC", "confidence": 0.652759462594986}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9933761954307556}]}, {"text": "Weighted accuracy is useful in such situations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9894178509712219}]}, {"text": "To better asses the performance on the minority (negative) class, we hence gave more importance to specificity (0.7 vs 0.3).", "labels": [], "entities": []}, {"text": "As regards weighted accuracy higher values in indicate better performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.987179160118103}]}, {"text": "Penalizing majority voting classifiers, both metrics are particularly appropriate in our framework.", "labels": [], "entities": []}, {"text": "Besides evaluation, the weighted average of FPR and FDR was also used to tune the parameters of the SVM classifier.", "labels": [], "entities": [{"text": "FPR", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9901652932167053}, {"text": "FDR", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9945634007453918}]}, {"text": "presents the results achieved by classifiers trained on different datasets, on the 21 splits produced from the test sets used for evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of positive/negative instances for  each partition of the WMT-12 training set.", "labels": [], "entities": [{"text": "WMT-12 training set", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.8891483147939047}]}, {"text": " Table 2: Number of positive and negative in- stances for each partition of the WMT-12 test set  and WMT-13 training set. \"*\": Average PET com- puted on all the instances in the WMT-13 dataset.", "labels": [], "entities": [{"text": "WMT-12 test set", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9684015909830729}, {"text": "WMT-13 training set", "start_pos": 101, "end_pos": 120, "type": "DATASET", "confidence": 0.9178153276443481}, {"text": "PET com- puted", "start_pos": 135, "end_pos": 149, "type": "METRIC", "confidence": 0.9503583759069443}, {"text": "WMT-13 dataset", "start_pos": 178, "end_pos": 192, "type": "DATASET", "confidence": 0.9841687381267548}]}, {"text": " Table 3: Weighted FPR-FDR (left table) and weighted Accuracy (right table) obtained by the binary QE  classifiers trained on different separations of the WMT-12 training set. Several arbitrary partitions of the  WMT-12 Test set and WMT-13 Training set are considered.", "labels": [], "entities": [{"text": "FPR-FDR", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9420934319496155}, {"text": "Accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.7116906642913818}, {"text": "WMT-12 training set", "start_pos": 155, "end_pos": 174, "type": "DATASET", "confidence": 0.9160893758138021}, {"text": "WMT-12 Test set", "start_pos": 213, "end_pos": 228, "type": "DATASET", "confidence": 0.9728395740191141}, {"text": "WMT-13 Training set", "start_pos": 233, "end_pos": 252, "type": "DATASET", "confidence": 0.9527929226557413}]}]}