{"title": [{"text": "Interpreting Situated Dialogue Utterances: an Update Model that Uses Speech, Gaze, and Gesture Information", "labels": [], "entities": [{"text": "Interpreting Situated Dialogue Utterances", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8057731240987778}]}], "abstractContent": [], "introductionContent": [{"text": "Speech by necessity unfolds overtime, and in spoken conversation, this time is shared between the participants.", "labels": [], "entities": []}, {"text": "Speakers are also by necessity located, and in face-to-face conversation, they share their (wider) location (that is, they are co-located).", "labels": [], "entities": []}, {"text": "The constraints that arise from this set of facts are often ignored in computational research on spoken dialogue, and where they are addressed, typically only one of the two is addressed.", "labels": [], "entities": []}, {"text": "Here, we present a model that computes in an incremental fashion an intention representation for dialogue acts that may comprise both spoken language and embodied cues such as gestures and gaze, where these representations are grounded in representations of the shared visual context.", "labels": [], "entities": []}, {"text": "The model is trained on conversational data and can be used as an understanding module in an incremental, situated dialogue system.", "labels": [], "entities": []}, {"text": "Our paper begins with related work and background and then specifies in an abstract way the task of the model.", "labels": [], "entities": []}, {"text": "We describe our model formally in Section 4, followed by three experiments with the model, the first establishing it with a traditional spoken language understanding (SLU) setting, the second to show that our model works well under situated conditions, and the third shows that our model can make use of embodied cues.", "labels": [], "entities": []}, {"text": "We finish the paper with a general discussion and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model's task is to predict a semantic frame, where the required slots of the frame are known beforehand and each slot value is predicted using a separate model P (I|U ).", "labels": [], "entities": []}, {"text": "We realise P (U |R) as a Naive Bayes classifier (NB) which counts cooccurrences of utterance features (words, bigrams, trigrams; so U is actually a tuple, not a single variable) and properties (but naively treats features as independent), and which is smoothed using addone smoothing.", "labels": [], "entities": []}, {"text": "As explained earlier, P (I) represents a uniform distribution at the beginning of an utterance, and the posteriori of the previous step, for later words.", "labels": [], "entities": []}, {"text": "We also train a discriminative model, P (R|U ), using a maximum entropy classifier (ME) using the same features as NB to classify properties.", "labels": [], "entities": []}, {"text": "3 3 http://opennlp.apache.org/", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: P (U |R) for our toy domain for some  values of U and R; we assume that this model is  learned from data (columns are excerpted from a  distribution over a larger vocabulary).", "labels": [], "entities": []}, {"text": " Table 3: Application of utterance the red ball,  where obj1 is the referred object", "labels": [], "entities": []}, {"text": " Table 4: Comparison of results from Pento: Naive  Bayes NB, Maximum Entropy ME, (Kennington  and Schlangen, 2012) K, (Heintze et al., 2010)  H, (Peldszus et al., 2012) P; values in parenthe- ses denote results from automatically transcribed  speech.", "labels": [], "entities": [{"text": "Maximum Entropy ME", "start_pos": 61, "end_pos": 79, "type": "METRIC", "confidence": 0.669486383597056}]}, {"text": " Table 5: Incremental Results for Pento slots with  varying sentence lengths.", "labels": [], "entities": []}, {"text": " Table 7: Incremental results for Pento slots with  varying sentence lengths.", "labels": [], "entities": []}]}