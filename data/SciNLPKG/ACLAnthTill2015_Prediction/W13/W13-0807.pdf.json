{"title": [{"text": "A Formal Characterization of Parsing Word Alignments by Synchronous Grammars with Empirical Evidence to the ITG Hypothesis", "labels": [], "entities": [{"text": "Formal Characterization of Parsing Word Alignments", "start_pos": 2, "end_pos": 52, "type": "TASK", "confidence": 0.6916849613189697}]}], "abstractContent": [{"text": "Deciding whether asynchronous grammar formalism generates a given word alignment (the alignment coverage problem) depends on finding an adequate instance grammar and then using it to parse the word alignment.", "labels": [], "entities": []}, {"text": "But what does it mean to parse a word alignment by asynchronous grammar?", "labels": [], "entities": [{"text": "parse a word alignment", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7669221609830856}]}, {"text": "This is formally undefined until we define an unambigu-ous mapping between grammatical derivations and word-level alignments.", "labels": [], "entities": []}, {"text": "This paper proposes an initial, formal characterization of alignment coverage as intersecting two partially ordered sets (graphs) of translation equivalence units, one derived by a grammar instance and another defined by the word alignment.", "labels": [], "entities": []}, {"text": "As a first sanity check, we report extensive coverage results for ITG on automatic and manual alignments.", "labels": [], "entities": [{"text": "coverage", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9787882566452026}, {"text": "ITG", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9168103337287903}]}, {"text": "Even for the ITG formalism, our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work.", "labels": [], "entities": []}], "introductionContent": [{"text": "The training data used by current statistical machine translation (SMT) models consists of source and target sentence pairs aligned together at the word level (word alignments).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.8005786836147308}]}, {"text": "For the hierarchical and syntactically-enriched SMT models, e.g.,), this training data is used for extracting statistically weighted Synchronous Context-Free Grammars (SCFGs).", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9719576835632324}, {"text": "extracting statistically weighted Synchronous Context-Free Grammars (SCFGs)", "start_pos": 99, "end_pos": 174, "type": "TASK", "confidence": 0.7299876080618964}]}, {"text": "Formally speaking, asynchronous grammar defines a set of (source-target) sentence pairs derived synchronously by the grammar.", "labels": [], "entities": []}, {"text": "Contrary to common * Institute for Logic, belief, however, asynchronous grammar (see e.g.,) does not accept (or parse) word alignments.", "labels": [], "entities": [{"text": "parse) word alignments", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.6429833397269249}]}, {"text": "This is because asynchronous derivation generates a tree pair with a bijective binary relation (links) between their nonterminal nodes.", "labels": [], "entities": []}, {"text": "For deciding whether a given word alignment is generated/accepted by a given synchronous grammar, it is necessary to interpret the synchronous derivations down to the lexical level.", "labels": [], "entities": []}, {"text": "However, it is formally defined yet how to unambiguously interpret the synchronous derivations of asynchronous grammar as word alignments.", "labels": [], "entities": []}, {"text": "One major difficulty is that synchronous productions, in their most general form, may contain unaligned terminal sequences.", "labels": [], "entities": []}, {"text": "Consider, for instance, the relatively non-complex synchronous production X \u2192 \u03b1 X (1) \u03b2 X (2) \u03b3 X (3) , X \u2192 \u03c3 X (2) \u03c4 X (1) \u00b5 X where superscript (i) stands for aligned instances of nonterminal X and all Greek symbols stand for arbitrary non-empty terminals sequences.", "labels": [], "entities": []}, {"text": "Given a word aligned sentence pair it is necessary to bind the terminal sequence by alignments consistent with the given word alignment, and then parse the word alignment with the thus enriched grammar rules.", "labels": [], "entities": []}, {"text": "This is not complex if we assume that each of the source terminal sequences is contiguously aligned with a target contiguous sequence, but difficult if we assume arbitrary alignments, including many-to-one and non-contiguously aligned chunks.", "labels": [], "entities": []}, {"text": "One important goal of this paper is to propose a formal characterization of what it means to synchronously parse a word alignment.", "labels": [], "entities": [{"text": "synchronously parse a word alignment", "start_pos": 93, "end_pos": 129, "type": "TASK", "confidence": 0.5837047755718231}]}, {"text": "Our formal characterization is borrowed from the \"parsing as intersection\" paradigm, e.g.,).", "labels": [], "entities": []}, {"text": "Conceptually, our characterization makes use of three algorithms.", "labels": [], "entities": []}, {"text": "Firstly, parse the unaligned sentence pair with the synchronous grammar to obtain a set of synchronous derivations, i.e., trees.", "labels": [], "entities": []}, {"text": "Secondly, interpret a word alignment as generating a set of synchronous trees representing the recursive translation equivalence relations of interest 1 perceived in the word alignment.", "labels": [], "entities": []}, {"text": "And finally, intersect the sets of nodes in the two sets of synchronous trees to check whether the grammar can generate (parts of) the word alignment.", "labels": [], "entities": []}, {"text": "The formal detail of each of these three steps is provided in sections 3 to 5.", "labels": [], "entities": []}, {"text": "We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by asynchronous grammar (learned from data).", "labels": [], "entities": [{"text": "alignment parsing", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.9608792364597321}]}, {"text": "This is useful for literature on learning from word aligned parallel corpora (e.g.,).", "labels": [], "entities": []}, {"text": "A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well.", "labels": [], "entities": [{"text": "alignment parsing", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.9638624787330627}]}, {"text": "We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis.", "labels": [], "entities": []}, {"text": "Our study highlights some of the technical choices left implicit in preceding work as explained in the next section.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Sets We use manually and automatically aligned corpora.", "labels": [], "entities": []}, {"text": "Manually aligned corpora come from two datasets.", "labels": [], "entities": []}, {"text": "The first ( consists of six language pairs: PortugueseEnglish, Portuguese-French, Portuguese-Spanish, English-Spanish, English-French and FrenchSpanish.", "labels": [], "entities": []}, {"text": "These datasets contain 100 sentence pairs each and distinguish Sure and Possible alignments.", "labels": [], "entities": []}, {"text": "Following, we treat these two equally.", "labels": [], "entities": []}, {"text": "The second manually aligned dataset () contains 987 sentence pairs from the English-German part of Europarl annotated using the Blinker guidelines.", "labels": [], "entities": [{"text": "English-German part of Europarl annotated", "start_pos": 76, "end_pos": 117, "type": "DATASET", "confidence": 0.7073548793792724}, {"text": "Blinker guidelines", "start_pos": 128, "end_pos": 146, "type": "DATASET", "confidence": 0.9170247316360474}]}, {"text": "The automatically aligned data comes from Europarl () in three language pairs (EnglishDutch, English-French and English-German).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9848355650901794}]}, {"text": "The corpora are automatically aligned using GIZA++) in combination with the growdiag-final-and heuristic.", "labels": [], "entities": []}, {"text": "With sentence length cutoff 40 on both sides these contain respectively 945k, 949k and 995k sentence pairs.", "labels": [], "entities": []}, {"text": "Grammatical Coverage (GC) is defined as the percentage word alignments (sentence pairs) in a parallel corpus that can be covered by an instance of the grammar (NF-ITG) (cf. Section 5).", "labels": [], "entities": [{"text": "Grammatical Coverage (GC)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8426968574523925}]}, {"text": "Clearly, GC depends on the chosen semantic interpretation of word alignments: contiguous TE's (phrase pairs) or discontiguous TE's. shows the Grammatical Coverage (GC) of NF-ITG for the different corpora dependent on the two alternative definitions of translation equivalence.", "labels": [], "entities": [{"text": "GC", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9872564077377319}, {"text": "Grammatical Coverage (GC)", "start_pos": 142, "end_pos": 167, "type": "METRIC", "confidence": 0.7254344820976257}]}, {"text": "The first thing to notice is that there is just a small difference between the Grammatical Coverage scores for these two definitions.", "labels": [], "entities": [{"text": "Grammatical Coverage", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.6402852237224579}]}, {"text": "The difference is in the order of a few percentage points, the largest difference is seen for Portuguese-French (79% v.s 74% Grammatical Coverage), for some language pairs there is no difference.", "labels": [], "entities": []}, {"text": "For the automatically aligned corpora the absolute difference is on average about 2%.", "labels": [], "entities": [{"text": "absolute difference", "start_pos": 42, "end_pos": 61, "type": "METRIC", "confidence": 0.9803714752197266}]}, {"text": "We attribute this to the fact that there are only very few discontiguous TEUs that can be covered by NF-ITG in this data.", "labels": [], "entities": []}, {"text": "The second thing to notice is that the scores are much higher for the corpora from the LREC dataset than they are for the manually aligned EnglishGerman corpus.", "labels": [], "entities": [{"text": "LREC dataset", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.9482972919940948}, {"text": "EnglishGerman corpus", "start_pos": 139, "end_pos": 159, "type": "DATASET", "confidence": 0.9344721436500549}]}, {"text": "The approximately double source and target length of the manually aligned EnglishGerman corpus, in combination with somewhat less dense alignments makes this corpus much harder than the LREC corpora.", "labels": [], "entities": [{"text": "EnglishGerman corpus", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.9615481197834015}]}, {"text": "Intuitively, one would expect that more alignment links make alignments more complicated.", "labels": [], "entities": []}, {"text": "This turns out to not always be the case.", "labels": [], "entities": []}, {"text": "Further inspection of the LREC alignments also shows that these alignments often consist of parts that are completely linked.", "labels": [], "entities": [{"text": "LREC alignments", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.756119579076767}]}, {"text": "Such completely linked parts are by definition treated as atomic TEUs, which could make the alignments look simpler.", "labels": [], "entities": []}, {"text": "This contrasts with the situation in the manually aligned English-German corpus whereon average less alignment links exist per word.", "labels": [], "entities": []}, {"text": "Examples 1 and 2 show that dense alignments can be simpler than less dense ones.", "labels": [], "entities": []}, {"text": "This is because sometimes the density implies idiomatic TEUs which leads to rather flat lexical productions.", "labels": [], "entities": []}, {"text": "We think that idiomatic TEUs reasonably belong in the lexicon.", "labels": [], "entities": []}, {"text": "When we look at the results for the automatically aligned corpora at the lowest rows in the table, we see that these are comparable to the results for the manually aligned English-German corpus (and much lower than the results for the LREC corpora).", "labels": [], "entities": []}, {"text": "This could be explained by the fact that the manually aligned English-German is not only Europarl data, but possibly also because the manual alignments themselves were obtained by initialization with the GIZA++ alignments.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9702728092670441}, {"text": "GIZA++ alignments", "start_pos": 204, "end_pos": 221, "type": "DATASET", "confidence": 0.8664373954137167}]}, {"text": "In any case, the manually and automatically acquired alignments for this data are not too different from the perspective of NF-ITG.", "labels": [], "entities": [{"text": "NF-ITG", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.9472166895866394}]}, {"text": "Further differences might exist if we would employ another class of grammars, e.g., full SCFGs.", "labels": [], "entities": []}, {"text": "One the one hand, we find that manual alignments are well but not fully covered by NF-ITG.", "labels": [], "entities": [{"text": "NF-ITG", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.8994276523590088}]}, {"text": "On the other, the automatic alignments are not covered well but NF-ITG.", "labels": [], "entities": [{"text": "NF-ITG", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9270323514938354}]}, {"text": "This suggests that these automatic alignments are difficult to cover by NF-ITG, and the reason could be that these alignments are built heuristically by trading precision for recall cf. (. reports that full ITG provides a few percentage points gains over NF-ITG.", "labels": [], "entities": [{"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9881916642189026}, {"text": "recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.9856588244438171}]}, {"text": "Overall, we find that our results for the LREC data are far higher) results but lower than the upperbounds of.", "labels": [], "entities": [{"text": "LREC data", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.7630525827407837}]}, {"text": "A similar observation holds for the EnglishGerman manually aligned EuroParl data, albeit the maximum length (15) used in () is different from ours (40).", "labels": [], "entities": [{"text": "EnglishGerman manually aligned EuroParl data", "start_pos": 36, "end_pos": 80, "type": "DATASET", "confidence": 0.8849592089653016}]}, {"text": "We attribute the difference between our results and Sogaard's approach to our choice to adopt lexical productions of NF-ITG that contain own internal alignments (the detailed version) and determined by the atomic TEUs of the word alignment.", "labels": [], "entities": []}, {"text": "Our results differ substantially from who report upperbounds (indeed our results still fall within these upperbounds for the LREC data).", "labels": [], "entities": [{"text": "LREC data", "start_pos": 125, "end_pos": 134, "type": "DATASET", "confidence": 0.8457012176513672}]}], "tableCaptions": [{"text": " Table 1: The grammatical coverage (GC) of NF-ITG for different corpora dependent on the interpretation  of word alignments: contiguous Translation Equivalence or discontiguous Translation Equivalence", "labels": [], "entities": [{"text": "grammatical coverage (GC)", "start_pos": 14, "end_pos": 39, "type": "METRIC", "confidence": 0.7970884919166565}]}]}