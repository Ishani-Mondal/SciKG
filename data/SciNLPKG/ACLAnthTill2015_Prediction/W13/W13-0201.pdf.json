{"title": [], "abstractContent": [{"text": "We are in the process of creating a pipeline for our HPSG grammar for Norwegian (NorSource).", "labels": [], "entities": [{"text": "HPSG grammar for Norwegian (NorSource)", "start_pos": 53, "end_pos": 91, "type": "DATASET", "confidence": 0.7901955672672817}]}, {"text": "NorSource uses the meaning representation Minimal Recursion Semantics (MRS).", "labels": [], "entities": [{"text": "NorSource", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9228888154029846}, {"text": "Minimal Recursion Semantics (MRS)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.7308913767337799}]}, {"text": "We present a step for validating an MRS and a step for pre-processing an MRS.", "labels": [], "entities": [{"text": "validating an MRS", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7030426065127054}]}, {"text": "The pre-processing step connects our MRS elements to a domain ontology and it can create additional states and roles.", "labels": [], "entities": [{"text": "MRS", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9670175909996033}]}, {"text": "The pipeline can be reused by other grammars from the Delph-In network.", "labels": [], "entities": []}], "introductionContent": [{"text": "NorSource 1 (), a grammar for Norwegian, is a Head-Driven Phrase Structure Grammar (HPSG) (, developed and maintained with the Linguistic Knowledge Builder (LKB) tool, and originally based on the HPSG Grammar Matrix, which is a starter kit for developing HPSG grammars).", "labels": [], "entities": [{"text": "NorSource 1", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9135803580284119}, {"text": "HPSG Grammar Matrix", "start_pos": 196, "end_pos": 215, "type": "DATASET", "confidence": 0.8326031764348348}]}, {"text": "An HPSG grammar can use Minimal Recursion Semantics (MRS) as meaning representation).", "labels": [], "entities": []}, {"text": "In order to speedup the parsing process (the unification algorithm), a HPSG grammar can be compiled and run (parsing) with the PET 2 tool).", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9696909189224243}]}, {"text": "The Flop program in PET compiles the LKB grammar and the Cheap program runs it.", "labels": [], "entities": [{"text": "LKB grammar", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.8088918626308441}]}, {"text": "An alternative to the PET system is the Answer Constraint Engine (ACE) 3 created by Woodley Packard.", "labels": [], "entities": [{"text": "Answer Constraint Engine (ACE)", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.8155922591686249}]}, {"text": "ACE can parse and generate using the compiled grammar.", "labels": [], "entities": [{"text": "ACE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.840172290802002}]}, {"text": "Our goal is to create a pipeline for the NorSource grammar and use it to create small question-answer systems or dialogue systems.", "labels": [], "entities": [{"text": "NorSource grammar", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.7290085554122925}]}, {"text": "The first step in the pipeline is the parsing process with ACE.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9888416528701782}, {"text": "ACE", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.7388417720794678}]}, {"text": "The next step is to select the most suitable MRS.", "labels": [], "entities": [{"text": "MRS", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.919840395450592}]}, {"text": "We use Velldal's ranking model.", "labels": [], "entities": []}, {"text": "The model is based on relevant sentences from our system, treebanked with [tsdb++] (.", "labels": [], "entities": []}, {"text": "The selected MRS is checked with the Swiss Army Knife of Underspesification (Utool)) and our own validating procedure.", "labels": [], "entities": [{"text": "MRS", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.5398233532905579}, {"text": "Swiss Army Knife of Underspesification (Utool))", "start_pos": 37, "end_pos": 84, "type": "DATASET", "confidence": 0.7135305069386959}]}, {"text": "Only well-formed MRSes are used in our pipeline.", "labels": [], "entities": [{"text": "MRSes", "start_pos": 17, "end_pos": 22, "type": "TASK", "confidence": 0.896654486656189}]}, {"text": "We also use Utool to solve the MRS and to eliminate any logically equivalent readings.", "labels": [], "entities": [{"text": "Utool", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.8030135035514832}, {"text": "MRS", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.5108767151832581}]}, {"text": "The next step is to pre-process the MRS (calculate event structure and generate roles), and the last step in our pipeline creates a First-Order Logic formula from the MRS (only the easy cases).", "labels": [], "entities": [{"text": "MRS", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.7456287741661072}]}, {"text": "Our contribution is the validating step and the pre-processing step.", "labels": [], "entities": [{"text": "validating", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.9696273803710938}]}, {"text": "In the next section, we give a brief introduction to MRS.", "labels": [], "entities": [{"text": "MRS", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9363037943840027}]}, {"text": "Then we present details from our validating procedure.", "labels": [], "entities": []}, {"text": "Next, we solve an MRS and eliminate logically equivalent readings with Utool.", "labels": [], "entities": [{"text": "MRS", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.6383702754974365}, {"text": "Utool", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9344072937965393}]}, {"text": "We preprocess the selected MRS in section 6.", "labels": [], "entities": [{"text": "MRS", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.7486532330513}]}, {"text": "At last, we look at away to create a First-Order Logic formula from a solved MRS and we present a few challenges from our research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}