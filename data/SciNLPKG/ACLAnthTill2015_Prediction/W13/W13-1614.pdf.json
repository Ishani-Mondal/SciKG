{"title": [], "abstractContent": [{"text": "We describe TWITA, the first corpus of Italian tweets, which is created via a completely automatic procedure, portable to any other language.", "labels": [], "entities": [{"text": "TWITA", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.7228721380233765}]}, {"text": "We experiment with sentiment analysis on two datasets from TWITA: a generic collection and a topic-specific collection.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.945062905550003}]}, {"text": "The only resource we use is a polarity lexicon, which we obtain by automatically matching three existing resources thereby creating the first polarity database for Italian.", "labels": [], "entities": []}, {"text": "We observe that albeit shallow, our simple system captures polarity distinctions matching reasonably well the classification done by human judges, with differences in performance across polarity values and on the two sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter is an online service which lets subscribers post short messages (\"tweets\") of up to 140 characters about anything, from good-morning messages to political stands.", "labels": [], "entities": []}, {"text": "Such micro texts area precious mine for grasping opinions of groups of people, possibly about a specific topic or product.", "labels": [], "entities": [{"text": "grasping opinions of groups of people", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.881438285112381}]}, {"text": "This is even more so, since tweets are associated to several kinds of meta-data, such as geographical coordinates of where the tweet was sent from, the id of the sender, the time of the day -information that can be combined with text analysis to yield an even more accurate picture of who says what, and where, and when.", "labels": [], "entities": []}, {"text": "The last years have seen an enormous increase in research on developing opinion mining systems of various sorts applying Natural Language Processing techniques.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.7704918086528778}]}, {"text": "Systems range from simple lookups in polarity or affection resources, i.e. databases where a polarity score (usually positive, negative, or neutral) is associated to terms, to more sophisticated models built through supervised, unsupervised, and distant learning involving various sets of features.", "labels": [], "entities": []}, {"text": "Tweets are produced in many languages, but most work on sentiment analysis is done for English (even independently of Twitter).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.9396668970584869}]}, {"text": "This is also due to the availability of tools and resources.", "labels": [], "entities": []}, {"text": "Developing systems able to perform sentiment analysis for tweets in anew language requires at least a corpus of tweets and a polarity lexicon, both of which, to the best of our knowledge, do not exist yet for Italian.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9265135228633881}]}, {"text": "This paper offers three main contributions in this respect.", "labels": [], "entities": []}, {"text": "First, we present the first of corpus of tweets for Italian, builtin such away that makes it possible to use the exact same strategy to build similar resources for other languages without any manual intervention (Section 2).", "labels": [], "entities": []}, {"text": "Second, we derive a polarity lexicon for Italian, organised by senses, also using a fully automatic strategy which can replicated to obtain such a resource for other languages (Section 3.1).", "labels": [], "entities": []}, {"text": "Third, we use the lexicon to automatically assign polarity to two subsets of the tweets in our corpus, and evaluate results against manually annotated data (Sections 3.2-3.4).", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran our system on both datasets described in Section 3.3, using all possible variations of two parameters, namely all combinations of part-of-speech tags and the application of the threshold scheme, as discussed in Section 3.2.", "labels": [], "entities": []}, {"text": "We measure overall accuracy as well as precision, recall, and f-score per polarity value.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9991735816001892}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9998263716697693}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9996801614761353}, {"text": "f-score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.991936445236206}]}, {"text": "In, we report best scores, and indicate in brackets the associated POS combination.", "labels": [], "entities": [{"text": "POS combination", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.9701568484306335}]}, {"text": "For instance, in, we can read that the recall of 0.701 for positive polarity is obtained when the system is run without polypathy threshold and using nouns, verbs, and adjectives (nva).", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.999495267868042}]}, {"text": "We can draw several observations from these results.", "labels": [], "entities": []}, {"text": "First, a fully automatic approach that leverages existing lexical resources performs better than a wild guess.", "labels": [], "entities": []}, {"text": "Performance is boosted when highly polypathic words are filtered out.", "labels": [], "entities": []}, {"text": "Second, while the system performs well at recognising especially neutral but also positive polarity, it is really bad at detecting negative polarity.", "labels": [], "entities": []}, {"text": "Especially in the topic-specific set, the system assigns  too many positive labels in place of negative ones, causing at the same time positive's precision and negative's recall to drop.", "labels": [], "entities": [{"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9671571850776672}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9889217615127563}]}, {"text": "We believe there are two explanations for this.", "labels": [], "entities": []}, {"text": "The first one is the \"positivebias\" of SentiWordNet, as observed by, which causes limited performance in the identification of negative polarity.", "labels": [], "entities": []}, {"text": "The second one is that we do not use any syntactic clues, such as for detecting negated statements.", "labels": [], "entities": []}, {"text": "Including some strategy for dealing with this should improve recognition of negative opinions, too.", "labels": [], "entities": [{"text": "recognition of negative opinions", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.8711822032928467}]}, {"text": "Third, the lower performance on the topic-specific dataset confirms the intuition that this task is harder, mainly because we operate a more subtle distinction when assigning a polarity label as we refer to one specific subject.", "labels": [], "entities": []}, {"text": "Deeper linguistic analysis, such as dependency parsing, might help, as only certain words would result as related to the intended target while others wouldn't.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7974278628826141}]}, {"text": "As far as parts of speech are concerned, there is a tendency for adverbs to be good indicators towards overall accuracy, and best scores are usually obtained exploiting adjectives and/or adverbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9896376132965088}]}], "tableCaptions": [{"text": " Table 1: Distribution of the tags assigned by the absolute  majority of the raters", "labels": [], "entities": []}]}