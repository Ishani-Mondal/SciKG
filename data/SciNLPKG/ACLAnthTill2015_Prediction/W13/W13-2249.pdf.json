{"title": [{"text": "DCU-Symantec at the WMT 2013 Quality Estimation Shared Task", "labels": [], "entities": [{"text": "DCU-Symantec", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.860575258731842}, {"text": "WMT 2013 Quality Estimation Shared Task", "start_pos": 20, "end_pos": 59, "type": "TASK", "confidence": 0.7286985566218694}]}], "abstractContent": [{"text": "We describe the two systems submitted by the DCU-Symantec team to Task 1.1. of the WMT 2013 Shared Task on Quality Estimation for Machine Translation.", "labels": [], "entities": [{"text": "WMT 2013 Shared Task on Quality Estimation for Machine Translation", "start_pos": 83, "end_pos": 149, "type": "TASK", "confidence": 0.5964103072881699}]}, {"text": "Task 1.1 involve estimating post-editing effort for English-Spanish translation pairs in the news domain.", "labels": [], "entities": []}, {"text": "The two systems use a wide variety of features , of which the most effective are the word-alignment, n-gram frequency, language model, POS-tag-based and pseudo-references ones.", "labels": [], "entities": []}, {"text": "Both systems perform at a similarly high level in the two tasks of scoring and ranking translations, although there is some evidence that the systems are over-fitting to the training data.", "labels": [], "entities": [{"text": "ranking translations", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.7175146341323853}]}], "introductionContent": [{"text": "The WMT 2013 Quality Estimation Shared Task involve both sentence-level and word-level quality estimation (QE).", "labels": [], "entities": [{"text": "WMT 2013 Quality Estimation Shared", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7165206670761108}, {"text": "word-level quality estimation (QE)", "start_pos": 76, "end_pos": 110, "type": "METRIC", "confidence": 0.7135031620661417}]}, {"text": "The sentence-level task consist of three subtasks: scoring and ranking translations with regard to post-editing effort (Task 1.1), selecting among several translations produced by multiple MT systems for the same source sentence (Task 1.2), and predicting post-editing time).", "labels": [], "entities": []}, {"text": "The DCU-Symantec team enter two systems to Task 1.1.", "labels": [], "entities": []}, {"text": "Given a set of source English news sentences and their Spanish translations, the goals are to predict the HTER score of each translation and to produce a ranking based on HTER for the set of translations.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9842576086521149}, {"text": "HTER", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9703627228736877}]}, {"text": "A set of 2,254 sentence pairs are provided for training.", "labels": [], "entities": []}, {"text": "On the ranking task, our system DCU-SYMC alltypes is second placed out of thirteen systems and our system DCU-SYMC combine is ranked fifth, according to the Delta Average metric.", "labels": [], "entities": [{"text": "Delta Average metric", "start_pos": 157, "end_pos": 177, "type": "METRIC", "confidence": 0.7818825642267863}]}, {"text": "According to the Spearman rank correlation, our systems are the joint-highest systems.", "labels": [], "entities": []}, {"text": "In the scoring task, the DCU-SYMC alltypes system is placed sixth out of seventeen systems according to Mean Absolute Error (MAE) and third according to Root Mean Squared Error (RMSE).", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 104, "end_pos": 129, "type": "METRIC", "confidence": 0.9466021557648977}, {"text": "Root Mean Squared Error (RMSE)", "start_pos": 153, "end_pos": 183, "type": "METRIC", "confidence": 0.7484040856361389}]}, {"text": "The DCU-SYMC combine system is placed fifth according to MAE and second according to RMSE.", "labels": [], "entities": [{"text": "MAE", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.6632421612739563}, {"text": "RMSE", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.5110517740249634}]}, {"text": "In this system description paper, we describe the features, the learning methods used, the results for the two submitted systems and some other systems we experiment with.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the results obtained with -SVR during 5-fold cross-validation on the training set and the final results obtained on the test set.", "labels": [], "entities": [{"text": "SVR", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9867466688156128}]}, {"text": "We selected two systems to submit amongst the different configurations based on MAE, RMSE and r.", "labels": [], "entities": [{"text": "MAE", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.7428897023200989}, {"text": "RMSE", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.5865687131881714}]}, {"text": "As several systems reach the same performance according to these metrics, we use the number of support vectors (noted SV) as an indicator of training data over-fitting.", "labels": [], "entities": [{"text": "SV", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.9684427976608276}]}, {"text": "We report the results obtained with some of our systems in.", "labels": [], "entities": []}, {"text": "The results show that the submitted systems DCU-SYMC alltypes and DCU-SYMC combine lead to the best scores on crossvalidation, but they do not outperform the system combining the 15 feature types without feature selection (15 types).", "labels": [], "entities": []}, {"text": "This system reaches the best scores on the test set compared to all our systems built on reduced feature sets.", "labels": [], "entities": []}, {"text": "This indicates that we over-fit and fail to generalise from the training data.", "labels": [], "entities": []}, {"text": "Amongst the systems built using reduced feature sets, the M5P-R M80 system, based on the tree binarisation approach using M5P-R, yields the best results on the test set on 3 out of 4 official metrics.", "labels": [], "entities": [{"text": "M5P-R M80", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8461422920227051}]}, {"text": "These results indicate that this system, trained on 16 features only, tends to estimate HTER scores more accurately on the unseen test data.", "labels": [], "entities": [{"text": "HTER scores", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.8363966345787048}]}, {"text": "The results of the two systems based on the M5P-R binarisation method are the best compared to all the other systems presented in this Section.", "labels": [], "entities": []}, {"text": "This feature binarisation and selection method leads to robust systems with few features: 31 and 16 for M5P-R M50 and M5P-R M80 respectively.", "labels": [], "entities": [{"text": "M5P-R M50", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.8110579550266266}]}, {"text": "Even though these systems do not lead to the best results, they outperform the two submitted systems on one metric used to evaluate the: Results obtained with different regression models, during cross-validation on the training set and on the test set, depending on the feature selection method.", "labels": [], "entities": []}, {"text": "Systems marked with were submitted for the shared task.", "labels": [], "entities": []}, {"text": "scoring task and two metrics to evaluate the ranking task.", "labels": [], "entities": []}, {"text": "On the systems built using reduced feature sets, we observe a difference of approximately 0.03pt absolute between the MAE and RMSE scores obtained during cross-validation and those on the test set.", "labels": [], "entities": [{"text": "MAE", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.960453987121582}, {"text": "RMSE", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.7134006023406982}]}, {"text": "Such a difference can be related to training data over-fitting, even though the feature sets obtained with the tree binarisation methods are small.", "labels": [], "entities": []}, {"text": "For instance, the system M5P M130 is trained on 4 features only, but the difference between cross-validation and test MAE scores is similar to the other systems.", "labels": [], "entities": [{"text": "M5P M130", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8070684969425201}, {"text": "MAE", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9305760264396667}]}, {"text": "We see on the final results that our feature selection methods is an over-fitting factor: by selecting the features which explain well the training set, the final model tends to generalise less.", "labels": [], "entities": []}, {"text": "The selected features are suited for the specificities of the training data, but are less accurate at predicting values on the unseen test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Features selected with the M5P-R M 50  binarisation approach. For each feature, the cor- responding rule indicates the binary feature value.  These features are included in the submitted sys- tem combine in addition to the features presented  in", "labels": [], "entities": [{"text": "M5P-R M 50  binarisation", "start_pos": 37, "end_pos": 61, "type": "DATASET", "confidence": 0.7804361581802368}]}, {"text": " Table 3: Results obtained with different regression models, during cross-validation on the training set  and on the test set, depending on the feature selection method. Systems marked with were submitted  for the shared task.", "labels": [], "entities": []}, {"text": " Table 4: Results obtained with the two feature sets contained in our submitted systems using M5P to  build the regression models instead of -SVR.", "labels": [], "entities": []}, {"text": " Table 5: Results obtained on WMT12 QE dataset with our best system (15 types) compared to WMT12  QE highest ranked team, in the Likert score prediction task.", "labels": [], "entities": [{"text": "WMT12 QE dataset", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.9382505615552267}, {"text": "WMT12  QE", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.8989421725273132}, {"text": "Likert score prediction task", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.6762823984026909}]}]}