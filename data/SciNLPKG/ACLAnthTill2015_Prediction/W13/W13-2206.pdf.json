{"title": [{"text": "Feature Decay Algorithms for Fast Deployment of Accurate Statistical Machine Translation Systems", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6541631122430166}]}], "abstractContent": [{"text": "We use feature decay algorithms (FDA) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction.", "labels": [], "entities": [{"text": "FDA", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9332088828086853}, {"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6122012635072073}]}, {"text": "We develop parallel FDA for solving computational scalability problems caused by the abundance of training data for SMT models and LM models and still achieve SMT performance that is on par with using all of the training data or better.", "labels": [], "entities": [{"text": "SMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9804829359054565}, {"text": "SMT", "start_pos": 159, "end_pos": 162, "type": "TASK", "confidence": 0.9886781573295593}]}, {"text": "Parallel FDA runs separate FDA models on randomized subsets of the training data and combines the instance selections later.", "labels": [], "entities": []}, {"text": "Parallel FDA can also be used for selecting the LM corpus based on the training set selected by parallel FDA.", "labels": [], "entities": []}, {"text": "The high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.9890326261520386}]}, {"text": "The relevancy of the selected LM corpus can reach up to 86% reduction in the number of OOV tokens and up to 74% reduction in the perplexity.", "labels": [], "entities": []}, {"text": "We perform SMT experiments in all language pairs in the WMT13 translation task and obtain SMT performance close to the top systems using significantly less resources for training and development.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.994769275188446}, {"text": "WMT13 translation task", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7833907008171082}, {"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9929534792900085}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) is a data intensive problem.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8469700515270233}]}, {"text": "If you have the translations for the source sentences you are translating in your training set or even portions of it, then the translation task becomes easier.", "labels": [], "entities": [{"text": "translation", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.9691716432571411}]}, {"text": "If some tokens are not found in your training data then you cannot translate them and if some translated word do not appear in your language model (LM) corpus, then it becomes harder for the SMT engine to find their correct position in the translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.988187849521637}]}, {"text": "Current SMT systems also face problems caused by the proliferation of various parallel corpora available for building SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9863994121551514}, {"text": "SMT", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.9746240973472595}]}, {"text": "The training data for many of the language pairs in the translation task, part of the Workshop on Machine translation (WMT13), have increased the size of the available parallel corpora for instance by web crawled corpora over the years.", "labels": [], "entities": [{"text": "translation task", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.9273587167263031}, {"text": "Machine translation (WMT13)", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.7770213723182678}]}, {"text": "The increased size of the training material creates computational scalability problems when training SMT models and can increase the amount of noisy parallel sentences found.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9799380898475647}]}, {"text": "As the training set sizes increase, proper training set selection becomes more important.", "labels": [], "entities": []}, {"text": "At the same time, when we are going to translate just a couple of thousand sentences, possibly belonging to the same target domain, it does not make sense to invest resources for training SMT models over tens of millions of sentences or even more.", "labels": [], "entities": [{"text": "SMT", "start_pos": 188, "end_pos": 191, "type": "TASK", "confidence": 0.9838058948516846}]}, {"text": "SMT models like Moses already have filtering mechanisms to create smaller parts of the built models that are relevant to the test set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9534804224967957}]}, {"text": "In this paper, we develop parallel feature decay algorithms (FDA) for solving computational scalability problems caused by the abundance of training data for SMT models and LM models and still achieve SMT performance that is on par with using all of the training data or better.", "labels": [], "entities": [{"text": "SMT", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.9791437983512878}, {"text": "SMT", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.989953875541687}]}, {"text": "Parallel FDA runs separate FDA models on randomized subsets of the training data and combines the instance selections later.", "labels": [], "entities": []}, {"text": "We perform SMT experiments in all language pairs of the WMT13) and obtain SMT performance close to the baseline Moses () system using less resources for training.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9951868653297424}, {"text": "WMT13", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9695098996162415}, {"text": "SMT", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9937392473220825}]}, {"text": "With parallel FDA, we can solve not only the instance selection problem for training data but also instance selection for the LM training corpus, which allows us to train higher order n-gram language models and model the dependencies better.", "labels": [], "entities": [{"text": "LM training corpus", "start_pos": 126, "end_pos": 144, "type": "DATASET", "confidence": 0.6335024833679199}]}, {"text": "Parallel FDA improves the scalability of FDA and allows rapid prototyping of SMT systems fora given target domain or task.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9758672118186951}]}, {"text": "Parallel FDA can be very useful for MT in target domains with limited resources or in disaster and crisis situations () where parallel corpora can be gathered by crawling and selected by parallel FDA.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9962353110313416}]}, {"text": "Parallel FDA also improves the computational requirements of FDA by selecting from smaller corpora and distributing the workload.", "labels": [], "entities": []}, {"text": "The high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.9890326261520386}]}, {"text": "The relevancy of the LM corpus selected can reach up to 86% reduction in the number of OOV tokens and up to 74% reduction in the perplexity.", "labels": [], "entities": []}, {"text": "We organize our work as follows.", "labels": [], "entities": []}, {"text": "We describe FDA and parallel FDA models in the next section.", "labels": [], "entities": []}, {"text": "We also describe how we extend the FDA model for LM corpus selection.", "labels": [], "entities": [{"text": "LM corpus selection", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8294925491015116}]}, {"text": "In section 3, we present our experimental results and in the last section, we summarize our contributions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with all language pairs in both directions in the WMT13 translation task, which include English-German (en-de), English-Spanish (en-es), English-French (en-fr), English-Czech (en-cs), and English-Russian (en-ru).", "labels": [], "entities": [{"text": "WMT13 translation task", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.809972325960795}]}, {"text": "We develop translation models using the phrase-based Moses ( SMT system.", "labels": [], "entities": []}, {"text": "We true-case all of the corpora, use 150-best lists during tuning, set the max-fertility of GIZA++ () to a value between 8-10, use 70 word classes learned over 3 iterations with mkcls tool during GIZA++ training, and vary the language model order between 5 to 9 for all language pairs.", "labels": [], "entities": [{"text": "max-fertility", "start_pos": 75, "end_pos": 88, "type": "METRIC", "confidence": 0.9736802577972412}]}, {"text": "The development set contains 3000 sentences randomly sampled from among all of the development sentences provided.", "labels": [], "entities": []}, {"text": "Since we do not know the best training set size that will maximize the performance, we rely on previous SMT experiments) to select the proper training set size.", "labels": [], "entities": [{"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.986700177192688}]}, {"text": "We choose close to 15 million words and its corresponding number of sentences for each training corpus and 10 million sentences for each LM corpus not including the selected training set, which is added later.", "labels": [], "entities": []}, {"text": "This corresponds to selecting roughly 15% of the training corpus for en-de and 35% for ru-en, and due to their larger size, 5% for en-es, 6% for cs-en, 2% for en-fr language pairs.", "labels": [], "entities": []}, {"text": "The size of the LM corpus allows us to build higher order models.", "labels": [], "entities": [{"text": "LM corpus", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.7275615781545639}]}, {"text": "The statistics of the training data selected by the parallel FDA is given in  After selecting the training set, we select the LM corpora using the words in the target side of the training set as the features.", "labels": [], "entities": []}, {"text": "For en, es, and fr, we have access to the LDC Gigaword corpora, from which we extract only the story type news and for en, we exclude the corpora from Xinhua News Agency (xin eng).", "labels": [], "entities": [{"text": "LDC Gigaword corpora", "start_pos": 42, "end_pos": 62, "type": "DATASET", "confidence": 0.9215651353200277}, {"text": "Xinhua News Agency", "start_pos": 151, "end_pos": 169, "type": "DATASET", "confidence": 0.9291503230730692}]}, {"text": "The size of the LM corpora from LDC and the monolingual LM corpora provided by WMT13 are given in.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.9318404197692871}]}, {"text": "For all target languages, we select 10M sentences with parallel FDA from the LM corpora and the remaining training sentences and add the selected training data to obtain the LM corpus.", "labels": [], "entities": [{"text": "LM corpus", "start_pos": 174, "end_pos": 183, "type": "DATASET", "confidence": 0.7654784917831421}]}, {"text": "Thus the size of the LM corpora is 10M plus the number of sentences in the training set as given in: Best BLEUc results obtained on the translation task together with the LM order used when obtaining the result compared with the best constrained Moses results in WMT12 and WMT13.", "labels": [], "entities": [{"text": "BLEUc", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9990401864051819}, {"text": "translation task", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.8918402194976807}, {"text": "WMT12", "start_pos": 263, "end_pos": 268, "type": "DATASET", "confidence": 0.8898484110832214}, {"text": "WMT13", "start_pos": 273, "end_pos": 278, "type": "DATASET", "confidence": 0.9050648212432861}]}, {"text": "The last row compares the BLEUc result with respect to using a different LM order.", "labels": [], "entities": [{"text": "BLEUc", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9989684820175171}]}, {"text": "age using about 5% of the available training data and 5% of the available LM corpus for instance for en.", "labels": [], "entities": [{"text": "LM corpus", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.7622804939746857}]}, {"text": "A smaller LM training corpus also allows us to train higher order n-gram language models and model the dependencies better and achieve lower perplexity as given in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Note that the training  set size for different translation directions differ  slightly since we run a parallel FDA for each.", "labels": [], "entities": []}, {"text": " Table 1: Comparison of the training data available  and the selected training set by parallel FDA for  each language pair. The size of the parallel cor- pora is given in millions (M) of words or thou- sands (K) of sentences.", "labels": [], "entities": []}, {"text": " Table 2. For  all target languages, we select 10M sentences with  parallel FDA from the LM corpora and the remain- ing training sentences and add the selected training  data to obtain the LM corpus. Thus the size of the  LM corpora is 10M plus the number of sentences  in the training set as given in", "labels": [], "entities": []}, {"text": " Table 2: The size of the LM corpora from LDC  and the monolingual language model corpora pro- vided in millions (M) of words.", "labels": [], "entities": []}, {"text": " Table 3: Best BLEUc results obtained on the translation task together with the LM order used when  obtaining the result compared with the best constrained Moses results in WMT12 and WMT13. The last  row compares the BLEUc result with respect to using a different LM order.", "labels": [], "entities": [{"text": "BLEUc", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9981191754341125}, {"text": "translation task", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.9189804494380951}, {"text": "WMT12", "start_pos": 173, "end_pos": 178, "type": "DATASET", "confidence": 0.929513156414032}, {"text": "WMT13", "start_pos": 183, "end_pos": 188, "type": "DATASET", "confidence": 0.9290646314620972}, {"text": "BLEUc", "start_pos": 217, "end_pos": 222, "type": "METRIC", "confidence": 0.9987220168113708}]}, {"text": " Table 4: Source (scov) and target (tcov) 2-gram feature coverage comparison of the training corpora  (train) with the training sets obtained with parallel FDA (FDA).", "labels": [], "entities": []}, {"text": " Table 5: Perplexity comparison of the LM built  from the training corpus (train), parallel FDA se- lected training corpus (FDA), and the parallel FDA  selected LM corpus (FDA LM).", "labels": [], "entities": [{"text": "FDA se- lected training corpus (FDA)", "start_pos": 92, "end_pos": 128, "type": "DATASET", "confidence": 0.6359234087997012}]}]}