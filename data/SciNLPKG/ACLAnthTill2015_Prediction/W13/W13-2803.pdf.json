{"title": [{"text": "Hybrid Selection of Language Model Training Data Using Linguistic Information and Perplexity", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore the selection of training data for language models using perplexity.", "labels": [], "entities": []}, {"text": "We introduce three novel models that make use of linguistic information and evaluate them on three different corpora and two languages.", "labels": [], "entities": []}, {"text": "In four out of the six scenarios a linguistically motivated method out-performs the purely statistical state-of-the-art approach.", "labels": [], "entities": []}, {"text": "Finally, a method which combines surface forms and the linguistically motivated methods outperforms the baseline in all the scenarios, selecting data whose perplexity is between 3.49% and 8.17% (depending on the corpus and language) lower than that of the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LMs) area fundamental piece in statistical applications that produce natural language text, such as machine translation and speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7807499766349792}, {"text": "speech recognition", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.7034971863031387}]}, {"text": "In order to perform optimally, a LM should be trained on data from the same domain as the data that it will be applied to.", "labels": [], "entities": []}, {"text": "This poses a problem, because in the majority of applications, the amount of domain-specific data is limited.", "labels": [], "entities": []}, {"text": "A popular strand of research in recent years to tackle this problem is that of training data selection.", "labels": [], "entities": [{"text": "training data selection", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.6715798775355021}]}, {"text": "Given a limited domain-specific corpus and a larger non-domain-specific corpus, the task consists on finding suitable data for the specific domain in the non-domain-specific corpus.", "labels": [], "entities": []}, {"text": "The underlying assumption is that a non-domain-specific corpus, if broad enough, contains sentences similar to a domain-specific corpus, which therefore, would be useful for training models for that domain.", "labels": [], "entities": []}, {"text": "This paper focuses on the approach that uses perplexity for the selection of training data.", "labels": [], "entities": []}, {"text": "The first works in this regard () use the perplexity according to a domain-specific LM to rank the text segments (e.g. sentences) of non-domain-specific corpora.", "labels": [], "entities": []}, {"text": "The text segments with perplexity less than a given threshold are selected.", "labels": [], "entities": []}, {"text": "A more recent method, which can be considered the state-of-the-art, is Moore-Lewis (.", "labels": [], "entities": []}, {"text": "It considers not only the crossentropy 1 according to the domain-specific LM but also the cross-entropy according to a LM built on a random subset (equal in size to the domainspecific corpus) of the non-domain-specific corpus.", "labels": [], "entities": []}, {"text": "The additional use of a LM from the nondomain-specific corpus allows to select a subset of the non-domain-specific corpus which is better (the perplexity of a test set of the specific domain has lower perplexity on a LM trained on this subset) and smaller compared to the previous approaches.", "labels": [], "entities": []}, {"text": "The experiment was carried out for English, using Europarl () as the domain-specific corpus and LDC Gigaword 2 as the non-domain-specific one.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9774552583694458}]}, {"text": "In this paper we study whether the use of two types of linguistic knowledge (lemmas and named entities) can contribute to obtain better results within the perplexity-based approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results, as previously seen in, differ with respect to the corpus but follow similar trends across languages.", "labels": [], "entities": []}, {"text": "For CC we obtain the best results using named entities.", "labels": [], "entities": []}, {"text": "The model ln obtains the best result for English (5.54% lower: Results for the different models perplexity than the baseline), while the model fn obtains the best result for Spanish (3.82%), although in both cases the difference between these two models is rather small.", "labels": [], "entities": []}, {"text": "For the other corpora, the best results are obtained without named entities.", "labels": [], "entities": []}, {"text": "In the case of EU, the baseline obtains the best result, although the model l is not very far (1.18% higher perplexity for English and 1.63% for Spanish).", "labels": [], "entities": []}, {"text": "This trend is reversed for UN, the model l obtaining the best scores but close to the baseline (-0.51%, -0.35%).", "labels": [], "entities": [{"text": "UN", "start_pos": 27, "end_pos": 29, "type": "DATASET", "confidence": 0.6051424145698547}]}, {"text": "shows the perplexities obtained by the method that combines the four models (column comb) for the threshold that yielded the best result in each scenario (see), compares these results (column diff) to those obtained by the baseline (column f) and shows the percentage of sentences that this method inspected from the sentences selected by the individual methods (column perc  The combination method outperforms the baseline and any of the individual linguistic models in all the scenarios.", "labels": [], "entities": []}, {"text": "The perplexity obtained by combining the models is substantially lower than that obtained by the baseline (ranging from 3.49% to 8.17%).", "labels": [], "entities": []}, {"text": "In all the scenarios, the combination method takes its sentences from roughly the top 70% sentences ranked by the individual methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of n-grams in LMs built using  the different models", "labels": [], "entities": []}, {"text": " Table 2: Results for the different models", "labels": [], "entities": []}, {"text": " Table 3: Results of the combination method", "labels": [], "entities": []}]}