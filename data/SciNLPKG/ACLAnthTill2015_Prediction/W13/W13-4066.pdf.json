{"title": [{"text": "Recipe For Building Robust Spoken Dialog State Trackers: Dialog State Tracking Challenge System Description", "labels": [], "entities": [{"text": "Building Robust Spoken Dialog State Trackers", "start_pos": 11, "end_pos": 55, "type": "TASK", "confidence": 0.5876829822858175}]}], "abstractContent": [{"text": "For robust spoken conversational interaction, many dialog state tracking algorithms have been developed.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7713180184364319}]}, {"text": "Few studies, however, have reported the strengths and weaknesses of each method.", "labels": [], "entities": []}, {"text": "The Dialog State Tracking Challenge (DSTC) is designed to address this issue by comparing various methods on the same domain.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge (DSTC)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8483511209487915}]}, {"text": "In this paper, we present a set of techniques that build a robust dialog state tracker with high performance: wide-coverage and well-calibrated data selection, feature-rich discriminative model design, generalization improvement techniques and unsupervised prior adaptation.", "labels": [], "entities": [{"text": "dialog state tracker", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.6804707447687784}]}, {"text": "The DSTC results show that the proposed method is superior to other systems on average on both the development and test datasets.", "labels": [], "entities": [{"text": "DSTC", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8923749923706055}]}], "introductionContent": [{"text": "Even though we have recently seen an explosive growth of interest in speech-enabled applications, there are still many problems to overcome in order to provide users with practical and profitable services.", "labels": [], "entities": []}, {"text": "One of the long-standing problems which may often frustrate users is Automatic Speech Recognition (ASR) error.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.7326785077651342}, {"text": "error", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.3974992036819458}]}, {"text": "Due to ASR error, it is barely possible to directly observe what the user said and finally figure out the true user goal.", "labels": [], "entities": [{"text": "ASR", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.7567081451416016}]}, {"text": "The aim of dialog state tracking is, therefore, to accurately estimate the true dialog state from erroneous observations as a dialog unfolds.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.8690356810887655}]}, {"text": "In order to achieve this goal, many dialog state tracking algorithms have been developed.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7978998819986979}]}, {"text": "Few studies, however, have reported the strengths and weaknesses of each method.", "labels": [], "entities": []}, {"text": "The Dialog State Tracking Challenge 1 (DSTC) was organized to advance state-of-the-art technologies for dialog state tracking by allowing for reliable comparisons between different approaches using the same datasets.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge 1 (DSTC)", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.7560862749814987}, {"text": "dialog state tracking", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.8432602485020956}]}, {"text": "Unlike other machine learning-based empirical tasks, DSTC is also carefully designed to take into consideration diverse realistic mismatches.", "labels": [], "entities": []}, {"text": "For instance, there are test datasets that were collected by systems using different speech recognizers, spoken language understanding (SLU) modules, and dialog managers.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 105, "end_pos": 140, "type": "TASK", "confidence": 0.7592983146508535}]}, {"text": "Also there are test datasets that were produced by similar systems but deployed at a different time (1 year later) with extended coverage.", "labels": [], "entities": []}, {"text": "Since such mismatches between training and test data may often happen in real deployment, it is important to build a tracker which constantly shows high performance across all test datasets despite various mismatches.", "labels": [], "entities": []}, {"text": "The aim of this paper is to describe a set of techniques used to build a robust tracker with high performance: wide-coverage and wellcalibrated data selection, feature-rich discriminative model design, generalization improvement techniques and unsupervised prior adaptation.", "labels": [], "entities": []}, {"text": "Our challenge systems are basically various combinations of those techniques.", "labels": [], "entities": []}, {"text": "The DSTC results demonstrate the effectiveness of each technique.", "labels": [], "entities": [{"text": "DSTC", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8771893978118896}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the challenge setup.", "labels": [], "entities": []}, {"text": "Section 3 elaborates on our proposed approaches.", "labels": [], "entities": []}, {"text": "Section 4 briefly describes previous research and other systems that participated in DSTC.", "labels": [], "entities": [{"text": "DSTC", "start_pos": 85, "end_pos": 89, "type": "TASK", "confidence": 0.7330384850502014}]}, {"text": "Section 5 presents and discusses the results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes with a brief summary and suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data is divided into 2 training sets and 4 test sets).", "labels": [], "entities": []}, {"text": "For standardized development sets, each training set is split in half.", "labels": [], "entities": []}, {"text": "Participants were asked to report results on the second half of each set.", "labels": [], "entities": []}, {"text": "The data from group A in train2, and test1 was collected using essentially the same dialog system.", "labels": [], "entities": []}, {"text": "Only a few updates were made to reflect changes to the bus schedule.", "labels": [], "entities": []}, {"text": "The data in test2 was collected using a different version of group A's dialog manager.", "labels": [], "entities": []}, {"text": "The data from group B in train3 and test3 were collected using essentially the same dialog system; the main difference is that test3 covers more bus routes.", "labels": [], "entities": []}, {"text": "Test4 tests the condition when training and testing using totally 2 http://research.microsoft.com/apps/pubs/?id=169024 different dialog systems, and when there is no same-system training data available.", "labels": [], "entities": []}], "tableCaptions": []}