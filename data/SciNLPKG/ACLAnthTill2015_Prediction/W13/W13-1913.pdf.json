{"title": [{"text": "Using the argumentative structure of scientific literature to improve information access", "labels": [], "entities": []}], "abstractContent": [{"text": "MEDLINE/PubMed contains structured abstracts that can provide argumentative labels.", "labels": [], "entities": [{"text": "MEDLINE/PubMed", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.901284396648407}]}, {"text": "Selection of abstract sentences based on the argumentative label has shown to improve the performance of information retrieval tasks.", "labels": [], "entities": [{"text": "information retrieval tasks", "start_pos": 105, "end_pos": 132, "type": "TASK", "confidence": 0.7922243277231852}]}, {"text": "These abstracts makeup less than one quarter of all the abstracts in MEDLINE/PubMed, so it is worthwhile to learn how to automatically label the non-structured ones.", "labels": [], "entities": [{"text": "MEDLINE/PubMed", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.8900976578394572}]}, {"text": "We have compared several machine learning algorithms trained on structured abstracts to identify argumentative labels.", "labels": [], "entities": []}, {"text": "We have performed an intrinsic evaluation on predicting argumentative labels for non-structured abstracts and an extrinsic evaluation to predict argumentative labels on abstracts relevant to Gene Reference Into Function (GeneRIF) indexing.", "labels": [], "entities": [{"text": "Gene Reference Into Function (GeneRIF) indexing", "start_pos": 191, "end_pos": 238, "type": "TASK", "confidence": 0.493939820677042}]}, {"text": "Intrinsic evaluation shows that argumentative labels can be assigned effectively to structured abstracts.", "labels": [], "entities": []}, {"text": "Algorithms that model the argumentative structure seem to perform better than other algorithms.", "labels": [], "entities": []}, {"text": "Extrinsic results show that assigning argumentative labels to non-structured abstracts improves the performance on GeneRIF indexing.", "labels": [], "entities": [{"text": "GeneRIF indexing", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.6260855495929718}]}, {"text": "On the other hand, the algorithms that model the argumentative structure of the abstracts obtain lower performance in the extrinsic evaluation.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The results based on the paragraphs outperform the ones based on the sentences.", "labels": [], "entities": []}, {"text": "Argumentative structure of the paragraphs seems to be easier, probably due to the fact that individual sentences have been shown to be noisy (, and this could explain this behaviour.", "labels": [], "entities": [{"text": "Argumentative structure of the paragraphs", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8456469535827636}]}, {"text": "Extrinsic evaluation is performed on the GeneRIF data set presented in the Methods section.", "labels": [], "entities": [{"text": "GeneRIF data set", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.9779777924219767}]}, {"text": "The idea of the evaluation is to assign one of the argumentative labels to the sentences, based on the models trained on structured abstracts, and evaluate the impact of this assignment in the selection of GeneRIF sentences.", "labels": [], "entities": []}, {"text": "From the set of machine learning algorithms intrinsically evaluated, we have selected the LR models trained with and without position information (Pos) and the CRF model.", "labels": [], "entities": [{"text": "position information (Pos)", "start_pos": 125, "end_pos": 151, "type": "METRIC", "confidence": 0.7356744766235351}]}, {"text": "The LR and CRF models are used to label the GeneRIF training and testing data with the argumentative labels.", "labels": [], "entities": [{"text": "GeneRIF training and testing data", "start_pos": 44, "end_pos": 77, "type": "DATASET", "confidence": 0.7928172588348389}]}, {"text": "shows the results of the extrinsic evaluation.", "labels": [], "entities": []}, {"text": "Results obtained with the argumentative label feature and with or without the set of features used in the baseline are compared to the baseline model, i.e. NB and the set of features presented in the Methods section.", "labels": [], "entities": []}, {"text": "In all the cases, precision (P), recall (R) and F 1 using the argumentative features improve over the baseline.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 18, "end_pos": 31, "type": "METRIC", "confidence": 0.9330622255802155}, {"text": "recall (R)", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9452649801969528}, {"text": "F 1", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9930786788463593}]}, {"text": "The intrinsic evaluation was performed either on sentences or paragraphs.", "labels": [], "entities": []}, {"text": "The sentence models perform better than the paragraph based models.", "labels": [], "entities": []}, {"text": "We find as well that LR with sentence position performs slightly better than when combined with the baseline features, with higher recall but lower precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9991443157196045}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9972742199897766}]}, {"text": "Contrary to the intrinsic results, LR performs better than CRF, even though both outperform the baseline.", "labels": [], "entities": [{"text": "LR", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9173485040664673}, {"text": "CRF", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.7562019228935242}]}, {"text": "This means that non-structured sentences do not necessarily follow the same argumentative structure as the structured abstracts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. The  distribution of labels shows that some labels like  CONCLUSIONS, METHODS and RESULTS are  very frequent. CONCLUSIONS and METHODS  are assigned to more than one paragraph since the  number is bigger compared to the number of cita- tions in each set. This seems to happen when more  than one journal label in the same citation map  to METHODS or CONCLUSION, e.g. PMID:  23538919.", "labels": [], "entities": [{"text": "METHODS", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.6334165930747986}, {"text": "RESULTS", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9812264442443848}, {"text": "METHODS", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.5233119130134583}, {"text": "METHODS", "start_pos": 348, "end_pos": 355, "type": "DATASET", "confidence": 0.920195996761322}, {"text": "PMID:  23538919", "start_pos": 376, "end_pos": 391, "type": "DATASET", "confidence": 0.8064686059951782}]}, {"text": " Table 2: Structured abstracts data set statistics", "labels": [], "entities": []}, {"text": " Table 4: Intrinsic evaluation of paragraph based labeling", "labels": [], "entities": [{"text": "labeling", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.7803062200546265}]}, {"text": " Table 6: GeneRIF extrinsic evaluation", "labels": [], "entities": [{"text": "GeneRIF", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.4865654408931732}]}]}