{"title": [{"text": "Corpus development for machine translation between standard and dialectal varieties", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7658772766590118}]}], "abstractContent": [{"text": "In this paper we describe the construction of a parallel corpus between the standard and a non-standard language variety, specifically standard Austrian German and Viennese dialect.", "labels": [], "entities": []}, {"text": "The resulting parallel corpus is used for statistical machine translation (SMT) from the standard to the non-standard variety.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.8349357148011526}]}, {"text": "The main challenges to our task are data scarcity and the lack of an authoritative orthogra-phy.", "labels": [], "entities": []}, {"text": "We started with the generation of abase corpus of manually transcribed and translated data from spoken text encoded in a specifically developed or-thography.", "labels": [], "entities": []}, {"text": "This data is used to train a first phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8597890138626099}]}, {"text": "To deal with out-of-vocabulary items we exploit the strong proximity between source and target variety with a backoff strategy that uses character-level models.", "labels": [], "entities": []}, {"text": "To arrive at the necessary size fora corpus to be used for SMT, we employ a boot-strapping approach.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9961467981338501}]}, {"text": "Integrating additional available sources (comparable corpora, such as Wikipedia) necessitates to identify parallel sentences out of substantially differing parallel documents.", "labels": [], "entities": []}, {"text": "As an additional task, the spelling of the texts has to be transformed into the above mentioned orthography of the target variety.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation between dialectal varieties and their cognate standard variety is a challenge quite different from translation between major languages with large resources on both sides.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6568210820357004}]}, {"text": "Instead of having huge corpora at hand that offer themselves for machine learning techniques, substantial written corpora of dialectal language varieties are rare.", "labels": [], "entities": []}, {"text": "In addition, there is no authoritative orthography, which calls for methods to normalize the spelling of existing written texts.", "labels": [], "entities": [{"text": "normalize the spelling of existing written texts", "start_pos": 79, "end_pos": 127, "type": "TASK", "confidence": 0.8025592991283962}]}, {"text": "Parallel resources fora standard language and a dialectal variety thereof are even less common.", "labels": [], "entities": []}, {"text": "But such parallel data is the workhorse of modern machine translation systems and key to producing sufficiently natural utterances.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7297481894493103}]}, {"text": "On the positive side, the relative proximity between a standard language and its varieties opens up new possibilities to gather parallel data, despite data sparsity.", "labels": [], "entities": []}, {"text": "In this paper we will outline methods to acquire such data, developed fora specific pair of varieties, Austrian German (AG), the standard variety, and a dialectal variety spoken in the capital, Viennese dialect (VD),).", "labels": [], "entities": []}, {"text": "From a linguistic perspective, it has to be noted that dialects generally are not really homogenous.", "labels": [], "entities": []}, {"text": "Lacking standardization initiatives, reinforcement by education or public media and predominantly being confined to oral usage, dialects most often form a dynamic continuum between different varieties and speaker groups.", "labels": [], "entities": []}, {"text": "Being defined by social group rather than geographical regions, the Viennese variety is a sociolect in the strict sense, where dialects in urban regions are generally associated with lower social classes).", "labels": [], "entities": []}, {"text": "Also, speakers with native competence usually adapt the register to the communicative situation as well as to the content of the utterances in a very dynamic way.", "labels": [], "entities": []}, {"text": "Switching between varieties and subtle gradual shifts area very natural phenomenon in such a linguistic situation.", "labels": [], "entities": []}, {"text": "While being aware that the linguistic conception of a dialect is not uncontroversial, we still think that it is feasible and appropriate to model a dialectal variety that conforms to a stereotype of that dialect.", "labels": [], "entities": []}, {"text": "The paper focuses on the generation of the resources necessary for statistical machine translation between a standard variety with rich resources (AG) and a dialectal variety (VD) with almost no resources.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.7171119848887125}]}, {"text": "The strategy is to create a minimal base corpus comprising bilingual data in a standardized orthography for VD, and in a second step applying a bootstrapping strategy in order to gain a suf- The work presented in this paper is based on the project 'Machine Learning Techniques for Modeling of Language Varieties' (MLT4MLV -ICT10-049) funded by the Vienna Science and Technology Fund (WWTF).", "labels": [], "entities": [{"text": "Modeling of Language Varieties' (MLT4MLV -ICT10-049)", "start_pos": 281, "end_pos": 333, "type": "TASK", "confidence": 0.7683887084325155}, {"text": "Vienna Science and Technology Fund (WWTF)", "start_pos": 348, "end_pos": 389, "type": "DATASET", "confidence": 0.9245532974600792}]}, {"text": "ficient amount of bilingual lexical resources and to increase the data on the basis of automatically generated translations.", "labels": [], "entities": []}, {"text": "As proximity between the varieties works on our side, we give detailed descriptions of how the linguistic closeness can be exploited to bootstrap the required resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report on some experiments using the data set described in the previous section to build statistical machine translation systems, using Moses ( ).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.6297694245974222}]}], "tableCaptions": [{"text": " Table 1: Corpus sizes (untokenised)", "labels": [], "entities": [{"text": "untokenised", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9274595975875854}]}, {"text": " Table 2: Comparison of accuracy of character- level models on the OOVs in DEVTEST. The plain  unigram/bigram models are trained on complete  sentences, whereas the cognate models are trained  on cognate pairs (unique or frequency weighted)  extracted from these sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9993754029273987}, {"text": "DEVTEST", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.860914945602417}]}, {"text": " Table 2. We can see that, in  general, the cognate models offer small improve- ments on the models trained on the whole sen- tences, and the unigram models are slightly better  than the bigram models.", "labels": [], "entities": [{"text": "improve- ments", "start_pos": 71, "end_pos": 85, "type": "METRIC", "confidence": 0.8523297309875488}]}, {"text": " Table 3. The backoff systems  are implemented by first examining the tuning and  test data for OOVs, then translating these using  the character-level model, and creating a second  phrase-table with the character-level model. This  second phrase table is used in Moses as a backoff  table.  For both test sets, the character-level transla- tion outperforms the word-level translation, but  the backoff offers the best performance of all. The  BLEU scores are relatively high compared to the", "labels": [], "entities": [{"text": "BLEU", "start_pos": 444, "end_pos": 448, "type": "METRIC", "confidence": 0.9995306730270386}]}]}