{"title": [{"text": "A Structured Distributional Semantic Model : Integrating Structure with Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a novel approach (SDSM) that incorporates structure in dis-tributional semantics.", "labels": [], "entities": []}, {"text": "SDSM represents meaning as relation specific distributions over syntactic neighborhoods.", "labels": [], "entities": []}, {"text": "We empirically show that the model can effectively represent the semantics of single words and provides significant advantages when dealing with phrasal units that involve word composition.", "labels": [], "entities": []}, {"text": "In particular, we demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches for composing distributional semantic representations on an artificial task of verb sense disambiguation and a real-world application of judging event coreference.", "labels": [], "entities": [{"text": "verb sense disambiguation", "start_pos": 216, "end_pos": 241, "type": "TASK", "confidence": 0.6698486606280009}, {"text": "judging event coreference", "start_pos": 274, "end_pos": 299, "type": "TASK", "confidence": 0.7915128469467163}]}], "introductionContent": [{"text": "With the advent of statistical methods for NLP, Distributional Semantic Models (DSMs) have emerged as powerful method for representing word semantics.", "labels": [], "entities": [{"text": "representing word semantics", "start_pos": 122, "end_pos": 149, "type": "TASK", "confidence": 0.6535833477973938}]}, {"text": "In particular, the distributional vector formalism, which represents meaning by a distribution over neighboring words, has gained the most popularity.", "labels": [], "entities": []}, {"text": "DSMs are widely used in information retrieval (), question answering (, semantic similarity computation (, automated dictionary building, automated essay grading, word-sense discrimination and disambiguation; * *Equally contributing authors), selectional preference modeling and identification of translation equivalents.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.8000838160514832}, {"text": "question answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9352304339408875}, {"text": "semantic similarity computation", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.7237038811047872}, {"text": "dictionary building", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.6982627660036087}, {"text": "word-sense discrimination", "start_pos": 163, "end_pos": 188, "type": "TASK", "confidence": 0.7699973583221436}, {"text": "selectional preference modeling", "start_pos": 243, "end_pos": 274, "type": "TASK", "confidence": 0.8025797804196676}, {"text": "identification of translation equivalents", "start_pos": 279, "end_pos": 320, "type": "TASK", "confidence": 0.8544857352972031}]}, {"text": "Systems that use DSMs implicitly make a bag of words assumption: that the meaning of a phrase can be reasonably estimated from the meaning of its constituents.", "labels": [], "entities": []}, {"text": "However, semantics in natural language is a compositional phenomenon, encompassing interactions between syntactic structures, and the meaning of lexical constituents.", "labels": [], "entities": []}, {"text": "It follows that the DSM formalism lends itself poorly to composition since it implicitly disregards syntactic structure.", "labels": [], "entities": []}, {"text": "For instance, the distributions for \"Lincoln\", \"Booth\", and \"killed\" when merged produce the same result regardless of whether the input is \"Booth killed Lincoln\" or \"Lincoln killed Booth\".", "labels": [], "entities": []}, {"text": "As suggested by and others, modeling the distribution over preferential attachments for each syntactic relation separately can yield greater expressive power.", "labels": [], "entities": []}, {"text": "Attempts have been made to model linguistic composition of individual word vectors, as well as remedy the inherent failings of the standard distributional approach.", "labels": [], "entities": []}, {"text": "The results show varying degrees of efficacy, but have largely failed to model deeper lexical semantics or compositional expectations of words and word combinations.", "labels": [], "entities": []}, {"text": "In this paper we propose an extension to the traditional DSM model that explicitly preserves structural information and permits the approximation of distributional expectation over dependency relations.", "labels": [], "entities": []}, {"text": "We extend the generic DSM model by representing a word as distributions over relationspecific syntactic neighborhoods.", "labels": [], "entities": []}, {"text": "One can think of the Structured DSM (SDSM) representation of a word/phrase as several vectors defined over the same vocabulary, each vector representing the word's selectional preferences fora different syntactic argument.", "labels": [], "entities": [{"text": "Structured DSM (SDSM) representation of a word/phrase", "start_pos": 21, "end_pos": 74, "type": "TASK", "confidence": 0.7633166909217834}]}, {"text": "We argue that this representation captures individual word semantics effectively, and is better able to express the semantics of composed units.", "labels": [], "entities": []}, {"text": "The overarching theme of our framework of evaluation is to explore the semantic space of the SDSM.", "labels": [], "entities": []}, {"text": "We do this by measuring its ability to discriminate between varying surface forms of the same underlying concept.", "labels": [], "entities": []}, {"text": "We perform the following set of experiments to evaluate its expressive power, and conclude the following: 1.", "labels": [], "entities": []}, {"text": "Experiments with single words on similarity scoring and substitute selection: SDSM performs at par with window-based distributional vectors.", "labels": [], "entities": [{"text": "similarity scoring", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.6680269837379456}, {"text": "substitute selection", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7153168767690659}, {"text": "SDSM", "start_pos": 78, "end_pos": 82, "type": "TASK", "confidence": 0.9572254419326782}]}, {"text": "2. Experiments with phrasal units on two-word composition: state-of-the-art results are produced on the dataset from in terms of correlation with human judgment.", "labels": [], "entities": []}, {"text": "3. Experiments with larger structures on the task of judging event coreferentiality: SDSM shows superior performance over state-ofthe-art window-based word embeddings, and simple models for composing distributional semantic representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe experiments and results for judging the expressive power of the structured distributional representation for individual words.", "labels": [], "entities": []}, {"text": "We use a similarity scoring task and a lexical substitute selection task for the purpose of this evaluation.", "labels": [], "entities": []}, {"text": "We compare the SDSM representation to standard window-based distributional vectors trained on the same corpus (Simple English Wikipedia).", "labels": [], "entities": [{"text": "Simple English Wikipedia)", "start_pos": 111, "end_pos": 136, "type": "DATASET", "confidence": 0.9270908087491989}]}, {"text": "We also experiment with different normalization techniques outlined in Section 3.2, which effectively lead to structured distributional representations with distinct interpretations.", "labels": [], "entities": []}, {"text": "We experimented with various similarity metrics and found that the normalized cityblock distance metric provides the most stable results.", "labels": [], "entities": []}, {"text": "Results in the rest of this section are thus reported using the normalized cityblock metric.", "labels": [], "entities": []}, {"text": "We also report experimental results for the two methods of alleviating sparsity discussed in Section 3.4, namely, densification and SVD.", "labels": [], "entities": []}, {"text": "The M&L dataset consists of polysemous intransitive verb and subject pairs that co-occur at least 50 times in the BNC corpus.", "labels": [], "entities": [{"text": "M&L dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.621232882142067}, {"text": "BNC corpus", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.9422444701194763}]}, {"text": "Additionally two landmark words are given for every polysemous verb, each corresponding to one of its senses.", "labels": [], "entities": []}, {"text": "The subject nouns provide contextual disambiguation for the senses of the verb.", "labels": [], "entities": []}, {"text": "For each tuple, a human assigned score on a 7-point scale is provided, indicating the compatibility of the landmark with the reference verb-subj pair.", "labels": [], "entities": []}, {"text": "For example, for the pair \"gun bomb\", landmark \"thunder\" is more similar to the verb than landmark \"prosper\".", "labels": [], "entities": []}, {"text": "The corpus contains 120 tuples and altogether 3600 human judgments.", "labels": [], "entities": []}, {"text": "Reliability of the human ratings is examined by calculating inter-annotator Spearman's \u03c1 correlation coefficient.", "labels": [], "entities": [{"text": "inter-annotator Spearman's \u03c1 correlation coefficient", "start_pos": 60, "end_pos": 112, "type": "METRIC", "confidence": 0.6668660044670105}]}, {"text": "For each tuple in the dataset, we derive the composed word-pair matrix for the reference verb-subj pair based on the algorithm described in Section 3.3 and query the single-word matrix for the landmark word.", "labels": [], "entities": []}, {"text": "A few modifications are made to adjust the algorithm for the current task: 1.", "labels": [], "entities": []}, {"text": "In our formulation, the dependency relation needs to be specified in order to compose a pair of words.", "labels": [], "entities": []}, {"text": "Hence, we determine the five most frequent relations between w 1 and w 2 by querying the PropStore.", "labels": [], "entities": [{"text": "PropStore", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.9595388174057007}]}, {"text": "We then use the algorithm in Section 3.3 to compose the verb-subj word pair using these relations, resulting in five composed representations.", "labels": [], "entities": []}, {"text": "2. The word pairs in M&L corpus are extracted from a parsed version of the BNC corpus, while our PropStore is built on Simple Wikipedia texts, whose vocabulary is significantly different from that of the BNC corpus.", "labels": [], "entities": [{"text": "M&L corpus", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.7724465280771255}, {"text": "BNC corpus", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9620579779148102}, {"text": "PropStore", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.9330776929855347}, {"text": "BNC corpus", "start_pos": 204, "end_pos": 214, "type": "DATASET", "confidence": 0.9268077611923218}]}, {"text": "This causes null returns in our PropStore queries, in which case we back-off to retrieving results for super-sense tags of both the words.", "labels": [], "entities": []}, {"text": "Finally, the composed matrix and the landmark matrix are compared against each other by different matrix distance measures, which results in a similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 143, "end_pos": 159, "type": "METRIC", "confidence": 0.9720827043056488}]}, {"text": "For a [subject, verb, landmark] tuple, we average the similarity scores yielded by the relations obtained in 1.", "labels": [], "entities": [{"text": "similarity", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9868440628051758}]}, {"text": "The Spearman Correlation \u03c1 between our similarity ratings and the ones assigned by human judges is computed overall the tuples.", "labels": [], "entities": [{"text": "Spearman Correlation \u03c1", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.7005385458469391}]}, {"text": "Following M&L's experiments, the inter-annotator agreement correlation coefficient serves an upper bound on the task.", "labels": [], "entities": [{"text": "inter-annotator agreement correlation coefficient", "start_pos": 33, "end_pos": 82, "type": "METRIC", "confidence": 0.6102383136749268}]}, {"text": "We evaluate our method on two datasets and compare it against four baselines, two of which use window based distributional vectors and two that employ weaker forms of composition.", "labels": [], "entities": []}, {"text": "IC Event Coreference Corpus: The dataset (citation suppressed), drawn from 100 news articles about violent events, contains manually created annotations for 2214 pairs of co-referent and non-coreferent events each.", "labels": [], "entities": [{"text": "IC Event Coreference Corpus", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.8053683638572693}]}, {"text": "Where available, events' semantic role-fillers for agent and patient are annotated as well.", "labels": [], "entities": []}, {"text": "When missing, empirical substitutes were obtained by querying the PropStore for the preferred word attachments.", "labels": [], "entities": [{"text": "PropStore", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.9371545314788818}]}], "tableCaptions": [{"text": " Table 1: Single Word Evaluation", "labels": [], "entities": [{"text": "Single Word Evaluation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7352041999499003}]}, {"text": " Table 2: Finklestein: Correlation using SVD", "labels": [], "entities": [{"text": "SVD", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.40275898575782776}]}, {"text": " Table 3: Two Word Composition Evaluation", "labels": [], "entities": [{"text": "Two Word Composition", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6574396292368571}]}, {"text": " Table 4: Cross-validation Performance on IC and ECB dataset", "labels": [], "entities": [{"text": "ECB dataset", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.7798534333705902}]}]}