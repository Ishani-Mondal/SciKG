{"title": [{"text": "Using a Keyness Metric for Single and Multi Document Summarisation", "labels": [], "entities": [{"text": "Single and Multi Document Summarisation", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.6811112761497498}]}], "abstractContent": [{"text": "In this paper we show the results of our participation in the MultiLing 2013 summarisation tasks.", "labels": [], "entities": [{"text": "MultiLing 2013 summarisation tasks", "start_pos": 62, "end_pos": 96, "type": "DATASET", "confidence": 0.7177637964487076}]}, {"text": "We participated with single-document and multi-document corpus-based summarisers for both Ara-bic and English languages.", "labels": [], "entities": []}, {"text": "The sum-marisers used word frequency lists and log likelihood calculations to generate single and multi document summaries.", "labels": [], "entities": []}, {"text": "The single and multi summaries generated by our systems were evaluated by Arabic and English native speaker participants and by different automatic evaluation met-rics, ROUGE, AutoSummENG, MeMoG and NPowER.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 169, "end_pos": 174, "type": "METRIC", "confidence": 0.9638417959213257}, {"text": "NPowER", "start_pos": 199, "end_pos": 205, "type": "DATASET", "confidence": 0.906385600566864}]}, {"text": "We compare our results to other systems that participated in the same tracks on both Arabic and English languages.", "labels": [], "entities": []}, {"text": "Our single-document summaris-ers performed particularly well in the automatic evaluation with our English single-document summariser performing better on average than the results of the other participants.", "labels": [], "entities": []}, {"text": "Our Arabic multi-document summariser performed well in the human evaluation ranking second.", "labels": [], "entities": []}], "introductionContent": [{"text": "Systems that can automatically summarise documents are becoming evermore desirable with the increasing volume of information available on the Web.", "labels": [], "entities": [{"text": "summarise documents", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.877911776304245}]}, {"text": "Automatic text summarisation is the process of producing a shortened version of a text by the use of computers.", "labels": [], "entities": [{"text": "Automatic text summarisation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5923001567522684}]}, {"text": "For example, reducing a text document or a group of related documents into a shorter version of sentences or paragraphs using automated tools and techniques.", "labels": [], "entities": [{"text": "reducing a text document or a group of related documents", "start_pos": 13, "end_pos": 69, "type": "TASK", "confidence": 0.7129103541374207}]}, {"text": "The summary should convey the key contributions of the text.", "labels": [], "entities": []}, {"text": "In other words, only key sentences should appear in the summary and the process of defining those sentences is highly dependent on the summarisation method used.", "labels": [], "entities": []}, {"text": "In automatic summarisation there are two main approaches that are broadly used, extractive and abstractive.", "labels": [], "entities": [{"text": "automatic summarisation", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.6513248383998871}]}, {"text": "The first method, the extractive summarisation, extracts, up to a certain limit, the key sentences or paragraphs from the text and orders them in away that will produce a coherent summary.", "labels": [], "entities": [{"text": "extractive summarisation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.5674924850463867}]}, {"text": "The extracted units differ from one summariser to another.", "labels": [], "entities": []}, {"text": "Most summarisers use sentences rather than larger units such as paragraphs.", "labels": [], "entities": [{"text": "summarisers", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.9559927582740784}]}, {"text": "Extractive summarisation methods are the focus method on automatic text summarisation.", "labels": [], "entities": [{"text": "Extractive summarisation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8410634696483612}, {"text": "automatic text summarisation", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.6128382881482443}]}, {"text": "The other method, abstractive summarisation, involves more language dependent tools and Natural Language Generation (NLG) technology.", "labels": [], "entities": [{"text": "abstractive summarisation", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.7024523317813873}, {"text": "Natural Language Generation (NLG)", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.8073586324850718}]}, {"text": "In our work we used extractive single and multidocument Arabic and English summarisers.", "labels": [], "entities": []}, {"text": "A successful summarisation approach needs a good guide to find the most important sentences that are relevant to a certain criterion.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9897482395172119}]}, {"text": "Therefore, the proposed methods should work on extracting the most important sentences from a set of related articles.", "labels": [], "entities": []}, {"text": "In this paper we present the results of our participation to the MultiLing 2013 summarisation tasks.", "labels": [], "entities": [{"text": "MultiLing 2013 summarisation tasks", "start_pos": 65, "end_pos": 99, "type": "DATASET", "confidence": 0.7331702262163162}]}, {"text": "MultiLing 2013 was built upon the Text Analysis Conference (TAC) MultiLing Pilot task of ).", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.8405432105064392}]}, {"text": "MultiLing 2013 this year asked for participants to run their summarisers on different languages having a corpus and gold standard summaries in the same seven languages (Arabic, Czech, English, French, Greek, Hebrew or Hindi) of TAC 2011 with a 50% increase to the corpora size.", "labels": [], "entities": [{"text": "TAC 2011", "start_pos": 228, "end_pos": 236, "type": "DATASET", "confidence": 0.8446255028247833}]}, {"text": "It also introduced three new languages (Chinese, Romanian and Spanish).", "labels": [], "entities": []}, {"text": "MultiLing 2013 this year introduced anew single-document summarisation pilot for 40 languages including the above mentioned languages (in our case.", "labels": [], "entities": []}, {"text": "In this paper we introduce the results of our single-document and multi-document summarisers at the MultiLing 2013 summarisation tasks.", "labels": [], "entities": [{"text": "MultiLing 2013 summarisation tasks", "start_pos": 100, "end_pos": 134, "type": "TASK", "confidence": 0.6570305824279785}]}, {"text": "We used a language independent corpus-based word frequency technique and the log-likelihood statistic to extract sentences with the maximum sum of log likelihood.", "labels": [], "entities": []}, {"text": "The output summary is expected to be no more than 250 words.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating the quality and consistency of a generated summary has proven to be a difficult problem (.", "labels": [], "entities": [{"text": "consistency", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9692036509513855}]}, {"text": "This is mainly because there is no obvious ideal, objective summary.", "labels": [], "entities": []}, {"text": "Two classes of metrics have been developed: form metrics and content metrics.", "labels": [], "entities": []}, {"text": "Form metrics focus on grammaticality, overall text coherence, and organisation.", "labels": [], "entities": []}, {"text": "They are usually measured on a point scale (.", "labels": [], "entities": []}, {"text": "Content metrics are more difficult to measure.", "labels": [], "entities": []}, {"text": "Typically, system output is compared sentence by sentence or unit by unit to one or more human-generated ideal summaries.", "labels": [], "entities": []}, {"text": "As with information retrieval, the percentage of information presented in the system's summary (precision) and the percentage of important information omitted from the summary (recall) can be assessed.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.7485322952270508}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9953877329826355}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9963998794555664}]}, {"text": "There are various models for system evaluation that may help in solving this problem.", "labels": [], "entities": []}, {"text": "This include automatic evaluations (e.g. ROUGE and AutoSummENG), and humanperformed evaluations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.8738783001899719}]}, {"text": "For the MultiLing 2013 task, the summaries generated by the participants were evaluated automatically based on humangenerated model summaries provided by fluent speakers of each corresponding language (native speakers in the general case).", "labels": [], "entities": [{"text": "MultiLing 2013 task", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.6376197338104248}]}, {"text": "The models used were, ROUGE variations (ROUGE1, ROUGE2, ROUGE-SU4)), the MeMoG variation () of AutoSummENG ( and NPowER (.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9282713532447815}, {"text": "AutoSummENG", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.946851372718811}, {"text": "NPowER", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.8076725006103516}]}, {"text": "ROUGE was not used to evaluate the single-document summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9728028178215027}]}, {"text": "The summaries were also evaluated manually by human participants.", "labels": [], "entities": []}, {"text": "For the manual evaluation the human evaluators were provided with the following guidelines: Each summary is to be assigned an integer grade from 1 to 5, related to the overall responsiveness of the summary.", "labels": [], "entities": []}, {"text": "We consider a text to be worth a 5, if it appears to coverall the important aspects of the corresponding document set using fluent, readable language.", "labels": [], "entities": []}, {"text": "A text should be assigned a 1, if it is either unreadable, nonsensical, or contains only trivial information from the document set.", "labels": [], "entities": []}, {"text": "We consider the content and the quality of the language to be equally important in the grading.", "labels": [], "entities": []}, {"text": "Note, the human evaluation results for the English language are not included in this paper as by the time of writing the results were not yet published.", "labels": [], "entities": []}, {"text": "We only report the human evaluation results of the Arabic multi-document summaries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: English Automatic Evaluation Scores  (single-document)", "labels": [], "entities": [{"text": "English Automatic Evaluation Scores", "start_pos": 10, "end_pos": 45, "type": "DATASET", "confidence": 0.6843778491020203}]}, {"text": " Table 3: Arabic Automatic Evaluation Scores  (single-document)", "labels": [], "entities": [{"text": "Arabic Automatic Evaluation Scores", "start_pos": 10, "end_pos": 44, "type": "DATASET", "confidence": 0.5171373039484024}]}, {"text": " Table 4: Arabic Manual Evaluation Scores (multi- document)", "labels": [], "entities": [{"text": "Arabic Manual Evaluation Scores", "start_pos": 10, "end_pos": 41, "type": "DATASET", "confidence": 0.7284524738788605}]}]}