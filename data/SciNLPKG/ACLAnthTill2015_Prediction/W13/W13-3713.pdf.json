{"title": [{"text": "Towards a psycholinguistically motivated dependency grammar for Hindi", "labels": [], "entities": []}], "abstractContent": [{"text": "The overall goal of our work is to build a dependency grammar-based human sentence processor for Hindi.", "labels": [], "entities": []}, {"text": "As a first step towards this end, in this paper we present a dependency grammar that is motivated by psycholinguistic concerns.", "labels": [], "entities": []}, {"text": "We describe the components of the grammar that have been automatically induced using a Hindi dependency treebank.", "labels": [], "entities": []}, {"text": "We relate some aspects of the grammar to relevant ideas in the psycholinguistics literature.", "labels": [], "entities": []}, {"text": "In the process , we also extract statistics and patterns for phenomena that are interesting from a processing perspective.", "labels": [], "entities": []}, {"text": "We finally present an outline of a dependency grammar-based human sentence processor for Hindi.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human sentence processing proposals and modeling works are overwhelmingly based on phrasestructure parsing and constituent based representation.", "labels": [], "entities": [{"text": "phrasestructure parsing", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.718005821108818}]}, {"text": "This is because most modern linguistic theories,,,, use phrase structure representation to analyze a sentence.", "labels": [], "entities": [{"text": "phrase structure representation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.7169777154922485}]}, {"text": "There is, however, an alternative approach to sentential syntactic representation, known as dependency representation, that is quite popular in Computational Linguistics (CL).", "labels": [], "entities": [{"text": "sentential syntactic representation", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.7195297082265218}, {"text": "dependency representation", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.6961545348167419}]}, {"text": "Unlike phrase structures where the actual words of the sentence appear as leaves, and the internal nodes are phrases, in a dependency grammar,,) a syntactic tree comprises of sentential words as nodes.", "labels": [], "entities": []}, {"text": "These words/nodes are connected to each other with edges/arcs.", "labels": [], "entities": []}, {"text": "The edges can be labeled to show the type of relation between a pair of node.", "labels": [], "entities": []}, {"text": "For example, in the sentence John kissed Mary, 'John' and 'Mary' are connected via arcs to 'kissed'; the former arc bears the label 'subject' and the latter arc the label 'object'.", "labels": [], "entities": []}, {"text": "Taken together, these nodes with their connections form a tree.", "labels": [], "entities": []}, {"text": "shows the dependency and the phrase structure trees for the above sentence.", "labels": [], "entities": []}, {"text": "There have been some previous attempts to use lexicalized grammars such as LTAG, CCG, etc.", "labels": [], "entities": []}, {"text": "These lexicalized grammars have been independently shown to be related to dependency grammar (.", "labels": [], "entities": []}, {"text": "For example, used categorial grammar to handle processing of empty categories.", "labels": [], "entities": []}, {"text": "Similarly, used dependency categorial grammar to process both local and non-local dependencies.", "labels": [], "entities": []}, {"text": "More recenly, Ta-bor and used a lexical grammar based parser and used lexicalized tree-adjoining grammar (LTAG) to model certain processing results. has recently proposed a psycholinguistically motivated LTAG (P-LTAG).", "labels": [], "entities": []}, {"text": "Despite the success of dependency paradigm in CL, it has remained unexplored in psycholinguistics.", "labels": [], "entities": []}, {"text": "To our knowledge, the work by Boston and colleagues,) is the only such attempt.", "labels": [], "entities": []}, {"text": "There are some very interesting open questions with respect to using dependency representation and dependency parsers while building a human sentence processing system.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7091459780931473}]}, {"text": "Can a processing model based on dependency parsing paradigm account for classic psycholinguistic phenomena?", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.746179461479187}]}, {"text": "Can one adapt a high performance dependency parser for psycholinguistic research?", "labels": [], "entities": []}, {"text": "If yes, then how?", "labels": [], "entities": []}, {"text": "How will the differences in different dependency parsing paradigms affect the predictive capacity of the models based on them?", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.724782332777977}]}, {"text": "This paper is arranged as follows, in Section 2 we mention some experimental works that have motivated the grammar design.", "labels": [], "entities": []}, {"text": "Section 3 discusses the grammar induction process and lists out the main components of the grammar.", "labels": [], "entities": [{"text": "grammar induction process", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.7628152966499329}]}, {"text": "In Section 4 we present statistics of Hindi word order variations as found in the treebank and point out some patterns that are interesting from a processing perspective.", "labels": [], "entities": []}, {"text": "We also talk about prediction rules that are an important component in the grammar.", "labels": [], "entities": []}, {"text": "Section 5 then presents a proposal for developing a human sentence processing system by adapting graph-based dependency parsing.", "labels": [], "entities": [{"text": "human sentence processing", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6221857070922852}, {"text": "dependency parsing", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7385346293449402}]}, {"text": "Finally in Section 6 we discuss some issues and challenges in using dependency grammar paradigm for human sentence processing.", "labels": [], "entities": [{"text": "human sentence processing", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.6511826813220978}]}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Some crucial design decisions in our research have been inspired by psycholinguistic experimental work.", "labels": [], "entities": []}, {"text": "In this section we will mention these works.", "labels": [], "entities": []}, {"text": "But before that, we will briefly discuss Hindi, the language that we are working with.", "labels": [], "entities": []}, {"text": "Hindi is one of the official languages of India.", "labels": [], "entities": []}, {"text": "Hindi is the fourth most widely spoken language in the world . It is a free-word order language and is head final.", "labels": [], "entities": []}, {"text": "It has relatively rich morphol-ogy with verb-subject 2 , noun-adjective agreement.", "labels": [], "entities": []}, {"text": "Examples (2) to (6) below show some of the possible word order variations possible with (1).", "labels": [], "entities": []}, {"text": "These permutations are not exhaustive -in fact all 4!", "labels": [], "entities": []}, {"text": "A great deal of experimental research has shown that working-memory limitations play a major role in sentence comprehension difficulty (e.g.,).", "labels": [], "entities": [{"text": "sentence comprehension difficulty", "start_pos": 101, "end_pos": 134, "type": "TASK", "confidence": 0.5763775606950124}]}, {"text": "We find numerous instances in natural language where a word needs to be temporarily retained in memory before it can be integrated as part of a larger structure.", "labels": [], "entities": []}, {"text": "Because of limited working-memory, retaining a word fora longer period can make sentence processing difficult.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7994115948677063}]}, {"text": "Abstracting away from details, on this view, one way in which processing complexity can be formulated is by using metrics that can incorporate dependent-head distance,).", "labels": [], "entities": []}, {"text": "This idea manifests itself in various forms in the psycholinguistics literature.", "labels": [], "entities": []}, {"text": "For example, proposes integration cost and storage cost to account for processing complexity.", "labels": [], "entities": []}, {"text": "have proposed a working memory-based theory that uses the notion of decay as one determinant of memory retrieval difficulty.", "labels": [], "entities": []}, {"text": "Elements that exists in memory without being retrieved fora longtime will decay more, compared to elements that have been retrieved recently or elements that are recent.", "labels": [], "entities": []}, {"text": "In addition to decay, the theory also incorporates the notion of interference.", "labels": [], "entities": []}, {"text": "Memory retrievals are feature based, and feature overlap during retrieval, in addition to decay, will cause difficulty.", "labels": [], "entities": []}, {"text": "As opposed to locality-based accounts mentioned above, expectation-based theories appeal to the predictive nature of sentence processor.", "labels": [], "entities": []}, {"text": "On this view, processing becomes difficult if the upcoming sentential material is less predictable.", "labels": [], "entities": []}, {"text": "Surprisal,) is one such account.", "labels": [], "entities": [{"text": "Surprisal", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7739914655685425}]}, {"text": "Informally, surprisal increases when a parser is required to build some low-probability structure.", "labels": [], "entities": [{"text": "surprisal", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.8765305876731873}]}, {"text": "Expectation-based theories can successfully account for so called anti-locality effects.", "labels": [], "entities": []}, {"text": "It has been noted that in some language phenomena, increasing the distance between the dependent and its head speeds up reading time at the head (see for such effects in German, and for Hindi).", "labels": [], "entities": []}, {"text": "This is, of course, contrary to the predictions made by locality-based accounts where such an increase should cause slowdown at the head.", "labels": [], "entities": []}, {"text": "There is considerable empirical evidence supporting the idea of predictive parsing in language comprehension (e.g.).", "labels": [], "entities": [{"text": "predictive parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9162069261074066}]}, {"text": "There is also some evidence that shows that the nature of predictive processing can be contingent on language specific characteristics.", "labels": [], "entities": [{"text": "predictive processing", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.9679969251155853}]}, {"text": "For example, argue that the verb-final nature of German subordinate clauses leads to the parser maintaining future predictions more effectively as compared to English.", "labels": [], "entities": []}, {"text": "As Hindi is a verb-final language, these experimental results become pertinent for this paper.", "labels": [], "entities": []}, {"text": "Locality-based results are generally formalized using the limited working memory model.", "labels": [], "entities": []}, {"text": "Such a model enforces certain resource limitations within which human sentence processing system operates.", "labels": [], "entities": []}, {"text": "On the other hand, expectation/prediction-based results have to be accounted by appealing to the nature of the processing system itself.", "labels": [], "entities": []}, {"text": "Hindi being a free word order language, experimental work that deal with the processing cost of word-order variation is also important to us.", "labels": [], "entities": []}, {"text": "Experimental work points to the fact that human sentence processing is sensitive to word order variation (e.g. Bader and Meng (1999), ).", "labels": [], "entities": []}, {"text": "However, it is still not clear as to why/how word order variation influences processing costs.", "labels": [], "entities": []}, {"text": "Processing costs could be due to variety of reasons (such as, syntactic complexity, frequency, information structure, prosody, memory constraints, etc).", "labels": [], "entities": []}, {"text": "So, there are three streams of experimental research that are relevant for us: (a) locality effects, (b) anti-locality/expectation effects, (c) word-order variation effects.", "labels": [], "entities": []}, {"text": "In section 3 and 4 we will discuss how insights from (b) and (c) inform some of our design decisions.", "labels": [], "entities": []}, {"text": "Later in section 5 while discussing human sentence processing, we will touch upon the notion of locality in our parsing approach.", "labels": [], "entities": [{"text": "human sentence processing", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.7157201369603475}]}], "tableCaptions": [{"text": " Table 1: Relative clause types (Occurrence in %).  Total RC count = 1198.", "labels": [], "entities": [{"text": "Occurrence", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9883576035499573}, {"text": "Total RC count", "start_pos": 52, "end_pos": 66, "type": "METRIC", "confidence": 0.9057174324989319}]}]}