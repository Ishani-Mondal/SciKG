{"title": [{"text": "What and where: An empirical investigation of pointing gestures and descriptions in multimodal referring actions", "labels": [], "entities": []}], "abstractContent": [{"text": "Pointing gestures are pervasive inhuman referring actions, and are often combined with spoken descriptions.", "labels": [], "entities": []}, {"text": "Combining gesture and speech naturally to refer to objects is an essential task in multimodal NLG systems.", "labels": [], "entities": []}, {"text": "However, the way gesture and speech should be combined in a referring act remains an open question.", "labels": [], "entities": []}, {"text": "In particular , it is not clear whether, in planning a pointing gesture in conjunction with a description , an NLG system should seek to minimise the redundancy between them, e.g. by letting the pointing gesture indicate locative information, with other, non-locative properties of a referent included in the description.", "labels": [], "entities": []}, {"text": "This question has a bearing on whether the gestural and spoken parts of referring acts are planned separately or arise from a common underlying computational mechanism.", "labels": [], "entities": []}, {"text": "This paper investigates this question empirically, using machine-learning techniques on anew corpus of dialogues involving multimodal references to objects.", "labels": [], "entities": []}, {"text": "Our results indicate that human pointing strategies interact with descriptive strategies.", "labels": [], "entities": []}, {"text": "In particular , pointing gestures are strongly associated with the use of locative features in referring expressions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Referring Expression Generation (REG) is considered a core task in many NLG systems.", "labels": [], "entities": [{"text": "Referring Expression Generation (REG)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9128388265768687}]}, {"text": "Typically, the REG task is defined in terms of identification: a referent needs to be unambiguously identified in a discourse, enabling the reader or listener to pick it out from among its potential distractors.", "labels": [], "entities": [{"text": "REG task", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.920484870672226}]}, {"text": "Most work in this area has focused on algorithms that select the content for definite descriptions, or on the best form fora referring expression given the discourse context, for example, whether it should be a full definite description, a reduced one, or a pronoun ().", "labels": [], "entities": []}, {"text": "Less attention has been payed to the role of gestures in referring actions and the way these can be coupled with discursive strategies for referent identification.", "labels": [], "entities": [{"text": "referent identification", "start_pos": 139, "end_pos": 162, "type": "TASK", "confidence": 0.8692260384559631}]}, {"text": "This question becomes particularly important in the context of multimodal systems, for example, those involving embodied conversational agents, where the 'naturalness' of an interaction hinges in part on the appropriate use of embodied actions, including referring actions.", "labels": [], "entities": []}, {"text": "Multimodal strategies can also make communication more efficient.", "labels": [], "entities": []}, {"text": "For example, found that the use of pointing gestures resulted in significantly faster resolution of ambiguous referring expressions; crucially, this result was replicated when the pointing gesture was artificially generated, rather than made by a human.", "labels": [], "entities": []}, {"text": "Like human communicators, embodied agents need the ability to plan multimodal referring acts, combining both linguistic reference and pointing.", "labels": [], "entities": []}, {"text": "An important question is whether these two components of a referring act should be planned in order to minimise redundancy between them or not.", "labels": [], "entities": []}, {"text": "For example, given that a pointing gesture can efficiently locate a target referent in a visual domain, should an accompanying description avoid mentioning locative properties, thereby minimising redundancy?", "labels": [], "entities": []}, {"text": "This question is the main focus of this paper.", "labels": [], "entities": []}, {"text": "However, it bears on a deeper issue, of relevance to the architecture of multimodal systems (and the cognitive architectures whose behaviours such systems seek to emulate): Should gestural and descriptive strategies be viewed as separate (implying that a REG module can plan its linguistic referring expressions more or less in-dependently of whether a pointing gesture is also used) or should they be viewed as tightly coupled?", "labels": [], "entities": []}, {"text": "If they are indeed coupled, are there any features of a linguistic description (for example, an object's location) which are excluded when a pointing gesture is used, or are linguistic features always redundant with pointing?", "labels": [], "entities": []}, {"text": "The present paper addresses these questions in a data-driven fashion, using a multimodal corpus of dialogues collected specifically to study referring actions at both the linguistic and gestural levels.", "labels": [], "entities": []}, {"text": "We focus on pointing (that is, deictic) gestures directed at an intended referent (as opposed to, say, iconic gestures) and investigate the extent to which pointing interacts with linguistic means for referent identification.", "labels": [], "entities": [{"text": "referent identification", "start_pos": 201, "end_pos": 224, "type": "TASK", "confidence": 0.855518251657486}]}, {"text": "Following an overview of previous work on pointing and reference (Section 2) and a description of the corpus (Section 3), we describe a number of machine-learning experiments that address the main empirical question (Section 4), concluding with a discussion.", "labels": [], "entities": [{"text": "pointing and reference", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9055891831715902}]}], "datasetContent": [{"text": "In much of the work discussed in Section 2, the generation of pointing gestures is viewed as dependent on physical characteristics of the referents, in other words on their being suitable for pointing.", "labels": [], "entities": []}, {"text": "This is especially true of work related to the trade-off hypothesis, in which the costs of pointing gestures are calculated as a function of the referent object's size and its distance from the speaker.", "labels": [], "entities": []}, {"text": "In the present paper, by contrast, we are interested in investigating the relation between pointing and linguistic means of referent identification.", "labels": [], "entities": [{"text": "referent identification", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.8341160118579865}]}, {"text": "More specifically, we address the question to what degree the different linguistic expressions used by the speaker to refer to objects in the MREDI dialogues, can be used to predict the occurrence of pointing gestures.", "labels": [], "entities": [{"text": "MREDI dialogues", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.48607926070690155}]}, {"text": "Note that this question addresses the correlation between properties in a description and the occurrence of pointing, rather than the issue of how pointing and describing should be planned.", "labels": [], "entities": []}, {"text": "Nevertheless, as we have emphasised in Section 2, the question of cooccurrence of the two referential strategies does have a bearing on architectural issues.", "labels": [], "entities": []}, {"text": "A first set of experiments were run in order to test the general trade-off hypothesis.", "labels": [], "entities": []}, {"text": "We tested a number of classifiers on the task of classifying the binary feature point, given all the linguistic features in the corpus.", "labels": [], "entities": []}, {"text": "More specifically, the attributes used for the classification were MapConfl, DIR, RP, AP, FP, S, Sh, C, D, I, Point.", "labels": [], "entities": [{"text": "MapConfl", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.8527477979660034}, {"text": "AP", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9176222681999207}, {"text": "FP", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.8731254935264587}]}, {"text": "They are all explained and exemplified in with the exception of MapConfl, which indicates whether a specific casein the data comes from a group or a singleton map.", "labels": [], "entities": []}, {"text": "This feature was included because, as noted in the previous section, whether a target landmark was a singleton or a group made a difference, presumably because groups are larger and more visually salient.", "labels": [], "entities": []}, {"text": "Note further that one of the Action features, GZ (gaze), is ignored in the experiments because it is an almost univocal predictor of pointing.", "labels": [], "entities": [{"text": "GZ", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9268548488616943}]}, {"text": "Indeed, gazing is involved roughly every time Point has the value y (yes) (but not the other way round).", "labels": [], "entities": [{"text": "gazing", "start_pos": 8, "end_pos": 14, "type": "TASK", "confidence": 0.9488673806190491}]}, {"text": "The experiments were run using the Weka) tool, which gives access to many different algorithms, and 10-fold crossvalidation was used throughout.", "labels": [], "entities": [{"text": "Weka) tool", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9107330044110616}]}, {"text": "The results are shown in  Two baselines were created to evaluate the results.", "labels": [], "entities": []}, {"text": "The first one is provided by the ZeroR classifier, which always chooses the most frequent class, in this case n (no pointing gesture).", "labels": [], "entities": []}, {"text": "The F-measure obtained by this method is somewhat high at 0.761, because there are relatively few pointing gestures in the data.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990830421447754}]}, {"text": "The second baseline, which provides a slightly more interesting result against which to evaluate the other classifiers, is provided by OneR.", "labels": [], "entities": [{"text": "OneR", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.964866578578949}]}, {"text": "It achieves an F-measure of 0.765 by predicting a pointing gesture if DIR >= 2.5, in other words if there are at least 2.5 occurrences of direction expressions in the utterance.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996174573898315}, {"text": "DIR", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9962561130523682}]}, {"text": "Using this rule has the effect of predicting a few of the pointing gestures, with an F-measure on they class (occurrence of pointing gestures) of 0.031.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.998992383480072}]}, {"text": "The other four sets of results were obtained by running four different classification algorithms with the same set of attributes.", "labels": [], "entities": []}, {"text": "Apart from SMO (an algorithm using support vector machines), all the classifiers perform better than the baseline.", "labels": [], "entities": [{"text": "SMO", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8428941369056702}]}, {"text": "The best results are produced by the decision tree classifier J48, which obtains an overall F-measure of 0.833, and an F-measure of 0.421 on they class.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9984946250915527}, {"text": "F-measure", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9972629547119141}]}, {"text": "The confusion matrix generated by J48 on this data-set is shown in: Predicting pointing given all the linguistic features in the corpus: confusion matrix.", "labels": [], "entities": []}, {"text": "The model created by the decision tree classifier (J48) is quite complex (size=57 and no. of leaves=29).", "labels": [], "entities": []}, {"text": "The first branching, which corresponds to no AP (Absolute Position) and no C (Colour), assigns n to as many as 1571 instances (with 115 errors).", "labels": [], "entities": [{"text": "AP", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9885330200195312}]}, {"text": "The tree is shown in.", "labels": [], "entities": []}, {"text": "The tree also shows that certain combinations of features are more likely to be associated with pointing gestures.", "labels": [], "entities": []}, {"text": "These are predominantly combinations including occurrences of AP, or, in the absence of absolute position, combinations including positive values for FP (Frequency of reference on Path) and DIR (Direction).", "labels": [], "entities": [{"text": "AP", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9922255277633667}, {"text": "FP", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.9975749850273132}, {"text": "DIR", "start_pos": 190, "end_pos": 193, "type": "METRIC", "confidence": 0.9727916121482849}]}, {"text": "The maximum entropy model, built by the logistic regression algorithm (Logistic), shows similar tendencies in that the attributes that are assigned the highest weights are AP, C and DIR.", "labels": [], "entities": [{"text": "AP", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.9790024757385254}]}, {"text": "These results confirm the general hypothesis that there is a strong relationship between linguistic features used in a description and pointing gestures.", "labels": [], "entities": []}, {"text": "Indeed, it is possible to predict pointing gestures on the basis of the linguistic features used.", "labels": [], "entities": []}, {"text": "In particular, the results suggest a difference between features that express locative properties and those having to do with the visual description of the same object (its colour, size and shape).", "labels": [], "entities": []}, {"text": "More specifically, it would seem that locative features are more useful to the classifiers than visual properties.", "labels": [], "entities": []}, {"text": "To test this second hypothesis, we ran a series of experiments where the task was still to predict pointing gestures, but different subsets of the linguistic features were tested one at the time.", "labels": [], "entities": []}, {"text": "For each feature combination, we run the classification using J48, Naive Bayes and the Logistic regression algorithm.", "labels": [], "entities": [{"text": "J48", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.961434006690979}]}, {"text": "In6), we show the best result obtained for each feature combination.", "labels": [], "entities": []}, {"text": "The classifiers are ordered from the most accurate to the least accurate, and the combination of features used by each of them is listed in the last column.", "labels": [], "entities": []}, {"text": "The best results and the two baselines from the previous set of experiments are included for the sake of comparison.", "labels": [], "entities": []}, {"text": "Note that the term Loc is used to refer to all the locative attributes AP, DIR, RP, AP and FP, while Visual refers to S, Sh and C. The best results are those obtained when the complete feature set is used in the training.", "labels": [], "entities": [{"text": "AP", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9339815974235535}, {"text": "FP", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9756750464439392}]}, {"text": "However, the next best results are achieved by the classifiers using the locative features, either alone or together with features concerning the map type, identity with a previously mentioned object and deictic reference, with an F-measure in the range 0.802-0.808.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 231, "end_pos": 240, "type": "METRIC", "confidence": 0.996396005153656}]}, {"text": "If visual features are used instead, the F-measure is in the range 0.775-0.779.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.999011754989624}]}, {"text": "The worst results are obtained if neither location nor visual description are used.", "labels": [], "entities": []}, {"text": "Thus, although the differences between the best and the worst classifiers are not dramatic, in this data we see a tendency for the locative features to be slightly better predictors of pointing gestures than features corresponding to visual descriptions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Features annotated in the dialogues. All features have frequency values, except for the Action  features, which are boolean.", "labels": [], "entities": []}, {"text": " Table 2: Descriptive statistics for features in the  corpus", "labels": [], "entities": []}, {"text": " Table 3: Frequency of occurrence of pointing ges- tures relative to different object types.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9110103845596313}]}, {"text": " Table 4: Predicting pointing gestures given all the  linguistic features in the corpus: classification re- sults.", "labels": [], "entities": []}, {"text": " Table 6: Predicting pointing gestures with different feature combinations: classification results.", "labels": [], "entities": []}]}