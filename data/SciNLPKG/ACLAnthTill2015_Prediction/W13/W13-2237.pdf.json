{"title": [{"text": "Online Learning Approaches in Computer Assisted Translation", "labels": [], "entities": [{"text": "Computer Assisted Translation", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7476905385653178}]}], "abstractContent": [{"text": "We present a novel online learning approach for statistical machine translation tailored to the computer assisted translation scenario.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.7249329487482706}, {"text": "computer assisted translation", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.7105781237284342}]}, {"text": "With the introduction of a simple online feature, we are able to adapt the translation model on the fly to the corrections made by the translators.", "labels": [], "entities": []}, {"text": "Additionally, we do online adaption of the feature weights with a large margin algorithm.", "labels": [], "entities": []}, {"text": "Our results show that our online adaptation technique outperforms the static phrase based statistical machine translation system by 6 BLEU points absolute , and a standard incremental adaptation approach by 2 BLEU points absolute.", "labels": [], "entities": [{"text": "online adaptation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6984612047672272}, {"text": "statistical machine translation", "start_pos": 90, "end_pos": 121, "type": "TASK", "confidence": 0.6623026927312216}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9982789754867554}, {"text": "BLEU", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.994752049446106}]}], "introductionContent": [{"text": "The growing needs of the localization and translation industry have recently boosted research around computer assisted translation (CAT) technology.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8847489953041077}, {"text": "computer assisted translation (CAT)", "start_pos": 101, "end_pos": 136, "type": "TASK", "confidence": 0.8437932829062144}]}, {"text": "The purpose of CAT is to increase the productivity of a human translator.", "labels": [], "entities": [{"text": "CAT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9773379564285278}]}, {"text": "A CAT tool comes as a package of a Translation Memory (TM), builtin spell checkers, a dictionary, a terminology list etc.", "labels": [], "entities": [{"text": "CAT", "start_pos": 2, "end_pos": 5, "type": "TASK", "confidence": 0.9643553495407104}, {"text": "Translation Memory (TM)", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7887065291404725}]}, {"text": "which help the translator while translating a sentence.", "labels": [], "entities": [{"text": "translating a sentence", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8429150581359863}]}, {"text": "Recent research has led to the integration of CAT tools with statistical machine translation (SMT) engines.", "labels": [], "entities": [{"text": "CAT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9844100475311279}, {"text": "statistical machine translation (SMT)", "start_pos": 61, "end_pos": 98, "type": "TASK", "confidence": 0.7957581033309301}]}, {"text": "SMT makes use of a large available parallel corpus to generate statistical models for translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.965910792350769}, {"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.9621813893318176}]}, {"text": "Due to their generalization capability, SMT systems area good fit in this scenario and a seamless integration of SMT engines in CAT have shown to increase translator's productivity . Although automatic systems generate reliable translations they are not accurate enough to be used directly and need postedition by human translators.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9904360175132751}, {"text": "postedition", "start_pos": 299, "end_pos": 310, "type": "TASK", "confidence": 0.9786704182624817}]}, {"text": "In state-of-the-art CAT tools, the SMT systems are static in nature and so they cannot adapt to these corrections.", "labels": [], "entities": [{"text": "CAT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9775453805923462}, {"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9769595861434937}]}, {"text": "When a SMT system keeps repeating the same error, productivity of translators as well as their trust in SMT technology are negatively affected.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9802415370941162}, {"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9652031064033508}]}, {"text": "As an example, technical documentation typically contains a lot of repetitions due to the employed writing style and pervasive use of terminology.", "labels": [], "entities": []}, {"text": "Hence, in order to provide useful hints, SMT systems are expected to behave consistently regarding the translation of domainspecific terms.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9914621114730835}]}, {"text": "However, if the user edits the translation of a technical term in the target text, most current SMT systems are incapable to learn from those corrections.", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9925262928009033}]}, {"text": "Online learning is a machine learning task where a predictor iteratively: (1) receives an input and outputs a label, (2) receives the correct label from a human and if the two labels do not match, it learns from the mistake.", "labels": [], "entities": [{"text": "Online learning", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7859397828578949}]}, {"text": "The task of learning from user corrections at the sentence level fits well the online learning scenario, and its expected usefulness is clearly related to the amount of repetitions occurring in the text.", "labels": [], "entities": []}, {"text": "The higher the number of repetititions in a document the more the SMT system has chances to translate consistently through the use of online learning.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9862581491470337}]}, {"text": "In this paper, we implemented two online learning methods through which a phrase-based SMT system evolves overtime, sentence after sentence, by taking advantage of the post-edition or translation of the previous sentence by the user.", "labels": [], "entities": [{"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8758276700973511}]}, {"text": "1 In the first approach, we focus on the translation model aspect of SMT which is represented by five conventional features, namely lexical and phrase translation probabilities in both directed and inverted directions, plus a phrase penalty score.", "labels": [], "entities": [{"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9912272095680237}]}, {"text": "Translation, language and reordering models are combined in a linear fashion to obtain a score for the translation hypothesis as shown in Equation 1.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9373218417167664}, {"text": "translation", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.9592810869216919}]}, {"text": "where hi (\u00b7) are the feature functions representing the models and \u03bb i are the linear weights.", "labels": [], "entities": []}, {"text": "The highest scored translation is the best hypothesis e * output by the system.", "labels": [], "entities": []}, {"text": "We extend the translation model with anew feature which provides extra phrase-pair scores changing according to the user feedback.", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9666332006454468}]}, {"text": "The scores of the new feature are adapted in a discriminative fashion, by rewarding phrase-pairs observed in the search space and in the reference, and penalizing phrase-pairs observed in the search space but not in the reference.", "labels": [], "entities": []}, {"text": "In the second approach, we also adapt the model weights of the linear combination after each test sentence by using a margin infused relaxed algorithm (MIRA).", "labels": [], "entities": [{"text": "margin infused relaxed algorithm (MIRA)", "start_pos": 118, "end_pos": 157, "type": "METRIC", "confidence": 0.8774918402944293}]}, {"text": "For assessing the robustness of our methods, we performed experiments on two datasets from different domains and language pairs ( \u00a76).", "labels": [], "entities": []}, {"text": "Moreover, our online learning approaches are compared against a static baseline system and against the incremental adaptation approach proposed by", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared our online learning approaches (Sections 3 and 4) and the stream based adaptation method (Section 5) on two datasets from different domains, namely Information Technology (IT) and TED talks, and two different language pairs.", "labels": [], "entities": []}, {"text": "The IT domain dataset is proprietary, it involves the translation of technical documents from English to Italian and has been used in the field test carried out under the MateCat project 2 . Experiments are also conducted on English to French TED talks dataset () to assess the robustness of the proposed approaches in a different scenario and to provide results on a publicly available dataset for the sake of reproducibility.", "labels": [], "entities": [{"text": "IT domain dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7214828630288442}, {"text": "TED talks dataset", "start_pos": 243, "end_pos": 260, "type": "DATASET", "confidence": 0.7236997385819753}]}, {"text": "The training, development (dev2010) and evaluation (tst2010 3 ) sets are the same as used in the last IWSLT last evaluation campaigns.", "labels": [], "entities": [{"text": "IWSLT last evaluation", "start_pos": 102, "end_pos": 123, "type": "DATASET", "confidence": 0.7645546793937683}]}, {"text": "In experiments on TED data, we considered the human reference translations as post edits, even if they were actually generated from scratch.", "labels": [], "entities": []}, {"text": "In our experiments, the extent of usefulness of online learning highly depends on the amount of repetition of text.", "labels": [], "entities": []}, {"text": "A reasonable way to measure the quantity of repetition in each document is through the repetition rate (.", "labels": [], "entities": [{"text": "repetition", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.8804239630699158}, {"text": "repetition rate", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9859546422958374}]}, {"text": "It computes the rate of non-singleton n-grams, n=1...4, averaging the values over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean.", "labels": [], "entities": []}, {"text": "Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and n r is the number of different n-grams occurring exactly r times: Statistics of the parallel sets and their repetition rate on both sides are reported in: Statistics of the parallel data along with the corresponding repetition rate (RR).", "labels": [], "entities": [{"text": "repetition rate", "start_pos": 49, "end_pos": 64, "type": "METRIC", "confidence": 0.9626258313655853}, {"text": "repetition rate (RR)", "start_pos": 363, "end_pos": 383, "type": "METRIC", "confidence": 0.9794010639190673}]}, {"text": "It can be noted that the repetition rates of IT and TED sets are significantly different, particularly high in IT documents, much lower in the TED talks.", "labels": [], "entities": [{"text": "repetition rates", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9812891185283661}]}], "tableCaptions": [{"text": " Table 1: Statistics of the parallel data along with  the corresponding repetition rate (RR).", "labels": [], "entities": [{"text": "repetition rate (RR)", "start_pos": 72, "end_pos": 92, "type": "METRIC", "confidence": 0.9752167582511901}]}, {"text": " Table 2: Result on the IT domain task (EN>IT). Baseline is a standard phrase based SMT system, +O  has the online feature, +NS adds normalization of online feature, +W has online weight adaptation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9526770114898682}]}, {"text": " Table 3: Result on the TED talk task (EN>FR). Baseline is a standard phrase based SMT system, +O  has the online feature, +NS adds normalization of online feature, +W includes online weight adaptation.", "labels": [], "entities": [{"text": "TED talk task", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.6350175937016805}, {"text": "FR", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.8684290051460266}, {"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9395464658737183}, {"text": "weight adaptation", "start_pos": 184, "end_pos": 201, "type": "TASK", "confidence": 0.6413785815238953}]}]}