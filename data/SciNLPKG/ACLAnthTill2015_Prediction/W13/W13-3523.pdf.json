{"title": [{"text": "Topic Models + Word Alignment = A Flexible Framework for Extracting Bilingual Dictionary from Comparable Corpus", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.6545983850955963}]}], "abstractContent": [{"text": "We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora.", "labels": [], "entities": []}, {"text": "Our approach is based on a novel combination of topic modeling and word alignment techniques.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7430406510829926}, {"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7896847128868103}]}, {"text": "Intuitively, our approach works by converting a comparable document-aligned corpus into a parallel topic-aligned corpus , then learning word alignments using co-occurrence statistics.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.7143751978874207}]}, {"text": "This topic-aligned corpus is similar in structure to the sentence-aligned corpus frequently used in statistical machine translation, enabling us to exploit advances in word alignment research.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.6327472527821859}, {"text": "word alignment", "start_pos": 168, "end_pos": 182, "type": "TASK", "confidence": 0.7919310927391052}]}, {"text": "Unlike many previous work, our framework does not require any language-specific knowledge for initialization.", "labels": [], "entities": []}, {"text": "Furthermore , our framework attempts to handle polysemy by allowing multiple translation probability models for each word.", "labels": [], "entities": []}, {"text": "On a large-scale Wikipedia corpus, we demonstrate that our framework reliably extracts high-precision translation pairs on a wide variety of comparable data conditions .", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.8608400523662567}]}], "introductionContent": [{"text": "A machine-readable bilingual dictionary plays a very important role in many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.6426288882891337}]}, {"text": "In machine translation (MT), dictionaries can help in the domain adaptation setting).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8602154493331909}]}, {"text": "In cross-lingual information retrieval (CLIR), dictionaries serve as efficient means for query translation).", "labels": [], "entities": [{"text": "cross-lingual information retrieval (CLIR)", "start_pos": 3, "end_pos": 45, "type": "TASK", "confidence": 0.7981054733196894}, {"text": "query translation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7251945585012436}]}, {"text": "Many other multilingual applications also rely on bilingual dictionaries as integral components.", "labels": [], "entities": []}, {"text": "One approach for building a bilingual dictionary resource uses parallel sentence-aligned corpora.", "labels": [], "entities": []}, {"text": "This is often done in the context of Statistical MT, using word alignment algorithms such as the IBM models (.", "labels": [], "entities": [{"text": "Statistical MT", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7967522740364075}, {"text": "word alignment", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7710271179676056}]}, {"text": "Unfortunately, parallel corpora maybe scarce for certain language-pairs or domains of interest (e.g., medical and microblog).", "labels": [], "entities": []}, {"text": "Thus, the use of comparable corpora for bilingual dictionary extraction has become an active research topic ().", "labels": [], "entities": [{"text": "bilingual dictionary extraction", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.5838253994782766}]}, {"text": "Here, a comparable corpus is defined as collections of document pairs written in different languages but talking about the same topic, such as interconnected Wikipedia articles.", "labels": [], "entities": []}, {"text": "The challenge with bilingual dictionary extraction from comparable corpus is that existing word alignment methods developed for parallel corpus cannot be directly applied.", "labels": [], "entities": [{"text": "bilingual dictionary extraction", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6510420441627502}, {"text": "word alignment", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.7157718241214752}]}, {"text": "We believe there are several desiderata for bilingual dictionary extraction algorithms: 1.", "labels": [], "entities": [{"text": "bilingual dictionary extraction", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6266318460305532}]}, {"text": "Low Resource Requirement: The approach should not rely on language-specific knowledge or a large scale seed lexicon.", "labels": [], "entities": []}, {"text": "2. Polysemy Handling: One should handle the fact that a word form may have multiple meanings, and such meanings maybe translated differently.", "labels": [], "entities": []}, {"text": "3. Scalability: The approach should run efficiently an massively large-scale datasets.", "labels": [], "entities": []}, {"text": "Our framework addresses the above desired points by exploiting a novel combination of topic models and word alignment, as shown in.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.750143826007843}]}, {"text": "Intuitively, our approach works by first converting a comparable document-aligned corpus into a par- allel topic-aligned corpus, then apply word alignment methods to model co-occurence within topics.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 140, "end_pos": 154, "type": "TASK", "confidence": 0.7159977853298187}]}, {"text": "By employing topic models, we avoid the need for seed lexicon and operate purely in the realm of unsupervised learning.", "labels": [], "entities": []}, {"text": "By using word alignment on topic model results, we can easily model polysemy and extract topic-dependent lexicons.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7289320826530457}]}, {"text": "Specifically, let we bean English word and w f be a French word.", "labels": [], "entities": []}, {"text": "One can think of traditional bilingual dictionary extraction as obtaining (w e , w f ) pairs in which the probability p(w e |w f ) or p(w f |w e ) is high.", "labels": [], "entities": [{"text": "bilingual dictionary extraction", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.6323101222515106}]}, {"text": "Our approach differs by modeling p(w e |w f , t) or p(w f |w e , t) instead, where t is a topic.", "labels": [], "entities": []}, {"text": "The key intuition is that it is easier to tease out the translation of a polysemous word e given p(w f |w e , t) rather than p(w f |w e ).", "labels": [], "entities": []}, {"text": "A word maybe polysemous, but given a topic, there is likely a one-to-one correspondence for the most appropriate translation.", "labels": [], "entities": []}, {"text": "For example, under the simple model p(w f |w e ), the English word \"free\" maybe translated into the Japanese word (as in free speech) or (as in free beer) with equal 0.5 probability; this low probability may cause both translation pairs to be rejected by the dictionary extraction algorithm.", "labels": [], "entities": [{"text": "dictionary extraction", "start_pos": 259, "end_pos": 280, "type": "TASK", "confidence": 0.7144059389829636}]}, {"text": "On the other hand, given p(w f |w e , t), where t is \"politics\" or \"shopping\", we can allow high probabilities for both words depending on context.", "labels": [], "entities": []}, {"text": "Our contribution is summarized as follows: \u2022 We propose a bilingual dictionary extraction framework that simultaneously achieves all three of the desiderata: low resource requirement, polysemy handling, and scalability.", "labels": [], "entities": [{"text": "dictionary extraction", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.6804734766483307}]}, {"text": "We are not aware of any previous works that address all three.", "labels": [], "entities": []}, {"text": "\u2022 Our framework is extremely flexible and simple-to-implement, consisting of a novel combination of existing topic modeling tools from machine learning and word alignment tools from machine translation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 156, "end_pos": 170, "type": "TASK", "confidence": 0.7626056969165802}, {"text": "machine translation", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.7305116653442383}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Datasets: the number of document  pairs (#doc), sentences (#sent) and vocabulary  size (#voc) in English (e) and Japanese (j). For  pre-processing, we did word segmentation on  Japanese using Kytea (Neubig et al., 2011) and  Porter stemming on English. A TF-IDF based  stop-word lists of 1200 in each language is ap- plied. #doc is smaller for Wiki because not all  Japanese articles in Comp100% have English ver- sions in Wikipedia during the crawl.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.7037001103162766}]}, {"text": " Table 2:  Precision on the Wiki dataset.  K=number of topics. Precision (Prec) is defined  as |{Gold(e,f )}", "labels": [], "entities": [{"text": "Wiki dataset", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9342093765735626}]}, {"text": " Table 3: Counts of various error types.", "labels": [], "entities": [{"text": "Counts", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9605069756507874}]}, {"text": " Table 4: Examples of topic-dependent translations given by p(w f |w e , t k ). The top portion shows ex- amples of polysemous English words. The bottom shows examples where English is not decisively  polysemous, but indeed has distinct translations in Japanese based on topic.", "labels": [], "entities": []}, {"text": " Table 5: Wall-clock times in minutes for Topic  Modeling (topic), Word Alignment (giza), and  p(w e |w f ) calculation. Overall time for Pro- posed (Prp) is topic+giza+Eq.2 and for Cue is  topic+Eq.4.", "labels": [], "entities": [{"text": "Topic  Modeling", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.8670011162757874}, {"text": "Word Alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.6917950958013535}]}]}