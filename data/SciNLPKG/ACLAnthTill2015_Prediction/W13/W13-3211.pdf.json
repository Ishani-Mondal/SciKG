{"title": [{"text": "A Generative Model of Vector Space Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel compositional, gener-ative model for vector space representations of meaning.", "labels": [], "entities": []}, {"text": "This model reformulates earlier tensor-based approaches to vector space semantics as a top-down process, and provides efficient algorithms for transformation from natural language to vectors and from vectors to natural language.", "labels": [], "entities": []}, {"text": "We describe procedures for estimating the parameters of the model from positive examples of similar phrases, and from distribu-tional representations, then use these procedures to obtain similarity judgments fora set of adjective-noun pairs.", "labels": [], "entities": []}, {"text": "The model's estimation of the similarity of these pairs correlates well with human annotations, demonstrating a substantial improvement over several existing compositional approaches in both settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector-based word representations have gained enormous popularity in recent years as a basic tool for natural language processing.", "labels": [], "entities": [{"text": "Vector-based word representations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7711623907089233}, {"text": "natural language processing", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6611136595408121}]}, {"text": "Various models of linguistic phenomena benefit from the ability to represent words as vectors, and vector space word representations allow many problems in NLP to be reformulated as standard machine learning tasks (.", "labels": [], "entities": []}, {"text": "Most research to date has focused on only one means of obtaining vectorial representations of words: namely, by representing them distributionally.", "labels": [], "entities": []}, {"text": "The meaning of a word is assumed to be fully specified by \"the company it keeps\", and word co-occurrence (or occasionally term-document) matrices are taken to encode this context adequately.", "labels": [], "entities": []}, {"text": "Distributional representations have been shown to work well fora variety of different tasks).", "labels": [], "entities": []}, {"text": "The problem becomes more complicated when we attempt represent larger linguistic structuresmultiword constituents or entire sentenceswithin the same vector space model.", "labels": [], "entities": []}, {"text": "The most basic issue is one of sparsity: the larger a phrase, the less frequently we expect it to occur in a corpus, and the less data we will have from which to estimate a distributional representation.", "labels": [], "entities": []}, {"text": "To resolve this problem, recent work has focused on compositional vector space models of semantics.", "labels": [], "entities": []}, {"text": "Based on the Fregean observation that the meaning of a sentence is composed from the individual meanings of its parts, research in compositional distributional semantics focuses on describing procedures for combining vectors for individual words in order to obtain an appropriate representation of larger syntactic constituents.", "labels": [], "entities": []}, {"text": "But various aspects of this account remain unsatisfying.", "labels": [], "entities": []}, {"text": "We have a continuous semantic space in which finitely many vectors are associated with words, but noway (other than crude approximations like nearest-neighbor) to interpret the \"meaning\" of all the other points in the space.", "labels": [], "entities": []}, {"text": "More generally, it's not clear that it even makes sense to talk about the meaning of sentences or large phrases in distributional terms, when there is no natural context to represent.", "labels": [], "entities": []}, {"text": "We can begin to address these concerns by turning the conventional account of composition in vector space semantics on its head, and describing a model for generating language from vectors in semantic space.", "labels": [], "entities": []}, {"text": "Our approach is still compositional, in the sense that a sentence's meaning can be inferred from the meanings of its parts, but we relax the requirement that lexical items correspond to single vectors by allowing any vector.", "labels": [], "entities": []}, {"text": "In the process, we acquire algorithms for both meaning inference and natural language generation.", "labels": [], "entities": [{"text": "meaning inference", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.8365458846092224}, {"text": "natural language generation", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.6776899000008901}]}, {"text": "Our contributions in this paper are as follows: \u2022 A new generative, compositional model of phrase meaning in vector space.", "labels": [], "entities": [{"text": "generative, compositional model of phrase meaning", "start_pos": 56, "end_pos": 105, "type": "TASK", "confidence": 0.8015652128628322}]}, {"text": "\u2022 A convex optimization procedure for mapping words onto their vector representations.", "labels": [], "entities": [{"text": "convex optimization", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6989955306053162}]}, {"text": "\u2022 A training algorithm which requires only positive examples of phrases with the same meaning.", "labels": [], "entities": []}, {"text": "\u2022 Another training algorithm which requires only distributional representations of phrases.", "labels": [], "entities": []}, {"text": "\u2022 A set of preliminary experimental results indicating that the model performs well on realworld data in both training settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since our goal is to ensure that the distance between natural language expressions in the vector space correlates with human judgments of their relatedness, it makes sense to validate this model by measuring precisely that correlation.", "labels": [], "entities": []}, {"text": "In the remainder of this section, we provide evidence of the usefulness of our approach by focusing on measurements of the similarity of adjective-noun pairs (ANs).", "labels": [], "entities": []}, {"text": "We describe two different parameter estimation procedures for different kinds of training data.", "labels": [], "entities": []}, {"text": "Experimental setup is similar to the previous section; however, instead of same-meaning pairs collected from a reference corpus, our training data is a set of distributional vectors.", "labels": [], "entities": []}, {"text": "We use the same noun vectors, and obtain these new latent pair vectors by estimating them in the same fashion from the same corpus.", "labels": [], "entities": []}, {"text": "In order to facilitate comparison with the other experiment, we collect all pairs (a i , n i ) such that both a i and n i appear in the training set used in Section 4.1 (although, once again, not necessarily together).", "labels": [], "entities": []}, {"text": "Initialization of \u0398 and E, regularization and noise parameters, as well as the crossvalidation procedure, all proceed as in the previous section.", "labels": [], "entities": []}, {"text": "We also use the same restricted evaluation set, again to allow the results of the two experiments to be compared.", "labels": [], "entities": []}, {"text": "We evaluate by measuring the correlation of cosine similarities in the learned model with human similarity judgments, and as before consider a variant of the model in which a single adjective matrix is shared.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the similarity judgment exper- iment.", "labels": [], "entities": []}]}