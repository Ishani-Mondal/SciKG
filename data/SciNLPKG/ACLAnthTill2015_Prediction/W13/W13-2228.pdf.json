{"title": [{"text": "QCRI-MES Submission at WMT13: Using Transliteration Mining to Improve Statistical Machine Translation", "labels": [], "entities": [{"text": "WMT13", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.5226722955703735}, {"text": "Statistical Machine Translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7793331344922384}]}], "abstractContent": [{"text": "This paper describes QCRI-MES's submission on the English-Russian dataset to the Eighth Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "English-Russian dataset", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.7002949714660645}, {"text": "Statistical Machine Translation", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.7163827419281006}]}, {"text": "We generate improved word alignment of the training data by incorporating an unsupervised translitera-tion mining module to GIZA++ and build a phrase-based machine translation system.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.7616913914680481}, {"text": "phrase-based machine translation", "start_pos": 143, "end_pos": 175, "type": "TASK", "confidence": 0.6122372647126516}]}, {"text": "For tuning, we use a variation of PRO which provides better weights by optimizing BLEU+1 at corpus-level.", "labels": [], "entities": [{"text": "BLEU+1", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9597273866335551}]}, {"text": "We translit-erate out-of-vocabulary words in a post-processing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised translit-eration mining system.", "labels": [], "entities": []}, {"text": "For the Russian to English translation direction, we apply linguistically motivated pre-processing on the Russian side of the data.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe the QCRI-Munich-EdinburghStuttgart (QCRI-MES) English to Russian and Russian to English systems submitted to the Eighth Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "QCRI-Munich-EdinburghStuttgart (QCRI-MES", "start_pos": 16, "end_pos": 56, "type": "DATASET", "confidence": 0.8697851498921713}, {"text": "Statistical Machine Translation", "start_pos": 144, "end_pos": 175, "type": "TASK", "confidence": 0.6329663296540579}]}, {"text": "We experimented using the standard Phrase-based Statistical Machine Translation System (PSMT) as implemented in the Moses toolkit ().", "labels": [], "entities": [{"text": "Phrase-based Statistical Machine Translation", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.5613110587000847}]}, {"text": "The typical pipeline for translation involves word alignment using GIZA++, phrase extraction, tuning and phrase-based decoding.", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9775815606117249}, {"text": "word alignment", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.779489666223526}, {"text": "phrase extraction", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7921857535839081}]}, {"text": "Our system is different from standard PSMT in three ways: \u2022 We integrate an unsupervised transliteration mining system () into the GIZA++ word aligner).", "labels": [], "entities": []}, {"text": "So, the selection of a word pair as a correct alignment is decided using both translation probabilities and transliteration probabilities.", "labels": [], "entities": []}, {"text": "\u2022 The MT system fails when translating out-ofvocabulary (OOV) words.", "labels": [], "entities": [{"text": "MT", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9842978119850159}, {"text": "translating out-ofvocabulary (OOV) words", "start_pos": 27, "end_pos": 67, "type": "TASK", "confidence": 0.7949646214644114}]}, {"text": "We build a statistical transliteration system on the transliteration pairs mined by the unsupervised transliteration mining system and transliterate them in a post-processing step.", "labels": [], "entities": []}, {"text": "\u2022 We use a variation of Pairwise Ranking Optimization (PRO) for tuning.", "labels": [], "entities": []}, {"text": "It optimizes BLEU at corpus-level and provides better feature weights that leads to an improvement in translation quality ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9720189571380615}]}, {"text": "We participate in English to Russian and Russian to English translation tasks.", "labels": [], "entities": [{"text": "Russian to English translation tasks", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.6753616750240325}]}, {"text": "For the Russian/English system, we present experiments with two variations of the parallel corpus.", "labels": [], "entities": []}, {"text": "One set of experiments are conducted using the standard parallel corpus provided by the workshop.", "labels": [], "entities": []}, {"text": "In the second set of experiments, we morphologically reduce Russian words based on their fine-grained POS tags and map them to their root form.", "labels": [], "entities": []}, {"text": "We do this on the Russian side of the parallel corpus, tuning set, development set and test set.", "labels": [], "entities": []}, {"text": "This improves word alignment and learns better translation probabilities by reducing the vocabulary size.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7794241905212402}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 talks about unsupervised transliteration mining and its incorporation to the GIZA++ word aligner.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7519995272159576}, {"text": "GIZA++ word aligner", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.8617166429758072}]}, {"text": "In Section 3, we describe the transliteration system.", "labels": [], "entities": []}, {"text": "Section 4 describes the extension of PRO that optimizes BLEU+1 at corpus level.", "labels": [], "entities": [{"text": "BLEU+1", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9239685336748759}]}, {"text": "Section 5 and Section 6 present English/Russian and Russian/English machine translation experiments respectively.", "labels": [], "entities": [{"text": "Russian/English machine translation", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.531289792060852}]}], "datasetContent": [{"text": "We use two variations of the parallel corpus to build and test the Russian to English system.", "labels": [], "entities": []}, {"text": "One system is built on the data provided by the workshop.", "labels": [], "entities": []}, {"text": "For the second system, we preprocess the Russian side of the data as described in Section 6.1.", "labels": [], "entities": []}, {"text": "Both the provided parallel corpus and the morph-reduced parallel corpus consist of 2M parallel sentences each.", "labels": [], "entities": []}, {"text": "We use them for the estimation of the translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9673726558685303}]}, {"text": "We use large training data for the estimation of monolingual language model -en \u2248 287.3M sentences.", "labels": [], "entities": []}, {"text": "We follow the identical procedure of interpolated language model as described in Section 5.1.", "labels": [], "entities": []}, {"text": "We use newstest2012a for tuning and newstest2012b (tst2012) for development.", "labels": [], "entities": [{"text": "newstest2012b", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9007365107536316}]}, {"text": "In this section, we present translation experiments in Russian to English direction.", "labels": [], "entities": []}, {"text": "We morphologically reduce the Russian side of the parallel data in a pre-processing step and train the translation system on that.", "labels": [], "entities": []}, {"text": "We compare its result with the Russian to English system trained on the un-processed parallel data.: BLEU scores of English to Russian machine translation system evaluated on tst2012 and tst2013 using baseline GIZA++ alignment and transliteration augmented-GIZA++ alignment and post-processed the output by transliterating OOVs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9992307424545288}, {"text": "English to Russian machine translation", "start_pos": 116, "end_pos": 154, "type": "TASK", "confidence": 0.6824841797351837}]}, {"text": "Human evaluation in WMT13 is performed on TA-GIZA++ tested on tst2013 (marked with *)", "labels": [], "entities": [{"text": "WMT13", "start_pos": 20, "end_pos": 25, "type": "TASK", "confidence": 0.535615861415863}, {"text": "TA-GIZA", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9286929965019226}]}], "tableCaptions": [{"text": " Table 1: BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 us- ing baseline GIZA++ alignment and translitera- tion augmented-GIZA++. OOV-TI presents the  score of the system trained using TA-GIZA++ af- ter transliterating OOVs", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985707998275757}, {"text": "English to Russian ma- chine translation", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.5751174262591771}]}, {"text": " Table 2: BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 and  tst2013 using baseline GIZA++ alignment and  transliteration augmented-GIZA++ alignment and  post-processed the output by transliterating OOVs.  Human evaluation in WMT13 is performed on  TA-GIZA++ tested on tst2013 (marked with *)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989625215530396}, {"text": "English to Russian ma- chine translation", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.583558589220047}, {"text": "WMT13", "start_pos": 264, "end_pos": 269, "type": "DATASET", "confidence": 0.841316282749176}]}, {"text": " Table 3: Russian to English machine translation  system evaluated on tst2012 and tst2013. Human  evaluation in WMT13 is performed on the system  trained using the original corpus with TA-GIZA++  for alignment (marked with *)", "labels": [], "entities": [{"text": "Russian to English machine translation", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.5461017370224}, {"text": "WMT13", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.8023430109024048}]}]}