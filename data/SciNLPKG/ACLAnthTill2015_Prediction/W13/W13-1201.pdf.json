{"title": [{"text": "Coping With Implicit Arguments And Events Coreference", "labels": [], "entities": [{"text": "Coreference", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.6182754635810852}]}], "abstractContent": [{"text": "In this paper we present ongoing work for the creation of a linguistically-based system for event coreference.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7666416466236115}]}, {"text": "We assume that this task requires deep understanding of text and that statistically-based methods, both supervised and unsupervised are inadequate.", "labels": [], "entities": []}, {"text": "The reason for this choice is due to the fact that event coreference can only take place whenever argumenthood is properly computed.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7487382292747498}]}, {"text": "It is a fact that in many cases, arguments of predicates are implicit and thus linguistically unexpressed.", "labels": [], "entities": []}, {"text": "This prevents training to produce sensible results.", "labels": [], "entities": []}, {"text": "We also assume that spatiotemporal locations need to betaken into account and this is also very often left implicit.", "labels": [], "entities": []}, {"text": "We used GETARUNS system to develop the coreference system which works on the basis of the discourse model and the automatically annotated markables.", "labels": [], "entities": [{"text": "GETARUNS", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.5945289731025696}]}, {"text": "We present data from the analysis, both on unexpressed implicit arguments and the description of the coreference algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP processing is more and more oriented towards semantic processing which in turn requires deep understanding of texts.", "labels": [], "entities": [{"text": "NLP processing", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6419566720724106}, {"text": "semantic processing", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.771012932062149}]}, {"text": "We assume that this is only possible if unexpressed implicit linguistic elements and semantically deficient items are taken into consideration).", "labels": [], "entities": []}, {"text": "One of the first problem in the analysis of any text is accounting for implicit or linguistically unexpressed information.", "labels": [], "entities": []}, {"text": "This kind of information is not available in dependecy-based current annotated corpora or is only partially available -as in Penn Treebank -but it cannot possibly be learnt.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.9945769011974335}]}, {"text": "The problem of null and pronominal elements is paramount in the recovery of Predicate-Argument Structures which constitutes the fundamental element onto which propositional semantics is made to work.", "labels": [], "entities": []}, {"text": "However, applying machine learning techniques on available treebanks is of no help.", "labels": [], "entities": []}, {"text": "State of the art systems are using more and more dependency representations which have lately shown great resiliency, robustness, scalability and great adaptability for semantic enrichment and processing.", "labels": [], "entities": []}, {"text": "However, by far the majority of systems available off the shelf don't support a fully semantically consistent representation and lack Null Elements or Antecedents for pronominal ones.", "labels": [], "entities": []}, {"text": "If we limit ourselves to Null Elements, and to PennTreebank (hence PT), we may note that Marcus ('94) referred explicitly to PredicateArgument Structures (hence PASs) and to the need to address this level of annotation.", "labels": [], "entities": [{"text": "PennTreebank", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.9854198098182678}]}, {"text": "He mentions explicitly that \"we intend to automatically extract a bank of PASs intended at the very least for parser evaluation from the resulting annotated corpus\" and further on \"the notation should make it easy to automatically recover PAS\" (ibid. 121).", "labels": [], "entities": []}, {"text": "He also mentions the need to allow fora clear and concise distinction between verb ARGUMENTs and ADJUNCTs, which he asserts to be very difficult to make, consistently.", "labels": [], "entities": [{"text": "ARGUMENTs", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9665610194206238}, {"text": "ADJUNCTs", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.6284356713294983}]}, {"text": "This happens to be true: the final version of PT II does not include coindexing in controversial cases and has coindexing for null SBJ only in a percentage of the cases.", "labels": [], "entities": [{"text": "PT II", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.7442367076873779}]}, {"text": "PT contains 36862 cases of null elements (including traces, expletives, gapping and ambiguity) as listed in, over 93532 simple clauses and 55600 utterances, fora percentage of 66.3%.", "labels": [], "entities": [{"text": "PT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8818565011024475}]}, {"text": "Of course this number does not include pronominal arguments which need to be bound -and are not bound in PT -to an antecedent in order to become semantically consistent.", "labels": [], "entities": [{"text": "PT", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9708502292633057}]}, {"text": "As to PT, the difficulty of the task is testified by the presence of non coindexed Null Elements: in particular we see that they are 8416, that is 22.83%.", "labels": [], "entities": [{"text": "PT", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9903254508972168}]}, {"text": "If we exclude all traces of WH and topicalization and limit ourselves to the category OTHER TRACES which includes all unexpressed SUBJects of infinitivals and gerundives, we come up with 12172 cases of Null non-coindexed elements, 33% of all cases.", "labels": [], "entities": [{"text": "OTHER TRACES", "start_pos": 86, "end_pos": 98, "type": "METRIC", "confidence": 0.8001744151115417}]}, {"text": "We should note that for how much large this number may seem, this still represents a small percentage when compared to the number of null elements in languages like Chinese or Romance languages like Italian, which allow for free null subjects insertion in tensed clauses.", "labels": [], "entities": []}, {"text": "Current statistically dependency parsers have made improvements in enriching their structural output representation ().", "labels": [], "entities": [{"text": "statistically dependency parsers", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.5735449890295664}]}, {"text": "However, coindexation is not always performed: when it is, its performance is computed separately because it is lower than accuracy for labeled/unlabeled tasks.", "labels": [], "entities": [{"text": "coindexation", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.9142308235168457}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9983456134796143}]}, {"text": "In particular, Schmid reports 84% F-score for empty elements prediction and 77% for coindexation on PT.", "labels": [], "entities": [{"text": "F-score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9995759129524231}, {"text": "empty elements prediction", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6485341389973959}, {"text": "coindexation", "start_pos": 84, "end_pos": 96, "type": "METRIC", "confidence": 0.873144268989563}, {"text": "PT", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.7380123734474182}]}, {"text": "However, other parsers have much worse results, with Johnson(2001) being the worst, with 68% F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9987713694572449}]}, {"text": "The presence of additional difficulties to predict empty categories is the cause of a bad drop in performance in Chinese -no more than 50% accuracy reported by compared to 74/77% of the labeled/unlabeled task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9990517497062683}]}, {"text": "Results reported by on recovering labeled empty elements in an experiment carried on a small subset of the Penn Chinese Treebank 6.0 reach an average of 60.5% of F-measure.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 6.0", "start_pos": 107, "end_pos": 132, "type": "DATASET", "confidence": 0.986359030008316}, {"text": "F-measure", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9919020533561707}]}, {"text": "As to recovery of specific items, we note that over a total number of 290 little_pro items recall fares around 50%.", "labels": [], "entities": [{"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9689064621925354}]}, {"text": "Of course the phenomenon is very much language dependent, as discussed above.", "labels": [], "entities": []}, {"text": "If we consider a language like Italian -which we described fully from structures annotated in the treebank called VIT (Delmonte 2004) -we can see that in addition to untensed sentences also simple clauses with tensed verbs show the same problem.", "labels": [], "entities": []}, {"text": "In fact, over 66.5% (9634 over 15874) of all simple clauses are subjectless, they have an omitted or unexpressed subject which is marked in linguistics with a little_pro and the agreement coming from morphology of the main verb.", "labels": [], "entities": []}, {"text": "Of the remaining lexically expressed subjects, only 64% (6166 over 9634) are in canonical position, that is in preverbal position and adjacent to the inflected verb.", "labels": [], "entities": []}, {"text": "The remaining 36% of lexically expresses subjects are positioned to the right or are separated from the verb by other constituents.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the coreference module with the sample text and produced the following output that we comment in this section.", "labels": [], "entities": []}, {"text": "For each event we have two vectors of information that we then use to evaluate its relevance and its possible coreference in the previous text.", "labels": [], "entities": []}, {"text": "The categories used are fully explained in and here we limit ourselves to a short description.", "labels": [], "entities": []}, {"text": "The event maybe a verb and be related to a propositional analysis or be a noun.", "labels": [], "entities": []}, {"text": "Nouns classified as activity or events are selected as markables: this classification is partially derived from NomBank associated information about eventive nominals.", "labels": [], "entities": []}, {"text": "Coreference links are activated by synonymity or just similarity, measured by WordNet synset, a Thesaurus or sharing identical semantic classes as indicated in SUMO-MILO or other similar computational lexica.", "labels": [], "entities": []}, {"text": "The certainty value varies accordingly: from more certain, say .9, to less certain .4.", "labels": [], "entities": [{"text": "certainty", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9985085129737854}]}, {"text": "Obviously, copulative predications are marked with certainty equal to 1 being properties predicated in the syntax of the subject.", "labels": [], "entities": [{"text": "copulative predications", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.6903178691864014}]}], "tableCaptions": [{"text": " Table 1. total counts for  the 13 texts distributed with the Event Coreference  Task. The system computed automatically  Controllers and Antecedents: the first are referred  to syntactically controlled Null Elements of  Relative and Interrogative Clauses. The second are  referred to SUBJects of infinitivals, and other  predicative structures both argumental and non- argumental. The table also includes counts of  Markables and Coreferent Links, again computed  automatically. There is no evaluation yet available.  What we wanted to show is the proportion of NEs", "labels": [], "entities": []}, {"text": " Table 1. Null Elements, Markables and Coreferents  automatically computed by Getaruns on the 13  texts of the Task", "labels": [], "entities": []}]}