{"title": [{"text": "SHEF-Lite: When Less is More for Translation Quality Estimation", "labels": [], "entities": [{"text": "Translation Quality Estimation", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8225774963696798}]}], "abstractContent": [{"text": "We describe the results of our submissions to the WMT13 Shared Task on Quality Estimation (subtasks 1.1 and 1.3).", "labels": [], "entities": [{"text": "WMT13 Shared Task on Quality Estimation", "start_pos": 50, "end_pos": 89, "type": "TASK", "confidence": 0.6827968060970306}]}, {"text": "Our submissions use the framework of Gaus-sian Processes to investigate lightweight approaches for this problem.", "labels": [], "entities": []}, {"text": "We focus on two approaches, one based on feature selection and another based on active learning.", "labels": [], "entities": []}, {"text": "Using only 25 (out of 160) features , our model resulting from feature selection ranked 1st place in the scoring variant of subtask 1.1 and 3rd place in the ranking variant of the subtask, while the active learning model reached 2nd place in the scoring variant using only \u223c25% of the available instances for training.", "labels": [], "entities": []}, {"text": "These results give evidence that Gaussian Processes achieve the state of the art performance as a modelling approach for translation quality estimation, and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive.", "labels": [], "entities": [{"text": "translation quality estimation", "start_pos": 121, "end_pos": 151, "type": "TASK", "confidence": 0.9109792709350586}]}], "introductionContent": [{"text": "The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (.", "labels": [], "entities": [{"text": "machine translation (MT) quality estimation (QE)", "start_pos": 15, "end_pos": 63, "type": "TASK", "confidence": 0.859917038679123}]}, {"text": "A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence.", "labels": [], "entities": []}, {"text": "The WMT13 QE shared task defined a group of tasks related to QE.", "labels": [], "entities": [{"text": "WMT13 QE shared task", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8551027029752731}]}, {"text": "In this paper, we present the submissions by the University of Sheffield team.", "labels": [], "entities": [{"text": "University of Sheffield team", "start_pos": 49, "end_pos": 77, "type": "DATASET", "confidence": 0.7998486161231995}]}, {"text": "Our models are based on Gaussian Processes (GP)), a non-parametric probabilistic framework.", "labels": [], "entities": []}, {"text": "We explore the application of GP models in two contexts: 1) improving the prediction performance by applying a feature selection step based on optimised hyperparameters and 2) reducing the dataset size (and therefore the annotation effort) by performing Active Learning (AL).", "labels": [], "entities": []}, {"text": "We submitted entries for two of the four proposed tasks.", "labels": [], "entities": []}, {"text": "Task 1.1 focused on predicting HTER scores (Human Translation Error Rate)) using a dataset composed of 2254 EnglishSpanish news sentences translated by Moses () and post-edited by a professional translator.", "labels": [], "entities": [{"text": "HTER scores (Human Translation Error Rate))", "start_pos": 31, "end_pos": 74, "type": "METRIC", "confidence": 0.6450972110033035}]}, {"text": "The evaluation used a blind test set, measuring MAE (Mean Absolute Error) and RMSE (Root Mean Square Error), in the case of the scoring variant, and DeltaAvg and Spearman's rank correlation in the case of the ranking variant.", "labels": [], "entities": [{"text": "MAE (Mean Absolute Error)", "start_pos": 48, "end_pos": 73, "type": "METRIC", "confidence": 0.8326767981052399}, {"text": "RMSE (Root Mean Square Error)", "start_pos": 78, "end_pos": 107, "type": "METRIC", "confidence": 0.9190423488616943}, {"text": "Spearman's rank correlation", "start_pos": 162, "end_pos": 189, "type": "METRIC", "confidence": 0.5457710027694702}]}, {"text": "Our submissions reached 1st (feature selection) and 2nd (active learning) places in the scoring variant, the task the models were optimised for, and outperformed the baseline by a large margin in the ranking variant.", "labels": [], "entities": []}, {"text": "The aim of task 1.3 aimed at predicting postediting time using a dataset composed of 800 English-Spanish news sentences also translated by Moses but post-edited by five expert translators.", "labels": [], "entities": [{"text": "predicting postediting time", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.8676300843556722}]}, {"text": "Evaluation was done based on MAE and RMSE on a blind test set.", "labels": [], "entities": [{"text": "MAE", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9754369854927063}, {"text": "RMSE", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.8821484446525574}]}, {"text": "For this task our models were notable to beat the baseline system, showing that more advanced modelling techniques should have been used for challenging quality annotation types and datasets such as this.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Submission results for tasks 1.1 and 1.3. The bold value shows a winning entry in the shared  task.", "labels": [], "entities": []}]}