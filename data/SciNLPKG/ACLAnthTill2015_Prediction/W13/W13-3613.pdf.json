{"title": [{"text": "Rule-based System for Automatic Grammar Correction Using Syntactic N-grams for English Language Learning (L2)", "labels": [], "entities": [{"text": "Automatic Grammar Correction", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6587546467781067}]}], "abstractContent": [{"text": "We describe the system developed for the CoNLL-2013 shared task-automatic En-glish L2 grammar error correction.", "labels": [], "entities": [{"text": "CoNLL-2013 shared task-automatic En-glish L2 grammar error correction", "start_pos": 41, "end_pos": 110, "type": "TASK", "confidence": 0.6761076226830482}]}, {"text": "The system is based on the rule-based approach.", "labels": [], "entities": []}, {"text": "It uses very few additional resources: a morphological analyzer and a list of 250 common uncountable nouns, along with the training data provided by the organizers.", "labels": [], "entities": []}, {"text": "The system uses the syntactic information available in the training data: this information is represented as syntactic n-grams, i.e. n-grams extracted by following the paths in dependency trees.", "labels": [], "entities": []}, {"text": "The system is simple and was developed in a short period of time (1 month).", "labels": [], "entities": []}, {"text": "Since it does not employ any additional resources or any sophisticated machine learning methods, it does not achieve high scores (specifically, it has low recall) but could be considered as a baseline system for the task.", "labels": [], "entities": [{"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9985858201980591}]}, {"text": "On the other hand, it shows what can be obtained using a simple rule-based approach and presents a few situations where the rule-based approach can perform better than ML approach .", "labels": [], "entities": []}], "introductionContent": [{"text": "There are two main approaches in the design of the modern linguistic experiments and the development of the natural language processing applications: rule-based and machine learning-based.", "labels": [], "entities": []}, {"text": "In practical applications of machine learning (ML), the best results are achieved by the methods that use supervised learning, i.e., that are based on manually prepared training data for learning.", "labels": [], "entities": [{"text": "machine learning (ML)", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.8008994936943055}]}, {"text": "It is also worth mentioning what can be considered a general rule for the combination of these two approaches: a system based on the mixed approach should obtain better results if each part of the system is applied according to its \"competence\".", "labels": [], "entities": []}, {"text": "Specifically, some problems are better solved by the application of the rules-like the rules for choosing the correct allomorph of the article \"a\" vs. \"an\", while other problems are better solved by the usage of ML methods-such as deciding the presence or absence of a definite or an indefinite determiner.", "labels": [], "entities": []}, {"text": "This paper describes the system developed for the CoNLL-2013 shared task.", "labels": [], "entities": [{"text": "CoNLL-2013 shared task", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.6127605239550272}]}, {"text": "The task consists of grammar correction in texts written by people learning English as a second language (L2).", "labels": [], "entities": [{"text": "grammar correction in texts written by people learning English as a second language", "start_pos": 21, "end_pos": 104, "type": "TASK", "confidence": 0.8220217617658468}]}, {"text": "There are five types of errors considered in the task: noun number, subject-verb agreement, verb form, article/determiner and choice of preposition.", "labels": [], "entities": []}, {"text": "The training data processed by the Stanford parser (de) is provided.", "labels": [], "entities": []}, {"text": "This data is part of the NUCLE corpus (.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9780218601226807}]}, {"text": "The data also contains the error types and the corrected version.", "labels": [], "entities": []}, {"text": "Development of the system was started only two months before the deadline, so it is also an interesting example of what can be done in a rather short period of time and with relatively little effort: only one person-month joint effort in total.", "labels": [], "entities": []}, {"text": "In our system, we considered mainly the rulebased approach.", "labels": [], "entities": []}, {"text": "Note that we used the ConLL data to extract preposition patterns, which can be considered as a very reduced form of machine learning with yes/no classifier, as well as to construct rules directly from the data.", "labels": [], "entities": [{"text": "ConLL data", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.8181625306606293}]}, {"text": "Another feature of our system is the widespread use of the syntactic information present in the provided data.", "labels": [], "entities": []}, {"text": "In our previous works, we generalized the use of syntactic information in NLP by introducing the concept of syntactic n-grams, i.e. n-grams constructed by following the dependency paths in a syntactic tree.", "labels": [], "entities": []}, {"text": "Note that they are not ngrams of POS tags, as could be assumed from the name; the name refers to the manner in which they: Example of syntactic tree (for extraction of syntactic n-grams). are constructed.", "labels": [], "entities": []}, {"text": "That is to say, in a dependency relation, there is always ahead word and a dependent word.", "labels": [], "entities": []}, {"text": "In the syntactic tree, this relation is graphically represented by an arrow: head \u2192 dependent.", "labels": [], "entities": []}, {"text": "As it can be observed in, we can also use the tree hierarchy-the headword is always \"higher\" in the syntactic tree.", "labels": [], "entities": []}, {"text": "The algorithm for the construction of syntactic n-grams is as follows: we start from the root word and move to each dependent word following the dependency relations.", "labels": [], "entities": []}, {"text": "At each step, the sequence of previous elements in the route taken are taken into account.", "labels": [], "entities": []}, {"text": "The last n words in the sequence correspond to the syntactic n-gram.", "labels": [], "entities": []}, {"text": "This could be reformulated as: we should take the last n words of the (unique) path from the root to the current word.", "labels": [], "entities": []}, {"text": "In other words, we start from the root and reach one of the dependent words.", "labels": [], "entities": []}, {"text": "If we want to construct bigrams, then we have a bigram already.", "labels": [], "entities": []}, {"text": "If we need other elements of the n-gram, then we move to the word that is dependent and continue to the words that are dependent on it.", "labels": [], "entities": []}, {"text": "If a word has several dependent words, we consider them one after another and thus, obtain several syntactic ngrams.", "labels": [], "entities": []}, {"text": "Note that the headword always appears before the dependent word in the syntactic n-gram during the construction process.", "labels": [], "entities": []}, {"text": "For example, from the tree presented in, the following syntactic bigrams can be extracted: likes-also, likes-dog, dog-my, likes-eating, eatingsausage.", "labels": [], "entities": []}, {"text": "Note that only two syntactic 3-grams can be constructed: likes-dog-my, likes-eatingsausage.", "labels": [], "entities": []}, {"text": "The construction process is the following: we start with the root word like.", "labels": [], "entities": []}, {"text": "It has several dependent words: dog, also, eating.", "labels": [], "entities": []}, {"text": "Considering them one after another, we obtain three syntactic bigrams.", "labels": [], "entities": []}, {"text": "Then we move onto the word dog.", "labels": [], "entities": []}, {"text": "It has only one dependent word: my.", "labels": [], "entities": []}, {"text": "This is another bigram dog-my.", "labels": [], "entities": []}, {"text": "However, the path from like also goes through it, so this is also the 3-gram like-dogmy, etc.", "labels": [], "entities": []}, {"text": "The reader can compare these syntactic n-grams with traditional n-grams and consider their advantages: there area lot less syntactic n-grams, they are less arbitrary, they have linguistic interpretation, etc.", "labels": [], "entities": []}, {"text": "Note that syntactic n-grams can be formed by words (lemmas, stems), POS tags, names of dependency relations, or they can be mixed, i.e., a combination of the mentioned types.", "labels": [], "entities": []}, {"text": "Being ngrams, they can be applied in any machine learning task where traditional n-grams are applied.", "labels": [], "entities": []}, {"text": "However, unlike traditional n-grams, they have a clear linguistic interpretation and can be considered as an introduction of linguistic (syntactic) information into machine learning methods.", "labels": [], "entities": []}, {"text": "Previously, we obtained better results by applying the syntactic n-grams to opinion mining and authorship attribution tasks compared to the traditional n-grams.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7995623052120209}, {"text": "authorship attribution", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.8484916090965271}]}, {"text": "Further in this paper, it is described how we use syntactic n-grams for the formulation of rules in our system and for the extraction of patterns.", "labels": [], "entities": []}, {"text": "The system described in this paper does not obtain high scores.", "labels": [], "entities": []}, {"text": "In our opinion, it could be considered a baseline system for the grammar correction task due to its simplicity, its use of very few additional resources and the speed of its development.", "labels": [], "entities": [{"text": "grammar correction task", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.8831121722857157}]}, {"text": "Concretely, if a more sophisticated system outperforms ours, it reflects well upon that system.", "labels": [], "entities": []}, {"text": "If it performs more poorly, its design should be revised.", "labels": [], "entities": []}, {"text": "On the other hand, this paper also discusses the few situations where the rule-based system can outperform an ML approach.", "labels": [], "entities": [{"text": "ML", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9506685733795166}]}, {"text": "As we mentioned earlier, the ideal system would combine both these approaches.", "labels": [], "entities": []}, {"text": "To quote, \"don't guess if you know\".", "labels": [], "entities": []}, {"text": "Further below, we describe the lexical resources that we used, the processing of each type of error and the evaluation of the system.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation, the organizers provided data similar to the training data from the same NU-CLE corpus, which also contained syntactic information.", "labels": [], "entities": [{"text": "NU-CLE corpus", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9606501162052155}]}, {"text": "The evaluation results were provided by the organizers using their evaluation script in Python).", "labels": [], "entities": []}, {"text": "The results obtained with this script for our system are: precision 17.4 %, recall 1.8%, and F1 measure 3.3% (the preliminary scores were: 12.4%, 1.2% and 2.2% correspondingly).", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9997701048851013}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9996142387390137}, {"text": "F1 measure", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9892051815986633}]}, {"text": "See the final remarks in this section, where we argue that the real values should be: precision 25%, recall 2.6%, and F1 measure 4.7%.", "labels": [], "entities": [{"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9997072815895081}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9995512366294861}, {"text": "F1 measure 4.7", "start_pos": 118, "end_pos": 132, "type": "METRIC", "confidence": 0.9740461707115173}]}, {"text": "The results are low, but as we mentioned previously, our system uses a rule-based approach with very few additional resources, so it cannot compete with ML based approaches that additionally rely on vast lexical resources and the Internet.", "labels": [], "entities": []}, {"text": "Due to its simplicity, low use of additional resources, and very short development time, we consider our system a possible baseline system for the task.", "labels": [], "entities": []}, {"text": "On the other hand, we showed that in some cases the rules should be used as a complementary technique for ML learning methods: don't guess if you know.", "labels": [], "entities": [{"text": "ML learning", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9388010203838348}]}, {"text": "The low recall of the system is to be expected as we process only clearly defined errors, ignoring more complex cases.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9995874762535095}]}, {"text": "It is always interesting to perform an analysis of the errors committed by a system.", "labels": [], "entities": []}, {"text": "Let us analyze the supposed errors committed by our system for the noun number error type.", "labels": [], "entities": []}, {"text": "It performed 18 corrections, 3 of which coincide with the marks in the corpus data.", "labels": [], "entities": []}, {"text": "Two of them are clear errors of the system: \"traffic jam\", where the word \"jam\" is used in a sense other than that of the \"substance\", and \"many respects\", where again the word \"respect\" has a different meaning to that of the uncountable noun.", "labels": [], "entities": [{"text": "traffic jam", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.6563796699047089}]}, {"text": "There are 13 cases listed below, that our system marked as errors, because they are uncountable nouns in plural, but they are not marked in the corpus.", "labels": [], "entities": []}, {"text": "Let us consider the nouns in capital letters: Note that the words \"equipment\" and \"usage\" in plural were marked as errors in the corpus.", "labels": [], "entities": []}, {"text": "In our opinion, it is inconsistent to mark these two as errors, and not to mark the words from this list as such.", "labels": [], "entities": []}, {"text": "While it is true that their use in plural is possible, it is clearly forced and is much less probable.", "labels": [], "entities": []}, {"text": "At least, students of English should learn to use these words in singular only.", "labels": [], "entities": []}, {"text": "Some of these mistakes (but not all) were corrected by the organizers for the final scoring data.", "labels": [], "entities": []}, {"text": "If we consider all these cases as correctly marked errors, then the precision of our system is around 25%, recall 2.6%, and F1 measure 4.7%.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9998372793197632}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9992944002151489}, {"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9996601343154907}]}], "tableCaptions": [{"text": " Table 1: Numbers of errors in training and test data  listed by type.", "labels": [], "entities": []}]}