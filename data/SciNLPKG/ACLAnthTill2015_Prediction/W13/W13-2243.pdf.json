{"title": [{"text": "FBK-UEdin participation to the WMT13 Quality Estimation shared-task", "labels": [], "entities": [{"text": "FBK-UEdin", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7418526411056519}, {"text": "WMT13 Quality Estimation shared-task", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.559368222951889}]}], "abstractContent": [{"text": "In this paper we present the approach and system setup of the joint participation of Fondazione Bruno Kessler and University of Edinburgh in the WMT 2013 Quality Estimation shared-task.", "labels": [], "entities": [{"text": "WMT 2013 Quality Estimation shared-task", "start_pos": 145, "end_pos": 184, "type": "TASK", "confidence": 0.6279036045074463}]}, {"text": "Our submissions were focused on tasks whose aim was predicting sentence-level Human-mediated Translation Edit Rate and sentence-level post-editing time (Task 1.1 and 1.3, respectively).", "labels": [], "entities": [{"text": "predicting sentence-level Human-mediated Translation Edit Rate", "start_pos": 52, "end_pos": 114, "type": "TASK", "confidence": 0.5316815425952276}]}, {"text": "We designed features that are built on resources such as automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.720267727971077}]}, {"text": "Our models consistently overcome the baselines for both tasks and performed particularly well for Task 1.3, ranking first among seven participants .", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) for Machine Translation (MT) is the task of evaluating the quality of the output of an MT system without relying on reference translations.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.58520627617836}, {"text": "Machine Translation (MT)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8666728258132934}]}, {"text": "The WMT 2013 QE Shared Task defined four different tasks covering both word and sentence level QE.", "labels": [], "entities": [{"text": "WMT 2013 QE Shared Task", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8502866268157959}]}, {"text": "In this work we describe the Fondazione Bruno Kessler (FBK) and University of Edinburgh approach and system setup of our participation to the shared task.", "labels": [], "entities": [{"text": "Fondazione Bruno Kessler (FBK)", "start_pos": 29, "end_pos": 59, "type": "DATASET", "confidence": 0.4981129765510559}]}, {"text": "We developed models for two sentence-level tasks: Task 1.1: Scoring and ranking for post-editing effort, and Task 1.3: Predicting post-editing time.", "labels": [], "entities": []}, {"text": "The first task aims at predicting the Humanmediated Translation Edit Rate (HTER)) between a suggestion generated by a machine translation system and its manually post-edited version.", "labels": [], "entities": [{"text": "Humanmediated Translation Edit Rate (HTER))", "start_pos": 38, "end_pos": 81, "type": "METRIC", "confidence": 0.735153249331883}]}, {"text": "The data set contains 2,754 English-Spanish sentence pairs post-edited by one translator (2,254 for training and 500 for test).", "labels": [], "entities": []}, {"text": "We participated only in the scoring mode of this task.", "labels": [], "entities": []}, {"text": "The second task requires to predict the time, in seconds, that was required to post edit a translation given by a machine translation system.", "labels": [], "entities": []}, {"text": "Participants are provided with 1,087 English-Spanish sentence pairs, source and suggestion, along with their respective post-edited sentence and postediting time in seconds (803 data points for training and 284 for test).", "labels": [], "entities": []}, {"text": "For both tasks we applied supervised learning methods and made use of information about word alignments, n-best diversity scores, word posterior probabilities, pseudo-references, and back translation to train our models.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.6731065958738327}]}, {"text": "In the remainder of this paper we describe the features designed for our participation (Section 2), the learning methods used to build our models (Section 3), the experiments that led to our submitted systems (Section 4), and we briefly conclude our experience in this evaluation task (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "For both tasks we setup a baseline system that uses the same 17 black box \"baseline\" features provided for the WMT 2012 QE shared task).", "labels": [], "entities": [{"text": "WMT 2012 QE shared task", "start_pos": 111, "end_pos": 134, "type": "DATASET", "confidence": 0.8090303421020508}]}, {"text": "The baseline model is trained with an SVM regression with RBF kernel and optimized parameters.", "labels": [], "entities": []}, {"text": "Parameter optimization for SVM regression models was performed with 1000 iterations of random search for which the process was set to minimize the mean absolute error (MAE) 2 . The parameters of SVR with RBF kernel (the penalty parameter C, the width of the insensitivity zone , and the RBF parameter \u03b3) are sampled from an exponential distribution.", "labels": [], "entities": [{"text": "SVM regression", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9288374781608582}, {"text": "mean absolute error (MAE)", "start_pos": 147, "end_pos": 172, "type": "METRIC", "confidence": 0.8819611072540283}, {"text": "RBF parameter \u03b3", "start_pos": 287, "end_pos": 302, "type": "METRIC", "confidence": 0.9022529323895773}]}, {"text": "Experiments for both tasks were run using 10-fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": "In Task 1.3 some data points were annotated by 2 or more post-editors and, in a normal crossvalidation scheme, the same data point might appear in the training and test set but annotated by different post-editors.", "labels": [], "entities": []}, {"text": "To address this characteristic we implemented a cross-validation that divides along source sentences, so that all translations of a source segment end up in either the training or test portion of a split.", "labels": [], "entities": []}, {"text": "The number of features available for both tasks is not the same (112 for Task 1.1 and 141 for Task 1.3) because there were fewer nbest diversity, pseudo-references and word posterior probability based features developed with different parameters due to time constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiments results for Task 1.1 on 10-fold cross-validation. \"Base\" are the 17 baseline features.  \"All\" corresponds to all the features described in Section 2 in a total of 141 features. \"SVR\" is support  vector regression, \"RL\" is randomized Lasso and \"ET\" is extremely randomized trees. MAE stands for  the average mean absolute error and RMSE is the root mean squared error. Parameters for SVR are C, ,  \u03b3 and for ET is the number of estimators.", "labels": [], "entities": []}, {"text": " Table 2: Experiments results for Task 1.3 on 10-fold cross-validation. \"Base\" are the 17 baseline features.  \"All\" corresponds to all the features described in Section 2 in a total of 141 features. \"SVR\" is support  vector regression, \"RL\" is randomized Lasso and \"ET\" is extremely randomized trees. MAE stands for  the average mean absolute error and RMSE is the root mean squared error. Parameters for SVR are C, ,  \u03b3 and for ET is the number of estimators.", "labels": [], "entities": []}, {"text": " Table 3: Official results for tasks 1.1 and 1.3 on  the test set.", "labels": [], "entities": []}]}