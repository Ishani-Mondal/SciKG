{"title": [{"text": "Detecting Narrativity to Improve English to French Translation of Simple Past Verbs", "labels": [], "entities": [{"text": "Detecting Narrativity to Improve English to French Translation of Simple Past Verbs", "start_pos": 0, "end_pos": 83, "type": "TASK", "confidence": 0.733645794292291}]}], "abstractContent": [{"text": "The correct translation of verb tenses ensures that the temporal ordering of events in the source text is maintained in the target text.", "labels": [], "entities": [{"text": "translation of verb tenses", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7899960577487946}]}, {"text": "This paper assesses the utility of automatically labeling English Simple Past verbs with a binary discursive feature , narrative vs. non-narrative, for statistical machine translation (SMT) into French.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 152, "end_pos": 189, "type": "TASK", "confidence": 0.7962843825419744}]}, {"text": "The narrativity feature, which helps deciding which of the French past tenses is a correct translation of the En-glish Simple Past, can be assigned with about 70% accuracy (F1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9993124008178711}, {"text": "F1", "start_pos": 173, "end_pos": 175, "type": "METRIC", "confidence": 0.9948817491531372}]}, {"text": "The narrativity feature improves SMT by about 0.2 BLEU points when a factored SMT system is trained and tested on automatically labeled English-French data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.992296040058136}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.999258816242218}, {"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9426419138908386}]}, {"text": "More importantly, manual evaluation shows that verb tense translation and verb choice are improved by respectively 9.7% and 3.4% (absolute), leading to an overall improvement of verb translation of 17% (relative).", "labels": [], "entities": [{"text": "verb tense translation", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.640712151924769}, {"text": "verb translation", "start_pos": 178, "end_pos": 194, "type": "TASK", "confidence": 0.6995929926633835}]}], "introductionContent": [{"text": "The correct rendering of verbal tenses is an important aspect of translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9792891144752502}]}, {"text": "Translating to a wrong verbal tense in the target language does not convey the same meaning as the source text, for instance by distorting the temporal order of the events described in a text.", "labels": [], "entities": [{"text": "Translating to a wrong verbal tense", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7835773328940073}]}, {"text": "Current statistical machine translation (SMT) systems may have difficulties in choosing the correct verb tense translations, in some language pairs, because these depend on a wider-range context than SMT systems consider.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.7688262561957041}, {"text": "SMT", "start_pos": 200, "end_pos": 203, "type": "TASK", "confidence": 0.9512895345687866}]}, {"text": "Indeed, decoding for SMT is still at the phrase or sentence level only, thus missing information from previously translated sentences (which is also detrimental to lexical cohesion and co-reference).", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9841146469116211}]}, {"text": "In this paper, we explore the merits of a discourse feature called narrativity in helping SMT systems to improve their translation choices for English verbs in the Simple Past tense (henceforth, SP) into one of the three possible French past tenses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9934136271476746}]}, {"text": "The narrativity feature characterizes each occurrence of an SP verb, either as narrative (for ordered events that happened in the past) or non-narrative (for past states of affairs).", "labels": [], "entities": [{"text": "SP verb", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.8999512195587158}]}, {"text": "Narrativity is potentially relevant to EN/FR translation because three French past tenses can potentially translate an English Simple Past (SP), namely the Pass\u00e9 Compos\u00e9 (PC), Pass\u00e9 Simple (PS) or Imparfait (IMP).", "labels": [], "entities": [{"text": "EN/FR translation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.8436497151851654}, {"text": "Imparfait", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9083936810493469}]}, {"text": "All of them can be correct translations of an EN SP verb, depending on its narrative or non-narrative role.", "labels": [], "entities": [{"text": "EN SP verb", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.6019429167111715}]}, {"text": "The narrativity feature can be of use to SMT only if it can be assigned with sufficient precision over a source text by entirely automatic methods.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9916371703147888}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9850062727928162}]}, {"text": "Moreover, a narrativity-aware SMT model is likely to make a difference with respect to baseline SMT only if it is based on additional features that are not captured by, e.g., a phrase-based SMT model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8473522663116455}, {"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9387518763542175}, {"text": "SMT", "start_pos": 190, "end_pos": 193, "type": "TASK", "confidence": 0.7357377409934998}]}, {"text": "In this study, we use a small amount of manually labeled instances to train a narrativity classifier for English texts.", "labels": [], "entities": []}, {"text": "The (imperfect) output of this classifier over the English side of a large parallel corpus will then be used to train a narrativity-aware SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9059034585952759}]}, {"text": "In testing mode, the narrativity classifier provides input to the SMT system, resulting (as we will show below) in improved tense and lexical choices for verbs, and a modest but statistically significant increase in BLEU and TER scores.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9906484484672546}, {"text": "BLEU", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9995774626731873}, {"text": "TER scores", "start_pos": 225, "end_pos": 235, "type": "METRIC", "confidence": 0.9767321646213531}]}, {"text": "Overall, the method is similar in substance to our previous work on the combination of a classifier for discourse connectives with an SMT system ).", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9809112548828125}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 exemplifies the hypothesized relation between narrativity and the translations of the English Simple Past into French, along with related work on modeling tense for MT.", "labels": [], "entities": [{"text": "translations of the English Simple Past", "start_pos": 76, "end_pos": 115, "type": "TASK", "confidence": 0.7143871287504832}, {"text": "MT", "start_pos": 175, "end_pos": 177, "type": "TASK", "confidence": 0.9807012677192688}]}, {"text": "The automatic labeling experiments are presented in Section 3.", "labels": [], "entities": []}, {"text": "Experiments with SMT systems are presented in Section 4, with results from both automatic (4.3) and manual translation scoring (4.4), followed by a discussion of results and suggestions on improving them (Section 5).", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9896889925003052}]}], "datasetContent": [{"text": "In order to obtain reliable automatic evaluation scores, we executed three runs of MERT tuning for each type of translation model.", "labels": [], "entities": [{"text": "MERT", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.6753798723220825}]}, {"text": "With MERT being a randomized, non-deterministic optimization process, each run leads to different feature weights and, as a consequence, to different BLEU scores when translating unseen data.: Average values of BLEU (the higher the better) and TER (the lower the better) over three tuning runs for each model on 'newstest 2010'.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9985020160675049}, {"text": "BLEU", "start_pos": 211, "end_pos": 215, "type": "METRIC", "confidence": 0.9986587762832642}, {"text": "TER", "start_pos": 244, "end_pos": 247, "type": "METRIC", "confidence": 0.9986133575439453}, {"text": "newstest 2010'", "start_pos": 313, "end_pos": 327, "type": "DATASET", "confidence": 0.9393566449483236}]}, {"text": "The starred values are significantly better (p < 0.05) than the baseline.", "labels": [], "entities": []}, {"text": "In terms of overall BLEU and TER scores, the FACTORED model improves performance over the BASELINE by +0.2 BLEU and -0.2 TER (as lower is better), and these differences are statistically significant at the 95% level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9993261098861694}, {"text": "TER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9948965907096863}, {"text": "FACTORED", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8098044991493225}, {"text": "BASELINE", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9852312803268433}, {"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9980800151824951}, {"text": "TER", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9914668798446655}]}, {"text": "On the contrary, the concatenated-label model (noted TAGGED) slightly decreases the global translation performance compared to the BASELINE.", "labels": [], "entities": [{"text": "TAGGED", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9906214475631714}, {"text": "BASELINE", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.6838927268981934}]}, {"text": "A similar behavior was observed when using labeled connectives in combination with SMT . The lower scores of the TAGGED model maybe due to the scarcity of data (by a factor of 0.5) when verb word-forms are altered by concatenating them with the narrativity labels.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9083167910575867}, {"text": "TAGGED", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.8175202012062073}]}, {"text": "The small improvement by the FACTORED model of overall scores (such as BLEU) is also related to the scarcity of SP verbs: although their translation is improved, as we will now show, the translation of all other words is not changed by our method, so only a small fraction of the words in the test data are changed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9991125464439392}]}, {"text": "To assess the improvement specifically due to the narrativity labels, we manually evaluated the FR translations by the FACTORED model for the 207 first SP verbs in the test set against the translations from the BASELINE model.", "labels": [], "entities": [{"text": "FR translations", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.535770907998085}, {"text": "FACTORED", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.4910995662212372}, {"text": "BASELINE", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.7224680781364441}]}, {"text": "As the TAGGED model did not result in good scores, we did not further consider it for evaluation.", "labels": [], "entities": [{"text": "TAGGED", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.8835136890411377}]}, {"text": "Manual scoring was performed along the following criteria for each occurrence of an SP verb, by bilingual judges looking both at the source sentence and its reference translation.", "labels": [], "entities": []}, {"text": "\u2022 summarize the counts and percentages of improvements and/or degradations of translation quality with the systems FACTORED and BASELINE.", "labels": [], "entities": [{"text": "FACTORED", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.654871940612793}, {"text": "BASELINE", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9078725576400757}]}, {"text": "The correctness of the labels, as evaluated by the human judges on SMT test data, is similar to the values given in Section 3 when evaluated against the test sentences of the narrativity classifier.", "labels": [], "entities": [{"text": "SMT test data", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.8478328982988993}]}, {"text": "As shown in, the narrativity information clearly helps the FACTORED system to generate more accurate French verb tenses in almost 10% of the cases, and also helps to find more accurate vocabulary for verbs in 3.4% of the cases.", "labels": [], "entities": [{"text": "FACTORED", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.6004145741462708}]}, {"text": "Overall, as shown in, the FAC-TORED model yields more correct translations of the verb phrases than the BASELINE in 9% of the cases -a small but non-negligible improvement.", "labels": [], "entities": [{"text": "FAC-TORED", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.8918729424476624}, {"text": "BASELINE", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9887928366661072}]}, {"text": "An example from the test data shown in illustrates the improved verb translation.", "labels": [], "entities": [{"text": "verb translation", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7251221835613251}]}, {"text": "The BASELINE system translates the SP verb looked incorrectly into the verb consid\u00e9rer (consider), in wrong number and its past participle only (consid\u00e9r\u00e9s, plural).", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9553620219230652}, {"text": "SP verb", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.8927900791168213}]}, {"text": "The FACTORED model generates the correct tense and number (IMP, semblait, singular) and the better verb sembler.", "labels": [], "entities": [{"text": "FACTORED", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.5734927654266357}, {"text": "IMP", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.8384252190589905}]}, {"text": "This example is scored as follows: the labeling is correct ('yes'), the tense was improved ('+'), the lexical choice was improved too ('+'), the BASE-LINE was incorrect while the FACTORED model was correct.", "labels": [], "entities": [{"text": "BASE-LINE", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.988860011100769}]}], "tableCaptions": [{"text": " Table 1: Performance of MaxEnt and CRF clas- sifiers on narrativity. We report recall, precision,  their mean (F1), and the kappa value for class  agreement.", "labels": [], "entities": [{"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9994422793388367}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9996144771575928}, {"text": "their mean (F1)", "start_pos": 100, "end_pos": 115, "type": "METRIC", "confidence": 0.8243208765983582}]}, {"text": " Table 2: Confusion matrix for the labels output  by the MaxEnt classifier (System) versus the gold  standard labels (Reference).", "labels": [], "entities": []}, {"text": " Table 3: Average values of BLEU (the higher the  better) and TER (the lower the better) over three  tuning runs for each model on 'newstest 2010'.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9994695782661438}, {"text": "TER", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9992384910583496}, {"text": "newstest 2010'", "start_pos": 132, "end_pos": 146, "type": "DATASET", "confidence": 0.9053642749786377}]}, {"text": " Table 4: Human evaluation of verb translations  into French, comparing the FACTORED model  against the BASELINE. The \u2206 values show the  clear improvement of the narrativity-aware fac- tored translation model.", "labels": [], "entities": [{"text": "FACTORED", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.8621681332588196}, {"text": "BASELINE", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9243515729904175}]}, {"text": " Table 5: Human evaluation of the global cor- rectness of 207 translations of EN SP verbs into  French. The FACTORED model yields 9% more  correct translations than the BASELINE one.", "labels": [], "entities": [{"text": "FACTORED", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8273505568504333}, {"text": "BASELINE", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.8373209834098816}]}]}