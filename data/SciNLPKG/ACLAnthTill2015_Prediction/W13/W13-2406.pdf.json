{"title": [{"text": "Parsing Russian: a Hybrid Approach", "labels": [], "entities": [{"text": "Parsing Russian", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8449189364910126}]}], "abstractContent": [{"text": "We present an approach for natural language parsing in which dependency and constituency parses are acquired simultaneously.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6384201447168986}, {"text": "constituency parses", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.6560214012861252}]}, {"text": "This leads to accurate parses represented in a specific way, richer than constituency or dependency tree.", "labels": [], "entities": []}, {"text": "It also allows reducing parsing time complexity.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9798092842102051}]}, {"text": "Within the proposed approach, we show how to treat some significant phenomena of the Russian language and also perform a brief evaluation of the parser implementation , known as DictaScope Syntax.", "labels": [], "entities": []}], "introductionContent": [{"text": "A syntactic parser inputs a sentence and produces information on syntactic relationships between parts of the sentence.", "labels": [], "entities": []}, {"text": "It is an open question which method is the most convenient one to represent these relationships.", "labels": [], "entities": []}, {"text": "In this paper, we are focusing on two of those methods.", "labels": [], "entities": []}, {"text": "The first one, a constituency tree (CT), is a representation of a sentence by a set of nested fragments -groups, each group corresponding to a syntactically coherent phrase.", "labels": [], "entities": []}, {"text": "The second one, a dependency tree (DT), expresses relationships by a set of syntactic links between pairs of tokens.", "labels": [], "entities": []}, {"text": "demonstrates correspondence between CT and DT: one is clearly derivable from another.", "labels": [], "entities": []}, {"text": "In applications, one usually needs to transform CT into DT due to the following fact: if a tree is correct, then subjects, objects and adverbials of some predicate X are always direct children of the node X in DT.", "labels": [], "entities": []}, {"text": "With a traditional CT framework these children can be obtained in much less intuitive way by browsing up and down through constituents, as shown in by dotted lines.", "labels": [], "entities": []}, {"text": "According to this comparison, DT transparently maps onto the level of semantic representation,: A constituency tree (upper) and a dependency tree (lower) fora sentence \"A blue ball lies on the sand\".", "labels": [], "entities": []}, {"text": "thereby DT-s are considered most appropriate in applications like sentiment analysis and fact extraction.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9683584868907928}, {"text": "fact extraction", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.8232927024364471}]}, {"text": "Despite the usefulness of DT-s, CT-s have a longer history of application as a computational model.", "labels": [], "entities": []}, {"text": "For now, probabilistic constituency parser by and its derivatives are considered the state of the art for English.", "labels": [], "entities": [{"text": "constituency parser", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.5989199727773666}]}, {"text": "Unfortunately, the framework of constituency parsing, taken alone, is not productive for languages such as Russian.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.733287900686264}]}, {"text": "It turns out that the number of rules in a grammar start to grow fast if one tries to describe an inflecting language with a free word order explicitly.", "labels": [], "entities": []}, {"text": "As a result, pure constituency parsers are not well known for Russian.", "labels": [], "entities": []}, {"text": "It has recently been confirmed by a Russian syntactic parsers task at the Dialogue conference (see http://www.dialog-21.ru), at which several parsers were presented and all of them used DT formalism as a basis.", "labels": [], "entities": []}, {"text": "In examples, we put indices for words in correspondence with English translation (often with omitted articles \"a\", \"the\"), refer to any word by its index, and to a phrase by indices of its starting and finishing word.", "labels": [], "entities": []}, {"text": "Dependency tree is called projective if each subtree corresponds to a continuous fragment of the source sentence.", "labels": [], "entities": []}, {"text": "There is evidence that more than 80% of the sentences are usually projective in European natural languages.", "labels": [], "entities": []}, {"text": "A famous example of nonprojectivity for Russian is 1 \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf592 \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf593 \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf594 \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf595\ud97b\udf59 \"I1've raised4 a monument2 for myself3 not made by hands5\" from Pushkin, where link 4\u21921 overlaps 2\u21925.", "labels": [], "entities": []}, {"text": "It would be possible to resolve problems with homogenous parts if additional vertices could be added to the DT, like in.G, representing supplementary constituents with synthesized grammar values.", "labels": [], "entities": []}, {"text": "Unfortunately, known approaches for dependency parsing assume that all vertices are predefined before the algorithm starts, so it is impossible to include new vertices on the fly without inventing new parsing methods.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8119125962257385}]}, {"text": "The idea of combining dependencies and constituencies directly is not anew one.", "labels": [], "entities": []}, {"text": "For Russian, suggested designating a standard relation between predicate and subject by a syntactic link, along with adding a separate constituent for compound predicative groups like \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 from \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 1 2 \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 3 4 \ud97b\udf59 \"The driver 4 must 2 give 3 way 1 (to smb.).", "labels": [], "entities": []}, {"text": "\", which has a nonprojective DT.", "labels": [], "entities": []}, {"text": "This solution immediately reduces the number of overlapping dependency links for compound predicates, because links that tend to over-lap are packed inside the constituent.", "labels": [], "entities": []}, {"text": "In constituents for groups of homogenous parts are prebuilt to be treated as single units during the next step by a dependency parser for Japanese.", "labels": [], "entities": []}, {"text": "In ( preprocessing of certain tokens connected to constituents is performed before the dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7825379371643066}]}, {"text": "In () a third-party black-box dependency parser is used to improve the results of author's constituency parser, achieving 10% boost in F 1 . Additionally, there is evidence that ABBYY Compreno () tracks the order in sequences of consecutive tokens so it actually exploits some kind of constituents throughout the process of dependency parsing.", "labels": [], "entities": [{"text": "F 1", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9835100471973419}, {"text": "dependency parsing", "start_pos": 324, "end_pos": 342, "type": "TASK", "confidence": 0.750584065914154}]}, {"text": "In this paper, we propose a method of representing a parse tree, which is a combination of CT and DT and eliminates some disadvantages of both.", "labels": [], "entities": []}, {"text": "Then we describe syntactic rules that guide the process of simultaneous bottom-up acquisition of multiple syntactically ambiguous DT-s and CTs fora given sentence, ranked by syntactic relevance.", "labels": [], "entities": [{"text": "simultaneous bottom-up acquisition of multiple syntactically ambiguous DT-s and CTs", "start_pos": 59, "end_pos": 142, "type": "TASK", "confidence": 0.7244529545307159}]}, {"text": "In addition, we discuss some properties of the rule base that is builtin the framework of proposed rules description language.", "labels": [], "entities": []}, {"text": "After that, we show that it is possible to extend the classical Cocke-Younger-Kasami algorithm to use grammar rules of arbitrary arity without grammar binarization and to exploit intermediate DT-s to increase efficiency.", "labels": [], "entities": []}, {"text": "Moreover, we demonstrate how to achieve a reasonable ranking of solutions without using any additional statistical information.", "labels": [], "entities": []}, {"text": "In conclusion, we discuss possible extensions of the approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "There is alack of corpora for Russian to evaluate parsers.", "labels": [], "entities": []}, {"text": "In 2012, a task for Russian syntactic parsers was held during the Dialogue conference.", "labels": [], "entities": [{"text": "Russian syntactic parsers", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.5713017880916595}]}, {"text": "The evaluation was conducted as follows: every parser processed a set of thousands of separate sentences from news and fiction, and then a \"golden standard\" of 800 sentences was selected and verified by several assessors.", "labels": [], "entities": []}, {"text": "During evaluation, some mistakes, such as prepositional phrase attachment, were not taken into account as syntactic parsers are originally not intended to deal with semantics.", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6847933133443197}]}, {"text": "Ignoring this, the method of evaluation was exactly UAS (unlabeled attach score, i.e. a number of nodes in a DT that have correct parents, see).", "labels": [], "entities": [{"text": "UAS", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9913632869720459}, {"text": "unlabeled attach score", "start_pos": 57, "end_pos": 79, "type": "METRIC", "confidence": 0.686499814192454}]}, {"text": "Our previous version of DictaScope Syntax parser, which was based on a modification of Eisner's algorithm), took part in that task in 2012, resulting with 5-th place out of 7 (systems have been ranged by F 1 ), with 86,3% precision, 98% recall and 0,917 F 1 . Our current evaluation of the new version of DictaScope Syntax parser, based on methods proposed in this paper, follows the technique from the).", "labels": [], "entities": [{"text": "F 1", "start_pos": 204, "end_pos": 207, "type": "METRIC", "confidence": 0.9751189351081848}, {"text": "precision", "start_pos": 222, "end_pos": 231, "type": "METRIC", "confidence": 0.9988853335380554}, {"text": "recall", "start_pos": 237, "end_pos": 243, "type": "METRIC", "confidence": 0.9988366961479187}, {"text": "F 1", "start_pos": 254, "end_pos": 257, "type": "METRIC", "confidence": 0.9903358817100525}]}, {"text": "We took 800 entries from the same set of sentences and marked them up in HT XML format.", "labels": [], "entities": []}, {"text": "In evaluation we followed the same principles as described in (, reaching 93.1% precision, 97% recall and 95% F 1 , which correspond to the 3rd place out of 7, with a lag of half percent from the second place.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9974735379219055}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9982712268829346}, {"text": "F 1", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9903947412967682}]}, {"text": "We have also marked up a corpus from Internet-forums and Wikipedia of 300 sentences, reaching 87% F 1 . Note on complexity.", "labels": [], "entities": [{"text": "F 1", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9717080593109131}]}, {"text": "It is known that fora sentence of n tokens CYK is O(n 3 ) algorithm by worst case complexity, and this complexity can be reduced to O(n 2.38 ) by algebraic tricks.", "labels": [], "entities": [{"text": "O", "start_pos": 132, "end_pos": 133, "type": "METRIC", "confidence": 0.9822134971618652}]}, {"text": "We have performed a time complexity evaluation of our parser on a corpus of 1 mln Russian sentences from Internet-news, averaging the time for every fixed length of the sentence.", "labels": [], "entities": []}, {"text": "We evaluated sentences with lengths from 3 to 40 tokens, 12 tokens average length.", "labels": [], "entities": []}, {"text": "Evaluation has showed the performance of 25 sentences per second average for one kernel of 3GHz Intel Quad.", "labels": [], "entities": []}, {"text": "The evaluation has also led to a plot given in.", "labels": [], "entities": []}, {"text": "It can be verified that the plot corresponds only to An 2 for some A, but not to An 3 . We can explain it in the following way.", "labels": [], "entities": []}, {"text": "With our grammar and Russian language, we have noticed that fora completed upper-triangular matrix of CYK, nonempty cells on each row are denser near the main diagonal.", "labels": [], "entities": []}, {"text": "Following this, for the part of the row that forms m cells to the right of the main diagonal, the density of nonempty cells in it is pm \u2264 cm for some c.", "labels": [], "entities": []}, {"text": "Now assume that 1) the maximum cost of the rule checking operation and 2) the maximum number of phrases' combinations that need to be verified against the rule base are some constants which depend on rules, 3) \u03c4 is the number of rules which are stored in a vocabulary with a key formed by grammar values from templates.", "labels": [], "entities": [{"text": "rule checking", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.8210312724113464}]}, {"text": "Then, the total number of rule checks is", "labels": [], "entities": []}], "tableCaptions": []}