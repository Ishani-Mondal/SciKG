{"title": [{"text": "Annotating Anaphoric Shell Nouns with their Antecedents", "labels": [], "entities": [{"text": "Annotating Anaphoric Shell Nouns with their Antecedents", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.5898270692144122}]}], "abstractContent": [{"text": "Anaphoric shell nouns such as this issue and this fact conceptually encapsulate complex pieces of information (Schmid, 2000).", "labels": [], "entities": []}, {"text": "We examine the feasibility of annotating such anaphoric nouns using crowd-sourcing.", "labels": [], "entities": []}, {"text": "In particular, we present our methodology for reliably annotating antecedents of such anaphoric nouns and the challenges we faced in doing so.", "labels": [], "entities": []}, {"text": "We also evaluated the quality of crowd annotation using experts.", "labels": [], "entities": [{"text": "crowd annotation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8262659013271332}]}, {"text": "The results suggest that most of the crowd annotations were good enough to use as training data for resolving such anaphoric nouns.", "labels": [], "entities": []}], "introductionContent": [{"text": "Anaphoric shell nouns (ASNs) such as this fact, this possibility, and this issue are common in all kinds of text.", "labels": [], "entities": [{"text": "Anaphoric shell nouns (ASNs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5694354226191839}]}, {"text": "They are called shell nouns because they provide nominal conceptual shells for complex chunks of information representing abstract concepts such as fact, proposition, and event).", "labels": [], "entities": []}, {"text": "An example is shown in (1).", "labels": [], "entities": []}, {"text": "(1) Despite decades of education and widespread course offerings, the survival rate for out-of-hospital cardiac arrest remains a dismal 6 percent or less worldwide.", "labels": [], "entities": [{"text": "survival rate", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.9836637377738953}]}, {"text": "This fact prompted the American Heart Association last November to simplify the steps of CPR to make it easier for laypeople to remember and to encourage even those who have not been formally trained to try it when needed.", "labels": [], "entities": [{"text": "American Heart Association last November", "start_pos": 23, "end_pos": 63, "type": "DATASET", "confidence": 0.8739228844642639}, {"text": "CPR", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9545233249664307}]}, {"text": "Here, the ASN this fact encapsulates the clause marked in bold from the preceding paragraph.", "labels": [], "entities": [{"text": "ASN", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.46178728342056274}]}, {"text": "ASNs play an important role in organizing a discourse.", "labels": [], "entities": []}, {"text": "First, they are used metadiscursively to talk about the current discourse.", "labels": [], "entities": []}, {"text": "In (1), the author characterizes the information presented in the context by referring to it as a fact -a thing that is indisputably the case.", "labels": [], "entities": []}, {"text": "Second, they are used as cohesive devices in a discourse.", "labels": [], "entities": []}, {"text": "In (1), for example, this fact on the one hand refers to the proposition marked in bold, and on the other, faces forward and serves as the starting point of the following paragraph.", "labels": [], "entities": []}, {"text": "Finally, as points out, like conjunctions so and however, ASNs may function as topic boundary markers and topic change markers.", "labels": [], "entities": []}, {"text": "Despite their importance, ASNs have not received much attention in Computational Linguistics.", "labels": [], "entities": [{"text": "ASNs", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9697185158729553}, {"text": "Computational Linguistics", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7801867425441742}]}, {"text": "Although there has been some effort to annotate certain anaphors with similar properties, i.e., demonstratives and the pronoun it), in contrast to ordinary nominal anaphora, there are not many annotated corpora available that could be used to study ASNs.", "labels": [], "entities": []}, {"text": "Indeed, many questions of annotation of ASNs must still be answered.", "labels": [], "entities": [{"text": "ASNs", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.6923516988754272}]}, {"text": "For example, the extent to which native speakers themselves agree on the resolution of such anaphors, i.e., on the precise antecedents, remains unclear.", "labels": [], "entities": [{"text": "resolution of such anaphors", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.8650416284799576}]}, {"text": "An essential first step in this field of research is therefore to clearly establish the extent of interannotator agreement on antecedents of ASNs as a measure of feasibility of the task.", "labels": [], "entities": []}, {"text": "In this paper, we describe our methodology for annotating ASNs using crowdsourcing, a cheap and fast way of obtaining annotation.", "labels": [], "entities": []}, {"text": "We also describe how we evaluated the feasibility of the task and the quality of the annotation, and the challenges we faced in doing so, both with regard to the task itself and the crowdsourcing platform we use.", "labels": [], "entities": []}, {"text": "The results suggest that most of the crowd-annotations were good enough to use as training data for ASN resolution.", "labels": [], "entities": [{"text": "ASN resolution", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.9860004186630249}]}], "datasetContent": [{"text": "Recall that in this experiment, annotators identify the sentence containing the antecedent and select the appropriate sentence label as their answer.", "labels": [], "entities": []}, {"text": "We know from our pilot annotation that the distribution of such labels is skewed: most of the ASN antecedents lie in the sentence preceding the anaphor sentence.", "labels": [], "entities": []}, {"text": "We observed the same trend in the results of this experiment.", "labels": [], "entities": []}, {"text": "In the ASN corpus, the crowd chose the preceding sentence 64% of the time, the same sentence 13% of the time, and longdistance sentences 23% of the time.", "labels": [], "entities": [{"text": "ASN corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8634059131145477}]}, {"text": "Considering the skewed distribution of labels, if we use traditional agreement coefficients, such as Cohen's \u03ba (1960) or Krippendorff's \u03b1 (2013), expected agreement is very high, which in turn results in a low reliability coefficient (in our case \u03b1 = 0.61) that does not necessarily reflect the true reliability of the annotation    One way to measure the reliability of the data, without taking chance correction into account, is to consider the distribution of the ASN instances with different levels of CrowdFlower confidence.", "labels": [], "entities": [{"text": "agreement", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9389050006866455}]}, {"text": "shows the percentages of instances in different confidence level bands for each shell noun as well as for all instances.", "labels": [], "entities": []}, {"text": "For example, for the shell noun fact, 8% of the total number of this fact instances were annotated with c < 0.5.", "labels": [], "entities": []}, {"text": "As we can see, most of the instances of the shell nouns fact, reason, question, and possibility were annotated with high confidence.", "labels": [], "entities": []}, {"text": "In addition, most of them occurred in the band 0.8 \u2264 c \u2264 1.", "labels": [], "entities": []}, {"text": "There are relatively few instances with low confidence for these nouns, suggesting the feasibility of reliable antecedent annotation for these nouns.", "labels": [], "entities": []}, {"text": "By contrast, the mental nouns issue and decision had a large number of low-confidence (c < 0.5) instances, bringing in the question of reliability of antecedent annotation of these nouns.", "labels": [], "entities": [{"text": "reliability", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.9544948935508728}]}, {"text": "Given these results with different confidence levels, the primary question is what confidence level should be considered acceptable?", "labels": [], "entities": []}, {"text": "For our task, we required that at least four trusted annotators out of eight annotators should agree on an answer for it to be acceptable.", "labels": [], "entities": []}, {"text": "We will talk about acceptability later in Section 7.", "labels": [], "entities": []}, {"text": "Recall that this experiment was about identifying the precise antecedent text segment given the sentence containing the antecedent.", "labels": [], "entities": []}, {"text": "Krippendorff's \u03b1 using Jaccard and Dice To compare our agreement results with previous efforts to annotate such antecedents, following Artstein and Poesio, we computed Krippendorff's \u03b1 using distance metrics Jaccard and Dice.", "labels": [], "entities": []}, {"text": "The general form of coefficient \u03b1 is: where D o and D e are observed and expected disagreements respectively.", "labels": [], "entities": []}, {"text": "\u03b1 = 1 indicates perfect reliability and u \u03b1 = 0 indicates the absence of reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.7804180979728699}, {"text": "reliability", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9548317790031433}]}, {"text": "When u \u03b1 < 0, either the sample size is very small or the disagreement is systematic.", "labels": [], "entities": []}, {"text": "Our agreement results are comparable to Artstein and Poesio's agreement results.", "labels": [], "entities": []}, {"text": "They had 20 annotators annotating 16 anaphor instances with segment antecedents, whereas we had 8 annotators annotating 2,323 ASN instances.", "labels": [], "entities": []}, {"text": "As Artstein and Poesio point out, expected disagreement in case of such antecedent annotation is close to maximal, as there is little overlap between segment antecedents of different anaphors and therefore \u03b1 pretty much reflects the observed agreement.", "labels": [], "entities": []}, {"text": "Krippendorff's unitizing \u03b1 ( u \u03b1) Following Kolhatkar and Hirst (2012), we use u \u03b1 for measuring reliability of the ASN antecedent annotation task.", "labels": [], "entities": [{"text": "reliability", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9746227264404297}, {"text": "ASN antecedent annotation task", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.8820830732584}]}, {"text": "This coefficient is appropriate when the annotators work on the same text, identify the units in the text that are relevant to the given research   question, and then label the identified units (Krippendorff, p.c.).", "labels": [], "entities": []}, {"text": "The general form of coefficient u \u03b1 is the same as in equation 1.", "labels": [], "entities": []}, {"text": "In our context, the annotators work on the same text, the ASN instances.", "labels": [], "entities": []}, {"text": "We define an elementary annotation unit (the smallest separately judged unit) to be a word token.", "labels": [], "entities": []}, {"text": "The annotators identify and locate ASN antecedents for the given anaphor in terms of sequences of elementary annotation units.", "labels": [], "entities": []}, {"text": "u \u03b1 incorporates the notion of distance between strings by using a distance function which is defined as the square of the distance between the non-overlapping tokens in our case.", "labels": [], "entities": []}, {"text": "The distance is 0 when the annotated units are exactly the same, and is the summation of the squares of the unmatched parts if they are different.", "labels": [], "entities": []}, {"text": "We compute observed and expected disagreement as explained by).", "labels": [], "entities": []}, {"text": "For our data, u \u03b1 was 0.54.", "labels": [], "entities": []}, {"text": "10 u \u03b1 was lower for the mental nouns issue and decision and the modal noun possibility compared to other shell nouns.", "labels": [], "entities": []}, {"text": "CrowdFlower confidence results We also examined different confidence levels for ASN antecedent annotation.", "labels": [], "entities": [{"text": "ASN antecedent annotation", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.8630767067273458}]}, {"text": "gives confidence results for all instances and for each noun.", "labels": [], "entities": [{"text": "confidence", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9830052256584167}]}, {"text": "In contrast with, the instances are more evenly distributed here.", "labels": [], "entities": []}, {"text": "As in experiment 1, the mental nouns issue and decision had many low confidence instances.", "labels": [], "entities": []}, {"text": "For the modal noun possibility, it was easy to identify the sentence containing the antecedent, but pinpointing the precise antecedent turned out to be difficult.", "labels": [], "entities": []}, {"text": "Now we discuss the nature of disagreement in ASN annotation.", "labels": [], "entities": [{"text": "ASN annotation", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.8590655028820038}]}, {"text": "Disagreement in experiment 1 There were two primary sources of disagreement in experiment 1.", "labels": [], "entities": [{"text": "Disagreement", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9133381247520447}]}, {"text": "First, the annotators had problems agreeing on the answer None.", "labels": [], "entities": [{"text": "None", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.8830391764640808}]}, {"text": "We instructed them to choose None when the sentence containing the antecedent was not labelled.", "labels": [], "entities": []}, {"text": "Nonetheless, some annotators chose sentences that did not precisely contain the actual antecedent but just hinted at it.", "labels": [], "entities": []}, {"text": "Second, sometimes it was hard to identify the precise antecedent sentence as the antecedent was either present in the blend of all labelled sentences or there were multiple possible answers, as shown in example (2).", "labels": [], "entities": []}, {"text": "(2) Any biography of Thomas More has to answer one fundamental question.", "labels": [], "entities": []}, {"text": "Why, out of all the many ambitious politicians of early Tudor England, did only one refuse to acquiesce to a simple piece of religious and political opportunism?", "labels": [], "entities": []}, {"text": "What was it about More that set him apart and doomed him to a spectacularly avoidable execution?", "labels": [], "entities": []}, {"text": "The innovation of Peter Ackroyd's new biography of More is that he places the answer to this question outside of More himself.", "labels": [], "entities": []}, {"text": "Here, the author formulates the question in a number of ways and any question mentioned in the preceding text can serve as the antecedent of the anaphor this question.", "labels": [], "entities": []}, {"text": "Hard instances Low agreement can indicate different problems: unclear guidelines, poorquality annotators, or difficult instances (e.g., not well understood linguistic phenomena)).", "labels": [], "entities": []}, {"text": "We can rule out the possibility of poor-quality annotators for two reasons.", "labels": [], "entities": []}, {"text": "First, we consider 8 diverse annotators who work independently.", "labels": [], "entities": []}, {"text": "Second, we use CrowdFlower's quality-control mechanisms and hence allow only trustworthy annotators to annotate our texts.", "labels": [], "entities": []}, {"text": "Regarding instructions, we take interannotator agreement as a measure for feasibility of the task, and hence we keep the annotation instruction as simple as possible.", "labels": [], "entities": []}, {"text": "This could be a source of low agreement.", "labels": [], "entities": []}, {"text": "The third possibility is hard instances.", "labels": [], "entities": []}, {"text": "Our results show that the mental nouns issue and decision had many low-confidence instances, suggesting the difficulty associated with the interpretation of these nouns (e.g., the very idea of what counts as an issue is fuzzy).", "labels": [], "entities": []}, {"text": "The shell noun decision was harder because most of its instances were court-decision related articles, which were in general hard to understand.", "labels": [], "entities": [{"text": "shell noun decision", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.602798581123352}]}, {"text": "CrowdFlower experiment 2 resulted in 1,810 ASN instances with c > 0.5.", "labels": [], "entities": []}, {"text": "The question is how good are these annotations from experts' point of view.", "labels": [], "entities": []}, {"text": "To examine the quality of the crowd annotation we asked two judges A and B to evaluate the acceptability of the crowd's answers.", "labels": [], "entities": []}, {"text": "The judges were highly-qualified academic editors: A, a researcher in Linguistics and B, a translator with a Ph.D. in History and Philosophy of Science.", "labels": [], "entities": []}, {"text": "From the crowd-annotated ASN antecedent data, we randomly selected 300 instances, 50 instances per shell noun.", "labels": [], "entities": []}, {"text": "We made sure to choose instances with borderline confidence (0.5 \u2264 c < 0.6), medium confidence (0.6 \u2264 c < 0.8), and high confidence (0.8 \u2264 c \u2264 1.0).", "labels": [], "entities": []}, {"text": "We asked the judges to rate the acceptability of the crowd-answers based on the extent to which they provided interpretation of the corresponding anaphor.", "labels": [], "entities": []}, {"text": "We gave them four options: perfectly (the crowd's answer is perfect and the judge would have chosen the same antecedent), reasonably (the crowd's answer is acceptable and is close to their answer),: Evaluation of ASN antecedent annotation.", "labels": [], "entities": [{"text": "ASN antecedent annotation", "start_pos": 213, "end_pos": 238, "type": "TASK", "confidence": 0.7032467325528463}]}, {"text": "P = perfectly, R = reasonably, I = implicitly, N = not at all implicitly (the crowd's answer only implicitly contains the actual antecedent), and not at all (the crowd's answer is not in anyway related to the actual antecedent).", "labels": [], "entities": []}, {"text": "11 Moreover, if they did not mark perfectly, we asked them to provide their antecedent string.", "labels": [], "entities": []}, {"text": "The two judges worked on the task independently and they were completely unaware of how the annotation data was collected.", "labels": [], "entities": []}, {"text": "shows the confusion matrix of the ratings of the two judges.", "labels": [], "entities": [{"text": "confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9871101379394531}]}, {"text": "Judge B was stricter than Judge A. Given the nature of the task, it was encouraging that most of the crowd-antecedents were rated as perfectly by both judges (72% by A and 62% by B).", "labels": [], "entities": []}, {"text": "Note that perfectly is rather a strong evaluation for ASN antecedent annotation, considering the nature of ASN antecedents themselves.", "labels": [], "entities": [{"text": "ASN antecedent annotation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.8320228656133016}]}, {"text": "If we weaken the acceptability criteria and consider the antecedents rated as reasonably to be also acceptable antecedents, 84.6% of the total instances were acceptable according to both judges.", "labels": [], "entities": []}, {"text": "Regarding the instances marked implicitly, most of the times the crowd's answer was the closest textual string of the judges' answer.", "labels": [], "entities": []}, {"text": "So we again might consider instances marked implicitly as acceptable answers.", "labels": [], "entities": []}, {"text": "For a very few instances (only about 5%) either of the judges marked not at all.", "labels": [], "entities": []}, {"text": "This was a positive result and suggests success of different steps of our annotation procedure: identifying broad region, identifying the set of most likely candidates, and identifying precise antecedent.", "labels": [], "entities": []}, {"text": "As we can see in, there were 7 instances where the judge A rated perfectly while the judge B rated not at all, i.e., completely contradictory judgements.", "labels": [], "entities": []}, {"text": "When we looked at these examples, they were rather hard and ambiguous cases.", "labels": [], "entities": []}, {"text": "An example is shown in (3).", "labels": [], "entities": []}, {"text": "The whether clause marked in the preceding sen-tence is the crowd's answer.", "labels": [], "entities": []}, {"text": "One of our judges rated this answer as perfectly, while the other rated it as not at all.", "labels": [], "entities": []}, {"text": "According to her the correct antecedent is whether Catholics who vote for Mr. Kerry would have to go to confession.", "labels": [], "entities": []}, {"text": "(3) Several Vatican officials said, however, that any such talk has little meaning because the church does not take sides in elections.", "labels": [], "entities": []}, {"text": "But the statements by several American bishops that Catholics who vote for Mr. Kerry would have to go to confession have raised the question in many corners about whether this is an official church position.", "labels": [], "entities": []}, {"text": "The church has not addressed this question publicly and, in fact, seems reluctant to be dragged into the fight...\"", "labels": [], "entities": []}, {"text": "There was no notable relation between the annotator's rating and the confidence level: many instances with borderline confidence were marked perfectly or reasonably, suggesting that instances with c \u2265 0.5 were reasonably annotated instances, to be used as training data for ASN resolution.", "labels": [], "entities": [{"text": "ASN resolution", "start_pos": 274, "end_pos": 288, "type": "TASK", "confidence": 0.9838638007640839}]}], "tableCaptions": [{"text": " Table 2: CrowdFlower confidence distribution for  CrowdFlower experiment 1. Each column shows  the distribution in percentages for confidence of  annotating antecedents of that shell noun. The fi- nal row shows the average confidence of the dis- tribution. Number of ASN instances = 2,822.", "labels": [], "entities": []}, {"text": " Table 3: Agreement using Krippendorff's \u03b1 for  CrowdFlower experiment 2. A&P = Artstein and  Poesio (2006).", "labels": [], "entities": [{"text": "A", "start_pos": 74, "end_pos": 75, "type": "METRIC", "confidence": 0.9901732206344604}]}, {"text": " Table 5: Evaluation of ASN antecedent annota- tion. P = perfectly, R = reasonably, I = implicitly,  N = not at all", "labels": [], "entities": []}]}