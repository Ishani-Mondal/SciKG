{"title": [{"text": "Letter N-Gram-based Input Encoding for Continuous Space Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a letter-based encoding for words in continuous space language models.", "labels": [], "entities": []}, {"text": "We represent the words completely by letter n-grams instead of using the word index.", "labels": [], "entities": []}, {"text": "This way, similar words will automatically have a similar representation.", "labels": [], "entities": []}, {"text": "With this we hope to better generalize to unknown or rare words and to also capture morphological information.", "labels": [], "entities": []}, {"text": "We show their influence in the task of machine translation using continuous space language models based on restricted Boltz-mann machines.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8064036667346954}]}, {"text": "We evaluate the translation quality as well as the training time on a German-to-English translation task of TED and university lectures as well as on the news translation task translating from English to German.", "labels": [], "entities": [{"text": "German-to-English translation task", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.7451728284358978}, {"text": "TED", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.8507874608039856}, {"text": "news translation task translating from English to German", "start_pos": 154, "end_pos": 210, "type": "TASK", "confidence": 0.8560737371444702}]}, {"text": "Using our new approach again in BLEU score by up to 0.4 points can be achieved.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9986947178840637}]}], "introductionContent": [{"text": "Language models play an important role in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6536303758621216}]}, {"text": "The most commonly used approach is n-gram-based language models).", "labels": [], "entities": []}, {"text": "In recent years Continuous Space Language Models (CSLMs) have gained a lot of attention.", "labels": [], "entities": [{"text": "Continuous Space Language Models (CSLMs)", "start_pos": 16, "end_pos": 56, "type": "TASK", "confidence": 0.7011024355888367}]}, {"text": "Compared to standard n-gram-based language models they promise better generalization to unknown histories or n-grams with only few occurrences.", "labels": [], "entities": []}, {"text": "Since the words are projected into a continuous space, true interpolation can be performed when an unseen sample appears.", "labels": [], "entities": []}, {"text": "The standard input layer for CSLMs is a so called 1-ofn coding where a word is represented as a vector with a single neuron turned on and the rest turned off.", "labels": [], "entities": []}, {"text": "In the standard approach it is problematic to infer probabilities for words that are not inside the vocabulary.", "labels": [], "entities": []}, {"text": "Sometimes an extra unknown neuron is used in the input layer to represent these words.", "labels": [], "entities": []}, {"text": "Since all unseen words get mapped to the same neuron, no real discrimination between those words can be done.", "labels": [], "entities": []}, {"text": "Furthermore, rare words are also hard to model, since there is too few training data available to estimate their associated parameters.", "labels": [], "entities": []}, {"text": "We try to overcome these shortcomings by using subword features to cluster similar words closer together and generalize better over unseen words.", "labels": [], "entities": []}, {"text": "We hope that words containing similar letter n-grams will yield a good indicator for words that have the same function inside the sentence.", "labels": [], "entities": []}, {"text": "Introducing a method for subword units also has the advantage that the input layer can be smaller, while still representing nearly the same vocabulary with unique feature vectors.", "labels": [], "entities": []}, {"text": "By using a smaller input layer, less weights need to be trained and the training is faster.", "labels": [], "entities": []}, {"text": "In this work we present the letter n-gram approach to represent words in an CSLM, and compare it to the word-based CSLM presented in.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows: First we will give an overview of related work.", "labels": [], "entities": []}, {"text": "After that we give a brief overview of restricted Boltzmann machines which are the basis of the letter-based CSLM presented in Section 4.", "labels": [], "entities": []}, {"text": "Then we will present the results of the experiments and conclude our work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the RBM-based language model on different statistical machine translation (SMT) tasks.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.7745941877365112}]}, {"text": "We will first analyze the letter-based word representation.", "labels": [], "entities": []}, {"text": "Then we will give a brief description of our SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9914127588272095}]}, {"text": "Afterwards, we describe in detail our experiments on the Germanto-English translation task.", "labels": [], "entities": [{"text": "Germanto-English translation task", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.7884555260340372}]}, {"text": "We will end with additional experiments on the task of translating English news documents into German.", "labels": [], "entities": [{"text": "translating English news documents", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.9002147018909454}]}], "tableCaptions": [{"text": " Table 1: Comparison of the vocabulary size and the possibility to have a unique representation of each  word in the training corpus.", "labels": [], "entities": []}, {"text": " Table 2: Results for German-to-English TED  translation task", "labels": [], "entities": [{"text": "TED  translation", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.8389250636100769}]}, {"text": " Table 3. In these  experiments all letter-based models outperformed  the baseline system. The bigram-based language  model performs worst and the 3-and 4-gram- based models perform only slightly worse than the  word index-based model.", "labels": [], "entities": []}, {"text": " Table 3: Results of German-to-English TED trans- lations using an additional in-domain language  model.", "labels": [], "entities": [{"text": "TED trans- lations", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7960734814405441}]}, {"text": " Table 4: Results of German-to-English TED trans- lations with additional in-domain language model  and adapted phrase table.", "labels": [], "entities": [{"text": "TED trans- lations", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7039796113967896}]}, {"text": " Table 5: Difference between caps and non-caps  letter n-gram models.", "labels": [], "entities": [{"text": "Difference", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.6696681976318359}]}, {"text": " Table 6. In this  case the baseline is outperformed by the word in- dex approach by approximately 1.1 BLEU points.  Except for the 4-gram model the results are similar  to the result for the TED task. All systems could  again outperform the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9987877011299133}, {"text": "TED task", "start_pos": 192, "end_pos": 200, "type": "TASK", "confidence": 0.8545477390289307}]}, {"text": " Table 6: Results the baseline of the German-to- English CSL task.", "labels": [], "entities": [{"text": "German-to- English CSL task", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.5009531319141388}]}, {"text": " Table 7: Results on German-to-English CSL cor- pus with additional in-domain language model.", "labels": [], "entities": []}, {"text": " Table 8: Results on German-to-English CSL with  additional in-domain language model and adapted  phrase table.", "labels": [], "entities": []}, {"text": " Table 9: Results for WMT2013 task English-to- German.", "labels": [], "entities": [{"text": "WMT2013 task English-to- German", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7683839201927185}]}]}