{"title": [{"text": "Predicting conjunct propagation and other extended Stanford Dependencies", "labels": [], "entities": [{"text": "Predicting conjunct propagation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8254748781522115}]}], "abstractContent": [{"text": "In this work, we present a data-driven method to enhance syntax trees with additional dependencies as defined in the well-known Stanford Dependencies scheme, so as to give more information about the structure of the sentence.", "labels": [], "entities": []}, {"text": "This hybrid method utilizes both machine learning and a rule-based approach, and achieves a performance of 93.1% in F 1-score, as evaluated using an existing treebank of Finnish.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9820360243320465}]}, {"text": "The resulting tool will be integrated into an existing Finnish parser and made publicly available at the address", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency-based analysis of syntax has recently become popular within natural language processing.", "labels": [], "entities": [{"text": "Dependency-based analysis of syntax", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8748718798160553}]}, {"text": "It has been argued to be preferable over constituency analysis in both parser evaluation and further applications, and indeed both dependency treebanks and parsers have emerged in recent years.", "labels": [], "entities": [{"text": "constituency analysis", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8653470575809479}, {"text": "parser evaluation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.914402574300766}]}, {"text": "Dependency formalisms usually require that all valid analyses must be trees, meaning that each token in a sentence must only have one governor, and the whole sentence must have one headword.", "labels": [], "entities": [{"text": "Dependency formalisms", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8287376165390015}]}, {"text": "Tree structures, however, do not necessarily allow the explicit representation of a number of relevant phenomena.", "labels": [], "entities": []}, {"text": "This is demonstrated by the wellknown Stanford Dependencies (SD) scheme (de, which is defined in multiple variants.", "labels": [], "entities": []}, {"text": "The basic variant requires sentence structures to be trees, and the other variants can then be used to add further dependencies on top of the tree structure, making the resulting structures graphs rather than trees.", "labels": [], "entities": []}, {"text": "Phenomena that are further analyzed in the non-basic variants of SD include relative clauses, open clausal complements, coordinations and prepositional phrases.", "labels": [], "entities": []}, {"text": "The dependencies present in non-basic variants of SD can be useful for applications that build on top of the syntactic analysis.", "labels": [], "entities": []}, {"text": "For instance, the clinical domain pilot study of has shown that these dependencies can be used in annotating argument structures of verbs using the popular PropBank scheme).", "labels": [], "entities": []}, {"text": "Also, have used the propagated and collapsed variant of the SD scheme to retrieve as semantically meaningful dependencies as possible in the context of textual entailments.", "labels": [], "entities": []}, {"text": "The nonbasic variants of SD are also extensively applied in information extraction, as seen for example in the BioNLP shared tasks on event extraction, where a number of top-ranking systems relied on SD analyses.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8895240426063538}, {"text": "event extraction", "start_pos": 134, "end_pos": 150, "type": "TASK", "confidence": 0.6786950975656509}]}, {"text": "In this work, we are concerned with three phenomena represented in the non-basic variants of SD.", "labels": [], "entities": []}, {"text": "Most importantly, we consider the dependencies that are the result of conjunct propagation.", "labels": [], "entities": [{"text": "conjunct propagation", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7497080564498901}]}, {"text": "They resolve, at least partially, ambiguities known as coordination scope ambiguities.", "labels": [], "entities": []}, {"text": "These are ambiguities where there are multiple ways to understand the scope of a coordination; for instance, in the phrase old men and women either both the men and the women are old, or alternatively, only the men.", "labels": [], "entities": []}, {"text": "Additionally, we consider dependencies that reveal the syntactic functions of relativizers and external subjects of open clausal complements.", "labels": [], "entities": []}, {"text": "We present a method that, given the basic syntactic tree of a sentence, predicts these additional dependencies as defined in the SD scheme using machine learning.", "labels": [], "entities": []}, {"text": "As training data, we use morphological and syntactic information gathered from an existing treebank of Finnish, which has human annotated conjunct propagation and additional dependencies present.", "labels": [], "entities": []}, {"text": "We begin with a discussion of related work and the treebank used as training material.", "labels": [], "entities": []}, {"text": "We then move onto the details of the method itself and present a thorough evaluation of the pipeline.", "labels": [], "entities": []}, {"text": "We make comparisons with several baseline methods, and conclude that the proposed method achieves a performance clearly superior to each of these baselines.", "labels": [], "entities": []}, {"text": "In particular, the method demonstrates performance clearly superior to that achieved by the commonly used Stanford tools.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of the predictions in terms of precision (P), recall (R), and F 1 -score (F) of the predicted second layer dependencies.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9398035705089569}, {"text": "recall (R)", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9622709900140762}, {"text": "F 1 -score (F)", "start_pos": 90, "end_pos": 104, "type": "METRIC", "confidence": 0.981057448046548}]}, {"text": "Precision is defined as the proportion of dependen-: Feature ablation study.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8809670805931091}]}, {"text": "Feature pairs refer to the second-degree polynomial expansion described in Section 4.1, and morph refers to features extracted from morphological tags other than the main POS.", "labels": [], "entities": []}, {"text": "cies in the evaluated output also present in the gold standard, and recall as the proportion of dependencies in the gold standard also present in the evaluated output.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9193187952041626}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9996097683906555}]}, {"text": "Using these, F 1 -score is defined as F = 2P RP +R . In addition to evaluating the performance using the gold-standard base layer annotation in the treebank, we also perform an evaluation with the base syntax layer produced by a dependency parser, discussed further in Section 5.2.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9500043541193008}]}], "tableCaptions": [{"text": " Table 1: Performance of the combined second  layer prediction, as well as the individual tasks  measured in terms of precision, recall, and F 1 - score on the gold-standard base syntax trees.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9994794726371765}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.99891197681427}, {"text": "F 1 - score", "start_pos": 141, "end_pos": 152, "type": "METRIC", "confidence": 0.9869486391544342}]}, {"text": " Table 2: Feature ablation study. Feature pairs re- fer to the second-degree polynomial expansion de- scribed in Section 4.1, and morph refers to features  extracted from morphological tags other than the  main POS.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the proposed machine  learning method in terms of precision, recall and  F 1 -score of propagated dependencies. The perfor- mance is compared to the four baselines defined in  Section 5.1.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9996429681777954}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9995936751365662}, {"text": "F 1 -score", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9839001148939133}]}, {"text": " Table 4: Performance of the combined second  layer prediction, as well as the individual tasks  measured in terms of precision, recall, and F 1 - score on top of statistical parser output.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9995506405830383}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9991519451141357}, {"text": "F 1 - score", "start_pos": 141, "end_pos": 152, "type": "METRIC", "confidence": 0.9867386817932129}]}, {"text": " Table 5: Performance of the method as compared  to the baselines of Section 5.1 on top of parser out- put.", "labels": [], "entities": []}]}