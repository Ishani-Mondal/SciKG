{"title": [{"text": "The Story of the Characters, the DNA and the Native Language", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents our approach to the 2013 Native Language Identification shared task, which is based on machine learning methods that work at the character level.", "labels": [], "entities": [{"text": "2013 Native Language Identification shared task", "start_pos": 40, "end_pos": 87, "type": "TASK", "confidence": 0.7840980092684428}]}, {"text": "More precisely , we used several string kernels and a kernel based on Local Rank Distance (LRD).", "labels": [], "entities": [{"text": "Local Rank Distance (LRD)", "start_pos": 70, "end_pos": 95, "type": "METRIC", "confidence": 0.7348568141460419}]}, {"text": "Actually, our best system was a kernel combination of string kernel and LRD.", "labels": [], "entities": []}, {"text": "While string kernels have been used before in text analysis tasks, LRD is a distance measure designed to work on DNA sequences.", "labels": [], "entities": []}, {"text": "In this work, LRD is applied with success in native language identification.", "labels": [], "entities": [{"text": "LRD", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.7507041096687317}, {"text": "native language identification", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6626777052879333}]}, {"text": "Finally, the Unibuc team ranked third in the closed NLI Shared Task.", "labels": [], "entities": []}, {"text": "This result is more impressive if we consider that our approach is language independent and linguistic theory neutral.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents our approach to the shared task on Native Language Identification, NLI 2013.", "labels": [], "entities": [{"text": "Native Language Identification, NLI 2013", "start_pos": 55, "end_pos": 95, "type": "TASK", "confidence": 0.6695778916279475}]}, {"text": "We approached this task with machine learning methods that work at the character level.", "labels": [], "entities": []}, {"text": "More precisely, we treated texts just as sequences of symbols (strings) and used different string kernels in conjunction with different kernel-based learning methods in a series of experiments to assess the best performance level that can be achieved.", "labels": [], "entities": []}, {"text": "Our aim was to investigate if identifying native language is possible with machine learning methods that work at the character level.", "labels": [], "entities": []}, {"text": "By disregarding features of natural language such as words, phrases, or meaning, our approach has an important advantage in that it is language independent.", "labels": [], "entities": []}, {"text": "Using words is natural in text analysis tasks like text categorization (by topic), authorship identification and plagiarism detection.", "labels": [], "entities": [{"text": "authorship identification", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.8206369876861572}, {"text": "plagiarism detection", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.7869916260242462}]}, {"text": "Perhaps surprisingly, recent results have proved that methods handling the text at character level can also be very effective in text analysis tasks;).", "labels": [], "entities": [{"text": "text analysis", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.7875483930110931}]}, {"text": "In () string kernels were used for document categorization with very good results.", "labels": [], "entities": [{"text": "document categorization", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7650069892406464}]}, {"text": "Trying to explain why treating documents as symbol sequences and using string kernels led to such good results the authors suppose that: \"the [string] kernel is performing something similar to stemming, hence providing semantic links between words that the word kernel must view as distinct\".", "labels": [], "entities": []}, {"text": "String kernels were also successfully used in authorship identification;.", "labels": [], "entities": [{"text": "authorship identification", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.9097453951835632}]}, {"text": "For example, the system described in () ranked first inmost problems and overall in the PAN 2012 Traditional Authorship Attribution tasks.", "labels": [], "entities": [{"text": "PAN 2012 Traditional Authorship Attribution tasks", "start_pos": 88, "end_pos": 137, "type": "DATASET", "confidence": 0.8187832236289978}]}, {"text": "A possible reason for the success of string kernels in authorship identification is given in (Popescu and: \"the similarity of two strings as it is measured by string kernels reflects the similarity of the two texts as it is given by the short words (2-5 characters) which usually are function words, but also takes into account other morphemes like suffixes ('ing' for example) which also can be good indicators of the author's style\".", "labels": [], "entities": [{"text": "authorship identification", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.863340437412262}]}, {"text": "Even more interesting is the fact that two methods, that are essentially the same, obtained very good results for text categorization (by topic)) and authorship identification (.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7870544791221619}, {"text": "authorship identification", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.8708129227161407}]}, {"text": "Both are based on SVM and a string kernel of length 5.", "labels": [], "entities": []}, {"text": "Traditionally, the two tasks, text categorization (by topic) and authorship identification are viewed as opposite.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7845147848129272}, {"text": "authorship identification", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.8420496582984924}]}, {"text": "When words are considered as features, for text categorization the (stemmed) content words are used (the stop words being eliminated), while for authorship identification the function words (stop words) are used as features, the others words (content words) being eliminated.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7522006034851074}, {"text": "authorship identification", "start_pos": 145, "end_pos": 170, "type": "TASK", "confidence": 0.7516023814678192}]}, {"text": "Then, why did the same string kernel (of length 5) work well in both cases?", "labels": [], "entities": []}, {"text": "In our opinion the key factor is the kernelbased learning algorithm.", "labels": [], "entities": []}, {"text": "The string kernel implicitly embeds the texts in a high dimensional feature space, in our case the space of all (sub)strings of length 5.", "labels": [], "entities": []}, {"text": "The kernel-based learning algorithm (SVM or another kernel method), aided by regularization, implicitly assigns a weight to each feature, thus selecting the features that are important for the discrimination task.", "labels": [], "entities": []}, {"text": "In this way, in the case of text categorization the learning algorithm (SVM) enhances the features (substrings) representing stems of content words, while in the case of authorship identification the same learning algorithm enhances the features (substrings) representing function words.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6932828575372696}, {"text": "authorship identification", "start_pos": 170, "end_pos": 195, "type": "TASK", "confidence": 0.7529024183750153}]}, {"text": "Using string kernels will make the corresponding learning method completely language independent, because the texts will be treated as sequences of symbols (strings).", "labels": [], "entities": []}, {"text": "Methods working at the word level or above very often restrict their feature space according to theoretical or empirical principles.", "labels": [], "entities": []}, {"text": "For example, they select only features that reflect various types of spelling errors or only some type of words, such as function words, for example.", "labels": [], "entities": []}, {"text": "These features prove to be very effective for specific tasks, but other, possibly good features, depending on the particular task, may exist.", "labels": [], "entities": []}, {"text": "String kernels embed the texts in a very large feature space (all substrings of length k) and leave it to the learning algorithm (SVM or others) to select important features for the specific task, by highly weighting these features.", "labels": [], "entities": []}, {"text": "A method that considers words as features cannot be language independent.", "labels": [], "entities": []}, {"text": "Even a method that uses only function words as features is not completely language independent because it needs a list of function words (specific to a language) and away to segment a text into words which is not an easy task for some languages, like Chinese.", "labels": [], "entities": []}, {"text": "Character n-grams were already used in native language identification).", "labels": [], "entities": [{"text": "native language identification", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6560980578263601}]}, {"text": "The reported performance when only character n-grams were used as features was modest compared with other type of features.", "labels": [], "entities": []}, {"text": "But, in the above mentioned works, the authors investigated only the bigrams and trigrams and not longer n-grams.", "labels": [], "entities": []}, {"text": "Particularly, we have obtained similar results with () when using character bigrams, but we have achieved the best performance using a range of 5 to 8 n-grams (see section 4.3).", "labels": [], "entities": []}, {"text": "We have used with success a similar approach for the related task of identifying translationese (Popescu, 2011).", "labels": [], "entities": [{"text": "identifying translationese", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.8029496967792511}]}, {"text": "The first application of string kernel ideas came in the field of text categorization, with the paper (), followed by applications in bioinformatics ().", "labels": [], "entities": [{"text": "text categorization", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7215282469987869}]}, {"text": "Computer science researchers have developed a wide variety of methods that can be applied with success in computational biology.", "labels": [], "entities": []}, {"text": "Such methods range from clustering techniques used to analyze the phylogenetic trees of different organisms (, to genetic algorithms used to find motifs or common patterns in a set of given DNA sequences ().", "labels": [], "entities": []}, {"text": "Most of these methods are based on a distance measure for strings, such as Hamming (), edit, Kendalltau, or rank distance.", "labels": [], "entities": [{"text": "Kendalltau", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.659553587436676}]}, {"text": "A similar idea to character n-grams was introduced in the early years of bioinformatics, where k-mers are used instead of single characters . There are recent studies that use k-mers for the phylogenetic analysis of organisms (), or for sequence alignment ().", "labels": [], "entities": [{"text": "sequence alignment", "start_pos": 237, "end_pos": 255, "type": "TASK", "confidence": 0.7628578841686249}]}, {"text": "Analyzing DNA at substring level is also more suited from a biological point of view, because DNA substrings may contain meaningful information.", "labels": [], "entities": []}, {"text": "For example, genes are encoded by a number close to 100 base pairs, or codons that encode the twenty standard amino acids are formed of 3-mers.", "labels": [], "entities": []}, {"text": "Local Rank Dis-tance (LRD) has been recently proposed as an extension of rank distance.", "labels": [], "entities": [{"text": "Local Rank Dis-tance (LRD)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.542084202170372}]}, {"text": "LRD drops the annotation step of rank distance, and uses k-mers instead of single characters.", "labels": [], "entities": [{"text": "LRD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8126237988471985}]}, {"text": "The work shows that LRD is a distance function and that it has very good results in phylogenetic analysis and DNA sequence comparison.", "labels": [], "entities": [{"text": "LRD", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9291228652000427}, {"text": "phylogenetic analysis", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.9353932738304138}, {"text": "DNA sequence comparison", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.5877638260523478}]}, {"text": "But, LRD can be applied to any kind of string sequences, not only to DNA.", "labels": [], "entities": []}, {"text": "Thus, LRD was transformed into a kernel and used for native language identification.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.6313449045022329}]}, {"text": "Despite the fact it has no linguistic motivation, LRD gives surprisingly good results for this task.", "labels": [], "entities": [{"text": "LRD", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.4676532745361328}]}, {"text": "Its performance level is lower than string kernel, but LRD can contribute to the improvement of string kernel when the two methods are combined.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, the kernel methods we used are briefly described.", "labels": [], "entities": []}, {"text": "Section 3 presents the string kernels and the LRD, and shows how to transform LRD into a kernel.", "labels": [], "entities": []}, {"text": "Section 4 presents details about the experiments.", "labels": [], "entities": []}, {"text": "It gives details about choosing the learning method, parameter tuning, combining kernels and results of submitted systems.", "labels": [], "entities": []}, {"text": "Finally, conclusions are given in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset for the NLI shared task is the TOEFL11 corpus (.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.9750745892524719}]}, {"text": "This corpus contains 9900 examples for training, 1100 examples for development (or validation) and another 1100 examples for testing.", "labels": [], "entities": []}, {"text": "Each example is an essay written in English by a person that is a non-native English speaker.", "labels": [], "entities": []}, {"text": "The people that produced the essays have one of the following native languages: German, French, Spanish, Italian, Chinese, Korean, Japanese, Turkish, Arabic, Telugu, Hindi.", "labels": [], "entities": []}, {"text": "For more details see (.", "labels": [], "entities": []}, {"text": "We participated only in the closed NLI shared task, where the goal of the task is to predict the native language of testing examples, only by using the training and development data.", "labels": [], "entities": []}, {"text": "In our approach, documents or essays from this corpus are treated as strings.", "labels": [], "entities": []}, {"text": "Thus, when we refer to strings throughout this paper, we really mean documents or essays.", "labels": [], "entities": []}, {"text": "Because we work at the character level, we didn't need to split the texts into words, or to do any NLP-specific preprocessing.", "labels": [], "entities": []}, {"text": "The only editing done to the texts was the replacing of sequences of consecutive space characters (space, tab, newline, etc.) with a single space character.", "labels": [], "entities": []}, {"text": "This normalization was needed in order to not artificially increase or decrease the similarity between texts as a result of different spacing.", "labels": [], "entities": []}, {"text": "Also all uppercase letters were converted to the corresponding lowercase ones.", "labels": [], "entities": []}, {"text": "We didn't use the additional information from prompts and English language proficiency level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy rates using 10-fold cross-validation on  the train set for different kernel methods with\u02c6kwith\u02c6 with\u02c6k 5 kernel.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9915428161621094}]}, {"text": " Table 4: Accuracy rates of submitted systems on different evaluation sets. The Unibuc team ranked third in the closed  NLI Shared Task with the kernel combination improved by the heuristic to level the predicted class distribution.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9503911137580872}]}]}