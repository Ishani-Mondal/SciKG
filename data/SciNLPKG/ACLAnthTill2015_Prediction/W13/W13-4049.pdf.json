{"title": [{"text": "In-Context Evaluation of Unsupervised Dialogue Act Models for Tutorial Dialogue", "labels": [], "entities": [{"text": "Tutorial Dialogue", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.919041246175766}]}], "abstractContent": [{"text": "Unsupervised dialogue act modeling holds great promise for decreasing the development time to build dialogue systems.", "labels": [], "entities": [{"text": "dialogue act modeling", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.746685802936554}]}, {"text": "Work to date has utilized manual annotation or a synthetic task to evaluate unsu-pervised dialogue act models, but each of these evaluation approaches has substantial limitations.", "labels": [], "entities": []}, {"text": "This paper presents an in-context evaluation framework for an un-supervised dialogue act model within tuto-rial dialogue.", "labels": [], "entities": []}, {"text": "The clusters generated by the model are mapped to tutor responses by a handcrafted policy, which is applied to unseen test data and evaluated by human judges.", "labels": [], "entities": []}, {"text": "The results suggest that in-context evaluation may better reflect the performance of a model than comparing against manual dialogue act labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "A central focus within the dialogue systems research community is developing techniques for rapidly constructing dialogue systems.", "labels": [], "entities": []}, {"text": "One technique that has proven highly promising is to take a corpus-based approach to dialogue system authoring, for example by bootstrapping policy learning, predicting what a human agent would do, or learning supervised dialogue act models ().", "labels": [], "entities": [{"text": "dialogue system authoring", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.6889161070187887}]}, {"text": "Traditionally, these corpus-based approaches require some amount of manual annotation prior to learning the dialogue models.", "labels": [], "entities": []}, {"text": "In many cases, this manual annotation is a problematic bottleneck for system development.", "labels": [], "entities": []}, {"text": "For tutorial dialogue systems, which aim to support students in acquiring skills or knowledge, heavy manual annotation is often required for learning models that classify student utterances with respect to dialogue acts), questioning strategies, or information sharing For dialogue act modeling in particular, recent work has demonstrated the great promise of unsupervised approaches, which are learned without the use of manual labels.", "labels": [], "entities": [{"text": "dialogue act modeling", "start_pos": 273, "end_pos": 294, "type": "TASK", "confidence": 0.6527389685312907}]}, {"text": "However, because gold standard labels are not apart of model learning, how to best evaluate unsupervised models represents a significant open research question.", "labels": [], "entities": []}, {"text": "Most quantitative evaluations of unsupervised dialogue act models have relied on agreement with manual dialogue act annotations, though these annotations were not used in model learning (.", "labels": [], "entities": []}, {"text": "Relying on manually tagged dialogue act labels to evaluate an unsupervised model has two major drawbacks: it does not fully avoid the manual annotation bottleneck, and it imposes a handauthored criterion onto a fully data-driven model, which maybe unnecessarily limiting.", "labels": [], "entities": []}, {"text": "Distinctions made by an unsupervised model maybe useful within a dialogue system, even if these categories are different from the distinctions made within a hand-authored dialogue act tagset.", "labels": [], "entities": []}, {"text": "This paper presents a novel evaluation framework for unsupervised dialogue act classification of user utterances within tutorial dialogue.", "labels": [], "entities": [{"text": "unsupervised dialogue act classification of user utterances within tutorial dialogue", "start_pos": 53, "end_pos": 137, "type": "TASK", "confidence": 0.7819077789783477}]}, {"text": "Instead of attempting to evaluate the model intrinsically, we evaluate its performance on an external task: triggering an appropriate utterance via a simple dialogue policy.", "labels": [], "entities": []}, {"text": "This evaluation, which does not require an end-to-end dialogue system, judges the model in the simulated context of the target task.", "labels": [], "entities": []}, {"text": "The results demonstrate that this in-context evaluation maybe equally useful as comparing against gold standard dialogue act labels, while substantially reducing the time required for human annotation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating unsupervised dialogue act clusters presents numerous challenges.", "labels": [], "entities": [{"text": "dialogue act clusters", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7040036916732788}]}, {"text": "In prior evaluations of query-likelihood clustering, we computed accuracy with respect to the manually applied dialogue act tags described earlier, demonstrating 41.64% accuracy fora model with 8 clusters, compared to 34.90% accuracy for the Rus et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9994194507598877}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9973828196525574}, {"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9965676069259644}]}, {"text": "(2012) k-means approach and 24.48% accuracy for Dirichlet process clustering) on our corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9994615912437439}, {"text": "Dirichlet process clustering", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.5446469783782959}]}, {"text": "However, the goal of the current work is to substantially reduce the human tagging required to evaluate the model.", "labels": [], "entities": []}, {"text": "We also aim to test the hypothesis that comparing against manual labels under-represents the utility of the unsupervised model.", "labels": [], "entities": []}, {"text": "That is, a dialogue policy built on the unsupervised model could perform better than the relatively low classification accuracy for manual tags would suggest.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.732212483882904}]}, {"text": "Our evaluation will explore this hypothesis.", "labels": [], "entities": []}, {"text": "In order to achieve these goals, we first trained an unsupervised dialogue act model on 75% of the corpus using the query-likelihood approach described in Section 3.", "labels": [], "entities": []}, {"text": "The resulting model has 21 clusters.", "labels": [], "entities": []}, {"text": "Then, we handcrafted a dialogue policy for tutor responses by qualitatively examining each cluster of training data and creating one tutor response for each cluster.", "labels": [], "entities": []}, {"text": "Some clusters and their corresponding tutor utterances are depicted in.", "labels": [], "entities": []}, {"text": "This policy was applied by classifying unseen utterances from a held-out test set (25% of the corpus) using the learned model ().", "labels": [], "entities": []}, {"text": "The result of this process is that for each student utterance from the test set, a tutor response is generated based on the policy.", "labels": [], "entities": []}, {"text": "This process resulted in 373 student utterances, one for each utterance in the 25% testing set, each paired with a corresponding tutor response generated by the hand-authored policy.", "labels": [], "entities": []}, {"text": "The evaluation goal is to determine whether the responses made by this policy are reasonable, which will represent the utility of the unsupervised dialogue act model for its intended use within a dialogue manager.", "labels": [], "entities": []}, {"text": "We used human judges to rate the output of the policy.", "labels": [], "entities": []}, {"text": "Thirty student utterances and tutor responses were randomly selected from the available utterances generated by the test set.", "labels": [], "entities": []}, {"text": "An example set of utterances and policies can be seen in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.8334290981292725}]}, {"text": "These items were placed in a survey that asked the reader to rate the extent to which each tutor response makes sense given the student utterance.", "labels": [], "entities": []}, {"text": "(One item was inadvertently omitted from the survey, resulting in 29 items that were evaluated by the judges and that will be analyzed here.)", "labels": [], "entities": []}, {"text": "To avoid bias introduced by the ordering of items, they were presented in a different randomized order for each of the seven judges who completed the survey.", "labels": [], "entities": []}, {"text": "(29 items from a comparison condition using manual tags were also randomly interleaved into the survey, as described later in this section.)", "labels": [], "entities": []}, {"text": "Judges used a rating scale from 1 to 4 (1=makes no sense, 2=makes a little sense, 3=makes a lot of sense, and 4=makes perfect sense).", "labels": [], "entities": []}, {"text": "Since the models only used the current student utterance, the dialogue history was also not shown to the human raters.", "labels": [], "entities": []}, {"text": "Across the seven judges, the average rating of the tutor responses selected by the unsupervised policy was 2.35.", "labels": [], "entities": []}, {"text": "We also collapsed the ratings into positive (\u22652.5 average across seven judges) and negative (<2.5 average).", "labels": [], "entities": []}, {"text": "With this binary categorization, 44.8% of the time tutor responses generated by the unsupervised policy were rated positively.", "labels": [], "entities": []}, {"text": "It is important to note that no information other than dialogue act was considered for generating the tutor responses; the tutor utterances were relatively content-free and based only on the dialogue act categorization given by the unsupervised model.", "labels": [], "entities": []}, {"text": "For comparison, we also constructed a handcrafted dialogue policy using the manual dialogue act labels and applied this policy to the same utterances as were used to evaluate the unsupervised model.", "labels": [], "entities": []}, {"text": "These pairs of student utterances and tutor responses were interleaved randomly on the same survey provided to seven human judges.", "labels": [], "entities": []}, {"text": "The same tutor responses as in the unsupervised policy were used whenever possible for this manual-tag policy.", "labels": [], "entities": []}, {"text": "The tutor responses generated from the manual-tag policy received an average score of 2.22, slightly lower than the average of 2.35 for tutor responses generated by the unsupervised policy.", "labels": [], "entities": []}, {"text": "The binary positivenegative split for these ratings reveals that 31% were rated positively (\u22652.5 average), compared to 44.8% for the unsupervised policy.", "labels": [], "entities": []}, {"text": "Direct comparisons between the unsupervised policy and the manual-tag policy must be interpreted with caution, in part because the unsupervised policy was more granular (based on 21 clusters) than the manual-tag policy (based on 9 tags) and also because it can be difficult to ensure that the two policies were of equal quality.", "labels": [], "entities": []}, {"text": "On the other hand, the unsupervised policy utilized no manual labels and was applied to an unseen test set, while the manual-tag policy was based on reliable tags applied to the actual utterances from the testing set.", "labels": [], "entities": []}, {"text": "Finally, we evaluated the extent to which the 4-category rating scheme was reliable across judges.", "labels": [], "entities": []}, {"text": "The weighted Kappa, used for ordinal scales because it penalizes disagreements less if they are closer together, was 0.30 averaged across all pairs of judges, indicating fair agreement.", "labels": [], "entities": [{"text": "weighted Kappa", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.5126791447401047}]}, {"text": "For the collapsed binary ratings, average pairwise ordinary Kappa was 0.36.", "labels": [], "entities": [{"text": "pairwise ordinary Kappa", "start_pos": 42, "end_pos": 65, "type": "METRIC", "confidence": 0.5548768043518066}]}], "tableCaptions": []}