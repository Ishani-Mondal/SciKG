{"title": [{"text": "A Phrase Orientation Model for Hierarchical Machine Translation", "labels": [], "entities": [{"text": "Phrase Orientation", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.9304825961589813}, {"text": "Hierarchical Machine Translation", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.796900729338328}]}], "abstractContent": [{"text": "We introduce a lexicalized reordering model for hierarchical phrase-based machine translation.", "labels": [], "entities": [{"text": "hierarchical phrase-based machine translation", "start_pos": 48, "end_pos": 93, "type": "TASK", "confidence": 0.5925149321556091}]}, {"text": "The model scores monotone , swap, and discontinuous phrase ori-entations in the manner of the one presented by Tillmann (2004).", "labels": [], "entities": []}, {"text": "While this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems (Koehn et al., 2007), it is however commonly not employed in hierarchical decoders.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 100, "end_pos": 144, "type": "TASK", "confidence": 0.5710986778140068}]}, {"text": "We describe how phrase orientation probabilities can be extracted from word-aligned training data for use with hierarchical phrase inventories, and show how orientations can be scored in hierarchical decoding.", "labels": [], "entities": []}, {"text": "The model is empirically evaluated on the NIST Chinese\u2192English translation task.", "labels": [], "entities": [{"text": "NIST Chinese\u2192English translation task", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.7933189272880554}]}, {"text": "We achieve a significant improvement of +1.2 %BLEU over atypical hierarchical baseline setup and an improvement of +0.7 %BLEU over a syntax-augmented hierarchical setup.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9982750415802002}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9983009696006775}]}, {"text": "On a French\u2192German translation task, we obtain again of up to +0.4 %BLEU.", "labels": [], "entities": [{"text": "French\u2192German translation task", "start_pos": 5, "end_pos": 35, "type": "TASK", "confidence": 0.621260267496109}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9996439218521118}]}], "introductionContent": [{"text": "In hierarchical phrase-based translation), a probabilistic synchronous context-free grammar (SCFG) is induced from bilingual training corpora.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.603099544843038}]}, {"text": "In addition to continuous lexical phrases as in standard phrase-based translation, hierarchical phrases with usually up to two nonterminals are extracted from the word-aligned parallel training data.", "labels": [], "entities": []}, {"text": "Hierarchical decoding is typically carried outwith a parsing-based procedure.", "labels": [], "entities": [{"text": "Hierarchical decoding", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8194668591022491}]}, {"text": "The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.970784604549408}, {"text": "translation candidates", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.9049594104290009}]}, {"text": "During decoding, a hierarchical translation rule implicitly specifies the placement of the target part of a subderivation which is substituting one of its nonterminals in a partial hypothesis.", "labels": [], "entities": []}, {"text": "The hierarchical phrase-based model thus provides an integrated reordering mechanism.", "labels": [], "entities": []}, {"text": "The reorderings which are being conducted by the hierarchical decoder area result of the application of SCFG rules, which generally means that there must have been some evidence in the training data for each reordering operation.", "labels": [], "entities": []}, {"text": "At first glance one might be tempted to believe that any additional designated phrase orientation modeling would be futile in hierarchical translation as a consequence of this.", "labels": [], "entities": [{"text": "phrase orientation modeling", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.7852398256460825}]}, {"text": "We argue that such a conclusion is false, and we will provide empirical evidence in this work that lexicalized phrase orientation scoring can be highly beneficial not only in standard phrase-based systems, but also in hierarchical ones.", "labels": [], "entities": [{"text": "phrase orientation scoring", "start_pos": 111, "end_pos": 137, "type": "TASK", "confidence": 0.8485156695048014}]}, {"text": "The purpose of a phrase orientation model is to assess the adequacy of phrase reordering during search.", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.792432427406311}]}, {"text": "In standard phrase-based translation with continuous phrases only and left-to-right hypothesis generation (, phrase reordering is implemented by jumps within the input sentence.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7151280045509338}]}, {"text": "The choice of the best order for the target sequence is made based on the language model score of this sequence and a distortion cost that is computed from the sourceside jump distances.", "labels": [], "entities": []}, {"text": "Though the space of admissible reorderings is inmost cases contrained by a maximum jump width or coverage-based restrictions () for efficiency reasons, the basic approach of arbitrarily jumping to uncovered positions on source side is still very permissive.", "labels": [], "entities": []}, {"text": "Lexicalized reordering models assist the decoder in taking a good decision.", "labels": [], "entities": []}, {"text": "Phrase-based decoding allows fora straightforward integration of lexicalized reordering models which assign different scores depending on how a currently translated phrase has been reordered with respect to its context.", "labels": [], "entities": []}, {"text": "Popular lexicalized reordering models for phrase-based translation distinguish three orientation classes: monotone, swap, and discontinuous.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.6751147508621216}]}, {"text": "To obtain such a model, scores for the three classes are calculated from the counts of the respective orientation occurrences in the word-aligned training data for each extracted phrase.", "labels": [], "entities": []}, {"text": "The left-to-right orientation of phrases during phrase-based search can be easily determined from the start and end positions of continuous phrases.", "labels": [], "entities": []}, {"text": "Approximations may need to be adopted for the right-to-left scoring direction.", "labels": [], "entities": [{"text": "Approximations", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9753708839416504}]}, {"text": "The utility of phrase orientation models in standard phrase-based translation is plausible and has been empirically established in practice.", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8287779092788696}, {"text": "phrase-based translation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7020295262336731}]}, {"text": "In hierarchical phrase-based translation, some other types of lexicalized reordering models have been investigated recently), but in none of them are the orientation scores conditioned on the lexical identity of each phrase individually.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.5693986614545187}]}, {"text": "These models are rather word-based and applied on block boundaries.", "labels": [], "entities": []}, {"text": "Experimental results obtained with these other types of lexicalized reordering models have been very encouraging, though.", "labels": [], "entities": []}, {"text": "There are certain reasons why assessing the adequacy of phrase reordering should be useful in hierarchical search: \u2022 Albeit phrase reorderings are always a result of the application of SCFG rules, the decoder is still able to choose from many different parses of the input sentence.", "labels": [], "entities": []}, {"text": "\u2022 The decoder can furthermore choose from many translation options for each given parse, which result in different reorderings and different phrases being embedded in the reordering non-terminals.", "labels": [], "entities": []}, {"text": "\u2022 All other models only weakly connect an embedded phrase with the hierarchical phrase it is placed into, in particular as the set of nonterminals of the hierarchical grammar only contains two generic non-terminal symbols.", "labels": [], "entities": []}, {"text": "We therefore investigate phrase orientation modeling for hierarchical translation in this work.", "labels": [], "entities": [{"text": "phrase orientation modeling", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8823857108751932}, {"text": "hierarchical translation", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6903929710388184}]}], "datasetContent": [{"text": "We evaluate the effect of phrase orientation scoring in hierarchical translation on the Chinese\u2192English 2008 NIST task 2 and on the French\u2192German language pair using the standard WMT 3 newstest sets for development and testing.", "labels": [], "entities": [{"text": "phrase orientation scoring", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8057874242464701}, {"text": "Chinese\u2192English 2008 NIST task 2", "start_pos": 88, "end_pos": 120, "type": "DATASET", "confidence": 0.7065187692642212}, {"text": "WMT 3 newstest sets", "start_pos": 179, "end_pos": 198, "type": "DATASET", "confidence": 0.8597867786884308}]}, {"text": "We work with a Chinese-English parallel training corpus of 3.0 M sentence pairs (77.5 M Chinese / 81.0 M English running words).", "labels": [], "entities": []}, {"text": "To train the German\u2192French baseline system, we use 2.0 M sentence pairs (53.1 M French / 45.8 M German running words) that are partly taken from the Europarl corpus () and have partly been collected within the Quaero project.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 149, "end_pos": 164, "type": "DATASET", "confidence": 0.9833577573299408}]}, {"text": "Word alignments are created by aligning the data in both directions with GIZA++ 5 and symmetrizing the two trained alignments.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6365498453378677}]}, {"text": "When extracting phrases, we apply several restrictions, in particular a maximum length often on source and target side for lexical phrases, a length limit of five on source and tenon target side for hierarchical phrases (including non-terminal symbols), and no more than two non-terminals per phrase.", "labels": [], "entities": []}, {"text": "A standard set of models is used in the baselines, comprising phrase translation probabilities and lexical translation probabilities in both directions, word and phrase penalty, binary features marking hierarchical rules, glue rule, and rules with non-terminals at the boundaries, three simple count-based binary features, phrase length ratios, and a language model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.723061129450798}]}, {"text": "The language models are 4-grams with modified Kneser-Ney smoothing () which have been trained with the SRILM toolkit.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.830818235874176}]}, {"text": "Model weights are optimized against BLEU) with MERT (Och, 2003) on 100-best lists.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.99818354845047}, {"text": "MERT", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9763279557228088}, {"text": "Och, 2003)", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.7462083399295807}]}, {"text": "For Chinese\u2192English we employ MT06 as development set, MT08 is used as unseen test set.", "labels": [], "entities": [{"text": "MT08", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8258253931999207}]}, {"text": "For German\u2192French we employ newstest2009 as development set, newstest2008, newstest2010, and newstest2011 are used as unseen test sets.", "labels": [], "entities": []}, {"text": "During decoding, a maximum length constraint often is applied to all non-terminals except the initial symbol S . Translation quality is measured in truecase with BLEU and TER).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9990623593330383}, {"text": "TER", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.993992030620575}]}, {"text": "The results on MT08 are checked for statistical significance over the baseline.", "labels": [], "entities": [{"text": "MT08", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.5599849224090576}, {"text": "statistical significance", "start_pos": 36, "end_pos": 60, "type": "METRIC", "confidence": 0.769157737493515}]}, {"text": "Confidence intervals have been computed using bootstrapping for BLEU and Cochran's approximate ratio variance for TER ().", "labels": [], "entities": [{"text": "Confidence intervals", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9430663287639618}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9967029690742493}, {"text": "approximate ratio variance", "start_pos": 83, "end_pos": 109, "type": "METRIC", "confidence": 0.8890081246693929}, {"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9844637513160706}]}, {"text": "comprises all results of our empirical evaluation on the Chinese\u2192English task.", "labels": [], "entities": []}, {"text": "We first compare the performance of the phrase orientation model in left-to-right direction only with the performance of the phrase orientation model in left-to-right and right-to-left direction (bidirectional).", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7700552642345428}]}, {"text": "In all experiments, monotone, swap, and discontinuous orientation costs are treated as being from different feature functions in the log-linear model combination: we assign a separate scaling factor to each of the orientations.", "labels": [], "entities": []}, {"text": "We have three more scaling factors than in the baseline for left-to-right direction only, and six more scaling factors for bidirectional phrase orientation scoring.", "labels": [], "entities": [{"text": "phrase orientation scoring", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.8050659000873566}]}, {"text": "As can be seen from the results table, the left-to-right model already yields again of 1.1 %BLEU over the baseline on the unseen test set (MT08).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9995158910751343}, {"text": "MT08", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.8583265542984009}]}, {"text": "The bidirectional model performs just slightly better (+1.2 %BLEU over the baseline).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9993157386779785}]}, {"text": "With both models, the TER is reduced significantly as well (-1.1 / -1.3 compared to the baseline).", "labels": [], "entities": [{"text": "TER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9977235198020935}]}, {"text": "We adopted the discriminative lexicalized reordering model (discrim.", "labels": [], "entities": []}, {"text": "RO) that has been suggested by for comparison purposes.", "labels": [], "entities": [{"text": "RO", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.955694317817688}]}, {"text": "The phrase orientation model provides clearly better translation quality in our experiments.: Experimental results for the NIST Chinese\u2192English translation task (truecase).", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7676625847816467}, {"text": "NIST Chinese\u2192English translation task", "start_pos": 123, "end_pos": 160, "type": "TASK", "confidence": 0.7235285739103953}]}, {"text": "On the test set, bold font indicates results that are significantly better than the baseline (p < .05).", "labels": [], "entities": []}, {"text": "As a next experiment, we bring in more reordering capabilities by augmenting the hierarchical grammar with a single swap rule supplementary to the initial rule and glue rule.", "labels": [], "entities": []}, {"text": "The swap rule allows adjacent phrases to be transposed.", "labels": [], "entities": []}, {"text": "The setup with swap rule and bidirectional phrase orientation model is about as good as the setup with just the bidirectional phrase orientation model and no swap rule.", "labels": [], "entities": []}, {"text": "If we furthermore mark the swap rule with a binary feature (binary swap feature), we end up at an improvement of +1.4 %BLEU over the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9993674159049988}]}, {"text": "The phrase orientation model again provides higher translation quality than the discriminative reordering model.", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8041452765464783}]}, {"text": "Ina third experiment, we investigate whether the phrase orientation model also has a positive influence when integrated into a syntax-augmented hierarchical system.", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7629083395004272}]}, {"text": "We configured a hierarchical setup with soft syntactic labels ( , a syntactic enhancement in the manner of preference grammars ().", "labels": [], "entities": []}, {"text": "On MT08, the syntax-augmented system performs 0.9 %BLEU above the baseline setup.", "labels": [], "entities": [{"text": "MT08", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.9185830354690552}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9992278814315796}]}, {"text": "We achieve an additional improvement of +0.7 %BLEU and -1.3 TER by including the bidirectional phrase orientation model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9995627999305725}, {"text": "TER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9932924509048462}, {"text": "phrase orientation", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.6744343489408493}]}, {"text": "Interestingly, the translation quality of the setup with soft syntactic labels (but without phrase orientation model) is worse than of the setup with phrase orientation model (but without soft syntactic labels) on MT08.", "labels": [], "entities": [{"text": "MT08", "start_pos": 214, "end_pos": 218, "type": "DATASET", "confidence": 0.9466724991798401}]}, {"text": "The combination of both extensions provides the best result, though.", "labels": [], "entities": []}, {"text": "Ina last experiment, we finally took a very strong setup which improves over the baseline by 2.5 %BLEU through the integration of phrase-level discriminative word lexicon (DWL) models and triplet lexicon models in source-to-target (s2t) and target-to-source (t2s) direction.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9997466206550598}]}, {"text": "The models have been presented by,.", "labels": [], "entities": []}, {"text": "We apply them in a similar manner as proposed by.", "labels": [], "entities": []}, {"text": "In this strong setup, the discriminative reordering model gives gains on the development set which barely carryover to the test set.", "labels": [], "entities": []}, {"text": "Adding the bidirectional phrase orientation model, in contrast, results in a nice gain of +0.7 %BLEU and a reduction of 1.3 points in TER on the test set, even on top of the DWL and triplet lexicon models.", "labels": [], "entities": [{"text": "phrase orientation", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.6935726553201675}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9996758699417114}, {"text": "TER", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9986754059791565}]}, {"text": "comprises the results of our empirical evaluation on the French\u2192German task.", "labels": [], "entities": []}, {"text": "The left-to-right phrase orientation model boosts the translation quality by up to 0.3 %BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.999262273311615}]}, {"text": "The reduction in TER is in a similar order of magnitude.", "labels": [], "entities": [{"text": "TER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9996384382247925}]}, {"text": "The bidirectional model performs a bit better again, with an advancement of up to 0.4 %BLEU and a maximal reduction in TER of 0.6 points.: Experimental results for the French\u2192German translation task (truecase).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9997095465660095}, {"text": "TER", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9589022397994995}, {"text": "French\u2192German translation task", "start_pos": 168, "end_pos": 198, "type": "TASK", "confidence": 0.591499137878418}]}, {"text": "newstest2009 is used as development set.", "labels": [], "entities": [{"text": "newstest2009", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9145144820213318}]}], "tableCaptions": [{"text": " Table 1: Experimental results for the NIST Chinese\u2192English translation task (truecase). On the test set,  bold font indicates results that are significantly better than the baseline (p < .05).", "labels": [], "entities": [{"text": "NIST Chinese\u2192English translation task", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.7206461628278097}]}, {"text": " Table 2: Experimental results for the French\u2192German translation task (truecase). newstest2009 is used  as development set.", "labels": [], "entities": [{"text": "French\u2192German translation task", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6334596455097199}, {"text": "newstest2009", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9403514266014099}]}]}