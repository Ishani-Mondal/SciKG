{"title": [{"text": "Shallow Semantic Analysis of Interactive Learner Sentences", "labels": [], "entities": [{"text": "Semantic Analysis of Interactive Learner Sentences", "start_pos": 8, "end_pos": 58, "type": "TASK", "confidence": 0.7608259071906408}]}], "abstractContent": [{"text": "Focusing on applications for analyzing learner language which evaluate semantic appropri-ateness and accuracy, we collect data from a task which models some aspects of interaction , namely a picture description task (PDT).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9980096220970154}, {"text": "picture description task (PDT)", "start_pos": 191, "end_pos": 221, "type": "TASK", "confidence": 0.7710612416267395}]}, {"text": "We parse responses to the PDT into dependency graphs with an an off-the-shelf parser, then use a decision tree to classify sentences into syntactic types and extract the logical subject , verb, and object, finding 92% accuracy in such extraction.", "labels": [], "entities": [{"text": "parse responses to the PDT", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.7089700698852539}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9993627667427063}]}, {"text": "The specific goal in this paper is to examine the challenges involved in extracting these simple semantic representations from interactive learner sentences.", "labels": [], "entities": [{"text": "extracting these simple semantic representations from interactive learner sentences", "start_pos": 73, "end_pos": 156, "type": "TASK", "confidence": 0.6365022559960684}]}], "introductionContent": [], "datasetContent": [{"text": "To evaluate this work, we need to address two major questions.", "labels": [], "entities": []}, {"text": "First, how accurately do we extract semantic information from potentially innovative sentences (section 5.2)?", "labels": [], "entities": []}, {"text": "Due to the simple structures of the sentences (section 5.1), we find high accuracy with our simple system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9991661310195923}]}, {"text": "Secondly, how many semantic forms does one need in order to capture the variability in meaning in learner sentences (section 5.3)?", "labels": [], "entities": []}, {"text": "We operationalize this second question by asking how well the set of native speaker semantic forms models a gold standard, with the intuition that a language is defined by native speaker usage, so their answers can serve as targets.", "labels": [], "entities": []}, {"text": "As we will see, this is a na\u00a8\u0131vena\u00a8\u0131ve view.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentence type examples, with distributions of types for native speakers (NS) and non-native speakers (NNS)", "labels": [], "entities": []}, {"text": " Table 3: Matching of semantic triples: NS/NNS: number of unique triples for NSs/NNSs. Comparing NNS types to NS  triples, TP: number of true positives (types); TN: number of true negatives; FN: number of false negatives.", "labels": [], "entities": [{"text": "FN", "start_pos": 191, "end_pos": 193, "type": "METRIC", "confidence": 0.997617781162262}]}, {"text": " Table 5: Distribution of valid tokens across types for a  single PDT item. Types in italics do not occur in the NS  sample, but could be inferred to expand coverage by re- combining elements of NS types that do occur.", "labels": [], "entities": [{"text": "NS  sample", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.8588933348655701}]}]}