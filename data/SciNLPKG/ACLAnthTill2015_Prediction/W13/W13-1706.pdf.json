{"title": [{"text": "A Report on the First Native Language Identification Shared Task", "labels": [], "entities": [{"text": "First Native Language Identification Shared", "start_pos": 16, "end_pos": 59, "type": "TASK", "confidence": 0.7477755904197693}]}], "abstractContent": [{"text": "Native Language Identification, or NLI, is the task of automatically classifying the L1 of a writer based solely on his or her essay written in another language.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5931969185670217}]}, {"text": "This problem area has seen a spike in interest in recent years as it can have an impact on educational applications tailored towards non-native speakers of a language, as well as authorship profiling.", "labels": [], "entities": [{"text": "authorship profiling", "start_pos": 179, "end_pos": 199, "type": "TASK", "confidence": 0.8317768573760986}]}, {"text": "While there has been a growing body of work in NLI, it has been difficult to compare methodologies because of the different approaches to pre-processing the data, different sets of languages identified, and different splits of the data used.", "labels": [], "entities": []}, {"text": "In this shared task, the first ever for Native Language Identification, we sought to address the above issues by providing a large corpus designed specifically for NLI, in addition to providing an environment for systems to be directly compared.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6520313819249471}]}, {"text": "In this paper, we report the results of the shared task.", "labels": [], "entities": []}, {"text": "A total of 29 teams from around the world competed across three different sub-tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "One quickly growing subfield in NLP is the task of identifying the native language (L1) of a writer based solely on a sample of their writing in another language.", "labels": [], "entities": [{"text": "identifying the native language (L1) of a writer", "start_pos": 51, "end_pos": 99, "type": "TASK", "confidence": 0.7971657395362854}]}, {"text": "The task is framed as a classification problem where the set of L1s is known a priori.", "labels": [], "entities": []}, {"text": "Most work has focused on identifying the native language of writers learning English as a second language.", "labels": [], "entities": []}, {"text": "To date this topic has motivated several papers and research projects.", "labels": [], "entities": []}, {"text": "Native Language Identification (NLI) can be useful fora number of applications.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7612890104452769}]}, {"text": "NLI can be used in educational settings to provide more targeted feedback to language learners about their errors.", "labels": [], "entities": []}, {"text": "It is well known that speakers of different languages make different kinds of errors when learning a language).", "labels": [], "entities": []}, {"text": "A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner's language.", "labels": [], "entities": []}, {"text": "In addition, native language is often used as a feature that goes into authorship profiling (, which is frequently used in forensic linguistics.", "labels": [], "entities": [{"text": "authorship profiling", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.8118897080421448}]}, {"text": "Despite the growing interest in this field, development has been encumbered by two issues.", "labels": [], "entities": []}, {"text": "First is the issue of data.", "labels": [], "entities": []}, {"text": "Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer.", "labels": [], "entities": []}, {"text": "Because of a scarcity of such corpora, most work has used the International Corpus of Learner English (ICLEv2) () for training and evaluation since it contains several hundred essays written by college-level English language learners.", "labels": [], "entities": [{"text": "International Corpus of Learner English (ICLEv2)", "start_pos": 62, "end_pos": 110, "type": "DATASET", "confidence": 0.9539067670702934}]}, {"text": "However, this corpus is quite small for training and testing statistical systems which makes it difficult to tell whether the systems that are developed can scale well to larger data sets or to different domains.", "labels": [], "entities": []}, {"text": "Since the ICLE corpus was not designed with the task of NLI in mind, the usability of the corpus for this task is further compromised by idiosyncrasies in the data such as topic bias (as shown by) and the occurrence of characters which only appear in essays written by speakers of certain languages.", "labels": [], "entities": [{"text": "ICLE corpus", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.7735827267169952}]}, {"text": "As a result, it is hard to draw conclusions about which features actually perform best.", "labels": [], "entities": []}, {"text": "The second issue is that there has been little consistency in the field in the use of cross-validation, the number of L1s, and which L1s are used.", "labels": [], "entities": []}, {"text": "As a result, comparing one approach to another has been extremely difficult.", "labels": [], "entities": []}, {"text": "The first Shared Task in Native Language Identification is intended to better unify this community and help the field progress.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6175225973129272}]}, {"text": "The Shared Task addresses the two deficiencies above by first using anew corpus (TOEF11, discussed in Section 3) that is larger than the ICLE and designed specifically for the task of NLI and second, by providing a common set of L1s and evaluation standards that everyone will use for this competition, thus facilitating direct comparison of approaches.", "labels": [], "entities": [{"text": "TOEF11", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.6956600546836853}, {"text": "ICLE", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.5445559024810791}]}, {"text": "In this report we describe the methods most participants used, the data they evaluated their systems on, the three sub-tasks involved, the results achieved by the different teams, and some suggestions and ideas about what we can do for the next iteration of the NLI shared task.", "labels": [], "entities": []}, {"text": "In the following section, we provide a summary of the prior work in Native Language Identification.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.6506882111231486}]}, {"text": "Next, in Section 3 we describe the TOEFL11 corpus used for training, development and testing in this shared task.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.8992405831813812}]}, {"text": "Section 4 describes the three sub-tasks of the NLI Shared Task as well as a review of the timeline.", "labels": [], "entities": []}, {"text": "Section 5 lists the 29 teams that participated in the shared task, and introduce abbreviations that will be used throughout this paper.", "labels": [], "entities": []}, {"text": "Sections 6 and 7 describe the results of the shared task and a separate post shared task evaluation where we asked teams to evaluate their system using cross-validation on a combination of the training and development data.", "labels": [], "entities": []}, {"text": "In Section 8 we provide a high-level view of the common features and machine learning methods teams tended to use.", "labels": [], "entities": []}, {"text": "Finally, we offer conclusions and ideas for future instantiations of the shared task in Section 9.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of essays per language per prompt in each data set", "labels": [], "entities": []}, {"text": " Table 3: Results for closed task", "labels": [], "entities": []}, {"text": " Table 4: Results for open-1 task", "labels": [], "entities": []}, {"text": " Table 5: Results for open-2 task", "labels": [], "entities": []}, {"text": " Table 6: Results for 10-fold cross-validation on  TOEFL11-TRAIN + TOEFL11-DEV", "labels": [], "entities": [{"text": "TOEFL11-TRAIN", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.7902156114578247}, {"text": "TOEFL11-DEV", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.531385064125061}]}, {"text": " Table 8: Common Features used in Shared Task", "labels": [], "entities": [{"text": "Shared Task", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8484425246715546}]}]}