{"title": [{"text": "Investigations in Exact Inference for Hierarchical Translation", "labels": [], "entities": [{"text": "Exact Inference", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.9399372041225433}, {"text": "Hierarchical Translation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8929024636745453}]}], "abstractContent": [{"text": "We present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6600894033908844}]}, {"text": "We also present a first implementation of that method along with experimental results shedding light on some fundamental issues.", "labels": [], "entities": []}, {"text": "In hierarchical translation , inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model.", "labels": [], "entities": [{"text": "hierarchical translation", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6584562808275223}]}, {"text": "We replace this intractable distribution by a sequence of tractable upper-bounds for which exact optimisers and samplers are easy to obtain.", "labels": [], "entities": []}, {"text": "Our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection, without recourse to pruning techniques that only provide approximate solutions.", "labels": [], "entities": [{"text": "exact inference", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.728164553642273}]}, {"text": "While the current implementation is limited in the size of inputs it can handle in reasonable time, our experiments provide insights towards obtaining future speedups, while staying in the same general framework.", "labels": [], "entities": []}], "introductionContent": [{"text": "In statistical machine translation (SMT), optimisation -the task of searching for an optimum translation -is performed over a high-complexity distribution defined by the intersection between a translation hypergraph and a target language model (LM).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8164127916097641}]}, {"text": "This distribution is too complex to be represented exactly and one typically resorts to approximation techniques such as beam-search () and cube-pruning, where maximisation is performed over a pruned representation of the full distribution.", "labels": [], "entities": []}, {"text": "Often, rather than finding a single optimum, one is really interested in obtaining a set of probabilistic samples from the distribution.", "labels": [], "entities": []}, {"text": "This is the case for minimum error rate training, minimum risk training () and minimum risk decoding ().", "labels": [], "entities": []}, {"text": "Due to the additional computational challenges posed by sampling, n-best lists, a by-product of optimisation, are typically used as approximation to true probabilistic samples.", "labels": [], "entities": []}, {"text": "A known issue with n-best lists is that they tend to be clustered around only one mode of the distribution.", "labels": [], "entities": []}, {"text": "A more direct procedure is to attempt to directly draw samples from the underlying distribution rather than rely on n-best list approximations (.", "labels": [], "entities": []}, {"text": "OS * () is a recent approach that stresses a unified view between the two types of inference, optimisation and sampling.", "labels": [], "entities": []}, {"text": "In this view, rather than resorting to pruning in order to cope with the tractability issues, one upperbounds the complex goal distribution with a simpler \"proposal\" distribution for which dynamic programming is feasible.", "labels": [], "entities": []}, {"text": "This proposal is incrementally refined to be closer to the goal until the maximum is found, or until the sampling performance exceeds a certain level.", "labels": [], "entities": []}, {"text": "This paper applies the OS * approach to the problem of inference in hierarchical SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.7391993999481201}]}, {"text": "Ina nutshell, the idea is to replace the intractable problem of intersecting a contextfree grammar with a full language model by the tractable problem of intersecting it with a simplified, optimistic version of this LM which \"forgets\" parts of n-gram contexts, and to incrementally add more context based on evidence of the need to do so.", "labels": [], "entities": []}, {"text": "Evidence is gathered by optimising or sampling from the tractable proxy distribution and focussing on the most serious over-optimistic estimates relative to the goal distribution.", "labels": [], "entities": []}, {"text": "Our main contribution is to provide an exact optimiser/sampler for hierarchical SMT that is efficient in exploring only a small fraction of the space of n-grams involved in a full intersection.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.7669766545295715}]}, {"text": "Although at this stage our experiments are limited to short sentences, they provide insights on the behavior of the technique and indicate directions towards a more efficient implementation within the same paradigm.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: \u00a72 provides background on OS * and hierarchical translation; \u00a73 describes our approach to exact inference in SMT; in \u00a74 the experimental setup is presented and findings are discussed; \u00a75 discusses related work, and \u00a76 concludes.", "labels": [], "entities": [{"text": "hierarchical translation", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.6837385445833206}, {"text": "SMT", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.9633379578590393}]}], "datasetContent": [{"text": "We used the Moses toolkit ( to extract a SCFG following from the 6 th version of the Europarl collection () (German-English portion).", "labels": [], "entities": [{"text": "Europarl collection", "start_pos": 85, "end_pos": 104, "type": "DATASET", "confidence": 0.9205246567726135}]}, {"text": "We trained language models using lmplz and interpolated the models trained on the English monolingual data made available by the WMT (Callison-Burch et al., 2012) (i.e. Europarl, newscommentaries, news-2012 and commoncrawl).", "labels": [], "entities": [{"text": "WMT", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.6419482827186584}, {"text": "Europarl", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.9697229862213135}, {"text": "commoncrawl", "start_pos": 211, "end_pos": 222, "type": "DATASET", "confidence": 0.8898066878318787}]}, {"text": "Tuning was performed via MERT using newstest2010 as development set; test sentences were extracted from newstest2011.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9634311199188232}, {"text": "MERT", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8897022008895874}]}, {"text": "Finally, we restricted our SCFGs to having at most 10 target productions fora given source production.", "labels": [], "entities": []}, {"text": "shows some properties of the initial grammar G(f ) as a function of the input sentence length (the quantities are averages over 20 sentences for each class of input length).", "labels": [], "entities": []}, {"text": "The number of unigrams grows linearly with the input length, while the number of unique bigrams compatible with strings generated by G(f ) appears to grow quadratically 9 and the size of the grammar in number of rules appears to be cubic -a consequence of having up to two nonterminals on the right-hand side of a rule.", "labels": [], "entities": []}, {"text": "shows the number of refinement operations until convergence in optimisation and sampling, as well as the total duration, as a function of the input length.", "labels": [], "entities": []}, {"text": "The plots will be discussed in detail below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Optimisation steps showing the iteration  (i), the number of rules in the grammar and the  translation associated to the optimum derivation.", "labels": [], "entities": []}, {"text": " Table 2: Optimisation with a 4-gram LM.", "labels": [], "entities": []}, {"text": " Table 3: Sampling with a 4-gram LM and reaching  a 5% acceptance rate.", "labels": [], "entities": [{"text": "Sampling", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9706534147262573}, {"text": "acceptance rate", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.9852192401885986}]}]}