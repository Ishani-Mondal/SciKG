{"title": [], "abstractContent": [{"text": "This paper describes the work process fora Multilingual Treebank Annotation Project executed for Google and coordinated by a small core team supervising the linguistic work conducted by linguists working online in various locations across the globe.", "labels": [], "entities": []}, {"text": "The task is to review an output of a dependency syntactic parser, including the POS types, dependency types and relations between the tokens, fix errors in output and prepare the data to a shape that can be used for further training of the parser engine.", "labels": [], "entities": []}, {"text": "In this paper we focus on the implemented Quality Assurance processes and methodology that are used to monitor the output of the four language teams engaged in the project.", "labels": [], "entities": []}, {"text": "On the quantitative side we monitor the throughput to spot any issues in particular language that would require intervention or improving the process.", "labels": [], "entities": []}, {"text": "This is combined with a qualitative analysis that is performed primarily by comparing the incoming parsed data, the reviewed data after the first round and after the final cross-review using snapshots to compile and compare statistics.", "labels": [], "entities": []}, {"text": "In addition, the possible inconsistencies in the annotations are checked and corrected automatically , where possible, inappropriate stages of the process to minimize the manual work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multilingual dependency parsing has become an important part of dependency parsing tasks, mainly due to growing needs of the crosslanguage sources for supporting machine translation, search and retrieval and other natural language applications.", "labels": [], "entities": [{"text": "Multilingual dependency parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6555360953013102}, {"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8274796605110168}, {"text": "machine translation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.7161798477172852}]}, {"text": "Different approaches to processing multilingual data have been investigated in recent years and their outputs compared in a series of CoNLL shared tasks on multilingual dependency parsing (.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 156, "end_pos": 187, "type": "TASK", "confidence": 0.6094819009304047}]}, {"text": "One of the possibilities for building multilingual parsers is training parsers from annotated data that was presented e.g. in models developed by and.", "labels": [], "entities": []}, {"text": "Preparing an annotated Treebank for training purposes is a resource-intensive task.", "labels": [], "entities": []}, {"text": "For that reason, such tasks have to be planned and coordinated in such away that the work is processed efficiently and unnecessary costs are eliminated.", "labels": [], "entities": []}, {"text": "One manner of achieving efficiency is to prepare annotated data for multiple languages at the same time, which allows the data annotation provider to establish a consistent environment for creating and maintaining cross-language annotation guidelines and processes.", "labels": [], "entities": []}, {"text": "In our current project, we are working with Google to review and prepare annotated data for training a multilingual parser using the Stanford typed dependencies modela simple model represented by part of speech and dependency relation types recognizable across languages.", "labels": [], "entities": []}, {"text": "The scope of this project covers manual review of 15 000 parsed sentences for each of four involved languages -German, French, Spanish and Brazilian Portuguese.", "labels": [], "entities": []}, {"text": "For German, Spanish and French, a supervised training model is used for parsing the data before annotation ().", "labels": [], "entities": []}, {"text": "For Brazilian Portuguese, a cross-lingual parser is used), where delexicalized model is trained on Spanish and French data with assistance of the part of speech tagger ().", "labels": [], "entities": []}, {"text": "Data corpus used for parsing is domain-based, the current scope of the project does not target representativeness.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.983988344669342}]}, {"text": "For German, French and Spanish, Wikipedia texts were used as the main data source, for Brazilian Portuguese, mainly news texts were included.", "labels": [], "entities": []}, {"text": "Brazilian Portuguese also follows a different timeline and for that reason, we don\u00b4t present any results gained for this language in the current paper.", "labels": [], "entities": []}, {"text": "Data are batched in groups of 100-500 sentences per file.", "labels": [], "entities": []}, {"text": "The parsing system performs the tokenization of the data that separates punctuation as individual tokens.", "labels": [], "entities": []}, {"text": "The pre-parsed data contains three levels of annotation: part-of-speech (POS) labels, binary dependencies between the tokens and dependen-cy relation labels (deprel).", "labels": [], "entities": []}, {"text": "All these levels are reviewed and corrected in the process.", "labels": [], "entities": []}, {"text": "The number of dependency relations varies slightly between the languages.", "labels": [], "entities": []}, {"text": "Some 51 to 57 labels are used.", "labels": [], "entities": []}, {"text": "However, one of the targets of the project is to review the inventory of the relations to ensure uniform representation across the languages and some adjustments to the inventory of the dependency labels were made in the initial phase of the project.", "labels": [], "entities": []}, {"text": "Files are processed in PML format () using the Tree Editor TrEd 2.0 () and CoNLL2009 stylesheet extension.", "labels": [], "entities": [{"text": "CoNLL2009 stylesheet", "start_pos": 75, "end_pos": 95, "type": "DATASET", "confidence": 0.8539185225963593}]}, {"text": "In order to seethe parser engine improvement and to be most efficient with the manual annotation, the data is processed in sprints made up of around 1500 sentences.", "labels": [], "entities": []}, {"text": "Each sprint follows the cycle described in section 5 in this paper and its output is used for training the parser.", "labels": [], "entities": []}, {"text": "Parser performance improves with each training and each manual annotation round requires less effort and time.", "labels": [], "entities": [{"text": "Parser", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7016553282737732}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Example of throughput statistics. The  figures are the number of sentences processed  by individual annotators", "labels": [], "entities": []}, {"text": " Table 3. Throughput development comparison  for French, Spanish and German.", "labels": [], "entities": []}]}