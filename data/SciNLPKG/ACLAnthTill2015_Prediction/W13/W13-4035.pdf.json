{"title": [{"text": "POMDP-based dialogue manager adaptation to extended domains", "labels": [], "entities": [{"text": "POMDP-based dialogue manager adaptation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7269736230373383}]}], "abstractContent": [{"text": "Existing spoken dialogue systems are typically designed to operate in a static and well-defined domain, and are not well suited to tasks in which the concepts and values change dynamically.", "labels": [], "entities": []}, {"text": "To handle dynamically changing domains, techniques will be needed to transfer and reuse existing dialogue policies and rapidly adapt them using a small number of dialogues in the new domain.", "labels": [], "entities": []}, {"text": "As a first step in this direction , this paper addresses the problem of automatically extending a dialogue system to include anew previously unseen concept (or slot) which can be then used as a search constraint in an information query.", "labels": [], "entities": []}, {"text": "The paper shows that in the context of Gaussian process POMDP optimi-sation, a domain can be extended through a simple expansion of the kernel and then rapidly adapted.", "labels": [], "entities": []}, {"text": "As well as being much quicker, adaptation rather than retraining from scratch is shown to avoid subjecting users to unacceptably poor performance during the learning stage.", "labels": [], "entities": []}], "introductionContent": [{"text": "Existing spoken dialogue systems are typically designed to operate in a static and well-defined domain, and are not well suited to tasks in which the concepts and values change dynamically.", "labels": [], "entities": []}, {"text": "For example, consider a spoken dialogue system installed in a car, which is designed to provide information about nearby hotels and restaurants.", "labels": [], "entities": []}, {"text": "In this case, not only will the data change as the car moves around, but the concepts (or slots) that a user might wish to use to frame a query will also change.", "labels": [], "entities": []}, {"text": "For example, a restaurant system designed to be used within cities might not have the concept of 'al fresco' dining and could not therefore handle a query such as \"Find me a French restaurant where I can eat outside\".", "labels": [], "entities": []}, {"text": "In order to make this possible, techniques will be needed to extend and adapt existing dialogue policies.", "labels": [], "entities": []}, {"text": "Adaptation can be viewed as a process of improving action selection in a different condition to the one in which the policy was originally trained.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9795742034912109}]}, {"text": "While adaptation has been extensively studied in speech recognition (see an overview in (), in spoken dialogue systems it is still relatively novel and covers a wide range of possible research topics (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.750981479883194}]}, {"text": "A recent trend in statistical dialogue modelling has been to model dialogue as a partially observable Markov decision process (POMDP).", "labels": [], "entities": [{"text": "statistical dialogue modelling", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.7758789459864298}]}, {"text": "This provides increased robustness to errors in speech understanding and automatic dialogue policy optimisation via reinforcement learning (.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.718835636973381}]}, {"text": "A POMDP-based dialogue manager maintains a distribution over every possible dialogue state at every dialogue turn.", "labels": [], "entities": []}, {"text": "This is called the belief state.", "labels": [], "entities": []}, {"text": "Based on that distribution the system chooses the action that gives the highest expected reward, measured by the Q-function.", "labels": [], "entities": []}, {"text": "The Q-function fora belief state and an action is the expected cumulative reward that can be obtained if that action is taken in that belief state.", "labels": [], "entities": []}, {"text": "The optimisation typically requires O(10 5 ) to O(10 6 ) dialogues, so is normally done in interaction with a simulated user).", "labels": [], "entities": []}, {"text": "In reinforcement learning, policy adaptation has been addressed in the context of transfer learning.", "labels": [], "entities": [{"text": "policy adaptation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7695739567279816}, {"text": "transfer learning", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.8906313180923462}]}, {"text": "The core idea is to exploit expertise gained in one domain (source domain) to improve learning in another domain (target domain).", "labels": [], "entities": []}, {"text": "A number of techniques have been developed but they have not been previously applied to dialogue management.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.8933840394020081}]}, {"text": "Gaussian process (GP) based reinforcement learning ( has been recently applied to POMDP dialogue policy optimisation in order to exploit the correlations between different belief states and thus reduce the number of dialogues needed for the learning process).", "labels": [], "entities": [{"text": "POMDP dialogue policy optimisation", "start_pos": 82, "end_pos": 116, "type": "TASK", "confidence": 0.5691195428371429}]}, {"text": "An important feature of a Gaussian process is that it can incorporate a prior mean and variance for the function it estimates, in this case the Qfunction.", "labels": [], "entities": []}, {"text": "Setting these appropriately can significantly speedup the process of learning.", "labels": [], "entities": []}, {"text": "If the mean or the variance are estimated in one environment, for example a particular user type or a particular domain, they can be used as a prior for adaptation in a different environment, i.e. another user type or another domain.", "labels": [], "entities": []}, {"text": "A Gaussian process does not depend on the belief state but on the correlation between two belief states encoded by the kernel function.", "labels": [], "entities": []}, {"text": "Therefore, if one defines a kernel function for two belief states in one domain, the policy can be used in a different domain, provided that the correlations between belief states follow a similar pattern.", "labels": [], "entities": []}, {"text": "This paper explores the problem of extending an existing domain by introducing a previously unseen slot.", "labels": [], "entities": []}, {"text": "Specifically, a simple restaurant system is considered which allows a user to search for restaurants based on food-type and area.", "labels": [], "entities": []}, {"text": "This domain is then extended by introducing an additional price-range slot.", "labels": [], "entities": []}, {"text": "The policy is trained for the basic two-slot domain and then reused in the extended domain by defining a modified kernel function and using adaptation.", "labels": [], "entities": []}, {"text": "This strategy not only allows for the knowledge of a previously trained policy to be reused but it also guards against poor performance in the early stages of learning.", "labels": [], "entities": []}, {"text": "This is particularly useful in a real-world situation where the adaptation is performed indirect interaction with users.", "labels": [], "entities": []}, {"text": "In addition, a potential application of this technique to reduce the number of training dialogues is examined.", "labels": [], "entities": []}, {"text": "The domain is decomposed into a series of simple domains and the policy is gradually adapted to the final domain with a smaller number of dialogues than are normally needed for training.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 the background on Gaussian processes in POMDP optimisation is given.", "labels": [], "entities": [{"text": "POMDP optimisation", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.5831556618213654}]}, {"text": "Then Section 3 gives a description of the Bayesian Update of Dialogue State dialogue manager, which is used as a test-bed for the experiments.", "labels": [], "entities": []}, {"text": "In Section 4, a simple method of kernel modification is described which allows a policy trained in the basic domain to be used in an extended domain.", "labels": [], "entities": [{"text": "kernel modification", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.802210122346878}]}, {"text": "Methods of fast adaptation are investigated in Section 5 and this adaptation strategy is then tested via interaction with humans using the Amazon Mechanical Turk service in Section 6.", "labels": [], "entities": [{"text": "fast adaptation", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.73520627617836}, {"text": "Amazon Mechanical Turk service", "start_pos": 139, "end_pos": 169, "type": "DATASET", "confidence": 0.940664991736412}]}, {"text": "Finally, the use of repeated adaptation to speedup the process of policy optimisation by learning gradually from simple to more complex domains is explored in Section 7, before presenting conclusions in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to adapt and evaluate policies with humans, we used crowd-sourcing via the Amazon Mechanical Turk service in a set-up similar to).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk service", "start_pos": 84, "end_pos": 114, "type": "DATASET", "confidence": 0.8415785133838654}]}, {"text": "The BUDS dialogue manager was incorporated in a live telephone-based spoken dialogue system.", "labels": [], "entities": [{"text": "BUDS dialogue manager", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.687716523806254}]}, {"text": "The Mechanical Turk users were assigned specific tasks in the extended TopTable domain.", "labels": [], "entities": [{"text": "TopTable domain", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9435503780841827}]}, {"text": "They were asked to find restaurants that have particular features as defined by the given task.", "labels": [], "entities": []}, {"text": "To elicit more complex dialogues, the users were sometimes asked to find more than one restaurant, and in cases where such a restaurant did not exist they were required to seek an alternative, for example find a Chinese restaurant instead of a Vietnamese one.", "labels": [], "entities": []}, {"text": "After each dialogue the users filled in a feedback form indicating whether they judged the dialogue to be successful or not.", "labels": [], "entities": []}, {"text": "Based on that binary rating, the subjective success was calculated as well as the average reward.", "labels": [], "entities": []}, {"text": "An objective rating can also be obtained by comparing the system outputs with the predefined task.", "labels": [], "entities": []}, {"text": "During policy adaptation, at the end of each call, users were asked to press 1 if they were satisfied (i.e. believed that they had been successful in fulfilling the assigned task) and 0 otherwise.", "labels": [], "entities": [{"text": "policy adaptation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.8384448885917664}]}, {"text": "The objective success was also calculated.", "labels": [], "entities": []}, {"text": "The dialogue was then only used for adaptation if the user rating agreed with the objective measure of success as in).", "labels": [], "entities": [{"text": "adaptation", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.9742979407310486}]}, {"text": "The performance based on user ratings during adaptation for both adaptation strategies is given in.", "labels": [], "entities": []}, {"text": "We then evaluated four policies with real users: the policy trained on the basic domain, the policy trained on the extended domain and the policy adapted to the extended domain using the prior and the policy adapted to the extended domain via interaction with real users using retraining.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "The results show two important features of these adaptation strategies.", "labels": [], "entities": []}, {"text": "The first is that it is possible to adapt the policy from one domain to another with a small number of dialogues.", "labels": [], "entities": []}, {"text": "Both adaptation techniques achieve results statistically indistinguishable from the matched case where the policy was trained directly in the extended domain.", "labels": [], "entities": []}, {"text": "The second important feature is that both adaptation strategies guarantee a minimum level of performance during training, which is better than the performance of the basic policy tested on the extended domain.", "labels": [], "entities": []}, {"text": "This is particularly important when training with real users so that they are not exposed to poor performance at anytime during training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Policy performance during adaptation", "labels": [], "entities": []}]}