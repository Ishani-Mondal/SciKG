{"title": [{"text": "Recognition of Named Entities Boundaries in Polish Texts", "labels": [], "entities": [{"text": "Recognition of Named Entities Boundaries in Polish Texts", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7688575014472008}]}], "abstractContent": [{"text": "In the paper we discuss the problem of low recall for the named entity (NE) recognition task for Polish.", "labels": [], "entities": [{"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9963279366493225}, {"text": "named entity (NE) recognition task", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.6597871099199567}]}, {"text": "We discuss to what extent the recall of NE recognition can be improved by reducing the space of NE categories.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9894261360168457}, {"text": "NE recognition", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.8593903481960297}]}, {"text": "We also present several extensions to the binary model which give an improvement of the recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.99824059009552}]}, {"text": "The extensions include: new features, application of external knowledge and post-processing.", "labels": [], "entities": []}, {"text": "For the partial evaluation the final model obtained 90.02% recall with 91.30% precision on the corpus of economic news.", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9996724128723145}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9993946552276611}]}], "introductionContent": [{"text": "Named entity recognition (NER) aims at identifying text fragments which refer to some objects and assigning a category of that object from a predefined set (for example: person, location, organization, artifact, other).", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.789053812623024}]}, {"text": "According to the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities there are several types of named entities, including: proper names, definite descriptions and noun phrases.", "labels": [], "entities": [{"text": "ACE (Automatic Content Extraction) English Annotation Guidelines", "start_pos": 17, "end_pos": 81, "type": "DATASET", "confidence": 0.5341502130031586}]}, {"text": "In this paper we focus on recognition of proper names (PNs) in Polish texts.", "labels": [], "entities": [{"text": "recognition of proper names (PNs) in Polish texts", "start_pos": 26, "end_pos": 75, "type": "TASK", "confidence": 0.8979740560054779}]}, {"text": "For Polish there are only a few accessible models for PN recognition.", "labels": [], "entities": [{"text": "PN recognition", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.9737161695957184}]}, {"text": "Marci\u00b4nczukMarci\u00b4nczuk and Janicki (2012) presented a hybrid model (a statistical model combined with some heuristics) which obtained 70.53% recall with 91.44% precision fora limited set of PN categories (first names, last names, names of countries, cities and roads) tested on the CEN corpus 1.", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9996254444122314}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9970428347587585}, {"text": "CEN corpus 1", "start_pos": 282, "end_pos": 294, "type": "DATASET", "confidence": 0.9842442472775778}]}, {"text": "A model for an extended set of PN categories (56 categories) presented by Marci\u00b4nczuk obtained much lower recall of 54% with 93% precision tested on the same corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9996216297149658}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9975473284721375}]}, {"text": "Savary Home page: http://nlp.pwr.wroc.pl/cen. and Waszczuk (2012) presented a statistical model which obtained 76% recall with 83% precision for names of people, places, organizations, time expressions and name derivations tested on the National Corpus of Polish 2 (.", "labels": [], "entities": [{"text": "Savary Home", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.930465430021286}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9980091452598572}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9991613626480103}, {"text": "National Corpus of Polish 2", "start_pos": 237, "end_pos": 264, "type": "DATASET", "confidence": 0.9788071751594544}]}, {"text": "There are also several other works on PN recognition for Polish where a rule-based approach was used.", "labels": [], "entities": [{"text": "PN recognition", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.9842053651809692}]}, {"text": "constructed a set of rules and tested them on 100 news from the Rzeczpospolita newspaper.", "labels": [], "entities": [{"text": "100 news from the Rzeczpospolita newspaper", "start_pos": 46, "end_pos": 88, "type": "DATASET", "confidence": 0.6292964220046997}]}, {"text": "The rules obtained 90.6% precision and 85.3% recall for person names and 87.9% precision and 56.6% recall for company names.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9995245933532715}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9989799857139587}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9994901418685913}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.998647153377533}]}, {"text": "Urba\u00b4nskaUrba\u00b4nska and Mykowiecka (2005) also constructed a set of rules for recognition of person and organization names.", "labels": [], "entities": [{"text": "recognition of person and organization names", "start_pos": 77, "end_pos": 121, "type": "TASK", "confidence": 0.7670354644457499}]}, {"text": "The rules were tested on 100 short texts from the Internet.", "labels": [], "entities": []}, {"text": "The rules obtained 98% precision and 89% recall for person names and 85% precision and 73% recall for organization names.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9994082450866699}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9985912442207336}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9996121525764465}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9983028173446655}]}, {"text": "Another rule-based approach for an extended set of proper names was presented by).", "labels": [], "entities": []}, {"text": "The rules were tested on 156 news from the Rzeczpospolita newspaper, the Tygodnik Powszechny newspaper and the news web portals.", "labels": [], "entities": []}, {"text": "The rules obtained 91% precision and 93% recall for country names, 55% precision and 73% recall for city names, 87% precision and 70% recall for road names and 82% precision and 66% recall for person names.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.999552309513092}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9988221526145935}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9992419481277466}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.98580002784729}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9992755055427551}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9950723052024841}, {"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.999221682548523}, {"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9973770380020142}]}, {"text": "The accessible models for PN recognition for Polish obtain relatively good performance in terms of precision.", "labels": [], "entities": [{"text": "PN recognition", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.978695273399353}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9983065128326416}]}, {"text": "However, in some NLP tasks like recognition of semantic relations between PNs, coreference resolution), machine translation) or sensitive data anonymization) the recall is much more important than the fine-grained categorization of PNs.", "labels": [], "entities": [{"text": "recognition of semantic relations between PNs", "start_pos": 32, "end_pos": 77, "type": "TASK", "confidence": 0.8609623412291209}, {"text": "coreference resolution", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.9225712716579437}, {"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.6875113248825073}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9989677667617798}]}, {"text": "Unfortunately, the only model recognising wide range of PN categories obtains only 54% recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.999403715133667}]}, {"text": "Therefore, our goal is to evaluate to what extent the recall for this model can be improved.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9992961883544922}]}], "datasetContent": [{"text": "In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr 3 (Broda et al., 2012b) and CEN (already mentioned in Section 1).", "labels": [], "entities": [{"text": "CEN", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.5283094048500061}]}, {"text": "The KPWr corpus consists of 747 documents containing near 200K tokens and 16.5K NEs.", "labels": [], "entities": [{"text": "KPWr corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8902760744094849}]}, {"text": "The CEN corpus consists of 797 documents containing 148K tokens and 13.6K NEs.", "labels": [], "entities": [{"text": "CEN corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9799931943416595}]}, {"text": "Both corpora were tagged using the morphological tagger WCRFT.", "labels": [], "entities": [{"text": "WCRFT", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9160600900650024}]}, {"text": "We used a 10-fold cross validation on the KPWr corpus to select the optimal model.", "labels": [], "entities": [{"text": "KPWr corpus", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9621877372264862}]}, {"text": "The CEN corpus was used fora cross-corpus evaluation of the selected model.", "labels": [], "entities": [{"text": "CEN corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9643228352069855}]}, {"text": "In this case the model was trained on the KPWr corpus and evaluated on the CEN corpus.", "labels": [], "entities": [{"text": "KPWr corpus", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9402761459350586}, {"text": "CEN corpus", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9873375296592712}]}, {"text": "We presented results for strict and partial matching evaluation.", "labels": [], "entities": []}, {"text": "The experiments were conducted using an open-source framework for named entity recognition called Liner2 4).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.6432768702507019}]}, {"text": "PR F 56nam model: The cross-domain evaluation of the basic and improved One-NAM models on CEN.", "labels": [], "entities": [{"text": "PR F 56nam", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7801016966501871}, {"text": "CEN", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.9694938063621521}]}], "tableCaptions": [{"text": " Table 1: Strict evaluation of the three NE models", "labels": [], "entities": []}, {"text": " Table 3: The 10-fold cross validation on the KPWr  corpus for One-NAM model with different exten- sions.", "labels": [], "entities": [{"text": "KPWr  corpus", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9351723492145538}]}, {"text": " Table 4. For the  strict evaluation, the recall was improved by al- most 4 percentage points with a small precision  improvement by almost 2 percentage points.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9996820688247681}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9994283318519592}]}, {"text": " Table 4: The cross-domain evaluation of the basic  and improved One-NAM models on CEN.", "labels": [], "entities": [{"text": "CEN", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.9783939719200134}]}]}