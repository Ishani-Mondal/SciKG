{"title": [{"text": "Evaluation of SPARQL query generation from natural language questions", "labels": [], "entities": [{"text": "SPARQL query generation", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.8300854961077372}]}], "abstractContent": [{"text": "SPARQL queries have become the standard for querying linked open data knowledge bases, but SPARQL query construction can be challenging and time-consuming even for experts.", "labels": [], "entities": [{"text": "SPARQL query construction", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.8169337113698324}]}, {"text": "SPARQL query generation from natural language questions is an attractive modality for interfacing with LOD.", "labels": [], "entities": [{"text": "SPARQL query generation from natural language questions", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8755585295813424}]}, {"text": "However, how to evaluate SPARQL query generation from natural language questions is a mostly open research question.", "labels": [], "entities": [{"text": "SPARQL query generation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8290724555651346}]}, {"text": "This paper presents some issues that arise in SPARQL query generation from natural language, a test suite for evaluating performance with respect to these issues, and a case study in evaluating a system for SPARQL query generation from natural language questions .", "labels": [], "entities": [{"text": "SPARQL query generation from natural language", "start_pos": 46, "end_pos": 91, "type": "TASK", "confidence": 0.8782013456026713}, {"text": "SPARQL query generation from natural language questions", "start_pos": 207, "end_pos": 262, "type": "TASK", "confidence": 0.8762217249189105}]}], "introductionContent": [{"text": "The SPARQL query language is the standard for retrieving linked open data from triple stores.", "labels": [], "entities": []}, {"text": "SPARQL is powerful, flexible, and allows the use of RDF, with all of its advantages over traditional databases.", "labels": [], "entities": []}, {"text": "However, SPARQL query construction has been described as \"absurdly difficult\", and even experienced users may struggle with it.", "labels": [], "entities": [{"text": "SPARQL query construction", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.845903734366099}]}, {"text": "For this reason, various methods have been suggested for aiding in SPARQL query generation, including assisted query construction ( and, most germaine to this work, converting natural language questions into SPARQL queries.", "labels": [], "entities": [{"text": "SPARQL query generation", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.915183424949646}, {"text": "query construction", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7731434404850006}]}, {"text": "Although a body of work on SPARQL query generation from natural language questions has been growing, no consensus has yet developed about how to evaluate such systems.", "labels": [], "entities": [{"text": "SPARQL query generation from natural language questions", "start_pos": 27, "end_pos": 82, "type": "TASK", "confidence": 0.8765367695263454}]}, {"text": "(Abacha and Zweigenbaum, 2012) evaluated their system by manual inspection of the SPARQL queries that they generated.", "labels": [], "entities": []}, {"text": "No gold standard was preparedthe authors examined each query and determined whether or not it accurately represented the original natural language question.", "labels": [], "entities": []}, {"text": "() used two human judges to manually examine the output of their system at three pointsdisambiguation, SPARQL query construction, and the answers returned.", "labels": [], "entities": [{"text": "SPARQL query construction", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.6255898177623749}]}, {"text": "If the judges disagreed, a third judge examined the output.", "labels": [], "entities": []}, {"text": "() does not have a formal evaluation, but rather gives two examples of the output of the SPARQL Assist system.", "labels": [], "entities": []}, {"text": "(This is not a system for query generation from natural language questions per se, but rather an application for assisting in query constructions through methods like autocompletion suggestions.)", "labels": [], "entities": [{"text": "query generation from natural language questions", "start_pos": 26, "end_pos": 74, "type": "TASK", "confidence": 0.8803386688232422}]}, {"text": "() is evaluated on the basis of a gold standard of answers from a static data set.", "labels": [], "entities": []}, {"text": "It is not clear how () is evaluated, although they give a nice classification of error types.", "labels": [], "entities": []}, {"text": "Reviewing this body of work, the trends that have characterized most past work are that either systems are not formally evaluated, or they are evaluated in a functional, black-box fashion, examining the mapping between inputs and one of two types of outputs-either the SPARQL queries themselves, or the answers returned by the SPARQL queries.", "labels": [], "entities": []}, {"text": "The significance of the work reported here is that it attempts to develop a unified methodology for evaluating systems for SPARQL query generation from natural language questions that meets a variety of desiderata for such a methodology and that is generalizable to other systems besides our own.", "labels": [], "entities": [{"text": "SPARQL query generation from natural language questions", "start_pos": 123, "end_pos": 178, "type": "TASK", "confidence": 0.8639963184084211}]}, {"text": "In the development of our system for SPARQL query generation from natural language questions, it became clear that we needed a robust approach to system evaluation.", "labels": [], "entities": [{"text": "SPARQL query generation from natural language questions", "start_pos": 37, "end_pos": 92, "type": "TASK", "confidence": 0.8956268685204642}]}, {"text": "The approach needed to meet a number of desiderata: \u2022 Automatability: It should be possible to automate tests so that they can be run automatically many times during the day and so that there is no opportunity for humans to miss errors when doing manual examination.", "labels": [], "entities": []}, {"text": "\u2022 Granularity: The approach should allow for granular evaluation of behavior-that is, rather than (or in addition to) just returning a single metric that characterizes performance over an entire data set, such as accuracy, it should allow for evaluation of functionality over specific types of inputs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9970531463623047}]}, {"text": "\u2022 Modularity: The approach should allow for evaluating individual modules of the system independently.", "labels": [], "entities": []}, {"text": "\u2022 Functionality: The approach should allow functional, black-box evaluation of the endto-end performance of the system as a whole.", "labels": [], "entities": []}, {"text": "The hypothesis being explored in the work reported here is that it is possible to conduct a principled fine-grained evaluation of software for SPARQL query generation from natural language questions that is effective in uncovering weaknesses in the software.", "labels": [], "entities": [{"text": "SPARQL query generation from natural language questions", "start_pos": 143, "end_pos": 198, "type": "TASK", "confidence": 0.8755249551364354}]}, {"text": "As in any software testing situation, various methods of evaluating the software exist.", "labels": [], "entities": []}, {"text": "A typical black-box approach would be to establish a gold standard of the SPARQL queries themselves, and/or of the answers that should be returned in response to a natural language question..", "labels": [], "entities": []}, {"text": "However, we ruled out applying the black-box approach to the SPARQL queries themselves because there are multiple correct SPARQL queries that are equivalent in terms of the triples that they will return from a linked open data source.", "labels": [], "entities": []}, {"text": "We ruled out a black-box approach based entirely on examining the triples returned from the query when the SPARQL query was executed against the triple store because the specific list of triples is subject to change unpredictably as the contents of the triple store are updated by the data maintainers.", "labels": [], "entities": []}, {"text": "We opted fora gray-box approach, in which we examine the output at multiple stages of processing.", "labels": [], "entities": []}, {"text": "The first was at the point of mapping to TUIs.", "labels": [], "entities": []}, {"text": "The Unified Medical Language System's Semantic Network contains a hierarchically grouped set of 133 semantic types, each with a Type Unique Identifier (TUI).", "labels": [], "entities": []}, {"text": "That is, for any given natural language question that should cause a mapping to a TUI, we examined if a TUI was generated by the system and, if so, if it was the correct TUI.", "labels": [], "entities": []}, {"text": "The second was the point of SPARQL query generation, where we focused on syntactic validity, rather than the entire SPARQL query (for the reason given above).", "labels": [], "entities": [{"text": "SPARQL query generation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8621820012728373}]}, {"text": "We also examined the output of the SPARQL query, but not in terms of exact match to a gold standard.", "labels": [], "entities": []}, {"text": "In practice, the queries would typically return along list of triples, and the specific list of triples is subject to change unpredictably as the contents of the triple store are updated by the OMIM maintainers.", "labels": [], "entities": []}, {"text": "For that reason, we have focused on ensuring that we know one correct triple which should occur in the output, and validating the presence of that triple in the output.", "labels": [], "entities": []}, {"text": "We have also inspected the output for triples that we knew from domain expertise should not be returned, although we have done that manually so far and have not formalized it in the test suite.", "labels": [], "entities": []}, {"text": "In this paper, we focus on one specific aspect of the gray-box evaluation: the mapping to TUIs.", "labels": [], "entities": []}, {"text": "As will be seen, mapping to TUIs when appropriate, and of course to the correct TUI, is an important feature of answering domain-specific questions.", "labels": [], "entities": []}, {"text": "As we developed our system beyond the initial prototype, it quickly became apparent that there was a necessity to differentiate between elements of the question that referred to specific entities in the triple store, and elements of the question that referred to general semantic categories.", "labels": [], "entities": []}, {"text": "For example, for queries like What genes are related to heart disease?, we noticed that heart disease was being mapped to the correct entity in the triple store, but genes, rather than being treated as a general category, was also being mapped (erroneously) to a particular instance in the triple store.", "labels": [], "entities": []}, {"text": "Given the predicates in the triple store, the best solution was to recognize general categories in questions and map them to TUIs.", "labels": [], "entities": []}, {"text": "Therefore, we developed a method to recognize general categories in questions and map them to TUIs.", "labels": [], "entities": []}, {"text": "Testing this functionality is the main topic of this paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}