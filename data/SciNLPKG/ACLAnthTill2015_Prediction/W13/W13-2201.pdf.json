{"title": [{"text": "Findings of the 2013 Workshop on Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7573472857475281}]}], "abstractContent": [{"text": "We present the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task.", "labels": [], "entities": [{"text": "WMT13 shared tasks", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.5512692928314209}, {"text": "translation task", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.90338134765625}, {"text": "run-time estimation of machine translation", "start_pos": 96, "end_pos": 138, "type": "TASK", "confidence": 0.6211653470993042}]}, {"text": "This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7305261492729187}, {"text": "translation tasks", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.8068762719631195}]}, {"text": "An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date.", "labels": [], "entities": []}, {"text": "The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries.", "labels": [], "entities": [{"text": "quality estimation task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7181267937024435}]}], "introductionContent": [{"text": "We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT) held at ACL 2013", "start_pos": 62, "end_pos": 116, "type": "TASK", "confidence": 0.7866284489631653}]}, {"text": "This workshop builds on seven previous WMT workshops (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.6497732400894165}]}, {"text": "This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.9257039129734039}]}, {"text": "1 In the translation task ( \u00a72), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data.", "labels": [], "entities": [{"text": "translation task", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.9308812916278839}]}, {"text": "We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8927954435348511}]}, {"text": "The Russian translation tasks were new this year, and were also the most popular.", "labels": [], "entities": [{"text": "Russian translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.5467128157615662}]}, {"text": "The system outputs for each task were evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "The human evaluation task ( \u00a73) involves asking human judges to rank sentences output by anonymized systems.", "labels": [], "entities": []}, {"text": "We obtained large numbers of rankings from two groups: researchers (who contributed evaluations proportional to the number of tasks they entered) and workers on Amazon's Mechanical Turk (who were paid).", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 161, "end_pos": 185, "type": "DATASET", "confidence": 0.9556789249181747}]}, {"text": "This year's effort was our largest yet by a wide margin; we managed to collect an order of magnitude more judgments than in the past, allowing us to achieve statistical significance on the majority of the pairwise system rankings.", "labels": [], "entities": []}, {"text": "This year, we are also clustering the systems according to these significance results, instead of presenting a total ordering over systems.", "labels": [], "entities": []}, {"text": "The focus of the quality estimation task ( \u00a76) is to produce real-time estimates of sentence-or word-level machine translation quality.", "labels": [], "entities": [{"text": "sentence-or word-level machine translation", "start_pos": 84, "end_pos": 126, "type": "TASK", "confidence": 0.508677750825882}]}, {"text": "This task has potential usefulness in a range of settings, such as prioritizing output for human post-editing, or selecting the best translations from a number of systems.", "labels": [], "entities": []}, {"text": "This year the following subtasks were proposed: prediction of percentage of word edits necessary to fix a sentence, ranking of up to five alternative translations fora given source sentence, prediction of post-editing time fora sentence, and prediction of word-level scores fora given translation (correct/incorrect and types of edits).", "labels": [], "entities": []}, {"text": "The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems.", "labels": [], "entities": []}, {"text": "This marks the second year we have conducted this task.", "labels": [], "entities": []}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8729994297027588}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8300673961639404}, {"text": "machine translation", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.8182332217693329}]}, {"text": "As before, all of the data, translations, and collected human judgments are publicly available.", "labels": [], "entities": []}, {"text": "We hope these datasets serve as a valuable resource for research into statistical machine translation, system combination, and automatic evaluation or prediction of translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6754633486270905}, {"text": "system combination", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.6937398761510849}, {"text": "automatic evaluation or prediction of translation", "start_pos": 127, "end_pos": 176, "type": "TASK", "confidence": 0.5698008785645167}]}, {"text": "parallel data provided by Yandex, Russian-English Wikipedia Headlines provided by CMU).", "labels": [], "entities": [{"text": "Yandex", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.9563813209533691}, {"text": "CMU", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9215813875198364}]}, {"text": "Some statistics about the training materials are given in.", "labels": [], "entities": []}], "datasetContent": [{"text": "As with past workshops, we contend that automatic measures of machine translation quality are an imperfect substitute for human assessments.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7519227862358093}]}, {"text": "We therefore conduct a manual evaluation of the system outputs and define its results to be the principal ranking of the workshop.", "labels": [], "entities": []}, {"text": "In this section, we describe how we collected this data and compute the results, and then present the official results of the ranking.", "labels": [], "entities": []}, {"text": "We run the evaluation campaign using an updated version of Appraise; the tool has been extended to support collecting judgments using Amazon's Mechanical Turk, replacing the annotation system used in previous WMTs.", "labels": [], "entities": []}, {"text": "The software, including all changes made for this year's workshop, is available from GitHub.", "labels": [], "entities": [{"text": "GitHub", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9450733661651611}]}, {"text": "This year differs from prior years in a few important ways: \u2022 We collected about ten times more judgments that we have in the past, using judgments from both participants in the shared task and non-experts hired on Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 215, "end_pos": 239, "type": "DATASET", "confidence": 0.9114420861005783}]}, {"text": "\u2022 Instead of presenting a total ordering of systems for each pair, we cluster them and report a ranking over the clusters.", "labels": [], "entities": []}, {"text": "Task 1.1 Predicting post-editing distance For the training of models, we provided the WMT12 quality estimation dataset: 2,254 EnglishSpanish news sentences extracted from previous WMT translation task English-Spanish test sets (WMT09, WMT10, and WMT12).", "labels": [], "entities": [{"text": "WMT12 quality estimation dataset", "start_pos": 86, "end_pos": 118, "type": "DATASET", "confidence": 0.6734710186719894}, {"text": "WMT translation task English-Spanish test sets", "start_pos": 180, "end_pos": 226, "type": "DATASET", "confidence": 0.7125861247380575}, {"text": "WMT09", "start_pos": 228, "end_pos": 233, "type": "DATASET", "confidence": 0.8460714221000671}, {"text": "WMT10", "start_pos": 235, "end_pos": 240, "type": "DATASET", "confidence": 0.7021276354789734}, {"text": "WMT12", "start_pos": 246, "end_pos": 251, "type": "DATASET", "confidence": 0.9555886387825012}]}, {"text": "These were translated by a phrase-based SMT Moses system trained on Europarl and News Commentaries corpora as provided by WMT, along with their source sentences, reference translations, post-edited translations, and HTER scores.", "labels": [], "entities": [{"text": "Europarl and News Commentaries corpora", "start_pos": 68, "end_pos": 106, "type": "DATASET", "confidence": 0.805815827846527}, {"text": "WMT", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.956759512424469}, {"text": "HTER", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.7474008798599243}]}, {"text": "We used TERp (default settings: tokenised, case insensitive, etc., but capped to 1) to compute the HTER scores.", "labels": [], "entities": [{"text": "TERp", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.995005190372467}, {"text": "HTER", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.4101801812648773}]}, {"text": "Likert scores in were also provided, as participants may choose to use them for the ranking variant.", "labels": [], "entities": []}, {"text": "As test data, we use a subset of the WMT13 English-Spanish news test set with 500 sentences, whose translations were produced by the same SMT system used for the training set.", "labels": [], "entities": [{"text": "WMT13 English-Spanish news test set", "start_pos": 37, "end_pos": 72, "type": "DATASET", "confidence": 0.9555387377738953}]}, {"text": "To compute the true HTER labels, the translations were post-edited under the same conditions as those on the training set.", "labels": [], "entities": []}, {"text": "As in any blind shared task, the HTER scores were solely used to evaluate the submissions, and were only released to participants after they submitted their systems.", "labels": [], "entities": [{"text": "HTER", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.764464259147644}]}, {"text": "A few variations of the training and test data were provided, including aversion with cases restored and aversion detokenized.", "labels": [], "entities": [{"text": "aversion", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9605486989021301}]}, {"text": "In addition, we provided a number of engine-internal information from Moses for glass-box feature extraction, such as phrase and word alignments, model scores, word graph, n-best lists and information from the decoder's search graph.", "labels": [], "entities": [{"text": "glass-box feature extraction", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.7389299670855204}, {"text": "phrase and word alignments", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.5667902752757072}]}, {"text": "Task 1.2 Selecting best translation As training data, we provided a large set of up to five alternative machine translations produced by different MT systems for each source sentence and ranked for quality by humans.", "labels": [], "entities": []}, {"text": "This was the outcome of the manual evaluation of the translation task from WMT09-WMT12.", "labels": [], "entities": [{"text": "translation task", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.8934359550476074}, {"text": "WMT09-WMT12", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9754565954208374}]}, {"text": "It includes two language pairs: German-English and English-Spanish, with 7,098 and 4,592 source sentences and up to five ranked translations, totalling 32,922 and 22,447 translations, respectively.", "labels": [], "entities": []}, {"text": "As test data, a set of up to five alternative machine translations per source sentence from the WMT08 test sets was provided, with 365 (1,810) and 264 (1,315) source sentences (translations) for German-English and English-Spanish, respectively.", "labels": [], "entities": [{"text": "WMT08 test sets", "start_pos": 96, "end_pos": 111, "type": "DATASET", "confidence": 0.9678649504979452}]}, {"text": "We note that there was some overlap between the MT systems used in the training data http://www.umiacs.umd.edu/ \u02dc snover/terp/ and test datasets, but not all systems were the same, as different systems participate in WMT over the years.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.924414336681366}, {"text": "WMT", "start_pos": 217, "end_pos": 220, "type": "TASK", "confidence": 0.854977548122406}]}, {"text": "news articles which were translated into Spanish using Moses and post-edited during a CAS-MACAT 11 field trial.", "labels": [], "entities": [{"text": "CAS-MACAT 11 field trial", "start_pos": 86, "end_pos": 110, "type": "DATASET", "confidence": 0.6574893817305565}]}, {"text": "Of these, 15 documents have been processed repeatedly by at least 2 out of 5 translators, resulting in a total of 1,087 segments.", "labels": [], "entities": []}, {"text": "For each segment we provided: \u2022 English source and Spanish translation.", "labels": [], "entities": []}, {"text": "\u2022 Spanish MT output which was used as basis for post-editing.", "labels": [], "entities": [{"text": "Spanish MT output", "start_pos": 2, "end_pos": 19, "type": "DATASET", "confidence": 0.6499941349029541}]}, {"text": "\u2022 Document and translator ID.", "labels": [], "entities": []}, {"text": "\u2022 Position of the segment within the document.", "labels": [], "entities": []}, {"text": "The metadata about translator and document was made available as we expect that translator performance and normalisation over document complexity can be helpful when predicting the time spend on a given segment.", "labels": [], "entities": []}, {"text": "For the training portion of the data we also provided: \u2022 Time to post-edit in seconds (Task 1.3).", "labels": [], "entities": [{"text": "Time", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9798349738121033}]}, {"text": "\u2022 Binary (Keep, Change) and multiclass (Keep, Substitute, Delete) labels on word level along with explicit tokenization (Task 2).", "labels": [], "entities": []}, {"text": "The labels in Task 2 are derived by computing WER between the original machine translation and its post-edited version.", "labels": [], "entities": [{"text": "WER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.993818461894989}]}, {"text": "Task 1.1 Predicting post-editing distance Evaluation is performed against the HTER and/or ranking of translations using the same metrics as in WMT12.", "labels": [], "entities": [{"text": "HTER", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.5151148438453674}, {"text": "WMT12", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.948738157749176}]}, {"text": "For the scoring variant of the task, we use two standard metrics for regression tasks: Both these metrics are non-parametric, automatic and deterministic (and therefore consistent), and extrinsically interpretable.", "labels": [], "entities": []}, {"text": "For instance, a MAE value of 10 means that, on average, the absolute difference between the hypothesized score and the reference score value is 10 percentage points (i.e., 0.10 difference in HTER scores).", "labels": [], "entities": [{"text": "MAE", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9953383207321167}]}, {"text": "The interpretation of RMSE is similar, with the difference that RMSE penalises larger errors more (via the square function).", "labels": [], "entities": []}, {"text": "For the ranking variant of the task, we use the DeltaAvg metric proposed in the 2012 edition of the task) as our main metric.", "labels": [], "entities": []}, {"text": "This metric assumes that each reference test instance has an extrinsic number associated with it that represents its ranking with respect to the other test instances.", "labels": [], "entities": []}, {"text": "For completeness, we present here again the definition of DeltaAvg.", "labels": [], "entities": []}, {"text": "The goal of the DeltaAvg metric is to measure how valuable a proposed ranking (which we calla hypothesis ranking) is, according to the true ranking values associated with the test instances.", "labels": [], "entities": []}, {"text": "We first define a parametrised version of this metric, called DeltaAvg[n].", "labels": [], "entities": []}, {"text": "The following notations are used: fora given entry sentence s, V (s) represents the function that associates an extrinsic value to that entry; we extend this notation to a set S, with V (S) representing the average of all V (s), s \u2208 S.", "labels": [], "entities": []}, {"text": "Intuitively, V (S) is a quantitative measure of the \"quality\" of the set S, as induced by the extrinsic values associated with the entries in S.", "labels": [], "entities": []}, {"text": "For a set of ranked entries Sand a parameter n, we denote by S 1 the first quantile of set S (the highest-ranked entries), S 2 the second quantile, and soon, for n quantiles of equal sizes.", "labels": [], "entities": []}, {"text": "We also use the notation S i,j = j k=i S k . Using these notations, we define: When the valuation function V is clear from the context, we write DeltaAvg[n] for DeltaAvg V [n].", "labels": [], "entities": []}, {"text": "The parameter n represents the number of quantiles we want to split the set S into.", "labels": [], "entities": []}, {"text": "For instance, n = 2 gives DeltaAvg[2] = V (S 1 )\u2212V (S), hence it measures the difference between the quality of the top quantile (top half) S 1 and the overall quality (represented by V (S)).", "labels": [], "entities": []}, {"text": "For n = 3, )/2, hence it measures an average difference across two cases: between the quality of the top quantile (top third) and the overall quality, and between the quality of the top two quantiles (S 1 \u222a S 2 , top two-thirds) and the overall quality.", "labels": [], "entities": []}, {"text": "In general, DeltaAvg[n] measures an average difference in quality across n \u2212 1 cases, with each case measuring the impact in quality of adding an additional quantile, from top to bottom.", "labels": [], "entities": []}, {"text": "Finally, we define: where N = |S|/2.", "labels": [], "entities": []}, {"text": "As before, we write DeltaAvg for DeltaAvg V when the valuation function V is clear from the context.", "labels": [], "entities": []}, {"text": "The DeltaAvg metric is an average across all DeltaAvg[n] values, for those n values for which the resulting quantiles have at least 2 entries (no singleton quantiles).", "labels": [], "entities": []}, {"text": "We present results for DeltaAvg using as valuation function V the HTER scores, as defined in Section 6.3.", "labels": [], "entities": [{"text": "HTER scores", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.5869268923997879}]}, {"text": "We also use Spearman's rank correlation coefficient \u03c1 as a secondary metric.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient \u03c1", "start_pos": 12, "end_pos": 53, "type": "METRIC", "confidence": 0.6338324646155039}]}], "tableCaptions": [{"text": " Table 2: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the  previous two workshops.", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9609232544898987}, {"text": "WMT13 manual evaluation", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.773111899693807}]}, {"text": " Table 3: \u03ba scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re- searcher annotations and MTurk annotations, respectively. See", "labels": [], "entities": [{"text": "WMT13r", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9515753984451294}, {"text": "WMT13m", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.8020373582839966}]}, {"text": " Table 4: \u03ba scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the  human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota- tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that  language pair.", "labels": [], "entities": [{"text": "WMT13r", "start_pos": 150, "end_pos": 156, "type": "DATASET", "confidence": 0.9238173365592957}, {"text": "WMT13m", "start_pos": 161, "end_pos": 167, "type": "DATASET", "confidence": 0.8308584094047546}]}, {"text": " Table 5: Agreement as a function of threshold for Turkers on  the Russian-English task. The threshold is the percentage of  controls a Turker must pass for her rankings to be accepted.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9588638544082642}]}, {"text": " Table 6: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between  systems indicate clusters according to bootstrap resampling at p-level p \u2264 .05. This method is also used to determine the  range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints  provided for the shared task.", "labels": [], "entities": [{"text": "WMT13 translation task", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8406533201535543}]}, {"text": " Table 7: Distribution of review statuses.", "labels": [], "entities": [{"text": "Distribution of review statuses", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8319651037454605}]}, {"text": " Table 8: Annotator agreement when reviewing monolingual  edits.", "labels": [], "entities": []}, {"text": " Table 9: Understandability of English\u2192Czech systems. The  \u00b1 values indicate empirical confidence bounds at 95%. Rank  ranges were also obtained in the same resampling: in 95% of  observations, the system was ranked in the given range.", "labels": [], "entities": []}, {"text": " Table 12: Official results for the ranking variant of the WMT13 Quality Estimation Task 1.1. The winning submissions are  indicated by a \u2022 (they are significantly better than all other submissions according to bootstrap resampling (10k times) with  95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant  level according to the same test. Oracle results that use human-references are also shown for comparison purposes.", "labels": [], "entities": [{"text": "WMT13 Quality Estimation Task", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.7009949758648872}]}, {"text": " Table 13: Official results for the scoring variant of the WMT13 Quality Estimation Task 1.1. The winning submission is  indicated by a \u2022 (it is significantly better than the other submissions according to bootstrap resampling (10k times) with 95%  confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant level  according to the same test. Oracle results that use human-references are also shown for comparison purposes.", "labels": [], "entities": [{"text": "WMT13 Quality Estimation Task", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.7595577985048294}]}, {"text": " Table 14: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric  Kendall's \u03c4 with ties penalised. The winning submissions are indicated by a \u2022. Oracle results that use human-references are  also shown for comparison purposes.", "labels": [], "entities": [{"text": "WMT13 Quality Estimation shared task", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.6415205836296082}]}, {"text": " Table 15: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric  Kendall's \u03c4 with ties penalised. The winning submissions are indicated by a \u2022. Oracle results that use human-references are  also shown for comparison purposes.", "labels": [], "entities": [{"text": "WMT13 Quality Estimation shared task", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.6247955799102783}]}, {"text": " Table 16: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric  Kendall's \u03c4 with ties ignored. The winning submissions are indicated by a \u2022. Oracle results that use human-references are also  shown for comparison purposes.", "labels": [], "entities": [{"text": "WMT13 Quality Estimation shared task", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.6596818447113038}]}, {"text": " Table 17: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric  Kendall's \u03c4 with ties ignored. The winning submissions are indicated by a \u2022. Oracle results that use human-references are also  shown for comparison purposes.", "labels": [], "entities": [{"text": "WMT13 Quality Estimation shared task", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.6386379361152649}]}, {"text": " Table 18: Official results for the Task 1.3 of the WMT13 Quality Estimation shared-task. The winning submissions are  indicated by a \u2022 (they are significantly better than all other submissions according to bootstrap resampling (10k times) with  95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant  level according to the same test.", "labels": [], "entities": [{"text": "WMT13 Quality Estimation shared-task", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.588402733206749}]}, {"text": " Table 19: Official results for Task 2: binary classification on word level of the WMT13 Quality Estimation shared-task. The  winning submissions are indicated by a \u2022.", "labels": [], "entities": [{"text": "WMT13 Quality Estimation shared-task", "start_pos": 83, "end_pos": 119, "type": "DATASET", "confidence": 0.7686225920915604}]}, {"text": " Table 20: Official results for Task 2: multiclass classification on word level of the WMT13 Quality Estimation shared-task.  The winning submissions are indicated by a \u2022.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7682383954524994}, {"text": "WMT13 Quality Estimation shared-task", "start_pos": 87, "end_pos": 123, "type": "DATASET", "confidence": 0.7993967533111572}]}, {"text": " Table 21: Head to head comparison, ignoring ties, for Czech-English systems", "labels": [], "entities": []}, {"text": " Table 22: Head to head comparison, ignoring ties, for English-Czech systems", "labels": [], "entities": []}]}