{"title": [{"text": "Discriminating Non-Native English with 350 Words", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes MITRE's participation in the native language identification (NLI) task at BEA-8.", "labels": [], "entities": [{"text": "native language identification (NLI) task", "start_pos": 50, "end_pos": 91, "type": "TASK", "confidence": 0.8263111284800938}, {"text": "BEA-8", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.7016533017158508}]}, {"text": "Our best effort performed at an accuracy of 82.6% in the eleven-way NLI task, placing it in a statistical tie with the best performing systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9993962049484253}]}, {"text": "We describe the variety of machine learning approaches that we explored , including Winnow, language model-ing, logistic regression and maximum-entropy models.", "labels": [], "entities": []}, {"text": "Our primary features were word and character n-grams.", "labels": [], "entities": []}, {"text": "We also describe several ensemble methods that we employed for combining these base systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Investigations into the effect of authors' latent attributes on language use have along history in linguistics.", "labels": [], "entities": []}, {"text": "The rapid growth of social media has sparked increased interest in automatically identifying author attributes such as gender and age (;.", "labels": [], "entities": []}, {"text": "There is also along history of computational aids for language pedagogy, both for first-and second-language acquisition.", "labels": [], "entities": [{"text": "first-and second-language acquisition", "start_pos": 82, "end_pos": 119, "type": "TASK", "confidence": 0.661592702070872}]}, {"text": "In particular, automated native language identification (NLI) is a useful aid to second language learning.", "labels": [], "entities": [{"text": "automated native language identification (NLI)", "start_pos": 15, "end_pos": 61, "type": "TASK", "confidence": 0.7533494489533561}]}, {"text": "This is our first foray into NLI, although we have recently described experiments aimed at identifying the gender of unknown Twitter authors).", "labels": [], "entities": []}, {"text": "We performed well using only character and word n-grams as evidence.", "labels": [], "entities": []}, {"text": "In the present work, we apply that same approach to NLI, and combine it with several other baseline classifiers.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we describe our high-performing system for identifying the native language of English writers.", "labels": [], "entities": [{"text": "identifying the native language of English writers", "start_pos": 75, "end_pos": 125, "type": "TASK", "confidence": 0.8695441569600787}]}, {"text": "We explore a varied set of learning algorithms and present two ensemble methods used to produce a better system than any of the individuals.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the data and task in detail as well as the evaluation metric.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss details of the particular system configuration that scored best for us.", "labels": [], "entities": []}, {"text": "We describe our experiments in Section 4, including our exploration of several different classifier types and parametrizations.", "labels": [], "entities": []}, {"text": "In Section 5 we present and analyze performance results, and inspect some of the features that were useful in discrimination.", "labels": [], "entities": []}, {"text": "Finally in Section 6 we summarize our findings, and describe possible extensions to the work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Native Language Identification was a shared task organized as part of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, 2013.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7138014237085978}]}, {"text": "The task was to identify an author's native language based on an English essay.", "labels": [], "entities": []}, {"text": "The data provided consisted of a set of 12,100 Test of English as a Foreign Language (TOEFL) examinations contributed by the Educational Testing Service (Blanchard et al., to appear).", "labels": [], "entities": [{"text": "Educational Testing Service", "start_pos": 125, "end_pos": 152, "type": "DATASET", "confidence": 0.8640155593554179}]}, {"text": "These were English essays written by native speakers of Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish.", "labels": [], "entities": []}, {"text": "A set of 1000 essays for each language was identified as training data, along with 100 per language for development, and another 100 per language fora final test set.", "labels": [], "entities": []}, {"text": "The mean length of an essay is 348 words.", "labels": [], "entities": []}, {"text": "The primary evaluation metric for shared task submissions was simple accuracy: the fraction of the test essays for which the correct native language was identified.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9028651118278503}]}, {"text": "A baseline accuracy would thus be about 9% (one out of eleven).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9560002684593201}]}, {"text": "Results were also reported in terms of F-measure on a per-language basis.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9981947541236877}]}, {"text": "Fmeasure is a harmonic mean of precision and recall: F = 2P RP +R . For the evaluation, the precision denominator was the number of items labeled with a particular language by the system and the recall denominator was the number of items marked with a particular language in the reference set.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.951849639415741}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9990988969802856}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9982463121414185}, {"text": "F", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9794051647186279}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9964953064918518}, {"text": "recall denominator", "start_pos": 195, "end_pos": 213, "type": "METRIC", "confidence": 0.97384113073349}]}, {"text": "The training, development, and test sets all had balanced distributions across the native languages, so error rates and accuracy did not favor any particular language in any set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9991759657859802}]}, {"text": "In all experiments described below, systems were trained initially on the 9900 training examples alone, with the 1100 item development set held back to allow for hyperparameter estimation.", "labels": [], "entities": []}, {"text": "When preparing our final test set submissions, the development set was folded into the training data, and all models were re-trained on this new dataset containing 11000 items.", "labels": [], "entities": []}, {"text": "Our components generate scores, but those scores were not always scaled in the same way.", "labels": [], "entities": []}, {"text": "Winnow (in Carnie) is a margin-based, mistake-driven learner generating scores which are interpretable only as sums of weights.", "labels": [], "entities": [{"text": "Winnow (in Carnie)", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.7121180057525635}]}, {"text": "SRILM produces log p(d j |h i ), but renormalizing those (with priors) into estimates of p(h i |d j ) is unreliable because the different submodels are not connected with smoothing.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7745077013969421}]}, {"text": "Logistic regression produces a distribution for p(h i |d j ).", "labels": [], "entities": []}, {"text": "We aimed to express these notions of confidence in away that was common to all systems.", "labels": [], "entities": []}, {"text": "We did this by relabeling system hypotheses after sorting by confidence, but not all metrics were equally good at this sorting.", "labels": [], "entities": []}, {"text": "We performed an ad hoc assessment of several candidate scoring functions.", "labels": [], "entities": []}, {"text": "Our goal was to find functions that best separated correct answers from incorrect answers in a sorted ranking.", "labels": [], "entities": []}, {"text": "We ran several candidates on our development set and measured the difference between the mean rank of correct answers and the mean rank of incorrect answers.", "labels": [], "entities": []}, {"text": "In each case h 1 was the best hypothesis generated by the system and h 2 is second best.", "labels": [], "entities": []}, {"text": "p(\u00b7) indicates probabilities, s(\u00b7) indicates nonprobability scores.", "labels": [], "entities": []}, {"text": "We chose those functions with the highest values.", "labels": [], "entities": []}], "tableCaptions": []}