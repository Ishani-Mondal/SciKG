{"title": [{"text": "Improving interpretation robustness in a tutorial dialogue system", "labels": [], "entities": [{"text": "Improving interpretation robustness", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9126546581586202}]}], "abstractContent": [{"text": "We present an experiment aimed at improving interpretation robustness of a tutorial dialogue system that relies on detailed semantic interpretation and dynamic natural language feedback generation.", "labels": [], "entities": [{"text": "natural language feedback generation", "start_pos": 160, "end_pos": 196, "type": "TASK", "confidence": 0.7528528422117233}]}, {"text": "We show that we can improve overall interpretation quality by combining the output of a semantic interpreter with that of a statistical classifier trained on the subset of student utterances where semantic interpretation fails.", "labels": [], "entities": []}, {"text": "This improves on a previous result which used a similar approach but trained the classifier on a substantially larger data set containing all student utterances.", "labels": [], "entities": []}, {"text": "Finally , we discuss how the labels from the statistical classifier can be integrated effectively with the dialogue system's existing error recovery policies.", "labels": [], "entities": [{"text": "error recovery", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.6891409009695053}]}], "introductionContent": [{"text": "Giving students formative feedback as they interact with educational applications, such as simulated training environments, problem-solving tutors, serious games, and exploratory learning environments, is known to be important for effective learning.", "labels": [], "entities": []}, {"text": "Suitable feedback can include context-appropriate confirmations, hints, and suggestions to help students refine their answers and increase their understanding of the subject.", "labels": [], "entities": []}, {"text": "Providing this type of feedback automatically, in natural language, is the goal of tutorial dialogue systems (.", "labels": [], "entities": []}, {"text": "Much work in NLP for educational applications has focused on automated answer grading).", "labels": [], "entities": []}, {"text": "Automated answer assessment systems are commonly trained on large text corpora.", "labels": [], "entities": [{"text": "answer assessment", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8647513091564178}]}, {"text": "They compare the text of a student answer with the text of one or more reference answers supplied by human instructors and calculate a score reflecting the quality of the match.", "labels": [], "entities": []}, {"text": "Automated grading methods are integrated into intelligent tutoring systems (ITS) by having system developers anticipate both correct and incorrect responses to each question, with the system choosing the best match (.", "labels": [], "entities": []}, {"text": "Such systems have wide domain coverage and are robust to ill-formed input.", "labels": [], "entities": []}, {"text": "However, as matching relies on shallow features and does not provide semantic representations of student answers, this approach is less suitable for dynamically generating adaptive natural language feedback ().", "labels": [], "entities": []}, {"text": "Real-time simulations and serious games are commonly used in STEM learning environments to increase student engagement and support exploratory learning.", "labels": [], "entities": [{"text": "STEM learning", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9065695703029633}]}, {"text": "Natural language dialogue can help improve learning in such systems by asking students to explain their reasoning, either directly during interaction, or during post-problem reflection).", "labels": [], "entities": []}, {"text": "Interpretation of student answers in such systems needs to be grounded in the current state of a dynamically changing environment, and feedback may also be generated dynamically to reflect the changing system state.", "labels": [], "entities": [{"text": "Interpretation of student answers", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.801381528377533}]}, {"text": "This is typically achieved by employing hand-crafted parsers and semantic interpreters to produce structured semantic representations of student input, which are then used to instantiate ab-stract tutorial strategies with the help of a natural language generation system.", "labels": [], "entities": []}, {"text": "Rule-based semantic interpreters are known to suffer from robustness and coverage problems, failing to interpret out-of-grammar student utterances.", "labels": [], "entities": [{"text": "Rule-based semantic interpreters", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6105704108874003}, {"text": "coverage", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9572537541389465}]}, {"text": "In the event of an interpretation failure, most systems have little information on which to base a feedback decision and typically respond by asking the student to rephrase, or simply giveaway the answer (though more sophisticated strategies are sometimes possible, see Section 4).", "labels": [], "entities": []}, {"text": "While statistical scoring approaches are more robust, they may still suffer from coverage issues when system designers fail to anticipate the full range of expected student answers.", "labels": [], "entities": [{"text": "coverage", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9936271905899048}]}, {"text": "In one study of a statistical system, a human judge labeled 33% of student utterances as not matching any of the anticipated responses, meaning that the system had no information to use as a basis for choosing the next action and fell back on a single strategy, giving away the answer (.", "labels": [], "entities": []}, {"text": "Recently, developed an annotated corpus of student responses (henceforth, the SRA corpus) with the goal of facilitating dynamic generation of tutorial feedback.", "labels": [], "entities": [{"text": "SRA corpus", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.7751561403274536}]}, {"text": "1 Student responses are assigned to one of 5 domain-and taskindependent classes that correspond to typical flaws found in student answers.", "labels": [], "entities": []}, {"text": "These classes can be used to help a system choose a feedback strategy based only on the student answer and a single reference answer.", "labels": [], "entities": []}, {"text": "showed that a statistical classifier trained on this data set can be used in combination with a semantic interpreter to significantly improve the overall quality of natural language interpretation in a dialogue-based ITS.", "labels": [], "entities": [{"text": "natural language interpretation", "start_pos": 165, "end_pos": 196, "type": "TASK", "confidence": 0.7514872550964355}]}, {"text": "The best results were obtained by using the classifier to label the utterances that the semantic interpreter failed to process.", "labels": [], "entities": []}, {"text": "In this paper we further extend this result by showing that we can obtain similar results by training the classifier directly on the subset of utterances that cannot be processed by the interpreter.", "labels": [], "entities": []}, {"text": "The distribution of labels across the classes is different in this subset compared to the rest of the corpus.", "labels": [], "entities": []}, {"text": "Therefore we can train a subset-specific classi-fier, reducing the amount of annotated training data needed without compromising performance of the combined system.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe an architecture for combining semantic interpretation and classification in a system with dynamic natural language feedback generation.", "labels": [], "entities": [{"text": "semantic interpretation and classification", "start_pos": 55, "end_pos": 97, "type": "TASK", "confidence": 0.7546890676021576}, {"text": "natural language feedback generation", "start_pos": 123, "end_pos": 159, "type": "TASK", "confidence": 0.7662464678287506}]}, {"text": "In Section 3 we describe an experiment to improve combined system performance using a classifier trained only on non-interpretable utterances.", "labels": [], "entities": []}, {"text": "We discuss future improvements in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Beetle portion of the SRA corpus contains 3941 unique student answers to 47 different explanation questions.", "labels": [], "entities": [{"text": "Beetle portion of the SRA corpus", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.7716848452885946}]}, {"text": "Each question is associated with one or more reference answers provided by expert tutors, and each student answer is manually annotated with the label assigned by the BEETLE II interpreter and a gold-standard correctness label.", "labels": [], "entities": [{"text": "BEETLE II interpreter", "start_pos": 167, "end_pos": 188, "type": "METRIC", "confidence": 0.8776553869247437}]}, {"text": "In our experiments, we follow the procedure described in (), using 10-fold cross-validation to evaluate the performance of the various stand-alone and combined systems.", "labels": [], "entities": []}, {"text": "We report the per-class F 1 scores as evaluation metrics, using the macro-averaged F 1 score as the primary evaluation metric.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.8588309089342753}, {"text": "F 1 score", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9186331232388815}]}, {"text": "used a statistical classifier based on lexical overlap, taken from (Dzikovska et al., 2012a), and evaluated 3 different rule-based policies for combining its output with that of the semantic interpreter.", "labels": [], "entities": []}, {"text": "In two of those policies the interpreter's output is always used if it is available, and the classifier's label is used fora (subset of) noninterpretable utterances: 1.", "labels": [], "entities": []}, {"text": "NoReject: the classifier's label is used in all cases where semantic interpretation fails, thus creating a system that never rejects student input as non-interpretable 2.", "labels": [], "entities": []}, {"text": "NoRejectCorrect: the classifier's label is used for non-interpretable utterances which are labeled as \"correct\" by the classifier.", "labels": [], "entities": []}, {"text": "This more conservative policy aims to ensure that correct student answers are always accepted, but incorrect answers may still be rejected with a request to rephrase.", "labels": [], "entities": []}, {"text": "We conducted anew experiment to evaluate these two policies together with an enhanced classifier, discussed in the next section.", "labels": [], "entities": []}, {"text": "Evaluation results are shown in.", "labels": [], "entities": []}, {"text": "Unless otherwise specified, all performance differences discussed in the text are significant on an approximate randomization significance test with 10,000 iterations).", "labels": [], "entities": []}, {"text": "Adding the new features to create the Sim20 classifier resulted in a performance improvement compared to the Sim8 classifier, raising macroaveraged F 1 from 0.45 to 0.48, with an improvement in contradiction detection as intended.", "labels": [], "entities": [{"text": "Sim20 classifier", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.9269769489765167}, {"text": "F 1", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9158603847026825}, {"text": "contradiction detection", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.6669656187295914}]}, {"text": "But these improvements did not translate into improvements in the combined systems.", "labels": [], "entities": []}, {"text": "Combinations using Sim20 performed exactly the same as the combinations using Sim8 (not shown due to space limitations, see).", "labels": [], "entities": [{"text": "Sim20", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.9122598767280579}]}, {"text": "Clearly, more sophisticated features are needed to obtain further performance gains in the combined systems.", "labels": [], "entities": []}, {"text": "However, we noted that the subset of noninterpretable utterances in the corpus has a different distribution of labels compared to the full data set.", "labels": [], "entities": []}, {"text": "In the complete data set, 1665 utterances (42%) are labeled as correct and 1049 (27%) as contradictory.", "labels": [], "entities": []}, {"text": "Among the 1416 utterances considered noninterpretable by the semantic interpreter, 371 (26%) belong to the \"correct\" class, and 598 (42%) to \"contradictory\" (other classes have similar distributions in both subsets).", "labels": [], "entities": []}, {"text": "We therefore hypothesized that a combination system that uses the classifier output only if an utterance is non-interpretable, may benefit from employing a classifier trained specifically on this subset rather than on the whole data set.", "labels": [], "entities": []}, {"text": "If our hypothesis is true, it offers an interesting possibility for combining rule-based and statistical classifiers in similar setups: if the classifier can be trained using only the examples that are problematic for the rule-based system, it can provide improved robustness at a significantly lower annotation cost.", "labels": [], "entities": []}, {"text": "We therefore trained another classifier, Sim20NI, using the same feature set as Sim20, but this time using only the instances rejected as non-interpretable by the semantic interpreter in each cross-validation fold (1416 utterances, 36% of all data instances).", "labels": [], "entities": []}, {"text": "We again used the NoReject and NoRejectCorrect policies to combine the output of Sim20NI with that of the semantic interpreter.", "labels": [], "entities": []}, {"text": "Evaluation results confirmed our hypothesis.", "labels": [], "entities": []}, {"text": "The system combinations that use Sim20 and Sim20NI perform identically on macro-averaged F 1 , with NoReject being the best combination policy in both cases and significantly outperforming the semantic interpreter alone.", "labels": [], "entities": []}, {"text": "However, the Sim20NI classifier has the advantage of needing significantly less annotated data to achieve this performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F 1 scores for three stand-alone systems, and for combination systems using the Sim20 and Sim20NI  classifiers together with the semantic interpreter. Stand-alone performance for Sim20NI is not shown since it was  trained only on the non-interpretable data subset and is therefore not applicable for the complete data set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9815650880336761}, {"text": "Sim20", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.9214464426040649}]}]}