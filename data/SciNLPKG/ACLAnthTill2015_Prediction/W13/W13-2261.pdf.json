{"title": [{"text": "Evaluating (and Improving) Sentence Alignment under Noisy Conditions", "labels": [], "entities": [{"text": "Evaluating (and Improving) Sentence Alignment", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.6888706982135773}]}], "abstractContent": [{"text": "Sentence alignment is an important step in the preparation of parallel data.", "labels": [], "entities": [{"text": "Sentence alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9431909024715424}]}, {"text": "Most aligners do not perform very well when the input is a noisy, rather than a highly-parallel, document pair.", "labels": [], "entities": []}, {"text": "Evaluating align-ers under noisy conditions would seem to require creating an evaluation dataset by manually annotating a noisy document for gold-standard alignments.", "labels": [], "entities": []}, {"text": "Such a costly process hinders our ability to evaluate an aligner under various types and levels of noise.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew evaluation framework for sentence aligners, which is particularly suitable for noisy-data evaluation.", "labels": [], "entities": []}, {"text": "Our approach is unique as it requires no manual labeling, instead relying on small parallel datasets (already at the disposal of MT researchers) to generate many evaluation datasets that mimic a variety of noisy conditions.", "labels": [], "entities": [{"text": "MT", "start_pos": 129, "end_pos": 131, "type": "TASK", "confidence": 0.9261146783828735}]}, {"text": "We use our framework to perform a comprehensive comparison of three aligners under noisy conditions.", "labels": [], "entities": []}, {"text": "Furthermore, our framework facilitates the fine-tuning of a state-of-the-art sentence aligner, allowing us to substantially increase its recall rates by anywhere from 5% to 14% (absolute) across several language pairs.", "labels": [], "entities": [{"text": "recall rates", "start_pos": 137, "end_pos": 149, "type": "METRIC", "confidence": 0.9852504432201385}]}], "introductionContent": [{"text": "Virtually all training pipelines of statistical machine translation systems expect training data to be in the form of a sequence of parallel sentence pairs.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.6186881065368652}]}, {"text": "This means that a pair of parallel documents must first be segmented into a sequence of aligned sentence pairs, discarding or combining sentences when needed, and aligning sentences as appropriate.", "labels": [], "entities": []}, {"text": "The performance and output of an SMT system is directly dependent on the amount and quality of available training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9944804906845093}]}, {"text": "Therefore, it is critical to perform this sentence alignment step properly, ensuring both high recall (to have as much training data as possible) and high precision (to avoid noisy training data).", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7514806687831879}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9993197917938232}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9991206526756287}]}, {"text": "While sentence aligners achieve excellent performance on highly-parallel, clean data, the task is much more difficult under noisy conditions.", "labels": [], "entities": [{"text": "sentence aligners", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7381408214569092}]}, {"text": "Some prior work has investigated evaluation under noisy conditions (see section 6), but the major focus of prior work has been the clean-data scenario, where accuracy rates exceed 98% (e.g.,).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9983483552932739}]}, {"text": "For one thing, this meant that the various sentence alignment algorithms differ only slightly in absolute terms.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7240232974290848}]}, {"text": "Similarly, finetuning anyone of those algorithms might not seem to have an impact on performance.", "labels": [], "entities": []}, {"text": "More importantly, this also meant that we do not have a clear understanding of how well these algorithms would perform under noisy conditions.", "labels": [], "entities": []}, {"text": "Arguably, there was little need to examine sentence alignment of noisy datasets in early MT research, since almost all training data came from high-quality, highly-parallel sources, such as UN documents or parliamentary proceedings.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.713194340467453}, {"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.991302490234375}]}, {"text": "However, recent efforts have attempted to utilize web resources and non-perfectly-parallel texts, such as Wikipedia articles and news stories (e.g.,,, and).", "labels": [], "entities": []}, {"text": "Such resources naturally contain significantly more noise, at a level that would render sentence alignment a much less straightforward task.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7702793180942535}]}, {"text": "Because sentence alignment algorithms had usually been evaluated under a clean-data scenario, there are fewer empirical results to guide those who wish to extract parallel data from noisy sources.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.8115680813789368}]}, {"text": "Furthermore, there is also no easy way to fine-tune an aligner of interest.", "labels": [], "entities": []}, {"text": "For building the Microsoft Translation service, we are continuously mining inherently-noisy web resources, from which we extract MT training data for dozens of the world's languages.", "labels": [], "entities": [{"text": "Microsoft Translation", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.6285600364208221}, {"text": "MT training", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.9045365750789642}]}, {"text": "Therefore, having a principled method to evaluate and fine-tune our aligner was critical.", "labels": [], "entities": []}, {"text": "In this paper, we describe our framework for evaluating sentence alignment under noisy conditions.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7067489475011826}]}, {"text": "We use this framework to examine and evaluate the Moore alignment algorithm, which was empirically shown to be stateof-the-art under clean conditions, and which we regularly use to extract parallel data from web resources to create training data.", "labels": [], "entities": [{"text": "Moore alignment", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.6422305703163147}]}, {"text": "We perform a comprehensive comparison of this aligner against two other algorithms, and furthermore use our framework to fine-tune the algorithm along dimensions of interest (such as the aligner's search parameters) by quantitatively evaluating how the aligner's performance is affected by such changes.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We briefly define sentence alignment and existing approaches in section 2.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7717506289482117}]}, {"text": "We then discuss the evaluation of alignment algorithms in section 3, and present our evaluation framework.", "labels": [], "entities": []}, {"text": "In section 4, we perform a comparative assessment of three alignment algorithms using our framework, illustrating the differences between them under noisy conditions.", "labels": [], "entities": []}, {"text": "In section 5, we present two additional applications of our framework, namely fine-tuning an aligner and performing training data cleanup.", "labels": [], "entities": []}, {"text": "Finally, we give an overview in section 6 of prior work that has tackled the specific issue of evaluating sentence aligners.", "labels": [], "entities": []}], "datasetContent": [{"text": "How can we create a noisy-data scenario under which to evaluate a sentence alignment algorithm?", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7256059944629669}]}, {"text": "One approach is to mimic prior work: in a dataset that is known to be noisy, have an annotator select the sentence pairs that should be aligned to each other.", "labels": [], "entities": []}, {"text": "However, this approach would be expensive and time-consuming.", "labels": [], "entities": []}, {"text": "We propose a completely different approach.", "labels": [], "entities": []}, {"text": "Rather than attempting to annotate corresponding sentences in a dataset that is known to be noisy, we deliberately introduce noise into a dataset that is already perfectly-aligned (and for which, as a consequence, we already know the sentence correspondence).", "labels": [], "entities": []}, {"text": "Specifically, we start with a parallel dataset D that we know to be perfectly-aligned.", "labels": [], "entities": []}, {"text": "Such datasets are abundant and readily available for MT researchers in the form of a myriad of tuning and test datasets across many language pairs and domains.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9917194843292236}]}, {"text": "We introduce noise into D (using any of the methods described below and detailed in subsection 4.2) to obtain a modified dataset D . The source side of Dis a subset of the source side of D (possibly reordered), and the same holds for the target side.", "labels": [], "entities": []}, {"text": "Since we know what the correct sentence alignments are in D, we also know, by mere construction, what the correct alignments in Dare as well.", "labels": [], "entities": []}, {"text": "This allows us to easily compute precision and recall of a sentence alignment algorithm when it is given D as input, without the need to collect a single annotation.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9989352822303772}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9979755282402039}, {"text": "sentence alignment", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.68488509953022}]}, {"text": "We employ several methods to create a noisy dataset D from a perfectly-aligned dataset D: 2 \u2022 Clean dataset.", "labels": [], "entities": []}, {"text": "The source and target sides of Dare exactly the unaltered source and target sides of D.", "labels": [], "entities": [{"text": "Dare", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.949928343296051}]}, {"text": "This represents the easiest test set fora sentence aligner, as the test set consists entirely of 1-to-1 mappings, all of which fall exactly along the search matrix diagonal.", "labels": [], "entities": []}, {"text": "The source side of Dis a subset of the source side of D, where the number of discarded sentences is determined by a source deletion rate del s . For example, fora dataset D with 1000 sentences on the source side and del s = 0.10, the source side of D consists of 900 randomly-chosen sentences from the source side of D (with no reordering).", "labels": [], "entities": []}, {"text": "The target side of Dis created similarly, using a target deletion rate del t . Note that the deletion on the target side is done independently from the deletion on the source side.", "labels": [], "entities": []}, {"text": "That is, the probability of deleting the ith sentence on the target side is del t , regardless of whether the ith sentence on the source side was deleted or not.", "labels": [], "entities": []}, {"text": "The source and target sides of Dare the same as those from D, but with random consecutive pairs of sentences combined into a single sentence.", "labels": [], "entities": []}, {"text": "As with random deletions, the combination processes on the source side and on the target side are independent from each other.", "labels": [], "entities": []}, {"text": "The source side of D consists of the source side of D, but in random order.", "labels": [], "entities": []}, {"text": "The target side of Dis also randomized.", "labels": [], "entities": [{"text": "Dis", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9578865170478821}]}, {"text": "\u2022 Length-aligned from same dataset.", "labels": [], "entities": [{"text": "Length-aligned", "start_pos": 2, "end_pos": 16, "type": "METRIC", "confidence": 0.9896534085273743}]}, {"text": "The source side of Dis exactly the same as the source side of D.", "labels": [], "entities": []}, {"text": "The noise is introduced into the target side, where all the target sentences from Dare preserved, but they are reordered.", "labels": [], "entities": []}, {"text": "The reordering is not completely stochastic.", "labels": [], "entities": []}, {"text": "Rather, an attempt is made to have the sentences length-aligned as much as possible.", "labels": [], "entities": []}, {"text": "This is somewhat of an adversarial scenario, since a length-based alignment method would align too many sentences that are completely unrelated to each other.", "labels": [], "entities": []}, {"text": "\u2022  We performed our filtering experiments on two systems, Arabic-English and Urdu-English, with the results displayed in, respectively.", "labels": [], "entities": []}, {"text": "In all cases but one, the BLEU score went up or down by less than a quarter of a point, indicating general stability in performance quality.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9852329790592194}]}, {"text": "the prior deletion probability is customized for each portion of our training data, based on our belief of how noisy that portion of the dataset is.", "labels": [], "entities": []}, {"text": "We are also expanding the experiments to include more language pairs.", "labels": [], "entities": []}, {"text": "For random deletions, we use six different deletion rates (from 0.00 to 0.25, with 0.05 increments), both on the source side and the target side, fora total of 35 test sets.", "labels": [], "entities": []}, {"text": "For random combinations, we use four different combination rates (from 0.00 to 0.15, with 0.05 increments), both on the source side and the target side, fora total of 15 test sets.", "labels": [], "entities": []}, {"text": "Note that we do not consider the case when both deletion/combination rates are 0.00, since that mimics the clean-dataset scenario.", "labels": [], "entities": []}, {"text": "For the length-aligned scenario, we align each source sentence with a randomly-selected sentence from the target side that is closest in length to that source sentence.", "labels": [], "entities": []}, {"text": "(We take the target-tosource length ratio into consideration, and multiply the source length by that ratio before trying to find the closest-length target sentence.)", "labels": [], "entities": []}, {"text": "If several target sentences have lengths that are equally close to the desired length, we pick one at random.", "labels": [], "entities": []}, {"text": "We note here that if the source sentences are processed sequentially, there will be a clustering of overly long target sentences at the bottom of the dataset, since such sentences are never chosen based on length -they are simply too long.", "labels": [], "entities": []}, {"text": "Therefore, we process the source sentences in random order rather than sequentially, to avoid this clustering of long sentences.", "labels": [], "entities": []}, {"text": "In the previous section, we utilized our framework to perform a comparison between three different aligners, by evaluating them under various noisydata circumstances.", "labels": [], "entities": []}, {"text": "In this section, we use our framework in two more applications relevant to sentence alignment and machine translation.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8026231825351715}, {"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.791825145483017}]}, {"text": "We provide our current training data as input to the sentence aligner, and treat the output of the aligner as a filtered version of our data, since sentences that are discarded (not aligned) by the aligner tend to be noisy data.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of this process, we compare models trained with pre-filtered data vs. ones trained with the filtered data.", "labels": [], "entities": []}, {"text": "We examine how the filtering affects the data and model size, since trimming those down would speedup training and translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 115, "end_pos": 126, "type": "TASK", "confidence": 0.967033863067627}]}, {"text": "This is especially relevant for us given the large number of language pairs for which we train models.", "labels": [], "entities": []}, {"text": "To ensure the translation quality doesn't degrade, we measure the effect on translation quality for two in-house evaluation datasets.", "labels": [], "entities": []}, {"text": "We consider three scenarios: \u2022 No filtering.", "labels": [], "entities": []}, {"text": "As a baseline, we use our training data as-is to train the MT system, without any filtering.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9778155088424683}]}, {"text": "We provide our training data as input to the sentence aligner, and use the aligner's output as the training data to train the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.9799979329109192}]}, {"text": "(We refer to this as 'uniform' filtering in contrast to the next scenario.)", "labels": [], "entities": []}, {"text": "\u2022 Filtering 'web' datasets.", "labels": [], "entities": []}, {"text": "Here, we apply sentence alignment filtering only to certain hand-picked datasets that we believe to contain a relatively high level of noise.", "labels": [], "entities": [{"text": "sentence alignment filtering", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.8589620391527811}]}, {"text": "The datasets are not picked by inspecting their content, but simply by deciding that any dataset that came from online sources (aka 'web' data) should undergo filtering.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of the MRE+ fine-tuning experi- ment for the 0.05 deletion rate scenario. For clar- ity, we show only recall rates -all precision rates  are 99% or higher.", "labels": [], "entities": [{"text": "MRE+ fine-tuning experi- ment", "start_pos": 25, "end_pos": 54, "type": "METRIC", "confidence": 0.9113500714302063}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.998910665512085}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.998163640499115}]}, {"text": " Table 3: Results of the MRE+ fine-tuning experi- ment for the 0.20 deletion rate scenario. For clar- ity, we show only recall rates -all precision rates  are 98% or higher.", "labels": [], "entities": [{"text": "MRE+ fine-tuning experi- ment", "start_pos": 25, "end_pos": 54, "type": "METRIC", "confidence": 0.9074283242225647}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9989398121833801}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9984435439109802}]}, {"text": " Table 4: Results of the data filtering experiments  for the Arabic-English system.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7549236416816711}]}, {"text": " Table 5: Results of the data filtering experiments  for the Urdu-English system.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7654532194137573}]}]}