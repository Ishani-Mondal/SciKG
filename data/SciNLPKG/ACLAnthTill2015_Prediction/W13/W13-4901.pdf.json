{"title": [{"text": "Working with a small dataset -semi-supervised dependency parsing for Irish", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7078841924667358}]}], "abstractContent": [{"text": "We present a number of semi-supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger, yet still relatively small, set of unlabelled sentences.", "labels": [], "entities": []}, {"text": "We take two popular dependency parsers-one graph-based and one transition-based-and compare results for both.", "labels": [], "entities": []}, {"text": "Results show that using semi-supervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 134, "end_pos": 141, "type": "TASK", "confidence": 0.9756802320480347}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.909672737121582}]}, {"text": "We also try to use morphological information in a targeted way and fail to see any improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Developing a data-driven statistical parser relies on the availability of a parsed corpus for the language in question.", "labels": [], "entities": []}, {"text": "In the case of Irish, the only parsed corpus available to date is a dependency treebank, which is currently underdevelopment and still relatively small, with only 803 gold-annotated trees ().", "labels": [], "entities": [{"text": "dependency treebank", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.6592231690883636}]}, {"text": "As treebank development is a labour-and time-intensive process, in this study we evaluate various approaches to bootstrapping a statistical parser with a set of unlabelled sentences to ascertain how accurate parsing output can beat this time.", "labels": [], "entities": []}, {"text": "We carryout a number of different semi-supervised bootstrapping experiments using self-training, co-training and sample-selectionbased co-training.", "labels": [], "entities": []}, {"text": "Our studies differ from previous similar experiments as our data is taken from a workin-progress treebank.", "labels": [], "entities": []}, {"text": "Thus, aside from the current small treebank which is used for training the initial seed model and for testing, there is no additional gold-labelled data available to us to directly compare supervised and semi-supervised approaches using training sets of comparable sizes.", "labels": [], "entities": []}, {"text": "In the last decade, data-driven dependency parsing has come to fore, with two main approaches dominating -transition-based and graph-based.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7418193519115448}]}, {"text": "In classic transition-based dependency parsing, the training phase consists of learning the correct parser action to take given the input string and the parse history, and the parsing phase consists of the greedy application of parser actions as dictated by the learned model.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 11, "end_pos": 46, "type": "TASK", "confidence": 0.6340494354565939}]}, {"text": "In contrast, graph-based dependency parsing involves the non-deterministic construction of a parse tree by predicting the maximumspanning-tree in the digraph for the input sentence.", "labels": [], "entities": [{"text": "graph-based dependency parsing", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.6745883226394653}]}, {"text": "In our study, we employ Malt (), a transition-based dependency parsing system, and Mate, a graph-based parser.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.6354306737581888}]}, {"text": "In line with similar experiments carried out on English (, we find that cotraining is more effective than self-training.", "labels": [], "entities": []}, {"text": "Cotraining Malt on the output of Mate proves to be the most effective method for improving Malt's performance on the limited data available for Irish.", "labels": [], "entities": [{"text": "Mate", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.7892850637435913}, {"text": "Irish", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.946494460105896}]}, {"text": "Yet, the improvement is relatively small (0.6% over the baseline for LAS, 0.3% for UAS) for the best co-trained model.", "labels": [], "entities": [{"text": "UAS", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.43184223771095276}]}, {"text": "The best Mate results are achieved through a non-iterative agreement-based co-training approach, in which Mate is trained on trees produced by Malt which exhibit a minimum agreement of 85% with Mate (LAS increase of 1.2% and UAS of 1.4%).", "labels": [], "entities": [{"text": "Malt", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.9555902481079102}, {"text": "LAS increase", "start_pos": 200, "end_pos": 212, "type": "METRIC", "confidence": 0.9876931607723236}, {"text": "UAS", "start_pos": 225, "end_pos": 228, "type": "METRIC", "confidence": 0.9995670914649963}]}, {"text": "The semi-supervised parsing experiments do not explicitly take into account the morphosyntactic properties of the Irish language.", "labels": [], "entities": []}, {"text": "In order to examine the effect of this type of information during parsing, we carryout some orthogonal experiments where we reduce word forms to lemmas and introduce morphological features in certain cases.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9672146439552307}]}, {"text": "These changes do not bring about an increase in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.9825863838195801}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9522649049758911}]}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 is an overview of Irish morphology.", "labels": [], "entities": [{"text": "Irish morphology", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.6859559118747711}]}, {"text": "In Section 3 our previous work carried out on the development of an Irish dependency treebank is discussed followed in Section 4 by a description of some of our prior parsing results.", "labels": [], "entities": [{"text": "Irish dependency treebank", "start_pos": 68, "end_pos": 93, "type": "DATASET", "confidence": 0.9491250514984131}]}, {"text": "Section 5 describes the self-training, cotraining and sample-selection-based co-training experiments, Section 6 presents the preliminary parsing experiments involving morphological features, and, finally, Section 7 discusses our future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our previous work (Lynn et al., 2012a), we carried out some preliminary parsing experiments with MaltParser and 10-fold cross-validation using 300 gold-standard trees.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.9559341669082642}]}, {"text": "We started outwith the feature template used by C \u00b8 etino\u02d8 glu et al. and examined the effect of omitting LEMMA, WORDFORM, POSTAG and CPOSTAG features and combinations of these, concluding that it was best to include all four types of information.", "labels": [], "entities": [{"text": "LEMMA", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9350458979606628}]}, {"text": "Our final LAS and UAS scores were 63.3% and 73.1% respectively.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9099604487419128}, {"text": "UAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.83331698179245}]}, {"text": "Following the changes we made to the labelling scheme as a result of the second IAA study (described above), we re-ran the same parsing experiments on the newly updated seed set of 300 sentences -the LAS increased to 66.5% and the UAS to 76.3% (.", "labels": [], "entities": [{"text": "IAA study", "start_pos": 80, "end_pos": 89, "type": "DATASET", "confidence": 0.8906874656677246}, {"text": "LAS", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.9592720866203308}, {"text": "UAS", "start_pos": 231, "end_pos": 234, "type": "METRIC", "confidence": 0.7882546782493591}]}, {"text": "In order to speedup the treebank creation, we also applied an active learning approach to bootstrapping the annotation process.", "labels": [], "entities": []}, {"text": "This work is also reported in.", "labels": [], "entities": []}, {"text": "The process involved training a MaltParser model on a small subset of the treebank data, and iteratively, parsing anew set of sentences, selecting a 50-sentence subset to hand-correct, and adding these new gold sentences to the training set.", "labels": [], "entities": []}, {"text": "We compared a passive setup, in which the parses that were selected for correction were chosen at random, to an active setup, in which the parses that were selected for correction were chosen based on the level of disagreement between two parsers (Malt and Mate).", "labels": [], "entities": []}, {"text": "The active approach to annotation resulted in superior parsing results to the passive approach (67.2% versus 68.1% LAS) but the difference was not statistically significant.", "labels": [], "entities": [{"text": "LAS", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9871059656143188}]}, {"text": "In order to alleviate data sparsity issues brought about by our lack of training material, we experiment with automatically expanding our training set using well known semi-supervised techniques.", "labels": [], "entities": []}, {"text": "In this and all subsequent experiments, we use both the same training data and unlabelled data that we refer to in Section 5.1.2.", "labels": [], "entities": []}, {"text": "Our co-training algorithm is given in and it is the same as the algorithm provided by.", "labels": [], "entities": []}, {"text": "Again, our experiments are carried out using Malt and Mate.", "labels": [], "entities": []}, {"text": "This time, the experiments are run concurrently as each parser is bootstrapped from the other parser's output.", "labels": [], "entities": []}, {"text": "(Mate output) is added to the training set Li A to make a larger training set of L i+1 A (Malt training set).", "labels": [], "entities": []}, {"text": "Conversely, the set of newly parsed sentences P i A (Malt output) is added to the training set Li B to make a larger training set of L i+1 B (Mate training set).", "labels": [], "entities": []}, {"text": "Two new parsing models (M i+1 A and M i+1 B ) are then induced by training Malt and Mate respectively with their new training sets.", "labels": [], "entities": []}, {"text": "The main algorithm for agreement-based co-training is given in.", "labels": [], "entities": []}, {"text": "Again, Malt and Mate are used.", "labels": [], "entities": []}, {"text": "However, this algorithm differs from the co-training algorithm in in that rather than adding the full set of 323 newly parsed trees (P i A and P i B ) to the training set at each iteration, selected subsets of these trees (P i A and P i B ) are added instead.", "labels": [], "entities": []}, {"text": "To define these subsets, we identify the trees that have 85% or higher agreement between the two parser output sets.", "labels": [], "entities": []}, {"text": "As a result, the number of trees in the subsets differ at each iteration.", "labels": [], "entities": []}, {"text": "For iteration 1, 89 trees reach the agreement threshold; iteration 2, 93 trees; iteration 3, 117 trees; iteration 4, 122 trees; iteration 5, 131 trees; iteration 6, 114 trees.", "labels": [], "entities": []}, {"text": "The number of trees in the training sets is much smaller compared with those in the experiments of Section 5.2.", "labels": [], "entities": [{"text": "Section 5.2", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.8084688484668732}]}, {"text": "As well as the size of the dataset, data sparsity is also confounded by the number of possible inflected forms fora given root form.", "labels": [], "entities": []}, {"text": "With this in mind, and following on from the discussion in Section 5.4, we carryout further parsing experiments in an attempt to make better use of morphological information during parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9813302159309387}, {"text": "parsing", "start_pos": 181, "end_pos": 188, "type": "TASK", "confidence": 0.9613534808158875}]}, {"text": "We attack this in two ways: by reducing certain words to their lemmas and by including morphological information in the optional FEATS (features) field.", "labels": [], "entities": [{"text": "FEATS", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.951445460319519}]}, {"text": "The reasoning behind reducing certain word forms to lemmas is to further reduce the differences between inflected forms of the same word, and the reasoning behind including morphological information is to make more explicit the similarity between two different word forms inflected in the same way.", "labels": [], "entities": []}, {"text": "All experiments are car-: Results with morphological features on the development set ried outwith MaltParser and our seed training set of 500 gold trees.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.9740671515464783}]}, {"text": "We focus on two phenomena: prepositional pronouns or pronominal prepositions (see Section 2) and verbs with incorporated subjects (see Section 2 and Section 5.4).", "labels": [], "entities": []}, {"text": "In the first experiment, we include extra morphological information for pronominal prepositions.", "labels": [], "entities": []}, {"text": "We ran three parsing experiments: (i) replacing the value of the surface form (FORM) of pronominal prepositions with their lemma form (LEMMA), for example agam\u2192ag, (ii) including morphological information for pronominal prepositions in the FEATS column.", "labels": [], "entities": [{"text": "FORM)", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9124805927276611}, {"text": "LEMMA", "start_pos": 135, "end_pos": 140, "type": "METRIC", "confidence": 0.924041211605072}, {"text": "FEATS", "start_pos": 240, "end_pos": 245, "type": "METRIC", "confidence": 0.8904154300689697}]}, {"text": "For example, in the case of agam 'at me', we include Per=1P|Num=Sg, (iii) we combine both approaches of reverting to lemma form and also including the morphological features.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "In the second experiment, we include morphological features for verbs with incorporated subjects: imperative verb forms, synthetic verb forms and autonomous verb forms such as those outlined in Section 5.4.", "labels": [], "entities": []}, {"text": "For each instance of these verb types, we included incorpSubj=true in the FEATS column.", "labels": [], "entities": [{"text": "FEATS", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9250431060791016}]}, {"text": "The results are also given in.", "labels": [], "entities": []}, {"text": "The experiments on the pronominal prepositions show a drop in parsing accuracy while the experiments carried out using verb morphological information showed no change in parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.884136438369751}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.6790584921836853}]}, {"text": "In the case of inflected prepositions, perhaps we have not seen any improvement because we have not focused on a phenomenon which is critical for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 146, "end_pos": 153, "type": "TASK", "confidence": 0.9706246256828308}]}], "tableCaptions": [{"text": " Table 1: Results for best performing models", "labels": [], "entities": []}, {"text": " Table 2: Results with morphological features on the de- velopment set", "labels": [], "entities": []}]}