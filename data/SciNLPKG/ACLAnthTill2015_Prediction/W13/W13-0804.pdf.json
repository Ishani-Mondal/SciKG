{"title": [{"text": "A Performance Study of Cube Pruning for Large-Scale Hierarchical Machine Translation", "labels": [], "entities": [{"text": "Hierarchical Machine Translation", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.6780752340952555}]}], "abstractContent": [{"text": "In this paper, we empirically investigate the impact of critical configuration parameters in the popular cube pruning algorithm for decoding in hierarchical statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 157, "end_pos": 188, "type": "TASK", "confidence": 0.6053930024305979}]}, {"text": "Specifically, we study how the choice of the k-best generation size affects translation quality and resource requirements in hierarchical search.", "labels": [], "entities": []}, {"text": "We furthermore examine the influence of two different granular-ities of hypothesis recombination.", "labels": [], "entities": []}, {"text": "Our experiments are conducted on the large-scale Chinese\u2192English and Arabic\u2192English NIST translation tasks.", "labels": [], "entities": [{"text": "NIST translation tasks", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7678868571917216}]}, {"text": "Besides standard hierarchical grammars, we also explore search with restricted recursion depth of hierarchical rules based on shallow-1 grammars.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cube pruning ) is a widely used search strategy in state-of-the-art hierarchical decoders.", "labels": [], "entities": []}, {"text": "Some alternatives and extensions to the classical algorithm as proposed by David Chiang have been presented in the literature since, e.g. cube growing, lattice-based hierarchical translation (, and source cardinality synchronous cube pruning . Standard cube pruning remains the commonly adopted decoding procedure in hierarchical machine translation research at the moment, though.", "labels": [], "entities": [{"text": "cube growing", "start_pos": 138, "end_pos": 150, "type": "TASK", "confidence": 0.752876490354538}, {"text": "hierarchical translation", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.6923334002494812}, {"text": "hierarchical machine translation", "start_pos": 317, "end_pos": 349, "type": "TASK", "confidence": 0.6343045830726624}]}, {"text": "The algorithm has meanwhile been implemented in many publicly available toolkits, as for example in Moses (), Joshua (), Jane (), cdec,, and NiuTrans (.", "labels": [], "entities": []}, {"text": "While the plain hierarchical approach to machine translation (MT) is only formally syntax-based, cube pruning can also be utilized for decoding with syntactically or semantically enhanced models, for instance those by,,,, ,, or.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8573169708251953}]}, {"text": "Here, we look into the following key aspects of hierarchical phrase-based translation with cube pruning: \u2022 Deep vs. shallow grammar.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6323153972625732}]}, {"text": "We conduct a comparative study of all combinations of these three factors in hierarchical decoding and present detailed experimental analyses with respect to translation quality and search efficiency.", "labels": [], "entities": []}, {"text": "We focus on two tasks which are of particular interest to the research community: the Chinese\u2192English and Arabic\u2192English NIST OpenMT translation tasks.", "labels": [], "entities": [{"text": "NIST OpenMT translation", "start_pos": 121, "end_pos": 144, "type": "TASK", "confidence": 0.7422332763671875}]}, {"text": "The paper is structured as follows: We briefly outline some important related work in the following section.", "labels": [], "entities": []}, {"text": "We subsequently give a summary of the grammars used in hierarchical phrase-based translation, including a presentation of the difference between a deep and a shallow-1 grammar (Section 3).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6268957257270813}]}, {"text": "Essential aspects of hierarchical search with the cube pruning algorithm are explained in Section 4.", "labels": [], "entities": []}, {"text": "We show how the k-best generation size is defined (we apply the limit without counting recombined candidates), and we present the two different hypothesis recombination schemes (recombination T and recombination LM).", "labels": [], "entities": []}, {"text": "Our empirical investigations and findings constitute the major part of this work: In Section 5, we first accurately describe our setup, then conduct a number of comparative experiments with varied parameters on the two translation tasks, and finally analyze and discuss the results.", "labels": [], "entities": []}, {"text": "We conclude the paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments which evaluate performance in terms of both translation quality and computational efficiency, i.e. translation speed and memory consumption, for combinations of deep or shallow-1 grammars with the two hypothesis recombination schemes and an exhaustive range of k-best generation size settings.", "labels": [], "entities": []}, {"text": "Empirical results are presented on the Chinese\u2192English and Arabic\u2192English 2008 NIST tasks (NIST, 2008).", "labels": [], "entities": [{"text": "NIST tasks (NIST, 2008)", "start_pos": 79, "end_pos": 102, "type": "DATASET", "confidence": 0.8596396786825997}]}, {"text": "We work with parallel training corpora of 3.0 M Chinese-English sentence pairs (77.5 M Chinese / 81.0 M English running words after preprocessing) and 2.5 M Arabic-English sentence pairs (54.3 M Arabic / 55.3 M English running words after preprocessing), respectively.", "labels": [], "entities": []}, {"text": "Word alignments are created by aligning the data in both directions with GIZA++ and symmetrizing the two trained alignments.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6383748054504395}]}, {"text": "When extracting phrases, we apply several restrictions, in particular a maximum length often on source and target side for lexical phrases, a length limit of five on source and tenon target side for hierarchical phrases (including non-terminal symbols), and no more than two gaps per phrase.", "labels": [], "entities": []}, {"text": "The decoder loads only the best translation options per distinct source side with respect to the weighted phrase-level model scores (100 for Chinese, 50 for Arabic).", "labels": [], "entities": []}, {"text": "The language models are 4-grams with modified Kneser-Ney smoothing) which have been trained with the SRILM toolkit.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.815255880355835}]}, {"text": "During decoding, a maximum length constraint often is applied to all non-terminals except the initial symbol S . Model weights are optimized with MERT (Och, 2003) on 100-best lists.", "labels": [], "entities": [{"text": "MERT", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9360155463218689}]}, {"text": "The optimized weights are obtained (separately for deep and for shallow-1 grammars) with a k-best generation size of 1 000 for Chinese\u2192English and of 500 for Arabic\u2192English and kept for all setups.", "labels": [], "entities": []}, {"text": "We employ MT06 as development sets.", "labels": [], "entities": [{"text": "MT06", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.9363322257995605}]}, {"text": "Translation quality is measured in truecase with BLEU () on the MT08 test sets.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9394229054450989}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9992365837097168}, {"text": "MT08 test sets", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9721850355466207}]}, {"text": "Data statistics for the preprocessed source sides of both the Chinese\u2192English MT08 test set and the Arabic\u2192English MT08 test set are given in.", "labels": [], "entities": [{"text": "Chinese\u2192English MT08 test set", "start_pos": 62, "end_pos": 91, "type": "DATASET", "confidence": 0.6468554139137268}, {"text": "Arabic\u2192English MT08 test set", "start_pos": 100, "end_pos": 128, "type": "DATASET", "confidence": 0.6814004927873611}]}, {"text": "Our translation experiments are conducted with the open source translation toolkit Jane ().", "labels": [], "entities": []}, {"text": "The core implementation of the toolkit is written in C++.", "labels": [], "entities": []}, {"text": "We compiled with GCC version 4.4.3 using its -O2 optimization flag.", "labels": [], "entities": []}, {"text": "We employ the SRILM libraries to perform language model scoring in the decoder.", "labels": [], "entities": [{"text": "language model scoring", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6389523843924204}]}, {"text": "In binarized version, the language models have a size of 3.6G (Chinese\u2192English) and 6.2G (Arabic\u2192English).", "labels": [], "entities": []}, {"text": "Language models and phrase tables have been copied to the local hard disks of the machines.", "labels": [], "entities": []}, {"text": "In all experiments, the language model is completely loaded beforehand.", "labels": [], "entities": []}, {"text": "Loading time of the language model and any other initialization steps are not included in the measured translation time.", "labels": [], "entities": []}, {"text": "Phrase tables are in the Jane toolkit's binarized format.", "labels": [], "entities": [{"text": "Phrase tables", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8242544829845428}, {"text": "Jane toolkit", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9711649715900421}]}, {"text": "The decoder initializes the prefix tree structure, required nodes get loaded from secondary storage into main memory on demand, and the loaded content is being cleared each time anew input sentence is to be parsed.", "labels": [], "entities": []}, {"text": "There is nearly no overhead due to unused data in main memory.", "labels": [], "entities": []}, {"text": "We do not rely on memory mapping.", "labels": [], "entities": [{"text": "memory mapping", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.6741006821393967}]}, {"text": "Memory statistics are with respect to virtual memory.", "labels": [], "entities": []}, {"text": "The hardware was equipped with RAM well beyond the requirements of the tasks, and sufficient memory has been reserved for the processes.", "labels": [], "entities": [{"text": "RAM", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.981564462184906}]}, {"text": "Figures 2 and 3 depict how the Chinese\u2192English and Arabic\u2192English setups behave in terms of translation quality.", "labels": [], "entities": []}, {"text": "The k-best generation size in cube pruning is varied between 10 and 10 000.", "labels": [], "entities": []}, {"text": "The four graphs in each plot illustrate the results with combinations of deep grammar and recombination scheme T, deep grammar and recombination scheme LM, shallow grammar and recombination scheme T, as well as shallow grammar and recombination scheme LM.", "labels": [], "entities": []}, {"text": "show the corresponding translation speed in words per second for these settings.", "labels": [], "entities": []}, {"text": "The maximum memory requirements in gigabytes are given in.", "labels": [], "entities": []}, {"text": "In order to visualize the trade-offs between translation quality and resource consumption somewhat better, we plotted translation quality against time requirements in Figures 8 and 9 and translation quality against memory requirements in.", "labels": [], "entities": []}, {"text": "Translation quality and model score (averaged overall sentences; higher is better) are nicely correlated for all configurations, as can be concluded from Figures 12 through 15.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics for the test sets. Numbers have  been replaced by a special category symbol.", "labels": [], "entities": []}, {"text": " Table 2: Average amount of hypernodes per sentence and average length of the preprocessed input sentences on the  NIST Chinese\u2192English (MT08) and Arabic\u2192English (MT08) tasks.", "labels": [], "entities": [{"text": "NIST Chinese\u2192English (MT08) and Arabic\u2192English (MT08) tasks", "start_pos": 115, "end_pos": 174, "type": "DATASET", "confidence": 0.8412452658017476}]}, {"text": " Table 4: Detailed statistics about the actual amount of derivations on the NIST Arabic\u2192English task (MT08).", "labels": [], "entities": [{"text": "NIST Arabic\u2192English task (MT08)", "start_pos": 76, "end_pos": 107, "type": "DATASET", "confidence": 0.7879076711833477}]}]}