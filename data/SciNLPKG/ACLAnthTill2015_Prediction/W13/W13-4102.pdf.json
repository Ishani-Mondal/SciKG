{"title": [{"text": "Topic Modeling with Sentiment Clues and Relaxed Labeling Schema", "labels": [], "entities": [{"text": "Topic Modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8211594521999359}]}], "abstractContent": [{"text": "This paper proposes a method to extract sentiment topics from a text collection.", "labels": [], "entities": [{"text": "extract sentiment topics from a text collection", "start_pos": 32, "end_pos": 79, "type": "TASK", "confidence": 0.8551867774554661}]}, {"text": "The method utilizes sentiment clues and a relaxed labeling schema to extract sentiment topics.", "labels": [], "entities": []}, {"text": "Experiments with a quantitative and a qualitative evaluations was done to confirm the performance of the method.", "labels": [], "entities": []}, {"text": "The quantitative evaluation with a polarity classification marked the accuracy of 0.701 in tweets and 0.691 in newswire texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.999489426612854}]}, {"text": "These performances are comparable to support vector machine baselines.", "labels": [], "entities": []}, {"text": "The qualitative evaluation of polarity topic extraction showed an overall accuracy of 0.729, and a higher accuracy of 0.889 for positive topic extraction.", "labels": [], "entities": [{"text": "polarity topic extraction", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6031874815622965}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9990111589431763}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9937718510627747}]}, {"text": "The result indicates the efficacy of our method in extracting sentiment topics.", "labels": [], "entities": [{"text": "extracting sentiment topics", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.9336426258087158}]}], "introductionContent": [{"text": "Continuous increase of text data arose an interest to develop a method to automatically analyze a large collection of texts.", "labels": [], "entities": []}, {"text": "Topic modeling methods such as Latent Dirichlet Allocation (LDA)() are popular methods for such analysis.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)()", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.6791901191075643}]}, {"text": "For example, they have been applied to analyze newswire topics (, scientific topics (), weblogs (, online reviews (, and microblogs ().", "labels": [], "entities": []}, {"text": "Topic modeling methods generally extract probability distributions of word as topics of a given text collection.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.778186023235321}]}, {"text": "Note that this definition is quite different from the definitions in sentiment analysis or opinion mining literatures () which basically define topic as an object of an opinion.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.937798798084259}]}, {"text": "Extracted topics are useful as a summary to catch abroad image of a text collection, but they are not always intuitively interpretable by humans.", "labels": [], "entities": []}, {"text": "Typical methods for estimating topic modeling parameters aim to maximize a likelihood of training data ().", "labels": [], "entities": [{"text": "estimating topic modeling", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.8585912783940634}]}, {"text": "This objective is known to form topics that are not always most semantically meaningful).", "labels": [], "entities": []}, {"text": "Approaches to extract more explicit topics using observed labels are being proposed., Labeled LDA (, and Partially Labeled Dirichlet Allocation (PLDA)) are such supervised topic models.", "labels": [], "entities": []}, {"text": "Labels of these supervised topic models are not required to be strictly designed.", "labels": [], "entities": []}, {"text": "Strictly designed labels here mean organized and controlled labels like the categories of Reuters Corpora (). and showed the effectiveness of using labels like del.icio.us tags, Twitter hashtags, and emoticons.", "labels": [], "entities": [{"text": "Reuters Corpora", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.9221455752849579}]}, {"text": "The use of these non-strict labels can avoid cost-intensive manual annotations of labels.", "labels": [], "entities": []}, {"text": "However, available labels completely depend on a community that provides them.", "labels": [], "entities": []}, {"text": "This is problematic when a text collection to analyze is already specified since we may not find labels that are suitable for an analysis.", "labels": [], "entities": []}, {"text": "Sentiment labels such as a product rating and a service rating are widely used labels that are community dependent.", "labels": [], "entities": []}, {"text": "For example, a hotel maybe positively rated for food but be negatively rated for room.", "labels": [], "entities": []}, {"text": "These labels have been used successfully to extract sentiments of various aspects.", "labels": [], "entities": []}, {"text": "However, these kind of rating labels cannot be expected to exist in communities other than review sites.", "labels": [], "entities": []}, {"text": "This paper presents a method to extract sentiment topics from a text collection.", "labels": [], "entities": [{"text": "extract sentiment topics from a text collection", "start_pos": 32, "end_pos": 79, "type": "TASK", "confidence": 0.8623210787773132}]}, {"text": "A noticeable characteristic of our method is that it does not require strictly designed sentiment labels.", "labels": [], "entities": []}, {"text": "The method uses sentiment clues and a relaxed labeling schema to extract sentiment topics.", "labels": [], "entities": []}, {"text": "Sentiment clue here denotes metadata or a lexical characteristic that strongly relates to a certain sentiment.", "labels": [], "entities": []}, {"text": "Some examples of sentiment clues are: a happy face emoticon that usually expresses a positive sentiment and asocial tag 1 of a disaster that tends to bear negative sentiment.", "labels": [], "entities": []}, {"text": "Sentiment label here is expected to be label that expresses a general sentiment like positive, neutral, or negative.", "labels": [], "entities": []}, {"text": "Relaxed labeling schema is a schema that defines a process of setting labels to a text using the given sentiment clues.", "labels": [], "entities": [{"text": "Relaxed labeling", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6788270771503448}]}, {"text": "The key feature of this schema is that a text with a sentiment clue gets a sentiment-cluespecific label and a sentiment label.", "labels": [], "entities": []}, {"text": "This assumes that words that co-occur with a sentiment clue tend to hold the same sentiment as the sentiment clue.", "labels": [], "entities": []}, {"text": "The assumption follows an idea from supervised sentiment classification methods of,, and which presume strong relationships between certain emoticons and certain sentiments.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7708306908607483}]}, {"text": "Our contributions in this paper are two-fold: (1) we propose a method that does not require strictly designed sentiment labels to extract sentiment topics from a text collection, (2) we show the effectiveness of our method by performing experiments with a quantitative and a qualitative evaluations.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our method in detail.", "labels": [], "entities": []}, {"text": "Section 3 explains data that are used in the experiment of the method.", "labels": [], "entities": []}, {"text": "Section 4 demonstrates the effectiveness of the method with an experiment.", "labels": [], "entities": []}, {"text": "Section 5 indicates related works of the method.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper with some future extensions to the method.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two data sets, Tweet and Newswire, are used to evaluate the performance of polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.9069953560829163}]}, {"text": "Tweet is an evaluation set of general tweets whose domain is same as the topic modeling data.", "labels": [], "entities": []}, {"text": "Newswire is an evaluation set of newswire texts whose domain is quite different from the topic modeling data.", "labels": [], "entities": [{"text": "Newswire", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9322388768196106}]}, {"text": "The details of these sets are described in the following subsections. c. A tweet does not have a POS tag that composes more than 80% of its words.", "labels": [], "entities": []}, {"text": "This condition is set to exclude tweets such as a list of nouns or an interjection that includes a repeated character.", "labels": [], "entities": []}, {"text": "We performed an experiment and two evaluations to confirm the effectiveness of the proposed method.", "labels": [], "entities": []}, {"text": "A discriminative polarity classification was performed as a quantitative evaluation.", "labels": [], "entities": [{"text": "discriminative polarity classification", "start_pos": 2, "end_pos": 40, "type": "TASK", "confidence": 0.6819666425387064}]}, {"text": "Note that this evaluation dose not directly evaluate the performance of a sentiment topic extraction.", "labels": [], "entities": [{"text": "sentiment topic extraction", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.8156416813532511}]}, {"text": "However, following the previous works that jointly modeled sentiment and topic (), we perform a sentiment classification evaluation.", "labels": [], "entities": [{"text": "sentiment classification evaluation", "start_pos": 96, "end_pos": 131, "type": "TASK", "confidence": 0.8505889574686686}]}, {"text": "A more direct evaluation will be presented in Section 4.4.2.", "labels": [], "entities": []}, {"text": "Using the parameter estimated topic model, document-topic distribution inferences were conducted to the polarity classification evaluation data described in Section 3.3.", "labels": [], "entities": [{"text": "polarity classification evaluation", "start_pos": 104, "end_pos": 138, "type": "TASK", "confidence": 0.7648811240990957}]}, {"text": "From there, a positive and a negative score were calculated for each tweet with the following equation: In the equation, dis a document (tweet), l is a label (either positive or negative), t l is a topic of l, and P (t l |d) is the posterior probability oft l given d.", "labels": [], "entities": []}, {"text": "For each tweet, a label that maximizes Equation 1 was set as a classification label.", "labels": [], "entities": [{"text": "Equation 1", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9768260419368744}]}, {"text": "The majority baseline is the case when all predictions were same.", "labels": [], "entities": []}, {"text": "This is positive for Tweet and negative for Newswire.", "labels": [], "entities": [{"text": "Newswire", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9910412430763245}]}, {"text": "These two emoticons are added to stop words since they are used as the labels of this SVM baseline.", "labels": [], "entities": []}, {"text": "As an implementation of SVM, LIBLINEAR 9 was used with L2-loss linear SVM and the cost parameter C set to 1.0.", "labels": [], "entities": [{"text": "LIBLINEAR", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9259745478630066}]}, {"text": "shows the results of polarity classifications.", "labels": [], "entities": [{"text": "polarity classifications", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8545666635036469}]}, {"text": "The proposed method marked an accuracy of 0.701 in Tweet, which is comparable to 0.705 of the SVM baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994452595710754}]}, {"text": "An accuracy was 0.691 for Newswire which is also comparable to 0.712 of the SVM baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9995926022529602}, {"text": "Newswire", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9853665828704834}, {"text": "SVM baseline", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.8350134789943695}]}, {"text": "However, the simple majority baseline has the highest accuracy of 0.753 in Newswire.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.999382734298706}, {"text": "Newswire", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.983639121055603}]}, {"text": "The quantitative evaluation evaluated the performance of the sentiment topic extraction indirectly http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/ with the sentiment classification.", "labels": [], "entities": [{"text": "sentiment topic extraction", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.8099034428596497}, {"text": "sentiment classification", "start_pos": 153, "end_pos": 177, "type": "TASK", "confidence": 0.8358868360519409}]}, {"text": "As a more direct qualitative evaluation, two persons manually evaluated the extracted 50 positive and 50 negative topics.", "labels": [], "entities": []}, {"text": "The evaluators were presented with top 40 probable words and top 20 probable tweets for each topic.", "labels": [], "entities": []}, {"text": "Top 40 probable words of topic t l were simply the top 40 words of the topic-word distribution P (w|t l ).", "labels": [], "entities": []}, {"text": "The extraction of top 20 probable tweets were more complex compared to the extraction of words.", "labels": [], "entities": []}, {"text": "Document-topic distribution inferences were run to the training data using the parameter estimated topic model.", "labels": [], "entities": []}, {"text": "For each topic t l , top 20 tweets of document-topic distribution P (t l |d) were extracted as the top 20 probable tweets oft l . The evaluators labeled positive, negative, or uninterpretable to each of the topics by examining the presented information.", "labels": [], "entities": []}, {"text": "The evaluators are instructed to label positive, negative, or uninterpretable.", "labels": [], "entities": []}, {"text": "Label uninterpretable is an exceptional label.", "labels": [], "entities": []}, {"text": "Topics with probable words and tweets that satisfy one of the following conditions were labeled uninterpretable: (a) majority of them are not in Japanese (b) majority of them are interjections or onomatopoeias, and (c) majority of them are neutral.", "labels": [], "entities": []}, {"text": "The agreement of the two evaluations was 0.406 in Cohen's Kappa.", "labels": [], "entities": [{"text": "Cohen's Kappa", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.6369560162226359}]}, {"text": "We extracted 59 topics that the two evaluators agreed with positive or negative, and measured the accuracies of the 50 positive and 50 negative topics.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9623490571975708}]}, {"text": "which indicates the success of the sentiment topics extraction.", "labels": [], "entities": [{"text": "sentiment topics extraction", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.9080417156219482}]}], "tableCaptions": [{"text": " Table 5: The compositions of the polarity classifi- cation evaluation data.", "labels": [], "entities": []}, {"text": " Table 7: The number of topics set to each labels.", "labels": [], "entities": []}, {"text": " Table 8: Examples of extracted labeled topics with Table 7 setting. Bracketed expressions in the table  are English explanations of preceding Japanese words that can not be directly translated.", "labels": [], "entities": [{"text": "Bracketed", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9825672507286072}]}, {"text": " Table 9: The polarity classification results. The  majority baseline is the case when all predictions  were same. This is positive for Tweet and negative  for Newswire.", "labels": [], "entities": [{"text": "Newswire", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9796739220619202}]}, {"text": " Table 10: The evaluation result of the 50 positive  topics and the 50 negative topics. #P and #N are  the number of topics that the two evaluators agreed  as positive and negative respectively.", "labels": [], "entities": []}]}