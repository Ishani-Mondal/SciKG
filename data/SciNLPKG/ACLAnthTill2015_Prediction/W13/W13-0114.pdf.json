{"title": [{"text": "Sources of Evidence for Implicit Argument Resolution", "labels": [], "entities": [{"text": "Implicit Argument Resolution", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.9300091862678528}]}], "abstractContent": [{"text": "Traditionally, semantic role labelling systems have focused on searching the fillers of those explicit roles appearing within sentence boundaries.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.6292867561181387}]}, {"text": "However, when the participants of a predicate are implicit and cannot be found inside sentence boundaries, this approach obtains incomplete predica-tive structures with null arguments.", "labels": [], "entities": []}, {"text": "Previous research facing this task have coincided in identifying the implicit argument filling as a special case of anaphora or coreference resolution.", "labels": [], "entities": [{"text": "implicit argument filling", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.6960416038831075}, {"text": "coreference resolution", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.9266709089279175}]}, {"text": "In this work, we review a number of theories that model the behaviour of discourse coreference and propose some adaptations to capture evidence for the implicit argument resolution task.", "labels": [], "entities": [{"text": "discourse coreference", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7043876796960831}, {"text": "implicit argument resolution task", "start_pos": 152, "end_pos": 185, "type": "TASK", "confidence": 0.682949922978878}]}, {"text": "We empirically demonstrate that exploiting such evidence our system outperforms previous approaches evaluated on the SemEval-2010 task 10 dataset.", "labels": [], "entities": [{"text": "SemEval-2010 task 10 dataset", "start_pos": 117, "end_pos": 145, "type": "DATASET", "confidence": 0.632585845887661}]}, {"text": "We complete our study identifying those cases that traditional coref-erence theories cannot cover.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most relevant tasks in the semantic processing of texts is identifying the arguments of a predicate.", "labels": [], "entities": [{"text": "semantic processing of texts", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.8086828589439392}]}, {"text": "Several systems have been developed to perform this task, called Semantic Role Labelling (SRL)).", "labels": [], "entities": [{"text": "Semantic Role Labelling (SRL))", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.7592650602261225}]}, {"text": "However they have traditionally focused on searching the fillers for the overtly realized arguments in the local context of the predicate.", "labels": [], "entities": []}, {"text": "In other words, only exploring those participants that share a syntactical relation with the predicate.", "labels": [], "entities": []}, {"text": "Since traditional SRL systems depend strongly on these syntactic relations, they cannot perform predictions when the candidate instantiation of the argument is not explicit.", "labels": [], "entities": [{"text": "SRL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9814528226852417}]}, {"text": "Nevertheless, some null instantiated arguments can be inferred from the context.", "labels": [], "entities": []}, {"text": "Using the nominal predicates of NomBank (), pointed out that the implicit arguments can add up to 65% to the coverage of the instantiations.", "labels": [], "entities": [{"text": "NomBank", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9275893568992615}]}, {"text": "As a consequence, increasing the number of connections between the predicates and their participants could help dramatically text understanding.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.9538843333721161}]}, {"text": "In FrameNet (), the predicates, called lexical-units (LU), evoke frames which roughly correspond to different events or scenarios.", "labels": [], "entities": []}, {"text": "For each frame, a set of possible arguments are defined.", "labels": [], "entities": []}, {"text": "These arguments are called Frame Elements (FE) and when they are not explicitly instantiated they are called Null Instantiations (NI).", "labels": [], "entities": []}, {"text": "When they can be inferred implicitly they are called Definite Null Instantiations (DNI).", "labels": [], "entities": [{"text": "Definite Null Instantiations (DNI", "start_pos": 53, "end_pos": 86, "type": "METRIC", "confidence": 0.7775278151035309}]}, {"text": "In the next example, the LU tenant n evoking the frame Residence has an instantiated FE, Resident, whose filler is [the tenants].", "labels": [], "entities": [{"text": "FE", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9769464135169983}]}, {"text": "The correct filler for the DNI corresponding to FE Location, [the house], appears two sentences before: \"Now, Mr. Holmes, with your permission, I will show you round the house.\"", "labels": [], "entities": [{"text": "FE Location", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9148404598236084}]}, {"text": "The various bedrooms and sitting-rooms had yielded nothing to a careful search.", "labels": [], "entities": []}, {"text": "Apparently [the tenants Residence ] Resident had brought little or nothing with them.", "labels": [], "entities": [{"text": "tenants Residence ] Resident", "start_pos": 16, "end_pos": 44, "type": "DATASET", "confidence": 0.9550193101167679}]}, {"text": "DNI Location Early studies on implicit arguments described this problem as a special case of anaphora or coreference resolution).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.8923606276512146}]}, {"text": "Also recent works cast this problem as an anaphora resolution task.", "labels": [], "entities": [{"text": "anaphora resolution task", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8034398754437765}]}, {"text": "In this work we present a detailed study of a set of features that have been traditionally used to model anaphora and coreference resolution tasks.", "labels": [], "entities": [{"text": "coreference resolution tasks", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.9276939829190572}]}, {"text": "We describe how these features manifest in a FrameNet based corpus for modeling implicit argument resolution, including an analysis of their benefits and drawbacks.", "labels": [], "entities": [{"text": "argument resolution", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7310945987701416}]}, {"text": "The paper is structured as follows: section 2 discusses the related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the SemEval-2010 task 10 dataset.", "labels": [], "entities": [{"text": "SemEval-2010 task 10 dataset", "start_pos": 24, "end_pos": 52, "type": "DATASET", "confidence": 0.6819989234209061}]}, {"text": "Section 4 reviews a number of sources of evidence applied to the anaphora or coreference resolution tasks.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.9426843523979187}]}, {"text": "We also propose how to adapt these features to select the appropriate fillers for the implicit arguments.", "labels": [], "entities": []}, {"text": "Section 5 presents some experiments we have carried out to test these features.", "labels": [], "entities": []}, {"text": "Section 6 discusses the initial results.", "labels": [], "entities": []}, {"text": "Finally, section 7 offers some concluding remarks and presents some future researching.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments reported in this paper, we have used the dataset distributed in SemEval-2010 for Task 10 \"Linking Events and their Participants in Discourse\".", "labels": [], "entities": [{"text": "Linking Events and their Participants in Discourse", "start_pos": 109, "end_pos": 159, "type": "TASK", "confidence": 0.8011527146611895}]}, {"text": "The corpus contains some chapters extracted from two Arthur Conan Doyle's stories.", "labels": [], "entities": []}, {"text": "\"The Tiger of San Pedro\" chapter from \"The Adventure of Wisteria Lodge\" was selected for training, while chapters 13 and 14 from \"The Hound of the Baskervilles\" were selected for testing.", "labels": [], "entities": [{"text": "The Tiger of San Pedro\" chapter from \"The Adventure of Wisteria Lodge\"", "start_pos": 1, "end_pos": 71, "type": "DATASET", "confidence": 0.7305787046750386}, {"text": "The Hound of the Baskervilles\"", "start_pos": 130, "end_pos": 160, "type": "DATASET", "confidence": 0.6559752027193705}]}, {"text": "The texts are annotated using the frame-semantic structure of FrameNet 1.3 including null instantiations, the type of the NI and the corresponding fillers for each DNI.", "labels": [], "entities": [{"text": "FrameNet 1.3", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.8945290148258209}]}, {"text": "The dataset also includes the annotation files for the lexical units and the full-text annotated corpus from FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.9433618187904358}]}, {"text": "The annotations are enriched with a constituent-based parsing and for the training document there are manual coreference annotations available.", "labels": [], "entities": []}, {"text": "In the previous section we have proposed an adaptation to the implicit argument filling task of some theories traditionally applied to capture evidence for anaphora and coreference resolution.", "labels": [], "entities": [{"text": "argument filling", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.700883686542511}, {"text": "coreference resolution", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.9757253527641296}]}, {"text": "Since the implicit role reference is a special case of coreference, we expect a similar behaviour also for this case.", "labels": [], "entities": [{"text": "coreference", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9708195328712463}]}, {"text": "In fact, our analysis using the training data of SemEval seems to confirm our initial hypothesis.", "labels": [], "entities": []}, {"text": "In order to evaluate the potential utility of these sources of evidence we have performed a set of experiments using the SemEval-2010 Task 10 testing-data.", "labels": [], "entities": [{"text": "SemEval-2010 Task 10 testing-data", "start_pos": 121, "end_pos": 154, "type": "DATASET", "confidence": 0.5770809501409531}]}, {"text": "In this section, we describe our strategy for solving the implicit arguments and the scorer system used in the evaluation.", "labels": [], "entities": []}, {"text": "Processing Steps: Any system presented to the implicit argument resolution subtask had to follow the following three steps: 1.", "labels": [], "entities": [{"text": "implicit argument resolution subtask", "start_pos": 46, "end_pos": 82, "type": "TASK", "confidence": 0.7291005775332451}]}, {"text": "Select the frame elements that are Null Instantiations.", "labels": [], "entities": []}, {"text": "2. Decide if the null instantiations are Definite.", "labels": [], "entities": []}, {"text": "3. In case of definite null instantiation, locate the corresponding filler.", "labels": [], "entities": []}, {"text": "For the first two steps, we have followed the strategy proposed by.", "labels": [], "entities": []}, {"text": "This method learns patterns of concurrent Frame Elements from explicit annotations.", "labels": [], "entities": []}, {"text": "The most common patterns help to identify a missing FE when the rest of the FEs appears explicitly.", "labels": [], "entities": [{"text": "FE", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9591688513755798}, {"text": "FEs", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9561732411384583}]}, {"text": "Following this simple approach, 66% of DNIs in the testing data can be recognized correctly.", "labels": [], "entities": []}, {"text": "For the last step of the subtask, we have modelled the sources of evidence presented in the previous section as features to train a Naive-Bayes algorithm.", "labels": [], "entities": []}, {"text": "We applied a maximum-likelihood method without any smoothing function.", "labels": [], "entities": []}, {"text": "Thus, having a set of features f, for each DNI we select as filler the candidate c that satisfies: Non-singleton, focus and centering features require a coreference annotation of the document to be analysed.", "labels": [], "entities": []}, {"text": "As we explain in Section 3, the training data of the SemEval task contains manually annotated coreference chains that can be used to exploit these features.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.8991199731826782}]}, {"text": "However, as the testing data does not contain this type of annotations, we applied an automatic coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.853226363658905}]}, {"text": "We used the software provided by Stanford 3 . In the following experiments, we present the results obtained using manual and predicted coreference.", "labels": [], "entities": []}, {"text": "Score measures: The scorer provided for the NI SemEval subtask works slightly different than previous scorers for traditional SRL tasks.", "labels": [], "entities": [{"text": "NI SemEval subtask", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.7810375094413757}, {"text": "SRL tasks", "start_pos": 126, "end_pos": 135, "type": "TASK", "confidence": 0.9412744045257568}]}, {"text": "Since the participants can appear repeatedly along the document, the scorer needs to take into account the coreference chains of the possible fillers.", "labels": [], "entities": []}, {"text": "Thus, if a system selects any of the mentions of the correct filler, the scorer will count it as correct.", "labels": [], "entities": []}, {"text": "For this purpose, the dataset provides a full manual coreference annotation.", "labels": [], "entities": []}, {"text": "In this subtask, the NI linking precision is defined as the number of all true positive links divided by the number of links made by a system.", "labels": [], "entities": [{"text": "NI linking precision", "start_pos": 21, "end_pos": 41, "type": "METRIC", "confidence": 0.7169444759686788}]}, {"text": "NI linking recall is defined as the number of true positive links divided by the number of links between an NI and its equivalence set in the gold standard.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.7147780060768127}]}, {"text": "NI linking F-Score is then calculated as the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "NI linking F-Score", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.6312717795372009}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9996273517608643}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.99802565574646}]}, {"text": "However, since any prediction including the head of the correct filler is scored positively, selecting very large spans of text would obtain very good results 4 . For example, and [no goodwill, madam] would be evaluated as positive results fora gold-standard annotation.", "labels": [], "entities": []}, {"text": "Therefore, the scorer also computes the overlap (Dice coefficient) between the words in the predicted filler (P) of an NI and the words in the gold standard one (G): Results on the SemEval-2010 test: shows available precision, recall, F-score and overlapping figures of the different systems using predicted and gold-standard coreference chains . Our simple strategy clearly outperforms) in terms of both precision and recall.", "labels": [], "entities": [{"text": "Dice coefficient)", "start_pos": 49, "end_pos": 66, "type": "METRIC", "confidence": 0.9565609892209371}, {"text": "precision", "start_pos": 216, "end_pos": 225, "type": "METRIC", "confidence": 0.9991466999053955}, {"text": "recall", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.9972653388977051}, {"text": "F-score", "start_pos": 235, "end_pos": 242, "type": "METRIC", "confidence": 0.9944453835487366}, {"text": "precision", "start_pos": 405, "end_pos": 414, "type": "METRIC", "confidence": 0.9989981055259705}, {"text": "recall", "start_pos": 419, "end_pos": 425, "type": "METRIC", "confidence": 0.9978645443916321}]}, {"text": "() seems to solve more accurately but a more limited number of cases.", "labels": [], "entities": []}, {"text": "We also include the results from) obtained when using for training a larger corpus extended heuristically (best) and the results obtained with no additional training data (no extra train).", "labels": [], "entities": []}, {"text": "Our approach obtains better results in all the cases except when they use extended training data with the gold-standard coreference chains.", "labels": [], "entities": []}, {"text": "In this case, our approach seems to achieve a similar performance but without exploiting extra training data.", "labels": [], "entities": []}, {"text": "Apparently, (Laparra and Rigau, 2012) presents better results but, as we explained previously, a low overlapping score means vague answers.", "labels": [], "entities": []}, {"text": "Although our approach outperforms previous approaches, such a low figures clearly reflect the inherent difficulty of the task.", "labels": [], "entities": []}, {"text": "DNI linking experiment: In order to check the sources of evidence independently of the rest of processes, we have performed a second experiment where we assume perfect results for the first two steps.", "labels": [], "entities": [{"text": "DNI linking", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.398992195725441}]}, {"text": "In other words, we apply our DNI filling strategy just to the correct DNIs in the document.", "labels": [], "entities": [{"text": "DNI filling", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.885898232460022}]}, {"text": "shows the relevance of a correct DNI identification (the first two steps of the process).", "labels": [], "entities": [{"text": "DNI identification", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.8890340328216553}]}, {"text": "Once again, without extra training data our strategy outperforms the model of . Again, when using extended training data their model seems to perform similar to ours.", "labels": [], "entities": []}, {"text": "Ablation tests: presents the results using the gold-standard coreference, when leaving out a type of feature one at a time.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9584937691688538}]}, {"text": "The table empirically demonstrates that all feature types contribute positively to solve this task.", "labels": [], "entities": []}, {"text": "The morpho-syntactic and semantic agreement seem to be the most relevant evidence in terms of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9993159770965576}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.996681272983551}]}, {"text": "That is, identifying the head of the correct filler.", "labels": [], "entities": []}, {"text": "On the other hand, syntactic features are the most relevant to detect the correct span of the fillers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of DNI and Explicit FE annotations for the SemEval-10 Task-10 corpus.", "labels": [], "entities": [{"text": "FE", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.7796141505241394}, {"text": "SemEval-10 Task-10 corpus", "start_pos": 60, "end_pos": 85, "type": "DATASET", "confidence": 0.7311646540959676}]}, {"text": " Table 2: Some examples of semantic types assigned to frame elements.", "labels": [], "entities": []}, {"text": " Table 3: Dialogue vs. monologue distributions", "labels": [], "entities": []}, {"text": " Table 4: Results using SemEval-2010 dataset.", "labels": [], "entities": [{"text": "SemEval-2010 dataset", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.8456387221813202}]}, {"text": " Table 5: Results using SemEval-2010 dataset on the correct DNIs.", "labels": [], "entities": [{"text": "SemEval-2010 dataset", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.8163872361183167}]}, {"text": " Table 6: Ablation tests using the gold-standard coreference.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9451079368591309}]}, {"text": " Table 7: Performance of FE having more  than 5 semantic types", "labels": [], "entities": [{"text": "FE", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.47724229097366333}]}, {"text": " Table 9: Performance in mixed contexts  with at least 10% of entities of each level", "labels": [], "entities": []}]}