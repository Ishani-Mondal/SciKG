{"title": [{"text": "A Hybrid Chinese Spelling Correction Using Language Model and Statistical Machine Translation with Reranking", "labels": [], "entities": [{"text": "Chinese Spelling Correction", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.6473846832911173}, {"text": "Statistical Machine Translation", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.6817453503608704}]}], "abstractContent": [{"text": "We describe the Nara Institute of Science and Technology (NAIST) spelling check system in the shared task.", "labels": [], "entities": [{"text": "Nara Institute of Science and Technology (NAIST) spelling check", "start_pos": 16, "end_pos": 79, "type": "DATASET", "confidence": 0.8911777843128551}]}, {"text": "Our system contains three components: a word segmenta-tion based language model to generate correction candidates; a statistical machine translation model to provide correction candidates and a Support Vector Machine (SVM) classifier to rerank the candidates provided by the previous two components.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.6260717511177063}]}, {"text": "The experimental results show that the k-best language model and the statistical machine translation model could generate almost all the correction candidates, while the precision is very low.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.604969302813212}, {"text": "precision", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9995052814483643}]}, {"text": "However, using the SVM classifier to rerank the candidates , we could obtain higher precision with a little recall dropping.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9993539452552795}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9976502060890198}]}, {"text": "To address the low resource problem of the Chinese spelling check, we generate 2 million artificial training data by simply replacing the character in the provided training sentence with the character in the confusion set.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spelling check, which is an automatic mechanism to detect and correct human spelling errors in every written language, has been an active research area in the field of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Spelling check", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7983096539974213}, {"text": "Natural Language Processing (NLP)", "start_pos": 168, "end_pos": 201, "type": "TASK", "confidence": 0.6645427048206329}]}, {"text": "However, spelling check in Chinese is very different from that in English or other alphabetical languages.", "labels": [], "entities": [{"text": "spelling check", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.568372368812561}]}, {"text": "First because there are no word delimiters between the Chinese words; moreover, the average length of a word is very short: usually one to four characters.", "labels": [], "entities": []}, {"text": "Therefore, error detection is a hard problem since it must be done within a context, say a sentence or along phrase with a certain meaning, and cannot be done within one word.", "labels": [], "entities": [{"text": "error detection", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7217024266719818}]}, {"text": "For instance, in the words \"\"(selfcontrol) and \"\"(oneself), the character \"\" or \"\" cannot be detected as an error without the context.", "labels": [], "entities": []}, {"text": "Other challenge in the Chinese spelling check is that there is no commonly available data set for this task and the related resource is scarce.", "labels": [], "entities": [{"text": "Chinese spelling check", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.5367834270000458}]}, {"text": "The SIGHAN 2013 shared task is to provide a common evaluation data set to compare the error detection and correction rates between different systems.", "labels": [], "entities": [{"text": "SIGHAN 2013 shared", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.7224709192911783}, {"text": "error detection and correction", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.7662393152713776}]}, {"text": "The evaluation includes two sub-tasks: 1) error detection and 2) error correction.", "labels": [], "entities": [{"text": "error detection", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.8295882046222687}, {"text": "error correction", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7956556081771851}]}, {"text": "In this paper, we present a system that combines the correction candidates produced by the language model based method and the statistical machine translation approach, and then uses an SVM classifier to rerank the correction candidates.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 127, "end_pos": 158, "type": "TASK", "confidence": 0.6065797905127207}]}, {"text": "To address the low resource problem, firstly, we generate around 2 million artificial sentences following a simple rule, which replaces each character in the provided 700 sentences with the character in the confusion set to generate anew training corpurs; secondly, we use unlabeled data corpus, the Chinese Gigaword, to train a language model 1 to estimate the real Chinese texts.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first briefly discuss the related work in Section 2 and overview of our system structure in Section 3.", "labels": [], "entities": []}, {"text": "Subsections 3.1, 3.2 and 3.3 describe the components of our system respectively.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss the experiment setting and experimental results.", "labels": [], "entities": []}, {"text": "Finally, we give the conclusions in the final section.", "labels": [], "entities": []}], "datasetContent": [{"text": "For comparison, we combined the outputs of the translation model component and the language model component in three different ways: 1.", "labels": [], "entities": []}, {"text": "NAIST-Run1: Union of the output candidates of the language model and the statistical machine translation model, and then reranked by SVM.", "labels": [], "entities": [{"text": "NAIST-Run1", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9784862995147705}, {"text": "statistical machine translation", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.6078436076641083}]}, {"text": "2. NAIST-Run2: Intersection of the output candidates of the language model and the statistical machine translation model, and then reranked by SVM.: Final results on sub-task 2.", "labels": [], "entities": [{"text": "NAIST-Run2", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.9256820678710938}, {"text": "statistical machine translation", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.6183944543202718}]}, {"text": "LocAcc, CorAcc and CorPrec denote location accuracy, correction accuracy and correction precision respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8808630704879761}, {"text": "correction", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.977794885635376}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.5216599702835083}, {"text": "correction", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9825741648674011}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.5327635407447815}]}, {"text": "3. NAIST-Run3: Only use the output of the language model and then reranked by SVM.", "labels": [], "entities": [{"text": "NAIST-Run3", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.9450172185897827}]}, {"text": "Here, we assume that union of the candidates might get a higher recall (NAIST-Run1), while the intersection of the candidates might get a higher precision (NAIST-Run2).", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9989859461784363}, {"text": "NAIST-Run1", "start_pos": 72, "end_pos": 82, "type": "DATASET", "confidence": 0.8485585451126099}, {"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9948292374610901}, {"text": "NAIST-Run2", "start_pos": 156, "end_pos": 166, "type": "DATASET", "confidence": 0.9315746426582336}]}, {"text": "In the final test, there are two data sets.", "labels": [], "entities": []}, {"text": "Each task corpus contains 1000 sentences.", "labels": [], "entities": []}, {"text": "As shown in, NAIST-Run1 obtained the highest detection recall and NAIST-Run2 got the highest detection precision.", "labels": [], "entities": [{"text": "NAIST-Run1", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.8255029916763306}, {"text": "detection", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9632678031921387}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.6532458066940308}, {"text": "NAIST-Run2", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.8349547982215881}, {"text": "detection precision", "start_pos": 93, "end_pos": 112, "type": "METRIC", "confidence": 0.7912546098232269}]}, {"text": "However, NAISTRun3 obtained the highest error location recall, the highest detection F-score and the error location Fscore.", "labels": [], "entities": [{"text": "NAISTRun3", "start_pos": 9, "end_pos": 18, "type": "DATASET", "confidence": 0.8657795190811157}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.6673303246498108}, {"text": "detection", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9652662873268127}, {"text": "F-score", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.6620157361030579}, {"text": "error location Fscore", "start_pos": 101, "end_pos": 122, "type": "METRIC", "confidence": 0.9074047207832336}]}, {"text": "We think the main reason is that the rate of sentences with error characters is much lower, around 5%, while NAIST-Run1 tends to find more correction candidates.", "labels": [], "entities": [{"text": "NAIST-Run1", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.9419417977333069}]}, {"text": "The final results of the error correction sub task are shown in.", "labels": [], "entities": [{"text": "error correction sub task", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7973222061991692}]}, {"text": "As we expect in Section 4.2, NAIST-Run2 obtained the correction precision, while NAIST-Run1 obtained both the highest location accuracy and the highest correction accuracy.", "labels": [], "entities": [{"text": "NAIST-Run2", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.8943872451782227}, {"text": "correction", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9970777034759521}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.6040657162666321}, {"text": "NAIST-Run1", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.8886834383010864}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.8683507442474365}, {"text": "correction accuracy", "start_pos": 152, "end_pos": 171, "type": "METRIC", "confidence": 0.8578206598758698}]}, {"text": "To evaluate the importance of the SVM reranking, we do another set of experiments on the 700 sample sentences with 5-fold cross-validation.", "labels": [], "entities": [{"text": "SVM reranking", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7466995716094971}]}, {"text": "We could obtain 34.7% of the error location precision and 69.1% of the error location recall using the language model based approach.", "labels": [], "entities": [{"text": "error location precision", "start_pos": 29, "end_pos": 53, "type": "METRIC", "confidence": 0.6173457900683085}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.5155574083328247}]}, {"text": "After the reranking by the SVM, the error location precision increased to 70.2%, while the error location recall dropped to 67.0%.", "labels": [], "entities": [{"text": "SVM", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7621417045593262}, {"text": "error location precision", "start_pos": 36, "end_pos": 60, "type": "METRIC", "confidence": 0.8360153834025065}, {"text": "error location recall", "start_pos": 91, "end_pos": 112, "type": "METRIC", "confidence": 0.8531795740127563}]}, {"text": "From this observation, the SVM reranking plays a crucial role for detection and correction of Chinese spelling errors.", "labels": [], "entities": [{"text": "correction of Chinese spelling errors", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.7294824004173279}]}], "tableCaptions": [{"text": " Table 2: Final results on sub-task 2. LocAcc,  CorAcc and CorPrec denote location accuracy,  correction accuracy and correction precision re- spectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9233955144882202}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.604791522026062}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.5646346211433411}]}, {"text": " Table 1: Final results on sub-task 1. FAR denotes the false-alarm rate. DAcc, DPr, Dre and DF-score in- dicate detection accuracy, detection precision, detection recall and detection f-score respectively. ELAcc,  ELPr, ELRe and ELF-score denote error location accuracy, error location precision, error location recall  and error location f-score respectively.", "labels": [], "entities": [{"text": "FAR", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9989950060844421}, {"text": "DAcc", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9650455117225647}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.6420261263847351}, {"text": "detection precision", "start_pos": 132, "end_pos": 151, "type": "METRIC", "confidence": 0.7124238312244415}, {"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.5708191394805908}, {"text": "ELAcc", "start_pos": 206, "end_pos": 211, "type": "METRIC", "confidence": 0.8426297307014465}, {"text": "error location accuracy", "start_pos": 246, "end_pos": 269, "type": "METRIC", "confidence": 0.49209609627723694}, {"text": "error location precision", "start_pos": 271, "end_pos": 295, "type": "METRIC", "confidence": 0.5802748998006185}]}]}