{"title": [{"text": "The Benefits of a Model of Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a case study of a difficult and important categorical annotation task (word sense) to demonstrate a probabilistic annotation model applied to crowdsourced data.", "labels": [], "entities": []}, {"text": "It is argued that standard (chance-adjusted) agreement levels are neither necessary nor sufficient to ensure high quality gold standard labels.", "labels": [], "entities": []}, {"text": "Compared to conventional agreement measures, application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "The quality of annotated data for computational linguistics is generally assumed to be good enough if a few annotators can be shown to be consistent with one another.", "labels": [], "entities": []}, {"text": "Metrics such as pairwise agreement and agreement coefficients measure consistency among annotators.", "labels": [], "entities": []}, {"text": "These descriptive statistics do not support inferences about corpus quality or annotator accuracy, and the absolute values one should aim for are debatable, as in the review by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8292263746261597}]}, {"text": "We argue that high chance-adjusted inter-annotator agreement is neither necessary nor sufficient to ensure high quality gold-standard labels.", "labels": [], "entities": []}, {"text": "Agreement measures reveal little about differences among annotators, and nothing about the certainty of the true label, given the observed labels from annotators.", "labels": [], "entities": [{"text": "certainty", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9957799911499023}]}, {"text": "In contrast, a probabilistic model of annotation supports statistical inferences about the quality of the observed and inferred labels.", "labels": [], "entities": []}, {"text": "This paper presents a case study of a particularly thorny annotation task that is of widespread interest, namely word-sense annotation.", "labels": [], "entities": [{"text": "word-sense annotation", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7758563458919525}]}, {"text": "The items that were annotated are occurrences of selected words in their sentence contexts, and the annotation labels are WordNet senses.", "labels": [], "entities": []}, {"text": "The annotations, collected through crowdsourcing, consist of one WordNet sense for each item from up to twenty-five different annotators, giving each word instance a large set of labels.", "labels": [], "entities": []}, {"text": "Note that application of an annotation model does not require this many labels for each item, and crowdsourced annotation data does not require a probabilistic model.", "labels": [], "entities": []}, {"text": "This case study, however, does demonstrate a mutual benefit.", "labels": [], "entities": []}, {"text": "A highly certain ground truth label for each annotated instance is the ultimate goal of data annotation.", "labels": [], "entities": []}, {"text": "Many issues, however, make this complicated for word sense annotation.", "labels": [], "entities": [{"text": "word sense annotation", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7918979922930399}]}, {"text": "The number of different senses defined fora word varies across lexical resources, and pairs of senses within a single sense inventory are not equally distinct.", "labels": [], "entities": []}, {"text": "A previous annotation effort using WordNet sense labels demonstrates a great deal of variation across words ().", "labels": [], "entities": []}, {"text": "On over 116 words, chance-adjusted agreement ranged from very high to chance levels.", "labels": [], "entities": []}, {"text": "As a result, the ground truth labels for many words are questionable.", "labels": [], "entities": []}, {"text": "On a random subset of 45 of the same words, the crowdsourced data presented here (available as noted below) yields a certainty measure for each ground truth label indicating high certainty for most instances.", "labels": [], "entities": [{"text": "certainty", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9518215656280518}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Table of annotations y indexed by word  instance ii and annotator jj.", "labels": [], "entities": []}, {"text": " Table 2: Agreement results for MASC words with  the three highest and lowest \u03b1 scores, by part of  speech, along with additional words discussed in  the text (boldface).", "labels": [], "entities": [{"text": "MASC words", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.8116438090801239}]}, {"text": " Table 3: Proportion of high quality labels per word", "labels": [], "entities": []}]}