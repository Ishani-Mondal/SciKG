{"title": [{"text": "On the contribution of discourse structure to topic segmentation", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7197187691926956}]}], "abstractContent": [{"text": "In this paper, we describe novel methods for topic segmentation based on patterns of discourse organization.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7753390073776245}]}, {"text": "Using a corpus of news texts, our results show that it is possible to use discourse features (based on Rhetorical Structure Theory) for topic segmentation and that we outperform some well-known methods.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7432608902454376}]}], "introductionContent": [{"text": "Topic segmentation aims at finding the boundaries among topic blocks in a text.", "labels": [], "entities": [{"text": "Topic segmentation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8325085639953613}]}, {"text": "This task is useful fora number of important applications such as information retrieval (, automatic summarization ( and questionanswering systems.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.8467156291007996}, {"text": "summarization", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.6734623908996582}]}, {"text": "In this paper, following, we assume that a text or a set of texts develop a main topic, exposing several subtopics as well.", "labels": [], "entities": []}, {"text": "We also assume that a topic is a particular subject that we write about or discuss, and subtopics are represented in pieces of text that cover different aspects of the main topic).", "labels": [], "entities": []}, {"text": "Therefore, the task of topic segmentation aims at dividing a text into topically coherent segments, or subtopics.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7732648551464081}]}, {"text": "The granularity of a subtopic is not defined, as a subtopic may contain one or more sentences or paragraphs.", "labels": [], "entities": []}, {"text": "Several methods have been tested for topic segmentation.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8146324753761292}]}, {"text": "There are, however, no studies on how discourse structure directly mirrors topic boundaries in texts and how they may contribute to such task, although such possible correlation has been suggested (e.g.,.", "labels": [], "entities": []}, {"text": "In this paper, we follow this research line, aiming at exploring the relationship of discourse and subtopics.", "labels": [], "entities": []}, {"text": "In particular, our interest is mainly on the potential of Rhetorical Structure Theory (RST) for this task.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.8027218083540598}]}, {"text": "We propose and evaluate automatic topic segmentation strategies based on the rhetorical structure of a text.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7392051517963409}]}, {"text": "We also compare our results to some well-known algorithms in the area, showing that we outperform these algorithms.", "labels": [], "entities": []}, {"text": "Our experiments were performed using a corpus of news texts manually annotated with RST and subtopics.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief background on text segmentation.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.783629983663559}]}, {"text": "Section 3 describes our automatic strategies to find the subtopics.", "labels": [], "entities": []}, {"text": "The corpus that we use is described in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents some results and Section 6 contains the conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents comparisons of the results of the algorithms over the reference corpus.", "labels": [], "entities": []}, {"text": "The performance of topic segmentation is usually measured using Recall (R), Precision (P), and F-measure (F) scores.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7807766497135162}, {"text": "Recall (R)", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9597988724708557}, {"text": "Precision (P)", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.9501645714044571}, {"text": "F-measure (F) scores", "start_pos": 95, "end_pos": 115, "type": "METRIC", "confidence": 0.9518835544586182}]}, {"text": "These scores quantify how closely the system subtopics correspond to the ones produced by humans.", "labels": [], "entities": []}, {"text": "Those measures compare the boundary correspondences without considering whether these are close to each other: if they are not the same (regardless of wheth-er they are closer or farther from one another), they score zero.", "labels": [], "entities": []}, {"text": "However, it is also important to know how close the identified boundaries are to the expected ones, since this may help to determine how serious the errors made by the algorithms are.", "labels": [], "entities": []}, {"text": "We propose a simple measure to this, which we call Deviation (D) from the reference annotations.", "labels": [], "entities": [{"text": "Deviation (D)", "start_pos": 51, "end_pos": 64, "type": "METRIC", "confidence": 0.7885144501924515}]}, {"text": "Considering two algorithms that propose the same amount of boundaries fora text and make one single mistake each (having, therefore, the same P, R, and F scores), the best one will be the one that deviates the least from the reference.", "labels": [], "entities": [{"text": "F", "start_pos": 152, "end_pos": 153, "type": "METRIC", "confidence": 0.9505658745765686}]}, {"text": "The best algorithm should be the one with the best balance among P, R, F, and D scores.", "labels": [], "entities": [{"text": "F", "start_pos": 71, "end_pos": 72, "type": "METRIC", "confidence": 0.9345415234565735}]}, {"text": "The results achieved for the investigated methods are reported in.", "labels": [], "entities": []}, {"text": "The first 4 rows show the results for the baselines.", "labels": [], "entities": []}, {"text": "The algorithms based on RST are in the last 6 rows.", "labels": [], "entities": [{"text": "RST", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8889408707618713}]}, {"text": "The last row represents the human performance, which we refer by topline.", "labels": [], "entities": []}, {"text": "It is interesting to have a topline because it possibly indicates the limits that automatic methods may achieve in the task.", "labels": [], "entities": []}, {"text": "To find the topline, a human annotator of the corpus was randomly selected for each text and his annotation was compared with the reference one.", "labels": [], "entities": []}, {"text": "As expected, the paragraph baseline was very good, having the best F values of the baseline set.", "labels": [], "entities": [{"text": "F", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.9830815196037292}]}, {"text": "This shows that, inmost of the texts, the subtopics are organized in paragraphs.", "labels": [], "entities": []}, {"text": "Although the sentence baseline has the best R, it has the worst D.", "labels": [], "entities": [{"text": "R", "start_pos": 44, "end_pos": 45, "type": "METRIC", "confidence": 0.9521918296813965}]}, {"text": "This is due to the fact that not every sentence is a subtopic, and to segment all of them becomes a problem when we are looking for major groups of subtopics.", "labels": [], "entities": []}, {"text": "TextTiling is the algorithm that deviates the least from the reference segmentation.", "labels": [], "entities": [{"text": "TextTiling", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9087340831756592}]}, {"text": "This happens because it is very conservative and detects only a few segments, sometimes only one (the end of the text), causing it to have a good deviation score, but penalizing R.", "labels": [], "entities": [{"text": "R", "start_pos": 178, "end_pos": 179, "type": "METRIC", "confidence": 0.846372127532959}]}, {"text": "As expected, the Topline (the human, therefore) has the best F with acceptable D.", "labels": [], "entities": [{"text": "F", "start_pos": 61, "end_pos": 62, "type": "METRIC", "confidence": 0.9948655962944031}, {"text": "D", "start_pos": 79, "end_pos": 80, "type": "METRIC", "confidence": 0.9533035755157471}]}, {"text": "Its F value is probably the best that an automatic method may expect to achieve.", "labels": [], "entities": [{"text": "F", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9987040758132935}]}, {"text": "It is 25% better than our best method (Relation_Depth).", "labels": [], "entities": [{"text": "Relation_Depth", "start_pos": 39, "end_pos": 53, "type": "METRIC", "confidence": 0.9027985135714213}]}, {"text": "There is, therefore, room for improvements, possibly using other discourse features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The first 4 rows  show the results for the baselines. The algorithms  based on RST are in the last 6 rows. The last row  represents the human performance, which we  refer by topline. It is interesting to have a topline  because it possibly indicates the limits that au- tomatic methods may achieve in the task. To find  the topline, a human annotator of the corpus was  randomly selected for each text and his annota- tion was compared with the reference one.", "labels": [], "entities": []}, {"text": " Table 1. Evaluation of algorithms", "labels": [], "entities": []}]}