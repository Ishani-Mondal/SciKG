{"title": [], "abstractContent": [{"text": "Distributional Compositional Semantics (DCS) methods combine lexical vectors according to algebraic operators or functions to model the meaning of complex linguistic phrases.", "labels": [], "entities": [{"text": "Distributional Compositional Semantics (DCS)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8168907811244329}]}, {"text": "On the other hand, several textual inference tasks rely on supervised kernel-based learning, whereas Tree Kernels (TK) have been shown suitable to the modeling of syntactic and semantic similarity between linguistic instances.", "labels": [], "entities": []}, {"text": "While the modeling of DCS for complex phrases is still an open research issue, TKs do not account for compositionality.", "labels": [], "entities": []}, {"text": "In this paper , a novel kernel called Compositionally Smoothed Partial Tree Kernel is proposed integrating DCS operators into the TK estimation.", "labels": [], "entities": []}, {"text": "Empirical results over Semantic Text Similarity and Question Classification tasks show the contribution of semantic compositions with respect to traditional TKs.", "labels": [], "entities": [{"text": "Semantic Text Similarity", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.7628610928853353}, {"text": "Question Classification tasks", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.7949482599894205}]}], "introductionContent": [{"text": "Since the introduction of Landauer and Dumais in and Schutze in, Distributional Semantic Models (DMSs) have been an active area of research in computational linguistics and a promising technique for solving the lexical acquisition bottleneck by unsupervised learning.", "labels": [], "entities": []}, {"text": "However, it is very difficult to reconcile these techniques with existing theories of meaning in language, which revolve around logical and ontological representations.", "labels": [], "entities": []}, {"text": "According to logical theories (), sentences should be translated to a logical form that can be interpreted as a description of the state of the world.", "labels": [], "entities": []}, {"text": "On the contrary, vector-based techniques are closer to the philosophy of \"meaning as context\", relying on the \"meaning just is use\" and Firth's \"you shall know a word by the company it keeps\" and the distributional hypothesis of, that words will occur in similar contexts if and only if they have similar meanings.", "labels": [], "entities": []}, {"text": "In these years attention has been focused on the question of how to combine word representations in order to characterize a model for sentence semantics.", "labels": [], "entities": []}, {"text": "Since these models are typically directed at the representation of isolated words, a well formed theory on how to combine vectors and to represent complex phrases still represents a research topic.", "labels": [], "entities": []}, {"text": "Distributional Compositional Semantic (DCS) models capture bi-gram semantics, but they are not sensitive to the syntactic structure yet.", "labels": [], "entities": [{"text": "Distributional Compositional Semantic (DCS)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7808028012514114}]}, {"text": "On the other hand, Convolution Kernels) are well-known similarity functions among such complex structures.", "labels": [], "entities": []}, {"text": "In particular, Tree Kernels (TKs) introduced in), are largely used in NLP for their ability in capturing text grammatical information, directly from syntactic parse trees.", "labels": [], "entities": []}, {"text": "cal semantic information, it is possible to formulate anew kernel function based on this tree representation, that takes into account for each node a distributional compositional metrics.", "labels": [], "entities": []}, {"text": "Thus, the idea is to i) use the SPTK formulation in order to exploit the lexical information of the leaves, ii) define a procedure to mark nodes of a constituency parse tree that allow to spread lexical bigrams across the non-terminal nodes, iii) apply smoothing metrics sensible to the compositionality between the non-terminal labels.", "labels": [], "entities": []}, {"text": "The resulting model has been called Compositionally Smoothed Partial Tree Kernel (CSPTK).", "labels": [], "entities": []}, {"text": "In Section 2, a summary of approaches for DCS and TKs is presented.", "labels": [], "entities": [{"text": "DCS", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9550907611846924}]}, {"text": "The entire process of marking parse trees is described in Section 3.", "labels": [], "entities": [{"text": "marking parse trees", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7691860993703207}]}, {"text": "Therefore in Section 4 the CSPTK similarity function is presented.", "labels": [], "entities": [{"text": "CSPTK", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.4065312147140503}, {"text": "similarity", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.5663721561431885}]}, {"text": "Finally, in Section 5, the CSPTK model is investigated in Semantic Text Similarity (STS) and Question Classification tasks.", "labels": [], "entities": [{"text": "Semantic Text Similarity (STS)", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.787399580081304}, {"text": "Question Classification tasks", "start_pos": 93, "end_pos": 122, "type": "TASK", "confidence": 0.8586175044377645}]}], "datasetContent": [{"text": "In this section the CSPTK model is used in Semantic Textual Similarity (STS) and Question Classification (QC) tasks.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.7187876552343369}, {"text": "Question Classification (QC) tasks", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.8485238254070282}]}, {"text": "The aim of this section is to measure the CSPTK capability to account for the similarity between sentences and as a feature to train machine learning classifiers.", "labels": [], "entities": []}, {"text": "In all experiments, sentences are processed with the Stanford CoreNLP 2 , for Part-of-speech tagging, lemmatization, and dependency and compositionally enriched parsing.", "labels": [], "entities": [{"text": "Stanford CoreNLP 2", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.9255348245302836}, {"text": "Part-of-speech tagging", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7126858830451965}]}, {"text": "In order to reduce data sparseness introduce by fined grained Part-ofSpeech classes, node are marked by coarse grained classes, e.g. looking:VBG or looked:VBD are simplified in look:V.", "labels": [], "entities": []}, {"text": "In order to estimate the basic lexical similarity function employed in the Tree Kernels operators, a co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus ().", "labels": [], "entities": [{"text": "UkWaC corpus", "start_pos": 181, "end_pos": 193, "type": "DATASET", "confidence": 0.9670306146144867}]}, {"text": "First, all words occurring more than 100 times (i.e. the targets) are represented through vectors.", "labels": [], "entities": []}, {"text": "The original space dimensions are generated from the set of the 20,000 most frequent words (i.e. features) in the UkWaC corpus.", "labels": [], "entities": [{"text": "UkWaC corpus", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.961512416601181}]}, {"text": "A co-occurrence WordSpace with a window of size 3 is acquired.", "labels": [], "entities": [{"text": "WordSpace", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.9310697317123413}]}, {"text": "Cooccurrencies are weighted by estimating the Pointwise Mutual Information between the 20k most frequent words.", "labels": [], "entities": []}, {"text": "The SVD reduction is then applied with a dimensionality cut of d = 250.", "labels": [], "entities": [{"text": "SVD reduction", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7178664803504944}]}, {"text": "Left contexts are treated differently from the right ones, in order to capture asymmetric syntactic behaviors (e.g., useful for verbs): 40,000 dimensional vectors are thus derived for each target.", "labels": [], "entities": []}, {"text": "Similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence Word Space, as in (Croce et al., 2011).", "labels": [], "entities": [{"text": "Word Space", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.8822304010391235}]}, {"text": "PTKct  First and second columns in show Pearson results of PTK ct and SPTK ct functions applied over a constituency tree, while the last column shows the CSPTK cct results over the compositionally labeled tree.", "labels": [], "entities": [{"text": "PTKct", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9295963644981384}, {"text": "Pearson", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9757133722305298}]}, {"text": "Notice how the introduction of the compositionality enrichment in a constituency tree structure, together with the CSPTK function led to a performance boost overall the training and test datasets.", "labels": [], "entities": []}, {"text": "In some cases, the boost between SPTK ct and CSPTK cct is remarkable, switching from .18 to .65 in MSRvid and from .24 to .37 in OnWN.", "labels": [], "entities": [{"text": "CSPTK cct", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8633996546268463}, {"text": "MSRvid", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.9651666879653931}, {"text": "OnWN", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.9862998127937317}]}, {"text": "The above difference is mainly due to the increasing sensitivity of PTK, SPTK and CSPTK to the incrementally rich lexical information.", "labels": [], "entities": []}, {"text": "This is especially evident in sentence pairs with very similar syntactic structure.", "labels": [], "entities": []}, {"text": "For example in the MSRvid dataset, a sentence pair is given by The man are playing soccer and A man is riding a motorcycle, that are strictly syntactically correlated.", "labels": [], "entities": [{"text": "MSRvid dataset", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9746382832527161}]}, {"text": "As aside effect, PTK provides a similarity score of .647 between the two sentences.", "labels": [], "entities": [{"text": "PTK", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.8644604086875916}, {"text": "similarity score", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.9799512624740601}]}, {"text": "It is a higher score with respect to the SPTK and CSPTK: differences between tree structures are confined only to the leaves.", "labels": [], "entities": [{"text": "CSPTK", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8555164337158203}]}, {"text": "By scoring .461, SPTK introduces an improvement as the distributional similarity (function \u03c3 in Eq.", "labels": [], "entities": [{"text": "SPTK", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.7097993493080139}, {"text": "distributional similarity", "start_pos": 55, "end_pos": 80, "type": "METRIC", "confidence": 0.7060813903808594}]}, {"text": "4) that acts as a smoothing factor between leaves better discriminates uncorrelated words, like motorcycle and soccer.", "labels": [], "entities": []}, {"text": "However, ambiguous words such verbs ride and play are still promoting a similarity that is locally misleading.", "labels": [], "entities": []}, {"text": "Notice that both PTK and SPTK receive a strong contribution in the recursive computation of the kernels by the left branching of the tree, as the subject is the same, i.e. man.", "labels": [], "entities": [{"text": "PTK", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.7890739440917969}]}, {"text": "Compositional information about direct objects (soccer vs. motorcycle) is better propagated by the CSPTK operator.", "labels": [], "entities": [{"text": "Compositional information about direct objects (soccer vs. motorcycle)", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.7983161211013794}]}, {"text": "Its final scores for the pair is .36, as semantic differences between the sentences are emphasized.", "labels": [], "entities": []}, {"text": "Even if grammatical types strongly contribute to the final score (as in PTK or SPTK), now the DCS computation over these nodes (the compounds traced from the leaves, i.e. (ride::v, motorcycle::n) and (play::v, soccer::n) is faced with less ambiguous verb phrases, that contribute with lower scores.", "labels": [], "entities": [{"text": "PTK", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8561059236526489}, {"text": "DCS computation", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.8184375166893005}]}], "tableCaptions": [{"text": " Table 1: Unsupervised results of Pearson correlation for", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.7761785089969635}]}, {"text": " Table 2: Results in the Question Classification task", "labels": [], "entities": [{"text": "Question Classification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8243730962276459}]}]}