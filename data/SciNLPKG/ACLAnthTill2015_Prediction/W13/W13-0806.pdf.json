{"title": [{"text": "Combining Top-down and Bottom-up Search for Unsupervised Induction of Transduction Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "We show that combining both bottom-up rule chunking and top-down rule segmentation search strategies in purely unsupervised learning of phrasal inversion transduction grammars yields significantly better translation accuracy than either strategy alone.", "labels": [], "entities": [{"text": "rule segmentation search", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.8020932078361511}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.8699789047241211}]}, {"text": "Previous approaches have relied on incrementally building larger rules by chunking smaller rules bottom-up; we introduce a complementary top-down model that incrementally builds shorter rules by segmenting larger rules.", "labels": [], "entities": []}, {"text": "Specifically, we combine iteratively chunked rules from Saers et al.", "labels": [], "entities": []}, {"text": "(2012) with our new iteratively segmented rules.", "labels": [], "entities": []}, {"text": "These integrate seamlessly because both stay strictly within a pure trans-duction grammar framework inducing under matching models during both training and testing-instead of decoding under a completely different model architecture than what is assumed during the training phases, which violates an elementary principle of machine learning and statistics.", "labels": [], "entities": []}, {"text": "To be able to drive induction top-down, we introduce a minimum description length objective that trades off maximum likelihood against model size.", "labels": [], "entities": [{"text": "minimum description length objective", "start_pos": 55, "end_pos": 91, "type": "METRIC", "confidence": 0.7073302417993546}]}, {"text": "We show empirically that combining the more liberal rule chunking model with a more conservative rule segmentation model results in significantly better translations than either strategy in isolation.", "labels": [], "entities": [{"text": "rule chunking", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.7188259214162827}]}], "introductionContent": [{"text": "In this paper we combine both bottom-up chunking and top-down segmentation as search directions in the unsupervised pursuit of an inversion transduction grammar (ITG); we also show that the combination of the resulting grammars is superior to either of them in isolation.", "labels": [], "entities": [{"text": "inversion transduction grammar (ITG)", "start_pos": 130, "end_pos": 166, "type": "TASK", "confidence": 0.686535527308782}]}, {"text": "For the bottom-up chunking approach we use the method reported in, and for the top-down segmentation approach, we introduce a minimum description length (MDL) learning objective.", "labels": [], "entities": [{"text": "minimum description length (MDL) learning objective", "start_pos": 126, "end_pos": 177, "type": "METRIC", "confidence": 0.7600869685411453}]}, {"text": "The new learning objective is similar to the Bayesian maximum a posteriori objective, and makes it possible to learn topdown, which is impossible using maximum likelihood, as the initial grammar that rewrites the start symbol to all sentence pairs in the training data already maximizes the likelihood of the training data.", "labels": [], "entities": []}, {"text": "Since both approaches result in stochastic ITGs, they can be easily combined into a single stochastic ITG which allows for seamless combination.", "labels": [], "entities": []}, {"text": "The point of our present work is that the two different search strategies result in very different grammars so that the combination of them is superior in terms of translation accuracy to either of them in isolation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.8654971718788147}]}, {"text": "The transduction grammar approach has the advantage that induction, tuning and testing are optimized on the exact same underlying model-this used to be a given in machine learning and statistical prediction, but has been largely ignored in the statistical machine translation (SMT) community, where most current SMT approaches to learning phrase translations that (a) require enormous amounts of run-time memory, and (b) contain a high degree of redundancy.", "labels": [], "entities": [{"text": "statistical prediction", "start_pos": 184, "end_pos": 206, "type": "TASK", "confidence": 0.8057075440883636}, {"text": "statistical machine translation (SMT)", "start_pos": 244, "end_pos": 281, "type": "TASK", "confidence": 0.7784537126620611}, {"text": "SMT", "start_pos": 312, "end_pos": 315, "type": "TASK", "confidence": 0.9814667105674744}]}, {"text": "In particular, phrase-based SMT models such as and often search for candidate translation segments and transduction rules by committing to a word alignment that is completely alien to the grammar, as it is learned with very different models,), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8222258687019348}]}, {"text": "The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations illustrates the above points well.", "labels": [], "entities": [{"text": "phrase-based direct translation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6287824710210165}]}, {"text": "Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints.", "labels": [], "entities": [{"text": "Transduction grammars", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.923652321100235}]}, {"text": "This approach was pioneered by, and there has been a lot of research since, usually referred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data.", "labels": [], "entities": []}, {"text": "This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese.", "labels": [], "entities": []}, {"text": "It does, however, constitute away to borrow nonterminal categories that help the translation model.", "labels": [], "entities": []}, {"text": "It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example,,, ,,,,,,).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7020750939846039}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8436979651451111}]}, {"text": "Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9854157567024231}]}, {"text": "A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 2, "end_pos": 16, "type": "TASK", "confidence": 0.691949263215065}]}, {"text": "The minimum description length (MDL) objective that we will be using to drive the learning will provide away to escape the maximum-likelihood-ofthe-data-given-the-model optimum that we start outwith.", "labels": [], "entities": [{"text": "minimum description length (MDL) objective", "start_pos": 4, "end_pos": 46, "type": "METRIC", "confidence": 0.7995341675622123}]}, {"text": "However, going only by MDL will also lead to a degenerate case, where the size of the grammar is allowed to shrink regardless of how unlikely the corpus becomes.", "labels": [], "entities": []}, {"text": "Instead, we will balance the length of the grammar with the probability of the corpus given the grammar.", "labels": [], "entities": []}, {"text": "This has a natural Bayesian interpretation where the length of the grammar acts as a prior over the structure of the grammar.", "labels": [], "entities": []}, {"text": "Similar approaches have been used before, but to induce monolingual grammars.", "labels": [], "entities": []}, {"text": "use a method similar to MDL called Bayesian model merging to learn the structure of hidden Markov models as well as stochastic contextfree grammars.", "labels": [], "entities": [{"text": "Bayesian model merging", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.6708743770917257}]}, {"text": "The SCFGs are induced by allowing sequences of nonterminals to be replaced with a single nonterminal (chunking) as well as allowing two nonterminals to merge into one.", "labels": [], "entities": []}, {"text": "uses it to learn nonterminal categories in a contextfree grammar.", "labels": [], "entities": []}, {"text": "It has also been used to interpret visual scenes by classifying the activity that goes on in a video sequences).", "labels": [], "entities": [{"text": "interpret visual scenes", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8616337180137634}]}, {"text": "Our work in this paper is markedly different to even the previous NLP work in that (a) we induce an inversion transduction grammar (Wu, 1997) rather than a monolingual grammar, and (b) we focus on learning the terminal segments rather than the nonterminal categories.", "labels": [], "entities": []}, {"text": "The idea of iteratively segmenting the existing sentence pairs to find good phrasal translations has also been tried before; introduces the Recursive Alignment Model, which recursively determines whether a bispan is a good enough translation on its own (using IBM model 1), or if it should be split into two bispans (either in straight or inverted order).", "labels": [], "entities": []}, {"text": "The model uses length of the input sentence to determine whether to split or not, and uses very limited local information about the split point to determine whereto split.", "labels": [], "entities": []}, {"text": "Training the parameters is done with a maximum likelihood objective.", "labels": [], "entities": []}, {"text": "In contrast, our model is one single generative model (as opposed to an ad hoc model), trained with a minimum description length objective (rather than trying to maximize the probability of the train-ing data).", "labels": [], "entities": []}, {"text": "The rest of the paper is structured so that we first take a closer look at the minimum description length principle that will be used to drive the top-down search (Section 2).", "labels": [], "entities": []}, {"text": "We then show how the top-down grammar is learned (Sections 3 and 4), before showing how we combine the new grammar with that of (Section 5).", "labels": [], "entities": []}, {"text": "We then detail the experimental setup that will substantiate our claims empirically (Section 6) before interpreting the results of those experiments (Section 7).", "labels": [], "entities": []}, {"text": "Finally, we offer some conclusions (Section 8).", "labels": [], "entities": []}], "datasetContent": [{"text": "We have made the claim that iterative top-down segmentation guided by the objective of minimizing the description length gives a better precision grammar than iterative bottom-up chunking, and that the combination of the two gives superior results to either approach in isolation.", "labels": [], "entities": [{"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9956871867179871}]}, {"text": "We have outlined how this can be done in practice, and we now substantiate that claim empirically.", "labels": [], "entities": []}, {"text": "We will initialize a stochastic bracketing inversion transduction grammar (BITG) to rewrite it's one nonterminal symbol directly into all the sentence pairs of the training data (iteration 0).", "labels": [], "entities": [{"text": "stochastic bracketing inversion transduction grammar (BITG)", "start_pos": 21, "end_pos": 80, "type": "TASK", "confidence": 0.6260981112718582}]}, {"text": "We will then segment the grammar iteratively a total of seven times (iterations 1-7).", "labels": [], "entities": []}, {"text": "For each iteration we will record the change in description length and test the grammar.", "labels": [], "entities": []}, {"text": "Each iteration requires us to biparse the training data, which we do with the cubic time algorithm described in , with abeam width of 100.", "labels": [], "entities": []}, {"text": "As training data, we use the IWSLT07 ChineseEnglish data set, which contains 46,867 sentence pairs of training data, 506 Chinese sentences of development data with 16 English reference translations, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain.", "labels": [], "entities": [{"text": "IWSLT07 ChineseEnglish data set", "start_pos": 29, "end_pos": 60, "type": "DATASET", "confidence": 0.9173119813203812}]}, {"text": "Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more \"word like\" sequences.", "labels": [], "entities": []}, {"text": "As the bottom-up grammar, we will reuse the grammar learned in, specifically, we will use the BITG that was bootstrapped from a bracketing finite-state transduction grammar (BF-STG) that has been chunked twice, giving biterminals where the monolingual segments are 0-4 tokens long.", "labels": [], "entities": [{"text": "BITG", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9552770853042603}]}, {"text": "The bottom-up grammar is trained on the same data as our model.", "labels": [], "entities": []}, {"text": "To test the learned grammars as translation models, we first tune the grammar parameters to the training data using expectation maximization and parse forests acquired with the above mentioned biparser, again with abeam width of 100.", "labels": [], "entities": []}, {"text": "To do the actual decoding, we use our in-house ITG decoder.", "labels": [], "entities": [{"text": "ITG decoder", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9432283341884613}]}, {"text": "The decoder uses a CKYstyle parsing algorithm and cube pruning to integrate the language model scores.", "labels": [], "entities": [{"text": "CKYstyle parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.8349417746067047}]}, {"text": "The decoder builds an efficient hypergraph structure which is then scored using both the induced grammar and the language model.", "labels": [], "entities": []}, {"text": "The weights for the language model and the grammar, are tuned towards BLEU) using MERT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9990226030349731}, {"text": "MERT", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.8333550095558167}]}, {"text": "We use the ZMERT (Zaidan, 2009) implementation of MERT as it is a robust and flexible implementation of MERT, while being loosely coupled with the decoder.", "labels": [], "entities": [{"text": "ZMERT (Zaidan, 2009)", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.7518295149008433}]}, {"text": "We use SRILM) for training a trigram language model on the English side of the training data.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.7122331261634827}]}, {"text": "To evaluate the quality of the resulting translations, we use BLEU, and NIST).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.998281717300415}, {"text": "NIST", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.7407090663909912}]}, {"text": "The results from running the experiments detailed in the previous section can be summarized in four graphs.", "labels": [], "entities": []}, {"text": "show the size of our new, segmenting model during induction, in terms of description length and in terms of rule count.", "labels": [], "entities": [{"text": "induction", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.9527169466018677}]}, {"text": "The initial ITG is at iteration 0, where the vast majority of the size is taken up by the model (DL (G)), and very little by the data (DL (C|G))-just as we predicted.", "labels": [], "entities": []}, {"text": "The trend over the induction phase is a sharp decrease in model size, and a moderate increase in data size, with the overall size constantly decreasing.", "labels": [], "entities": []}, {"text": "Note that, although the number of rules rises, the total description length decreases.", "labels": [], "entities": []}, {"text": "Again, this is precisely what we expected.", "labels": [], "entities": []}, {"text": "The size of the model learned according to is close to 30 Mbits-far off the chart.", "labels": [], "entities": []}, {"text": "This shows that our new top-down approach is indeed learning a more parsimonious grammar than the bottom-up approach.", "labels": [], "entities": []}, {"text": "shows the translation quality of the learned model.", "labels": [], "entities": [{"text": "translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9524353742599487}]}, {"text": "The thin flat lines show the quality of the bottom-up approach, whereas the thick curves shows the quality of the new, top-down model presented in this paper without (dotted line), and without the bottom-up model (solid line).", "labels": [], "entities": []}, {"text": "Although the MDL-based model is better than the old model, the combination of the two is still superior.", "labels": [], "entities": []}, {"text": "It is particularly encouraging to see that the over-fitting that seems to take place after iteration 3 with the MDL-based approach is ameliorated with the bottom-up model.", "labels": [], "entities": []}], "tableCaptions": []}