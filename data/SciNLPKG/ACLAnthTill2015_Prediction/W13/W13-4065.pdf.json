{"title": [], "abstractContent": [{"text": "Ina spoken dialog system, dialog state tracking deduces information about the user's goal as the dialog progresses, synthesizing evidence such as dialog acts over multiple turns with external data sources.", "labels": [], "entities": [{"text": "dialog state tracking deduces information", "start_pos": 26, "end_pos": 67, "type": "TASK", "confidence": 0.7554352283477783}]}, {"text": "Recent approaches have been shown to overcome ASR and SLU errors in some applications.", "labels": [], "entities": [{"text": "ASR", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9870887398719788}]}, {"text": "However, there are currently no common testbeds or evaluation measures for this task, hampering progress.", "labels": [], "entities": []}, {"text": "The dialog state tracking challenge seeks to address this by providing a heterogeneous corpus of 15K human-computer dialogs in a standard format, along with a suite of 11 evaluation metrics.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8425038456916809}]}, {"text": "The challenge received a total of 27 entries from 9 research groups.", "labels": [], "entities": []}, {"text": "The results show that the suite of performance metrics cluster into 4 natural groups.", "labels": [], "entities": []}, {"text": "Moreover, the dialog systems that benefit most from dialog state tracking are those with less discriminative speech recognition confidence scores.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.6906226277351379}]}, {"text": "Finally , generalization is a key problem: in 2 of the 4 test sets, fewer than half of the entries out-performed simple baselines.", "labels": [], "entities": [{"text": "generalization", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9677807092666626}]}], "introductionContent": [], "datasetContent": [{"text": "The output of a dialog state tracker is a probability distribution over a set of given dialog state hypotheses, plus the REST meta-hypothesis.", "labels": [], "entities": [{"text": "dialog state tracker", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7485266327857971}]}, {"text": "To evaluate this output, a label is needed for each dialog state hypothesis indicating its correctness.", "labels": [], "entities": []}, {"text": "In this task-oriented domain, we note that the user enters the call with a specific goal in mind.", "labels": [], "entities": []}, {"text": "Further, when goal changes do occur, they are usually explicitly marked: since all of the systems first collect slot values, and then provide bus timetables, if the user wishes to change their goal, they need to start over from the beginning.", "labels": [], "entities": []}, {"text": "These \"start over\" transitions are obvious in the logs.", "labels": [], "entities": []}, {"text": "This structure allows the correctness of each dialog state to be equated to the correctness of the SLU items it contains.", "labels": [], "entities": []}, {"text": "As a result, in the DSTC we labeled the correctness of SLU hypotheses in each turn, and then assumed these labels remain valid until either the call ends, or until a \"start over\" event.", "labels": [], "entities": [{"text": "DSTC", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.902411699295044}]}, {"text": "Thus to produce the labels, the labeling task followed was to assign a correctness value to every SLU hypothesis on the N-best list, given a transcript of the words actually spoken in the dialog up to the current turn.", "labels": [], "entities": []}, {"text": "To accomplish this, first all user speech was transcribed.", "labels": [], "entities": []}, {"text": "The TRAIN1 datasets had been transcribed using crowd-sourcing in a prior project ; the remainder were transcribed by professionals.", "labels": [], "entities": [{"text": "TRAIN1 datasets", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8505918085575104}]}, {"text": "Then each SLU hypothesis was labled as corrector incorrect.", "labels": [], "entities": []}, {"text": "When a transcription exactly and unambiguously matched a recognized slot value, such as the bus route \"sixty one c\", labels were assigned automatically.", "labels": [], "entities": []}, {"text": "The remainder were assigned using crowdsourcing, where three workers were shown the true words spoken and the recognized concept, and asked to indicate if the recognized concept was correct -even if it did not match the recognized words exactly.", "labels": [], "entities": []}, {"text": "Workers were also shown dialog history, which helps decipher the user's meaning when their speech was ambiguous.", "labels": [], "entities": []}, {"text": "If the 3 workers were not unanimous in their labels (about 4% of all turns), the item was labeled manually by the organizers.", "labels": [], "entities": []}, {"text": "The REST meta-hypothesis was not explicitly labeled; rather, it was deemed to be correct if none of the prior SLU results were labeled as correct.", "labels": [], "entities": [{"text": "REST", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.7983912229537964}]}, {"text": "In this challenge, state tracking performance was measured on each of the 9 slots separately, and also on a joint dialog state consisting of all the slots.", "labels": [], "entities": []}, {"text": "So at each turn in the dialog, a tracker output 10 scored lists: one for each slot, plus a 10th list where each dialog state contains values from all slots.", "labels": [], "entities": []}, {"text": "Scores were constrained to be in the range and to sum to 1.", "labels": [], "entities": []}, {"text": "To evaluate tracker output, at each turn, each hypothesis (including REST) on each of the 10 lists was labeled as corrector incorrect by looking up its corresponding SLU label(s).", "labels": [], "entities": [{"text": "REST", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9384605288505554}]}, {"text": "The scores and labels overall of the dialogs were then compiled to compute 11 metrics.", "labels": [], "entities": []}, {"text": "Accuracy measures the percent of turns where the top-ranked hypothesis is correct.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9963979721069336}]}, {"text": "This indicates the correctness of the item with the maximum score.", "labels": [], "entities": []}, {"text": "L2 measures the L 2 distance between the vector of scores, and a vector of zeros with 1 in the position of the correct hypothesis.", "labels": [], "entities": [{"text": "L 2 distance", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.7789610425631205}]}, {"text": "This indicates the quality of all scores, when the scores as viewed as probabilities.", "labels": [], "entities": []}, {"text": "AvgP measures the mean score of the first correct hypothesis.", "labels": [], "entities": [{"text": "AvgP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9914143681526184}]}, {"text": "This indicates the quality of the score assigned to the correct hypothesis, ignoring the distribution of scores to incorrect hypotheses.", "labels": [], "entities": []}, {"text": "MRR measures the mean reciprocal rank of the first correct hypothesis.", "labels": [], "entities": [{"text": "mean reciprocal rank", "start_pos": 17, "end_pos": 37, "type": "METRIC", "confidence": 0.733265240987142}]}, {"text": "This indicates the quality of the ordering the scores produces (without necessarily treating the scores as probabilities).", "labels": [], "entities": []}, {"text": "The remaining measures relate to receiveroperating characteristic (ROC) curves, which measure the discrimination of the score for the highest-ranked state hypothesis.", "labels": [], "entities": [{"text": "receiveroperating characteristic (ROC)", "start_pos": 33, "end_pos": 71, "type": "METRIC", "confidence": 0.7929847002029419}]}, {"text": "Two versions of ROC are computed -V1 and V2.", "labels": [], "entities": [{"text": "ROC", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.480959951877594}]}, {"text": "V1 computes correct-accepts (CA), false-accepts (FA), and false-rejects (FR) as fractions of all utterances, so for example useful indication of overall performance because they combine discrimination and overall accuracy -i.e., the maximum CA.V 1(s) value is equal to accuracy computed above.", "labels": [], "entities": [{"text": "false-accepts (FA)", "start_pos": 34, "end_pos": 52, "type": "METRIC", "confidence": 0.6723358333110809}, {"text": "false-rejects (FR)", "start_pos": 58, "end_pos": 76, "type": "METRIC", "confidence": 0.6316207647323608}, {"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9964994192123413}, {"text": "CA.V 1(s)", "start_pos": 241, "end_pos": 250, "type": "METRIC", "confidence": 0.9264932751655579}, {"text": "accuracy", "start_pos": 269, "end_pos": 277, "type": "METRIC", "confidence": 0.9982588887214661}]}, {"text": "V2 considers fractions of correctly classified utterances, so for example The V2 metrics are useful because they measure the discrimination of the scoring independently of accuracy -i.e., the maximum value of CA.V 2(s) is always 1, regardless of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9984502792358398}, {"text": "CA.V 2(s)", "start_pos": 209, "end_pos": 218, "type": "METRIC", "confidence": 0.9526331067085266}, {"text": "accuracy", "start_pos": 246, "end_pos": 254, "type": "METRIC", "confidence": 0.9958463311195374}]}, {"text": "From these ROC statistics, several metrics are computed.", "labels": [], "entities": [{"text": "ROC", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8734050989151001}]}, {"text": "ROC.V1.EER computes F A.V 1(s) where F A.V 1(s) = F R.V 1(s).", "labels": [], "entities": []}, {"text": "The metrics ROC.V1.CA05, ROC.V1.CA10, and ROC.V1.CA20 compute CA.V 1(s) when F A.V 1(s) = 0.05, 0.10, and 0.20 respectively.", "labels": [], "entities": [{"text": "ROC.V1.CA05", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.8425554633140564}]}, {"text": "ROC.V2.CA05, ROC.V2.CA10, and ROC.V2.CA20 do the same using the V2 versions.", "labels": [], "entities": []}, {"text": "Apart from what to measure, there is currently no standard that specifies when to measure -i.e., which turns to include when computing each metric.", "labels": [], "entities": []}, {"text": "So for this challenge, a set of 3 schedules were used.", "labels": [], "entities": []}, {"text": "schedule2 include turns where the target slot is either present on the SLU N-best list, or where the target slot is included in a system confirmation action -i.e., where there is some observable new information about the target slot.", "labels": [], "entities": [{"text": "SLU N-best list", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.6802136500676473}]}, {"text": "schedule3 includes only the last turn of a dialog.", "labels": [], "entities": [{"text": "schedule3", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9089610576629639}]}, {"text": "In sum, for each tracker, one measurement is reported for each test set (4), schedule (3), and metric (11) for each of the 9 slots, the \"joint\" slot, and a weighted average of the individual slots (11), fora total of 4 \u00b7 3 \u00b7 11 \u00b7 11 = 1452 measurements per tracker.", "labels": [], "entities": []}, {"text": "In addition, each tracker reported average latency per turn -this ranged from 10ms to 1s.", "labels": [], "entities": [{"text": "latency", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9684303998947144}]}, {"text": "We begin by looking atone illustrative metric, schedule2 accuracy averaged over slots, which measures the accuracy of the top dialog hypothesis for every slot when it either appears on the SLU N-best list or is confirmed by the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8522132635116577}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9977436065673828}]}, {"text": "1 Results in show two key trends.", "labels": [], "entities": []}, {"text": "First, relative to the baselines, performance on the test data is markedly lower than the training data.", "labels": [], "entities": []}, {"text": "Comparing TRAIN2 to TEST1/TEST2 and TRAIN3 to TEST3, the relative gain over the baselines is much lower on test data.", "labels": [], "entities": [{"text": "TRAIN2", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9049097299575806}, {"text": "TRAIN3", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9145660996437073}]}, {"text": "Moreover, only 38% of trackers performed better than a simple majority-class baseline on TEST4, for which there was no matched training data.", "labels": [], "entities": [{"text": "TEST4", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9142741560935974}]}, {"text": "These findings suggests that generalization is an important open issues for dialog state trackers.", "labels": [], "entities": [{"text": "generalization", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9880169630050659}, {"text": "dialog state trackers", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.8318715691566467}]}, {"text": "Second, indicates that the gains made Figure 5: Percent of highest-scored dialog state hypotheses which did not appear in the top-ranked SLU position vs. schedule2 accuracy overall slots.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9907637238502502}]}, {"text": "Trackers -including those with the highest accuracyfor TEST1 and TEST2 rarely assigned the highest score to an SLU hypothesis other than the top.", "labels": [], "entities": [{"text": "accuracyfor", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9976853132247925}, {"text": "TEST1", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.6041057109832764}, {"text": "TEST2", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.6092861890792847}]}, {"text": "All trackers for TEST3 and TEST4 assigned the highest score to an SLU hypothesis other than the top in a non-trivial percent of turns. by the trackers over the baselines are larger for Group A systems (TEST1 and TEST2) than for Group B (TEST3) and C (TEST4) systems.", "labels": [], "entities": []}, {"text": "Whereas the baselines consider only the top SLU hypothesis, statistical trackers can make use of the entire N-best list, increasing recall -compare the 1-best and N-best SLU recall rates in.", "labels": [], "entities": [{"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9995143413543701}, {"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9068586826324463}]}, {"text": "However, Group A trackers almost never assigned the highest score to an item below the top position in the SLU N-best list.", "labels": [], "entities": [{"text": "SLU N-best list", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.7675384283065796}]}, {"text": "Rather, the larger gains for Group A systems seem due to the relatively poor discrimination of Group A's SLU confidence score (): whereas the trackers use a multitude of features to assign scores, the baselines rely entirely on the SLU confidence for their scores, so undiscriminative SLU confidence measures hamper baseline performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of the datasets. One turn includes a system output and a user response. Slots are  named entity types such as bus route, origin neighborhood, date, time, etc. N-best SLU Recall indicates  the fraction of concepts which appear anywhere on the SLU N-best list.", "labels": [], "entities": []}]}