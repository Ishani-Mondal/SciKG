{"title": [{"text": "Natural Language Generation with Vocabulary Constraints", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5921169420083364}]}], "abstractContent": [{"text": "We investigate data driven natural language generation under the constraint that all words must come from a fixed arbitrary vocabulary.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.7953656315803528}]}, {"text": "This constraint is then extended such that a user specified word must also appear in the sentence.", "labels": [], "entities": []}, {"text": "We present fast approximations to the ideal rejection samplers and increase variability in generated text through controlled smoothing.", "labels": [], "entities": []}, {"text": "Data driven Natural Language Generation (NLG) is a fascinating topic explored by academics and artists alike, but motivating its empirical study is a difficult task.", "labels": [], "entities": [{"text": "Data driven Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7201793342828751}]}, {"text": "While many language models used in statistical NLP are generative and can easily produce sample sentences from distributions estimated from data, if all that is required is a plausible sentence one might as well pick one at random from any existing corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLG is useful when constraints are applied such that only certain plausible sentences are valid.", "labels": [], "entities": []}, {"text": "The majority of NLG applies the semantic constraint of \"what to say\", producing sentences with communicative goals.", "labels": [], "entities": []}, {"text": "Other work such as ours investigates constraints in structure; producing sentences of a certain form without concern for their meaning.", "labels": [], "entities": []}, {"text": "We motivate two specific constraints concerning the words that are allowed in a sentence.", "labels": [], "entities": []}, {"text": "The first sets a fixed vocabulary such that only sentences where all words are in-vocab are allowed.", "labels": [], "entities": []}, {"text": "The second demands not only that all words are in-vocab, but specifies the inclusion of a single arbitrary word somewhere in the sentence.", "labels": [], "entities": []}, {"text": "These contraints are most natural in the case of language education, where students have small known vocabularies and exercises that reinforce the knowledge of arbitrary words are required.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our models on sentences drawn from the Simple English Wikipedia 1 . We obtained these sentences from a data dump which we liberally filtered to remove items such as lists and sentences longer than 15 words or shorter then 3 words.", "labels": [], "entities": [{"text": "Simple English Wikipedia 1", "start_pos": 48, "end_pos": 74, "type": "DATASET", "confidence": 0.7835502997040749}]}, {"text": "We parsed this data with the recently updated Stanford Parser ( to Penn Treebank constituent form, and removed any sentence that did not parse to atop level S containing at least one NP and one VP child.", "labels": [], "entities": [{"text": "Stanford Parser ( to Penn Treebank constituent form", "start_pos": 46, "end_pos": 97, "type": "DATASET", "confidence": 0.8831419423222542}]}, {"text": "Even with such strong filters, we retained over 140K sentences for use as training data, and provide this exact set of parse trees for use in future work.", "labels": [], "entities": []}, {"text": "Inspired by the application in language education, for our vocabulary list we use the English Vocabulary Profile, which predicts student vocabulary at different stages of learning English as a second language.", "labels": [], "entities": []}, {"text": "We take the most basic American English vocabulary (the A1 list), and retrieve all inflections for each word using SimpleNLG (, yielding a vocabulary of 1226 simple words and punctuation.", "labels": [], "entities": [{"text": "SimpleNLG", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.492536336183548}]}, {"text": "To mitigate noise in the data, we discard any pair of context and outcome that appears only once in the training data, and estimate the parameters of the unconstrained model using EM.", "labels": [], "entities": []}], "tableCaptions": []}