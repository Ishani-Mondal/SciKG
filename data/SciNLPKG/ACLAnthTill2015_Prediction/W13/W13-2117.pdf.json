{"title": [{"text": "Abstractive Meeting Summarization with Entailment and Fusion", "labels": [], "entities": [{"text": "Abstractive Meeting Summarization", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5190767049789429}]}], "abstractContent": [{"text": "We propose a novel end-to-end framework for abstractive meeting summariza-tion.", "labels": [], "entities": []}, {"text": "We cluster sentences in the input into communities and build an entail-ment graph over the sentence communities to identify and select the most relevant sentences.", "labels": [], "entities": []}, {"text": "We then aggregate those selected sentences by means of a word graph model.", "labels": [], "entities": []}, {"text": "We exploit a ranking strategy to select the best path in the word graph as an abstract sentence.", "labels": [], "entities": []}, {"text": "Despite not relying on the syntactic structure, our approach significantly outperforms previous models for meeting summarization in terms of in-formativeness.", "labels": [], "entities": [{"text": "summarization", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.8628562092781067}]}, {"text": "Moreover, the longer sentences generated by our method are competitive with shorter sentences generated by the previous word graph model in terms of grammaticality.", "labels": [], "entities": []}], "introductionContent": [{"text": "The huge amount of data generated everyday in meetings calls for developing automated methods to efficiently process these data to meet users' needs.", "labels": [], "entities": []}, {"text": "Automatic summarization is a popular task that can help users to browse a large amount of recorder speech in text format.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5298920571804047}]}, {"text": "This paper tackles the task of recorded meeting summarization, addressing the key limitations of existing approaches by proposing the following contributions: 1) Various approaches that were recently developed for meeting summarization (such as () focus on extracting important sentences (or dialogue acts) from speech transcripts, either manual transcripts or automatic speech recognition (ASR) output.", "labels": [], "entities": [{"text": "meeting summarization", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.49650685489177704}, {"text": "meeting summarization", "start_pos": 214, "end_pos": 235, "type": "TASK", "confidence": 0.7442731857299805}, {"text": "extracting important sentences (or dialogue acts) from speech transcripts", "start_pos": 257, "end_pos": 330, "type": "TASK", "confidence": 0.6980478167533875}, {"text": "automatic speech recognition (ASR) output", "start_pos": 361, "end_pos": 402, "type": "TASK", "confidence": 0.7661429473331997}]}, {"text": "However, it has been observed in the context of meeting summarization users generally prefer concise abstracts over extracts, and abstracts lead to higher objective task scores; likewise automatic abstractive summaries are preferred in comparison with human extracts (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7256340980529785}]}, {"text": "Moreover, most of the abstractive summarization approaches focus on one component of the system, such as generation (e.g.,) or content selection (e.g.,), instead of developing the full framework for abstractive summarization.", "labels": [], "entities": []}, {"text": "To address these limitations, as the main contribution of this paper, we propose a full pipeline to generate an abstractive summary for each meeting transcript.", "labels": [], "entities": []}, {"text": "Our system is similar to that of in terms of generating abstractive summaries for meeting transcripts.", "labels": [], "entities": []}, {"text": "However, we take a lighter supervision for the content selection phase and a different approach towards the language generation phase, which does not rely on the conventional Natural Language Generation (NLG) architecture).", "labels": [], "entities": [{"text": "language generation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7108951210975647}, {"text": "Natural Language Generation (NLG)", "start_pos": 175, "end_pos": 208, "type": "TASK", "confidence": 0.7718203167120615}]}, {"text": "2) We propose a word graph based approach to aggregate and generate the abstractive sentence summary.", "labels": [], "entities": []}, {"text": "Our work extends the word graph method proposed by with the following novel contributions: i) We take advantage of lexical knowledge to merge similar nodes by finding their relations in WordNet; ii) We generate new sentences through generalization and aggregation of the original ones, which means that our generated sentences are not necessarily composed of the original words; and iii) We adopt anew ranking strategy to select the best path in the graph by taking the information content and the grammaticality (i.e. fluency) of the sentence into consideration.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 186, "end_pos": 193, "type": "DATASET", "confidence": 0.9363064169883728}]}, {"text": "3) In order to generate an abstract summary fora meeting, we have to be able to capture the overall content of the conversation.", "labels": [], "entities": []}, {"text": "This can be done by identifying the essential content from the most informative sentences using the semantic relations among all sentences in a meeting transcript.", "labels": [], "entities": []}, {"text": "How-ever, most current methods disregard such relations in favor of statistical models of word distributions and frequencies.", "labels": [], "entities": []}, {"text": "Moreover, the data from meeting transcripts often contains many highly redundant sentences.", "labels": [], "entities": []}, {"text": "As one of the key contributions of this paper, we propose to build a multidirectional entailment graph over the sentences to identify and select relevant information from the most informative sentences.", "labels": [], "entities": []}, {"text": "4) The textual data from meeting conversation transcripts are typically in a casual style and do not exhibit a clear syntactic structure with proper grammar and spelling.", "labels": [], "entities": []}, {"text": "Therefore, abstractive summarization approaches developed for formal texts, such as scientific or news articles, often are not satisfactory when dealing with informal texts.", "labels": [], "entities": []}, {"text": "Our proposed method for abstractive meeting summarization requires minimal syntactic and structural information and is robust in dealing with text that suffers from transcription errors, ill-formed sentences and unknown lexical choices such as typically formed in meeting transcripts.", "labels": [], "entities": [{"text": "abstractive meeting summarization", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.6663341323534647}]}, {"text": "We evaluate our system over meeting transcripts.", "labels": [], "entities": []}, {"text": "Our result compares favorably to the result of previous extractive and abstractive approaches in terms of information content.", "labels": [], "entities": []}, {"text": "Moreover, we show that our method can generate longer sentences with competitive grammaticality scores, in comparison with previous abstractive approaches.", "labels": [], "entities": []}, {"text": "Furthermore, we evaluate the impact of each component of our system through an ablation test.", "labels": [], "entities": []}, {"text": "As an additional result of our experiments, we also show that using semantic relations (entailment graph) is important in efficiently performing the final step of our summarization pipeline (i.e., the sentence fusion).", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 201, "end_pos": 216, "type": "TASK", "confidence": 0.7159835249185562}]}], "datasetContent": [{"text": "We now describe a preliminary, formative evaluation of our framework, whose main goal is to assess strengths and weaknesses of the various components and generate ideas for further developments.", "labels": [], "entities": []}, {"text": "To verify the effectiveness of our approach, we experiment with the AMI meeting corpus) that consists of 140 multi-party meetings with a wide range of annotations, including abstactive summaries for each meeting and links from each sentence in the summary to the set of sentences in the original transcripts that sentence is summarizing.", "labels": [], "entities": [{"text": "AMI meeting corpus", "start_pos": 68, "end_pos": 86, "type": "DATASET", "confidence": 0.9351124366124471}]}, {"text": "We use this information as our gold standard since it provides many examples in which a set of sentences in the meeting (a community) is linked to a human written sentence summarizing that community.", "labels": [], "entities": []}, {"text": "In our experiments, we use human authored transcripts.", "labels": [], "entities": []}, {"text": "Note that our approach is not specific to conversations, however it is designed to deal with ill-formed sentences and structural errors.", "labels": [], "entities": []}, {"text": "Moreover, the first two components of our system are supervised, while the word graph model is completely unsupervised and domain independent.", "labels": [], "entities": []}, {"text": "In order to train our logistic regression classifier for the first phase of our pipeline, we randomly select a training set that consists of 98 meetings.", "labels": [], "entities": []}, {"text": "Note that there are about one million sentence pair instances in the training set since we consider every pairing of sentences within a meeting.", "labels": [], "entities": []}, {"text": "The rest is selected as a test set for the evaluation phase.", "labels": [], "entities": []}, {"text": "For preprocessing our dataset we use OpenNLP 3 for tokenization and part-of-speech tagging.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9704397320747375}, {"text": "part-of-speech tagging", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7154271304607391}]}, {"text": "When the number of sentences in each community is more than 10 (which happens in around 10% of the cases), the community is first clustered using the MajorClust () algorithm when sentences are represented as normalized tfidf vectors and the similarity between the sentences is measured using cosine similarity.", "labels": [], "entities": []}, {"text": "Then, each cluster is treated as a separate community.", "labels": [], "entities": []}, {"text": "The clustering guarantees a higher overlap between the sentences as well as a word graph with fewer paths.", "labels": [], "entities": []}, {"text": "Next, we construct a word graph over each cluster and rank the valid paths.", "labels": [], "entities": []}, {"text": "We choose the first ranked path as the abstractive summary of the cluster.", "labels": [], "entities": []}, {"text": "For our language model, we use a tri-gram smoothed language model trained using the newswire text provided in the English Gigaword corpus (Graff and Cieri, ).", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 114, "end_pos": 137, "type": "DATASET", "confidence": 0.7872745990753174}]}, {"text": "To evaluate performance, we use the ROUGE-1 and ROUGE-2 (unigram and bigram overlap) F1 score, which correlate well with human rankings of summary quality ().", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9774060845375061}, {"text": "ROUGE-2 (unigram and bigram overlap) F1 score", "start_pos": 48, "end_pos": 93, "type": "METRIC", "confidence": 0.7775640421443515}]}, {"text": "We also ignore stopwords to reduce the impact of high overlap when matching them.", "labels": [], "entities": []}, {"text": "Furthermore, to evaluate the grammaticality of our generated summaries in comparison with the original word graph method, following common practice (), we randomly selected 10 meeting summaries (total 150 sentences).", "labels": [], "entities": []}, {"text": "Then, we asked annotators to give one  of three possible ratings for each sentence in a summary based on grammaticality: perfect (2 pts), only one mistake (1 pt) and not acceptable (0 pts), ignoring the capitalization or punctuation.", "labels": [], "entities": [{"text": "perfect", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9791886210441589}]}, {"text": "Each meeting was rated by two annotators (Computer Science graduate students).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of different summarization algorithms  on human transcripts for meeting conversations. 5", "labels": [], "entities": [{"text": "summarization", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.971985399723053}]}]}