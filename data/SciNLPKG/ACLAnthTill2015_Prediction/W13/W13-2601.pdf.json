{"title": [], "abstractContent": [{"text": "Cross-linguistic studies on unsupervised word segmentation have consistently shown that English is easier to segment than other languages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7467227280139923}]}, {"text": "In this paper, we propose an explanation of this finding based on the notion of segmentation ambiguity.", "labels": [], "entities": []}, {"text": "We show that English has a very low segmentation ambiguity compared to Japanese and that this difference correlates with the segmentation performance in a unigram model.", "labels": [], "entities": []}, {"text": "We suggest that segmentation ambiguity is linked to a trade-off between syllable structure complexity and word length distribution.", "labels": [], "entities": [{"text": "segmentation ambiguity", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.8970984220504761}]}], "introductionContent": [{"text": "During the course of language acquisition, infants must learn to segment words from continuous speech.", "labels": [], "entities": []}, {"text": "Experimental studies show that they start doing so from around 7.5 months of age.", "labels": [], "entities": []}, {"text": "Further studies indicate that infants are sensitive to a number of word boundary cues, like prosody (), transition probabilities, phonotactics (), coarticulation) and combine these cues with different weights (.", "labels": [], "entities": []}, {"text": "Computational models of word segmentation have played a major role in assessing the relevance and reliability of different statistical cues present in the speech input.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7203654199838638}, {"text": "reliability", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.9198304414749146}]}, {"text": "Some of these models focus mainly on boundary detection, and assess different strategies to identify them ().", "labels": [], "entities": [{"text": "boundary detection", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8200702965259552}]}, {"text": "Other models, sometimes called lexicon-building algorithms, learn the lexicon and the segmentation at the same time and use knowledge about the extracted lexicon to segment novel utterances.", "labels": [], "entities": []}, {"text": "State-of-the-art lexicon-building segmentation algorithms are typically reported to yield better performance than word boundary detection algorithms.", "labels": [], "entities": [{"text": "word boundary detection", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.6699657837549845}]}, {"text": "As seen in, however, the performance varies considerably across languages with English winning by a high margin.", "labels": [], "entities": []}, {"text": "This raises a generalizability issue for NLP applications, but also for the modeling of language acquisition since, obviously, it is not the case that in some languages, infants fail to acquire an adult lexicon.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7044795751571655}]}, {"text": "Are these performance differences only due to the fact that the algorithms might be optimized for English?", "labels": [], "entities": []}, {"text": "Or do they also reflect some intrinsic linguistic differences between languages?", "labels": [], "entities": []}, {"text": "F The aim of the present work is to understand why English usually scores better than other languages, as far as unsupervised segmentation is concerned.", "labels": [], "entities": []}, {"text": "As a comparison point, we chose Japanese because it is among the languages that have given the poorest word segmentation scores.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.6784595847129822}]}, {"text": "In fact, found an F-score around 0.41 using both Brent (1999)'s MBDP-1 and Venkataraman (2001)'s NGS-u models, and found an F-score that goes from 0.40 to 0.55 depending on the corpus used.", "labels": [], "entities": [{"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9968024492263794}, {"text": "MBDP-1", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.8639572262763977}, {"text": "F-score", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.9913041591644287}]}, {"text": "Japanese also differs typologically from English along several phonological dimensions such as number of syllabic types, phonotactic constraints and rhythmic structure.", "labels": [], "entities": []}, {"text": "Although most lexiconbuilding segmentation algorithms do not attempt to model these dimensions, they still might be relevant to speech segmentation and help explain the performance difference.", "labels": [], "entities": [{"text": "speech segmentation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7292315363883972}]}, {"text": "The structure of the paper is as follows.", "labels": [], "entities": []}, {"text": "First, we present the class of lexical-building segmentation algorithm that we use in this paper (Adaptor Grammar), and our English and Japanese corpora.", "labels": [], "entities": []}, {"text": "We then present data replicating the basic finding that segmentation performance is better for English than for Japanese.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.9690731167793274}]}, {"text": "We then explore the hypothesis that this finding is due to an intrinsic difference in segmentation ambiguity in the two languages, and suggest that the source of this difference rests in the structure of the phonological lexicon in the two languages.", "labels": [], "entities": []}, {"text": "Finally, we use these insights to try and reduce the gap between Japanese and English segmentation through a modification of the Unigram model where multiple linguistic levels are learned jointly.", "labels": [], "entities": [{"text": "Japanese and English segmentation", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.6045448482036591}]}], "datasetContent": [{"text": "For the evaluation, we used the same measures as Brent (1999), and, namely token Precision (P), Recall (R) and F-score (F).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 81, "end_pos": 94, "type": "METRIC", "confidence": 0.9104263037443161}, {"text": "Recall (R)", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9647077172994614}, {"text": "F-score (F)", "start_pos": 111, "end_pos": 122, "type": "METRIC", "confidence": 0.9520507901906967}]}, {"text": "Precision is defined as the number of correct word tokens found out of all tokens posited.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9886759519577026}]}, {"text": "Recall is the number of correct word tokens found out of all tokens in the gold standard.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9872660636901855}, {"text": "gold standard", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.8623870313167572}]}, {"text": "The F-score is defined as the harmonic mean of Precision and Recall , F = 2 * P * RP +R . We will refer to these scores as the segmentation scores.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9852749109268188}, {"text": "Precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9822633862495422}, {"text": "Recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.8793457746505737}, {"text": "F", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9449543952941895}, {"text": "segmentation", "start_pos": 127, "end_pos": 139, "type": "TASK", "confidence": 0.9527561068534851}]}, {"text": "In addition, we define similar measures for word boundaries and word types in the lexicon.", "labels": [], "entities": []}], "tableCaptions": []}