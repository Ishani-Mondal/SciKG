{"title": [{"text": "Lemmatization and Morphosyntactic Tagging of Croatian and Serbia\u0148", "labels": [], "entities": [{"text": "Morphosyntactic Tagging", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.6631009876728058}]}], "abstractContent": [{"text": "We investigate state-of-the-art statistical models for lemmatization and morphosyn-tactic tagging of Croatian and Serbian.", "labels": [], "entities": [{"text": "morphosyn-tactic tagging", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.6592011451721191}]}, {"text": "The models stem from anew manually annotated SETIMES.HR corpus of Croa-tian, based on the SETimes parallel corpus.", "labels": [], "entities": [{"text": "SETIMES.HR corpus of Croa-tian", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.8667320311069489}, {"text": "SETimes parallel corpus", "start_pos": 90, "end_pos": 113, "type": "DATASET", "confidence": 0.7801859378814697}]}, {"text": "We train models on Croatian text and evaluate them on samples of Croat-ian and Serbian from the SETimes corpus and the two Wikipedias.", "labels": [], "entities": [{"text": "SETimes corpus", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9580075442790985}]}, {"text": "Lemmatization accuracy for the two languages reaches 97.87% and 96.30%, while full morphosyn-tactic tagging accuracy using a 600-tag tagset peaks at 87.72% and 85.56%, respectively.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9831687211990356}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.8863369226455688}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9378529787063599}]}, {"text": "Part of speech tagging accuracies reach 97.13% and 96.46%.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.7450293600559235}, {"text": "accuracies", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.8747068047523499}]}, {"text": "Results indicate that more complex methods of Croatian-to-Serbian annotation projection are not required on such dataset sizes for these particular tasks.", "labels": [], "entities": [{"text": "Croatian-to-Serbian annotation projection", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.7016773621241251}]}, {"text": "The SETIMES.HR corpus, its resulting models and test sets are all made freely available.", "labels": [], "entities": [{"text": "SETIMES.HR corpus", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.82811039686203}]}], "introductionContent": [{"text": "Part of speech tagging (POS tagging) is an natural language processing task in which words are annotated with the corresponding grammatical categories -parts of speech: verb, noun, adjective, pronoun, etc.", "labels": [], "entities": [{"text": "Part of speech tagging (POS tagging) is an natural language processing task in which words are annotated with the corresponding grammatical categories -parts of speech: verb, noun, adjective, pronoun, etc.", "start_pos": 0, "end_pos": 205, "type": "Description", "confidence": 0.7676433340097085}]}, {"text": "-in a given context.", "labels": [], "entities": []}, {"text": "It is also frequently called morphosyntactic tagging (MSD tagging, i.e., tagging with morphosyntactic descriptions), especially when addressing highly inflected languages, for which the tagging process often includes assigning additional subcategories to words, such as gender and case for nouns or tense and person for verbs.", "labels": [], "entities": []}, {"text": "POS/MSD tagging is a well-known task and an important preprocessing step in natural language processing.", "labels": [], "entities": [{"text": "POS/MSD tagging", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5885428041219711}]}, {"text": "It is often preceded or followed by lemmatization -the process of mapping inflected word forms to corresponding base forms or lemmas.", "labels": [], "entities": []}, {"text": "State of the art in POS/MSD tagging and lemmatization across languages is generally achievedboth in terms of per token accuracy and speed and robustness -by statistical methods, which involve training annotation models on manually annotated corpora.", "labels": [], "entities": [{"text": "POS/MSD tagging", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8296715915203094}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9582095146179199}]}, {"text": "In this paper, we investigate the possibility of utilizing statistical models trained on corpora of Croatian in lemmatization and MSD tagging of Croatian and Serbian.", "labels": [], "entities": [{"text": "MSD tagging", "start_pos": 130, "end_pos": 141, "type": "TASK", "confidence": 0.8874903321266174}]}, {"text": "We present anew manually annotated corpus of Croatian -the SETIMES.HR corpus.", "labels": [], "entities": [{"text": "SETIMES.HR corpus", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.7019885331392288}]}, {"text": "We test a number of lemmatizers and MSD taggers on Croatian and Serbian test sets from two different domains and consider options of annotation transfer between the two languages.", "labels": [], "entities": []}, {"text": "We also outline a first version of the Multext East v5 tagset and three usable reductions of this tagset.", "labels": [], "entities": [{"text": "Multext East v5 tagset", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.9607928544282913}]}, {"text": "Special emphasis is given to rapid resource development and public availability of our research.", "labels": [], "entities": []}, {"text": "Thus, the SETIMES.HR corpus, the test sets and the best lemmatization and MSD tagging models are made publicly available.", "labels": [], "entities": [{"text": "SETIMES.HR corpus", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.8255412876605988}, {"text": "MSD tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.8760568201541901}]}, {"text": "In the following section, we discuss related work on lemmatization and tagging of Croatian and Serbian.", "labels": [], "entities": [{"text": "tagging of Croatian and Serbian", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.7853655219078064}]}, {"text": "We then present the SETIMES.HR corpus and the test sets, selected lemmatizers and morphosyntactic taggers and the experimental method.", "labels": [], "entities": [{"text": "SETIMES.HR corpus", "start_pos": 20, "end_pos": 37, "type": "DATASET", "confidence": 0.7050132006406784}]}, {"text": "Finally we provide a discussion of the evaluation results and indicate future work directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we define specific experiment goals and the experiment design.", "labels": [], "entities": []}, {"text": "We also present the datasets and tools used in the experiment.", "labels": [], "entities": []}, {"text": "We do four batches of experiments: 1.", "labels": [], "entities": []}, {"text": "to identify the best available tool and underlying paradigm for lemmatization and tagging of both languages by observing overall accuracy and execution time, 2.", "labels": [], "entities": [{"text": "lemmatization and tagging", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.6027493278185526}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9992082715034485}]}, {"text": "to establish the need for annotation projection from Croatian SETIMES.HR corpus to its Serbian counterpart, 3.", "labels": [], "entities": [{"text": "Croatian SETIMES.HR corpus", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.6983505884806315}]}, {"text": "to select the best of the proposed MTE-based tagsets for both tasks and 4.", "labels": [], "entities": []}, {"text": "to provide in-depth evaluation of the selected top-performing lemmatizer and tagger on both languages by using the top-performing tagset.", "labels": [], "entities": []}, {"text": "In the first experiment batch, we test the tools only on Croatian data from SETimes.", "labels": [], "entities": [{"text": "Croatian data from SETimes", "start_pos": 57, "end_pos": 83, "type": "DATASET", "confidence": 0.8516611009836197}]}, {"text": "The second batch establishes the need for -or needlessness of -annotation projection for improved processing of Serbian text by testing the tools selected in the first batch on both languages.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7671930193901062}]}, {"text": "The in-depth evaluation of the third and fourth experiment batch includes, for both languages and all test sets, observing the influence of tagset selection to overall accuracy and investigating tool performance in more detail.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9985760450363159}]}, {"text": "We measure precision, recall and F 1 scores for selected parts of speech and inspect lemmatization and tagging confusion matrices for detailed analysis and possible prediction of tool operation in real-world language processing environments.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9994909763336182}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9994888305664062}, {"text": "F 1 scores", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9901129206021627}]}, {"text": "We aim for the experiment to serve as underlying documentation for enabling prospective users in implementing more complex natural language processing systems for Croatian and Serbian by using these resources.", "labels": [], "entities": []}, {"text": "Additionally, the overview of the usability of tools available is informative for researchers developing basic language technologies for other languages.", "labels": [], "entities": []}, {"text": "We test statistical significance of observed differences in our results by using the approximate randomization test.", "labels": [], "entities": []}, {"text": "All models are trained on SETIMES.HR.", "labels": [], "entities": [{"text": "SETIMES.HR", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.4822862446308136}]}, {"text": "To at least partially avoid the possible pitfall of exclusive in-domain testing, we define two test sets for each language.", "labels": [], "entities": []}, {"text": "The first test set consists of 100 Croatian-Serbian parallel sentence pairs taken by random sampling from the relative complement of the SETimes parallel corpus and SETIMES.HR.", "labels": [], "entities": [{"text": "SETimes parallel corpus", "start_pos": 137, "end_pos": 160, "type": "DATASET", "confidence": 0.737272580464681}]}, {"text": "The second test set is taken from the Croatian and Serbian Wikipedia by manually selecting 20 matching Wikipedia articles and manually extracting 100 approximate sentence pairs.", "labels": [], "entities": []}, {"text": "We chose manual over random sampling from Wikipedia to account for the fact that a certain number of articles is virtually identical between the two Wikipedias due to language similarity and mutual copying between Wikipedia users.", "labels": [], "entities": []}, {"text": "All four test sets were manually annotated using the same procedure that was used for SETIMES.HR.", "labels": [], "entities": [{"text": "SETIMES.HR", "start_pos": 86, "end_pos": 96, "type": "TASK", "confidence": 0.8596234321594238}]}, {"text": "The stats are given in.", "labels": [], "entities": []}, {"text": "In addition, we have verified the difference between language test sets by measuring lexical coverage using HML as a high-coverage morphological lexicon of Croatian.", "labels": [], "entities": []}, {"text": "For the Croatian SETimes and Wikipedia samples, we detected 5.2% and 3.9% out-of-vocabulary word forms and 11.40% and 8.86% were observed for the corresponding Serbian samples, supporting well-foundedness of the test sets in terms of maintaining the differences between the two languages.", "labels": [], "entities": [{"text": "Croatian SETimes and Wikipedia samples", "start_pos": 8, "end_pos": 46, "type": "DATASET", "confidence": 0.7955808758735656}]}], "tableCaptions": [{"text": " Table 1: Stats for SETIMES.HR and test sets", "labels": [], "entities": [{"text": "SETIMES.HR", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.9121617674827576}]}, {"text": " Table 1.  For purposes of this experiment, we propose an  alteration of the baseline MTE v4 tagset in form  of a first version for the MTE v5 standard.", "labels": [], "entities": [{"text": "MTE v4 tagset", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.9069284001986185}, {"text": "MTE v5 standard", "start_pos": 136, "end_pos": 151, "type": "DATASET", "confidence": 0.7990539868672689}]}, {"text": " Table 2. They reflect the design choices we  made: MTE v5 has a comparable amount of tags  as MTE v4, gaining additional tags in the adjective  subset, but losing tags in the verb and abbreviation  subsets, while the reductions subsequently lower  the overall MSD tag count. set.test  wiki.test", "labels": [], "entities": []}, {"text": " Table 2: Tagset variation in tag counts", "labels": [], "entities": []}, {"text": " Table 3. In terms of lemmati-set.test  wiki.test", "labels": [], "entities": []}, {"text": " Table 4: Overall tagging accuracy with and without  the inflectional lexicon", "labels": [], "entities": [{"text": "tagging", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.973381757736206}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9891703724861145}]}, {"text": " Table 5: Overall lemmatization accuracy with and  without the inflectional lexicon", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9960585832595825}]}, {"text": " Table 6: HunPos POS and MSD tagging accuracy  for all tagsets", "labels": [], "entities": [{"text": "MSD tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.6973206102848053}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9654417037963867}]}, {"text": " Table 7: CST lemmatization accuracy for all tagsets", "labels": [], "entities": [{"text": "CST lemmatization", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7314126789569855}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9643521308898926}]}, {"text": " Table 8: Statistical significance of differences in  full MSD tagging between tagsets (p-values using  approximate randomization)", "labels": [], "entities": [{"text": "MSD tagging between tagsets", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.9228095710277557}]}, {"text": " Table 9: Precision (P), recall (R) and F 1 score for  POS only (1st column) and full MSD (2nd column)  on Croatian and Serbian", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9320281744003296}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9497541636228561}, {"text": "F 1 score", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9463430047035217}, {"text": "MSD", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.867828905582428}]}]}