{"title": [], "abstractContent": [{"text": "This paper describes shallow semantically-informed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at the Workshop on Statistical Machine Translation (WMT 13).", "labels": [], "entities": [{"text": "Hierarchical Phrase-based SMT", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.6740203301111857}, {"text": "Dublin City University", "start_pos": 140, "end_pos": 162, "type": "DATASET", "confidence": 0.9537651737531027}, {"text": "translation task", "start_pos": 188, "end_pos": 204, "type": "TASK", "confidence": 0.8959145843982697}, {"text": "Statistical Machine Translation (WMT 13)", "start_pos": 248, "end_pos": 288, "type": "TASK", "confidence": 0.7655881260122571}]}, {"text": "The system uses PBSMT and HPBSMT decoders with multiple LMs, but will run only one decoding path decided before starting translation.", "labels": [], "entities": []}, {"text": "Therefore the paper does not present a multi-engine system combination.", "labels": [], "entities": []}, {"text": "We investigate three types of shallow semantics: (i) Quality Estimation (QE) score, (ii) genre ID, and (iii) context ID derived from context-dependent language models.", "labels": [], "entities": [{"text": "Quality Estimation (QE) score", "start_pos": 53, "end_pos": 82, "type": "METRIC", "confidence": 0.7939201096693674}, {"text": "genre ID", "start_pos": 89, "end_pos": 97, "type": "TASK", "confidence": 0.6455708146095276}]}, {"text": "Our results show that the improvement is 0.8 points absolute (BLEU) for EN-ES and 0.7 points for ES-EN compared to the standard PBSMT system (single best system).", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9853506684303284}]}, {"text": "It is important to note that we developed this method when the standard (confusion network-based) system combination is ineffective such as in the case when the input is only two.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes shallow semanticallyinformed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at WMT 13.", "labels": [], "entities": [{"text": "Hierarchical Phrase-based SMT", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6759076913197836}, {"text": "Dublin City University", "start_pos": 139, "end_pos": 161, "type": "DATASET", "confidence": 0.952277660369873}, {"text": "translation task", "start_pos": 187, "end_pos": 203, "type": "TASK", "confidence": 0.8946564793586731}, {"text": "WMT 13", "start_pos": 231, "end_pos": 237, "type": "DATASET", "confidence": 0.8182918727397919}]}, {"text": "Our objectives are to incorporate several shallow semantics into SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9908257126808167}]}, {"text": "The first semantics is the QE score fora given input sentence which can be used to select the decoding path either of HPBSMT or PBSMT.", "labels": [], "entities": [{"text": "QE score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.95023313164711}, {"text": "HPBSMT", "start_pos": 118, "end_pos": 124, "type": "DATASET", "confidence": 0.9651027917861938}]}, {"text": "Although we call this a QE score, this score is not quite a standard one which does not have access to translation output information.", "labels": [], "entities": []}, {"text": "The second semantics is genre ID which is intended to capture domain adaptation.", "labels": [], "entities": [{"text": "genre ID", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.7386027574539185}, {"text": "domain adaptation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7247650176286697}]}, {"text": "The third semantics is context ID: this context ID is used to adjust the context for the local words.", "labels": [], "entities": []}, {"text": "Context ID is used in a continuous-space LM, but is implicit since the context does not appear in the construction of a continuous-space LM.", "labels": [], "entities": [{"text": "Context ID", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.8162384629249573}]}, {"text": "Note that our usage of the term semantics refers to meaning constructed by a sentence or words.", "labels": [], "entities": []}, {"text": "The QE score works as a sentence level switch to select HPBSMT or PBSMT, based on the semantics of a sentence.", "labels": [], "entities": [{"text": "QE score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.7638180255889893}, {"text": "HPBSMT", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.9492760300636292}]}, {"text": "The genre ID gives an indication that the sentence is to be translated by genre IDsensitive MT systems, again based on semantics on a sentence level.", "labels": [], "entities": []}, {"text": "The context-dependent LM can be interpreted as supplying the local context to a word, capturing semantics on a word level.", "labels": [], "entities": []}, {"text": "The architecture presented in this paper is substantially different from multi-engine system combination.", "labels": [], "entities": []}, {"text": "Although the system has multiple paths, only one path is chosen at decoding when processing unseen data.", "labels": [], "entities": []}, {"text": "Note that standard multi-engine system combination using these three semantics has been presented before (.", "labels": [], "entities": []}, {"text": "This paper also compares the two approaches.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the motivation for our approach.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our proposed systems, while in Section 4 we describe the experimental results.", "labels": [], "entities": []}, {"text": "We conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Moses () for PBSMT and HPBSMT systems in our experiments.", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8118752241134644}, {"text": "HPBSMT", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.861718475818634}]}, {"text": "The GIZA++ implementation of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.8243005275726318}]}, {"text": "For phrase extraction the grow-diag-final heuristics described in () is used to derive the refined alignment from bidirectional alignments.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8581869602203369}]}, {"text": "We then perform MERT process which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing) trained with SRILM).", "labels": [], "entities": [{"text": "MERT", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.961098849773407}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9980717301368713}, {"text": "SRILM", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.502426266670227}]}, {"text": "For the HPBSMT system, the chart-based decoder of Moses () is used.", "labels": [], "entities": [{"text": "HPBSMT", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.9294523596763611}]}, {"text": "Most of the procedures are identical with the PBSMT systems except the rule extraction process.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.851006418466568}]}, {"text": "The procedures to handle three kinds of semantics are implemented using the already mentioned algorithm.", "labels": [], "entities": []}, {"text": "We use libSVM (, and Mallet) for Latent Dirichlet Allocation (LDA) (.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.6847032407919565}]}, {"text": "For the corpus, we used all the resources provided for the translation task at WMT13 for lan- Experimental results are shown in.", "labels": [], "entities": [{"text": "translation task", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.9041120111942291}, {"text": "WMT13", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.934822142124176}]}, {"text": "The left-most column (sem-inform) shows our results.", "labels": [], "entities": []}, {"text": "The sem-inform made a improvement of 0.8 BLEU points absolute compared to the PBSMT results in EN-ES, while the standard system combination lost 0.1 BLEU points absolute compared to the single worst.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9977922439575195}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9974532723426819}]}, {"text": "For ES-EN, the sem-inform made an improvement of 0.7 BLEU points absolute compared to the PBSMT results.", "labels": [], "entities": [{"text": "ES-EN", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.431696355342865}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9991217255592346}]}, {"text": "These improvements over both of PBSMT and HPBSMT are statistically significant by a paired bootstrap test).", "labels": [], "entities": [{"text": "HPBSMT", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.8260233998298645}]}], "tableCaptions": []}