{"title": [{"text": "Uses of Monolingual In-Domain Corpora for Cross-Domain Adaptation with Hybrid MT Approaches", "labels": [], "entities": [{"text": "Cross-Domain Adaptation", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7242439985275269}]}], "abstractContent": [{"text": "Resource limitation is challenging for cross-domain adaption.", "labels": [], "entities": []}, {"text": "This paper employs patterns identified from a monolingual in-domain corpus and patterns learned from the post-edited translation results, and translation model as well as language model learned from pseudo bilingual corpora produced by a baseline MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 247, "end_pos": 249, "type": "TASK", "confidence": 0.9506530165672302}]}, {"text": "The adaptation from a government document domain to a medical record domain shows the rules mined from the monolingual in-domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual dictionary and corpus are important resources for MT applications.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9955822825431824}]}, {"text": "They are used for lexical choice and model construction.", "labels": [], "entities": [{"text": "model construction", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7393611371517181}]}, {"text": "However, not all resources are available in bilingual forms in each domain.", "labels": [], "entities": []}, {"text": "For example, medical records are in English only in some countries.", "labels": [], "entities": []}, {"text": "In such a case, only bilingual dictionary and monolingual corpus is available.", "labels": [], "entities": []}, {"text": "Lack of bilingual corpus makes domain adaptation more challenging.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7532918751239777}]}, {"text": "A number of adaptation approaches) have been proposed.", "labels": [], "entities": []}, {"text": "They address the reliability of a model in anew domain and count the domain similarities between a model and the indomain development data.", "labels": [], "entities": []}, {"text": "The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered.", "labels": [], "entities": []}, {"text": "propose semisupervised methods which use monolingual data in source language to improve translation performance.", "labels": [], "entities": []}, {"text": "present lightlysupervised training to generate additional training data from the translation results of monolingual data.", "labels": [], "entities": []}, {"text": "To deal with the resource-poor issue, generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus.", "labels": [], "entities": []}, {"text": "Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification () is another direction.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7202902138233185}]}, {"text": "Simplifying a source language text makes the translation easier in a background MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9717497229576111}]}, {"text": "propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.7467370629310608}]}, {"text": "They focus on the treatments of input text only, but do not consider how to adapt the background MT to the specific domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9709402322769165}]}, {"text": "The translation performance depends on the coverage of the simplification rules and the quality of the background system.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.966973602771759}]}, {"text": "This paper adopts the simplificationtranslation-restoration methodology), but emphasizes on how to update bilingual translation rules, translation model and language model, which are two kernels of rulebased and statistics-based MT systems, respectively.", "labels": [], "entities": [{"text": "MT", "start_pos": 229, "end_pos": 231, "type": "TASK", "confidence": 0.9418461322784424}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 specifies the proposed hybrid MT approaches to resource-limited domains.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9927564263343811}]}, {"text": "The characteristics of available resources including their types, their linguality, their belonging domains, and their belonging languages are analyzed and their uses in translation rule mining and model construction are presented.", "labels": [], "entities": [{"text": "translation rule mining", "start_pos": 170, "end_pos": 193, "type": "TASK", "confidence": 0.8947285016377767}, {"text": "model construction", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.7414402961730957}]}, {"text": "Section 3 discusses how to adapt an MT system from a government document domain to a medical record domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9882291555404663}]}, {"text": "The experimental setups reflect various settings.", "labels": [], "entities": []}, {"text": "Section 4 concludes the remarks.", "labels": [], "entities": []}, {"text": "sketches the overall picture of our proposed hybrid MT approaches.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9893274307250977}]}, {"text": "A resource is represented in terms of its linguality, domain, language, and type, where MO/BI denotes monolingual/bilingual, ID/OD denotes in-domain/outdomain, and SL/TL denotes source language/target language.", "labels": [], "entities": [{"text": "MO/BI", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.8386602600415548}]}, {"text": "For example, an MO-ID-SL corpus and an MO-ID-TL corpus mean monolingual in-domain corpora in source and in target languages, respectively.", "labels": [], "entities": [{"text": "MO-ID-SL corpus", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.8508259057998657}, {"text": "MO-ID-TL corpus", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.8057894706726074}]}, {"text": "Similarly, a BI-OD corpus and a BI-ID dictionary denote a bilingual out-domain corpus, and a bilingual in-domain dictionary, respectively.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Resources used in each hybrid MT method", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9793233871459961}]}, {"text": " Table 2: BLEU of each hybrid MT method", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9996774196624756}, {"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9691684246063232}]}]}