{"title": [{"text": "The C-Score -Proposing a Reading Comprehension Metrics as a Common Evaluation Measure for Text Simplification", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7424058318138123}]}], "abstractContent": [{"text": "This article addresses the lack of common approaches for text simplification evaluation , by presenting the first attempt fora common evaluation metrics.", "labels": [], "entities": [{"text": "text simplification evaluation", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.8670435945192972}]}, {"text": "The article proposes reading comprehension evaluation as a method for evaluating the results of Text Simplification (TS).", "labels": [], "entities": [{"text": "reading comprehension evaluation", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.6650412281354269}, {"text": "Text Simplification (TS)", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.84390789270401}]}, {"text": "An experiment , as an example application of the evaluation method, as well as three for-mulae to quantify reading comprehension, are presented.", "labels": [], "entities": []}, {"text": "The formulae produce an unique score, the C-score, which gives an estimation of user's reading comprehension of a certain text.", "labels": [], "entities": []}, {"text": "The score can be used to evaluate the performance of a text simplification engine on pairs of complex and simplified texts, or to compare the performances of different TS methods using the same texts.", "labels": [], "entities": []}, {"text": "The approach can be particularly useful for the modern crowd-sourcing approaches, such as those employing the Amazon's Mechanical Turk 1 or CrowdFlower 2.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk 1", "start_pos": 110, "end_pos": 136, "type": "DATASET", "confidence": 0.9008657217025757}]}, {"text": "The aim of this paper is thus to propose an evaluation approach and to motivate the TS community to start a relevant discussion, in order to come up with a common evaluation metrics for this task.", "labels": [], "entities": [{"text": "TS community", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.6973812580108643}]}], "introductionContent": [], "datasetContent": [{"text": "As mentioned in the previous section, until now, the different authors adopted different combinations of metrics, without reaching to a common approach, which would allow the comparison of different systems.", "labels": [], "entities": []}, {"text": "As the different TS evaluation methods are applied on a variety of different text units (words, sentences, texts), this makes the comparison between approaches even harder.", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.912628710269928}]}, {"text": "As the aim of this article is to propose a text simplification evaluation metrics which would take into account text comprehensibility and reading comprehension, in this discussion we will focus mostly on the approaches, whose aim is to simplify texts for target readers and their evaluation strategies.", "labels": [], "entities": []}, {"text": "The existing TS evaluation approaches focus either on the quality of the generated text/sentences, or on the effectiveness of text simplification on reading comprehension.", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9334037601947784}]}, {"text": "The first group of approaches include human judges ratings of simplification, content preservation, and grammaticality, standard MT evaluation scores (BLEU and NIST), a variety of other automatic metrics (perplexity, precision/recall/F-measure, and edit distance).", "labels": [], "entities": [{"text": "content preservation", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.7651239633560181}, {"text": "MT evaluation", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.8394385576248169}, {"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9880822896957397}, {"text": "NIST", "start_pos": 160, "end_pos": 164, "type": "DATASET", "confidence": 0.8875828981399536}, {"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9987080097198486}, {"text": "recall", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.6435695290565491}, {"text": "F-measure", "start_pos": 234, "end_pos": 243, "type": "METRIC", "confidence": 0.5168765187263489}]}, {"text": "The methods, aiming to evaluate the text simplification impact on reading comprehension, use, instead, reading speed, reading errors, speech errors, comprehension questions, answer correctness, and users' feedback.", "labels": [], "entities": []}, {"text": "Several approaches use a variety of readability formulae (the Flesch, Flesch-Kincaid, Coleman-Liau, and Lorge formulae for English, as well as readability formulae for other languages, such as for Spanish).", "labels": [], "entities": []}, {"text": "Due to the criticisms of readability formulae, which often restrict themselves to a very superficial text level, they can be considered to stand on the borderline between the two previously described groups of TS evaluation approaches.", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 210, "end_pos": 223, "type": "TASK", "confidence": 0.9261710047721863}]}, {"text": "As can be seen from the discussion below, different TS systems employ a combination of the listed evaluation approaches.", "labels": [], "entities": [{"text": "TS", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9817302227020264}]}, {"text": "As one of the first text simplification systems for target reader populations, PSET, seems to have applied different evaluation strategies for different of its components, without running an evaluation of the system as a whole.", "labels": [], "entities": []}, {"text": "The lexical simplification component, which replaced technical terms with more frequent synonyms, was evaluated via user feedback, comprehension questions and the use of the Lorge readability formula.", "labels": [], "entities": []}, {"text": "The syntactic simplification system evaluated its single components and the system as a whole from different points of view, to a different extent, and used different evaluation strategies.", "labels": [], "entities": []}, {"text": "Namely, the text comprehensibility was evaluated via reading time and answers' correctness given by sixteen aphasic readers; the components replacing passive with active voice and splitting sentences were evaluated for content preservation and grammaticality via four human judges' ratings; and finally, the anaphora resolution component was evaluated using precision and recall.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 219, "end_pos": 239, "type": "TASK", "confidence": 0.7193549424409866}, {"text": "anaphora resolution", "start_pos": 308, "end_pos": 327, "type": "TASK", "confidence": 0.685691624879837}, {"text": "precision", "start_pos": 358, "end_pos": 367, "type": "METRIC", "confidence": 0.9994195699691772}, {"text": "recall", "start_pos": 372, "end_pos": 378, "type": "METRIC", "confidence": 0.9951164722442627}]}, {"text": "Siddharthan (2003) did not carryout evaluation with target readers, while three human judges rated the grammaticality and the meaning preservation of ninety-five sentences.", "labels": [], "entities": []}, {"text": "used precision, recall and f-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9996989965438843}, {"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9996638298034668}, {"text": "f-measure", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9887700080871582}]}, {"text": "Other approaches, using human judges are those of, who also used precision and recall and, who employed three annotators comparing pairs of words and indicating them same, simpler, or more complex.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9992384910583496}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.997080385684967}]}, {"text": "run two experiments, the larger one involving 230 subjects and measured oral reading rate, oral reading errors, response correctness to comprehension questions and finally, speech errors.", "labels": [], "entities": [{"text": "oral reading rate", "start_pos": 72, "end_pos": 89, "type": "METRIC", "confidence": 0.7982507348060608}, {"text": "oral reading errors", "start_pos": 91, "end_pos": 110, "type": "METRIC", "confidence": 0.7055534521738688}]}, {"text": "used 7 readability measures for Spanish to evaluate the degree of simplification, and twenty-five human annotators to evaluate on a Likert scale the grammaticality of the output and the preservation of the original meaning.", "labels": [], "entities": []}, {"text": "The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().", "labels": [], "entities": [{"text": "TS", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9795114994049072}, {"text": "MT task", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9213049411773682}, {"text": "MT evaluation", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.8762330710887909}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9969872832298279}, {"text": "NIST", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.8099390268325806}, {"text": "TERp", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9217404723167419}]}, {"text": "In addition, Woodsend and Lapata (2011) apply two readability measures (Flesch-Kincaid, Coleman-Liau) to evaluate the actual reduction in complexity and human judges ratings for simplification, meaning preservation, and grammaticality.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 194, "end_pos": 214, "type": "TASK", "confidence": 0.8226487636566162}]}, {"text": "apply the Flesch readability score and n-gram language model perplexity, and Coster and Kauchak (2011) -two additional automatic techniques (the word-level-F1 and simple string accuracy), taken from sentence compression evaluation).", "labels": [], "entities": [{"text": "Flesch readability score", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8796101212501526}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.7172299027442932}, {"text": "sentence compression evaluation", "start_pos": 199, "end_pos": 230, "type": "TASK", "confidence": 0.8064349492390951}]}, {"text": "As we consider that the aim of text simplification for human readers is to improve text comprehensibility, we argue that reading comprehension must be evaluated, and that evaluating just the quality of produced sentences is not enough.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7153880000114441}]}, {"text": "Differently from the approaches that employ human judges, we consider that it is better to test real human comprehension with target readers populations, rather than to make conclusions about the extent of population's understanding on the basis of the opinion of a small number of human judges.", "labels": [], "entities": []}, {"text": "In addition, we consider that measuring reading speed, rate, as well as reading and speed errors, requires much more complicated and expensive tools, than having an online system to measure time to reply and recognize correct answers.", "labels": [], "entities": [{"text": "reading and speed errors", "start_pos": 72, "end_pos": 96, "type": "METRIC", "confidence": 0.6892643421888351}]}, {"text": "Finally, we consider that cloze tests are an evaluation method that cannot really reflect the complexity of reading comprehension (for example for measuring manipulations of the syntactic structure of sentences), and for this reason, we select multiple-choice questions as the testing method, which we consider the most reflecting the specificities of the complexity of a text, more accessible than eye-tracking technologies, and more objective than users' feedback.", "labels": [], "entities": []}, {"text": "The approach does not explicitly evaluate the fluency, grammaticality and content preservation of the simplified text, but can be coupled with such additional evaluation.", "labels": [], "entities": []}, {"text": "The closest to ours approach is that of who evaluated reading comprehension with over ninety readers with and without dyslexia.", "labels": [], "entities": []}, {"text": "Besides using eye-tracking (reading time and fixations duration), different reading devices, and users rating the text according to how easy it is it read, to understand and to remember, they obtain also a comprehension score based on multiplechoice questions (MCQ) with 3 answers (1 correct, 1 partially correct and 1 wrong).", "labels": [], "entities": [{"text": "remember", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9921014904975891}]}, {"text": "The difference with our approach is that we consider that having only one correct answer (as suggested by), is a more objective evaluation, rather than having one partially correct answer, which would introduce subjectivity in evaluation.", "labels": [], "entities": []}, {"text": "To support our motivation, some state-of-the-art approaches state the scarcity of evaluation with target readers, note that there are no commonly accepted evaluation measures (), attempt to address the need of developing reading comprehension evaluation methods, and propose common evaluation frameworks ().", "labels": [], "entities": []}, {"text": "More concretely, propose the magnitude estimation of readability judgements and the delayed sentence recall as reading comprehension evaluation methods.", "labels": [], "entities": []}, {"text": "As seen in the overview, besides the multitude of existing approaches, and the few approaches attempting to propose a common evaluation framework, there are no widely accepted evaluation metrics or methods, which would allow the com-parison of existing approaches.", "labels": [], "entities": []}, {"text": "The next section presents our evaluation approach, which we offer as a candidate for common evaluation metrics.", "labels": [], "entities": []}, {"text": "The metrics proposed in this article, was developed in the context of a previously conducted large-scale text simplification evaluation experiment.", "labels": [], "entities": [{"text": "text simplification evaluation", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.7509344021479288}]}, {"text": "The experiment aimed to determine whether a manual, rule-based text simplification approach (namely a controlled language), can re-write existing texts into more understandable versions.", "labels": [], "entities": []}, {"text": "Impact on reading comprehension was necessary to evaluate, as the purpose of text simplification was to enhance in first place the reading comprehension of emergency instructions.", "labels": [], "entities": []}, {"text": "The controlled language used for simplification was the Controlled Language for Crisis Management (CLCM, more details in), which was developed on the basis of existing psychological and psycholinguistic literature discussing human comprehension under stress, which ensures its psychological validity.", "labels": [], "entities": []}, {"text": "The text units evaluated in this experiments were whole texts, and more concretely pairs of original texts and their simplified versions.", "labels": [], "entities": []}, {"text": "We argue that using whole texts for measuring reading comprehension is better than single sentences, as the texts provide more context for understanding.", "labels": [], "entities": []}, {"text": "The experiment took place in the format of an online experiment, conducted via a specially developed web interface, and required users to read several texts and answer Multiple-Choice Questions (MCQ), testing the readers' understanding of each of the texts.", "labels": [], "entities": []}, {"text": "Due to the purpose of the text simplification (emergency situations simulation), users were required to read the texts in a limited time, as to imitate a stressful situation with no time to think and re-read the text.", "labels": [], "entities": []}, {"text": "This aspect will not betaken into account in the evaluation, as the purpose is to propose a general formula, applicable to a variety of different text simplification experiments.", "labels": [], "entities": []}, {"text": "After reading the text in a limited time, the text was hidden from the readers, and they were presented with a screen, asking if they were ready to proceed with the questions.", "labels": [], "entities": []}, {"text": "Next, each question was displayed one by one, along with its answers, with the readers not having the option to go back to the text.", "labels": [], "entities": []}, {"text": "In order to ensure the constant attention of the readers and to reduce readers' tiredness factor, the texts were kept short (about 150-170 words each), and the number of texts to be read by the reader was kept to four.", "labels": [], "entities": []}, {"text": "In addition, to ensure comparability, all the texts were selected in away to be more or less of the same length.", "labels": [], "entities": []}, {"text": "The experiment employed a collection of a total of eight texts, four of which original, non simplified ('complex') versions, and the other four -their manually simplified versions.", "labels": [], "entities": []}, {"text": "Each user had to read two complex and two simplified texts, none of which was a variant of the other.", "labels": [], "entities": []}, {"text": "The interface automatically randomized the order of displaying the texts, to ensure that different users would get different combinations of texts in one of the following two different sequences: This was done in order to minimize the impact of the order of displaying the texts on the text comprehension results.", "labels": [], "entities": []}, {"text": "After reading each text, the readers were prompted to answer between four and five questions about each text.", "labels": [], "entities": []}, {"text": "The MCQ method was selected as it is considered being the most objective and easily measurable way of assessing comprehension.", "labels": [], "entities": []}, {"text": "The number of questions and answers was selected in away to not tire the reader (four to five questions per text and four to five answers for each question), and the questions and answers themselves were designed following the the best MCQ practices.", "labels": [], "entities": []}, {"text": "Some of the practices followed involved ensuring that there is only one correct answer per question, making all wrong answers (or 'distractors') grammatically, and as text length consistent with the correct answer, in order to avoid giving hints to the reader, and making all distractors plausible and equally attractive.", "labels": [], "entities": []}, {"text": "Similarly to the texts, the questions and answers were also displayed in different order to different readers, to avoid that the order influences the comprehension results.", "labels": [], "entities": []}, {"text": "The correct answer was displayed in different positions to avoid learning its position and internally marked in away to distinguish it during evaluation from all the distractors in whatever position it was displayed.", "labels": [], "entities": []}, {"text": "The questions required understanding of key aspects of the texts, to avoid relying on pure texts' memorization (such asunder which conditions what was supposed to be done, explanations, and the order in which actions needed to be taken).", "labels": [], "entities": []}, {"text": "The information, evaluating the users' comprehension, collected during the experiment, was, on one hand the time for answering each question, and on the other hand, the number of correct answers given by all participants while replying to the same question.", "labels": [], "entities": []}, {"text": "Besides the fact that we used a specially developed interface, this evaluation approach can be applied to any experiment employing an interface capable of calculating the time for answering and to distinguish the correct answers from the incorrect ones.", "labels": [], "entities": []}, {"text": "The efficiency of the experiment design was thoroughly tested by running it through several rounds of pilot experiments and requiring participants' feedback.", "labels": [], "entities": []}, {"text": "We claim that the evaluation approach proposed in this paper can be applied to more simply organized experiments, as the randomization aspects are not reflected in the evaluation formulae.", "labels": [], "entities": []}, {"text": "The final experiment involved 103 participants, collected via a request sent to several mailing lists.", "labels": [], "entities": []}, {"text": "The participants were 55 percent women and 44 percent male, and ranged from undergraduate students to retired academicians (i.e. corresponded to nineteen to fifty-nine years old).", "labels": [], "entities": []}, {"text": "As the experiment allowed entering lots of personal data, it was also known that participants had a variety of professions (including NLP people, teachers, and lawyers), knew English from the beginner through intermediate, to native level, and spoke a large variety of native languages, allowing to have native speakers from many of the World's language families (Non Indo-European and IndoEuropean included).", "labels": [], "entities": []}, {"text": "shows the coarsegrained classification made at the time of the experiment, and the distribution of participants per native languages.", "labels": [], "entities": []}, {"text": "A subset of specific native language participants will be selected to give an example of applying the evaluation metrics to areal evaluation experiment.", "labels": [], "entities": []}, {"text": "In order to obtain results, we have asked the participants to enter a rich selection of information, and recorded the chosen answer (be it corrector not), and the time which each participant employed to give each answer (correct or wrong).", "labels": [], "entities": []}, {"text": "shows the data we recorded for each single answer of every participant.", "labels": [], "entities": []}, {"text": "The data in is: Entry id is each given answer, the Domain background (answer y -yes and n -no) indicates whether the participant has any previous knowledge of the experiment (crisis management) domain.", "labels": [], "entities": [{"text": "Entry", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9840146899223328}]}, {"text": "As each text, question and com- plex/simplified texts pair are given reference numbers, respectively Text number, Question number, and Texts pair number record that.", "labels": [], "entities": []}, {"text": "As required by the evaluation method, each entry records also the Time to reply each question (measured in 'milliseconds'), and the Answer number.", "labels": [], "entities": [{"text": "Time", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9961208701133728}, {"text": "Answer number", "start_pos": 132, "end_pos": 145, "type": "METRIC", "confidence": 0.9866297245025635}]}, {"text": "As said before, the correct answers are marked in a special way, allowing to distinguish them at a later stage, when counting the number of correct answers.", "labels": [], "entities": []}, {"text": "In order to correctly evaluate the performance of the text simplification method on the basis of the above described experiment, the data obtained was thoughtfully analyzed.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7525184154510498}]}, {"text": "The two criteria selected to best describe the users' performance were time to reply and number of correct answers.", "labels": [], "entities": [{"text": "time", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9875057935714722}]}, {"text": "The evaluation was done offline, after collecting the data from the participants.", "labels": [], "entities": []}, {"text": "The evaluation analysis aimed to test the following two hypotheses: If the text simplification approach has a positive impact on the reading comprehension: 1.", "labels": [], "entities": []}, {"text": "The percentage of correct answers given for the simplified text will be higher than the percentage of correct answers given for the complex text.", "labels": [], "entities": []}, {"text": "2. The time to recognize the correct answer and reply correctly to the questions about the simplified text will be significantly lower than the time to recognize the correct answer and reply correctly to the questions about the complex text.", "labels": [], "entities": []}, {"text": "The two hypotheses were tested previously by employing only the key variables (time to reply and number of correct answers).", "labels": [], "entities": []}, {"text": "It has been proven that comprehension increases with the percentage of correct answers and decreases with the increase of the time to reply.", "labels": [], "entities": [{"text": "comprehension", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.9516109824180603}]}, {"text": "On the basis of these facts, we define the C-Score (a text Comprehension Score) -an objective evaluation metrics, which allows to give a reading comprehension estimate to a text, or to compare two texts or two or more text simplification approaches.", "labels": [], "entities": []}, {"text": "The C-Score is calculated text per text.", "labels": [], "entities": [{"text": "C-Score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8344576954841614}]}, {"text": "In order to address a variety of situations, we propose three versions of the C-Score, which cover, gradually, all possible variables which can affect comprehension in such an experiment.", "labels": [], "entities": []}, {"text": "In the following sections we present their formulae, the variables involved, and discuss their results, advantages and shortcomings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Participant's information recorded for  each answer.", "labels": [], "entities": []}, {"text": " Table 5: C-Score Simple for one text.", "labels": [], "entities": []}]}