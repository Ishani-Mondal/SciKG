{"title": [{"text": "Quality Estimation for Machine Translation Using the Joint Method of Evaluation Criteria and Statistical Modeling", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7943529188632965}]}], "abstractContent": [{"text": "This paper is to introduce our participation in the WMT13 shared tasks on Quality Estimation for machine translation without using reference translations.", "labels": [], "entities": [{"text": "WMT13 shared tasks", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.5347683727741241}, {"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7977844178676605}]}, {"text": "We submitted the results for Task 1.1 (sentence-level quality estimation), Task 1.2 (system selection) and Task 2 (word-level quality estimation).", "labels": [], "entities": [{"text": "sentence-level quality estimation", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.6044696470101675}]}, {"text": "In Task 1.1, we used an enhanced version of BLEU metric without using reference translations to evaluate the translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9794965386390686}]}, {"text": "In Task 1.2, we utilized a probability model Na\u00ef ve Bayes (NB) as a classification algorithm with the features borrowed from the traditional evaluation met-rics.", "labels": [], "entities": []}, {"text": "In Task 2, to take the contextual information into account, we employed a discrimi-native undirected probabilistic graphical model Conditional random field (CRF), in addition to the NB algorithm.", "labels": [], "entities": []}, {"text": "The training experiments on the past WMT corpora showed that the designed methods of this paper yielded promising results especially the statistical models of CRF and NB.", "labels": [], "entities": [{"text": "WMT corpora", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.8087874352931976}]}, {"text": "The official results show that our CRF model achieved the highest F-score 0.8297 in binary classification of Task 2.", "labels": [], "entities": [{"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9991133809089661}]}], "introductionContent": [{"text": "Due to the fast development of Machine translation, different automatic evaluation methods for the translation quality have been proposed in recent years.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8772022128105164}]}, {"text": "One of the categories is the lexical similarity based metric.", "labels": [], "entities": []}, {"text": "This kind of metrics includes the edit distance based method, such as WER (, Multi-reference WER (), PER (, the works of), () and (; the precision based method, such as BLEU (), NIST, and SIA (); recall based method, such as ROUGE (; and the combination of precision and recall, such as GTM (, METE-OR (), BLANC (), AMBER (Chen and Kuhn, 2011), PORT (, and LEPOR (.", "labels": [], "entities": [{"text": "PER", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9780796766281128}, {"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9963939785957336}, {"text": "BLEU", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9954181909561157}, {"text": "NIST", "start_pos": 178, "end_pos": 182, "type": "DATASET", "confidence": 0.7863150835037231}, {"text": "recall", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9852957725524902}, {"text": "ROUGE", "start_pos": 225, "end_pos": 230, "type": "METRIC", "confidence": 0.9568451642990112}, {"text": "precision", "start_pos": 257, "end_pos": 266, "type": "METRIC", "confidence": 0.9979965090751648}, {"text": "recall", "start_pos": 271, "end_pos": 277, "type": "METRIC", "confidence": 0.9887113571166992}, {"text": "BLANC", "start_pos": 306, "end_pos": 311, "type": "METRIC", "confidence": 0.8857319355010986}, {"text": "PORT", "start_pos": 345, "end_pos": 349, "type": "METRIC", "confidence": 0.9387950897216797}, {"text": "LEPOR", "start_pos": 357, "end_pos": 362, "type": "METRIC", "confidence": 0.9914175271987915}]}, {"text": "Another category is the using of linguistic features.", "labels": [], "entities": []}, {"text": "This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA), ( and), phrase information used by and), sentence structure used by; the semantic similarity, such as textual entailment used by and, Synonyms used by METEOR (,,; paraphrase used by).", "labels": [], "entities": [{"text": "POS", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8759271502494812}, {"text": "TESLA", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.7923779487609863}, {"text": "METEOR", "start_pos": 251, "end_pos": 257, "type": "DATASET", "confidence": 0.5136151909828186}]}, {"text": "The traditional evaluation metrics tend to evaluate the hypothesis translation as compared to the reference translations that are usually offered by human efforts.", "labels": [], "entities": [{"text": "hypothesis translation", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7250411659479141}]}, {"text": "However, in the practice, there is usually no golden reference for the translated documents, especially on the internet works.", "labels": [], "entities": []}, {"text": "How to evaluate the quality of automatically translated documents or sentences without using the reference translations becomes anew challenge in front of the NLP researchers.: Developed POS mapping for Spanish and universal tagset 2 Related Works perform a research about reference-free SMT evaluation method on sentence level.", "labels": [], "entities": [{"text": "SMT evaluation", "start_pos": 288, "end_pos": 302, "type": "TASK", "confidence": 0.8922321200370789}]}, {"text": "This work uses both linear and nonlinear combinations of language model and SVM classifier to find the badly translated sentences.", "labels": [], "entities": []}, {"text": "conduct the sentencelevel MT evaluation utilizing the regression learning and based on a set of weaker indicators of fluency and adequacy as pseudo references.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.8307918906211853}]}, {"text": "use the Confidence Estimation features and a learning mechanism trained on human annotations.", "labels": [], "entities": [{"text": "Confidence Estimation", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6753387749195099}]}, {"text": "They show that the developed models are highly biased by difficulty level of the input segment, therefore they are not appropriate for comparing multiple systems that translate the same input segments.", "labels": [], "entities": []}, {"text": "discussed the issues between the traditional machine translation evaluation and the quality estimation tasks recently proposed.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.8631060719490051}]}, {"text": "The traditional MT evaluation metrics require reference translations in order to measure a score reflecting some aspects of its quality, e.g. the BLEU and NIST.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.891044408082962}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9700217247009277}, {"text": "NIST", "start_pos": 155, "end_pos": 159, "type": "DATASET", "confidence": 0.9695966839790344}]}, {"text": "The quality estimation addresses this problem by evaluating the quality of translations as a prediction task and the features are usually extracted from the source sentences and target (translated) sentences.", "labels": [], "entities": []}, {"text": "They also show that the developed methods correlate better with human judgments at segment level as compared to traditional metrics.", "labels": [], "entities": []}, {"text": "perform the MT evaluation using the IBM model one with the information of morphemes, 4-gram POS and lexicon probabilities.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9834226369857788}]}, {"text": "use the cross-lingual textual entailment to push semantics into the MT evaluation without using reference translations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.8898996114730835}]}, {"text": "This evaluation work mainly focuses on the adequacy estimation.", "labels": [], "entities": []}, {"text": "Avramidis (2012) performs an automatic sentence-level ranking of multiple machine translations using the features of verbs, nouns, sentences, subordinate clauses and punctuation occurrences to derive the adequacy information.", "labels": [], "entities": []}, {"text": "Other descriptions of the MT Quality Estimation tasks can be gained in the works of and).", "labels": [], "entities": [{"text": "MT Quality Estimation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8926947712898254}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance on the WMT12 corpus", "labels": [], "entities": [{"text": "WMT12", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9469742774963379}]}, {"text": " Table 3.  The testing results show similar scores as com- pared to the training scores (the MAE score is  around 0.16 and the RMSE score is around 0.22),  which shows a stable performance of the devel- oped model EBLEU. However, the performance  of EBLEU is not satisfactory currently as shown  in the", "labels": [], "entities": [{"text": "MAE score", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.982013076543808}, {"text": "RMSE score", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9861743450164795}, {"text": "EBLEU", "start_pos": 214, "end_pos": 219, "type": "DATASET", "confidence": 0.9019815325737}]}, {"text": " Table 2 and Table 3. This is due to the fact  that we only used the POS information as lin- guistic feature. This could be further improved  by the combination of lexical information and  other linguistic features such as the sentence  structure, phrase similarity, and text entailment.", "labels": [], "entities": []}, {"text": " Table 3: Performance on the WMT13 corpus", "labels": [], "entities": [{"text": "WMT13 corpus", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9613683819770813}]}, {"text": " Table 4: Convert absolute scores into ranks", "labels": [], "entities": [{"text": "Convert absolute", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7494875490665436}]}, {"text": " Table 5: NB-LPR and SVM-LPR training", "labels": [], "entities": []}, {"text": " Table 5. The NB- LPR performs lower scores than the SVM-LPR  but faster than SVM-LPR.", "labels": [], "entities": [{"text": "SVM-LPR", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.8887427449226379}]}, {"text": " Table 6: QE Task 1.2 testing scores", "labels": [], "entities": [{"text": "QE Task 1.2 testing scores", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.6317825436592102}]}, {"text": " Table 9: QE Task 2 official testing scores", "labels": [], "entities": [{"text": "QE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.6229374408721924}]}]}