{"title": [{"text": "Multimodality and Dialogue Act Classification in the RoboHelper Project", "labels": [], "entities": [{"text": "Dialogue Act Classification", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.6701973676681519}]}], "abstractContent": [{"text": "We describe the annotation of a multi-modal corpus that includes pointing gestures and haptic actions (force exchanges).", "labels": [], "entities": []}, {"text": "Haptic actions are rarely analyzed as full-fledged components of dialogue, but our data shows haptic actions are used to advance the state of the interaction.", "labels": [], "entities": []}, {"text": "We report our experiments on recognizing Dialogue Acts in both offline and online modes.", "labels": [], "entities": [{"text": "recognizing Dialogue Acts", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.8827193975448608}]}, {"text": "Our results show that multimodal features and the dialogue game aid in DA classification.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.9792865514755249}]}], "introductionContent": [{"text": "When people collaborate on physical or virtual tasks that involve manipulation of objects, dialogues become rich in gestures of different kinds; the actions themselves that collaborators engage in also perform a communicative function.", "labels": [], "entities": []}, {"text": "Collaborators gesture while speaking, e.g. saying \"Try there?\" while pointing to a faraway location; they perform actions to reply to their partner's utterances, e.g. opening a cabinet to comply with \"please check cabinet number two\".", "labels": [], "entities": []}, {"text": "Conversely, they use utterances to reply to their partner's gestures and actions, e.g. saying \"not there, try the other one\" after their partner opens a cabinet.", "labels": [], "entities": []}, {"text": "Gestures and actions are an important part of such dialogues; while the role of pointing gestures has been explored, the role that haptic actions (force exchanges) play in an interaction has not.", "labels": [], "entities": []}, {"text": "In this paper, we present our corpus of multimodal dialogues in a home care setting: a helper is helping an elderly person perform activities of daily living (ADLs) such as preparing dinner.", "labels": [], "entities": []}, {"text": "We investigate how to apply Dialogue Act (DA) classification to these multimodal dialogues.", "labels": [], "entities": [{"text": "Dialogue Act (DA) classification", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.6081561048825582}]}, {"text": "First, an utterance may not directly follow a spoken utterance, but a gesture or a haptic action.", "labels": [], "entities": []}, {"text": "Likewise, the next move is not necessarily an utterance, it can be a gesture (pointing or haptics) only, or a multimodal utterance.", "labels": [], "entities": []}, {"text": "Third, when people use gestures and actions together with utterances, the utterances become shorter, hence the textual context that has been used to advantage in many previous models is impoverished.", "labels": [], "entities": []}, {"text": "Our contributions concern: exploring the dialogue functions of what we call Haptic-Ostensive (H-O) actions (, namely haptics actions that often perform a referential function; experimenting with both offline and online DA classification, whereas most previous work only focuses on offline classification (; highlighting the role played by multimodal features and dialogue structure (in the form of dialogue games) as concerns DA classification.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 219, "end_pos": 236, "type": "TASK", "confidence": 0.9183761775493622}, {"text": "DA classification", "start_pos": 426, "end_pos": 443, "type": "TASK", "confidence": 0.9392362236976624}]}, {"text": "Our work is part of the RoboHelper project) whose ultimate goal is to deploy robotic assistants for the elderly so that they can safely remain living in their home.", "labels": [], "entities": []}, {"text": "The models we derive from our experiments are the building blocks of a multimodal information-state based dialogue manager, whose architecture is shown in.", "labels": [], "entities": []}, {"text": "The dialogue manager performs reference resolution, specifically resolving third person pronouns and deictics in utterances; classifies utterances to DAs; infers the dialogue games for utterances; updates the dialogue state, and finally decides what the next step is in the interaction.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7504671812057495}]}, {"text": "We have discussed our approach to multimodal reference resolution in.", "labels": [], "entities": [{"text": "multimodal reference resolution", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.667708178361257}]}, {"text": "In this paper, we focus on the Dialogue Act classification component.", "labels": [], "entities": [{"text": "Dialogue Act classification", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7136033773422241}]}, {"text": "We will also touch on Dialogue Game inference.", "labels": [], "entities": [{"text": "Dialogue Game inference", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.7980753978093466}]}, {"text": "Our collaborators are developing the speech processing, vision and haptic recognition components, that, when integrated with the dialogue manager we are building,: System Architecture will make the interface situated in and able to deal with areal environment.", "labels": [], "entities": [{"text": "haptic recognition", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7058836221694946}]}, {"text": "After discussing related work in Section 2, we present our multimodal corpus and the multidimensional annotation scheme we devised in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we discuss all the features we used to build machine learning models to classify DAs.", "labels": [], "entities": []}, {"text": "Sections 5 is devoted to our experiments and the results we obtained.", "labels": [], "entities": []}, {"text": "We conclude and discuss future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran experiments classifying the DA tag for the current utterance.", "labels": [], "entities": [{"text": "DA tag", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.6570442020893097}]}, {"text": "We employ supervised learning approaches, specifically: Conditional Random Field (CRF) (), Maximum Entropy (MaxEnt), Naive Bayes (NB), and Decision Tree (DT).", "labels": [], "entities": [{"text": "Maximum Entropy (MaxEnt)", "start_pos": 91, "end_pos": 115, "type": "METRIC", "confidence": 0.788606834411621}]}, {"text": "These algorithms are widely used for DA classification).", "labels": [], "entities": [{"text": "DA classification", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.9886575639247894}]}, {"text": "We used Mallet) to build CRF models.", "labels": [], "entities": [{"text": "Mallet)", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.9773050248622894}]}, {"text": "MaxEnt models were built using the MaxEnt 3 package from the Apache OpenNLP package.", "labels": [], "entities": []}, {"text": "Naive Bayes and Decision Tree models were built with the Weka () package (for decision trees, we used the J48 implementation).", "labels": [], "entities": []}, {"text": "All the results we will show below were obtained using 10 fold cross validation.", "labels": [], "entities": []}, {"text": "We ran the DA classification experiments with three goals.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.9313099086284637}]}, {"text": "First, we wanted to assess the effectiveness of different types of features, especially, the effectiveness of gesture, H-O action, location and dialogue game features.", "labels": [], "entities": []}, {"text": "Second, we wanted to compare the performances of different machine learning algorithms on such a multimodal dialogue dataset.", "labels": [], "entities": []}, {"text": "Third, we wanted to investigate the performances of different algorithms in the online and offline experiment settings.", "labels": [], "entities": []}, {"text": "The DA classification task could be treated as a sequence labeling problem ().", "labels": [], "entities": [{"text": "DA classification task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9556118448575338}, {"text": "sequence labeling", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.597428023815155}]}, {"text": "However, different from other sequence labeling problems such as part-of-speech tagging, a dialogue system cannot wait until the whole dialogue ends to classify the current DA.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6932061612606049}, {"text": "part-of-speech tagging", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7166105210781097}]}, {"text": "A dialogue system needs online DA classification models to classify the DAs when anew utterance is processed by the system.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9287386536598206}]}, {"text": "There are two differences between online and offline DA classification modes.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.9024918377399445}]}, {"text": "First, when we generate the dialogue history and dialogue game features, we use the previously classified DA tag results for online mode, while we use the gold-standard DA tags for offline mode.", "labels": [], "entities": []}, {"text": "Second, MaxEnt (using beam search) and CRF evaluate and classify all the utterances in a dialogue at the same time in offline mode; however in online mode, MaxEnt and CRF can only work on the partial sequence up to the utterance to classify.", "labels": [], "entities": []}, {"text": "Whereas this may sound obvious, it explains why the performance of these classifiers maybe even more negatively affected in online mode with respect to their offline performance, as compared to other classifiers.", "labels": [], "entities": []}, {"text": "We will see that indeed this will happen for CRF, but not for MaxEnt.", "labels": [], "entities": [{"text": "CRF", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8891075849533081}, {"text": "MaxEnt", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.9816940426826477}]}, {"text": "To evaluate feature effectiveness, we group the features into seven groups: textual features (TX), utterance features (UT), pointing gesture feature (PT), H-O action features (H-O), location features (LO), dialogue game feature (DG), dialogue history features (DH).", "labels": [], "entities": []}, {"text": "Then we generate all the combinations of feature groups to run experiments.", "labels": [], "entities": []}, {"text": "For each classification algorithm, we ran 10-fold cross-validation experiments, for each feature group combination, in both online and offline mode.", "labels": [], "entities": []}, {"text": "It would be impossible to report all our results.", "labels": [], "entities": []}, {"text": "Similarly to (, we report our results with single feature groups and incremental feature group combinations, as shown in Table 5.", "labels": [], "entities": []}, {"text": "Whereas all combinations were tried, the omitted results do not shed any additional light on the problem.", "labels": [], "entities": []}, {"text": "The majority baseline, which always assigns the most frequent tag to every utterance, has an accuracy of 20.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.999602735042572}]}, {"text": "The CRF offline model performs best, which confirms the results of ().", "labels": [], "entities": []}, {"text": "This is due to the strong correlation between dialogue history features (DH) and the states of the CRF.", "labels": [], "entities": []}, {"text": "In online mode, when there is noise in the previous DA tags, the CRF's performance drops significantly (p\u2264.005, using \u03c7 2 ).", "labels": [], "entities": []}, {"text": "A significant drop in performance from offline to online mode also happens to NB (p\u2264.005) and DT (p<.025).", "labels": [], "entities": [{"text": "NB", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.7317065000534058}, {"text": "DT", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.8544212579727173}]}, {"text": "MaxEnt performs very stably, the best online model performs only .015 worse than the best offline model.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9268158078193665}]}, {"text": "The best MaxEnt offline model beats the other algorithms' best models except CRF, while the MaxEnt online model outperforms all the other algorithms' online models.", "labels": [], "entities": [{"text": "CRF", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.7179591655731201}]}, {"text": "Our results thus demonstrate that MaxEnt works best for online DA classification on our data.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8682675361633301}, {"text": "DA classification", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9287787973880768}]}, {"text": "As concerns features, for online models, textual features (TX) are the most predictive as a feature type used by itself.", "labels": [], "entities": []}, {"text": "When we add pointing gesture (PT), H-O features (H-O) and location features (LO) together to textual features, we notice a significant performance improvement for most models (except CRF models).", "labels": [], "entities": []}, {"text": "For MaxEnt, which gives the best results for online models, none of the gesture, H-O action and location features alone significantly improve the results, but all three to-  gether do.", "labels": [], "entities": []}, {"text": "This confirms the finding of () that non-verbal features help DA classification.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.9796639978885651}]}, {"text": "To assess which feature is the most important among those three non-verbal features, we examined the experiment results with a leave-one-out strategy, that is for each classifier in offline and online modes, we leave one of the gesture, H-O and location features out from the full experiment feature set (TX+PT+H-O+LO+UT+DG+DH).", "labels": [], "entities": []}, {"text": "No significant difference was discovered.", "labels": [], "entities": []}, {"text": "When the dialogue game features (DG) are added to the models, performance increases significantly for CRF offline model (p<.005), MaxEnt offline (p<.005) and online (p<.05) models, NB online model (p<.05) and DT online model (p<.005).", "labels": [], "entities": []}, {"text": "It confirms previous findings, including by our group, that dialogue game features (DG) play a very important role in DA classification, even via the simple approximation we used.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.9884010553359985}]}, {"text": "When the dialogue history features (DH) are added to the models, performance increased significantly for all the offline models and the MaxEnt online model, with p<.005.", "labels": [], "entities": []}, {"text": "This confirms previous findings that dialogue history helps with DA classification.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.9859753847122192}]}], "tableCaptions": [{"text": " Table 1: Find Subcorpus: Length in seconds", "labels": [], "entities": [{"text": "Length", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9919918179512024}]}, {"text": " Table 3. More than 90% of pointing gestures are  used with utterances. Only 377 out of 596 H-O ac- tions are included in the moves, mostly because the  H-O action \"Close\" frequently follows an \"Open\"  action (these cases are not detected by the algo- rithm, because they don't advance the dialogue).", "labels": [], "entities": []}, {"text": " Table 3: Moves Statistics in Find Corpus", "labels": [], "entities": [{"text": "Moves", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.9728893637657166}, {"text": "Find Corpus", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.6677822768688202}]}, {"text": " Table 4: Dialogue Act Counts in Find Corpus", "labels": [], "entities": [{"text": "Find Corpus", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9604935050010681}]}]}