{"title": [{"text": "Evaluating State Representations for Reinforcement Learning of Turn-Taking Policies in Tutorial Dialogue", "labels": [], "entities": [{"text": "Evaluating State Representations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8188894192377726}, {"text": "Reinforcement Learning of Turn-Taking Policies in Tutorial Dialogue", "start_pos": 37, "end_pos": 104, "type": "TASK", "confidence": 0.7386237531900406}]}], "abstractContent": [{"text": "Learning and improving natural turn-taking behaviors for dialogue systems is a topic of growing importance.", "labels": [], "entities": []}, {"text": "In task-oriented dialogue where the user can engage in task actions in parallel with dialogue, unrestricted turn taking maybe particularly important for dialogue success.", "labels": [], "entities": []}, {"text": "This paper presents a novel Markov Decision Process (MDP) representation of dialogue with unrestricted turn taking and a parallel task stream in order to automatically learn effective turn-taking policies fora tutorial dialogue system from a corpus.", "labels": [], "entities": []}, {"text": "It also presents and evaluates an approach to automatically selecting features for an MDP state representation of this dialogue.", "labels": [], "entities": []}, {"text": "The results suggest that the MDP formulation and the feature selection framework hold promise for learning effective turn-taking policies in task-oriented dialogue systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Determining when to make a dialogue move is a topic of growing importance in dialogue systems.", "labels": [], "entities": []}, {"text": "While systems historically relied on explicit turntaking cues, more recent work has focused on learning and improving on natural turn-taking behaviors).", "labels": [], "entities": []}, {"text": "For tutorial dialogue in particular, effectively timing system moves can substantially impact the success of the dialogue.", "labels": [], "entities": []}, {"text": "For example, failing to provide helpful feedback to a student who is confused may lead to decreased learning) or to disengagement (Forbes-Riley and Litman 2012), while providing tutorial feedback or interventions at inappropriate times could also have a negative impact on the outcome of the dialogue (D'.", "labels": [], "entities": [{"text": "D'.", "start_pos": 302, "end_pos": 305, "type": "METRIC", "confidence": 0.8825271725654602}]}, {"text": "Reinforcement Learning (RL) is a widely used approach to constructing effective dialogue policies using either MDPs or POMDPs (.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7907045722007752}]}, {"text": "To date, RL has been applied to learn the most effective dialogue move to make, but has not been applied to learning the timings of these moves, although the related concept of when to release a turn has been explored.", "labels": [], "entities": [{"text": "RL", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.8230888247489929}]}, {"text": "The domain of tutorial dialogue poses an additional modeling challenge: the dialogue is task-oriented, but unlike many task-oriented dialogues in which all information is communicated via dialogue, students solve problems within a separate task stream which conveys essential information for dialogue management decisions.", "labels": [], "entities": []}, {"text": "This paper addresses dialogue with both unrestricted turn taking and a parallel task stream with a novel Markov Decision Process representation.", "labels": [], "entities": [{"text": "turn taking", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.7228434979915619}]}, {"text": "Because turn boundaries are not clearly defined or enforced, we apply RL to the problem of when to make a dialogue move, rather than what type of dialogue move to make.", "labels": [], "entities": []}, {"text": "In order to determine which criteria are most relevant to making this decision, the approach utilizes a feature selection approach based on anew Separation Ratio metric and compares the selected features against an existing approach based on expected cumulative reward ().", "labels": [], "entities": []}, {"text": "Finally, the resulting feature spaces are evaluated with simulated users acquired in a supervised fashion from held-out portions of the corpus.", "labels": [], "entities": []}, {"text": "The results inform the development of turn-taking policies in task-oriented dialogue systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "A series of simulated dialogues was used to evaluate the two resulting feature spaces via the policies derived using them.", "labels": [], "entities": []}, {"text": "These simulations were based on five-fold cross-validation, as in prior work (, with policies trained on four of the five folds and simulated users learned from the remaining fold.", "labels": [], "entities": []}, {"text": "As noted above, the rewards in the MDP were based on student learning gain, but learning gain (like user satisfaction in other dialogue domains) is not directly observable during the dialogues.", "labels": [], "entities": []}, {"text": "However, we found that students in the high learning gain group had fewer non-zero task actions (actions that changed the edit distance to the final task solution) than students in the low learning gain group (p < 0.05).", "labels": [], "entities": []}, {"text": "Therefore, number of non-zero task actions is used as a measure of dialogue success, with lower numbers being better.", "labels": [], "entities": []}, {"text": "We derived the average change in edit distance on each state transition from the testing folds, and defined that a simulated dialogue would end when the edit distance reached zero (i.e., the student arrived at the correct solution).", "labels": [], "entities": []}, {"text": "shows the results of running 5,000 simulations in each fold for both the learned policy and for an anti-policy where each decision was reversed.", "labels": [], "entities": []}, {"text": "The anti-policy is included to provide a point of comparison for the policies learned in each feature space, and offers insight into the quality of the learned policies, similar to the inverse policies learned in prior work ().", "labels": [], "entities": []}, {"text": "The table shows that the learned policies in the ECR feature space had slightly better results overall (lower number of non-zero task actions), while the SR feature space had larger separation between the learned policies and antipolicies.", "labels": [], "entities": [{"text": "ECR feature space", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.8938008745511373}, {"text": "SR feature space", "start_pos": 154, "end_pos": 170, "type": "DATASET", "confidence": 0.7905782063802084}]}, {"text": "These results suggest that feature selection based on SR was able to identify important decision criteria with only a minor decrease in reward compared to ECR..", "labels": [], "entities": [{"text": "SR", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9421394467353821}]}, {"text": "Results of simulated dialogues (lower non-zero task action count is better)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Results of simulated dialogues (lower  non-zero task action count is better)", "labels": [], "entities": []}]}