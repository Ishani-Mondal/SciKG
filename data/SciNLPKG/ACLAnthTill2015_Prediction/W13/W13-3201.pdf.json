{"title": [{"text": "Vector Space Semantic Parsing: A Framework for Compositional Vector Space Models", "labels": [], "entities": [{"text": "Vector Space Semantic Parsing", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7553464025259018}]}], "abstractContent": [{"text": "We present vector space semantic parsing (VSSP), a framework for learning compo-sitional models of vector space semantics.", "labels": [], "entities": [{"text": "vector space semantic parsing (VSSP)", "start_pos": 11, "end_pos": 47, "type": "TASK", "confidence": 0.7524827974183219}]}, {"text": "Our framework uses Combinatory Categorial Grammar (CCG) to define a correspondence between syntactic categories and semantic representations, which are vectors and functions on vectors.", "labels": [], "entities": []}, {"text": "The complete correspondence is a direct consequence of minimal assumptions about the semantic representations of basic syntactic categories (e.g., nouns are vectors), and CCG's tight coupling of syntax and semantics.", "labels": [], "entities": []}, {"text": "Furthermore, this correspondence permits nonuniform semantic representations and more expressive composition operations than previous work.", "labels": [], "entities": []}, {"text": "VSSP builds a CCG semantic parser respecting this correspondence; this semantic parser parses text into lambda calculus formulas that evaluate to vector space representations.", "labels": [], "entities": [{"text": "VSSP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9158272743225098}]}, {"text": "In these formulas, the meanings of words are represented by parameters that can be trained in a task-specific fashion.", "labels": [], "entities": []}, {"text": "We present experiments using noun-verb-noun and adverb-adjective-noun phrases which demonstrate that VSSP can learn composition operations that RNN (Socher et al., 2011) and MV-RNN (Socher et al., 2012) cannot.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models represent the semantics of natural language using vectors and operations on vectors).", "labels": [], "entities": []}, {"text": "These models are most commonly used for individual words and short phrases, where vectors are created using distributional information from a corpus.", "labels": [], "entities": []}, {"text": "Such models achieve impressive performance on standardized tests, correlate well with human similarity judgments (, and have been successfully applied to a number of natural language tasks).", "labels": [], "entities": []}, {"text": "While vector space representations for individual words are well-understood, there remains much uncertainty about how to compose vector space representations for phrases out of their component words.", "labels": [], "entities": []}, {"text": "Recent work in this area raises many important theoretical questions.", "labels": [], "entities": []}, {"text": "For example, should all syntactic categories of words be represented as vectors, or are some categories, such as adjectives, different?", "labels": [], "entities": []}, {"text": "Using distinct semantic representations for distinct syntactic categories has the advantage of representing the operational nature of modifier words, but the disadvantage of more complex parameter estimation.", "labels": [], "entities": []}, {"text": "Also, does semantic composition factorize according to a constituency parse tree)?", "labels": [], "entities": []}, {"text": "A binarized constituency parse cannot directly represent many intuitive intra-sentence dependencies, such as the dependence between a verb's subject and its object.", "labels": [], "entities": []}, {"text": "What is needed to resolve these questions is a comprehensive theoretical framework for compositional vector space models.", "labels": [], "entities": []}, {"text": "In this paper, we observe that we already have such a framework: Combinatory Categorial Grammar (CCG).", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar (CCG)", "start_pos": 65, "end_pos": 101, "type": "TASK", "confidence": 0.7289100388685862}]}, {"text": "CCG provides a tight mapping between syntactic categories and semantic types.", "labels": [], "entities": [{"text": "CCG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8664129972457886}]}, {"text": "If we assume that nouns, sentences, and other basic syntactic categories are represented by vectors, this mapping prescribes semantic types for all other syntactic categories.", "labels": [], "entities": []}, {"text": "For example, we get that adjectives are functions from noun vectors to noun vectors, and that prepo- sitions are functions from a pair of noun vectors to a noun vector.", "labels": [], "entities": []}, {"text": "These semantic type specifications permit a variety of different composition operations, many of which cannot be represented in previously-proposed frameworks.", "labels": [], "entities": []}, {"text": "Parsing in CCG applies these functions to each other, naturally deriving a vector space representation for an entire phrase.", "labels": [], "entities": []}, {"text": "The CCG framework provides function type specifications for each word's semantics, given its syntactic category.", "labels": [], "entities": []}, {"text": "Instantiating this framework amounts to selecting particular functions for each word.", "labels": [], "entities": []}, {"text": "Vector space semantic parsing (VSSP) produces these per-word functions in a two-step process.", "labels": [], "entities": [{"text": "Vector space semantic parsing (VSSP)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8141424655914307}]}, {"text": "The first step chooses a parametric functional form for each syntactic category, which contains as-yet unknown per-word and global parameters.", "labels": [], "entities": []}, {"text": "The second step estimates these parameters using a concrete task of interest, such as predicting the corpus statistics of adjective-noun compounds.", "labels": [], "entities": []}, {"text": "We present a stochastic gradient algorithm for this step which resembles training a neural network with backpropagation.", "labels": [], "entities": []}, {"text": "These parameters may also be estimated in an unsupervised fashion, for example, using distributional statistics.", "labels": [], "entities": []}, {"text": "presents an overview of VSSP.", "labels": [], "entities": [{"text": "VSSP", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.5313371419906616}]}, {"text": "The input to VSSP is a natural language phrase and a lexicon, which contains the parametrized functional forms for each word.", "labels": [], "entities": [{"text": "VSSP", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8247035145759583}]}, {"text": "These per-word representations are combined by CCG semantic parsing to produce a logical form, which is a symbolic mathematical formula for producing the vector fora phrase -for example, A red v ball is a formula that performs matrix-vector multiplication.", "labels": [], "entities": [{"text": "CCG semantic parsing", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7541297078132629}]}, {"text": "This formula is evaluated using learned per-word and global parameters (values for A red and v ball ) to produce the language's vector space representation.", "labels": [], "entities": []}, {"text": "The contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, we demonstrate how CCG provides a theoretical basis for vector space models.", "labels": [], "entities": []}, {"text": "Second, we describe VSSP, which is a method for concretely instantiating this theoretical framework.", "labels": [], "entities": [{"text": "VSSP", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.40116584300994873}]}, {"text": "Finally, we perform experiments comparing VSSP against other compositional vector space models.", "labels": [], "entities": [{"text": "VSSP", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6315301060676575}]}, {"text": "We perform two case studies of composition using noun-verb-noun and adverb-adjective-noun phrases, finding that VSSP can learn composition operations that existing models cannot.", "labels": [], "entities": [{"text": "VSSP", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.7474252581596375}]}, {"text": "We also find that VSSP produces intuitively reasonable parameters.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Data for propositional logic experiment.", "labels": [], "entities": [{"text": "propositional logic experiment", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7741848230361938}]}, {"text": " Table 4: Training error on the propositional logic  data set. VSSP achieves zero error because its  verb representation can learn arbitrary logical op- erations.", "labels": [], "entities": [{"text": "VSSP", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.6451934576034546}]}, {"text": " Table 5: Data for adverb-adjective-noun compo- sition experiment. Higher first dimension values  represent larger objects.", "labels": [], "entities": []}, {"text": " Table 6: Training error of each composition model  on the adverb-adjective-noun experiment.", "labels": [], "entities": [{"text": "Training error", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9417364597320557}]}]}