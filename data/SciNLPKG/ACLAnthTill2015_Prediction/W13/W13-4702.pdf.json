{"title": [{"text": "LexToPlus: A Thai Lexeme Tokenization and Normalization Tool", "labels": [], "entities": [{"text": "Thai Lexeme Tokenization", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.7903404235839844}]}], "abstractContent": [{"text": "The increasing popularity of social media has a large impact on the evolution of language usage.", "labels": [], "entities": []}, {"text": "The evolution includes the transformation of some existing terms to enhance the expression of the writer's emotion and feeling.", "labels": [], "entities": []}, {"text": "Text processing tasks on social media texts have become much more challenging.", "labels": [], "entities": [{"text": "Text processing tasks on social media texts", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8504463859966823}]}, {"text": "In this paper, we propose LexToPlus, a Thai lexeme tokenizer with term normalization process.", "labels": [], "entities": [{"text": "LexToPlus", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.8564808964729309}]}, {"text": "Lex-ToPlus is designed to handle the intentional errors caused by the repeated characters at the end of words.", "labels": [], "entities": [{"text": "Lex-ToPlus", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9009174108505249}]}, {"text": "LexToPlus is a dictionary-based parser which detects existing terms in a dictionary.", "labels": [], "entities": [{"text": "LexToPlus", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9273552298545837}]}, {"text": "Unknown tokens with repeated characters are merged and removed.", "labels": [], "entities": []}, {"text": "We performed statistical analysis and evaluated the performance of the proposed approach by using a Twit-ter corpus.", "labels": [], "entities": [{"text": "Twit-ter corpus", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.9461590349674225}]}, {"text": "The experimental results show that the proposed algorithm yields an accuracy of 96.3% on a test data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9995324611663818}]}, {"text": "The errors are mostly caused by the out-of-vocabulary problem which can be solved by adding newly found terms into the dictionary .", "labels": [], "entities": []}], "introductionContent": [{"text": "Thailand is among the top countries having a large population on social networking websites such as Facebook and Twitter.", "labels": [], "entities": []}, {"text": "The recent statistics show that the number of social media users in Thailand has reached 18 millions (approximately 25% of the total population) as of the first quarter of 2013 . In addition to the enormous amount of texts being created daily, another challenging issue is the language usage on social media is much different from the traditional and formal language.", "labels": [], "entities": []}, {"text": "Social media texts include chat message, SMS, comments and posts.", "labels": [], "entities": []}, {"text": "These texts are usually short and noisy, i.e., contain some ill-formed, out-ofvocabulary, abbreviated, transliterated and homophonic transformed terms.", "labels": [], "entities": []}, {"text": "These special characteristics are due to many reasons including inconvenience in typing on virtual keyboards of smartphones and intentional transformation of existing terms to better express the emotion and feeling of the writers.", "labels": [], "entities": []}, {"text": "As a result, performing basic text processing tasks such as term tokenization has become much more challenging.", "labels": [], "entities": [{"text": "term tokenization", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7415148019790649}]}, {"text": "Tokenizing Thai written texts is more difficult than languages in which word boundary markers are placed between words.", "labels": [], "entities": [{"text": "Tokenizing Thai written texts", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9086545258760452}]}, {"text": "Thai language is considered as an unsegmented language in which words are written continuously without the use of word delimiters.", "labels": [], "entities": []}, {"text": "Word segmentation is considered a basic yet very important NLP task in many unsegmented languages.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6947081238031387}]}, {"text": "The main goal of word segmentation task is to assign correct word boundaries on given text strings.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7659568786621094}]}, {"text": "Previous approaches applied to Thai word segmentation can be broadly classified as dictionary-based and machine learning.", "labels": [], "entities": [{"text": "Thai word segmentation", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7339798808097839}]}, {"text": "The dictionary-based approach relies on a set of terms from a dictionary for parsing and segmenting input texts into word tokens.", "labels": [], "entities": [{"text": "parsing and segmenting input texts into word tokens", "start_pos": 77, "end_pos": 128, "type": "TASK", "confidence": 0.82932198792696}]}, {"text": "During the parsing process, series of characters are looked upon the dictionary for matching terms.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9710242748260498}]}, {"text": "The performance of the dictionary-based approach depends on the quality and size of the word set in the dictionary.", "labels": [], "entities": []}, {"text": "Recent works in Thai word segmentation have adopted machine learning algorithms.", "labels": [], "entities": [{"text": "Thai word segmentation", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7020458479722341}]}, {"text": "The machine learning approach relies on a model trained from a corpus by using sequential labeling algorithms.", "labels": [], "entities": []}, {"text": "Using the annotated corpus in which word boundaries are explicitly marked with a special character, the algorithm could be applied to train a model based on the features (e.g., character types) surrounding these boundaries.", "labels": [], "entities": []}, {"text": "The errors caused during the tokenization process can be categorized into two classes, unintentional and intentional.", "labels": [], "entities": [{"text": "tokenization process", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.9045226275920868}]}, {"text": "The unintentional errors are the typographical errors caused by careless typing).", "labels": [], "entities": []}, {"text": "This type of errors has been rigorously studied in the area of word editing and optical character recognition (OCR).", "labels": [], "entities": [{"text": "word editing", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.8191649317741394}, {"text": "optical character recognition (OCR)", "start_pos": 80, "end_pos": 115, "type": "TASK", "confidence": 0.763109510143598}]}, {"text": "There are three cases of typographical errors: insertion, deletion and transposition.", "labels": [], "entities": [{"text": "insertion", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.8062096834182739}]}, {"text": "Insertion error is caused by additional characters in a word.", "labels": [], "entities": [{"text": "Insertion error", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.6399442702531815}]}, {"text": "Deletion error is caused by missing characters in a word.", "labels": [], "entities": []}, {"text": "Transposition error are caused by swapping of characters in the adjacent positions.", "labels": [], "entities": []}, {"text": "shows some examples of Thai word errors for all cases.", "labels": [], "entities": []}, {"text": "The correct spellings are shown in parentheses with translations.: Unintentional spelling error types and examples The scope of this paper does not include the unintentional errors which have been well studied.", "labels": [], "entities": [{"text": "Unintentional spelling error types", "start_pos": 67, "end_pos": 101, "type": "METRIC", "confidence": 0.7981725931167603}]}, {"text": "Instead we focus on intentional errors, i.e., words in which users intentionally create and type.", "labels": [], "entities": []}, {"text": "Based on our preliminary study, the intentional errors can be classified into four categories: insertion, transformation, transliteration and onomatopoeia.", "labels": [], "entities": []}, {"text": "The intentional insertion error is caused by typing repeated characters at the end of a word to emphasize the emotion or feeling.", "labels": [], "entities": []}, {"text": "The transformation error is caused by alteration of existing terms and can be categorized into two subtypes: homophonic and syllable trimming.", "labels": [], "entities": [{"text": "syllable trimming", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7082778066396713}]}, {"text": "The homophonic terms refer to terms with the same or similar pronunciation to existing terms.", "labels": [], "entities": []}, {"text": "The syllable trimming is a transformed term by deleting one or more syllables from an existing term for the purpose of reducing the keystrokes.", "labels": [], "entities": [{"text": "syllable trimming", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7406090497970581}]}, {"text": "The transliterated terms are created by using the Thai character set to create new terms from other languages.", "labels": [], "entities": []}, {"text": "The last intentional error is the onomatopoeia terms which are created to phonetically imitate various sounds.", "labels": [], "entities": []}, {"text": "In this paper, we propose a solution for tokenizing and normalizing texts with the intentional insertion errors, i.e., users insert repeated characters at the end of words.", "labels": [], "entities": []}, {"text": "The statistics on a 2-million Twitter corpus show that this type of errors accounts for approximately 4.8% of corpus size.", "labels": [], "entities": []}, {"text": "Our proposed method is a longest matching dictionary-based approach with a rule-based normalization process.", "labels": [], "entities": []}, {"text": "From our initial evaluation, the dictionary-based approach can handle the case of repeated characters better than the machinelearning based.", "labels": [], "entities": []}, {"text": "More analysis and discussion will be given in the paper.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In next section, we review some related works in word segmentation, text tokenization and term normalization for both segmented and unsegmented languages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7630950808525085}, {"text": "text tokenization", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7159459590911865}, {"text": "term normalization", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7269707918167114}]}, {"text": "In Section 3, we first give a formal definition of the tokenization task.", "labels": [], "entities": [{"text": "tokenization task", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.9241647124290466}]}, {"text": "Then we present the proposed algorithm for implementing LexToPlus.", "labels": [], "entities": [{"text": "LexToPlus", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.8901432752609253}]}, {"text": "In Section 4, we give the performance evaluation by using a data set collected from Twitter.", "labels": [], "entities": []}, {"text": "Some examples of errors are presented with some discussion.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper with the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of the proposed approach, we perform an experiment by using a set of 1,000 randomly selected Twitter posts written in Thai language.", "labels": [], "entities": []}, {"text": "Each post contains some repeated characters and is manually assigned with correct word boundary markers.", "labels": [], "entities": []}, {"text": "For the proposed DCB-Norm algorithm, we use a lexicon from LEXiTRON 2 which contains 35,328 general terms.", "labels": [], "entities": []}, {"text": "We also include another lexicon consisting of 1,341 terms frequently found in Twitter corpus.", "labels": [], "entities": []}, {"text": "Words obtained from Twitter lexicon include chat, slangs and transliterated words from other languages.", "labels": [], "entities": []}, {"text": "The performance evaluation is carried out on a notebook with a 2 GHz Intel Core 2 Duo CPU, 4 GB RAM running under Mac OS X.", "labels": [], "entities": []}, {"text": "We evaluate the algorithm in terms of accuracy, i.e., the number 2 LEXiTRON, http://lexitron.nectec.or.th of correctly tokenized texts over the total number of test texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9993903636932373}, {"text": "LEXiTRON", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9756050109863281}]}, {"text": "We also evaluate the running time efficiency.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "The overall accuracy is equal to 96.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9998339414596558}]}, {"text": "To analyze the errors, we manually look at the incorrectly tokenized results.", "labels": [], "entities": []}, {"text": "We observe that in the case of all words in the text are in the dictionary, the words are recognized and the repeated characters are correctly removed.", "labels": [], "entities": []}, {"text": "However, the problem is mostly due to out-of-vocabulary (OOV) which causes the incorrect assignment of word boundary markers.", "labels": [], "entities": []}, {"text": "As a result, the words with repeated characters at the end could be not normalized correctly.", "labels": [], "entities": []}, {"text": "Two error types associated with OOV problem is homophonic transformation and transliteration.", "labels": [], "entities": [{"text": "homophonic transformation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.7040028125047684}]}, {"text": "shows some examples from the error analysis.", "labels": [], "entities": []}, {"text": "The simplest solution to the OOV problem is to manually collect newly created terms from the corpus.", "labels": [], "entities": [{"text": "OOV", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9248939752578735}]}], "tableCaptions": []}