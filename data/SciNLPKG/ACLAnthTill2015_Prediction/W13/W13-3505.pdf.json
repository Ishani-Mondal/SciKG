{"title": [{"text": "Graph-Based Posterior Regularization for Semi-Supervised Structured Prediction", "labels": [], "entities": [{"text": "Graph-Based Posterior Regularization", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.44682250420252484}, {"text": "Structured Prediction", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.6943216323852539}]}], "abstractContent": [{"text": "We present a flexible formulation of semi-supervised learning for structured models , which seamlessly incorporates graph-based and more general supervision by extending the posterior regularization (PR) framework.", "labels": [], "entities": []}, {"text": "Our extension allows for any regularizer that is a convex, differentiable function of the appropriate marginals.", "labels": [], "entities": []}, {"text": "We show that surprisingly, non-linearity of such regularization does not increase the complexity of learning, provided we use multiplicative updates of the structured ex-ponentiated gradient algorithm.", "labels": [], "entities": []}, {"text": "We illustrate the extended framework by learning conditional random fields (CRFs) with quadratic penalties arising from a graph Laplacian.", "labels": [], "entities": []}, {"text": "On sequential prediction tasks of handwriting recognition and part-of-speech (POS) tagging, our method makes significant gains over strong baselines.", "labels": [], "entities": [{"text": "sequential prediction", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7347380816936493}, {"text": "handwriting recognition", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.9056167602539062}, {"text": "part-of-speech (POS) tagging", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6118528246879578}]}], "introductionContent": [{"text": "Recent success of graph-based semi-supervised learning builds on access to plentiful unsupervised data and accurate similarity measures between data examples (;;;).", "labels": [], "entities": []}, {"text": "Many approaches, such as and use graph-based learning in the transductive setting, where unlabeled examples are classified without learning a parametric predictive model.", "labels": [], "entities": []}, {"text": "While predicted labels can then be leveraged to learn such a model (e.g. a CRF), this pipelined approach misses out on the benefits of modeling sequential correlations during graph propagation.", "labels": [], "entities": []}, {"text": "In this work we seek to better integrate graph propagation with estimation of a structured, parametric predictive model.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7362577021121979}]}, {"text": "To do so, we build on the posterior regularization (PR) framework of.", "labels": [], "entities": []}, {"text": "PR is a principled means of providing weak supervision during structured model estimation.", "labels": [], "entities": [{"text": "PR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8581337332725525}, {"text": "structured model estimation", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.5780414640903473}]}, {"text": "More concretely, PR introduces a penalty whenever the model's posteriors over latent variables contradict the specified weak supervision.", "labels": [], "entities": [{"text": "PR", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.8697143793106079}]}, {"text": "show how to efficiently optimize a likelihood-plus-posterior-penalty type objective in the case where the penalty is linear in the model's marginals.", "labels": [], "entities": []}, {"text": "Yet, there are many forms of supervision that cannot be expressed as a linear function of marginals.", "labels": [], "entities": []}, {"text": "For example, graph Laplacian regularization.", "labels": [], "entities": []}, {"text": "In this work, we extend PR to allow for penalties expressed as any convex, differentiable function of the marginals and derive an efficient optimization method for such penalties.", "labels": [], "entities": [{"text": "PR", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.944284200668335}]}, {"text": "In our experiments, we explore graph Laplacian posterior regularizers for two applications: handwriting recognition and POS tagging.", "labels": [], "entities": [{"text": "handwriting recognition", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.9297692477703094}, {"text": "POS tagging", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.8376681208610535}]}, {"text": "The methods of,, and are the most closely related to this work.", "labels": [], "entities": []}, {"text": "describes coupling a graph regularizer with a maxmargin objective for pitch accent prediction and handwriting recognition tasks.", "labels": [], "entities": [{"text": "pitch accent prediction", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.6306379238764445}, {"text": "handwriting recognition tasks", "start_pos": 98, "end_pos": 127, "type": "TASK", "confidence": 0.90257328748703}]}, {"text": "Their method suffers from scalability issues though; it relies on optimization in the dual, which requires inversion of a matrix whose dimension grows with graph size.", "labels": [], "entities": []}, {"text": "The more recent work of tackles the POS tagging task and provides a more scalable method.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.8867896397908529}]}, {"text": "Their method is a multi-step procedure that iterates two main steps, graph propagation and likelihood optimization, until convergence.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7308784276247025}, {"text": "likelihood optimization", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7231611013412476}]}, {"text": "Actually computing the optimum for the graph propagation step would require a matrix inversion similar to that used by, but they skirt this issue by using an heuristic update rule.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7614555954933167}]}, {"text": "Unfortunately though, no guarantees for the quality of this update are established.", "labels": [], "entities": []}, {"text": "proceed very similarly, adapting the iterative procedure to include supervision from bi-text data, but applying the same heuristic update rule.", "labels": [], "entities": []}, {"text": "The work we present here similarly avoids the complexity of a large matrix inversion and iterates steps related to graph propagation and likelihood optimization.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.7297757267951965}, {"text": "likelihood optimization", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.6884072124958038}]}, {"text": "But in contrast to and it comes with guarantees for the optimality of each step and convergence of the overall procedure.", "labels": [], "entities": [{"text": "convergence", "start_pos": 84, "end_pos": 95, "type": "METRIC", "confidence": 0.9576311111450195}]}, {"text": "Further, our approach is based on optimizing a joint objective, which affords easier analysis and extensions using other constraints or optimization methods.", "labels": [], "entities": []}, {"text": "The key enabling insight is a surprising factorization of the non-linear regularizer, which can be exploited using multiplicative updates.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effect of a graph Laplacian PR penalty on two different sequence prediction tasks: part-of-speech (POS) tagging and handwriting recognition.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7006519436836243}, {"text": "part-of-speech (POS) tagging", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.613408362865448}, {"text": "handwriting recognition", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.9112565219402313}]}, {"text": "Our experiments are conducted in a semi-supervised setting, where only a small number, l, of labeled sequences are available during training.", "labels": [], "entities": []}, {"text": "Both the l labeled sequences and the remainder of the dataset (instances l + 1 through n) are used to construct a graph Laplacian 2 . We train a second-order CRF using the methods described in Section 3 and report results fora test set consisting of instances l + 1 through n.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Handwriting recognition errors.", "labels": [], "entities": [{"text": "Handwriting recognition", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8850682973861694}]}]}