{"title": [{"text": "Feature Space Selection and Combination for Native Language Identification", "labels": [], "entities": [{"text": "Feature Space Selection", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5462639530499777}, {"text": "Native Language Identification", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6867886583010355}]}], "abstractContent": [{"text": "We decribe the submissions made by the National Research Council Canada to the Native Language Identification (NLI) shared task.", "labels": [], "entities": [{"text": "National Research Council Canada", "start_pos": 39, "end_pos": 71, "type": "DATASET", "confidence": 0.9125848263502121}, {"text": "Native Language Identification (NLI) shared task", "start_pos": 79, "end_pos": 127, "type": "TASK", "confidence": 0.8306479975581169}]}, {"text": "Our submissions rely on a Support Vector Machine classifier, various feature spaces using a variety of lexical, spelling, and syntactic features, and on a simple model combination strategy relying on a majority vote between classifiers.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, a clas-sifier relying on purely lexical features performed very well and proved difficult to out-perform significantly using various combinations of feature spaces.", "labels": [], "entities": []}, {"text": "However, the combination of multiple predictors allowed to exploit their different strengths and provided a significant boost in performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe the National Research Council Canada's submissions to the Native Language Identification 2013 shared task.", "labels": [], "entities": [{"text": "National Research Council Canada", "start_pos": 16, "end_pos": 48, "type": "DATASET", "confidence": 0.9287013113498688}, {"text": "Native Language Identification 2013 shared task", "start_pos": 70, "end_pos": 117, "type": "TASK", "confidence": 0.7364959468444189}]}, {"text": "Our submissions rely on fairly straightforward statistical modelling techniques, applied to various feature spaces representing lexical and syntactic information.", "labels": [], "entities": []}, {"text": "Our most successful submission was actually a combination of models trained on different sets of feature spaces using a simple majority vote.", "labels": [], "entities": []}, {"text": "Much of the work on Natural Language Processing is motivated by the desire to have machines that can help or replace humans on language-related tasks.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6622903048992157}]}, {"text": "Many tasks such as topic or genre classification, entity extraction, disambiguation, are fairly straightforward for humans to complete.", "labels": [], "entities": [{"text": "topic or genre classification", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6913657486438751}, {"text": "entity extraction", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.8379891812801361}]}, {"text": "Machines typically trade-off some performance for ease of application and reduced cost.", "labels": [], "entities": []}, {"text": "Equally fascinating are tasks that seem non-trivial to humans, but on which machines, through appropriate statistical analysis, discover regularities and dependencies that are far from obvious to humans.", "labels": [], "entities": []}, {"text": "Examples may include categorizing text by author gender ( or detecting whether a text is an original or a translation ().", "labels": [], "entities": [{"text": "detecting whether a text is an original or a translation", "start_pos": 61, "end_pos": 117, "type": "TASK", "confidence": 0.6900430798530579}]}, {"text": "This is one motivation for addressing the problem of identifying the native language of an author in this shared task.", "labels": [], "entities": []}, {"text": "In the following section, we describe various aspects of the models and features we used on this task.", "labels": [], "entities": []}, {"text": "In section 3, we describe our experimental settings and summarize the results we obtained.", "labels": [], "entities": []}, {"text": "We discuss and conclude in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe the experimental setting that we used to prepare our submissions, and the final performance we obtained on the shared task).", "labels": [], "entities": []}, {"text": "In order to test the performance of various choices of feature spaces and their combination, we setup a cross-validation experimental setting.", "labels": [], "entities": []}, {"text": "We originally sampled 9 equal sized disjoint folds of 1100 documents each from the training data.", "labels": [], "entities": []}, {"text": "We used stratified sampling across the languages and the prompts.", "labels": [], "entities": []}, {"text": "This made sure that the folds respected the uniform distribution across languages, as well as the distribution across prompts, which was slightly uneven for some languages.", "labels": [], "entities": []}, {"text": "These 9 folds were later augmented with a 10th fold containing the development data released during the evaluation.", "labels": [], "entities": []}, {"text": "All systems were evaluated by computing the accuracy (or equivalently the micro-averaged F-score) on the cross-validated predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9995546936988831}, {"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9261109232902527}]}, {"text": "We submitted four systems to the shared task evaluation: 1.", "labels": [], "entities": []}, {"text": "BOW2 ltc +CHAR3 ltc : Uses counts of word bigrams and character trigrams, both weighted independently with the ltc weighting scheme (tf-idf with cosine normalization); 2.", "labels": [], "entities": []}, {"text": "BOW2 ltc +DEP ltc : Uses counts of word bigrams and syntactic dependencies, both weighted independently with the ltc weighting scheme; 3.", "labels": [], "entities": [{"text": "DEP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7456849217414856}]}, {"text": "BOW2 ltc +CHAR3 ltc +POS2 nnc : Same as system #1, adding counts of bigrams of part-ofspeech tags, independently cosine-normalized; 4. 3-system vote: Combination of the three submissions using majority vote.", "labels": [], "entities": []}, {"text": "The purpose of submission #1 was to check the performance that we could get using only surface form information (words and spelling).", "labels": [], "entities": []}, {"text": "As shown on, it reached an average test accuracy of 79.5%, which places it in the middle of the pack overall submissions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9857810139656067}]}, {"text": "For us, it establishes a baseline of what is achievable without any additional syntactic information provided by either taggers or parsers.", "labels": [], "entities": []}, {"text": "Our submissions #2 and #3 were meant to check the effect of adding syntactic features to basic lexical information.", "labels": [], "entities": []}, {"text": "We evaluated various combinations of feature spaces using cross-validation performance and found out that these two combinations seemed to bring a small boost in performance.", "labels": [], "entities": []}, {"text": "Unfortunately, as shown on, this did not reflect on the actual test results.", "labels": [], "entities": []}, {"text": "The test performance of submission #2 was a mere 0.2% higher than our baseline, when we expected +0.6% from the cross-validation estimate.", "labels": [], "entities": []}, {"text": "The test performance for submission #3 was 0.5% below that of the baseline, whereas we expected a small increase.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The four systems submitted by NRC, plus a  more extensive voting combination. System 1 uses only  surface information. Systems 2 and 3 use two types of  syntactic information and system #4 uses a majority vote  among the three previous submissions. The last (unsub- mitted) uses a majority vote among ten systems.", "labels": [], "entities": [{"text": "NRC", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9440945982933044}]}, {"text": " Table 2: Resulting accuracy scores and significance vs.  NRC top submission (3-system vote).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9649733901023865}, {"text": "significance", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9809609651565552}, {"text": "NRC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8692102432250977}]}]}