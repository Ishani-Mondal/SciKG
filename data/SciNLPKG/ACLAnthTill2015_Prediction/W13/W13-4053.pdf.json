{"title": [{"text": "A Semi-supervised Approach for Natural Language Call Routing", "labels": [], "entities": [{"text": "Natural Language Call Routing", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.742806613445282}]}], "abstractContent": [{"text": "Natural Language call routing remains a complex and challenging research area in machine intelligence and language understanding.", "labels": [], "entities": [{"text": "Natural Language call routing", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.707606628537178}, {"text": "language understanding", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7323748171329498}]}, {"text": "This paper is in the area of classifying user utterances into different categories.", "labels": [], "entities": [{"text": "classifying user utterances", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.8319319685300192}]}, {"text": "The focus is on design of algorithm that combines supervised and unsupervised learning models in order to improve classification quality.", "labels": [], "entities": []}, {"text": "We have shown that the proposed approach is able to outper-form existing methods on a large dataset and do not require morphological and stop-word filtering.", "labels": [], "entities": []}, {"text": "In this paper we present anew formula for term relevance estimation, which is a modification of fuzzy rules relevance estimation for fuzzy classifier.", "labels": [], "entities": [{"text": "term relevance estimation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.5976710716883341}]}, {"text": "Using this formula and only 300 frequent words for each class, we achieve an accuracy rate of 85.55% on the database excluding the \"garbage\" class (it includes utterances that cannot be assigned to any useful class or that can be assigned to more than one class).", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.9897537231445312}]}, {"text": "Dividing the \"garbage\" class into the set of subclasses by agglomera-tive hierarchical clustering we achieve about 9% improvement of accuracy rate on the whole database.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 133, "end_pos": 146, "type": "METRIC", "confidence": 0.979970246553421}]}], "introductionContent": [{"text": "Natural language call routing can be treated as an instance of topic categorization of documents (where the collection of labeled documents is used for training and the problem is to classify the remaining set of unlabeled test documents) but it also has some differences.", "labels": [], "entities": [{"text": "Natural language call routing", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6004453673958778}]}, {"text": "For instance, in document classification there are much more terms in one object than in single utterance from call routing task, where even one-word utterances are common.", "labels": [], "entities": [{"text": "document classification", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7281333208084106}, {"text": "call routing task", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.7738661666711172}]}, {"text": "A number of works have recently been published on natural language call classification.", "labels": [], "entities": [{"text": "natural language call classification", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.713131308555603}]}, {"text": "B. Carpenter, J. Chu-Carroll, C.-H. Lee and H.-K. Kuo proposed approaches using a vector-based information retrieval technique, the algorithms designed by A. L. Gorin, G. Riccardi, and J. H. Wright use a probabilistic model with salient phrases.", "labels": [], "entities": []}, {"text": "R. E. Schapire and Y. Singer focused on a boosting-based system for text categorization.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8156519532203674}]}, {"text": "The most similar work has been done by A. Albalate, D. Suendermann, R. Pieraccini, A. Suchindranath, S. Rhinow, J. Liscombe, K. Dayanidhi, and W. Minker.", "labels": [], "entities": []}, {"text": "They have worked on the data with the same structure: the focus was on the problem of big part of non-labeled data and only few labeled utterances for each class, methods of matching the obtained clusters and the given classes have also been considered; they provided the comparison of several classification methods that are able to perform on the large scale data.", "labels": [], "entities": []}, {"text": "The information retrieval approach for call routing is based on the training of the routing matrix, which is formed by statistics of appearances of words and phrases in a training set (usually after morphological and stop-word filtering).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7774728834629059}, {"text": "call routing", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.894889235496521}]}, {"text": "The new caller request is represented as a feature vector and is routed to the most similar destination vector.", "labels": [], "entities": []}, {"text": "The most commonly used similarity criterion is the cosine similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.981519341468811}]}, {"text": "The performance of systems, based on this approach, often depends on the quality of the destination vectors.", "labels": [], "entities": []}, {"text": "In this paper we propose anew term relevance estimation approach based on fuzzy rules relevance for fuzzy classifier (H. Ishibuchi, T. Nakashima, and T. Murata., 1999) to improve routing accuracy.", "labels": [], "entities": [{"text": "routing", "start_pos": 179, "end_pos": 186, "type": "TASK", "confidence": 0.9553093910217285}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.6486266255378723}]}, {"text": "We have also used a decision rule different from the cosine similarity.", "labels": [], "entities": []}, {"text": "We assign relevancies to every destination (class), calculate the sums of relevancies of words from the current utterance and choose the destination with the highest sum.", "labels": [], "entities": []}, {"text": "The database for training and performance evaluation consists of about 300.000 user utterances recorded from caller interactions with commercial automated agents.", "labels": [], "entities": []}, {"text": "The utterances were manually transcribed and classified into 20 classes (call reasons), such as appointments, operator, bill, internet, phone or video.", "labels": [], "entities": []}, {"text": "Calls that cannot be routed certainly to one reason of the list are classified to class _TE_NOMATCH.", "labels": [], "entities": [{"text": "TE", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9924783706665039}, {"text": "NOMATCH", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.433658242225647}]}, {"text": "A significant part of the database (about 27%) consists of utterances from the \"garbage\" class (_TE_NOMATCH).", "labels": [], "entities": [{"text": "TE", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9816993474960327}]}, {"text": "Our proposed approach decomposes the routing task into two steps.", "labels": [], "entities": [{"text": "routing task", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9199661910533905}]}, {"text": "On the first step we divide the \"garbage\" class into the set of subclasses by one of the clustering algorithms and on the second step we define the call reason considering the \"garbage\" subclasses as separate classes.", "labels": [], "entities": []}, {"text": "We apply genetic algorithms with the whole numbers alphabet, vector quantization network and hierarchical agglomerative clustering in order to divide \"garbage\" class into subclasses.", "labels": [], "entities": []}, {"text": "The reason to perform such a clustering is due to simplify the detection of the class with non-uniform structure.", "labels": [], "entities": []}, {"text": "Our approach uses the concept of salient phrases: for each call reason (class) only 300 words with the highest term relevancies are chosen.", "labels": [], "entities": []}, {"text": "It allows us to eliminate the need for the stop and ignore word filtering.", "labels": [], "entities": [{"text": "word filtering", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7426638901233673}]}, {"text": "The algorithms are implemented in C++.", "labels": [], "entities": []}, {"text": "As a baseline for results comparison we have tested some popular classifiers from RapidMiner, which we have applied to the whole database and the database with decomposition.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In Section II, we describe the problem and how we perform the preprocessing.", "labels": [], "entities": []}, {"text": "Section III describes in detail the way of the term relevance calculating and the possible rules of choosing the call class.", "labels": [], "entities": [{"text": "term relevance calculating", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7456794182459513}]}, {"text": "In Section IV we present the clustering algorithms which we apply to simplify the \"garbage\" class detection.", "labels": [], "entities": [{"text": "class detection", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.8028971254825592}]}, {"text": "Section V reports on the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we provide concluding remarks in Section VI.", "labels": [], "entities": []}], "datasetContent": [{"text": "The approach described above has been applied on the preprocessed corpus which has been provided by Speech Cycle company.", "labels": [], "entities": [{"text": "Speech Cycle", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.8439407646656036}]}, {"text": "We propose that only terms with highest value of RC (product of Rand C) are contributed to the total sum.", "labels": [], "entities": []}, {"text": "We have investigated the dependence of the new TRE approach on the frequent words number).", "labels": [], "entities": [{"text": "TRE", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.6173039078712463}]}, {"text": "The best accuracy rate was obtained with more than 300 frequent words.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9788107872009277}]}, {"text": "By using only limited set of words we eliminated the need of stop and ignore words filtering.", "labels": [], "entities": []}, {"text": "This also shows that the method works better if utterance includes terms with high C values.", "labels": [], "entities": []}, {"text": "This approach requires informative well-defined classes and enough data for statistical model.", "labels": [], "entities": []}, {"text": "We have tested standard classification algorithms (k-nearest neighbors algorithms, Bayes classifiers, Decision Stump, Rule Induction, perceptron) and the proposed approach on the database with \"garbage\" class and on the database without it).", "labels": [], "entities": []}, {"text": "The proposed algorithm outperforms all other methods with has an accuracy rate of 85.55%.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.991347461938858}]}, {"text": "provides accuracies of different decision rules.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.968579113483429}]}, {"text": "Applying the proposed formula to the whole database we obtain 61% and 55% of classification quality on train and test data.", "labels": [], "entities": []}, {"text": "We should also mention that the common tf.idf approach gives us on the given data 45% and 38% of accuracy rate on the train and test data.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 97, "end_pos": 110, "type": "METRIC", "confidence": 0.9807598292827606}]}, {"text": "The proposed approach performs significantly better on this kind of data.", "labels": [], "entities": []}, {"text": "Using the agglomerative hierarchical clustering we achieve about 9% improvement.", "labels": [], "entities": []}, {"text": "The best classification quality is obtained with 35 subclasses on the train data (68.7%) and 45 subclasses on the test data (63.9%).", "labels": [], "entities": []}, {"text": "Clustering into 35 subclasses gives 63.7% of accuracy rate on the test data.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9825659394264221}]}], "tableCaptions": [{"text": " Table 2. Performance of the new TRE approach", "labels": [], "entities": [{"text": "TRE", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.4683043658733368}]}]}