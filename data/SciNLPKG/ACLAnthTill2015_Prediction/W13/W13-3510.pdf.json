{"title": [{"text": "Documents and Dependencies: an Exploration of Vector Space Models for Semantic Composition", "labels": [], "entities": [{"text": "Semantic Composition", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7099879384040833}]}], "abstractContent": [{"text": "In most previous research on distribu-tional semantics, Vector Space Models (VSMs) of words are built either from topical information (e.g., documents in which a word is present), or from syntac-tic/semantic types of words (e.g., dependency parse links of a word in sentences), but not both.", "labels": [], "entities": [{"text": "dependency parse links of a word in sentences", "start_pos": 230, "end_pos": 275, "type": "TASK", "confidence": 0.8257082402706146}]}, {"text": "In this paper, we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases.", "labels": [], "entities": [{"text": "semantic composition of adjective-noun phrases", "start_pos": 106, "end_pos": 152, "type": "TASK", "confidence": 0.8480414032936097}]}, {"text": "Through extensive experiments on benchmark datasets, we find that even though a type-based VSM is effective for semantic composition, it is often outperformed by a VSM built using a combination of topic-and type-based statistics.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.8135169446468353}]}, {"text": "We also introduce anew evaluation task wherein we predict the composed vector representation of a phrase from the brain activity of a human subject reading that phrase.", "labels": [], "entities": []}, {"text": "We exploit a large syntactically parsed corpus of 16 billion tokens to build our VSMs, with vectors for both phrases and words, and make them publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models (VSMs) of word semantics use large collections of text to represent word meanings.", "labels": [], "entities": []}, {"text": "Each word vector is composed of features, where features can be derived from global corpus co-occurrence patterns (e.g. how often a word appears in each document), or local corpus co-occurrence patterns patterns (e.g. how often two words appear together in the same sentence, or are linked together in dependency parsed sentences).", "labels": [], "entities": []}, {"text": "These two feature types represent different aspects of word meaning (, and can be compared with the paradigmatic/syntagmatic distinction).", "labels": [], "entities": []}, {"text": "Global patterns give a more topic-based meaning (e.g. judge might appear in documents also containing court and verdict).", "labels": [], "entities": []}, {"text": "Certain local patterns give a more type-based meaning (e.g. the noun judge might be modified by the adjective harsh, or be the subject of decide, as would related and substitutable words such as referee or conductor).", "labels": [], "entities": []}, {"text": "Global patterns have been used in Latent Semantic Analysis and LDA Topic models (.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.671219160159429}]}, {"text": "Local patterns based on word co-occurrence in a fixed width window were used in Hyperspace Analogue to.", "labels": [], "entities": []}, {"text": "Subsequent models added increasing linguistic sophistication, up to full syntactic and dependency parses.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.6974280774593353}]}, {"text": "In this paper we systematically explore the utility of a global, topic-based VSM built from what we call Document features, and a local, type-based VSM built from Dependency features.", "labels": [], "entities": []}, {"text": "Our Document VSM represents each word w by a vector where each feature is a specific document, and the feature value is the number of mentions of word win that document.", "labels": [], "entities": []}, {"text": "Our Dependency VSM represents word w with a vector where each feature is a dependency parse link (e.g., the word w is the subject of the verb \"eat\"), and the feature value is the number of instances of this dependency feature for word w across a large text corpus.", "labels": [], "entities": []}, {"text": "We also consider a third Combined VSM in which the word vector is the concatenation of its Document and Dependency features.", "labels": [], "entities": []}, {"text": "All three models subsequently normalize frequencies using positive pointwise mutual-information (PPMI), and are dimensionality reduced using singular value decomposition (SVD).", "labels": [], "entities": []}, {"text": "This is the first systematic study of the utility of Document and Dependency features for semantic composition.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.808094710111618}]}, {"text": "We construct all three VSMs (Dependencies, Documents, Combined) using the same text corpus and preprocessing pipeline, and make the resulting VSMs available for download (http://www.cs.cmu. edu/ \u02dc afyshe/papers/conll2013/).", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first freely available VSM that includes entries for both words and adjective-noun phrases, and it is built from a much larger corpus than previously shared resources (16 billion words, 50 million documents).", "labels": [], "entities": []}, {"text": "Our main contributions include: \u2022 We systematically study complementarity of topical (Document) and type (Dependency) features in Vector Space Model (VSM) for semantic composition of adjective-noun phrases.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is one of the first studies of this kind.", "labels": [], "entities": []}, {"text": "\u2022 Through extensive experiments on standard benchmark datasets, we find that a VSM built from a combination of topical and type features is more effective for semantic composition, compared to a VSM built from Document and Dependency features alone.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 159, "end_pos": 179, "type": "TASK", "confidence": 0.800222635269165}]}, {"text": "\u2022 We introduce a novel task: to predict the vector representation of a composed phrase from the brain activity of human subjects reading that phrase.", "labels": [], "entities": []}, {"text": "\u2022 We explore two composition methods, addition and dilation, and find that while addition performs well on corpus-only tasks, dilation performs best on the brain activity task.", "labels": [], "entities": []}, {"text": "\u2022 We build our VSMs, for both phrases and words, from a large syntactically parsed text corpus of 16 billion tokens.", "labels": [], "entities": []}, {"text": "We also make the resulting VSM publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate how Document and Dependency dimensions can interact and compliment each other,  we can perform a qualitative comparison between the nearest neighbors (NNs) of words and phrases in the three VSMs -Dependency, Document, and Combined.", "labels": [], "entities": []}, {"text": "Note that single words and phrases can be neighbors of each other, demonstrating that our VSMs can generalize across syntactic types.", "labels": [], "entities": []}, {"text": "In the Document VSM, we get more topically related words as NNs (e.g., vet and leash for dog); and in the Dependency VSM, we see words that might substitute for one another in a sentence (e.g., gorgeous for beautiful).", "labels": [], "entities": [{"text": "Document VSM", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8378539681434631}, {"text": "Dependency VSM", "start_pos": 106, "end_pos": 120, "type": "DATASET", "confidence": 0.7175197899341583}]}, {"text": "The two feature sets can work together to up-weight the most suitable NNs (as in beautiful), or help to drown out noise (as in the NNs for bad publicity in the Document VSM).", "labels": [], "entities": [{"text": "Document VSM", "start_pos": 160, "end_pos": 172, "type": "DATASET", "confidence": 0.9058548510074615}]}], "tableCaptions": [{"text": " Table 1: The nearest neighbors of three queries under three VSMs: all 2000 dimensions (Deps & Docs);  1000 Document dimensions (Docs); 1000 Dependency dimensions (Deps).", "labels": [], "entities": []}]}