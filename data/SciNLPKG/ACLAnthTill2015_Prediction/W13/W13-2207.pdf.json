{"title": [{"text": "CUni Multilingual Matrix in the WMT 2013 Shared Task", "labels": [], "entities": [{"text": "WMT 2013 Shared Task", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.7661511227488518}]}], "abstractContent": [{"text": "We describe our experiments with phrase-based machine translation for the WMT 2013 Shared Task.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6571404139200846}, {"text": "WMT 2013 Shared Task", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.7311038076877594}]}, {"text": "We trained one system for 18 translation directions between English or Czech on one side and English, Czech, Ger-man, Spanish, French or Russian on the other side.", "labels": [], "entities": []}, {"text": "We describe a set of results with different training data sizes and subsets.", "labels": [], "entities": []}, {"text": "For the pairs containing Russian, we describe a set of independent experiments with slightly different translation models.", "labels": [], "entities": []}], "introductionContent": [{"text": "With so many official languages, Europe is a paradise for machine translation research.", "labels": [], "entities": [{"text": "machine translation research", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.8873075842857361}]}, {"text": "One of the largest bodies of electronically available parallel texts is being nowadays generated by the European Union and its institutions.", "labels": [], "entities": []}, {"text": "At the same time, the EU also provides motivation and boosts potential market for machine translation outcomes.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8439559042453766}]}, {"text": "Most of the major European languages belong to one of three branches of the IndoEuropean language family: Germanic, Romance or Slavic.", "labels": [], "entities": []}, {"text": "Such relatedness is responsible for many structural similarities in European languages, although significant differences still exist.", "labels": [], "entities": []}, {"text": "Within the language portfolio selected for the WMT shared task, English, French and Spanish seem to be closer to each other than to the rest.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8591707348823547}]}, {"text": "German, despite being genetically related to English, differs in many properties.", "labels": [], "entities": []}, {"text": "Its word order rules, shifting verbs from one end of the sentence to the other, easily create long-distance dependencies.", "labels": [], "entities": []}, {"text": "Long German compound words are notorious for increasing out-of-vocabulary rate, which has led many researchers to devising unsupervised compound-splitting techniques.", "labels": [], "entities": []}, {"text": "Also, uppercase/lowercase distinction is more important because all German nouns start with an uppercase letter by the rule.", "labels": [], "entities": []}, {"text": "Czech is a language with rich morphology (both inflectional and derivational) and relatively free word order.", "labels": [], "entities": []}, {"text": "In fact, the predicateargument structure, often encoded by fixed word order in English, is usually captured by inflection (especially the system of 7 grammatical cases) in Czech.", "labels": [], "entities": []}, {"text": "While the free word order of Czech is a problem when translating to English (the text should be parsed first in order to determine the syntactic functions and the English word order), generating correct inflectional affixes is indeed a challenge for Englishto-Czech systems.", "labels": [], "entities": []}, {"text": "Furthermore, the multitude of possible Czech word forms (at least order of magnitude higher than in English) makes the data sparseness problem really severe, hindering both directions.", "labels": [], "entities": []}, {"text": "Most of the above characteristics of Czech also apply to Russian, another Slavic language.", "labels": [], "entities": []}, {"text": "Similar issues have to be expected when translating between Russian and English.", "labels": [], "entities": []}, {"text": "Still, there are also interesting divergences between Russian and Czech, especially on the syntactic level.", "labels": [], "entities": []}, {"text": "Russian sentences typically omit copula in the present tense and there is also no direct equivalent of the verb \"to have\".", "labels": [], "entities": []}, {"text": "Periphrastic constructions such as \"there is XXX by him\" are used instead.", "labels": [], "entities": []}, {"text": "These differences make the Czech-Russian translation interest-ing as well.", "labels": [], "entities": []}, {"text": "Interestingly enough, results of machine translation between Czech and Russian has so far been worse than between English and any of the two languages, language relatedness notwithstanding.", "labels": [], "entities": [{"text": "machine translation between Czech and Russian", "start_pos": 33, "end_pos": 78, "type": "TASK", "confidence": 0.8008040338754654}]}, {"text": "Our goal is to run one system under as similar conditions as possible to all eighteen translation directions, to compare their translation accuracies and see why some directions are easier than others.", "labels": [], "entities": []}, {"text": "The current version of the system does not include really languagespecific techniques: we neither split German compounds, nor do we address the peculiarities of Czech and Russian mentioned above.", "labels": [], "entities": []}, {"text": "In an independent set of experiments, we tried to deal with the data sparseness of Russian language with the addition of a backoff model with a simple stemming and some additional data; those experiments were done for Russian and Czech|English combinations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the first set of experiments we wanted to use the same setting for all language pairs.", "labels": [], "entities": []}, {"text": "BLEU scores were computed by our system, comparing truecased tokenized hypothesis with truecased tokenized reference translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9889737367630005}]}, {"text": "Such scores must differ from the official evaluation-see Section 3.2.4 for discussion of the final results.", "labels": [], "entities": []}, {"text": "The confidence interval for most of the scores lies between \u00b10.5 and \u00b10.6 BLEU % points.", "labels": [], "entities": [{"text": "confidence interval", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9844222664833069}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9841070175170898}]}, {"text": "The set of baseline experiments were trained on the supervised truecased combination of News Commentary and Europarl.", "labels": [], "entities": [{"text": "News Commentary", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9233138561248779}, {"text": "Europarl", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.9390823841094971}]}, {"text": "As we had lemmatizers for the languages, word alignment was computed on lemmas.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8011291027069092}]}, {"text": "(But our previous experiments showed that there was little difference between using lemmas and lowercased 4-character \"stems\".)", "labels": [], "entities": []}, {"text": "A hexagram language model was trained on the monolingual version of the News Commentary + Europarl corpus (typically a slightly larger superset of the target side of the parallel corpus).", "labels": [], "entities": [{"text": "News Commentary + Europarl corpus", "start_pos": 72, "end_pos": 105, "type": "DATASET", "confidence": 0.8720512628555298}]}, {"text": "Ina separate set of experiments, we tried to take a basic Moses framework and change the setup a little for better results on morphologically rich languages.", "labels": [], "entities": []}, {"text": "Tried combinations were Russian-Czech and Russian-English.: Final BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9762569665908813}]}, {"text": "BLEU is truecased computed by the system, BLEU l is the official lowercased evaluation by matrix.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9890705347061157}, {"text": "BLEU l", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9717575311660767}]}, {"text": "BLEU t is official truecased evaluation.", "labels": [], "entities": [{"text": "BLEU t", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9599554240703583}]}, {"text": "Although lower official scores are expected, notice the larger gap in en-fr and cs-fr translation.", "labels": [], "entities": []}, {"text": "There seems to be a problem in our French detokenization procedure.", "labels": [], "entities": [{"text": "French detokenization procedure", "start_pos": 35, "end_pos": 66, "type": "DATASET", "confidence": 0.7272251645723978}]}], "tableCaptions": [{"text": " Table 1: Number of sentence pairs and tokens  for every language pair in the parallel training  corpus. Languages are identified by their ISO  639 codes: cs = Czech, de = German, en =  English, es = Spanish, fr = French, ru = Rus- sian. Every line corresponds to the respective  version of EuroParl + News Commentary; the  second part presents the extra corpora.", "labels": [], "entities": [{"text": "EuroParl + News Commentary", "start_pos": 291, "end_pos": 317, "type": "DATASET", "confidence": 0.9640851616859436}]}, {"text": " Table 2: Number of segments (paragraphs  in Gigaword, sentences elsewhere) and tokens  of additional monolingual training corpora.  \"newsc+euro\" are the monolingual versions of  the News Commentary and Europarl parallel  corpora. \"news.all\" denotes all years of the  Crawled News corpus for the given language.", "labels": [], "entities": [{"text": "Europarl parallel  corpora", "start_pos": 203, "end_pos": 229, "type": "DATASET", "confidence": 0.882072647412618}, {"text": "Crawled News corpus", "start_pos": 268, "end_pos": 287, "type": "DATASET", "confidence": 0.7051188051700592}]}, {"text": " Table 3: BLEU scores of the baseline experi- ments (left column) on News Test 2013 data,  computed by the system on tokenized data,  versus similar setup with Gigaword. The dif- ference was typically not significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999230146408081}, {"text": "News Test 2013 data", "start_pos": 69, "end_pos": 88, "type": "DATASET", "confidence": 0.9805917888879776}, {"text": "Gigaword", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9138438701629639}]}, {"text": " Table 5: Final BLEU scores. BLEU is true- cased computed by the system, BLEU l is  the official lowercased evaluation by matrix.  statmt.org. BLEU t is official truecased eval- uation. Although lower official scores are ex- pected, notice the larger gap in en-fr and cs-fr  translation. There seems to be a problem in  our French detokenization procedure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9408106207847595}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9984017014503479}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9982537627220154}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.996705949306488}, {"text": "French detokenization", "start_pos": 324, "end_pos": 345, "type": "DATASET", "confidence": 0.8568241596221924}]}, {"text": " Table 6: Number of sentence pairs and tokens  for every language pair.", "labels": [], "entities": []}, {"text": " Table 7: Number of sentences and tokens for  every language.", "labels": [], "entities": []}, {"text": " Table 8: Lowercased and cased BLEU scores", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9844569563865662}]}]}