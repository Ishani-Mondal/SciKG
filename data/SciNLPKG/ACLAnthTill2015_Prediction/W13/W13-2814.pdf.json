{"title": [{"text": "A Hybrid Word Alignment Model for Phrase-Based Statistical Ma- chine Translation", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.724537193775177}, {"text": "Phrase-Based Statistical Ma- chine Translation", "start_pos": 34, "end_pos": 80, "type": "TASK", "confidence": 0.6928511659304301}]}], "abstractContent": [{"text": "This paper proposes a hybrid word alignment model for Phrase-Based Statistical Machine translation (PB-SMT).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7195208966732025}, {"text": "Phrase-Based Statistical Machine translation (PB-SMT)", "start_pos": 54, "end_pos": 107, "type": "TASK", "confidence": 0.6672496029308864}]}, {"text": "The proposed hybrid alignment model provides most informative alignment links which are offered by both un-supervised and semi-supervised word alignment models.", "labels": [], "entities": []}, {"text": "Two unsupervised word alignment models (GIZA++ and Berkeley aligner) and a rule based aligner are combined together.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7003472447395325}]}, {"text": "The rule based aligner only aligns named entities (NEs) and chunks.", "labels": [], "entities": []}, {"text": "The NEs are aligned through transliteration using a joint source-channel model.", "labels": [], "entities": []}, {"text": "Chunks are aligned employing a bootstrapping approach by translating the source chunks into the target language using a baseline PB-SMT system and subsequently validating the target chunks using a fuzzy matching technique against the target corpus.", "labels": [], "entities": []}, {"text": "All the experiments are carried out after single-tokenizing the multi-word NEs.", "labels": [], "entities": []}, {"text": "Our best system provided significant improvements over the baseline as measured by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9958689212799072}]}], "introductionContent": [{"text": "Word alignment is the backbone of PB-SMT system or any data driven approaches to Machine Translation (MT) and it has received a lot of attention in the area of statistical machine translation (SMT) (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7412929683923721}, {"text": "Machine Translation (MT)", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.8508768081665039}, {"text": "statistical machine translation (SMT)", "start_pos": 160, "end_pos": 197, "type": "TASK", "confidence": 0.778208926320076}]}, {"text": "Word alignment is not an end task in itself and is usually used as an intermediate step in SMT.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7329315841197968}, {"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9948046803474426}]}, {"text": "Word alignment is defined as the detection of corresponding alignment of words from parallel sentences that are translation of each other.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6955100297927856}]}, {"text": "Statistical machine translation usually suffers from many-to-many word links which existing statistical word alignment algorithms cannot handle well.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7060221135616302}, {"text": "statistical word alignment", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.6623166302839915}]}, {"text": "The unsupervised word alignment models are based on IBM models 1-5 () and the HMM model (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7523737549781799}]}, {"text": "Models 3, 4 and 5 are based on fertility based models which are asymmetric.", "labels": [], "entities": []}, {"text": "To improve alignment quality, the Berkeley Aligner is based on the symmetric property by intersecting alignments induced in each translation direction.", "labels": [], "entities": [{"text": "Berkeley Aligner", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.8970766067504883}]}, {"text": "In the present work, we propose improvement of word alignment quality by combining three word alignment tables (i) GIZA++ alignment (ii) Berkeley Alignment and (iii) rule based alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7585139870643616}]}, {"text": "Our objective is to perceive the effectiveness of the Hybrid model in word alignment by improving the quality of translation in the SMT system.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.8441812694072723}, {"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9860948324203491}]}, {"text": "In the present work, we have implemented a rule based alignment model by considering several types of chunks which are automatically extracted on the source side.", "labels": [], "entities": []}, {"text": "Each individual source chunk is translated using a baseline PB-SMT system and validated with the target chunks on the target side.", "labels": [], "entities": []}, {"text": "The validated source-target chunks are added in the rule based alignment table.", "labels": [], "entities": []}, {"text": "Work has been carried out into three directions: (i) three alignment tables are combined together by taking their union; (ii) extra alignment pairs are added into the alignment table.", "labels": [], "entities": []}, {"text": "This is a well-known practice in domain adaptation in SMT (; (iii) the alignment table is updated through semisupervised alignment technique.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.767837405204773}, {"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9727007150650024}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "The proposed hybrid word alignment model is described in Section 3.", "labels": [], "entities": [{"text": "hybrid word alignment", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.6741708219051361}]}, {"text": "Section 4 presents the tools and resources used for the various experiments.", "labels": [], "entities": []}, {"text": "Section 5 includes the results obtained, together with some analysis.", "labels": [], "entities": []}, {"text": "Section 6 concludes and provides avenues for further work.", "labels": [], "entities": []}, {"text": "proposed a multilingual filtering algorithm that generates bilingual chunk alignment from Chinese-English parallel corpus.", "labels": [], "entities": [{"text": "bilingual chunk alignment", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6542886396249136}]}, {"text": "The algorithm has three steps, first, from the parallel corpus; the most frequent bilingual chunks are extracted.", "labels": [], "entities": []}, {"text": "Secondly, the participating chunks for alignments are combined into a cluster and finally one English chunk is generated corresponding to a Chinese chunk by analyzing the highest co-occurrences of English chunks.", "labels": [], "entities": []}, {"text": "Bilingual knowledge can be extracted using chunk alignment ).", "labels": [], "entities": [{"text": "chunk alignment", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7765142619609833}]}, {"text": "proposed a bootstrapping method for chunk alignment; they used an SMT based model for chunk translation and then aligned the sourcetarget chunk pairs after validating the translated chunk.", "labels": [], "entities": [{"text": "chunk alignment", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.8031371235847473}, {"text": "chunk translation", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7384296804666519}]}, {"text": "simplified the task of automatic word alignment as several consecutive words together correspond to a single word in the opposite language by using the word aligner itself, i.e., by bootstrapping on its output.", "labels": [], "entities": [{"text": "automatic word alignment", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6199404299259186}]}, {"text": "A Maximum Entropy model based approach for English-Chinese NE alignment which significantly outperforms IBM Model4 and HMM has been proposed by . They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score and the distortion score for distinguishing identical NEs in the same sentence.", "labels": [], "entities": [{"text": "NE alignment", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.7213240414857864}, {"text": "distortion score", "start_pos": 270, "end_pos": 286, "type": "METRIC", "confidence": 0.969077080488205}]}, {"text": "presented an approach where capitalization cues have been used for identifying NEs on the English side.", "labels": [], "entities": []}, {"text": "Statistical techniques are applied to decide which portion of the target language corresponds to the specified English NE, for simultaneous NE identification and translation.", "labels": [], "entities": [{"text": "NE identification", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.8478274941444397}]}], "datasetContent": [{"text": "We have randomly selected 500 sentences each for the development set and the test set from the initial parallel corpus.", "labels": [], "entities": []}, {"text": "The rest are considered as the training corpus.", "labels": [], "entities": []}, {"text": "The training corpus was filtered with the maximum allowable sentence length of 100 words and sentence length ratio of 1:2 (either way).", "labels": [], "entities": [{"text": "sentence length ratio", "start_pos": 93, "end_pos": 114, "type": "METRIC", "confidence": 0.6790777742862701}]}, {"text": "Finally the training corpus contained 22,492 sentences.", "labels": [], "entities": []}, {"text": "In addition to the target side of the parallel corpus, a monolingual Bengali corpus containing 488,026 words from the tourism domain was used for building the target language model.", "labels": [], "entities": []}, {"text": "We experimented with different n-gram settings for the language model and the maximum phrase length and found that a 4-gram language model and a maximum phrase length of 7 produced the optimum baseline result.", "labels": [], "entities": []}, {"text": "We carried out the rest of the experiments using these settings.", "labels": [], "entities": []}, {"text": "We experimented with the system over various combinations of word alignment models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.7329903990030289}]}, {"text": "Our hypothesis focuses mainly on the theme that proper alignment of words will result in improvement of the system performance in terms of translation quality.", "labels": [], "entities": []}, {"text": "141,821 chunks were identified from the source corpus, of which 96,438 (68%) chunks were aligned by the system.", "labels": [], "entities": []}, {"text": "39,931 and 28,107 NEs were identified from the source and target sides of the parallel corpus respectively, of which 22,273 NEs are unique in English and 22,010 NEs in Bengali.", "labels": [], "entities": []}, {"text": "A total of 14,023 NEs have been aligned through transliteration.", "labels": [], "entities": []}, {"text": "The experiments have been carried outwith various experimental settings: (i) single tokenization of NEs on both sides of the parallel corpus, (ii) using Berkeley Aligner with unsupervised training, (iii) union of the three alignment models: rule based, GIZA++ with GDFA and Berkeley Alignment, (iv) hybridization of the three alignment models and (v) supervised Berkeley Aligner.", "labels": [], "entities": []}, {"text": "Eextrinsic evaluation was carried out on the MT quality using BLEU () and NIST).", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.965788722038269}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9964996576309204}, {"text": "NIST", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.8833996057510376}]}, {"text": "1 Evaluation results for different experimental setups.", "labels": [], "entities": []}, {"text": "(The \u2017 \u2020' marked systems produce statistically significant improvements on BLEU over the baseline system)  The baseline system (Exp 1) is the state-of-art PB-SMT system where GIZA++ with grow-diagfinal-and has been used as the word alignment model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9970808625221252}, {"text": "word alignment", "start_pos": 227, "end_pos": 241, "type": "TASK", "confidence": 0.8073298037052155}]}, {"text": "Experiment 2 provides better results than experiment 1 which signifies that Berkeley Aligner performs better than GIZA++ for the English-Bengali translation task.", "labels": [], "entities": [{"text": "English-Bengali translation task", "start_pos": 129, "end_pos": 161, "type": "TASK", "confidence": 0.7262866993745168}]}, {"text": "The union of all thee alignments (Exp 3) provides better scores than the baseline; however it cannot beat the results obtained with the Berkeley Aligner alone.", "labels": [], "entities": [{"text": "Berkeley Aligner", "start_pos": 136, "end_pos": 152, "type": "DATASET", "confidence": 0.9410602450370789}]}, {"text": "Hybrid alignment model with GIZA++ as the standard alignment (Exp 4a) produces statistically significant improvements over the baseline.", "labels": [], "entities": []}, {"text": "Similarly the use of Berkeley Aligner as the standard alignment for hybrid alignment model (Exp 4b) also results in statistically significant improvements over Exp 2.", "labels": [], "entities": []}, {"text": "These two experiments (Exp 4a and 4b) demonstrate the effectiveness of the hybrid alignment model.", "labels": [], "entities": []}, {"text": "It is to be noticed that hybrid alignment model works better with the Berkeley Aligner than with GIZA++.", "labels": [], "entities": [{"text": "Berkeley Aligner", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.9224964082241058}]}, {"text": "Single-tokenization of the NEs (Exp 5, 6, 7a and 7b) improves the system performance to some extent over the corresponding experiments without single-tokenization (Exp 1, 2, 4a and 4b); however, these improvements are not statistically significant.", "labels": [], "entities": []}, {"text": "The Berkeley semi-supervised alignment method using a bootstrapping approach together with single-tokenization of NEs provided the overall best performance in terms of both BLEU and NIST and the corresponding improvement is statistically significant on BLEU over rest of the experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.9976929426193237}, {"text": "NIST", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.8036646842956543}, {"text": "BLEU", "start_pos": 253, "end_pos": 257, "type": "METRIC", "confidence": 0.9846624135971069}]}], "tableCaptions": []}