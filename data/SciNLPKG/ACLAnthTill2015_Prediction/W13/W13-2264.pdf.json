{"title": [{"text": "An MT Error-driven Discriminative Word Lexicon using Sentence Structure Features", "labels": [], "entities": [{"text": "MT Error-driven Discriminative Word Lexicon", "start_pos": 3, "end_pos": 46, "type": "TASK", "confidence": 0.8763388276100159}]}], "abstractContent": [{"text": "The Discriminative Word Lexicon (DWL) is a maximum-entropy model that predicts the target word probability given the source sentence words.", "labels": [], "entities": []}, {"text": "We present two ways to extend a DWL to improve its ability to model the word translation probability in a phrase-based machine translation (PBMT) system.", "labels": [], "entities": [{"text": "word translation", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7077643722295761}, {"text": "phrase-based machine translation (PBMT)", "start_pos": 106, "end_pos": 145, "type": "TASK", "confidence": 0.7818764249483744}]}, {"text": "While DWLs are able to model the global source information, they ignore the structure of the source and target sentence.", "labels": [], "entities": []}, {"text": "We propose to include this structure by modeling the source sentence as a bag-of-n-grams and features depending on the surrounding target words.", "labels": [], "entities": []}, {"text": "Furthermore , as the standard DWL does not get any feedback from the MT system, we change the DWL training process to explicitly focus on addressing MT errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9481309056282043}, {"text": "MT", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9745843410491943}]}, {"text": "By using these methods we are able to improve the translation performance by up to 0.8 BLEU points compared to a system that uses a standard DWL.", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9563924670219421}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9990466237068176}]}], "introductionContent": [{"text": "In many state-of-the-art SMT systems, the phrasebased (  approach is used.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9899019002914429}]}, {"text": "In this approach, instead of building the translation by translating word byword, sequences of source and target words, so-called phrase pairs, are used as the basic translation unit.", "labels": [], "entities": []}, {"text": "A table of correspondences between source and target phrases forms the translation model.", "labels": [], "entities": []}, {"text": "Target language fluency is modeled by a language model storing monolingual n-gram occurrences.", "labels": [], "entities": []}, {"text": "A log-linear combination of these main models as well as additional features is used to score the different translation hypotheses.", "labels": [], "entities": []}, {"text": "Then the decoder searches for the translation with the highest score.", "labels": [], "entities": []}, {"text": "One problem of this approach is that bilingual context is only modeled within the phrase pairs.", "labels": [], "entities": []}, {"text": "Therefore, different approaches to increase the context available during decoding have been presented.", "labels": [], "entities": []}, {"text": "One promising approach is the Discriminative Word Lexicon (DWL).", "labels": [], "entities": []}, {"text": "In this approach, a discriminative model is used to predict the probability of a target word given the words in the source sentence.", "labels": [], "entities": []}, {"text": "In contrast to other models in the phrase-based system, this approach is capable of modeling the translation probability using information from the whole sentence.", "labels": [], "entities": []}, {"text": "Thus it is possible to model long-distance dependencies.", "labels": [], "entities": []}, {"text": "But the model is notable to use the structure of the sentence, since the source sentence is modeled only as a bagof-words.", "labels": [], "entities": []}, {"text": "Furthermore, the DWL is trained to discriminate between all translation options without knowledge about the other models used in a phrase-based machine translation system such as the translation model, language model etc.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 131, "end_pos": 163, "type": "TASK", "confidence": 0.674175500869751}]}, {"text": "In contrast, we try to feedback information about possible errors of the MT system into the DWL.", "labels": [], "entities": [{"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9626871943473816}, {"text": "DWL", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.949139416217804}]}, {"text": "Thereby, the DWLs are able to focus on improving the errors of the other models of an MT system.", "labels": [], "entities": [{"text": "DWLs", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8390582203865051}, {"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9809271097183228}]}, {"text": "We will introduce features that encode information about the source sentence structure.", "labels": [], "entities": []}, {"text": "Furthermore, the surrounding target words will also be used in the model to encode information about the target sentence structure.", "labels": [], "entities": []}, {"text": "Finally, we incorporate information from the other models into the creation of the training examples.", "labels": [], "entities": []}, {"text": "We create the negative training examples using possible errors of the other models. proach.", "labels": [], "entities": [{"text": "proach", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.8961329460144043}]}, {"text": "Thereby, they are able to use global source information.", "labels": [], "entities": []}, {"text": "This was extended by by a feature selection strategy in order to reduce the number of weights.", "labels": [], "entities": []}, {"text": "Ina first approach to use information about MT errors in the training of DWLs was presented.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9832658767700195}]}, {"text": "They select the training examples by using phrase table information also.", "labels": [], "entities": []}, {"text": "The DWLs are related to work that was done in the area of word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.8147574464480082}]}, {"text": "presented an approach to disambiguate between different phrases instead of performing the disambiguation at word level.", "labels": [], "entities": []}, {"text": "A different lexical model that uses target side information was presented in.", "labels": [], "entities": []}, {"text": "The focus of this work was to model complex morphology on the target language.", "labels": [], "entities": []}], "datasetContent": [{"text": "After presenting the different approaches to perform feature and example selection, we will now evaluate them.", "labels": [], "entities": []}, {"text": "First, we will give a short overview of the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9910786747932434}]}, {"text": "Then we will give a detailed evaluation on the task of translating German lectures into English and analyze the influence of the presented approaches.", "labels": [], "entities": []}, {"text": "Afterwards, we will present overview experiments on the German-to-English and English-to-German translation task of WMT 13 Shared Translation Task.", "labels": [], "entities": [{"text": "WMT 13 Shared Translation Task", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.7318917632102966}]}, {"text": "In addition to the experiments on the TED data, we also tested the models in the systems for the WMT 2013.", "labels": [], "entities": [{"text": "TED data", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9299200177192688}, {"text": "WMT 2013", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.8489429354667664}]}, {"text": "The systems are similar to the one used before, but were trained on all available training data and use additional models.", "labels": [], "entities": []}, {"text": "The systems were tested on newstest2012.", "labels": [], "entities": [{"text": "newstest2012", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9698784947395325}]}, {"text": "The results for German to English are summarized in.", "labels": [], "entities": []}, {"text": "In this case the DWLs were trained on the EPPS and the NC corpus.", "labels": [], "entities": [{"text": "DWLs", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.832461416721344}, {"text": "EPPS", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9834481477737427}, {"text": "NC corpus", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.9491209983825684}]}, {"text": "Since the corpora are bigger, we perform an additional weight filtering on the models.", "labels": [], "entities": []}, {"text": "The baseline system uses already a DWL trained with the bag-of-words features and the training examples were created using the phrase table.", "labels": [], "entities": []}, {"text": "If we add the bag-of-n-grams features up to a n-gram length of 3, we cannot improve the translation quality on this task.", "labels": [], "entities": [{"text": "translation", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.9467632174491882}]}, {"text": "But by additionally generating the negative training examples using the 300-Best list, we can improve this system by 0.2 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.999477207660675}]}, {"text": "We also tested the approach also on the reverse direction.", "labels": [], "entities": []}, {"text": "Since the German morphology is much more complex than the English one, we hope that in this case the target features can help more.", "labels": [], "entities": []}, {"text": "The results for this task are shown in.", "labels": [], "entities": []}, {"text": "Here, the baseline system again already uses DWLs.", "labels": [], "entities": []}, {"text": "If we add the bag-of-n-grams features and generate the training examples from the 300-Best list, we can again slightly improve the translation quality.", "labels": [], "entities": []}, {"text": "In this case we can improve the translation quality by additional 0.1 BLEU points by adding the target features.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9524293541908264}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9989940524101257}]}, {"text": "This leads to an overall improvement by nearly 0.2 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9987385869026184}]}], "tableCaptions": [{"text": " Table 1: Experiments using different source features", "labels": [], "entities": []}, {"text": " Table 2. In these experiments we used the  source features using unigrams, bigrams and tri-", "labels": [], "entities": []}, {"text": " Table 2: Experiments using different methods to create training examples", "labels": [], "entities": []}, {"text": " Table 4: Overview of results for TED lectures", "labels": [], "entities": [{"text": "TED lectures", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.6563815176486969}]}, {"text": " Table 3: Experiments using different target features", "labels": [], "entities": []}, {"text": " Table 5: Experiments on German to English WMT  2013", "labels": [], "entities": [{"text": "German to English WMT", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.4304826930165291}]}, {"text": " Table 6: Experiments on English to German WMT  2013", "labels": [], "entities": [{"text": "English to German WMT", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.44585972279310226}]}]}