{"title": [{"text": "Philosophers are Mortal: Inferring the Truth of Unseen Facts", "labels": [], "entities": [{"text": "Inferring the Truth of Unseen Facts", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.7286701798439026}]}], "abstractContent": [{"text": "Large databases of facts are prevalent in many applications.", "labels": [], "entities": []}, {"text": "Such databases are accurate, but as they broaden their scope they become increasingly incomplete.", "labels": [], "entities": []}, {"text": "In contrast to extending such a database, we present a system to query whether it contains an arbitrary fact.", "labels": [], "entities": []}, {"text": "This work can bethought of as re-casting open domain information extraction: rather than growing a database of known facts, we smooth this data into a database in which any possible fact has membership with some confidence.", "labels": [], "entities": [{"text": "open domain information extraction", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6095899119973183}]}, {"text": "We evaluate our system predicting held out facts, achieving 74.2% accuracy and outperforming multiple baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9965428709983826}]}, {"text": "We also evaluate the system as a common-sense filter for the ReVerb Open IE system , and as a method for answer validation in a Question Answering task.", "labels": [], "entities": [{"text": "ReVerb Open IE system", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.8315522074699402}, {"text": "answer validation in a Question Answering task", "start_pos": 105, "end_pos": 151, "type": "TASK", "confidence": 0.7056969021047864}]}], "introductionContent": [{"text": "Databases of facts, such as Freebase ( or Open Information Extraction (Open IE) extractions, are useful fora range of NLP applications from semantic parsing to information extraction.", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE) extractions", "start_pos": 42, "end_pos": 91, "type": "TASK", "confidence": 0.6355468519032001}, {"text": "semantic parsing", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.7225580662488937}, {"text": "information extraction", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.8390387296676636}]}, {"text": "However, as the domain of a database grows, it becomes increasingly impractical to collect completely, and increasingly unlikely that all the elements intended for the database are explicitly mentioned in the source corpus.", "labels": [], "entities": []}, {"text": "In particular, common-sense facts are rarely explicitly mentioned, despite their abundance.", "labels": [], "entities": []}, {"text": "It would be useful to infer the truth of such unseen facts rather than assuming them to be implicitly false.", "labels": [], "entities": []}, {"text": "A growing body of work has focused on automatically extending large databases with a finite set of additional facts.", "labels": [], "entities": []}, {"text": "In contrast, we propose a system to generate the (possibly infinite) completion of such a database, with a degree of confidence for each unseen fact.", "labels": [], "entities": []}, {"text": "This task can be cast as querying whether an arbitrary element is a member of the database, with an informative degree of confidence.", "labels": [], "entities": []}, {"text": "Since often the facts in these databases are devoid of context, we refine our notion of truth to reflect whether we would assume a fact to be true without evidence to the contrary.", "labels": [], "entities": []}, {"text": "In this vein, we can further refine our task as determining whether an arbitrary fact is plausibletrue in the absence contradictory evidence.", "labels": [], "entities": []}, {"text": "In addition to general applications of such large databases, our approach can further be integrated into systems which can make use of probabilistic membership.", "labels": [], "entities": []}, {"text": "For example, certain machine translation errors could be fixed by determining that the target translation expresses an implausible fact.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7167870402336121}]}, {"text": "Similarly, the system can be used as a soft feature for semantic compatibility in coreference; e.g., the types of phenomena expressed in Hobbs' selectional constraints.", "labels": [], "entities": []}, {"text": "Lastly, it is useful as a common-sense filter; we evaluate the system in this role by filtering implausible facts from Open IE extractions, and filtering incorrect responses fora question answering system.", "labels": [], "entities": [{"text": "Open IE extractions", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.5113291144371033}, {"text": "question answering", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.7199472188949585}]}, {"text": "Our approach generalizes word similarity metrics to a notion of fact similarity, and judges the membership of an unseen fact based on the aggregate similarity between it and existing members of the database.", "labels": [], "entities": []}, {"text": "For instance, if we have not seen the fact that philosophers are mortal 1 but we know that Greeks are mortal, and that philosophers and Greeks are similar, we would like to infer that the fact is nonetheless plausible.", "labels": [], "entities": []}, {"text": "We implement our approach on both a large open-domain database of facts extracted from the Open IE system), and ConceptNet (), a hand curated database of commonsense facts.", "labels": [], "entities": [{"text": "Open IE system", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.8541327714920044}]}], "datasetContent": [{"text": "A natural way to evaluate our system is to use the same regime as our training, evaluating on held out facts.", "labels": [], "entities": []}, {"text": "For both domains we train on a balanced dataset of 20,000 training and 10,000 test examples.", "labels": [], "entities": []}, {"text": "Performance is measured in terms of classification accuracy, with a random baseline of 50%.", "labels": [], "entities": [{"text": "classification", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.8940788507461548}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9544729590415955}]}, {"text": "The similar fact count baseline performs nearly at random chance, suggesting that our sampled negative facts cannot be predicted solely on the basis of connectedness with the rest of the database.", "labels": [], "entities": []}, {"text": "Furthermore, we outperform the cosine baseline, supporting the intuition that aggregating similarity metrics is useful.", "labels": [], "entities": []}, {"text": "To evaluate the informativeness of the confidence our system produces, we can allow our system to abstain from unsure judgments.", "labels": [], "entities": []}, {"text": "Recall refers to the percentage of facts the system chooses to make a guess on; precision is the percentage of those facts which are classified correctly.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9762586355209351}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9994600415229797}]}, {"text": "From this, we can create a precision/recall curve -presented in", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.999221682548523}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.884503960609436}]}], "tableCaptions": [{"text": " Table 1: A summary of similarity metrics used to  calculate fact similarity. For the thesaurus based  metrics, the two synsets being compared are de- noted by w 1 and w 2 ; the lowest common subsumer  is denoted as lcs. For distributional similarity met- rics, the two word vectors are denoted by w 1 and  w 2 . For metrics which require a probability distri- bution, we pass the vectors through a sigmoid to  obtain p i =", "labels": [], "entities": []}, {"text": " Table 5: Classification accuracy for the Answer  Validation Exercise task. The baseline is accept- ing all answers as correct (all validated); a second  baseline (filter only) incorporates only the n-gram  overlap threshold. The median and top performing  scores for both years are provided for comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9823760390281677}, {"text": "Answer  Validation Exercise task", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.9512942135334015}]}]}