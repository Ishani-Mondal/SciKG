{"title": [{"text": "The TALP-UPC Approach to System Selection: ASIYA Features and Pairwise Classification using Random Forests", "labels": [], "entities": [{"text": "System Selection", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7343949973583221}]}], "abstractContent": [{"text": "This paper describes the TALP-UPC participation in the WMT'13 Shared Task on Quality Estimation (QE).", "labels": [], "entities": [{"text": "WMT'13 Shared Task on Quality Estimation (QE)", "start_pos": 55, "end_pos": 100, "type": "TASK", "confidence": 0.7081349028481377}]}, {"text": "Our participation is reduced to task 1.2 on System Selection.", "labels": [], "entities": [{"text": "System Selection", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.8416636288166046}]}, {"text": "We used abroad set of features (86 for German-to-English and 97 for English-to-Spanish) ranging from standard QE features to features based on pseudo-references and semantic similarity.", "labels": [], "entities": []}, {"text": "We approached system selection by means of pairwise ranking decisions.", "labels": [], "entities": [{"text": "system selection", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.8118176162242889}]}, {"text": "For that, we learned Random Forest classifiers especially tailored for the problem.", "labels": [], "entities": [{"text": "Random Forest classifiers", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.658665935198466}]}, {"text": "Evaluation at development time showed considerably good results in a cross-validation experiment , with Kendall's \u03c4 values around 0.30.", "labels": [], "entities": [{"text": "Kendall's \u03c4", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.7072566747665405}]}, {"text": "The results on the test set dropped significantly, raising different discussions to betaken into account.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we discuss the TALP-UPC 1 participation in the WMT'13 Shared Task on Quality Estimation (QE).", "labels": [], "entities": [{"text": "WMT'13 Shared Task on Quality Estimation (QE)", "start_pos": 61, "end_pos": 106, "type": "TASK", "confidence": 0.6830417580074735}]}, {"text": "Our participation is circumscribed to task 1.2, which deals with System Selection.", "labels": [], "entities": [{"text": "System Selection", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.8428502976894379}]}, {"text": "Concretely, we were required to rank up to five alternative translations for the same source sentence produced by multiple MT systems, in the absence of any reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.9142958521842957}]}, {"text": "We used abroad set of features; mainly available through the last version of the ASIYA toolkit for MT evaluation (.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9614871740341187}]}, {"text": "Concretely, we derived 86 features for the German-to-English subtask and 97 features for English-to-Spanish.", "labels": [], "entities": []}, {"text": "These features cover different approaches and include standard Quality Estimation features, as provided by the above mentioned ASIYA toolkit and Quest (), but also a variety of features based on pseudoreferences, explicit semantic analysis ( and specialized language models.", "labels": [], "entities": []}, {"text": "See section 3 for details.", "labels": [], "entities": []}, {"text": "In order to model the ranking problem associated to the system selection task, we adapted it to a classification task of pairwise decisions.", "labels": [], "entities": []}, {"text": "We trained Random Forest classifiers (and compared them to SVM classifiers), expanding the work of, from which a full ranking can be derived and the best system per sentence identified.", "labels": [], "entities": []}, {"text": "Evaluation at development time, using crossvalidation, showed considerably good and stable results for both language pairs, with correlation values around 0.30 (Kendall \u03c4 coefficient) classification accuracies around 52% (pairwise classification) and 41% (best translation identification).", "labels": [], "entities": [{"text": "Kendall \u03c4 coefficient) classification accuracies", "start_pos": 161, "end_pos": 209, "type": "METRIC", "confidence": 0.9055347839991251}, {"text": "translation identification", "start_pos": 261, "end_pos": 287, "type": "TASK", "confidence": 0.9425244927406311}]}, {"text": "Unfortunately, the results on the test set were significantly lower.", "labels": [], "entities": []}, {"text": "Current research is devoted to explain the behavior of the system at testing time.", "labels": [], "entities": []}, {"text": "On the one hand, it seems clear that more research regarding the assignment of ties is needed in order to have a robust model.", "labels": [], "entities": []}, {"text": "On the other hand, the release of the gold standard annotations for the test set will facilitate a deeper analysis and understanding of the current results.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the ranking models studied for the system selection problem.", "labels": [], "entities": [{"text": "system selection problem", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8494797547658285}]}, {"text": "Section 3 describes the features used for learning.", "labels": [], "entities": []}, {"text": "Section 4 presents the setting for parameter optimization and feature selection and the results obtained.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7244155406951904}, {"text": "feature selection", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7187601029872894}]}, {"text": "Finally, Section 5 summarizes the lessons learned so far and outlines some lines for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the experiments carried out to select the best feature set, learner, and learner configuration.", "labels": [], "entities": []}, {"text": "Additionally, we present the final performance within the task.", "labels": [], "entities": []}, {"text": "The setup experiments were addressed doing two separate 10-fold cross validations on the training data and averaging the final results.", "labels": [], "entities": []}, {"text": "We evaluated the results through three indicators: Kendall's \u03c4 with no penalization for the ties, accuracy in determining the pairwise relationship between candidate translations, and global accuracy in selecting the best candidate for each source sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9992743134498596}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9511264562606812}]}, {"text": "First, we compared our SVM learner against Random Forests with the two variants of data preprocessing (LINEAR and RBF).", "labels": [], "entities": [{"text": "LINEAR", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9837274551391602}, {"text": "RBF", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.7189362049102783}]}, {"text": "In terms of Kendall's \u03c4 , we found that the Random Forests (RF) were clearly better compared to SVM implementation.", "labels": [], "entities": []}, {"text": "Concretely, depending on the final feature set, we found that RF achieved a \u03c4 between 0.23 and 0.29 while SVM achieved a \u03c4 between 0.23 and 0.25.", "labels": [], "entities": [{"text": "RF", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.6556013822555542}]}, {"text": "With respect to the accuracy measures we did not find noticeable differences between methods as their results moved from 49% to 52%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995087385177612}]}, {"text": "However, considering the accuracy in terms of selecting only the best system there was a difference of two points (42.2% vs. 40.0%) between methods, being RF again the best system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995406866073608}]}, {"text": "Regarding the pairwise preprocessing the results between RBF and LINEAR based preprocessing were comparable, being RBF slightly better than LINEAR.", "labels": [], "entities": [{"text": "RBF", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.851837694644928}, {"text": "RBF", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9458799362182617}]}, {"text": "Hence, we selected Random Forests with RBF pairwise preprocessing as our final learner.", "labels": [], "entities": []}, {"text": "For the feature selection process, we considered the most relevant combinations of feature groups.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8714476823806763}]}, {"text": "shows the set-up results for the de-en subtask and shows the results for the en-es subtask.", "labels": [], "entities": []}, {"text": "In terms of \u03c4 we observed similar results between the two language pairs.", "labels": [], "entities": []}, {"text": "However accuracies for the de-en subtask were one point above the ones for en-es.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.9948229789733887}]}, {"text": "Regarding the features used, we found that the best feature combination to use was composed of: i) a baseline QE feature set (Asiya or Quest) but not both of them, ii) Length Model, iii) Pseudo-reference aligned based features and the use of iv) adapted language models.", "labels": [], "entities": []}, {"text": "However, within the de-en subtask, we found that substituting Length Model and Aligned Pseudo-references by the features based on Semantic Roles (SEM) could bring marginally better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9963834285736084}]}, {"text": "We also noticed that the learner was sensitive to the features used so selecting the appropriate set of features was crucial to achieve a good performance.", "labels": [], "entities": []}, {"text": "In, 5 and 6 we present the official results for the WMT'13 Quality Estimation Task, in all evaluation variants.", "labels": [], "entities": [{"text": "WMT'13 Quality Estimation Task", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7138672918081284}]}, {"text": "In each table we compare to the best/worst performing systems and also to the official baseline.", "labels": [], "entities": []}, {"text": "We can observe that in general the results on the test sets drop significantly, compared to our   set-up experiments.", "labels": [], "entities": []}, {"text": "Restricting to the evaluation setting in which ties are not penalized (i.e., corresponding to our setting during system and parameter tuning), we can see that the results corresponding to de-en) are comparable to our set-up results and close to the best performing system.", "labels": [], "entities": []}, {"text": "However, in the en-es language pair the final results are comparatively much lower).", "labels": [], "entities": []}, {"text": "We find this behavior strange.", "labels": [], "entities": []}, {"text": "In this respect, we analyzed the inter-annotator agreement within the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9302965402603149}]}, {"text": "Concretely we computed the Cohen's \u03ba for all overlapping annotations concerning at least 4 systems for both language pairs.", "labels": [], "entities": []}, {"text": "The results of our analysis are presented in and therefore it confirms our hypothesis that en-es annotations had more noise providing an explanation for the accuracy decrease of our QE models and setting the subtask into a more challenging scenario.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9989350438117981}]}, {"text": "However, further research will be needed to analyze other factors such as oracles and improvement on automatic metrics prediction and reliability compared to linguistic expert annotators.", "labels": [], "entities": []}, {"text": "Another remaining issue for our research concerns investigating better ways to deal with ties, as their penalization lowered our results dramatically.", "labels": [], "entities": []}, {"text": "In this direction we plan to work further on", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Set-up results for de-en", "labels": [], "entities": [{"text": "de-en", "start_pos": 29, "end_pos": 34, "type": "TASK", "confidence": 0.8000630140304565}]}, {"text": " Table 2: Setup results for en-es", "labels": [], "entities": []}, {"text": " Table 3: Official results for the de-en subtask (ties  penalized)", "labels": [], "entities": []}, {"text": " Table 5: Official results for the de-en subtask (ties  ignored)", "labels": [], "entities": []}, {"text": " Table 6: Official results for the en-es subtask (ties  ignored)", "labels": [], "entities": []}, {"text": " Table 7: Golden standard test set agreement coef- ficients measured by Cohen's \u03ba", "labels": [], "entities": []}]}