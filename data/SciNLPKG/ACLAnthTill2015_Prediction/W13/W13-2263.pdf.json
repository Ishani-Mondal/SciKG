{"title": [{"text": "Hidden Markov Tree Model for Word Alignment", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.763299435377121}]}], "abstractContent": [{"text": "We propose a novel unsupervised word alignment model based on the Hidden Markov Tree (HMT) model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7414507269859314}]}, {"text": "Our model assumes that the alignment variables have a tree structure which is isomorphic to the target dependency tree and models the distortion probability based on the source dependency tree, thereby incorporating the syntactic structure from both sides of the parallel sentences.", "labels": [], "entities": []}, {"text": "In English-Japanese word alignment experiments, our model outperformed an IBM Model 4 baseline by over 3 points alignment error rate.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.6832848340272903}, {"text": "alignment error rate", "start_pos": 112, "end_pos": 132, "type": "METRIC", "confidence": 0.9483405947685242}]}, {"text": "While our model was sensitive to posterior thresholds, it also showed a performance comparable to that of HMM alignment models.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.8080333769321442}]}], "introductionContent": [{"text": "Automatic word alignment is the first step in the pipeline of statistical machine translation.", "labels": [], "entities": [{"text": "Automatic word alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5988920529683431}, {"text": "statistical machine translation", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.773503303527832}]}, {"text": "Translation models are usually extracted from wordaligned bilingual corpora, and lexical translation probabilities based on word alignment models are also used for translation.", "labels": [], "entities": []}, {"text": "The most widely used models are the IBM Model 4 ( and Hidden Markov Models (HMM) ().", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8676222761472067}]}, {"text": "These models assume that alignments are largely monotonic, possibly with a few jumps.", "labels": [], "entities": []}, {"text": "While such assumption might be adequate for alignment between similar languages, it does not necessarily hold between a pair of distant languages like English and Japanese.", "labels": [], "entities": []}, {"text": "Recently, several models have focused on incorporating syntactic structures into word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7665699422359467}]}, {"text": "As an extension to the HMM alignment, present a distortion model conditioned on the source-side dependency tree, and propose a distortion model based on the path through the source-side phrase-structure tree.", "labels": [], "entities": []}, {"text": "Some supervised models receive syntax trees as their input and use them to generate features and to guide the search (, and other models learn a joint model for parsing and word alignment from word-aligned parallel trees).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 173, "end_pos": 187, "type": "TASK", "confidence": 0.6733877956867218}]}, {"text": "In the context of phrase-to-phrase alignment, propose a Bayesian subtree alignment model trained with parallel sampling.", "labels": [], "entities": [{"text": "phrase-to-phrase alignment", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.7220813632011414}]}, {"text": "None of these models, however, can incorporate syntactic structures from both sides of the language pair and can be trained computationally efficiently in an unsupervised manner at the same time.", "labels": [], "entities": []}, {"text": "The Hidden Markov Tree (HMT) model) is one such model that satisfies the above-mentioned properties.", "labels": [], "entities": []}, {"text": "The HMT model assumes a tree structure of the hidden variables, which fits well with the notion of word-toword dependency, and it can be trained from unlabeled data via the EM algorithm with the same order of time complexity as HMMs.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel word alignment model based on the HMT model and show that it naturally enables unsupervised training based on both source and target dependency trees in a tractable manner.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8003465831279755}]}, {"text": "We also compare our HMT word alignment model with the IBM Model 4 and the HMM alignment models in terms of the standard alignment error rates on a publicly available English-Japanese dataset.", "labels": [], "entities": [{"text": "HMT word alignment", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7640429337819418}]}], "datasetContent": [{"text": "We evaluate the performance of our HMT alignment model in terms of the standard alignment error rate 2 (AER) on a publicly available EnglishJapanese dataset, and compare it with the IBM Model 4 ( and HMM alignment with distance-based (HMM) and syntax-based (S-HMM) distortion models (.", "labels": [], "entities": [{"text": "HMT alignment", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.8945543467998505}, {"text": "alignment error rate 2 (AER)", "start_pos": 80, "end_pos": 108, "type": "METRIC", "confidence": 0.8944080982889447}, {"text": "EnglishJapanese dataset", "start_pos": 133, "end_pos": 156, "type": "DATASET", "confidence": 0.9151698350906372}]}, {"text": "We use the data from the Kyoto Free Translation Task (KFTT) version 1.3 (Neubig, 2011).", "labels": [], "entities": [{"text": "Kyoto Free Translation Task (KFTT) version 1.3 (Neubig, 2011)", "start_pos": 25, "end_pos": 86, "type": "DATASET", "confidence": 0.7749541188989367}]}, {"text": "Table 1 shows the corpus statistics.", "labels": [], "entities": []}, {"text": "Note that these numbers are slightly different from the ones observed under the dataset's default training procedure because of the difference in the preprocessing scheme, which is explained below.", "labels": [], "entities": []}, {"text": "1 \u2212 AER on this dataset.", "labels": [], "entities": [{"text": "AER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9996433258056641}]}], "tableCaptions": [{"text": " Table 2: Alignment error rates (AER) based on  each model's peak performance.", "labels": [], "entities": [{"text": "Alignment error rates (AER)", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.8795865376790365}]}]}