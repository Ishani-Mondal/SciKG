{"title": [{"text": "Improving Pointwise Mutual Information (PMI) by Incorporating Significant Co-occurrence", "labels": [], "entities": [{"text": "Improving Pointwise Mutual Information (PMI)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8988289322171893}]}], "abstractContent": [{"text": "We design anew co-occurrence based word association measure by incorporating the concept of significant co-occurrence in the popular word association measure Pointwise Mutual Information (PMI).", "labels": [], "entities": [{"text": "word association measure Pointwise Mutual Information (PMI)", "start_pos": 133, "end_pos": 192, "type": "TASK", "confidence": 0.5327890382872688}]}, {"text": "By extensive experiments with a large number of publicly available datasets we show that the newly introduced measure performs better than other co-occurrence based measures and despite being resource-light, compares well with the best known resource-heavy dis-tributional similarity and knowledge based word association measures.", "labels": [], "entities": []}, {"text": "We investigate the source of this performance improvement and find that of the two types of significant co-occurrence-corpus-level and document-level, the concept of corpus level significance combined with the use of document counts in place of word counts is responsible for all the performance gains observed.", "labels": [], "entities": []}, {"text": "The concept of document level significance is not helpful for PMI adaptation.", "labels": [], "entities": [{"text": "PMI adaptation", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.9885777235031128}]}], "introductionContent": [{"text": "Co-occurrence based word association measures like PMI, LLR, and Dice are popular since they are easy to understand and computationally efficient.", "labels": [], "entities": []}, {"text": "They measure the strength of association between two words by comparing the word pair's corpus-level bigram frequency to some function of the unigram frequencies of the individual words.", "labels": [], "entities": []}, {"text": "Recently anew measure called Co-occurrence Significance Ratio (CSR) was introduced in) based on the notion of significant co-occurrence.", "labels": [], "entities": [{"text": "Co-occurrence Significance Ratio (CSR)", "start_pos": 29, "end_pos": 67, "type": "METRIC", "confidence": 0.7578362623850504}]}, {"text": "Since CSR was found to perform better than other co-occurrence measures, in this work, our goal was to incorporate the concept of significant co-occurrence in traditional word-association measures to design new measures that may perform better than both CSR and the traditional measures.", "labels": [], "entities": []}, {"text": "Two different notions of significant cooccurrence are employed in CSR: \u2022 Corpus-level significant co-occurrence determines whether the ratio of observed bigram occurrences to their expected occurrences across the corpus can be explained as a pure chance phenomenon, and, \u2022 Document-level significant co-occurrence determines whether a large fraction of a word-pair's occurrences within a given document have smaller spans than that under a null model where the words in the document are permuted randomly.", "labels": [], "entities": []}, {"text": "While both these notions are employed in an integrated fashion in CSR, on analyzing CSR details, we realized that these two concepts are independent and can be applied separately to any word association measure which is a ratio of some variable's observed frequency to its expected frequency.", "labels": [], "entities": []}, {"text": "We incorporate the concepts of corpus-level and document-level significant cooccurrence in PMI to design anew measure that performs better than both PMI and CSR, as well as other co-occurrence based word association measures.", "labels": [], "entities": []}, {"text": "To incorporate document level significance, we need to use document level counts instead of word level counts (this distinction is explained in detail in Section 4.3).", "labels": [], "entities": []}, {"text": "To investigate whether the performance gains observed are because of the concept of significant co-occurrence or simply because of the fact that we are using document counts instead of the word counts, we also design document count based baseline version of PMI called PMId, and several intermediate variants whose definitions are given in.", "labels": [], "entities": []}, {"text": "not contribute to the PMI performance improvement.", "labels": [], "entities": [{"text": "PMI", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.896629273891449}]}, {"text": "Two newly designed, best-performing measures cPMId and cPMIz have almost identical performance.", "labels": [], "entities": []}, {"text": "As the definitions in show, cPMId incorporates corpus level significance in a document count based version of PMI but does not employ the concept of document level significance, whereas cPMIz employs both corpus and document level significance.", "labels": [], "entities": []}, {"text": "This demonstrates that the concept of corpus level significance combined with document counts is responsible for all the performance gains observed.", "labels": [], "entities": []}, {"text": "To summarize, we make the following contributions in this work: \u2022 We incorporate the notion of significant cooccurrence in PMI to design anew measure cPMId that performs better than PMI as well as other popular co-occurrence based wordassociation measures on both free association and semantic relatedness tasks.", "labels": [], "entities": []}, {"text": "In addition, despite being resource-light, cPMId performs as well as the best known distributional similarity and knowledge based measures which are resource-intensive.", "labels": [], "entities": []}, {"text": "\u2022 We investigate the source of this performance improvement and find that of the two notions We consider only those word-pair occurrences where inter-word distance between x and y is atmost s, the span threshold.", "labels": [], "entities": []}, {"text": "For a particular occurrence of x, we get a window of size son either side within which y can occur.", "labels": [], "entities": []}, {"text": "Strictly speaking, there should be a factor 2s in the denominator of the formula for PMI.", "labels": [], "entities": [{"text": "PMI", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.8904442191123962}]}, {"text": "Since we are only interested in the relative rankings of word-pairs, we follow the standard practice of ignoring the 2s factor, as its removal affects only the absolute PMI values but not the relative rankings. of significance -corpus-level and documentlevel significant co-occurrence, the concept of document level significant co-occurrence is not helpful for PMI adaptation.", "labels": [], "entities": [{"text": "PMI adaptation", "start_pos": 361, "end_pos": 375, "type": "TASK", "confidence": 0.9924668073654175}]}, {"text": "The concept of corpus level significance combined with document counts is responsible for all the performance gains observed.", "labels": [], "entities": []}], "datasetContent": [{"text": "Having introduced various measures, we wish to determine whether the incorporation of corpus and document level significance improves the performance of PMI.", "labels": [], "entities": []}, {"text": "Also, if the adapted versions perform better than PMI, what are the sources of the improvements.", "labels": [], "entities": [{"text": "PMI", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.4847239851951599}]}, {"text": "Is it the concept of corpus level or document level significance or both, or is the performance gain simply a result of the fact that we are counting documents instead of words?", "labels": [], "entities": []}, {"text": "Since the newly introduced measures have multiple parameters, how sensitive is their performance to the parameter values.", "labels": [], "entities": []}, {"text": "To answer these questions, we repeat the experiments performed in), using the exact same dataset, resources, and methodology -the same 1.24 Gigawords Wikipedia corpus and the same eight publicly available datasets -Edinburgh (,,,, WhiteAbrams (), GoldfarbHalpern (, Wordsim (), and Esslli.", "labels": [], "entities": [{"text": "Gigawords Wikipedia corpus", "start_pos": 140, "end_pos": 166, "type": "DATASET", "confidence": 0.7541922330856323}, {"text": "Edinburgh", "start_pos": 215, "end_pos": 224, "type": "DATASET", "confidence": 0.8519911170005798}, {"text": "GoldfarbHalpern", "start_pos": 247, "end_pos": 262, "type": "DATASET", "confidence": 0.9051803946495056}]}, {"text": "Of these, Wordsim measures semantic relatedness which encompasses relations like synonymy, meronymy, antonymy, and functional association ().", "labels": [], "entities": []}, {"text": "All other datasets measure free association which refers to the first response given by a subject on being given a stimulus word.", "labels": [], "entities": []}, {"text": "Each measure is evaluated by the correlation between the ranking of word-associations produced by the measure and the gold-standard human ranking for that dataset.", "labels": [], "entities": []}, {"text": "Since all methods have at least one parameter, we perform five-fold cross validation.", "labels": [], "entities": []}, {"text": "The span parameter sis varied between 5 and 50 words, and and \u03b4 are varied between 0 and 1.", "labels": [], "entities": []}, {"text": "Each dataset is partitioned into five folds -four for   training and one for testing.", "labels": [], "entities": []}, {"text": "For each association measure, the parameter values that perform best on four training folds is used for the remaining one testing fold.", "labels": [], "entities": []}, {"text": "The performance of a measure on a dataset is its average Spearman rank correlation over 5 runs with 5 different test folds.", "labels": [], "entities": [{"text": "Spearman rank correlation", "start_pos": 57, "end_pos": 82, "type": "METRIC", "confidence": 0.829792300860087}]}, {"text": "Results of the 5-fold cross validation are shown in.", "labels": [], "entities": []}, {"text": "From the results we conclude that the concept of significant co-occurrence improves the performance of PMI.", "labels": [], "entities": [{"text": "PMI", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9612768888473511}]}, {"text": "The newly designed measures cPMId and cPMIz perform better than both PMI and CSR on all eight datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: 5-fold cross validation comparison of rank coefficients for different measures. The number of word-pairs in each", "labels": [], "entities": []}, {"text": " Table 5: 5-fold cross validation performance of cPMId for various parameter combinations. * indicates a varying parameter.", "labels": [], "entities": []}, {"text": " Table 6: 5-fold cross validation comparison of cPMId with other PMI variants.", "labels": [], "entities": []}, {"text": " Table 7: 5-fold cross validation comparison of cPMId with other co-occurrence based measures.", "labels": [], "entities": []}, {"text": " Table 8: Comparison of cPMId with knowledge-based and distributional similarity based measures for the Wordsim dataset.", "labels": [], "entities": [{"text": "Wordsim dataset", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.9047759473323822}]}]}