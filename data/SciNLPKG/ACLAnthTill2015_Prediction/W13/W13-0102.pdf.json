{"title": [{"text": "Evaluating Topic Coherence Using Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces distributional semantic similarity methods for automatically measuring the coherence of a set of words generated by a topic model.", "labels": [], "entities": []}, {"text": "We construct a semantic space to represent each topic word by making use of Wikipedia as a reference corpus to identify context features and collect frequencies.", "labels": [], "entities": []}, {"text": "Relatedness between topic words and context features is measured using variants of Pointwise Mutual Information (PMI).", "labels": [], "entities": []}, {"text": "Topic coherence is determined by measuring the distance between these vectors computed using a variety of metrics.", "labels": [], "entities": []}, {"text": "Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic modelling is a popular statistical method for (soft) clustering documents ().", "labels": [], "entities": [{"text": "Topic modelling", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8078442215919495}]}, {"text": "Latent Dirichlet Allocation (LDA) (, one type of topic model, has been widely used in NLP and applied to a range of tasks including word sense disambiguation, multi-document summarisation) and generation of comparable corpora.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6345274647076925}, {"text": "word sense disambiguation", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.6717159152030945}, {"text": "multi-document summarisation", "start_pos": 159, "end_pos": 187, "type": "TASK", "confidence": 0.6101836562156677}]}, {"text": "A variety of approaches has been proposed to evaluate the topics generated by these models.", "labels": [], "entities": []}, {"text": "The first to be explored were extrinsic methods, measuring the performance achieved by a model in a specific task or using statistical methods.", "labels": [], "entities": []}, {"text": "For example, topic models have been evaluated by measuring their accuracy for information retrieval).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9989833235740662}, {"text": "information retrieval", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.7527041435241699}]}, {"text": "Statistical methods have also been applied to measure the predictive likelihood of a topic model in held-out documents by computing their perplexity.", "labels": [], "entities": []}, {"text": "gives a detailed description of such statistical metrics.", "labels": [], "entities": []}, {"text": "However, these approaches do not provide any information about how interpretable the topics are to humans.", "labels": [], "entities": []}, {"text": "shows some example topics generated by a topic model.", "labels": [], "entities": []}, {"text": "The first three topics appear quite coherent, all the terms in each topic are associated with a common theme.", "labels": [], "entities": []}, {"text": "On the other hand, it is difficult to identify a coherent theme connecting all of the words in topics 4 and 5.", "labels": [], "entities": []}, {"text": "These topics are difficult to interpret and could be considered as \"junk\" topics.", "labels": [], "entities": []}, {"text": "Interpretable topics are important in applications such as visualisation of document collections), where automatically generated topics are used to provide an overview of the collection and the top-n words in each topic used to represent it. showed that humans find topics generated by models with high predictive likelihood to be less coherent than topics generated from others with lower predictive likelihood.", "labels": [], "entities": []}, {"text": "Following Chang's findings, recent work on evaluation of topic models has been focused on automatically measuring the coherence of generated topics by comparing them against human judgements).", "labels": [], "entities": []}, {"text": "define topic coherence as the average semantic relatedness between topic words and report the best correlation with humans using the Pointwise Mutual Information (PMI) between topic words in Wikipedia.", "labels": [], "entities": []}, {"text": "Topics are represented by top-n most probable words.", "labels": [], "entities": []}, {"text": "Following this direction, we explore methods for automatically determining the coherence of topics.", "labels": [], "entities": []}, {"text": "We propose a novel approach for measuring topic coherence based on the distributional hypothesis which states that words with similar meanings tend to occur in similar context.", "labels": [], "entities": []}, {"text": "Wikipedia is used as a reference corpus to create a distributional semantic model.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8894551396369934}]}, {"text": "Each topic word is represented as a bag of highly co-occurring context words that are weighted using either PMI or a normalised version of PMI (NPMI).", "labels": [], "entities": []}, {"text": "We also explore creating the vector space using differing numbers of context terms.", "labels": [], "entities": []}, {"text": "All methods are evaluated by measuring correlation with humans on three different sets of topics.", "labels": [], "entities": []}, {"text": "Results indicating that measures on the fuller vector space are comparable to the state-of-the-art proposed by, while performance consistently improves using a reduced vector space.", "labels": [], "entities": []}, {"text": "The remainder of this article is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents background work related to topic coherence evaluation.", "labels": [], "entities": [{"text": "topic coherence evaluation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.7182560960451762}]}, {"text": "Section 3 describes the distributional methods for measuring topic coherence.", "labels": [], "entities": []}, {"text": "Section 4 explains the experimental set-up used for evaluation.", "labels": [], "entities": []}, {"text": "Our results are described in Section 5 and the conclusions in Section 6.", "labels": [], "entities": []}, {"text": "proposed a method for generating coherent topics which used a mixture of Dirichlet distributions to incorporate domain knowledge.", "labels": [], "entities": []}, {"text": "Their approach prefers words that have similar probability (high or low) within all topics and rejects words that have different probabilities across topics.", "labels": [], "entities": []}, {"text": "describe the first attempt to automatically evaluate topics inferred from topic models.", "labels": [], "entities": []}, {"text": "Three criteria are applied to identify junk or insignificant topics.", "labels": [], "entities": []}, {"text": "Those criteria are in the form of probability distributions over the highest probability words.", "labels": [], "entities": []}, {"text": "For example, topics in which the probability mass is distributed approximately equally across all words are considered likely to be difficult to interpret.", "labels": [], "entities": []}], "datasetContent": [{"text": "Human judgements of topic coherence were collected through a crowdsourcing platform, CrowdFlower . Participants were presented with 10 word sets, each of which represents a topic.", "labels": [], "entities": []}, {"text": "They asked to judge topic coherence on a 3-point Likert scale from 1-3, where 1 denotes a \"Useless\" topic (i.e. words appear random and unrelated to each other), 2 denotes \"Average\" quality (i.e. some of the topic words are coherent and interpretable but others are not), and 3 denotes a \"Useful\" topic (i.e. one that is semantically coherent, meaningful and interpretable).", "labels": [], "entities": []}, {"text": "Each participant was asked to judge up to 100 topics from a single collection.", "labels": [], "entities": []}, {"text": "The average response for each topic was calculated as the coherency score for the gold-standard.", "labels": [], "entities": []}, {"text": "To ensure reliability and avoid random answers in the survey, we used a number of questions with predefined answer (either totally random words as topics or obvious topics such as week days).", "labels": [], "entities": [{"text": "reliability", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9760918021202087}]}, {"text": "Annotations from participants that failed to answer these questions correctly were removed.", "labels": [], "entities": [{"text": "Annotations", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9655594825744629}]}, {"text": "We run three surveys, one for each topic collection of 100 topics.", "labels": [], "entities": []}, {"text": "The total number of filtered responses obtained for the NYT dataset was 1, 778 from 26 participants, while for the 20NG dataset we collected 1, 707 answers from 24 participants.", "labels": [], "entities": [{"text": "NYT dataset", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9664479494094849}, {"text": "20NG dataset", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.8923675119876862}]}, {"text": "The participants were recruited by a broadcast email sent to all academic staff and graduate students in our institution.", "labels": [], "entities": []}, {"text": "For the Genomics dataset the emails were sent only to members of the medical school and biomedical engineering departments.", "labels": [], "entities": [{"text": "Genomics dataset", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.7882501482963562}]}, {"text": "We collected 1, 050 judgements from 12 participants for this data set.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement (IAA) is measured as the average of the Spearman correlation between the set of scores of each survey respondent and the average of the other respondents' scores.", "labels": [], "entities": [{"text": "Inter-annotator agreement (IAA)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.8740137457847595}]}, {"text": "The IAA in the three surveys is 0.70, 0.64 and 0.54 for NYT, 20NG and Genomics respectively.", "labels": [], "entities": [{"text": "IAA", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9994833469390869}, {"text": "NYT", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9306779503822327}, {"text": "Genomics", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8507775664329529}]}, {"text": "shows the results obtained for all of the methods on the three datasets.", "labels": [], "entities": []}, {"text": "Performance of each method is measured as the average Spearman correlation with human judgements.", "labels": [], "entities": []}, {"text": "The top row of each table shows the result using the average PMI approach (Newman et al., 2010b) while the next two rows show the results obtained by substituting PMI with NPMI and the method proposed by.", "labels": [], "entities": []}, {"text": "The main part of each table shows performance using the approaches described in Section 3 using various combinations of methods for constructing the semantic space and determining the similarity between vectors.", "labels": [], "entities": []}, {"text": "Using the average PMI between topic words correlates well with human judgements, 0.71 for NYT, 0.73 for 20NG and 0.75 for Genomics confirming results reported by.", "labels": [], "entities": [{"text": "PMI", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9673495888710022}, {"text": "NYT", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.7785122990608215}]}, {"text": "However, NPMI performs better than PMI, with an improvement in correlation of 0.03 for all datasets.", "labels": [], "entities": [{"text": "correlation", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9916701316833496}]}, {"text": "The improvement is down to the fact that NPMI reduces the impact of low frequency counts in word cooccurrences and therefore uses more reliable estimates.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of methods for measuring topic coherence (Spearman Rank correlation with human  judgements).", "labels": [], "entities": [{"text": "Spearman Rank correlation", "start_pos": 64, "end_pos": 89, "type": "METRIC", "confidence": 0.8572251598040262}]}]}