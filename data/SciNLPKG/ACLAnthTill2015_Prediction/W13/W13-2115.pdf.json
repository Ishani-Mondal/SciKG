{"title": [{"text": "Generating student feedback from time-series data using Reinforcement Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a statistical Natural Language Generation (NLG) method for summarisa-tion of time-series data in the context of feedback generation for students.", "labels": [], "entities": [{"text": "statistical Natural Language Generation (NLG)", "start_pos": 14, "end_pos": 59, "type": "TASK", "confidence": 0.7331760142530713}, {"text": "feedback generation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7738776206970215}]}, {"text": "In this paper, we initially present a method for collecting time-series data from students (e.g. marks, lectures attended) and use example feedback from lecturers in a data-driven approach to content selection.", "labels": [], "entities": [{"text": "content selection", "start_pos": 192, "end_pos": 209, "type": "TASK", "confidence": 0.6983224004507065}]}, {"text": "We show a novel way of constructing a reward function for our Reinforcement Learning agent that is informed by the lecturers' method of providing feedback.", "labels": [], "entities": []}, {"text": "We evaluate our system with undergraduate students by comparing it to three baseline systems: a rule-based system, lecturer-constructed summaries and a Brute Force system.", "labels": [], "entities": []}, {"text": "Our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers.", "labels": [], "entities": []}, {"text": "Our findings suggest that the learning agent needs to take into account both the student and lec-turers' preferences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-to-text generation refers to the task of automatically generating text from non-linguistic data).", "labels": [], "entities": [{"text": "Data-to-text generation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7040445357561111}]}, {"text": "The goal of this work is to develop a method for summarising time-series data in order to provide continuous feedback to students across the entire semester.", "labels": [], "entities": [{"text": "summarising time-series", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.8448894023895264}]}, {"text": "As a case study, we took a module in Artificial Intelligence and asked students to fill out a very short diarytype questionnaire on a weekly basis.", "labels": [], "entities": []}, {"text": "Questions included, for example, number of deadlines, number of classes attended, severity of personal issues.", "labels": [], "entities": []}, {"text": "These data were then combined with the marks from the weekly lab reflecting the students' performance.", "labels": [], "entities": []}, {"text": "As data is gathered each week in the lab, we now have a set of time-series data and our goal is to automatically create feedback.", "labels": [], "entities": []}, {"text": "The goal is to present a holistic view through these diary entries of how the student is doing and what factors maybe affecting performance.", "labels": [], "entities": []}, {"text": "Feedback is very important in the learning process but very challenging for academic staff to complete in a timely manner given the large number of students and the increasing pressures on academics' time.", "labels": [], "entities": []}, {"text": "This is where automatic feedback can play apart, providing a tool for teachers that can give insight into factors that may not be immediately obvious).", "labels": [], "entities": []}, {"text": "As reflected in NSS surveys 1 , students are not completely satisfied with how feedback is currently delivered.", "labels": [], "entities": [{"text": "NSS surveys 1", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.9494559367497762}]}, {"text": "The 2012 NSS survey, for all disciplines reported an 83% satisfaction rate with courses, with 70% satisfied with feedback.", "labels": [], "entities": [{"text": "NSS survey", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.8745914995670319}]}, {"text": "This has improved from recent years (in 2006 this was 60% for feedback) but shows that there is still room for improvement in how teachers deliver feedback and its content.", "labels": [], "entities": []}, {"text": "In the next section (Section 2) a discussion of the related work is presented.", "labels": [], "entities": []}, {"text": "In Section 3, a description of the methodology is given as well as the process of the data collection from students, the template construction and the data collection with lecturers.", "labels": [], "entities": []}, {"text": "In Section 4, the Reinforcement Learning implementation is described.", "labels": [], "entities": [{"text": "Reinforcement Learning", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8755013346672058}]}, {"text": "In Section 5, the evaluation results are presented, and finally, in Sections 6 and 7, a conclusion and directions for future work are discussed.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the system using the reward function and with students.", "labels": [], "entities": []}, {"text": "In both these evaluations, we compared feedback reports generated using our Reinforcement Learning agent with four other baseline systems.", "labels": [], "entities": []}, {"text": "Here we present a brief description of the baseline systems.", "labels": [], "entities": []}, {"text": "Baseline 1: Rule-based system.", "labels": [], "entities": []}, {"text": "This system selects factors and templates for generation using a set of rules.", "labels": [], "entities": []}, {"text": "These hand-crafted rules were derived from a combination of the L&T expert's advice and a student's preferences and is therefore a challenging baseline and represents a middle ground between the L&T expert's advice and a student's preferences.", "labels": [], "entities": []}, {"text": "An example rule is: if the mark average is less than 50% then refer to revision.", "labels": [], "entities": [{"text": "mark average", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.9775440990924835}, {"text": "revision", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.989589273929596}]}, {"text": "Baseline 2: Brute Force system.", "labels": [], "entities": []}, {"text": "This system performs a search of the state space, by exploring randomly as many different feedback summaries as possible.", "labels": [], "entities": []}, {"text": "The Brute Force algorithm is shown below: In each run the algorithm constructs a feedback summary, then it calculates its reward, using the same reward function used for the Reinforcement Learning approach, and if the reward of the new feedback is better than the previous, it keeps the new one as the best.", "labels": [], "entities": [{"text": "Reinforcement Learning", "start_pos": 174, "end_pos": 196, "type": "TASK", "confidence": 0.8783594071865082}]}, {"text": "It repeats this process for 10,000 times for each scenario.", "labels": [], "entities": []}, {"text": "Finally, the algorithm returns the summary that scored the highest ranking.", "labels": [], "entities": []}, {"text": "Baseline 3: Lecturer-produced summaries.", "labels": [], "entities": []}, {"text": "These are the summaries produced by the lecturers, as described in Section 2.4, for Task 2 using template-generated utterances.", "labels": [], "entities": []}, {"text": "Baseline 4: Random system: The Random system constructs feedback summaries by selecting factors and templates randomly as described in Task 3 (in Section 3.4).", "labels": [], "entities": []}, {"text": "presents the results of the evaluation performed using the Reward Function, comparing the learned policy with the four baseline systems.", "labels": [], "entities": []}, {"text": "Each system generated 26 feedback summaries.", "labels": [], "entities": []}, {"text": "On average the learned policy scores significantly higher than any other baseline for the given scenarios (p <0.05 in a paired t-test).: The table summarises the average rewards that are assigned to summaries produced from the different systems.", "labels": [], "entities": []}, {"text": "A subjective evaluation was conducted using 1st year students of Computer Science as participants.", "labels": [], "entities": []}, {"text": "We recruited 17 students, who were all English native speakers.", "labels": [], "entities": []}, {"text": "The participants were shown 4 feedback summaries in a random order, one generated by the learned policy, one from the rule-based system (Baseline 1), one from the Brute Force system (Baseline 2) and one summary produced by a lecturer using the templates (Baseline 3).", "labels": [], "entities": []}, {"text": "Given the poor performance of the Random system in terms of reward, Baseline 4 was omitted from this study.", "labels": [], "entities": []}, {"text": "Overall there were 26 different scenarios, as described in Section 3.1.", "labels": [], "entities": []}, {"text": "All summaries presented to a participant were generated from the same scenario.", "labels": [], "entities": []}, {"text": "The participants then had to rank the summaries in order of preference: 1 for the most preferred and 4 for the least preferred.", "labels": [], "entities": []}, {"text": "Each participant repeated the process for 4.5 scenarios on average (the participant was allowed to opt out at any stage).", "labels": [], "entities": []}, {"text": "The mode values of the rankings of the preferences of the students are shown in.", "labels": [], "entities": []}, {"text": "The web-based system used for the evaluation is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: The table summarises the average re- wards that are assigned to summaries produced  from the different systems.", "labels": [], "entities": []}]}