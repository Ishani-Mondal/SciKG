{"title": [{"text": "A Two-Stage Approach for Generating Unbiased Estimates of Text Complexity", "labels": [], "entities": [{"text": "Generating Unbiased Estimates of Text Complexity", "start_pos": 25, "end_pos": 73, "type": "TASK", "confidence": 0.5984552105267843}]}], "abstractContent": [{"text": "Many existing approaches for measuring text complexity tend to overestimate the complexity levels of informational texts while simultaneously underestimating the complexity levels of literary texts.", "labels": [], "entities": []}, {"text": "We present a two-stage estimation technique that successfully addresses this problem.", "labels": [], "entities": []}, {"text": "At Stage 1, each text is classified into one or another of three possible ge-nres: informational, literary or mixed.", "labels": [], "entities": []}, {"text": "Next, at Stage 2, a complexity score is generated for each text by applying one or another of three possible prediction models: one optimized for application to informational texts, one optimized for application to literary texts, and one optimized for application to mixed texts.", "labels": [], "entities": []}, {"text": "Each model combines lexical, syntactic and discourse features, as appropriate, to best rep-licate human complexity judgments.", "labels": [], "entities": []}, {"text": "We demonstrate that resulting text complexity predictions are both unbiased, and highly correlated with classifications provided by experienced educators.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated text analysis systems, such as readability metrics, are frequently used to assess the probability that texts with varying combinations of linguistic features will be more or less accessible to readers with varying levels of reading comprehension skill.", "labels": [], "entities": []}, {"text": "This paper introduces TextEvaluator, a fully-automated text analysis system designed to facilitate such work.", "labels": [], "entities": []}, {"text": "Our approach for addressing these differences can be summarized as follows.", "labels": [], "entities": []}, {"text": "First, a large set of lexical, syntactic and discourse features is extracted from each text.", "labels": [], "entities": []}, {"text": "Next, either human raters, or an automated genre classifier is used to classify each text into one or another of three possible genre categories: informational, literary, or mixed.", "labels": [], "entities": []}, {"text": "Finally, a complexity score is generated for each text by applying one or another of three possible prediction models: one optimized for application to informational texts, one optimized for application to literary texts, and one optimized for application to mixed texts.", "labels": [], "entities": []}, {"text": "We demonstrate that resulting complexity measures are both unbiased, and highly correlated with text grade level (GL) classifications provided by experienced educators.", "labels": [], "entities": []}, {"text": "TextEvaluator successfully addresses an important limitation of many existing readability metrics: the tendency to over-predict the complexity levels of informational texts, while simultaneously under-predicting the complexity levels of literary texts.", "labels": [], "entities": []}, {"text": "We illustrate this phenomenon, and argue that it results from two fundamental differences between informational and literary texts: (a) differences in the way that common every-day words are used and combined; and (b) differences in the rate at which rare words are repeated.", "labels": [], "entities": []}, {"text": "Our paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes related work on readability assessment.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.8527895212173462}]}, {"text": "Section 3 describes the two corpora assembled for use in this study, and outlines how genre and GL classifications were assigned.", "labels": [], "entities": [{"text": "GL classifications", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7002431750297546}]}, {"text": "Section 4 illustrates the problem of genre bias by considering the specific biases detected in two widely-used readability metrics.", "labels": [], "entities": [{"text": "genre bias", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.8122899234294891}]}, {"text": "Section 5 describes the TextEvaluator features, methods and results.", "labels": [], "entities": []}, {"text": "Section 6 presents a summary and discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Levels of Precision, Recall and F1 obtained for 1, 089 texts in the training and validation datasets.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990729093551636}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9985331296920776}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9991624355316162}]}, {"text": "Speeches are not included in this summary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Numbers of passages in the model develop- ment/training dataset, by grade level and genre.", "labels": [], "entities": []}, {"text": " Table 2. Numbers of passages in the validation dataset,  by grade band and genre.", "labels": [], "entities": [{"text": "validation dataset", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.902113139629364}]}, {"text": " Table 3. Mean GL classifications, by Average WF  score, for informational and literary passages targeted at  readers in grades 3 through 12.", "labels": [], "entities": [{"text": "Mean GL classifications", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.9019135038057963}, {"text": "Average WF  score", "start_pos": 38, "end_pos": 55, "type": "METRIC", "confidence": 0.832596997419993}]}, {"text": " Table 4. Levels of Precision, Recall and F1 obtained for  1, 089 texts in the training and validation datasets.  Speeches are not included in this summary.", "labels": [], "entities": [{"text": "Precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9965686798095703}, {"text": "Recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.997685432434082}, {"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9973006844520569}]}, {"text": " Table 6. Correlation between readability scores and  human grade band classifications for the 168 Common  Core texts in the validation dataset.", "labels": [], "entities": [{"text": "Common  Core texts in the validation dataset", "start_pos": 99, "end_pos": 143, "type": "DATASET", "confidence": 0.8279690146446228}]}]}