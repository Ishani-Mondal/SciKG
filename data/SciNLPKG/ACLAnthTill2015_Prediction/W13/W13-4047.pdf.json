{"title": [{"text": "Impact of ASR N-Best Information on Bayesian Dialogue Act Recognition", "labels": [], "entities": [{"text": "ASR N-Best Information on Bayesian Dialogue Act Recognition", "start_pos": 10, "end_pos": 69, "type": "TASK", "confidence": 0.8423749655485153}]}], "abstractContent": [{"text": "A challenge in dialogue act recognition is the mapping from noisy user inputs to dialogue acts.", "labels": [], "entities": [{"text": "dialogue act recognition", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.7981359362602234}]}, {"text": "In this paper we describe an approach for re-ranking dialogue act hypotheses based on Bayesian classifiers that incorporate dialogue history and Automatic Speech Recognition (ASR) N-best information.", "labels": [], "entities": [{"text": "re-ranking dialogue act hypotheses", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.9190678149461746}, {"text": "Automatic Speech Recognition (ASR) N-best", "start_pos": 145, "end_pos": 186, "type": "TASK", "confidence": 0.73859675015722}]}, {"text": "We report results based on the Let's Go dialogue corpora that show (1) that including ASR N-best information results in improved dialogue act recognition performance (+7% accuracy), and (2) that competitive results can be obtained from as early as the first system dialogue act, reducing the need to wait for subsequent system dialogue acts.", "labels": [], "entities": [{"text": "ASR N-best information", "start_pos": 86, "end_pos": 108, "type": "METRIC", "confidence": 0.8326905369758606}, {"text": "dialogue act recognition", "start_pos": 129, "end_pos": 153, "type": "TASK", "confidence": 0.636139988899231}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9973763227462769}]}], "introductionContent": [{"text": "The primary challenge of a Dialogue Act Recogniser (DAR) is to find the correct mapping between a noisy user input and its true dialogue act.", "labels": [], "entities": [{"text": "Dialogue Act Recogniser (DAR)", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6510044236977895}]}, {"text": "In standard \"slot-filling\" dialogue systems a dialogue act is generally represented as DialogueActType(attribute-value pairs), see Section 3.", "labels": [], "entities": []}, {"text": "While a substantial body of research has investigated different types of models and methods for dialogue act recognition in spoken dialogue systems (see Section 2), here we focus on re-ranking the outputs of an existing DAR for evaluation purposes.", "labels": [], "entities": [{"text": "dialogue act recognition", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7078848083813986}]}, {"text": "In practice the re-ranker should be part of the DAR itself.", "labels": [], "entities": [{"text": "DAR", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.743278980255127}]}, {"text": "We propose to use multiple Bayesian classifiers to re-rank an initial set of dialogue act hypotheses based on information from the dialogue history as well as ASR N-best lists.", "labels": [], "entities": []}, {"text": "In particular the latter type of information helps us to learn mappings between dialogue acts and common mis-recognitions.", "labels": [], "entities": []}, {"text": "We present experimental results based on the Let's Go dialogue corpora which indicate that re-ranking hypotheses using ASR N-best information can lead to improved recognition.", "labels": [], "entities": []}, {"text": "In addition, we compare the recognition accuracy overtime and find that high accuracy can be obtained with as little context as one system dialogue act, so that there is often no need to take a larger context into account.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.7989009022712708}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9975663423538208}]}], "datasetContent": [{"text": "We compared 7 different dialogue act recognisers in terms of classification accuracy.", "labels": [], "entities": [{"text": "dialogue act recognisers", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.657486230134964}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9772332310676575}]}, {"text": "The comparison was made against gold standard data from a human-labelled corpus.", "labels": [], "entities": []}, {"text": "(Semi-Random) is a recogniser choosing a random dialogue act from the Let's Go N-best parsing hypotheses.", "labels": [], "entities": [{"text": "Let's Go N-best parsing", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.4815869152545929}]}, {"text": "(Inc i ) is our proposed approach considering a context of i system dialogue acts, and (Ceiling) is a recogniser choosing the correct dialogue act from the Let's Go N-best parsing hypotheses.", "labels": [], "entities": []}, {"text": "The latter was used as a gold standard from manual annotations, which reflects the proportion of correct labels in the Nbest parsing hypotheses.", "labels": [], "entities": []}, {"text": "We also assessed the impact of ASR N-best information on probabilistic inference.", "labels": [], "entities": [{"text": "ASR N-best", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.7426207363605499}]}, {"text": "To this end, we compared Bayes nets with a focus on the random variable \"* nbest\", which in one case contains induced distributions from data and in the other case contains an equal distribution of slot values.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the former setting will lead to better performance.", "labels": [], "entities": []}, {"text": "shows the classification accuracy of our dialogue act recognisers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9885908961296082}]}, {"text": "The first point to notice is that the incorporation of ASR N-best information makes an important difference.", "labels": [], "entities": [{"text": "ASR N-best", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.7256226241588593}]}, {"text": "The performance of recogniser IncK (K being the number of system dialogue acts) is 66.9% without ASR N-best information and 73.9% with ASR Nbest information (the difference is significant   p < 0.05).", "labels": [], "entities": [{"text": "recogniser IncK", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.6718163192272186}]}, {"text": "The latter represents a substantial improvement over the semi-random baseline (62.9%) and Lets Go dialogue act recognizer (69%), both significant at p < 0.05.", "labels": [], "entities": [{"text": "Lets Go dialogue act recognizer", "start_pos": 90, "end_pos": 121, "type": "TASK", "confidence": 0.7289980769157409}]}, {"text": "A second point to notice is that the differences between Inc i (\u2200 i>0) recognisers were not significant.", "labels": [], "entities": []}, {"text": "We can say that the use of one system dialogue act as context is as competitive as using a larger set of system dialogue acts.", "labels": [], "entities": []}, {"text": "This suggests that dialogue act recognition carried out at early stages (e.g. after the first dialogue act) in an utterance does not degrade recognition performance.", "labels": [], "entities": [{"text": "dialogue act recognition", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.7592666745185852}]}, {"text": "The effect is possibly domain-specific and generalisations remain to be investigated.", "labels": [], "entities": []}, {"text": "Generally, we were able to observe that more than half of the errors made by the Bayesian classifiers were due to noise in the environment and caused by the users themselves, which interfered with ASR results.", "labels": [], "entities": [{"text": "ASR", "start_pos": 197, "end_pos": 200, "type": "TASK", "confidence": 0.9849880933761597}]}, {"text": "Detecting when users do not convey dialogue acts to the system is therefore still a standing challenge for dialogue act recognition.", "labels": [], "entities": [{"text": "Detecting", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9666399955749512}, {"text": "dialogue act recognition", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.7147488991419474}]}], "tableCaptions": []}