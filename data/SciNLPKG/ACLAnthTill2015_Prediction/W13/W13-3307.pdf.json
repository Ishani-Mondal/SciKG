{"title": [{"text": "Translation of \"It\" in a Deep Syntax Framework", "labels": [], "entities": [{"text": "Translation of \"It\"", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7918484568595886}]}], "abstractContent": [{"text": "We present a novel approach to the translation of the English personal pronoun it to Czech.", "labels": [], "entities": [{"text": "translation of the English personal pronoun it", "start_pos": 35, "end_pos": 81, "type": "TASK", "confidence": 0.8402318954467773}]}, {"text": "We conduct a linguistic analysis on how the distinct categories of it are usually mapped to their Czech counterparts.", "labels": [], "entities": []}, {"text": "Armed with these observations, we design a discriminative translation model of it, which is then integrated into the TectoMT deep syntax MT framework.", "labels": [], "entities": [{"text": "TectoMT deep syntax MT", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.5020270273089409}]}, {"text": "Features in the model take advantage of rich syntactic annotation TectoMT is based on, external tools for anaphoricity resolution, lexical co-occurrence frequencies measured on a large parallel corpus and gold coref-erence annotation.", "labels": [], "entities": [{"text": "anaphoricity resolution", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.6852090656757355}]}, {"text": "Even though the new model for it exhibits no improvement in terms of BLEU, manual evaluation shows that it outperforms the original solution in 8.5% sentences containing it.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9988258481025696}]}], "introductionContent": [{"text": "After it has long been neglected, retaining cohesion of a text larger than a single sentence in Machine Translation (MT) has recently become a discussed topic.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.8527291893959046}]}, {"text": "Correct translation of referential expressions is in many cases essential for humans to grasp the meaning of a translated text.", "labels": [], "entities": []}, {"text": "Especially, the translation of pronouns attracts a higher rate of interest.", "labels": [], "entities": []}, {"text": "In the previous works of Le, and, it has been shown that current MT systems perform poorly in producing the correct forms of pronouns.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9809347987174988}]}, {"text": "As regards English, the personal pronoun it is the most complicated case.", "labels": [], "entities": []}, {"text": "Not only can it corefer with almost any noun phrase (making it hard to pick the correct gender and number if the target language is morphologically rich), but it can also corefer with a larger discourse segment or play the role of a filler in certain grammatical constructions.", "labels": [], "entities": []}, {"text": "In this work, we turn our attention to the translation of the English personal pronoun it into Czech.", "labels": [], "entities": [{"text": "translation of the English personal pronoun it", "start_pos": 43, "end_pos": 89, "type": "TASK", "confidence": 0.7634490983826774}]}, {"text": "Even if we ignore morphology and merge all related surface forms into one, we cannot find a single Czech expression that would comprise all functions of the English it.", "labels": [], "entities": []}, {"text": "Moreover, there is no simple one-to-one mapping from categories of it to Czech expressions.", "labels": [], "entities": []}, {"text": "For instance, one would expect that the translation of it which is coreferential with a noun phrase has to agree in number and gender with the translation of its antecedent.", "labels": [], "entities": []}, {"text": "However, there are cases when it is more suitable to translate it as the demonstrative pronoun to, whose gender is always neuter.", "labels": [], "entities": []}, {"text": "The aim of this work is to build an English-toCzech translation model for the personal pronoun it within the TectoMT framework ( \u02c7 Zabokrtsk\u00b4y.", "labels": [], "entities": []}, {"text": "TectoMT is a tree-to-tree translation system with transfer via tectogrammatical layer, a deep syntactic layer which follows the Prague tectogrammatics theory Therefore, its translation model outputs the deep syntactic representation of a Czech expression.", "labels": [], "entities": []}, {"text": "Selecting the correct grammatical categories and thus producing a concrete surface form of a deep syntactic representation is provided by the translation synthesis stage, which we do not focus on in this work.", "labels": [], "entities": [{"text": "translation synthesis", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.9750794768333435}]}, {"text": "The mapping between it and corresponding Czech expressions depends on many aspects.", "labels": [], "entities": []}, {"text": "We address them by introducing features based on syntactic annotation and anaphoricity resolver output.", "labels": [], "entities": []}, {"text": "Furthermore, we make use of lexical cooccurrence counts aggregated on a large automatically annotated Czech-English parallel corpus CzEng 1.0 (.", "labels": [], "entities": [{"text": "Czech-English parallel corpus CzEng 1.0", "start_pos": 102, "end_pos": 141, "type": "DATASET", "confidence": 0.8511135816574097}]}, {"text": "Coreference links also appear to be a source of valuable features.", "labels": [], "entities": []}, {"text": "In contrast to the related work, we prefer a discriminative model to a commonly used generative model.", "labels": [], "entities": []}, {"text": "The former allows us to feed it with many syntactic and lexical features that may affect the output, which would hardly be possible in the latter.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were conducted in two settings that differ in the usage of features extracted from gold coreferential relations.", "labels": [], "entities": []}, {"text": "To mitigate a possible error caused by a wrong classifier choice, we built several models based on various Machine Learning classification methods.", "labels": [], "entities": [{"text": "Machine Learning classification", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.7349730134010315}]}, {"text": "If not explicitly mentioned, the methods below are applied with default parameters: \u2022 Vowpal Wabbit.", "labels": [], "entities": [{"text": "Vowpal Wabbit", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.7664958834648132}]}, {"text": "Binary logistic regression with one-against-all strategy for handling multiple classes.", "labels": [], "entities": []}, {"text": "The optimum has been found using the online method (Stochastic Gradient Descent).", "labels": [], "entities": []}, {"text": "We varied the parameters of the number of passes over the data and the L2 regularization weight.", "labels": [], "entities": []}, {"text": "\u2022 AI::MaxEntropy.", "labels": [], "entities": [{"text": "MaxEntropy", "start_pos": 6, "end_pos": 16, "type": "DATASET", "confidence": 0.7721391916275024}]}, {"text": "The optimum has been found using the batch method (L-BFGS).", "labels": [], "entities": []}, {"text": "9 k-nearest neighbors classifier with the parameter k being varied.", "labels": [], "entities": []}, {"text": "Support Vector Machines with one-against-one strategy to handle multiple classes.", "labels": [], "entities": []}, {"text": "We varied the choice of a kernel.", "labels": [], "entities": []}, {"text": "The accuracy evaluated on both training and test sets is shown in (columns Acc:Train and Acc:Test).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999569833278656}, {"text": "Acc", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9952930808067322}, {"text": "Acc:Test", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8967851400375366}]}, {"text": "The baseline resolver simply picks the most frequent class in the training set, which is PersPron.", "labels": [], "entities": []}, {"text": "For both experimental settings, the standard deviation measured on the test set is less than 1% in total, if the method's best configuration of parameters is taken and the result on decision trees, which we did not tune, is excluded.", "labels": [], "entities": []}, {"text": "This shows that all classifiers are consistent in their: Intrinsic (accuracy on the training and test data) and extrinsic (BLEU score) evaluation of translation model of it in configuration with (all feats) and without gold coreferential features (all feats + coref).", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9735488295555115}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9703744649887085}, {"text": "extrinsic (BLEU score", "start_pos": 112, "end_pos": 133, "type": "METRIC", "confidence": 0.8483743816614151}]}, {"text": "By introducing linguistically motivated features exploiting the deep-syntactic description of the sentence, we gained 17% in total over the baseline.", "labels": [], "entities": []}, {"text": "Moreover, adding features based on the gold coreference annotation results in a further 0.5% improvement.", "labels": [], "entities": []}, {"text": "ing the coreferential features using the Machine Learning method that performed best in the intrinsic test, i.e. AI::MaxEntropy (see Section 6).", "labels": [], "entities": []}, {"text": "The new method was compared to the rulebased approach originally used in TectoMT, which works as follows.", "labels": [], "entities": []}, {"text": "In the transfer stage, all occurrences of it are translated to a demonstrative ten.", "labels": [], "entities": []}, {"text": "In the synthesis stage, another rule is fired, which determines whether ten is omitted on the surface.", "labels": [], "entities": []}, {"text": "Then, omitting it corresponds either to a structural change (Null class) or an unexpressed personal pronoun (a subset of PersPron class).", "labels": [], "entities": []}, {"text": "It makes this original approach difficult to compare with the scores in, as the translation model of it is applied in the transfer stage, where we do not know yet if a personal pronoun is to be expressed or not.", "labels": [], "entities": []}, {"text": "Thus, we consider it the most appropriate to use final translated sentences produced by two versions of TectoMT in order to compare the different way they handle it.", "labels": [], "entities": []}, {"text": "The shift from the original settings to anew model for it results in 166 changed sentences.", "labels": [], "entities": []}, {"text": "In terms of BLEU score, we observe a marginal drop from 0.1404 to 0.1403 when using the new approach.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9754569828510284}]}, {"text": "Other classifiers achieved the same or new better than old 24 old better than new 13 both equally wrong 9 both equally correct 4: The results of manual evaluation conducted on 50 sentences translated by TectoMT in the original settings (old) and with the new translation model for it (new) similar score which correlates with the findings from intrinsic evaluation (see).", "labels": [], "entities": []}, {"text": "It accords with a similar experience of Le and and gives another evidence that the BLEU metric is inaccurate for measuring pronoun translation.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 83, "end_pos": 94, "type": "METRIC", "confidence": 0.9726243317127228}, {"text": "pronoun translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7032461762428284}]}, {"text": "Manual evaluation gives a more realistic view.", "labels": [], "entities": []}, {"text": "We randomly sampled 50 out of the 166 sentences that differ and one annotator assessed which of the two systems gave a better translation.", "labels": [], "entities": []}, {"text": "shows that in almost half of the cases the change was an improvement.", "labels": [], "entities": []}, {"text": "Including the sentences that are acceptable for both settings, the new approach picked the correct Czech counterpart of it in 22% more sentences than the original approach.", "labels": [], "entities": []}, {"text": "Since the proportion of the changed sentences accounts for almost 39% of all sentences containing it, the overall proportion of improved sentences with it is around 8.5% in total.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of classes in the data sets.", "labels": [], "entities": []}, {"text": " Table 2: Intrinsic (accuracy on the training and test data) and extrinsic (BLEU score) evaluation of  translation model of it in configuration with (all feats) and without gold coreferential features (all feats  + coref).", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9835553765296936}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9927984476089478}, {"text": "extrinsic (BLEU score", "start_pos": 65, "end_pos": 86, "type": "METRIC", "confidence": 0.8517828732728958}]}]}