{"title": [], "abstractContent": [{"text": "We derive a spectral algorithm for learning the parameters of a refinement HMM.", "labels": [], "entities": []}, {"text": "This method is simple, efficient, and can be applied to a wide range of supervised sequence labeling tasks.", "labels": [], "entities": [{"text": "supervised sequence labeling tasks", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.6861242800951004}]}, {"text": "Like other spectral methods, it avoids the problem of local optima and provides a consistent estimate of the parameters.", "labels": [], "entities": []}, {"text": "Our experiments on a phoneme recognition task show that when equipped with informative feature functions, it performs significantly better than a supervised HMM and competitively with EM.", "labels": [], "entities": [{"text": "phoneme recognition task", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8236916462580363}]}], "introductionContent": [{"text": "Consider the task of supervised sequence labeling.", "labels": [], "entities": [{"text": "supervised sequence labeling", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6262456973393759}]}, {"text": "We are given a training set where the j'th training example consists of a sequence of observations x N and asked to predict the correct labels on a test set of observations.", "labels": [], "entities": []}, {"text": "A common approach is to learn a joint distribution over sequences p(a 1 . .", "labels": [], "entities": []}, {"text": "a N , x 1 . .", "labels": [], "entities": []}, {"text": "x N ) as a hidden Markov model (HMM).", "labels": [], "entities": []}, {"text": "The downside of HMMs is that they assume each label a i is independent of labels before the previous label a i\u22121 . This independence assumption can be limiting, particularly when the label space is small.", "labels": [], "entities": []}, {"text": "To relax this assumption we can refine each label a i with a hidden state hi , which is not observed in the training data, and model the joint distribution p(a 1 . .", "labels": [], "entities": []}, {"text": "a N , x 1 . .", "labels": [], "entities": []}, {"text": "x N , h 1 . .", "labels": [], "entities": []}, {"text": "This refinement HMM (R-HMM), illustrated in, is able to propagate information forward through the hidden state as well as the label.", "labels": [], "entities": []}, {"text": "Unfortunately, estimating the parameters of an R-HMM is complicated by the unobserved hidden variables.", "labels": [], "entities": []}, {"text": "A standard approach is to use the expectation-maximization (EM) algorithm which Figure 1: (a) An R-HMM chain.", "labels": [], "entities": []}, {"text": "(b) An equivalent representation where labels and hidden states are intertwined. has no guarantee of finding the global optimum of its objective function.", "labels": [], "entities": []}, {"text": "The problem of local optima prevents EM from yielding statistically consistent parameter estimates: even with very large amounts of data, EM is not guaranteed to estimate parameters which are close to the \"correct\" model parameters.", "labels": [], "entities": []}, {"text": "In this paper, we derive a spectral algorithm for learning the parameters of R-HMMs.", "labels": [], "entities": []}, {"text": "Unlike EM, this technique is guaranteed to find the true parameters of the underlying model under mild conditions on the singular values of the model.", "labels": [], "entities": []}, {"text": "The algorithm we derive is simple and efficient, relying on singular value decomposition followed by standard matrix operations.", "labels": [], "entities": []}, {"text": "We also describe the connection of R-HMMs to L-PCFGs.", "labels": [], "entities": []}, {"text": "present a spectral algorithm for L-PCFG estimation, but the na\u00a8\u0131vena\u00a8\u0131ve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque.", "labels": [], "entities": []}, {"text": "We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs.", "labels": [], "entities": []}, {"text": "We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example,.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate empirical success for the R-HMM spectral algorithm.", "labels": [], "entities": []}, {"text": "The spectral algorithm performs competitively with EM on a phoneme recognition task, and is more stable with respect to the number of hidden states.", "labels": [], "entities": [{"text": "phoneme recognition task", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8198721408843994}]}, {"text": "present experiments with a parsing algorithm and also demonstrate it is competitive with EM.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9675283432006836}]}, {"text": "Our set of experiments comes as an additional piece of evidence that spectral algorithms can function as a viable, efficient and more principled alternative to the EM algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply the spectral algorithm for learning R-HMMs to the task of phoneme recognition.", "labels": [], "entities": [{"text": "phoneme recognition", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7627686262130737}]}, {"text": "The goal is to predict the correct sequence of phonemes a 1 . .", "labels": [], "entities": []}, {"text": "a N fora given a set of speech frames x 1 . .", "labels": [], "entities": []}, {"text": "x N . Phoneme recognition is often modeled with a fixed-structure HMM trained with EM, which makes it a natural application for spectral training.", "labels": [], "entities": [{"text": "Phoneme recognition", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.7590546309947968}]}, {"text": "We train and test on the TIMIT corpus of spoken language utterances (.", "labels": [], "entities": [{"text": "TIMIT corpus of spoken language utterances", "start_pos": 25, "end_pos": 67, "type": "DATASET", "confidence": 0.8435130516688029}]}, {"text": "The label set consists of l = 39 English phonemes following a standard phoneme set (.", "labels": [], "entities": []}, {"text": "For training, we use the sx and si utterances of the TIMIT training section made up of  Each utterance consists of a speech signal aligned with phoneme labels.", "labels": [], "entities": [{"text": "TIMIT training section", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.8255922595659891}]}, {"text": "As preprocessing, we divide the signal into a sequence of N overlapping frames, 25ms in length with a 10ms step size.", "labels": [], "entities": []}, {"text": "Each frame is converted to a feature representation using MFCC with its first and second derivatives fora total of 39 continuous features.", "labels": [], "entities": []}, {"text": "To discretize the problem, we apply vector quantization using euclidean k-means to map each frame into n = 10000 observation classes.", "labels": [], "entities": []}, {"text": "After preprocessing, we have 3696 skeletal sequence with a 1 . .", "labels": [], "entities": []}, {"text": "a N as the frame-aligned phoneme labels and x 1 . .", "labels": [], "entities": []}, {"text": "x N as the observation classes.", "labels": [], "entities": []}, {"text": "For testing, we use the core test portion of TIMIT, consisting of 192 utterances, and for development we use 200 additional utterances.", "labels": [], "entities": [{"text": "TIMIT", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.4927714467048645}]}, {"text": "Accuracy is measured by the percentage of frames labeled with the correct phoneme.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9918417930603027}]}, {"text": "During inference, we calculate marginals \u00b5 for each label at each position i and choose the one with the highest marginal probability, a * i = arg max a\u2208 \u00b5(a, i).", "labels": [], "entities": []}, {"text": "The spectral method requires defining feature functions \u03c6, \u03c8, \u03be, and \u03c5.", "labels": [], "entities": []}, {"text": "We use binary-valued feature vectors which we specify through features templates, for instance the template a i \u00d7 xi corresponds to binary values for each possible label and output pair (ln binary dimensions).", "labels": [], "entities": []}, {"text": "gives the full set of templates.", "labels": [], "entities": []}, {"text": "These feature functions are specially for the phoneme labeling task.", "labels": [], "entities": [{"text": "phoneme labeling task", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.8415383100509644}]}, {"text": "We note that the HTK baseline explicitly models the position within the current  phoneme as part of the HMM structure.", "labels": [], "entities": [{"text": "HTK baseline", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.7547882497310638}]}, {"text": "The spectral method is able to encode similar information naturally through the feature functions.", "labels": [], "entities": []}, {"text": "We implement several baseline for phoneme recognition: UNIGRAM chooses the most likely label, arg max a\u2208 p(a|x i ), at each position; HMM is a standard HMM trained with maximumlikelihood estimation; EM(m) is an R-HMM with m hidden states estimated using EM; and SPECTRAL(m) is an R-HMM with m hidden states estimated with the spectral method described in this paper.", "labels": [], "entities": [{"text": "phoneme recognition", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7788035571575165}]}, {"text": "We also compare to HTK, a fixed-structure HMM with three segments per phoneme estimated using EM with the HTK speech toolkit.", "labels": [], "entities": []}, {"text": "See for more details on this method.", "labels": [], "entities": []}, {"text": "An important consideration for both EM and the spectral method is the number of hidden states min the R-HMM.", "labels": [], "entities": [{"text": "EM", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.892994225025177}]}, {"text": "More states allow for greater label refinement, with the downside of possible overfitting and, in the case of EM, more local optima.", "labels": [], "entities": [{"text": "label refinement", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.6628719419240952}]}, {"text": "To determine the best number of hidden states, we optimize both methods on the development set fora range of m values between 1 to 32.", "labels": [], "entities": []}, {"text": "For EM, we run 200 training iterations on each value of m and choose the iteration that scores best on the development set.", "labels": [], "entities": [{"text": "EM", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8849070072174072}]}, {"text": "As the spectral algorithm is noniterative, we only need to evaluate the development set once perm value.", "labels": [], "entities": []}, {"text": "shows the development accuracy of the two method as we adjust the value of m.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9488641619682312}]}, {"text": "EM accuracy peaks at 4 hidden states and then starts degrading, whereas the spectral method continues to improve until 24 hidden states.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9482720494270325}]}, {"text": "Another important consideration for the spectral method is the feature functions.", "labels": [], "entities": []}, {"text": "The analysis suggests that the best feature functions are highly informative of the underlying hidden states.", "labels": [], "entities": []}, {"text": "To test this empirically we run spectral estimation with a reduced set of features by ablating the templates indicating adjacent phonemes and relative position.", "labels": [], "entities": []}, {"text": "shows that removing these features does have a significant effect on development accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.994176983833313}]}, {"text": "Without either type of feature, development accuracy drops by 1.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9728514552116394}]}, {"text": "We can interpret the effect of the features in a more principled manner.", "labels": [], "entities": []}, {"text": "Informative features yield greater singular values for the matrices \u2126 a 1 and \u2126 a 2 , and these singular values directly affect the sample complexity of the algorithm; see for details.", "labels": [], "entities": []}, {"text": "In sum, good feature functions lead to well-conditioned \u2126 a 1 and \u2126 a 2 , which in turn require fewer samples for convergence.", "labels": [], "entities": []}, {"text": "gives the final performance for the baselines and the spectral method on the TIMIT test set.", "labels": [], "entities": [{"text": "TIMIT test set", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9472491145133972}]}, {"text": "For EM and the spectral method, we use best performing model from the development data, 4 hidden states for EM and 24 for the spectral method.", "labels": [], "entities": []}, {"text": "The experiments show that R-HMM models score significantly better than a standard HMM and comparatively to the fixedstructure HMM.", "labels": [], "entities": []}, {"text": "In training the R-HMM models, the spectral method performs competitively with EM while avoiding the problems of local optima.", "labels": [], "entities": []}], "tableCaptions": []}