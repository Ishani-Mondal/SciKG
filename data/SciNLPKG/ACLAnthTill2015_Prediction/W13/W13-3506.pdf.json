{"title": [], "abstractContent": [{"text": "This paper proposes a boosting algorithm that uses a semi-Markov perceptron.", "labels": [], "entities": []}, {"text": "The training algorithm repeats the training of a semi-Markov model and the update of the weights of training samples.", "labels": [], "entities": []}, {"text": "In the boosting , training samples that are incorrectly segmented or labeled have large weights.", "labels": [], "entities": []}, {"text": "Such training samples are aggressively learned in the training of the semi-Markov perceptron because the weights are used as the learning ratios.", "labels": [], "entities": []}, {"text": "We evaluate our training method with Noun Phrase Chunk-ing, Text Chunking and Extended Named Entity Recognition.", "labels": [], "entities": []}, {"text": "The experimental results show that our method achieves better accuracy than a semi-Markov perceptron and a semi-Markov Conditional Random Fields.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.998907208442688}]}], "introductionContent": [{"text": "Natural Language Processing (NLP) basic tasks, such as Noun Phrase Chunking, Text Chunking, and Named Entity Recognition, are realized by segmenting words and labeling to the segmented words.", "labels": [], "entities": [{"text": "Noun Phrase Chunking", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.6749731103579203}, {"text": "Text Chunking", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.6463763117790222}, {"text": "Named Entity Recognition", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.6478865245978037}]}, {"text": "To realize these tasks, supervised learning algorithms have been applied successfully.", "labels": [], "entities": []}, {"text": "In the early stages, algorithms for training classifiers, including Maximum Entropy Models (), AdaBoost-based learning algorithms (), and Support Vector Machines (SVMs) ( were widely used.", "labels": [], "entities": []}, {"text": "Recently, learning algorithms for structured prediction, such as linear-chain structured predictions, and semi-Markov modelbased ones, have been widely used.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.7392027676105499}]}, {"text": "The examples of linear-chain structured predictions include Conditional Random Fields (CRFs) () and structured perceptron).", "labels": [], "entities": []}, {"text": "The examples of semi-Markov model-based ones include semi-Markov model perceptron (), and semi-Markov CRFs ().", "labels": [], "entities": []}, {"text": "Among these methods, semi-Markov-based ones have shown good performance in terms of accuracy ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9993749260902405}]}, {"text": "One of the reasons is that a semi-Markov learner trains models that assign labels to hypothesized segments (i.e., word chunks) instead of labeling to individual words.", "labels": [], "entities": []}, {"text": "This enables use of features that cannot be easily used in word level processing such as the beginning word of a segment, the end word of a segment, and soon.", "labels": [], "entities": []}, {"text": "To obtain higher accuracy, boosting methods have been applied to learning methods for training classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9979459643363953}]}, {"text": "Boosting is a method to create a final hypothesis by repeatedly generating a weak hypothesis and changing the weights of training samples in each training iteration with a given weak learner such as a decision stump learner) and a decision tree learner).", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, there are no approaches that apply boosting to learning algorithms for structured prediction.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7211124002933502}]}, {"text": "In other words, if we can successful apply boosting to learning algorithms for structured prediction, we expect to obtain higher accuracy.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7316894829273224}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9979768395423889}]}, {"text": "This paper proposes a boosting algorithm fora semi-Markov perceptron.", "labels": [], "entities": []}, {"text": "Our learning method uses a semi-Markov perceptron as a weak learner, and AdaBoost is used as the boosting algorithm.", "labels": [], "entities": []}, {"text": "To apply boosting to the semi-Markov perceptron, the following methods are proposed; 1) Use the weights of training samples decided by AdaBoost as the learning ratios of the semi-Markov perceptron, and 2) Training on AdaBoost with the loss between the correct output of a training sample and the incorrect output that has the highest score.", "labels": [], "entities": [{"text": "AdaBoost", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.9546357989311218}, {"text": "AdaBoost", "start_pos": 217, "end_pos": 225, "type": "DATASET", "confidence": 0.9538882970809937}]}, {"text": "By the first method, the semi-Markov perceptron can aggressively learn training samples that are in-correctly classified at previous iteration because such training samples have large weights.", "labels": [], "entities": []}, {"text": "The second method is a technique to apply AdaBoost to learning algorithms for structured prediction that generate negative samples from N-best outputs), or consider all possible candidates).", "labels": [], "entities": []}, {"text": "We also prove the convergence of our training method.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In Section 2, we describe AdaBoost and Semi-Markov perceptron is described in Section 3.", "labels": [], "entities": [{"text": "AdaBoost", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9608343839645386}]}, {"text": "Our proposed method is described in Section 4, and the experimental setting, the experimetal results and related work are described in Section 5, 6, and 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation metrics are recall (RE), precision (P R), and F-measure (F M ) defined as follows: where C ok is the number of correctly recognized chunks with their correct labels, C all is the number of all chunks in a gold standard data, and C rec is the number of all recognized chunks.", "labels": [], "entities": [{"text": "recall (RE)", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9662005752325058}, {"text": "precision (P R)", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.9344160556793213}, {"text": "F-measure (F M )", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.9647887229919434}]}, {"text": "lists features used in our experiments.", "labels": [], "entities": []}, {"text": "For NP Chunking and Text Chunking, we added features derived from segments in addition to ENER features.", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.5903226137161255}, {"text": "Text Chunking", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.5760569870471954}]}, {"text": "wk is the k-th word, and pk is the Part-OfSpeech (POS) tag of k-th word.", "labels": [], "entities": []}, {"text": "bp is the position of the first word of the current segment in a given word sequence.", "labels": [], "entities": []}, {"text": "ep indicates the position of the last word of the current segment.", "labels": [], "entities": []}, {"text": "ip is the position of words inside the current segment (bp < ip < ep).", "labels": [], "entities": []}, {"text": "If the length of the current segment is 2, we use features that indicate there is no inside word as the features of ip-th words.", "labels": [], "entities": []}, {"text": "t j is the NE class label of j-th segment.", "labels": [], "entities": []}, {"text": "CL j is the length of the current segment, whether it be 1, 2, 3, 4, or longer than 4.", "labels": [], "entities": []}, {"text": "W B j indicates word bigrams, and P B j indicates POS bigrams inside the current segment.", "labels": [], "entities": []}, {"text": "We used a machine with Intel(R) Xeon(R) CPU X5680 @ 3.33GHz and 72 GB memory.", "labels": [], "entities": []}, {"text": "In the following, our proposed method is referred as SemiBoost.", "labels": [], "entities": []}, {"text": "shows the experimental results on NP Chunking.", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.6502043604850769}]}, {"text": "Semi-Boost showed the best accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9994946718215942}]}, {"text": "Semi-Boost showed 0.28 higher F-measure than Semi-PER and Semi-CRF.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9991775155067444}]}, {"text": "To compare the results, we employed a McNemar paired test on the labeling disagreements as was done in.", "labels": [], "entities": [{"text": "McNemar paired test", "start_pos": 38, "end_pos": 57, "type": "METRIC", "confidence": 0.7781446576118469}]}, {"text": "All the results indicate that there is a significant difference (p < 0.01).", "labels": [], "entities": []}, {"text": "This result shows that Semi-Boost showed high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9987331032752991}]}, {"text": "shows the experimental results on Text Chunking.", "labels": [], "entities": [{"text": "Text Chunking", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.5545845031738281}]}, {"text": "Semi-Boost showed 0.36 higher Fmeasure than Semi-CRF, and 0.05 higher Fmeasure than Semi-PER.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.999446451663971}, {"text": "Fmeasure", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9994326233863831}]}, {"text": "The result of McNemar test indicates that there is a significant difference (p < 0.01) between Semi-Boost and Semi-CRF.", "labels": [], "entities": []}, {"text": "However, there is no significant difference between Semi-Boost and Semi-PER.", "labels": [], "entities": []}, {"text": "shows the experimental results on ENER.", "labels": [], "entities": [{"text": "ENER", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.868395984172821}]}], "tableCaptions": [{"text": " Table 2: Results of NP Chunking.", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.928048849105835}]}, {"text": " Table 3: Results of Text Chunking.", "labels": [], "entities": [{"text": "Text Chunking", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.6505671441555023}]}, {"text": " Table 4: Experimental results for ENER.", "labels": [], "entities": [{"text": "ENER", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.7822712659835815}]}, {"text": " Table 5: Training time of each learner (second)  for NP Chunking (NP), Text Chunking (TC) and  ENER. The number of Semi-Boost iteration is  only one time. The +20 cores means training of  Semi-Boost with 20 cores.  Learner  NP  TC  ENER  Semi-PER  475  559  13,559  Semi-CRF  2,120 8,228 N/A  Semi-Boost 499  619  32,370  +20 cores  487  650  19,598", "labels": [], "entities": []}, {"text": " Table 6: The best results for NP Chunking (F M ).", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.932612955570221}]}, {"text": " Table 7: The best results for Text Chunking (F M ).", "labels": [], "entities": [{"text": "Text Chunking", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.6278972923755646}]}]}