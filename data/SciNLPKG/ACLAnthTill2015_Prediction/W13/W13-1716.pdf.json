{"title": [], "abstractContent": [{"text": "Our submission for this NLI shared task used for the most part standard features found in recent work.", "labels": [], "entities": []}, {"text": "Our focus was instead on two other aspects of our system: at a high level, on possible ways of constructing ensembles of multiple classifiers; and at a low level, on the gran-ularity of part-of-speech tags used as features.", "labels": [], "entities": []}, {"text": "We found that the choice of ensemble combination method did not lead to much difference in results, although exploiting the varying behaviours of linear versus logistic regression SVM classifiers could be promising in future work; but part-of-speech tagsets showed noticeable differences.", "labels": [], "entities": []}, {"text": "We also note that the overall architecture, with its feature set and ensemble approach, had an accuracy of 83.1% on the test set when trained on both the training data and development data supplied, close to the best result of the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9993900060653687}]}, {"text": "This suggests that basically throwing together all the features of previous work will achieve roughly the state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Among the efflorescence of work on Native Language Identification (NLI) noted by the shared task organisers, there are two trends in recent work in particular that we considered in building our submission.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.8084822495778402}]}, {"text": "The first is the proposal and use of new features that might have relevance to NLI: for example,, motivated by the Contrastive Analysis Hypothesis from the field of Second Language Acquisition, introduced syntactic structure as a feature; Swanson and Charniak (2012) introduced more complex Tree Substitution (TSG) structures, learned by Bayesian inference; and Bykh and Meurers (2012) used recurring n-grams, inspired by the variation n-gram approach to corpus error annotation detection.", "labels": [], "entities": [{"text": "Second Language Acquisition", "start_pos": 165, "end_pos": 192, "type": "TASK", "confidence": 0.662533164024353}, {"text": "corpus error annotation detection", "start_pos": 455, "end_pos": 488, "type": "TASK", "confidence": 0.6690331175923347}]}, {"text": "Starting from the features introduced in these papers and others, then, other recent papers have compiled a comprehensive collection of features based on the earlier work - is an example, combining and analysing most of the features used in previous work.", "labels": [], "entities": []}, {"text": "Given the timeframe of the shared task, there seemed to be not much mileage in trying new features that were likely to be more peripheral to the task.", "labels": [], "entities": []}, {"text": "A second trend, most apparent in 2012, was the examination of other corpora besides the International Corpus of Learner English used in earlier work, and in particular the use of cross-corpus evaluation ( to avoid topic bias in determining native language.", "labels": [], "entities": [{"text": "International Corpus of Learner English", "start_pos": 88, "end_pos": 127, "type": "DATASET", "confidence": 0.9618376851081848}]}, {"text": "Possible topic bias had been a reason for avoiding a full range of n-grams, in particular those containing content words (; the development of new corpora and the analysis of the effect of topic bias mitigated this.", "labels": [], "entities": []}, {"text": "The consequent use of a full range of n-grams further reinforced the view that novel features were unlikely to be a major source of interesting results.", "labels": [], "entities": []}, {"text": "We therefore concentrated on two areas: the use of classifier ensembles, and the choice of part-ofspeech tags.", "labels": [], "entities": []}, {"text": "With classifier ensembles, noted that these were highly useful in their system; but while that paper had extensive fea-ture descriptions, it did not discuss in detail the approach to its ensembles.", "labels": [], "entities": []}, {"text": "We therefore decided to examine a range of possible ensemble architectures.", "labels": [], "entities": []}, {"text": "With part-of-speech tags, most work has used the Penn Treebank tagset, including those based on syntactic structure.", "labels": [], "entities": [{"text": "Penn Treebank tagset", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.9925134976704916}]}, {"text": "Kochmar (2011) on the other hand used the CLAWS tagset, 1 which is much richer and more oriented to linguistic analysis than the Penn Treebank one.", "labels": [], "entities": [{"text": "CLAWS tagset", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9451199769973755}, {"text": "Penn Treebank", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.9938395023345947}]}, {"text": "Given the much larger size of the TOEFL11 corpus used for this shared task than the corpora used for much earlier work, data sparsity could be less of an issue, and the tagset a viable one for future work.", "labels": [], "entities": [{"text": "TOEFL11 corpus", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9470281600952148}]}, {"text": "The description of our submission is therefore in three parts.", "labels": [], "entities": []}, {"text": "In \u00a72 we present the system description, with a focus on the ensemble architectures we investigated; in \u00a73 we list the features we used, which are basically those of much of the previous work; in \u00a74 we present results of some of the variants we tried, particularly with respect to ensembles and tagsets; and in \u00a75 we discuss some of the interesting characteristics of the data we noted during the shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report our results using 10-fold cross-validation on the combined training and development sets, as well as by training a model using the training and development data and running it on the test set.", "labels": [], "entities": []}, {"text": "We note that for our submission, we trained only on the training data; the results here thus differ from the official ones.", "labels": [], "entities": []}, {"text": "We also experimented with some other feature types that were not included in the final system.", "labels": [], "entities": []}, {"text": "CCG SuperTag n-grams In order to introduce additional rich syntactic information into our system, we investigated the use CCG SuperTags as feature for NLI classification.", "labels": [], "entities": [{"text": "CCG SuperTag n-grams", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8990076382954916}, {"text": "NLI classification", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.8273163735866547}]}, {"text": "We used the C&C CCG  Parser and SuperTagger () to extract SuperTag n-grams from the corpus, which were then used as features to construct classifiers.", "labels": [], "entities": [{"text": "C&C CCG  Parser", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.880501639842987}]}, {"text": "The best results were achieved by using n-grams of size 2-4, which achieved classification rates of around 44%.", "labels": [], "entities": [{"text": "classification rates", "start_pos": 76, "end_pos": 96, "type": "METRIC", "confidence": 0.7281221598386765}]}, {"text": "However, adding these features to our ensemble did not improve the overall system accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9984115362167358}]}, {"text": "We believe that this is because when coupled with the other syntactic features in the system, the information provided by the SuperTags is redundant, and thus they were excluded from our final ensemble.", "labels": [], "entities": []}, {"text": "Hapax Legomena and Dis Legomena The special word categories Hapax Legomena and Dis legomena refer to words that appear only once and twice, respectively, in a complete text.", "labels": [], "entities": []}, {"text": "In practice, these features area subset of our Word Unigram feature, where Hapax Legomena correspond to unigrams with an occurrence count of 1 and Hapax dis legomena are unigrams with a count of 2.", "labels": [], "entities": []}, {"text": "In our experimental results we found that Hapax Legomena alone provides an accuracy of 61%.", "labels": [], "entities": [{"text": "Hapax Legomena", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.7391902208328247}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9996480941772461}]}, {"text": "Combining the two features together yields an accuracy of 67%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9997361302375793}]}, {"text": "This is an interesting finding as both of these features alone provide an accuracy close to the whole set of word unigrams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9992654919624329}]}], "tableCaptions": [{"text": " Table 1: Classification results for our individual features.", "labels": [], "entities": []}, {"text": " Table 2: Classification accuracy results for POS n-grams  of size N using both the PTB and RASP tagset. The larger  RASP tagset performed significantly better for all N.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9217456579208374}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.975646436214447}, {"text": "PTB", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8711906671524048}, {"text": "RASP tagset", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.8075793087482452}]}, {"text": " Table 3: Classification results for Function Word n-grams  of size N. Our proposed Function Word bigram and tri- gram features outperform the commonly used unigrams.", "labels": [], "entities": []}, {"text": " Table 4: Classification results for our ensembles, best re- sult in column in bold (binary values with L1-and L2- regularized solvers).", "labels": [], "entities": []}, {"text": " Table 5: Classification results for our individual features.", "labels": [], "entities": []}]}