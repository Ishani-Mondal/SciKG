{"title": [{"text": "Automated Scoring of a Summary Writing Task Designed to Measure Reading Comprehension", "labels": [], "entities": [{"text": "Automated Scoring", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6050454676151276}, {"text": "Summary Writing Task", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7793442606925964}, {"text": "Measure Reading Comprehension", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.7182373205820719}]}], "abstractContent": [{"text": "We introduce a cognitive framework for measuring reading comprehension that includes the use of novel summary writing tasks.", "labels": [], "entities": []}, {"text": "We derive NLP features from the holistic rubric used to score the summaries written by students for such tasks and use them to design a preliminary, automated scoring system.", "labels": [], "entities": []}, {"text": "Our results show that the automated approach performs well on summaries written by students for two different passages.", "labels": [], "entities": [{"text": "summaries written by students", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.8902581483125687}]}], "introductionContent": [{"text": "In this paper, we present our preliminary work on automatic scoring of a summarization task that is designed to measure the reading comprehension skills of students from grades 6 through 9.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.880792647600174}]}, {"text": "We first introduce our underlying reading comprehension assessment framework (Sabatini and O'Reilly, In Press; Sabatini et al., In Press) that motivates the task of writing summaries as a key component of such assessments in \u00a72.", "labels": [], "entities": [{"text": "In Press", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.9077425003051758}]}, {"text": "We then describe the summarization task in more detail in \u00a73.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9060271978378296}]}, {"text": "In \u00a74, we describe our approach to automatically scoring summaries written by students for this task and compare the results we obtain using our system to those obtained by human scoring.", "labels": [], "entities": []}, {"text": "Finally, we conclude in \u00a76 with a brief discussion and possible future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The grade distribution of the students who wrote  summaries for each of the two passages.", "labels": [], "entities": []}, {"text": " Table 2: Exact and adjacent agreements of the most- frequent-score baseline and of the 5-fold cross-validation  predictions from the logistic regression classifier, for both  passages.", "labels": [], "entities": []}]}