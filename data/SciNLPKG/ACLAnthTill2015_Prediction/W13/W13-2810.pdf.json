{"title": [{"text": "Unsupervised Transduction Grammar Induction via Minimum Description Length", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a minimalist, unsupervised learning model that induces relatively clean phrasal inversion transduction grammars by employing the minimum description length principle to drive search over a space defined by two opposing extreme types of ITGs.", "labels": [], "entities": []}, {"text": "In comparison to most current SMT approaches, the model learns a very parsimonious phrase translation lexicons that provide an obvious basis for generalization to abstract translation schemas.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9914393424987793}, {"text": "phrase translation lexicons", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.8009044726689657}]}, {"text": "To do this, the model maintains internal consistency by avoiding use of mismatched or unrelated models , such as word alignments or probabilities from IBM models.", "labels": [], "entities": []}, {"text": "The model introduces a novel strategy for avoiding the pitfalls of premature pruning in chunking approaches, by incrementally splitting an ITG while using a second ITG to guide this search.", "labels": [], "entities": []}], "introductionContent": [{"text": "We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs () that employs a theoretically well-founded minimum description length (MDL) objective to explicitly drive two opposing, extreme ITGs towards one minimal ITG.", "labels": [], "entities": []}, {"text": "This represents anew attack on the problem suffered by most current SMT approaches of learning phrase translations that require enormous amounts of run-time memory, contain a high degree of redundancy, and fails to provide an obvious basis for generalization to abstract translation schemas.", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9936972260475159}, {"text": "learning phrase translations", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6506096323331197}]}, {"text": "In particular, phrasal SMT models such as and often search for candidate translation segments and transduction rules by committing to a word alignment based on very different assumptions, and heuristically derive lexical segment translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8103110194206238}]}, {"text": "In fact, it is possible to improve the performance by tossing away most of the learned segmental translations.", "labels": [], "entities": []}, {"text": "In addition to preventing such wastefulness, our work aims to also provide an obvious basis for generalization to abstract translation schemas by driving the search for phrasal rules by simultaneously using two opposing types of ITG constraints that have both individually been empirically proven to match phrase reordering patterns across translations well.", "labels": [], "entities": []}, {"text": "We adopt a more \"pure\" methodology for evaluating transduction grammar induction than typical system building papers.", "labels": [], "entities": [{"text": "transduction grammar induction", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.8469712734222412}]}, {"text": "Instead of embedding our learned ITG in the midst of many other heuristic components for the sake of a short term boost in BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9982156753540039}, {"text": "BLEU", "start_pos": 291, "end_pos": 295, "type": "METRIC", "confidence": 0.9971722960472107}]}, {"text": "The common practice of plugging some aspect of a learned ITG into either (a) along pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before-see for example;;;;;;;;-obscures the specific traits of the induced grammar.", "labels": [], "entities": []}, {"text": "Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured.", "labels": [], "entities": []}, {"text": "When the structure of an ITG is induced without supervision, it has so far been assumed that smaller rules get clumped together into larger rules.", "labels": [], "entities": []}, {"text": "This is a natural way to search, since maximum likelihood (ML) tends to improve with longer rules, which is typically balanced with Bayesian priors.", "labels": [], "entities": [{"text": "maximum likelihood (ML)", "start_pos": 39, "end_pos": 62, "type": "METRIC", "confidence": 0.831820285320282}]}, {"text": "Bayesian priors are also used in Gibbs sampling, as well as other nonparametric learning methods).", "labels": [], "entities": [{"text": "Gibbs sampling", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7812451124191284}]}, {"text": "All of the above evaluate their models by feeding them into mismatched decoders, making it hard to evaluate how accurate the learned models themselves were.", "labels": [], "entities": []}, {"text": "In this work we take a radically different approach, and start with the longest rules possible and attempt to segment them into shorter rules iteratively.", "labels": [], "entities": []}, {"text": "This makes ML useless, since our initial model maximizes it.", "labels": [], "entities": [{"text": "ML", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9582664370536804}]}, {"text": "Instead, we balance the ML objective with a minimum description length (MDL) objective, which let us escape the initial ML optimum by rewarding model parsimony.", "labels": [], "entities": [{"text": "minimum description length (MDL) objective", "start_pos": 44, "end_pos": 86, "type": "METRIC", "confidence": 0.7898199387959072}]}, {"text": "Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints ().", "labels": [], "entities": [{"text": "Transduction grammars", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9402720332145691}]}, {"text": "This complicates the learning process by adding external constraints that are bound to match the translation model poorly.", "labels": [], "entities": []}, {"text": "It does, however, constitute away to borrow nonterminal categories that help the translation model.", "labels": [], "entities": []}, {"text": "MDL has been used before in monolingual grammar induction, as well as to interpret visual scenes).", "labels": [], "entities": [{"text": "monolingual grammar induction", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.730625589688619}]}, {"text": "Our work is markedly different in that we (a) induce an ITG rather than a monolingual grammar, and (b) focus on learning the terminal segments rather than the nonterminal categories.", "labels": [], "entities": []}, {"text": "Iterative segmentation has also been used before, but only to derive a word alignment as part of a larger pipeline).", "labels": [], "entities": [{"text": "Iterative segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7561721801757812}, {"text": "word alignment", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.7184625864028931}]}, {"text": "The paper is structured as follows: we start by describing the MDL principle (Section 2).", "labels": [], "entities": []}, {"text": "We then describe the initial ITGs (Section 3), followed by the algorithm that induces an MDL-optimal ITG from them (Section 4).", "labels": [], "entities": []}, {"text": "After that we describe the experiments (Section 5), and the results (Section 6).", "labels": [], "entities": []}, {"text": "Finally, we offer some conclusions (Section 7).", "labels": [], "entities": []}], "datasetContent": [{"text": "To test whether minimum description length is a good driver for unsupervised inversion transduction induction, we implemented and executed the method described above.", "labels": [], "entities": [{"text": "inversion transduction induction", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.7052844067414602}]}, {"text": "We start by initializing one long and one short ITG.", "labels": [], "entities": []}, {"text": "The parameters of the long ITG cannot be adjusted to fit the data better, but the parameters of the short ITG can be tuned to the right-hand sides of the long ITG.", "labels": [], "entities": []}, {"text": "We do so with an implementation of the cubic time algorithm described in , with abeam width of 100.", "labels": [], "entities": []}, {"text": "We then run the introduced algorithm.", "labels": [], "entities": []}, {"text": "As training data, we use the IWSLT07 ChineseEnglish data set, which contains 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain.", "labels": [], "entities": [{"text": "IWSLT07 ChineseEnglish data set", "start_pos": 29, "end_pos": 60, "type": "DATASET", "confidence": 0.9179458320140839}]}, {"text": "Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more \"word like\" sequences.", "labels": [], "entities": []}, {"text": "After each iteration, we use the long ITG to translate the held out test set with our in-house ITG decoder.", "labels": [], "entities": [{"text": "ITG", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.9467115998268127}]}, {"text": "The decoder uses a CKY-style parsing algorithm and cube pruning to integrate the language model scores.", "labels": [], "entities": [{"text": "CKY-style parsing", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.6650016903877258}]}, {"text": "The decoder builds an efficient hypergraph structure which is scored using both the induced grammar and a language model.", "labels": [], "entities": []}, {"text": "We use SRILM) for training a trigram language model on the English side of the training corpus.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.647713303565979}]}, {"text": "To evaluate the resulting translations, we use BLEU () and NIST.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9983525276184082}, {"text": "NIST", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9383143186569214}]}, {"text": "We also perform a combination experiment, where the grammar at different stages of the learning process (iterations) are interpolated with each other.", "labels": [], "entities": []}, {"text": "This is a straight-forward linear interpolation, where the probabilities of the rules are added up and the grammar is renormalized.", "labels": [], "entities": []}, {"text": "Although it makes little sense from an MDL point of view to increase the size of the grammar so indiscriminately, it does make sense from an engineering point of view, since more rules typically means better coverage, which in turn typically means better translations of unknown data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of decoding. NIST and BLEU are the translation scores at each iteration, followed  by the number of rules in the grammar, followed by the average (as measured by mean and mode) number  of English tokens in the rules.", "labels": [], "entities": [{"text": "NIST", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9475019574165344}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9983415603637695}]}, {"text": " Table 2: The results of decoding with combined grammars. NIST and BLEU are the translation scores for  each combination, followed by the number of rules in the grammar, followed by the average (as measured  by mean and mode) number of English tokens in the rules.", "labels": [], "entities": [{"text": "NIST", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.7313418984413147}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9981379508972168}]}]}