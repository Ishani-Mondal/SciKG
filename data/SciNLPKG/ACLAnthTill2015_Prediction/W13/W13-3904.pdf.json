{"title": [], "abstractContent": [{"text": "We present a visual aid for the hearing impaired to enable access to internet videos.", "labels": [], "entities": []}, {"text": "The visual tool is in the form of a time synchronized lip movement corresponding to the speech in the video which is embedded in the original internet video.", "labels": [], "entities": []}, {"text": "Conventionally, access to the audio or speech, in a video, by the hearing impaired is provided by means of either text subtitles or sign language gestures by an interpreter.", "labels": [], "entities": []}, {"text": "The proposed tool would be beneficial, especially in situations where such aids are not readily available or generating such aids is difficult.", "labels": [], "entities": []}, {"text": "We have conducted a number experiments to determine the feasibility and usefulness of the proposed visual aid.", "labels": [], "entities": []}], "introductionContent": [{"text": "As per World Health Organization, over 360 million people which account for 5% of the world's total population suffer from hearing loss and a significant majority of them live in developing nations.", "labels": [], "entities": []}, {"text": "Moreover, one third of people over the age of 65 years, especially from South Asia, Asia Pacific and Sub-Saharan Africa are affected by disabling hearing loss.", "labels": [], "entities": []}, {"text": "A person with hearing impairment, especially acquired deafness in adulthood, can with some training interpret spoken speech by observing lip movements corresponding to the spoken speech.", "labels": [], "entities": []}, {"text": "Lip reading, also known as speech-reading in literature, allows access to speech through visual reading of the movement of the lips, face and tongue in the absence of audible sound.", "labels": [], "entities": [{"text": "Lip reading", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6796421408653259}]}, {"text": "Lip reading also makes use of the information associated with the context, the knowledge of the language, and also the residual hearing of the person.", "labels": [], "entities": [{"text": "Lip reading", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7919721305370331}]}, {"text": "Hearing impairment can prove to be a major handicap especially when a person wishes to understand an internet video while viewing it.", "labels": [], "entities": []}, {"text": "Any tool that can make video assessible is useful for the hearing impaired.", "labels": [], "entities": []}, {"text": "This motivates our work in developing a tool that allows for viewing a video without having to actually hear the audio track of the video.", "labels": [], "entities": []}, {"text": "Text based subtitles is one way by which a person with hearing loss interprets what is being spoken in a video.", "labels": [], "entities": []}, {"text": "However, text subtitles are not always readily available; especially in a country like India where subtitling is not mandated bylaw unlike in some of the developed nations (example).", "labels": [], "entities": []}, {"text": "Moreover, manual generation of subtitles is along drawn, laborious and an expensive process.", "labels": [], "entities": []}, {"text": "An alternative is to automatically generate text subtitles using an Automatic Speech Recognition (ASR) engine, but non-availability of ASR engines fora resource deficient language hinders generation of accurate subtitles, additionally, generating subtitles in the script of the spoken language would be another impediment.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.7100101113319397}]}, {"text": "IBM's SiSi (Say It, Sign It) is an automatic sign language generator for spoken audio.", "labels": [], "entities": []}, {"text": "SiSi uses a speech recognition module that converts the spoken speech into text; the text is interpreted into gestures, that are used to animate an avatar which signs in British Sign Language.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7413705885410309}, {"text": "British Sign Language", "start_pos": 170, "end_pos": 191, "type": "DATASET", "confidence": 0.8777868747711182}]}, {"text": "SiSi largely depends on the accuracy of recognition of audio.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9988679885864258}]}, {"text": "eSign project was primarily designed to help interpret textual internet content using sign language.", "labels": [], "entities": []}, {"text": "eSign synthesizes the signing gestures using Signing Gesture Markup Language (SiGML), along with information regarding speed and viewpoint.", "labels": [], "entities": [{"text": "speed", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.9886345863342285}]}, {"text": "However, there are about 200 different sign languages, each with a vocabulary of considerable size.", "labels": [], "entities": []}, {"text": "Building an automated system that would generate sign language interpretation of audio would then be complex owing to not only the nonavailability of an efficient ASR engine but also the difficulty associated in translation of generated text, to sign language gestures for resource deficient languages.", "labels": [], "entities": [{"text": "sign language interpretation of audio", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.7940605401992797}, {"text": "ASR", "start_pos": 163, "end_pos": 166, "type": "TASK", "confidence": 0.960494875907898}, {"text": "translation of generated text", "start_pos": 212, "end_pos": 241, "type": "TASK", "confidence": 0.8542403131723404}]}, {"text": "In this paper, we propose a tool for visual subtitling which is largely based on associating a visual lip movement corresponding to the audio track of the video.", "labels": [], "entities": []}, {"text": "The essential idea is based on the fact that recognition accuracies of audio, even for resource deficient languages is higher in viseme space than in the phoneme space.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: We describe the process of generation of visemes in Section 2 and describe the experimental work and evaluation of the proposed tool in Section 3 and conclude in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "All the evaluation results are for English videos only because the subjects were trained in Indian English lipreading.", "labels": [], "entities": []}, {"text": "We found access to people who are familiar with lipreading in a language other than English was hard to find.", "labels": [], "entities": []}, {"text": "The participants were asked to lipread a video often naturally recorded sentences to establish a baseline.", "labels": [], "entities": []}, {"text": "Only the mouth portion of the face was used in the baseline videos.", "labels": [], "entities": []}, {"text": "Evaluation was done under different experimental setups, namely, \u2022 Visual subtitles generated using three different visual features, namely, (a) MPEG-4 FAPs, (b) animated viseme images and and (c) real viseme images.", "labels": [], "entities": [{"text": "MPEG-4 FAPs", "start_pos": 145, "end_pos": 156, "type": "DATASET", "confidence": 0.7495909333229065}]}, {"text": "\u2022 Visual subtitles with and without the context of video.", "labels": [], "entities": []}, {"text": "\u2022 Videos played at different rates, namely, played at their original speed and half the speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9668944478034973}]}, {"text": "\u2022 Videos comprised of animated clip, classroom lecture, dias/conference lecture.", "labels": [], "entities": []}, {"text": "The participants' understanding based on visual observation of the visual subtitles was evaluated in terms of number of words correctly recognized.", "labels": [], "entities": [{"text": "understanding", "start_pos": 18, "end_pos": 31, "type": "METRIC", "confidence": 0.9677467942237854}]}, {"text": "In summary, visual subtitles were better understood under the following conditions (a) with the context of video, (b) when played at half the original speed and (c) when generated with real viseme images Note 4 MPEG-4 standard does not define FAPs for teeth, which play a significant role in lipreading, hence this aspect needs to be considered during the generation of Visual subtitles using MPEG-4 FAPs.", "labels": [], "entities": [{"text": "FAPs", "start_pos": 243, "end_pos": 247, "type": "METRIC", "confidence": 0.9712315797805786}]}], "tableCaptions": []}