{"title": [{"text": "POS Tagging for Historical Texts with Sparse Training Data", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7515023052692413}]}], "abstractContent": [{"text": "This paper presents a method for part-of-speech tagging of historical data and evaluates it on texts from different corpora of historical German (15th-18th century).", "labels": [], "entities": [{"text": "part-of-speech tagging of historical", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.7970375716686249}]}, {"text": "Spelling normalization is used to prepro-cess the texts before applying a POS tag-ger trained on modern German corpora.", "labels": [], "entities": [{"text": "Spelling normalization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9489820599555969}]}, {"text": "Using only 250 manually normalized tokens as training data, the tagging accuracy of a manuscript from the 15th century can be raised from 28.65% to 74.89%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9774398803710938}]}], "introductionContent": [], "datasetContent": [{"text": "Normalization is evaluated separately for each text, using apart of that text for training and evaluating on a different part of the same text.", "labels": [], "entities": [{"text": "Normalization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9333420991897583}]}, {"text": "To address the question of how much training data is needed, evaluation is performed with different sizes of the training set in a range between 100 and 1,000 tokens.", "labels": [], "entities": []}, {"text": "The evaluation set is kept at a fixed size of 1,000 tokens.", "labels": [], "entities": []}, {"text": "Normalization accuracy is calculated by taking the average of 10 trials with randomly drawn training and evaluation sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9660102725028992}]}, {"text": "The results of this evaluation are shown in.", "labels": [], "entities": []}, {"text": "The baseline score fora text is defined as the percentage of matching tokens between the unmodified, historical text and its gold-standard normalization.", "labels": [], "entities": []}, {"text": "There is a clear difference between the Anselm texts, with scores of 23% and 39%, and the GerManC-GS texts, which range from 72% to 83%.", "labels": [], "entities": [{"text": "Anselm texts", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.8839904069900513}, {"text": "GerManC-GS texts", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.9279160797595978}]}, {"text": "This shows that spelling variation affects significantly more wordforms in the Anselm texts.", "labels": [], "entities": [{"text": "Anselm texts", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9534159004688263}]}, {"text": "The age of a text is likely to be the main factor for this, as even within the group of GerManC-GS texts, a clear tendency for newer texts to have higher baseline scores can be observed.", "labels": [], "entities": []}, {"text": "Spelling normalization with the Norma tool shows rather positive results even for small training samples: with only 100 tokens used for training, it achieves a normalization accuracy of 69% for the Anselm texts, and raises the score for the GerManC-GS texts by 5-10 percentage points.", "labels": [], "entities": [{"text": "Spelling normalization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9518049359321594}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9211866855621338}, {"text": "Anselm texts", "start_pos": 198, "end_pos": 210, "type": "DATASET", "confidence": 0.8581397831439972}, {"text": "GerManC-GS texts", "start_pos": 241, "end_pos": 257, "type": "DATASET", "confidence": 0.787446916103363}]}, {"text": "Using 250 tokens results in another noticeable increase inaccuracy, although the relative gain from increasing the training size even further attenuates after this point.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Texts used for the evaluation", "labels": [], "entities": []}, {"text": " Table 2: Normalization accuracy after training on n tokens and evaluating on 1,000 tokens (average of  10 random training and evaluation sets), compared to the \"baseline\" score of the full text without any  normalization", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9738636016845703}]}, {"text": " Table 3: Tagging accuracy on the gold-standard  normalizations (OrigP = original punctuation,  ModP = modern punctuation, NoP = no punctu- ation)", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9567530751228333}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9884039163589478}]}, {"text": " Table 5: POS tagging accuracy on texts without punctuation and capitalization, for tagging on the original  data, the gold-standard normalization, and automatic normalizations using the first n tokens as training  data", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7095219194889069}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9870184063911438}]}]}