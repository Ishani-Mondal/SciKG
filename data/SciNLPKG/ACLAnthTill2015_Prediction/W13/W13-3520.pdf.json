{"title": [{"text": "Polyglot: Distributed Word Representations for Multilingual NLP", "labels": [], "entities": [{"text": "Distributed Word Representations", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6376489599545797}]}], "abstractContent": [{"text": "Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7319340705871582}]}, {"text": "In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias.", "labels": [], "entities": []}, {"text": "We quantitatively demonstrate the utility of our word em-beddings by using them as the sole features for training apart of speech tagger fora subset of these languages.", "labels": [], "entities": []}, {"text": "We find their performance to be competitive with near state-of-art methods in English, Dan-ish and Swedish.", "labels": [], "entities": []}, {"text": "Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings.", "labels": [], "entities": []}, {"text": "We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building multilingual processing systems is a challenging task.", "labels": [], "entities": []}, {"text": "Every NLP task involves different stages of preprocessing and calculating intermediate representations that will serve as features for later stages.", "labels": [], "entities": []}, {"text": "These stages vary in complexity and requirements for each individual language.", "labels": [], "entities": []}, {"text": "Despite recent momentum towards developing multilingual tools (), most of NLP research still focuses on rich resource languages.", "labels": [], "entities": []}, {"text": "Common NLP systems and tools rely heavily on English specific features and they are infrequently tested on multiple datasets.", "labels": [], "entities": []}, {"text": "This makes them hard to port to new languages and tasks.", "labels": [], "entities": []}, {"text": "A serious bottleneck in the current approach for developing multilingual systems is the requirement of familiarity with each language under consideration.", "labels": [], "entities": []}, {"text": "These systems are typically carefully tuned with hand-manufactured features designed by experts in a particular language.", "labels": [], "entities": []}, {"text": "This approach can yield good performance, but tends to create complicated systems which have limited portability to new languages, in addition to being hard to enhance and maintain.", "labels": [], "entities": []}, {"text": "Recent advancements in unsupervised feature learning present an intriguing alternative.", "labels": [], "entities": []}, {"text": "Instead of relying on expert knowledge, these approaches employ automatically generated taskindependent features (or word embeddings) given large amounts of plain text.", "labels": [], "entities": []}, {"text": "Recent developments have led to state-of-art performance in several NLP tasks such as language modeling, and syntactic tasks such as sequence tagging ).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7292488515377045}, {"text": "sequence tagging", "start_pos": 133, "end_pos": 149, "type": "TASK", "confidence": 0.7013411819934845}]}, {"text": "These embeddings are generated as a result of training \"deep\" architectures, and it has been shown that such representations are well suited for domain adaptation tasks.", "labels": [], "entities": []}, {"text": "We believe two problems have held back the research community's adoption of these methods.", "labels": [], "entities": []}, {"text": "The first is that learning representations of words involves huge computational costs.", "labels": [], "entities": []}, {"text": "The process usually involves processing billions of words over weeks.", "labels": [], "entities": []}, {"text": "The second is that so far, these systems have been built and tested mainly on English.", "labels": [], "entities": []}, {"text": "In this work we seek to remove these barriers to entry by generating word embeddings for over a hundred languages using state-of-the-art techniques.", "labels": [], "entities": []}, {"text": "Specifically, our contributions include: \u2022 Word embeddings -We will release word embeddings for the hundred and seventeen languages that have more than 10,000 articles on Wikipedia.", "labels": [], "entities": []}, {"text": "Each language's vocabulary will contain up to 100,000 words.", "labels": [], "entities": []}, {"text": "The embeddings will be publicly available at (www.cs.stonybrook.edu/ \u02dc dsl), for the research community to study their characteristics and build systems for new languages.", "labels": [], "entities": []}, {"text": "We believe our embeddings represent a valuable resource because they contain a minimal amount of normalization.", "labels": [], "entities": []}, {"text": "For example, we do not lowercase words for European languages as other studies have done for English.", "labels": [], "entities": []}, {"text": "This preserves features of the underlying language.", "labels": [], "entities": []}, {"text": "\u2022 Quantitative analysis -We investigate the embedding's performance on a part-ofspeech (PoS) tagging task, and conduct qualitative investigation of the syntactic and semantic features they capture.", "labels": [], "entities": [{"text": "Quantitative analysis", "start_pos": 2, "end_pos": 23, "type": "TASK", "confidence": 0.8949487507343292}]}, {"text": "Our experiments represent a valuable chance to evaluate distributed word representations for NLP as the experiments are conducted in a consistent manner and a large number of languages are covered.", "labels": [], "entities": []}, {"text": "As the embeddings capture interesting linguistic features, we believe the multilingual resource we are providing gives researchers a chance to create multilingual comparative experiments.", "labels": [], "entities": []}, {"text": "\u2022 Efficient implementation -Training these models was made possible by our contributions to Theano (machine learning library ().", "labels": [], "entities": [{"text": "Theano", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.9670859575271606}]}, {"text": "These optimizations empower researchers to produce word embeddings under different settings or for different corpora than Wikipedia.", "labels": [], "entities": []}, {"text": "The rest of this paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we give an overview of semi-supervised learning and learning representations related work.", "labels": [], "entities": []}, {"text": "We then describe, in Section 3, the network used to generate the word embeddings and its characteristics.", "labels": [], "entities": []}, {"text": "Section 4 discusses the details of the corpus collection and preparation steps we performed.", "labels": [], "entities": [{"text": "corpus collection", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.720826581120491}]}, {"text": "Next, in Section 5, we discuss our experimental setup and the training progress overtime.", "labels": [], "entities": []}, {"text": "In Section 6 we discuss the semantic features captured by the embeddings by showing examples of the word groupings in multiple languages.", "labels": [], "entities": []}, {"text": "Finally, in Section 7 we demonstrate the quality of our learned features by training a PoS tagger on several languages and then conclude.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics of a subset of the languages pro- cessed. The second column reports the number of  tokens found in the corpus in millions while the  third column reports the word types found in thou- sands. The coverage indicates the percentage of  the corpus that will be matching words in a vocab- ulary consists of the most frequent 100 thousand  words.", "labels": [], "entities": []}, {"text": " Table 4: Results of our model against several PoS datasets. The performance is measured using accuracy  over the test datasets. Third column represents the total accuracy of the tagger the former two columns  reports the accuracy over known words and OOV words (unknown). The results are compared to the  TnT tagger results reported by (Petrov et al., 2012).", "labels": [], "entities": [{"text": "PoS datasets", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8433943390846252}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9962546825408936}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.989117443561554}, {"text": "accuracy", "start_pos": 222, "end_pos": 230, "type": "METRIC", "confidence": 0.9843701124191284}]}, {"text": " Table 5: Coverage statistics of the embedding's  vocabulary on the part of speech datasets after nor- malization. Token coverage is the raw percentage  of words which were known, while the Word cov- erage ignores repeated words.", "labels": [], "entities": []}]}