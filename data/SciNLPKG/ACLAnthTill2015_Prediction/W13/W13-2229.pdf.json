{"title": [{"text": "Tunable Distortion Limits and Corpus Cleaning for SMT", "labels": [], "entities": [{"text": "Corpus Cleaning", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7333642095327377}, {"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9845157861709595}]}], "abstractContent": [{"text": "We describe the Uppsala University system for WMT13, for English-to-German translation.", "labels": [], "entities": [{"text": "Uppsala University system", "start_pos": 16, "end_pos": 41, "type": "DATASET", "confidence": 0.9757819374402364}, {"text": "WMT13", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.7619920969009399}]}, {"text": "We use the Docent decoder, a local search decoder that translates at the document level.", "labels": [], "entities": [{"text": "Docent decoder", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.8427223861217499}]}, {"text": "We add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to Do-cent.", "labels": [], "entities": []}, {"text": "We also investigate cleaning of the noisy Common Crawl corpus.", "labels": [], "entities": [{"text": "Common Crawl corpus", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.9721558292706808}]}, {"text": "We show that we can use alignment-based filtering for cleaning with good results.", "labels": [], "entities": []}, {"text": "Finally we investigate effects of corpus selection for recasing.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we present the Uppsala University submission to WMT 2013.", "labels": [], "entities": [{"text": "Uppsala University submission to WMT 2013", "start_pos": 29, "end_pos": 70, "type": "DATASET", "confidence": 0.9503624439239502}]}, {"text": "We have submitted one system, for translation from English to German.", "labels": [], "entities": [{"text": "translation from English to German", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.8467241287231445}]}, {"text": "In our submission we use the document-level).", "labels": [], "entities": []}, {"text": "In the current setup, we take advantage of Docent in that we introduce tunable distortion limits, that is, modeling distortion limits as soft constraints instead of as hard constraints.", "labels": [], "entities": []}, {"text": "In addition we perform experiments on corpus cleaning.", "labels": [], "entities": [{"text": "corpus cleaning", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.832106351852417}]}, {"text": "We investigate how the noisy Common Crawl corpus can be cleaned, and suggest an alignmentbased cleaning method, which works well.", "labels": [], "entities": [{"text": "Common Crawl corpus", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9395734667778015}]}, {"text": "We also investigate corpus selection for recasing.", "labels": [], "entities": [{"text": "corpus selection", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7423858046531677}]}, {"text": "In Section 2 we introduce our decoder, Docent, followed by a general system description in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our experiments with corpus cleaning, and in Section 5 we describe experiments with tunable distortion limits.", "labels": [], "entities": [{"text": "corpus cleaning", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.793247640132904}]}, {"text": "In Section 6 we investigate corpus selection for recasing.", "labels": [], "entities": [{"text": "corpus selection", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7525010108947754}]}, {"text": "In Section 7 we compare our results with Docent to results using Moses ().", "labels": [], "entities": []}, {"text": "We conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Size of Common Crawl after the different  cleaning steps and reduction in size compared to  the previous step", "labels": [], "entities": []}, {"text": " Table 2: Reasons and correctness for removing sentences based on language ID for 93 sentences out of  a 1000 sentence subset, divided into wrong lang(uage), non-corr(esponding) pairs, and corr(esponding)  pairs.", "labels": [], "entities": []}, {"text": " Table 3: Results of alignment-based cleaning for different values of the filtering parameters, with pre- cision, recall and F-score for the identification of erroneous sentence pairs and the percentage of kept  sentence pairs", "labels": [], "entities": [{"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9972054362297058}, {"text": "F-score", "start_pos": 125, "end_pos": 132, "type": "METRIC", "confidence": 0.9956040382385254}]}, {"text": " Table 4: Cross-entropy (CE) and relative interpo- lation weights (IP) compared to EP-NC for the  Common Crawl corpus, with different cleaning", "labels": [], "entities": [{"text": "Cross-entropy (CE)", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7631167769432068}, {"text": "relative interpo- lation weights (IP)", "start_pos": 33, "end_pos": 70, "type": "METRIC", "confidence": 0.897653765976429}, {"text": "Common Crawl corpus", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.9040918151537577}]}, {"text": " Table 5: Bleu scores with different types of clean- ing and without Common Crawl", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9304214119911194}]}, {"text": " Table 6: Bleu scores for different distortion limit  (DL) settings", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9908022880554199}, {"text": "distortion limit  (DL)", "start_pos": 36, "end_pos": 58, "type": "METRIC", "confidence": 0.9715089797973633}]}, {"text": " Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model and  translation model (TM) for recasing", "labels": [], "entities": [{"text": "recasing", "start_pos": 127, "end_pos": 135, "type": "TASK", "confidence": 0.85300213098526}]}, {"text": " Table 8: Bleu scores for Docent initialized ran- domly or with stack decoding compared to Moses.  Tuning is performed with either Moses or Docent.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9864517450332642}]}]}