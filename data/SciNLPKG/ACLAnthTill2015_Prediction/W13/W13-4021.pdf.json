{"title": [], "abstractContent": [{"text": "We model human responses to speech recognition errors from a corpus of human clarification strategies.", "labels": [], "entities": [{"text": "speech recognition errors", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7620181938012441}]}, {"text": "We employ learning techniques to study 1) the decision to either stop and ask a clarification question or to continue the dialogue without clarification, and 2) the decision to ask a targeted clarification question or a more generic question.", "labels": [], "entities": []}, {"text": "Targeted clarification questions focus specifically on the part of an utterance that is misrecognized, in contrast with generic requests to 'please repeat' or 'please rephrase'.", "labels": [], "entities": []}, {"text": "Our goal is to generate targeted clarification strategies for handling errors in spoken dialogue systems, when appropriate.", "labels": [], "entities": []}, {"text": "Our experiments show that linguistic features, in particular the inferred part-of-speech of a misrecognized word are predictive of human clarification decisions.", "labels": [], "entities": []}, {"text": "A combination of linguistic features predicts a user's decision to continue or stop a dialogue with accuracy of 72.8% over a majority baseline accuracy of 59.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9993651509284973}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9876818060874939}]}, {"text": "The same set of features predict the decision to ask a targeted question with accuracy of 74.6% compared with the majority baseline of 71.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9995920062065125}]}], "introductionContent": [{"text": "Clarification questions are common in human-human dialogue.", "labels": [], "entities": []}, {"text": "They help dialogue participants maintain dialogue flow and resolve misunderstandings.", "labels": [], "entities": []}, {"text": "finds that in human-human dialogue speakers most frequently use reprise clarification questions to resolve recognition errors.", "labels": [], "entities": []}, {"text": "Reprise clarification questions use portions of the misunderstood utterance which are thought to be correctly recognized to target the part of an utterance that was misheard or misunderstood.", "labels": [], "entities": []}, {"text": "In the following example from, Speaker B has failed to hear the word toast and so constructs a clarification question using a portion of the correctly understood utterance -the word some -to query the portion of the utterance B has failed to understand: A: Can I have some toast please?", "labels": [], "entities": []}, {"text": "Unlike human conversational partners, most dialogue systems today employ generic 'please repeat/rephrase' questions asking a speaker to repeat or rephrase an entire utterance.", "labels": [], "entities": []}, {"text": "Our goal is to introduce reprise, or targeted, clarifications into an automatic spoken system.", "labels": [], "entities": []}, {"text": "Targeted clarifications can be especially useful for systems accepting unrestricted speech, such as tutoring systems, intelligent agents, and speech translation systems.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.721344843506813}]}, {"text": "Using a reprise question, a user can correct an error by repeating only a portion of an utterance.", "labels": [], "entities": []}, {"text": "Targeted questions also provide natural grounding and implicit confirmation by signalling to the conversation partner which parts of an utterance have been recognized.", "labels": [], "entities": []}, {"text": "In order to handle a misrecognition, the system must first identify misrecognized words (, determine the type of question to ask, and construct the question.", "labels": [], "entities": []}, {"text": "In this work, we address two points necessary for determining the type of question to ask: \u2022 Is it appropriate fora system to ask a clarification question when a misrecognized word is detected?", "labels": [], "entities": []}, {"text": "\u2022 Is it possible to ask a targeted clarification question fora given sentence and an error segment?", "labels": [], "entities": []}, {"text": "To answer these questions, we analyze a corpus of human responses to transcribed utterances with missing information which we collected using Amazon Mechanical.", "labels": [], "entities": [{"text": "Amazon Mechanical", "start_pos": 142, "end_pos": 159, "type": "DATASET", "confidence": 0.9536008536815643}]}, {"text": "Although the data collection was text-based, we asked annotators to respond as they would in a dialogue.", "labels": [], "entities": []}, {"text": "In Section 2, we describe related work on error recovery strategies in dialogue systems.", "labels": [], "entities": [{"text": "error recovery", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7088610678911209}]}, {"text": "In Section 3, we describe the corpus used in this experiment.", "labels": [], "entities": []}, {"text": "In Section 4, we describe our experiments on human clarification strategy modelling.", "labels": [], "entities": [{"text": "human clarification strategy modelling", "start_pos": 45, "end_pos": 83, "type": "TASK", "confidence": 0.804800234735012}]}, {"text": "We conclude in Section 5 with our plan for applying our models in spoken systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use our AMT annotations to build classifiers for 1) choice of action: stop and engage in clarification vs. continue dialogue; and 2) type of clarification question (targeted vs. non-targeted) to ask.", "labels": [], "entities": []}, {"text": "For the continue/stop experiment, we aim to determine whether a system should stop and ask a clarification question.", "labels": [], "entities": []}, {"text": "For the targeted vs. non-targeted experiment, we aim to determine whether it is possible to ask a targeted clarification question.", "labels": [], "entities": []}, {"text": "Using the Weka () machine learning framework, we build classifiers to predict AMT decisions.", "labels": [], "entities": [{"text": "Weka () machine learning framework", "start_pos": 10, "end_pos": 44, "type": "DATASET", "confidence": 0.8874172449111939}, {"text": "AMT decisions", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.8900683522224426}]}, {"text": "We automatically assign POS tags to transcripts using the Stanford tagger.", "labels": [], "entities": [{"text": "Stanford tagger", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.9334816336631775}]}, {"text": "We compare models built with an automatically tagged POS for an error word (POS-auto) with one built with POS guessed by a user (POSguess).", "labels": [], "entities": []}, {"text": "Although a dialogue manager may not have access to a correct POS, it may simulate this by predicting POS of the error.", "labels": [], "entities": [{"text": "POS", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9582027196884155}]}, {"text": "We assign dependency tags using the AMU dependency parser) which has been optimized on the Transtac dataset.", "labels": [], "entities": [{"text": "AMU dependency parser", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.6912938356399536}, {"text": "Transtac dataset", "start_pos": 91, "end_pos": 107, "type": "DATASET", "confidence": 0.9607708156108856}]}, {"text": "We hypothesize that a user's dialogue move depends on the syntactic structure of a sentence as well as on syntactic and semantic information about the error word and its syntactic parent.", "labels": [], "entities": []}, {"text": "To capture sentence structure, we use features associated with the whole sentence: POS ngram, all pairs of parent-child dependency tags in a sentence (Dep-pair), and all semantic roles (Sem-presence) in a sentence.", "labels": [], "entities": []}, {"text": "To capture the syntactic and semantic role of a misrecognized word, we use features associated with this word: POS tag, dependency tag (Dep-tag), POS of the parent word (Parent-POS), and semantic role of an error word (Sem-role).", "labels": [], "entities": [{"text": "POS", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9516847133636475}]}, {"text": "We first model individual annotators' decisions for each of the three annotation instances.", "labels": [], "entities": []}, {"text": "We measure the value that each feature adds to a model, using annotators' POS guess (POS-guess).", "labels": [], "entities": [{"text": "POS guess (POS-guess)", "start_pos": 74, "end_pos": 95, "type": "METRIC", "confidence": 0.8652759790420532}]}, {"text": "Next, we model a joint annotators' decision using the automatically assigned POS-auto feature.", "labels": [], "entities": []}, {"text": "This model simulates a system behaviour in a dialogue with a user where a system chooses a single dialogue move for each situation.", "labels": [], "entities": []}, {"text": "We run 10-fold cross validation using the Weka J48 Deci-2 If any annotators asked a targeted question, we assign a positive label to this instance, and negative otherwise.", "labels": [], "entities": [{"text": "Weka J48 Deci-2", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9628708362579346}]}, {"text": "shows the results of continue/stop classification.", "labels": [], "entities": [{"text": "continue/stop classification", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.8030862361192703}]}, {"text": "A majority baseline method predicts the most frequent class continue and has 59.1% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9989802241325378}]}, {"text": "In comparison, our classifier, built with all features, achieves 72.8% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9989176988601685}]}, {"text": "Next, we evaluate the utility of each feature by removing it from the feature set and comparing the model built without it with a model built on all features.", "labels": [], "entities": []}, {"text": "POS is the most useful feature, as we expected: when it is removed from the feature set, the f-measure decreases by 6.7%.", "labels": [], "entities": []}, {"text": "A model trained on the POS-guess feature alone outperforms a model trained on all other features.", "labels": [], "entities": []}, {"text": "Word position in the sentence is the next most salient feature, contributing 2% to the f-measure.", "labels": [], "entities": []}, {"text": "The syntactic dependency features Syn-Dep, Dep-pair, and Parent POS together contribute 1.9%.", "labels": [], "entities": []}, {"text": "Next, we predict a majority decision for each sentence.", "labels": [], "entities": []}, {"text": "shows the accuracy of this prediction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9998049139976501}]}, {"text": "A majority baseline has an accuracy of 59.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9997554421424866}]}, {"text": "When we use a model trained on the POS-auto feature alone, accuracy rises to 66.1%, while a combination of all features further increases it to 69.2%.: Stop/Continue experiment predicting majority decision, using POS-auto.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.999683141708374}, {"text": "predicting majority decision", "start_pos": 177, "end_pos": 205, "type": "TASK", "confidence": 0.8416548371315002}]}, {"text": "\u2020indicates statistically significant difference from the majority baseline (p<.01).", "labels": [], "entities": []}, {"text": "In this experiment, we classify each instance into targeted or not targeted categories.", "labels": [], "entities": []}, {"text": "The targeted category comprises the cases in which an annotator chooses to stop and ask a targeted question.", "labels": [], "entities": []}, {"text": "We are interested in identifying these cases in order to determine whether a system should try to ask a targeted clarification question.", "labels": [], "entities": []}, {"text": "shows the results of this experiment.", "labels": [], "entities": []}, {"text": "The majority baseline predicts not targeted and has a 71.8% accuracy because inmost cases, no question is asked.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.999147891998291}]}, {"text": "A model trained on all features increases accuracy to 74.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9997279047966003}]}, {"text": "POS is the most salient feature, contributing 3.8% to the f-measure.", "labels": [], "entities": [{"text": "POS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8773314356803894}, {"text": "f-measure", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.8637393712997437}]}, {"text": "All models that use POS feature are significantly different from the baseline.", "labels": [], "entities": []}, {"text": "The next most salient features are POS ngram and a combination of syntactic dependency features contributing 1% and .5% to the f-measure respectively.", "labels": [], "entities": [{"text": "POS ngram", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.5509604811668396}]}, {"text": "shows system performance in predicting a joint annotators' decision of whether a targeted question can be asked.", "labels": [], "entities": []}, {"text": "A joint decision in this experiment is considered not targeted when none of the annotators chooses to ask a targeted question.", "labels": [], "entities": []}, {"text": "We aim at identifying the cases where position of an error word makes it difficult to ask a clarification question, such as fora sentence XXX somebody steal these supplies.", "labels": [], "entities": []}, {"text": "Using the automatically assigned POS (POS-auto) feature alone achieves an accuracy of 62.2%, which is almost 10% above the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.999514102935791}]}, {"text": "A combination of all features, surprisingly, lowers the accuracy to 59.4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9995473027229309}]}, {"text": "Interestingly, a combination of all features less POS increases accuracy: Targeted/not experiment predicting majority decision, using POS tag feature POS-auto.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9993374943733215}, {"text": "Targeted/not experiment predicting majority decision", "start_pos": 74, "end_pos": 126, "type": "TASK", "confidence": 0.6397915993418012}]}, {"text": "\u2020indicates statistically significant difference from the majority baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotation summary for single-word and  multi-word error cases. Absolute annotator agreement  is shown for single-word error cases.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8882591128349304}, {"text": "Absolute annotator agreement", "start_pos": 74, "end_pos": 102, "type": "METRIC", "confidence": 0.9021664460500082}]}, {"text": " Table 3: Stop/Continue experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   \u2020indicates statistically significant difference from the  majority baseline (p<.01)", "labels": [], "entities": [{"text": "predicting individ- ual annotator's decision", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.8111219235828945}, {"text": "POS-guess", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9765809774398804}, {"text": "Accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9986574649810791}, {"text": "F- measure", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9936274687449137}, {"text": "Difference", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9903407692909241}]}, {"text": " Table 5: Targeted/not experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   \u2020indicates statistically significant difference from the  majority baseline (p<.05)", "labels": [], "entities": [{"text": "predicting individ- ual annotator's decision", "start_pos": 34, "end_pos": 78, "type": "TASK", "confidence": 0.8408388921192714}, {"text": "POS-guess", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9758144021034241}, {"text": "Accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9988152980804443}, {"text": "F- measure", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9940058986345927}, {"text": "Difference", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9884705543518066}]}, {"text": " Table 6: Targeted/not experiment predicting majority  decision, using POS tag feature POS-auto.  \u2020indicates  statistically significant difference from the majority  baseline.", "labels": [], "entities": [{"text": "predicting majority  decision", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.8833313981691996}]}]}