{"title": [{"text": "Intensionality was only alleged: On adjective-noun composition in distributional semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional semantics has very successfully modeled semantic phenomena at the word level, and recently interest has grown in extending it to capture the meaning of phrases via semantic composition.", "labels": [], "entities": [{"text": "Distributional semantics", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7512288391590118}]}], "introductionContent": [{"text": "Distributional semantics (see, for an overview) has been very successful in modeling lexical semantic phenomena, from psycholinguistic facts such as semantic priming) to tasks such as picking the right synonym on a TOEFL exercise).", "labels": [], "entities": []}, {"text": "More recently, interest has increased in using distributional models to account not only for word meaning but also for phrase meaning, i.e. semantic composition (.", "labels": [], "entities": []}, {"text": "Adjectival modification of nouns is a particularly useful and at the same time challenging testbed for different distributional models of composition, because syntactically it is very simple, while the semantic effect of the composition is very variable and potentially complex due to the frequent context dependence of the relation between the adjective and the noun (Asher, 2011, provides recent discussion).", "labels": [], "entities": [{"text": "Adjectival modification of nouns", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8832075446844101}]}, {"text": "As a comparatively underexplored area of semantic theory, it is also an empirical domain where distributional models can give feedback to theoreticians about how adjectival modification works.", "labels": [], "entities": [{"text": "semantic theory", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.8774682283401489}]}, {"text": "In the formal semantic tradition, the analysis of adjectives has been largely motivated by the general entailment patterns in which they participate.", "labels": [], "entities": []}, {"text": "For example, if something is a white towel, then it is both white and a towel.", "labels": [], "entities": []}, {"text": "This use of white is intersective: it yields an adjective-noun phrase (hereafter, AN phrase) whose denotation is the intersection of the denotations of the adjective and the noun.", "labels": [], "entities": []}, {"text": "If someone is a skillful surgeon, then she is a surgeon but not necessarily skillful in general.", "labels": [], "entities": []}, {"text": "Such adjectives are subsective: The denotation of the phrase is a subset of that of the noun.", "labels": [], "entities": []}, {"text": "Finally, if someone is an alleged murderer, we cannot be sure that she is a murderer, and it is not even grammatical to say that she is \"alleged\".", "labels": [], "entities": []}, {"text": "Intensional adjectives thus do not appear to describe attributes or relations; rather, they are almost universally modeled as higher-order properties, whereas intersective and subsective (hereafter, non-intensional) adjectives have been given both first-order and higher-order analyses.", "labels": [], "entities": []}, {"text": "Given these facts, we can expect that intensional adjectives will be more difficult to model computationally than non-intensional adjectives.", "labels": [], "entities": []}, {"text": "Moreover, they raise specific issues for the increasingly popular distributional approaches to semantics.", "labels": [], "entities": []}, {"text": "First, as intensional adjectives cannot be modeled as first-order properties, it is hard to predict what their representations might look like or what their semantic effect would be in standard distributional models of composition based on vector addition or multiplication.", "labels": [], "entities": []}, {"text": "This is so because addition and multiplication correspond to feature combination (see Section 2 for discussion), and it is not obvious what set of distinctive distributional features an intensional adjective would contribute on a consistent basis.", "labels": [], "entities": []}, {"text": "In, we presented a first distributional semantic study of intensional adjectives.", "labels": [], "entities": []}, {"text": "However, our study was limited in two ways.", "labels": [], "entities": []}, {"text": "First, it compared intensional adjectives with a very narrow class of non-intensional adjectives, namely color terms; this raises doubts about the generality of our results.", "labels": [], "entities": []}, {"text": "Second, the study had methodological weaknesses, as we did not separate training and test data, nor did we do any systematic parameter tuning prior to carrying out our experiments.", "labels": [], "entities": []}, {"text": "This paper adresses these limitations by covering a wider variety of adjectives and using a better implementation of the composition functions, and performs several qualitative analyses on the results.", "labels": [], "entities": []}, {"text": "Our results confirm that high quality adjective composition is possible in distributional models: Meaningful vectors can be composed, if we take phrase vectors directly extracted from the corpus as a benchmark.", "labels": [], "entities": []}, {"text": "In addition, we find (perhaps unsurprisingly) that models that replicate higher-order predication within a distributional approach, such as and, fare better than models based on vector addition or multiplication.", "labels": [], "entities": []}, {"text": "However, unlike our previous study, we find no difference in the relative success of the different composition models on intensional vs. non-intensional modification, nor in relevant aspects of the distributional representations of corpus-harvested phrases.", "labels": [], "entities": []}, {"text": "Rather, two relevant effects involve the polysemy of the noun and the extent to which the adjective denotes atypical attribute of the entity described by the noun.", "labels": [], "entities": []}, {"text": "These results indicate that, in general, adjectival modification is more complex than simple feature intersection, even for adjectives like white or ripe.", "labels": [], "entities": [{"text": "adjectival modification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7681271731853485}]}, {"text": "We therefore find tentative support for modeling adjectives as higher-order functors as a rule, despite the fact that entailment phenomena do not force such a conclusion and certain facts have even been used to argue against it.", "labels": [], "entities": []}, {"text": "The results also raise deeper and more general questions concerning the extent to which the entailment-based classification is cognitively salient, and point to the need for clarifying how polysemy and typicality intervene in the composition process and how they are to be reflected in semantic representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "A distributional semantic space is a matrix whose rows represent target elements in terms of (functions of) their patterns of co-occurrence with contexts (columns or dimensions).", "labels": [], "entities": []}, {"text": "Several parameters must be manually fixed or tuned to instantiate the space.", "labels": [], "entities": []}, {"text": "Our source corpus is given by the concatenation of the ukWaC corpus, a mid-2009 dump of the English Wikipedia and the British National Corpus, 2 fora total of about 2.8 billion tokens.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9897472262382507}, {"text": "English Wikipedia", "start_pos": 92, "end_pos": 109, "type": "DATASET", "confidence": 0.8431942462921143}, {"text": "British National Corpus", "start_pos": 118, "end_pos": 141, "type": "DATASET", "confidence": 0.9248218536376953}]}, {"text": "The corpora have been dependency-parsed with the MALT parser), so it is straightforward to extract all cases of adjective-noun modification.", "labels": [], "entities": []}, {"text": "We use part-of-speech-aware lemmas as our representations both for target elements and dimensions.", "labels": [], "entities": []}, {"text": "(e.g., we distinguish between noun and verb forms of can).", "labels": [], "entities": []}, {"text": "The target elements in our semantic space are the 4K most frequent adjectives, the 8K most frequent nouns, and approximately 30K AN phrases.", "labels": [], "entities": []}, {"text": "The phrases were composed only of adjectives and nouns in the semantic space, and were chosen as follows: a) all the phrases for the dataset that we evaluate on (see Section 3.3 below), and b) the top 10K most frequent phrases, excluding the 1,000 most frequent ones to avoid highly collocational / non-compositional phrases.", "labels": [], "entities": []}, {"text": "The phrases were used for training purposes, and also entered in the computation of the nearest neighbors.", "labels": [], "entities": []}, {"text": "The dimensions of our semantic space are the top 10K most frequent content words in the corpus (nouns, adjectives, verbs and adverbs).", "labels": [], "entities": []}, {"text": "We use a bag-of-words representation: Each target word or phrase is represented in terms of its co-occurrences with content words within the same sentence.", "labels": [], "entities": []}, {"text": "Note that this also applies to the AN phrases: We build vectors for phrases in the same way we do for adjectives and nouns, by collecting co-occurrence counts with the dimensions of the space (.", "labels": [], "entities": []}, {"text": "This way, we have the same type of representation for, say, hard, rock, and hard rock.", "labels": [], "entities": []}, {"text": "We will call the vectors directly extracted from the corpus (as opposed to derived compositionally) observed vectors.", "labels": [], "entities": []}, {"text": "We optimized the remaining parameters of our semantic space construction on the independent task of maximizing correlation with human semantic relatedness ratings on the MEN benchmark 3 (see the references on distributional semantics at the beginning of Section 2 above for an explanation of the parameters).", "labels": [], "entities": [{"text": "MEN benchmark 3", "start_pos": 170, "end_pos": 185, "type": "DATASET", "confidence": 0.9235232273737589}]}, {"text": "We found that the best model on this task was one where all dimensions where used (as opposed to removing the 50 or 300 most frequent dimensions), the co-occurrence matrix was weighted by Pointwise Mutual Information (as opposed to: no weighting, logarithm transform, Local Mutual Information), dimensionality reduction was performed by Nonnegative Matrix Factorization 4 (as opposed to: no reduction, Singular Value Decomposition), and the dimensionality of the reduced space was 350 (among values from 50 to 350 in steps of 50).", "labels": [], "entities": []}, {"text": "The best performing model achieved very high 0.78 (Pearson) and 0.76 (Spearman) correlation scores with the MEN dataset, suggesting that we are using a high-quality semantic space.", "labels": [], "entities": [{"text": "correlation", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.5770790576934814}, {"text": "MEN dataset", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.9600427448749542}]}, {"text": "We evaluate the models on a set of 16 intensional adjectives and a set of 16 non-intensional adjectives, paired according to frequency (see).", "labels": [], "entities": []}, {"text": "The intensional adjectives were chosen starting from the candidate list elaborated for, with two modifications.", "labels": [], "entities": []}, {"text": "First, the frequency criteria were altered, allowing the addition of seven more adjectives (e.g., alleged and putative).", "labels": [], "entities": []}, {"text": "Second, we removed adjectives that can be used predicatively with the same intensional interpretation despite having been claimed to meet the entailment test for intensionality; this excludes, e.g., false (cp. This sentence is false).", "labels": [], "entities": []}, {"text": "Adjectives that have a non-intensional predicative use alongside a non-predicative intensional one, e.g., possible (cp.", "labels": [], "entities": []}, {"text": "The possible winner vs. ??The winner was possible, but Peace was possible) were left in, despite the potential for introducing some noise.", "labels": [], "entities": []}, {"text": "The non-intensional adjectives were chosen by generating, for each intensional adjective, a list of the 20 adjectives closest in frequency and taking from that list the closest match in frequency that was morphologically simple (excluding, e.g., unexpected or photographic) and unambiguously an adjective (excluding, e.g., super and many).", "labels": [], "entities": []}, {"text": "We used all the AN phrases in the corpus with a frequency of at least 20 for all adjectives except the underrepresented ones (nasty, mock, probable, hypothetical, impossible, naive, presumed, putative, vile, meagre, ripe), for which we selected at most 200 phrases, taking phrases down to a frequency of 5 if needed.", "labels": [], "entities": []}, {"text": "For each adjective, we randomly sampled 50 phrases for testing (total: 1,600).", "labels": [], "entities": []}, {"text": "The rest were used for training, as described above.", "labels": [], "entities": []}, {"text": "The results and analyses in sections 4 and 5 concern the test data only.: Predicted-to-observed vector cosines for each model (mean \u00b1 standard deviation), globally and by adjective type.", "labels": [], "entities": []}, {"text": "The last two columns show the average % of the 50 nearest neighbors that are adjectives (NN=A) and nouns (NN=N), as opposed to AN phrases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Predicted-to-observed vector cosines for each model (mean \u00b1 standard deviation), globally  and by adjective type. The last two columns show the average % of the 50 nearest neighbors that are  adjectives (NN=A) and nouns (NN=N), as opposed to AN phrases.", "labels": [], "entities": []}]}