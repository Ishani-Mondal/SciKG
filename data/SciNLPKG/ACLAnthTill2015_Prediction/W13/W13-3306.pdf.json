{"title": [{"text": "Machine Translation with Many Manually Labeled Discourse Connectives", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7470064163208008}]}], "abstractContent": [{"text": "The paper presents machine translation experiments from English to Czech with a large amount of manually annotated discourse connectives.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.6891074478626251}]}, {"text": "The gold-standard discourse relation annotation leads to better translation performance in ranges of 4-60% for some ambiguous English con-nectives and helps to find correct syntacti-cal constructs in Czech for less ambiguous connectives.", "labels": [], "entities": []}, {"text": "Automatic scoring confirms the stability of the newly built discourse-aware translation systems.", "labels": [], "entities": []}, {"text": "Error analysis and human translation evaluation point to the cases where the annotation was most and where less helpful.", "labels": [], "entities": [{"text": "human translation evaluation", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.6909957726796468}]}], "introductionContent": [{"text": "Recently, research in statistical machine translation (SMT) has renewed interest in the fact that fora variety of linguistic phenomena one needs information from a longer-range context.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.8226479440927505}]}, {"text": "Current statistical translation models and decoding algorithms operate at the sentence and/or phrase level only, not considering already translated context from previous sentences.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.6910203099250793}]}, {"text": "This local distance is in many cases too restrictive to correctly model lexical cohesion, referential expressions (noun phrases, pronouns), and discourse markers, all of which relate to the sentence(s) before the one to be translated.", "labels": [], "entities": []}, {"text": "Discourse relations between sentences are often conveyed by explicit discourse connectives (DC), such as although, because, but, since, while.", "labels": [], "entities": []}, {"text": "DCs play a significant role in coherence and readability of a text.", "labels": [], "entities": []}, {"text": "Likewise, if a wrong connective is used in translation, the target text can be fully incomprehensible or not conveying the same meaning as was established by the discourse relations in the source text.", "labels": [], "entities": []}, {"text": "In English, about 100 types of such explicit connectives have been annotated in the Penn Discourse TreeBank (PDTB, see Section 4), signaling discourse relations such as temporality or contrast between two spans of text.", "labels": [], "entities": [{"text": "Penn Discourse TreeBank (PDTB", "start_pos": 84, "end_pos": 113, "type": "DATASET", "confidence": 0.9559110760688782}]}, {"text": "Depending on the set of relations used, there can be up to 130 such relations and combinations thereof.", "labels": [], "entities": []}, {"text": "Discourse relations can also be present implicitly (inferred from the context), without any explicit marker being present.", "labels": [], "entities": []}, {"text": "Although annotation for implicit DCs exists as well, we only deal with explicit DCs in this paper.", "labels": [], "entities": []}, {"text": "DCs are difficult to translate mainly because a same English connective can signal different discourse relations in different contexts and when the target language has either different connectives according to the source relations signaled or uses different lexical or syntactical constructs in place of the English connective.", "labels": [], "entities": []}, {"text": "In this paper, we present MT experiments from English (EN) to Czech (CZ) with a large amount of manually annotated DCs.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9948654770851135}]}, {"text": "The corpus, the parallel Prague Czech-English Dependency Treebank (PCEDT) (Section 4), is directly usable for MT experiments: the entire discourse annotation in EN is paralleled with a human CZ translation.", "labels": [], "entities": [{"text": "Prague Czech-English Dependency Treebank (PCEDT)", "start_pos": 25, "end_pos": 73, "type": "DATASET", "confidence": 0.7803835996559688}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9951044321060181}]}, {"text": "This means that we can build and evaluate, against the CZ reference, a translation system, that learns from the EN gold standard discourse relations.", "labels": [], "entities": [{"text": "CZ reference", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9118875563144684}, {"text": "EN gold standard discourse relations", "start_pos": 112, "end_pos": 148, "type": "DATASET", "confidence": 0.9284600734710693}]}, {"text": "These then have no distortion from wrongly labeled connectives as it is given in related work (Section 3) where automatic classifiers have been used to label the connectives with a certain error rate.", "labels": [], "entities": []}, {"text": "Furthermore, we can use the sense labels for 100 types of EN connectives, whereas related work only focused on a few highly ambiguous connectives that are especially problematic for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 182, "end_pos": 193, "type": "TASK", "confidence": 0.9632773399353027}]}, {"text": "The paper starts by illustrating difficult translations involving connectives (Section 2) and discusses related work in Section 3.", "labels": [], "entities": []}, {"text": "The resources and data used are introduced in Section 4.", "labels": [], "entities": []}, {"text": "The MT experiments are explained in Section 5 and automatic evaluation is given in Section 6.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9882441759109497}]}, {"text": "We further provide a detailed manual evaluation and error analysis for the CZ translations generated by our SMT systems (Section 7).", "labels": [], "entities": [{"text": "error", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.952282190322876}, {"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9736281633377075}]}, {"text": "Future work described in Section 8 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following, we describe a series of SMT experiments that made direct use of the EN/CZ text as provided with the PCEDT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9947472214698792}, {"text": "EN/CZ text", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.6270327866077423}, {"text": "PCEDT", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.9746808409690857}]}, {"text": "The SMT models were all phrase-based and trained with the Moses decoder ( , either on plain text for the BASELINE or on text where the EN connective word-forms have been concatenated with the PDTB sense labels.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9884017109870911}, {"text": "BASELINE", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.708695113658905}]}, {"text": "All texts have been tokenized and lowercased with the Moses tools before training SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.971916913986206}]}, {"text": "In future work, we will build factored translation models (  as well, as this would reduce the label scarcity that was likely a problem when just concatenating word-forms and labels (see Sections 7 and 8).", "labels": [], "entities": []}, {"text": "For SYSTEM1 in the following, we inserted, into the English side of the PCEDT data, the full sense labels from the PDTB, which can be, as already mentioned, as detailed as containing 3 sense levels and allowing for composite tags (where annotators chose that two senses hold at the same time).", "labels": [], "entities": [{"text": "PCEDT data", "start_pos": 72, "end_pos": 82, "type": "DATASET", "confidence": 0.951388031244278}, {"text": "PDTB", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.9386385679244995}]}, {"text": "SYS-TEM1 therefore operates on a total of 63 distinct and observed sense tags for all DCs.", "labels": [], "entities": []}, {"text": "For SYSTEM2, we reduced the sense labels to contain only senses from PDTB sense hierarchy level 2 and 1, not allowing for composite senses, i.e. for those instances that were annotated with two senses we discarded the secondary (but not less important) sense.", "labels": [], "entities": []}, {"text": "This reduced the set of senses for SYSTEM2 to 22.", "labels": [], "entities": []}, {"text": "The procedure is exemplified in the example below with an EN sentence 1 (WSJ section 2300) containing a complex PDTB sense tag that has been kept for SYSTEM1.", "labels": [], "entities": [{"text": "WSJ section 2300)", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.8591952621936798}]}, {"text": "For SYS-TEM2 we have reduced the sense of when to: <CONTINGENCYCONDITIONGENERAL>.", "labels": [], "entities": [{"text": "sense of when", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9202475746472677}, {"text": "CONTINGENCYCONDITIONGENERAL", "start_pos": 52, "end_pos": 79, "type": "METRIC", "confidence": 0.9873887300491333}]}, {"text": "Sentence 2 (WSJ section 2341) contains two already simplified sense tags.", "labels": [], "entities": [{"text": "WSJ section 2341)", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.8545052111148834}]}, {"text": "The original PDTB sense tags for meanwhile and as were respectively <COMPARISONCONTRASTJUXTAPOSITION> and <CONTINGENCYPRAGMATICCAUSE-JUSTIFICATION>, where JUXTAPOSITION and JUSTIFICATION were dropped because they stem from the third level of the PDTB sense hierarchy:  Most automatic MT scoring relies on n-gram matching of a system's candidate translation against (usually) only one human reference translation.", "labels": [], "entities": [{"text": "MT scoring", "start_pos": 284, "end_pos": 294, "type": "TASK", "confidence": 0.9900479912757874}]}, {"text": "For DCs therefore, automatic scores do not reveal much of a system's performance, as often only one or two words, i.e. the DC is changed.", "labels": [], "entities": []}, {"text": "When a candidate translation however contains a more accurate and correct connective, the translation output is often more coherent and readable than the baseline's output, see Section 7.", "labels": [], "entities": []}, {"text": "Automatic evaluation has been done using the MultEval tool, version 0.5.1).", "labels": [], "entities": []}, {"text": "The BLEU scores are computed by jBLEU V0.1.1 (an exact reimplementation of NIST's mtevalv13.pl without tokenization).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9984219074249268}, {"text": "jBLEU V0.1.1", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.5077688992023468}, {"text": "NIST's mtevalv13.pl", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.8722975850105286}]}, {"text": "provides an overview of the BLEU scores for the BASELINE and systems 1 and 2 on the full test set (newstest2012 + PDTB section 23), and on PDTB section 23 only, the latter containing 2,416 sentences and 923 labeled DCs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9985646605491638}, {"text": "BASELINE", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.4751995801925659}, {"text": "newstest2012", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.9364995956420898}, {"text": "PDTB section 23", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.8635069926579794}, {"text": "PDTB section 23", "start_pos": 139, "end_pos": 154, "type": "DATASET", "confidence": 0.9262265761693319}]}, {"text": "In order to gain reliable automatic evaluation scores, we executed 5 runs of MERT for each translation model configuration.", "labels": [], "entities": []}, {"text": "MERT is implemented as a randomized, non-deterministic optimization process, so that each run leads to different feature weights and as a consequence, to different BLEU scores when translating unseen text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9986395239830017}]}, {"text": "The scores from the 5 runs were then averaged and with a t-test we calculated the confidence p-values for the score differences.", "labels": [], "entities": []}, {"text": "When these are below 0.05, they confirm that it is statistically likely, that such scores would occur again in other tuning runs.", "labels": [], "entities": []}, {"text": "In terms of BLEU, neither SYSTEM1 nor SYSTEM2 therefore performs significantly better or worse than the In order to show how little the DC labeling actually affects the BLEU score, we randomized all connective sense tags in PDTB test section 23 and translated again 5 times (with the weights from each tuning run) with both, SYSTEM1 and SYS-TEM2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9978945851325989}, {"text": "BLEU score", "start_pos": 169, "end_pos": 179, "type": "METRIC", "confidence": 0.9751895666122437}, {"text": "PDTB test section 23", "start_pos": 224, "end_pos": 244, "type": "DATASET", "confidence": 0.9325180798768997}]}, {"text": "With randomized labels, both systems perform statistically significantly worse (p = 0.01, marked with a star in 20.8*: BLEU scores when testing on the combined test set (newstest2012 + PDTB 23); on PDTB section 23 only (2416 sentences, 923 connectives); and when randomizing the sense tags (PDTB 23 random), for the BASELINE system and the two systems using PDTB connective labels: SYSTEM1: complex labels, SYSTEM2: simplified labels.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9995266199111938}, {"text": "BASELINE", "start_pos": 316, "end_pos": 324, "type": "DATASET", "confidence": 0.70490562915802}]}, {"text": "When testing on randomized sense labels (PDTB 23 random), the BLEU scores are statistically significantly lower than the ones on the correctly labeled test set (PDTB 23), which is indicated by starred values.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9992939233779907}]}, {"text": "Automatic MT scoring does therefore not reveal actual changes in translation quality due to DC usage.", "labels": [], "entities": [{"text": "MT scoring", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9753526747226715}]}, {"text": "In the next section, we manually analyze samples of the translation output by SYSTEM2 that reached the highest scores observed in some of the single tuning runs before averaging.", "labels": [], "entities": []}, {"text": "Two human judges went both through two random samples of SYSTEM2 translations from WSJ section 23, namely sentences 1-300 and 1000-2416.", "labels": [], "entities": [{"text": "WSJ section 23", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.8111385901769003}]}, {"text": "In these sentences, there were 630 observed connectives.", "labels": [], "entities": []}, {"text": "The judges counted the translations that were better, equal and worse in terms of the DCs as output by SYSTEM2 versus the BASELINE system.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.7794814109802246}]}, {"text": "We then summarized the counts over the two samples and give the scores as \u2206(%) in.", "labels": [], "entities": []}, {"text": "To further test if we just had bad samples, the judges went through another set of translations, containing 50 DCs, for which the counts are summarized in as well.", "labels": [], "entities": []}, {"text": "A translation was counted as being correct when it generated a valid CZ connective for the corresponding context, without grading the rest of the sentences.", "labels": [], "entities": []}, {"text": "Overall, it was found that the number of better translations is only slightly higher for SYSTEM2 than the ones from the BASELINE system.", "labels": [], "entities": [{"text": "SYSTEM2", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.7196953296661377}, {"text": "BASELINE system", "start_pos": 120, "end_pos": 135, "type": "DATASET", "confidence": 0.8212088048458099}]}, {"text": "The vast majority of DCs was translated correctly by both the BASELINE and SYSTEM2, and in very few cases, both systems translated the DCs incorrectly.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.8133264183998108}]}, {"text": "SYSTEM2 appeared to systematically repeat one mistake, namely translating the very frequent connective but preferably with jen\u017ee, which is correct but rare in CZ (the primary and default equivalent for but in CZ is ale).", "labels": [], "entities": []}, {"text": "This 'mis-learning' likely happened to a frequent correspondence of butjen\u017ee in the SMT training data, which then does not necessarily scale to and be of appropriate style in the testing data.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.8812532126903534}]}, {"text": "If one disregards these occurrences, translates between about 8 and 20% of all connectives better than the BASELINE (discounted percentages for jen\u017ee in).", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.999566376209259}]}, {"text": "The results seem therefore to be dependent on the parts of the test set evaluated and the DCs occurring in them.", "labels": [], "entities": []}, {"text": "The only slight quantitative improvements and cases were SYSTEM2 performed worse are most likely due to the overall scarcity of the PDTB sense tags (cf. Section 4).", "labels": [], "entities": [{"text": "PDTB sense tags", "start_pos": 132, "end_pos": 147, "type": "DATASET", "confidence": 0.7883232434590658}]}, {"text": "Especially for SYS-TEM1 but to some extent also for SYSTEM2, rare sense tags such as CONTINGENCYPRAGMATIC-CAUSE might not be seen often or even not at all in the SMT training data and therefore not be learned appropriately to provide good translations for the test data.", "labels": [], "entities": [{"text": "CONTINGENCYPRAGMATIC-CAUSE", "start_pos": 85, "end_pos": 111, "type": "METRIC", "confidence": 0.9491540193557739}, {"text": "SMT training", "start_pos": 162, "end_pos": 174, "type": "TASK", "confidence": 0.8908848166465759}]}, {"text": "In relation to that, simply concatenating the sense tags onto the connective word-forms leads to scarcity of the latter, whereas other ways to include linguistic labels in SMT, such as factored translation models, would account for the labels as additional translation features, which will be investigated in future work (Section 8).", "labels": [], "entities": [{"text": "SMT", "start_pos": 172, "end_pos": 175, "type": "TASK", "confidence": 0.9822852611541748}]}, {"text": "In the following, we analyze cases where SYS-TEM2 translates the connectives better and more appropriately than the BASELINE.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9540767073631287}]}, {"text": "These cases include highly ambiguous connectives, temporal DCs with verbal ing-forms and conditionals.", "labels": [], "entities": []}, {"text": "In general, for the very ambiguous EN connectives (e.g. as, when, while), disambiguated for SYSTEM2 with the PDTB sense tags, we indeed obtained more accurate translations than those generated by the BASELINE.", "labels": [], "entities": [{"text": "PDTB sense tags", "start_pos": 109, "end_pos": 124, "type": "DATASET", "confidence": 0.867254356543223}, {"text": "BASELINE", "start_pos": 200, "end_pos": 208, "type": "DATASET", "confidence": 0.5795029997825623}]}, {"text": "One of the human judges had a close look at 25 randomly sampled instances of as, taken from the manually evaluated sets mentioned above.", "labels": [], "entities": []}, {"text": "In these test cases, 68% of all occurrences of as were better translated by and only 4% of the translations were degraded when compared to the BASELINE.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.8397202491760254}]}, {"text": "For details, see 4 . In the following translation example (WSJ section 2365), and often elsewhere, the BASELINE system treats the connective as as a preposition jako with the meaning She worked as a teacher.", "labels": [], "entities": [{"text": "WSJ section 2365", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.8919739127159119}, {"text": "BASELINE", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.7205206751823425}]}, {"text": "This frequent interpretation seems to be learned quite reasonably from the SMT training data, it is however incorrect whereas actually functions as a DC.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.8651869893074036}]}, {"text": "SYSTEM2, in agreement with the tagging, then correctly generates the causal connective proto\u017ee: SOURCE: In the occupied lands, underground leaders of the Arab uprising rejected a U.S. plan to arrange IsraeliPalestinian talks as<CONTINGENCYCAUSE> Shamir opposed holding such discussions in Cairo.", "labels": [], "entities": []}, {"text": "BASELINE: *Na okupovan\u00b4ych\u00fazem\u00edchokupovan\u00b4ychokupovan\u00b4ych\u00b4okupovan\u00b4ych\u00fazem\u00edch, podzemn\u00ed v\u016fdc\u016f arabsk\u00b4ycharabsk\u00b4ych povst\u00e1n\u00ed odm\u00edtl americk\u00b4yamerick\u00b4y pl\u00e1n uspo\u0159\u00e1dat izraelsko-palestinsk\u00e9 rozhovory jako\u0160amirajako\u02c7jako\u0160amira proti po\u0159\u00e1d\u00e1n\u00ed takov\u00b4ychtakov\u00b4ych diskus\u00ed v K\u00e1hi\u0159e.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.888466477394104}]}, {"text": "SYSTEM2: Na okupovan\u00b4ych\u00fazem\u00edchokupovan\u00b4ychokupovan\u00b4ych\u00b4okupovan\u00b4ych\u00fazem\u00edch, podzemn\u00ed v\u016fdc\u016f arabsk\u00e9ho povst\u00e1n\u00ed odm\u00edtl americk\u00b4yamerick\u00b4y pl\u00e1n uspo\u0159\u00e1dat izraelsko-palestinsk\u00e9 rozhovory, proto\u017e\u011b Samira proti po\u0159\u00e1d\u00e1n\u00ed takov\u00b4ychtakov\u00b4ych diskus\u00ed v K\u00e1hi\u0159e.", "labels": [], "entities": []}, {"text": "DCs can also be translated to other syntactical constructs available in the target language that convey the same discourse relation without any  In CZ, these either should be rendered as a verbal clause or a nominalization.", "labels": [], "entities": []}, {"text": "We accounted for translations as being well-formed, if the SMT systems generated one of these possibilities correctly, i.e. not only the connective/preposition but also the verb/noun.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9710507392883301}]}, {"text": "In CZ, it must be decided between using a preposition (e.g. p\u0159ed) or a connective (e.g. ne\u017e).", "labels": [], "entities": []}, {"text": "A good translation would for example be: before climbing = PREP+NP or DC+V, and a bad translation: before climbing = PREP+V/ADJ or DC+NP.", "labels": [], "entities": []}, {"text": "The following example (WSJ section 2381) is a SYSTEM2 output where the sense tag in English helped to translate the connective before more correctly by DC+V, whereas the BASELINE renders this wrongly by using PREP+ADJ: Mr. Weisman predicts stocks will appear to stabilize in the next few days before<TEMPORALASYNCHRONOUS> declining again, trapping more investors.", "labels": [], "entities": [{"text": "WSJ section 2381", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.8056439161300659}, {"text": "BASELINE", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9293978214263916}]}, {"text": "BASELINE: *Pan Weisman p\u0159edpov\u00edd\u00e1, \u02c7 ze akcie budou stabilizovat v p\u0159\u00ed\u0161t\u00edch n\u011bkolika dnech p\u0159ed/PREP klesaj\u00edc\u00edm/ADJ op\u011bt odchytu v\u00edce investor\u016f.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9773743748664856}]}, {"text": "SYSTEM2: Pan Weisman p\u0159edpov\u00edd\u00e1, \u02c7 ze akcie bude stabilizovat, jak se zd\u00e1, v p\u0159\u00ed\u0161t\u00edch n\u011bkolika dn\u00ed, ne\u017e/DC op\u011bt klesat/V, zablokov\u00e1n\u00ed v\u00edce investor\u016f.", "labels": [], "entities": []}, {"text": "A further difficult casein CZ is the binding of conditionals with personal pronouns, e.g. if I = kdybych, if you = kdybys, if he/she = kdyby etc.", "labels": [], "entities": []}, {"text": "In the following example (WSJ section 2386), the BASELINE system completely missed to render the personal pronoun (but still generated the correct conditional connective if-pokud), whereas SYSTEM2 outputs the much better if I-kdybych.", "labels": [], "entities": [{"text": "WSJ section 2386", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.7841816345850626}, {"text": "BASELINE", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9382396340370178}]}, {"text": "However, apart from the better connective, SYS-TEM2's translation is worse than the BASELINE's, because the first verb form is misconjugated and the second verb (will take) is missing: From the automatic and manual translation evaluation, we conclude that using the sense tags for all 100 connectives in EN is not the most appropriate method, and that only certain connectives such as as, when, while, yet and a few others are very problematic in translation due to the many discourse relations they can signal.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9424043893814087}]}, {"text": "In future work, we will therefore analyze in more detail which connectives and which sense labels from the PDTB should actually be included in the data to train SMT.: Translation outputs for the EN connective as, which was translated more correctly by SYSTEM2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as -jako.", "labels": [], "entities": [{"text": "SMT.", "start_pos": 161, "end_pos": 165, "type": "TASK", "confidence": 0.9910193085670471}, {"text": "BASELINE", "start_pos": 316, "end_pos": 324, "type": "METRIC", "confidence": 0.8841403722763062}]}, {"text": "The erroneous translations are marked in bold.", "labels": [], "entities": []}, {"text": "The PDTB sense tags indicate the meaning of the CZ translations and are encoded as follows: Synchrony (Sy), Asynchrony (Asy), Contingency (Co), Cause (Ca).", "labels": [], "entities": [{"text": "PDTB sense tags", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8041661977767944}, {"text": "Cause (Ca)", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.91456438601017}]}], "tableCaptions": [{"text": " Table 2: Performance of SYSTEM2 (simplified PDTB tags) when manually counting for improved, equal  and degraded translations compared to the BASELINE, in samples from the PDTB section 23 test set.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9234445095062256}, {"text": "PDTB section 23 test set", "start_pos": 172, "end_pos": 196, "type": "DATASET", "confidence": 0.8889614224433899}]}, {"text": " Table 3: Translation outputs for the EN con- nective as, which was translated more correctly  by SYSTEM2 thanks to the disambiguating sense  tags compared to the BASELINE that often just  produces the prepositional as -jako. The erro- neous translations are marked in bold. The PDTB  sense tags indicate the meaning of the CZ trans- lations and are encoded as follows: Synchrony  (Sy), Asynchrony (Asy), Contingency (Co), Cause  (Ca).", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9652639627456665}, {"text": "Cause  (Ca)", "start_pos": 423, "end_pos": 434, "type": "METRIC", "confidence": 0.9062195122241974}]}]}