{"title": [{"text": "Better Word Representations with Recursive Neural Networks for Morphology", "labels": [], "entities": [{"text": "Word Representations", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.689080998301506}]}], "abstractContent": [{"text": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks.", "labels": [], "entities": [{"text": "Vector-space word representations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7817753950754801}]}, {"text": "However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled.", "labels": [], "entities": []}, {"text": "As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors.", "labels": [], "entities": []}, {"text": "This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes.", "labels": [], "entities": []}, {"text": "We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologically-aware word representations.", "labels": [], "entities": []}, {"text": "Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including anew dataset we introduce focused on rare words to complement existing ones in an interesting way.", "labels": [], "entities": []}], "introductionContent": [{"text": "The use of word representations or word clusters pretrained in an unsupervised fashion from lots of text has become a key \"secret sauce\" for the success of many NLP systems in recent years, across tasks including named entity recognition, part-ofspeech tagging, parsing, and semantic role labeling.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 213, "end_pos": 237, "type": "TASK", "confidence": 0.620326985915502}, {"text": "part-ofspeech tagging", "start_pos": 239, "end_pos": 260, "type": "TASK", "confidence": 0.7710902094841003}, {"text": "parsing", "start_pos": 262, "end_pos": 269, "type": "TASK", "confidence": 0.9365226030349731}, {"text": "semantic role labeling", "start_pos": 275, "end_pos": 297, "type": "TASK", "confidence": 0.6042424341042837}]}, {"text": "This is particularly true in deep neural network models ), but it is also true in conventional feature-based models ().", "labels": [], "entities": []}, {"text": "Deep learning systems give each word a distributed representation, i.e., a dense lowdimensional real-valued vector or an embedding.", "labels": [], "entities": []}, {"text": "The main advantage of having such a distributed representation over word classes is that it can capture various dimensions of both semantic and syntactic information in a vector where each dimension corresponds to a latent feature of the word.", "labels": [], "entities": []}, {"text": "As a result, a distributed representation is compact, less susceptible to data sparsity, and can implicitly represent an exponential number of word clusters.", "labels": [], "entities": []}, {"text": "However, despite the widespread use of word clusters and word embeddings, and despite much work on improving the learning of word representations, from feed-forward networks () to hierarchical models and recently recurrent neural networks (), these approaches treat each full-form word as an independent entity and fail to capture the explicit relationship among morphological variants of a word.", "labels": [], "entities": []}, {"text": "The fact that morphologically complex words are often rare exacerbates the problem.", "labels": [], "entities": []}, {"text": "Though existing clusterings and embeddings represent well frequent words, such as \"distinct\", they often badly model rare ones, such as \"distinctiveness\".", "labels": [], "entities": []}, {"text": "In this work, we use recursive neural networks (, in a novel way to model morphology and its compositionality.", "labels": [], "entities": []}, {"text": "Essentially, we treat each morpheme as a basic unit in the RNNs and construct representations for morphologically complex words on the fly from their morphemes.", "labels": [], "entities": []}, {"text": "By training a neural language model (NLM) and integrating RNN structures for complex words, we utilize contextual information in an interesting way to learn morphemic semantics and their compositional properties.", "labels": [], "entities": []}, {"text": "Our model has the capability of building representations for any new unseen word comprised of known morphemes, giving the model an infinite (if still incomplete) covered vocabulary.", "labels": [], "entities": []}, {"text": "Our learned representations outperform publicly available embeddings by a good margin on word similarity tasks across many datasets, which include our newly released dataset focusing on rare words (see Section 5).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.72074756026268}]}, {"text": "The detailed analysis in Section 6 reveals that our models can blend well syntactic information, i.e., the word structure, and the semantics in grouping related words.", "labels": [], "entities": []}], "datasetContent": [{"text": "As our focus is in learning morphemic semantics, we do not start training from scratch, but rather, initialize our models with existing word representations.", "labels": [], "entities": [{"text": "learning morphemic semantics", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.6497671604156494}]}, {"text": "In our experiments, we make use of two publicly-available embeddings (50-dimensional) provided by ) (denoted as C&W)  do not consider the global sentence-level context information.", "labels": [], "entities": []}, {"text": "It is worth to note that these aspects of the HSMN embedding -incorporating global context and maintaining multiple prototypes -are orthogonal to our approach, which would be interesting to investigate in future work.", "labels": [], "entities": []}, {"text": "For the context-sensitive morphoRNN model, we follow to use the April 2010 snapshot of the Wikipedia corpus.", "labels": [], "entities": [{"text": "April 2010 snapshot of the Wikipedia corpus", "start_pos": 64, "end_pos": 107, "type": "DATASET", "confidence": 0.7910175153187343}]}, {"text": "All paragraphs containing non-roman characters are removed while the remaining text are lowercased and then tokenized.", "labels": [], "entities": []}, {"text": "The resulting clean corpus contains about 986 million tokens.", "labels": [], "entities": []}, {"text": "Each digit is then mapped into 0, i.e. 2013 will become 0000.", "labels": [], "entities": []}, {"text": "Other rare words not in the vocabularies of C&W and HSMN are mapped to an UNKNOWN token, and we use <s> and </s> for padding tokens representing the beginning and end of each sentence.", "labels": [], "entities": [{"text": "C&W", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8664013544718424}]}, {"text": "Follow (Huang et al., 2012)'s implementation, which our code is based on initially, we use 50-dimensional vectors to represent morphemic and word embeddings.", "labels": [], "entities": []}, {"text": "For cimRNN, the regularization weight \u03bb is set to 10 \u22122 . For csmRNN, we use 10-word windows of text as the local context, 100 hidden units, and no weight regularization.", "labels": [], "entities": []}, {"text": "As evidenced in, most existing word similarity datasets contain frequent words and few of them possesses enough rare or morphologically complex words that we could really attest the expressiveness of our morphoRNN models.", "labels": [], "entities": []}, {"text": "In fact, we believe a good embedding in general should be able to learn useful representations for not just frequent words but also rare ones.", "labels": [], "entities": []}, {"text": "That motivates us to construct another dataset focusing on rare words to complement existing ones.", "labels": [], "entities": []}, {"text": "Our dataset construction proceeds in three stages: (1) select a list of rare words, (2) for each of the rare words, find another word (not necessarily rare) to form a pair, and (3) collect human judgments on how similar each pair is.", "labels": [], "entities": []}, {"text": "( (100, 1000] ununtracked unrolls undissolved unrehearsed unflagging unfavourable unprecedented unmarried uncomfortable -al apocalyptical traversals bestowals acoustical extensional organismal directional diagonal spherical -ment obtainment acquirement retrenchments discernment revetment rearrangements confinement establishment management word 1 untracked unflagging unprecedented apocalyptical organismal diagonal obtainment discernment confinement word 2 inaccessible constant new prophetic system line acquiring knowing restraint: Rare words (top) -word 1 by affixes and frequencies and sample word pairs (bottom).", "labels": [], "entities": [{"text": "acoustical extensional organismal directional diagonal spherical -ment obtainment acquirement retrenchments discernment revetment rearrangements confinement establishment management word 1 untracked unflagging unprecedented apocalyptical organismal diagonal obtainment discernment confinement word", "start_pos": 159, "end_pos": 456, "type": "TASK", "confidence": 0.7647331983878695}]}, {"text": "Rare word selection: our choices of rare words (word 1 ) are based on their frequencies -based on five bins,,,, and the affixes they possess.", "labels": [], "entities": [{"text": "Rare word selection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6602781017621359}]}, {"text": "To create a diverse set of candidates, we randomly select 15 words for each configuration (a frequency bin, an affix).", "labels": [], "entities": []}, {"text": "At the scale of Wikipedia, a word with frequency of 1-5 is most likely a junk word, and even restricted to words with frequencies above five, there are still many non-English words.", "labels": [], "entities": []}, {"text": "To counter such problems, each word selected is required to have a non-zero number of synsets in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9732995629310608}]}, {"text": "(top) gives examples of rare words selected and organized by frequencies and affixes.", "labels": [], "entities": []}, {"text": "It is interesting to find out that words like obtainment and acquirement are extremely rare (not in traditional dictionaries) but are perfectly understandable.", "labels": [], "entities": []}, {"text": "We also have less frequent words like revetment from French or organismal from biology.", "labels": [], "entities": []}, {"text": "Pair construction: following (Huang et al., 2012), we create pairs with interesting relationships for each word 1 as follow.", "labels": [], "entities": []}, {"text": "First, a WordNet synset of word 1 is randomly selected, and we construct a set of candidates which connect to that synset through various relations, e.g., hypernyms, hyponyms, holonyms, meronyms, and attributes.", "labels": [], "entities": []}, {"text": "A word 2 is then randomly selected from these candidates, and the process is repeated another time to generate a total of two pairs for each word 1 . Sample word pairs are given in in which word 2 includes mostly frequent words, implying a balance of words in terms of frequencies in our dataset.", "labels": [], "entities": []}, {"text": "We collected 3145 pairs after this stage Human judgment: we use Amazon Mechanical Turk to collect 10 human similarity ratings on a scale of per word pair.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.9565328160921732}]}, {"text": "Such procedure has been demonstrated by in replicating ratings for the MC dataset, achieving close inter-annotator agreement with expert raters.", "labels": [], "entities": [{"text": "replicating", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.967420220375061}, {"text": "MC dataset", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.7893244624137878}]}, {"text": "Since our pairs contain many rare words which are challenging even to native speakers, we ask raters to indicate for each pair if they do not know the first word, the second word, or both.", "labels": [], "entities": []}, {"text": "We use such information to collect reliable ratings by either discard pairs which many people do not know or collect additional ratings to ensure we have 10 ratings per pair.", "labels": [], "entities": []}, {"text": "As a result, only 2034 pairs are retained.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Word similarity datasets and their  statistics: number of pairs/raters/type counts as  well as rating scales. The number of complex  words are shown as well (both type and token  counts). RW denotes our new rare word dataset.", "labels": [], "entities": []}, {"text": " Table 4: Word distribution by frequencies -dis- tinct words in each dataset are grouped based on  frequencies and counts are reported for the fol- lowing bins : unknown | [1, 100] / [101, 1000] /  [1001, 10000] /", "labels": [], "entities": []}, {"text": " Table 6: Nearest neighbors. We show morphologically related words and their closest words in different  representations (\"unaffect\" is a pseudo-word; \u2205 marks no results due to unknown words).", "labels": [], "entities": []}, {"text": " Table 7: Word similarity task -shown are Spear- man's rank correlation coefficient (\u03c1 \u00d7 100) be- tween similarity scores assigned by neural lan- guage models and by human annotators. stem in- dicates baseline systems in which unknown words  are represented by their stem vectors. cimRNN and  csmRNN refer to our context insensitive and sensi- tive morphological RNNs respectively.", "labels": [], "entities": [{"text": "Spear- man's rank correlation coefficient", "start_pos": 42, "end_pos": 83, "type": "METRIC", "confidence": 0.5944238901138306}]}]}