{"title": [{"text": "Inter-annotator Agreement for Dependency Annotation of Learner Language", "labels": [], "entities": [{"text": "Dependency Annotation of Learner Language", "start_pos": 30, "end_pos": 71, "type": "TASK", "confidence": 0.8360082745552063}]}], "abstractContent": [{"text": "This paper reports on a study of inter-annotator agreement (IAA) fora dependency annotation scheme designed for learner En-glish.", "labels": [], "entities": []}, {"text": "Reliably-annotated learner corpora area necessary step for the development of POS tagging and parsing of learner language.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.8830834031105042}]}, {"text": "In our study, three annotators marked several layers of annotation over different levels of learner texts, and they were able to obtain generally high agreement, especially after discussing the disagreements among themselves, without researcher intervention, illustrating the feasibility of the scheme.", "labels": [], "entities": []}, {"text": "We pinpoint some of the problems in obtaining full agreement , including annotation scheme vagueness for certain learner innovations, interface design issues, and difficult syntactic constructions.", "labels": [], "entities": []}, {"text": "In the process, we also develop ways to calculate agreements for sets of dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learner corpora have been essential for developing error correction systems and intelligent tutoring systems (e.g.,.", "labels": [], "entities": [{"text": "error correction", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.6397268027067184}]}, {"text": "So far, error annotation has been the main focus, to the exclusion of corpora and annotation for more basic NLP development, despite the need for parse information for error detection, learner proficiency identification, and acquisition research (.", "labels": [], "entities": [{"text": "error detection", "start_pos": 168, "end_pos": 183, "type": "TASK", "confidence": 0.7106914967298508}, {"text": "learner proficiency identification", "start_pos": 185, "end_pos": 219, "type": "TASK", "confidence": 0.6827561457951864}]}, {"text": "Indeed, there is very little work on POS tagging or parsing ( learner language, and, not coincidentally, there is alack of annotated data and standards for these tasks.", "labels": [], "entities": [{"text": "POS tagging or parsing", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7784567922353745}]}, {"text": "One issue is in knowing how to handle innovative learner forms: some map to a target form before annotating syntax (e.g.,, while others propose directly annotating the text (e.g.,).", "labels": [], "entities": []}, {"text": "We follow this latter strand and further our work towards a syntactically-annotated corpus of learner English by: a) presenting an annotation scheme for dependencies, integrated with other annotation layers, and b) testing the inter-annotator agreement for this scheme.", "labels": [], "entities": []}, {"text": "Despite concerns that direct annotation of the linguistic properties of learners may not be feasible (e.g.,, we find that annotators have generally strong agreement, especially after adjudication, and the reasons for disagreement often have as much to do with the complexities of syntax or interface issues as they do with learner innovations.", "labels": [], "entities": []}, {"text": "Probing grammatical annotation can lead to advancements in research on POS tagging and syntactic parsing of learner language, for it shows what can be annotated reliably and what needs additional diagnostics.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9009598195552826}, {"text": "syntactic parsing of learner language", "start_pos": 87, "end_pos": 124, "type": "TASK", "confidence": 0.8270818710327148}]}, {"text": "We specifically report on inter-annotator agreement (IAA) for the annotation scheme described in section 2, focusing on dependency annotation.", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA", "start_pos": 26, "end_pos": 56, "type": "METRIC", "confidence": 0.8033161163330078}]}, {"text": "There are numerous studies investigating inter-annotator agreement between coders for different types of grammatical annotation schemes, focusing on part-of-speech, syntactic, or semantic annotation (e.g.,;).", "labels": [], "entities": []}, {"text": "For learner language, a number of error annotation projects include measures of interannotator agreement, (see, e.g.,), but as far as we are aware, there have been no studies on IAA for grammatical annotation.", "labels": [], "entities": []}, {"text": "We have conducted an IAA study to investigate the quality and robustness of our annotation scheme, as reported in section 3.", "labels": [], "entities": [{"text": "IAA", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.4330178201198578}]}, {"text": "In section 4, we report quantitative results and a qualitative analysis of this study to tease apart disagreements due to inherent ambiguity or text difficulty from those due to the annotation scheme and/or the guidelines.", "labels": [], "entities": []}, {"text": "The study has already reaped benefits by helping us to revise our annotation scheme and guidelines, and the insights gained here should be applicable for future development of other annotation schemes and to parsing studies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 208, "end_pos": 215, "type": "TASK", "confidence": 0.9728833436965942}]}, {"text": "On a final note, our dependency annotation allows for multiple heads for each token in the corpus, violating the so-called single-head constraint).", "labels": [], "entities": []}, {"text": "In the process of evaluating these dependencies (see section 4.1), we also make some minor contributions towards comparing sets of dependencies, moving beyond just F-measure (e.g., to account for partial agreements.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9600843191146851}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Annotation time, in minutes, for phase 1 (*times  for two sentences were not reported and are omitted)", "labels": [], "entities": [{"text": "Annotation time", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9673022329807281}]}, {"text": " Table 2: Overview of agreement rates before & after discussion (phases 2 & 4)", "labels": [], "entities": []}, {"text": " Table 3: MASI percentages for dependencies, Text 1", "labels": [], "entities": [{"text": "MASI percentages", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9087368249893188}]}, {"text": " Table 4: MASI percentages for dependencies, Text 2", "labels": [], "entities": [{"text": "MASI percentages", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9181548953056335}]}, {"text": " Table 5: Agreement rates for subcategorization, Text 1", "labels": [], "entities": []}, {"text": " Table 6: Agreement rates for subcategorization, Text 2", "labels": [], "entities": []}]}