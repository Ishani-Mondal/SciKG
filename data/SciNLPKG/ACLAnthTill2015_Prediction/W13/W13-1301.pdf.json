{"title": [{"text": "Annotation of Online Shopping Images without Labeled Training Examples", "labels": [], "entities": []}], "abstractContent": [{"text": "We are interested in the task of image annotation using noisy natural text as training data.", "labels": [], "entities": [{"text": "image annotation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7599650919437408}]}, {"text": "An image and its caption convey different information, but are generated by the same underlying concepts.", "labels": [], "entities": []}, {"text": "In this paper, we learn latent mixtures of topics that generate image and product descriptions on shopping websites by adapting a topic model for multilingual data (Mimno et al., 2009).", "labels": [], "entities": []}, {"text": "We use the trained model to annotate test images without corresponding text.", "labels": [], "entities": []}, {"text": "We capture visual properties such as color, texture, shape, and orientation by computing low-level image features , and measure the contribution of each type of visual feature towards the accuracy of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9985042810440063}]}, {"text": "Our model significantly outper-forms both a competitive baseline and a previous topic model-based system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Image annotation is a classic problem in Computer Vision.", "labels": [], "entities": [{"text": "Image annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.812497615814209}, {"text": "Computer Vision", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7904208898544312}]}, {"text": "Given a query image, the task is to generate a set of textual labels that describe the visual content.", "labels": [], "entities": []}, {"text": "The typical approach to these problems is to use supervised models, which require large numbers of hand-annotated examples for each of the labels.", "labels": [], "entities": []}, {"text": "However, the amount of information available on the web continues to grow, the task of organizing and describing visual data becomes increasingly complex.", "labels": [], "entities": []}, {"text": "For example, a shopping website might arrange products into broad categories such as \"shoes\" and \"handbags\" with each category containing tens of thousands of products that are difficult for users to search and navigate.", "labels": [], "entities": []}, {"text": "It is often infeasible to discover all of the attributes within those categories that are relevant to users and create labeled training examples for each of them.", "labels": [], "entities": []}, {"text": "Instead, we approach this problem by discovering visual attributes from noisy natural language captions.", "labels": [], "entities": []}, {"text": "That is, given a collection of images and captions found on the web, we learn a model of visual and textual features.", "labels": [], "entities": []}, {"text": "Then given a query image with no text, we can generate likely descriptive words.", "labels": [], "entities": []}, {"text": "This is a difficult task because image captions on the web are often noisy and incomplete: some captions might not describe a particular visual feature, might use a synonym for that feature, or might describe information that is not visual in the image at all.", "labels": [], "entities": []}, {"text": "A secondary motivation for this work is to use the image annotations as a component in language generation systems such as for automatic image captioning.", "labels": [], "entities": [{"text": "automatic image captioning", "start_pos": 127, "end_pos": 153, "type": "TASK", "confidence": 0.6060791611671448}]}, {"text": "We point to examples of previous work such as where image annotations generated from a topic model are used to help generate full sentences to describe images.", "labels": [], "entities": []}, {"text": "Much of the current research in image captioning is limited by the current technology for object recognition in Computer Vision.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7419087141752243}, {"text": "object recognition", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7613635063171387}]}, {"text": "For example, SBU-Flickr dataset () with 1 million images and captions, is considered to be general-domain but is actually built by querying Flickr using a pre-defined term list related to visual attributes that there are trained recognition systems for.", "labels": [], "entities": [{"text": "SBU-Flickr dataset", "start_pos": 13, "end_pos": 31, "type": "DATASET", "confidence": 0.800402045249939}]}, {"text": "While these systems can accurately generate descriptions for common visual objects and attributes, they are not as well-suited for describing the \"long-tail\" of visual attributes which appear in many domain-specific", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we model image and text features from the training data using a generative model.", "labels": [], "entities": []}, {"text": "We adapt the Polylingual topic model from to train on multi-modal data, and then use the trained model to generate annotations for test images.", "labels": [], "entities": []}, {"text": "We evaluate our model on two categories of shopping images using a variety of types of computed image features.", "labels": [], "entities": []}, {"text": "For image annotation we outperform both a difficult baseline and previous work.", "labels": [], "entities": [{"text": "image annotation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8648069798946381}]}, {"text": "We use the Attribute Discovery Dataset from.", "labels": [], "entities": [{"text": "Attribute Discovery Dataset", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.6049206356207529}]}, {"text": "The dataset consists of pairs of images and captions taken from the shopping website like.com.", "labels": [], "entities": []}, {"text": "The data has four categories: women's shoes, handbags, earrings, and neckties.", "labels": [], "entities": []}, {"text": "We run our model on two categories, shoes and handbags, due to their larger sizes -14764 and 9145 image-caption pairs respectively -and diversity of features.", "labels": [], "entities": []}, {"text": "This is a reasonable amount of data in the shopping images domain; more than half of the number of comparable products sold on large retail websites such as Zappos.com or Amazon.com.", "labels": [], "entities": []}, {"text": "Compared to general datasets such as Pascal Sentences, the images in the Attribute Discovery Dataset are more uniform.", "labels": [], "entities": [{"text": "Attribute Discovery Dataset", "start_pos": 73, "end_pos": 100, "type": "DATASET", "confidence": 0.5971049070358276}]}, {"text": "All image files are 280x280 pixel JPEGs, and images of products are typically taken from similar angles against a white or a light-colored solid background.", "labels": [], "entities": []}, {"text": "Only rarely do the images have noisy backgrounds, such as a person wearing the item, or the same item displayed in multiple colors in one image.", "labels": [], "entities": []}, {"text": "However, this does not necessarily make our task much easier, since the visual attributes we wish to learn are not pre-defined as they are in a general-domain dataset.", "labels": [], "entities": []}, {"text": "And the lack of hand-annotated data means no negative examples of when an attribute is not present, which are typically used to train visual classifiers.", "labels": [], "entities": []}, {"text": "Furthermore, the captions are extremely noisy in this dataset.", "labels": [], "entities": []}, {"text": "Compared to the 20 object types in the Pascal Sentences dataset, or about one hundred in COREL, here there are thousands of words that can be used to describe features in the images, including synonyms, multiple stems of words, and misspellings.", "labels": [], "entities": [{"text": "Pascal Sentences dataset", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.6861409147580465}]}, {"text": "In addition to explicit visual descriptions of the products, the captions describe \"less visual\" features such as details about the construction of the item, during which season or activity it would be appropriate to wear, or feelings that could be evoked by looking at the item.", "labels": [], "entities": []}, {"text": "These features are difficult to represent as specific visual attributes, but can be identified visually by domain-experts.", "labels": [], "entities": []}, {"text": "Captions can also include information that is non-visual such as sizing and shipping information, or whether the item is on sale.", "labels": [], "entities": [{"text": "Captions", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9611440896987915}]}, {"text": "The captions can be either full English sentences, a list of features, or sometimes just a few words.", "labels": [], "entities": []}, {"text": "Longer captions in the dataset are truncated to 250 characters in length.", "labels": [], "entities": []}, {"text": "From our own obervations, we estimate about 10% of the captions in the shoes dataset contain few or no descriptive words.", "labels": [], "entities": [{"text": "shoes dataset", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.8395292162895203}]}, {"text": "At least 3.7% of the shoes captions are entirely Javascript code, have significant portions of code, or very long URLs.", "labels": [], "entities": []}, {"text": "Another 5-6% either contain no information besides sizing or shipping information, only the brand name or model number of the shoe, or the caption is so short that there are only one or two descriptive words that could be used in our model.", "labels": [], "entities": []}, {"text": "In the womens' shoes category, we take some simple steps to remove URLs and code to avoid learning accidental correlation with legitimate features.", "labels": [], "entities": []}, {"text": "However, we still use all image and caption pairs in the training set, including those which end up having empty captions, since they are still useful for learning topics for visual features.", "labels": [], "entities": []}, {"text": "For the handbags captions, we did not try to remove code or long URLs since it seemed to be less of a problem in that category.", "labels": [], "entities": []}, {"text": "We first run our model on the larger category, shoes.", "labels": [], "entities": []}, {"text": "For both systems and baselines, we find the 10, 15, and 20 most likely words for the test images.", "labels": [], "entities": []}, {"text": "We evaluate by computing precision and recall against descriptive words from the held-out captions for those images.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9995507597923279}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9993466734886169}]}, {"text": "We compute macro-averages of these scores because there is a lot of variation between the sizes of the captions in the dataset.", "labels": [], "entities": []}, {"text": "The split between training and test instances is 80/20%.", "labels": [], "entities": []}, {"text": "We also evaluate the contributions of different types of image features.", "labels": [], "entities": []}, {"text": "We evaluate the model for each image feature individually (along with the text features), as well as combinations of image features.", "labels": [], "entities": []}, {"text": "We compare against the MixLDA system and a strong baseline.", "labels": [], "entities": [{"text": "MixLDA", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.8970468640327454}]}, {"text": "We choose MixLDA because it is relatively easy to re-implement and because it  has previously outperformed other image annotation systems when trained on natural language captions.", "labels": [], "entities": []}, {"text": "Because the MixLDA model originally only used SIFT features, we compare it against the SIFTonly version of our model, with each system using the same computed image and text features.", "labels": [], "entities": []}, {"text": "We re-implement the MixLDA system mostly as it is described in, with a few changes to make it more comparable to our model: Obviously in our version of MixLDA the test instances are only the unseen image as there is no other surrounding text.", "labels": [], "entities": []}, {"text": "The number of topics is 200 (the original MixLDA had more but that did not seem to help here), and the \u03b1 and \u03b2 hyperparameters are optimized every 10 iterations.", "labels": [], "entities": [{"text": "MixLDA", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.929161012172699}]}, {"text": "We also compare our model against corpus frequency of words in the training set.", "labels": [], "entities": []}, {"text": "Although this may seem like a trivial baseline, previous work on image annotation from both computer vision) and natural language processing () has shown that a large portion of the keyword probability mass can often be accounted for by a very small number of words, allowing systems to game better-looking results by simply guessing the frequency distribution of the text vocabulary.", "labels": [], "entities": []}, {"text": "We find this to be especially true in the domain-specific case, where common terms (eg shoe, sole, heel, upper) are used in almost every caption, and in some captions account for most words used (such as the second example in).", "labels": [], "entities": []}, {"text": "While domain-frequent words are also needed for generating new captions, we don't want them to account for all of the words our system generates.", "labels": [], "entities": []}, {"text": "Of course, a human evaluation would be another possible way of addressing this issue, but it would be difficult and expensive to find enough people who have sufficient knowledge of womens' clothing and would be able to accurately say whether the generated words are appropriate or not (words such as hobo, PU, stacked, upper, and vamp).", "labels": [], "entities": []}, {"text": "Also, although the gold image captions are noisy, the num- Create a timeless look with these Andie dress sandals from Coloriffics.", "labels": [], "entities": []}, {"text": "Dyeable white satin matte satin or metallic satin upper in a two-piece dress-sandal style with an open round toe crossing pleated vamp straps with a dazzling rhinestone clasp and a wraparound heel strap with an adjustable buckle closure.", "labels": [], "entities": []}, {"text": "Treesje Dakota Shoulder Bag Black Shine -Designer Handbags: Example results for unseen images.", "labels": [], "entities": [{"text": "Treesje Dakota Shoulder Bag Black Shine", "start_pos": 0, "end_pos": 39, "type": "DATASET", "confidence": 0.8289191325505575}]}, {"text": "Both the top words generated by our model and the original held-out captions for the images are shown.", "labels": [], "entities": []}, {"text": "(Note: In the third example, \"hobo\" is actually the term that is used to describe that shape of handbag.) ber of test documents is very large so we can find significance on precision and recall using bootstrap resampling.", "labels": [], "entities": [{"text": "ber", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9976334571838379}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9991278052330017}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9979655742645264}]}, {"text": "We also ran the baseline system and our system on the handbags category of the dataset.", "labels": [], "entities": []}, {"text": "We did not modify the system in anyway when using the bags dataset, just gave it different file for input.", "labels": [], "entities": [{"text": "bags dataset", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.8144041001796722}]}], "tableCaptions": [{"text": " Table 1: Examples of data from the Attribute Discovery Dataset (Berg et al., 2010). The images are fairly clean and  uniform, while captions have more noise and variation.", "labels": [], "entities": [{"text": "Attribute Discovery Dataset", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.6309163371721903}]}, {"text": " Table 2: Results of evaluation in the women's shoes cateogory (top 10-20 words).", "labels": [], "entities": []}, {"text": " Table 4: Results of evaluation in the handbags cateogory (top 10-20 words).", "labels": [], "entities": []}]}