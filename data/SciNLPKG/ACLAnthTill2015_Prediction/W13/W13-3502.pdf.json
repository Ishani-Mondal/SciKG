{"title": [{"text": "Analysis of Stopping Active Learning based on Stabilizing Predictions", "labels": [], "entities": [{"text": "Stopping Active Learning", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.9018617471059164}]}], "abstractContent": [{"text": "Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies.", "labels": [], "entities": []}, {"text": "This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP).", "labels": [], "entities": []}, {"text": "Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented.", "labels": [], "entities": []}, {"text": "Specifically, if the Kappa agreement between two models exceeds a threshold T (where T > 0), then the difference in F-measure performance between those models is bounded above by 4(1\u2212T) T in all cases.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9661603569984436}]}, {"text": "If precision of the positive conjunction of the models is assumed to be p, then the bound can be tightened to 4(1\u2212T) (p+1)T .", "labels": [], "entities": [{"text": "precision", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9992473125457764}]}], "introductionContent": [{"text": "Active learning (AL), also called query learning and selective sampling, is an approach to reduce the costs of creating training data that has received considerable interest (e.g.,).", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7808362662792205}]}, {"text": "Within the NLP community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies.", "labels": [], "entities": []}, {"text": "The main idea is that by judiciously selecting which examples to have labeled, annotation effort will be focused on the most helpful examples and less annotation effort will be required to achieve given levels of performance than if a passive learning policy had been used.", "labels": [], "entities": []}, {"text": "Historically, the problem of developing methods for detecting when to stop AL was tabled for future work and the research literature was focused on how to select which examples to have labeled and analyzing the selection methods ().", "labels": [], "entities": [{"text": "detecting when to stop AL", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6222335219383239}]}, {"text": "However, to realize the savings in annotation effort that AL enables, we must have a method for knowing when to stop the annotation process.", "labels": [], "entities": []}, {"text": "The challenge is that if we stop too early while useful generalizations are still being made, then we can windup with a model that performs poorly, but if we stop too late after all the useful generalizations are made, then human annotation effort is wasted and the benefits of using active learning are lost.", "labels": [], "entities": []}, {"text": "Recently research has begun to develop methods for stopping AL.", "labels": [], "entities": [{"text": "stopping AL", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9316745102405548}]}, {"text": "The methods are all heuristics based on estimates of model confidence, error, or stability.", "labels": [], "entities": []}, {"text": "Although these heuristic methods have appealing intuitions and have had experimental success on a small handful of tasks and datasets, the methods are not widely usable in practice yet because our community's understanding of the stopping methods remains too coarse and inexact.", "labels": [], "entities": []}, {"text": "Pushing forward on understanding the mechanics of stopping at a more exact level is therefore crucial for achieving the design of widely usable effective stopping criteria.", "labels": [], "entities": []}, {"text": "Bloodgood and Vijay-Shanker (2009a) introduce the terminology aggressive and conservative to describe the behavior of stopping methods and conduct an empirical evaluation of the different published stopping methods on several datasets.", "labels": [], "entities": []}, {"text": "While most stopping methods tend to behave conservatively, stopping based on stabilizing predictions computed via inter-model Kappa agreement has been shown to be consistently aggressive without losing performance (in terms of F-Measure 2 ) in several published empirical tests.", "labels": [], "entities": [{"text": "F-Measure 2 )", "start_pos": 227, "end_pos": 240, "type": "METRIC", "confidence": 0.9649030367533366}]}, {"text": "This method stops when the Kappa agreement between consecutively learned models during AL exceeds a threshold for three consecutive iterations of AL.", "labels": [], "entities": [{"text": "Kappa agreement", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.9169591665267944}]}, {"text": "Although this is an intuitive heuristic that has performed well in published experimental results, there has not been any theoretical analysis of the method.", "labels": [], "entities": []}, {"text": "The current paper presents the first theoretical analysis of stopping based on stabilizing predictions.", "labels": [], "entities": [{"text": "stopping", "start_pos": 61, "end_pos": 69, "type": "TASK", "confidence": 0.955242395401001}]}, {"text": "The analysis helps to explain at a deeper and more exact level why the method works as it does.", "labels": [], "entities": []}, {"text": "The results of the analysis help to characterize classes of problems where the method can be expected to work well and where (unmodified) it will not be expected to work as well.", "labels": [], "entities": []}, {"text": "The theory is suggestive of modifications to improve the robustness of the stopping method for certain classes of problems.", "labels": [], "entities": []}, {"text": "And perhaps most important, the approach that we use in our analysis provides an enabling framework for more precise analysis of stopping criteria and possibly other parts of the active learning decision space.", "labels": [], "entities": []}, {"text": "In addition, the information presented in this pa-1 Aggressive methods stop sooner, aggressively trying to reduce unnecessary annotations while conservative methods are careful not to risk losing model performance, even if it means annotating many more examples than were necessary.", "labels": [], "entities": []}, {"text": "For the rest of this paper, we will use F-measure to denote F1-measure, that is, the balanced harmonic mean of precision and recall, which is a standard metric used to evaluate NLP systems.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9885091185569763}, {"text": "F1-measure", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9974181652069092}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9822679758071899}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9918965697288513}]}, {"text": "per is useful for works that consider switching between different active learning strategies and operating regions such as (.", "labels": [], "entities": []}, {"text": "Knowing when to switch strategies, for example, is similar to the stopping problem and is another setting where detailed understanding of the variance of stabilization estimates and their link to performance ramifications is useful.", "labels": [], "entities": []}, {"text": "More exact understanding of the mechanics of stopping is also useful for applications of co-training, and agreement-based co-training () in particular.", "labels": [], "entities": [{"text": "mechanics of stopping", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.6716864109039307}]}, {"text": "Finally, the proofs of the Theorems regarding the relationships between Cohen's Kappa statistic and F-measure maybe of broader use in works that consider interannotator agreement and its ramifications for performance appraisals, a topic that has been of longstanding interest in computational linguistics.", "labels": [], "entities": []}, {"text": "In the next section we summarize the stabilizing predictions (SP) stopping method.", "labels": [], "entities": [{"text": "stabilizing predictions (SP) stopping", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.6243777523438135}]}, {"text": "Section 3 analyzes SP and Section 4 concludes.", "labels": [], "entities": [{"text": "SP", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9130018353462219}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Estimates of the variance of\u02c6Kof\u02c6 of\u02c6K. For each dataset, the estimate of the variance of\u02c6Kof\u02c6 of\u02c6K is computed  (using Equation 5) from the contingency table at the point at which SP stopped AL and the average of  all the variances (across all folds of CV) is displayed. The last row contains the macro-average of the  average variances for all the datasets.", "labels": [], "entities": []}, {"text": " Table 8: Values of the scaling factor from Theo- rem 3.5 for different precision values.", "labels": [], "entities": [{"text": "Theo- rem 3.5", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8157881051301956}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9807212352752686}]}]}