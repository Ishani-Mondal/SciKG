{"title": [{"text": "Automatically Deriving Event Ontologies fora CommonSense Knowledge Base", "labels": [], "entities": [{"text": "Automatically Deriving Event Ontologies", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6955482736229897}, {"text": "CommonSense Knowledge Base", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.9644600749015808}]}], "abstractContent": [{"text": "We describe work aimed at building commonsense knowledge by reading word definitions using deep understanding techniques.", "labels": [], "entities": []}, {"text": "The end result is a knowledge base allowing complex concepts to be reasoned about using OWL-DL reasoners.", "labels": [], "entities": []}, {"text": "We show that we can use this system to automatically create a mid-level ontology for WordNet verbs that has good agreement with human intuition with respect to both the hypernym and causality relations.", "labels": [], "entities": []}, {"text": "We present a detailed error analysis that reveals areas of future work needed to enable high-performance learning of conceptual knowledge by reading.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most researchers agree that attaining deep language understanding will require systems that have large amounts of commonsense knowledge.", "labels": [], "entities": []}, {"text": "Such knowledge will need to be expressed in terms that support semantic lexicons as used by parsing systems, with concept hierarchies and semantic roles, and provide knowledge required for disambiguation as well as deriving key entailments.", "labels": [], "entities": []}, {"text": "While there have been many attempts to hand-build such knowledge, most notably within the Cyc project, as well as ontology-building efforts such as SUMO), GUM (), DOLCE () and EuroWordNet, these fall short of encoding the range and depth of needed knowledge.", "labels": [], "entities": [{"text": "EuroWordNet", "start_pos": 176, "end_pos": 187, "type": "DATASET", "confidence": 0.9799692034721375}]}, {"text": "This motivates work in building a commonsense knowledge base automatically from reading online sources.", "labels": [], "entities": []}, {"text": "Learning by reading offers the opportunity not only to amass a significant knowledge base for processing online sources, but also allows for learning on demand -i.e., looking up something in a dictionary or Wikipedia when needed.", "labels": [], "entities": []}, {"text": "Recently, there has been significant interest in acquiring knowledge using information extraction techniques (e.g.,).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7642134726047516}]}, {"text": "Such work, however, remains close to the surface level of language -involving mostly uninterpreted words and phrases and surface relations between them (e.g., is-a-subject-of, is-an-object-of), or a limited number of pre-specified relations.", "labels": [], "entities": []}, {"text": "In addition, information extraction tends to focus more on learning facts (e.g., Rome is the capital of Italy) than conceptual knowledge (e.g., kill means cause to die).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8729135692119598}]}, {"text": "We have been exploring the feasibility of building extensive knowledge bases by reading definitional sources such as online dictionaries and encyclopedias such as Wikipedia.", "labels": [], "entities": []}, {"text": "So far, we have focussed on what knowledge can be derived by reading the glosses in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9771308898925781}]}, {"text": "This is a good start for the project for several reasons.", "labels": [], "entities": []}, {"text": "First, WordNet is the most used lexical resource in computational linguistics, and so a knowledge base indexed to WordNet would be most readily accessible for use in other projects.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.958454430103302}, {"text": "WordNet", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.960439145565033}]}, {"text": "Second, a significant portion (i.e., about 50%) of the content words in WordNet glosses have been sense tagged by hand, thus giving us considerable help on tackling the word sense disambiguation problem.", "labels": [], "entities": [{"text": "WordNet glosses", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9377291202545166}, {"text": "word sense disambiguation", "start_pos": 169, "end_pos": 194, "type": "TASK", "confidence": 0.7055580218633016}]}, {"text": "And third, WordNet has hand-built semantic structures, such as the hypernym and troponym hierarchies, as well as tagged relations such as cause, and part-of, which give us a hand-coded standard to compare against.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9485374093055725}]}, {"text": "While most previous work on extracting knowledge from WordNet has focused on exploiting these hand-built relations, we focus solely on what can be extracted by understanding the glosses, which consist of short definitions (e.g., kill: cause to die) and a few examples (e.g., This man killed several people when he tried to rob a bank), and use the handbuilt relations for evaluation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.7372870445251465}]}, {"text": "The goal is a system that is not WordNet specific, but could be used on any source of definitional knowledge.", "labels": [], "entities": []}, {"text": "This projects shares some of the same goals with the work of, who convert definitions from a machine readable dictionary of Japanese into underspecified semantic representations using Robust Minimal Recursion Semantics and construct an ontology based on extracted hypernyms and synonyms.", "labels": [], "entities": []}, {"text": "While many complain about WordNet, it is an unparalleled lexical resource.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9556640982627869}]}, {"text": "Attempts to use WordNet as an ontology to support reasoning have mainly focussed on nouns, because the noun hypernym hierarchy provides a relatively good subclass hierarchy (e.g.,).", "labels": [], "entities": []}, {"text": "The situation is not the same for verbs however.", "labels": [], "entities": []}, {"text": "Verbs in WordNet have no organization into an ontology of event types in terms of major conceptual categories such as states, processes, accomplishments and achievements (cf..", "labels": [], "entities": []}, {"text": "Instead, WordNet has a set of 15 semantic domains that serve as unique beginners for verbs, such as verbs of motion and verbs of communication.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.9313172101974487}]}, {"text": "The verbs are then organized around a troponym hierarchy -capturing manner modifications (e.g., destroy is a killing done in a particular way).", "labels": [], "entities": []}, {"text": "Fellbaum (1998) argues against a top-level verb distinction between events and states, or be and do as suggested in, for several reasons.", "labels": [], "entities": []}, {"text": "A goal of WordNet was to reflect human lexical organization, and there is alack of psycholinguistic evidence that humans have strong associations between abstract concepts such as do and more specific verbs.", "labels": [], "entities": []}, {"text": "This lack of a hierarchical midlevel 1 ontology for events creates a significant obstacle to unifying WordNet with ontologies that are built to encode commonsense knowledge and support reasoning.", "labels": [], "entities": []}, {"text": "In this paper, we report on work that attempts to address this problem and bring formal ontologies and lexical resources together in away that captures the detailed knowledge implicit in the lexical resources.", "labels": [], "entities": []}, {"text": "Specifically, we focus on building an ontology by reading word definitions --and use WordNet glosses as our test case for evaluating the feasibility of doing so.", "labels": [], "entities": []}, {"text": "It is important to remember here that our goal is to develop new techniques for building knowledge bases by reading definitions in general, and our work is not specific to WordNet, though we use WordNet for evaluation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 172, "end_pos": 179, "type": "DATASET", "confidence": 0.9599440097808838}, {"text": "WordNet", "start_pos": 195, "end_pos": 202, "type": "DATASET", "confidence": 0.9445705413818359}]}, {"text": "It is always difficult to evaluate the usefulness and correctness of ontologies.", "labels": [], "entities": []}, {"text": "We resort to using several focussed evaluations of particular types of knowledge using human judgement.", "labels": [], "entities": []}, {"text": "In some of these cases, we find that WordNet itself provides some information related to these aspects, so we can compare the coverage and accuracy of our automatically constructed ontology with the explicitly coded information in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9457404613494873}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.992341160774231}, {"text": "WordNet", "start_pos": 231, "end_pos": 238, "type": "DATASET", "confidence": 0.9747080206871033}]}, {"text": "For example, we can evaluate the coverage of our event hierarchy by comparing to the WordNet troponym hierachy, and we can compare the causal relationships we derive between events with the explicitly annotated cause relations in WordNet.", "labels": [], "entities": [{"text": "WordNet troponym hierachy", "start_pos": 85, "end_pos": 110, "type": "DATASET", "confidence": 0.9134734272956848}, {"text": "WordNet", "start_pos": 230, "end_pos": 237, "type": "DATASET", "confidence": 0.9803746342658997}]}], "datasetContent": [{"text": "While we have built a knowledge base containing significant amounts of conceptual information by reading the glosses, here we focus on evaluating just two aspects of this knowledge base.", "labels": [], "entities": []}, {"text": "First is the hierarchical relations between the bare WordNet classes, which is a mid-level ontology for WordNet verbs.", "labels": [], "entities": []}, {"text": "The second involves causal relationships that can be derived from the knowledge.", "labels": [], "entities": []}, {"text": "Some of these are trivial (e.g., kill%2:35:00 causes die%2:39:00), while others are revealed from inference.", "labels": [], "entities": []}, {"text": "For instance, the subsumption algorithm will compute that the verb class air%2:32:03 causes the event of something becoming known%3:00:00.", "labels": [], "entities": []}, {"text": "There is much more information in this knowledge base than we are going to evaluate here.", "labels": [], "entities": []}, {"text": "For instance, it contains knowledge about the changes of state and transitions that serve to define many verbs, and in Allen et al we demonstrate an ability to perform temporal inference using the knowledge base.", "labels": [], "entities": []}, {"text": "But in this paper we focus solely on evaluating just the hierarchical and causal relations between bare WordNet classes in order to enable a direct comparison with WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 164, "end_pos": 171, "type": "DATASET", "confidence": 0.9616103768348694}]}, {"text": "We randomly selected 6N (N=8) pairs of verb concepts (A, B) from those that our system successfully processed (columns 0-11 in), such that at least N of them fell into each of the four categories \"{WordNet, our OWL-DL knowledge base} says that A {is a kind of, causes} B\", and such that 2N pairs were unrelated in either source.", "labels": [], "entities": [{"text": "OWL-DL knowledge base", "start_pos": 211, "end_pos": 232, "type": "DATASET", "confidence": 0.8919839461644491}]}, {"text": "We then presented the pairs in different randomized orders to a set of human judges and asked them to identify whether there was a causal or hierarchical relation between the events, or whether they were unrelated.", "labels": [], "entities": []}, {"text": "As judges, we used six researchers who had been involved with the project as well as five people who have no relation to the work.", "labels": [], "entities": []}, {"text": "We computed the inter-rater agreement (IRA) using Cohen's kappa score.", "labels": [], "entities": [{"text": "inter-rater agreement (IRA)", "start_pos": 16, "end_pos": 43, "type": "METRIC", "confidence": 0.9075062870979309}]}, {"text": "Kappa was computed for each pair of judges, then averaged to provide a single index of IRA.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9097597599029541}, {"text": "IRA", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9950239658355713}]}, {"text": "The resulting kappa indicated substantial agreement, \u03ba = 0.63.", "labels": [], "entities": []}, {"text": "In order to eliminate the cases where their was no consensus among the judges, we only consider the cases in which eight or more judges agreed, which was 83% of the samples, and used the majority decision as the gold standard.", "labels": [], "entities": []}, {"text": "We can then evaluate the accuracy of the hand-coded relations in WordNet against two versions of our system: one processing the raw glosses in WordNet and the other with 79 corrected word sense tags out of over 5000 glosses processed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9991948008537292}, {"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9700455069541931}, {"text": "WordNet", "start_pos": 143, "end_pos": 150, "type": "DATASET", "confidence": 0.973648726940155}]}, {"text": "The precision and recall results are shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996671676635742}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9993194341659546}]}, {"text": "The most important property we desire is that the knowledge produced is accurate, i.e., the precision score.", "labels": [], "entities": [{"text": "accurate", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9972080588340759}, {"text": "precision score", "start_pos": 92, "end_pos": 107, "type": "METRIC", "confidence": 0.9817005693912506}]}, {"text": "This reflects the ability of the systems to produce accurate knowledge from processing glosses.", "labels": [], "entities": []}, {"text": "If precision is high, we could always improve recall by processing more definitional sources.", "labels": [], "entities": [{"text": "precision", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9992595314979553}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9987493753433228}]}, {"text": "We see that the precision scores for the system generated relations are quite good, over 80% for the hypernym relations and a perfect 100% for the causal relations.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9995700716972351}]}, {"text": "Regarding WordNet, we see that the hand-coded relations had a 100% precision, indicating that the structural information in WordNet is highly accurate.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.965344250202179}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9991618394851685}, {"text": "WordNet", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.9653539061546326}]}, {"text": "The recall numbers, however, show that a significant number of possible relations are missed, especially for causal relations.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9973340034484863}]}, {"text": "This suggests that it is worth exploring whether the information implicit in the glosses is redundant given the hand-coding, or whether they serve as an important additional source of knowledge.", "labels": [], "entities": []}, {"text": "We can explore this by comparing the sets of relations produced by the system with the relations in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.9736577868461609}]}, {"text": "If they overlap significantly, then the hand-built WordNet relations are fairly complete.", "labels": [], "entities": []}, {"text": "If they are disjoint, then the glosses contain an important additional source of these structural relations.", "labels": [], "entities": []}, {"text": "The analysis is summarized in.", "labels": [], "entities": []}, {"text": "We look at each relation proposed by WordNet or the system, and look at the overlap", "labels": [], "entities": [{"text": "WordNet", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9562000036239624}]}], "tableCaptions": [{"text": " Table 1: The number of new senses introduced with each iteration", "labels": [], "entities": []}, {"text": " Table 4: Comparing the Redundancy between WordNet & System-generated relations", "labels": [], "entities": []}, {"text": " Table 3: Precision and Recall Scores Against Human Judgement", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9267085194587708}, {"text": "Recall Scores", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.8843795657157898}, {"text": "Human Judgement", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7047169953584671}]}]}