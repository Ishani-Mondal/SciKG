{"title": [{"text": "Word Recognition from Continuous Articulatory Movement Time-Series Data using Symbolic Representations", "labels": [], "entities": [{"text": "Word Recognition", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7062378376722336}]}], "abstractContent": [{"text": "Although still in experimental stage, articulation-based silent speech interfaces may have significant potential for facilitating oral communication in persons with voice and speech problems.", "labels": [], "entities": []}, {"text": "An articulation-based silent speech interface converts articulatory movement information to audible words.", "labels": [], "entities": []}, {"text": "The complexity of speech production mechanism (e.g., co-articulation) makes the conversion a formidable problem.", "labels": [], "entities": [{"text": "conversion", "start_pos": 80, "end_pos": 90, "type": "TASK", "confidence": 0.9705443382263184}]}, {"text": "In this paper, we reported a novel, real-time algorithm for recognizing words from continuous articulatory movements.", "labels": [], "entities": []}, {"text": "This approach differed from prior work in that (1) it focused on word-level, rather than phoneme-level; (2) online segmentation and recognition were conducted at the same time; and (3) a symbolic representation (SAX) was used for data reduction in the original articulatory movement time-series.", "labels": [], "entities": [{"text": "online segmentation and recognition", "start_pos": 108, "end_pos": 143, "type": "TASK", "confidence": 0.6561863720417023}, {"text": "data reduction", "start_pos": 230, "end_pos": 244, "type": "TASK", "confidence": 0.7392338514328003}]}, {"text": "A data set of 5,900 isolated word samples of tongue and lip movements was collected using electromagnetic articulograph from eleven English speakers.", "labels": [], "entities": []}, {"text": "The average speaker-dependent recognition accuracy was up to 80.00%, with an average latency of 302 miliseconds for each word prediction.", "labels": [], "entities": [{"text": "speaker-dependent recognition", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6610535532236099}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9591979384422302}, {"text": "latency", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9653064608573914}]}, {"text": "The results demonstrated the effectiveness of our approach and its potential for building a real-time articulation-based silent speech interface for clinical applications.", "labels": [], "entities": []}, {"text": "The across-speaker variation of the recognition accuracy was discussed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.802761971950531}]}], "introductionContent": [{"text": "Persons who lose their voice after laryngectomy (a surgical removal of the larynx due to the treatment of cancer) or who have speech impairment struggle with daily communication.", "labels": [], "entities": []}, {"text": "In 2012, more than 52,000 new cases of head and neck cancers were estimated in the United States.", "labels": [], "entities": [{"text": "head and neck cancers", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.5805287733674049}]}, {"text": "Currently, there are only limited treatment options for these individuals, which include (1) \"esophageal speech\", which involves oscillation of the esophagus and can be difficult to learn; (2) electrolarynx, which is a mechanical device resulting in a robotic-like voice; and (3) augmented and alternative communication (AAC) devices (e.g., text-to-speech synthesizers operated with keyboards), which are limited by slow manual text input.", "labels": [], "entities": []}, {"text": "New assistive technologies are needed to provide a more efficient oral communication mode with natural voice for those individuals.", "labels": [], "entities": []}, {"text": "Silent speech interfaces (SSIs), although still in early development stages (e.g., speaker-dependent recognition, small-vocabulary, devices are not ready for clinical use), may provide an alternative interaction modality for persons with voice and speech problems.", "labels": [], "entities": [{"text": "Silent speech interfaces (SSIs)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6044105639060339}, {"text": "speaker-dependent recognition", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.7253676503896713}]}, {"text": "The common purpose of SSIs is to convert non-audio articulatory data to text that drives a text-tospeech (TTS) synthesizer (e.g.,) (see fora schematic of our SSI design).", "labels": [], "entities": []}, {"text": "Potential articulatory data transduction methods for SSIs include ultrasound, surface electromyography electrodes, and electromagnetic articulograph (EMA).", "labels": [], "entities": [{"text": "SSIs", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.9754884839057922}]}, {"text": "The current project used EMA, which registers the 3D motion of sensors adhered to the tongue and lips.", "labels": [], "entities": []}, {"text": "One major challenge for building effective SSIs is developing accurate and fast algorithms that recognize words or sentences based on articulatory data (i.e., without audio information).", "labels": [], "entities": []}, {"text": "Articulatory data have been successfully used to improve the accuracy of voiced speech recognition from both healthy talkers and neurologically impaired individuals.", "labels": [], "entities": [{"text": "accuracy of voiced speech recognition", "start_pos": 61, "end_pos": 98, "type": "TASK", "confidence": 0.6502360880374909}]}, {"text": "This typically involves the use of articulatory features (AFs), which include lip rounding, tongue tip position, and manner of production, for example.", "labels": [], "entities": [{"text": "lip rounding", "start_pos": 78, "end_pos": 90, "type": "TASK", "confidence": 0.6946586519479752}]}, {"text": "Phonemelevel AF-based approaches have typically obtained word recognition accuracies less than 50% because articulation can vary significantly within those categorical features depending on the surrounding sounds and the speaking context.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.724007785320282}]}, {"text": "These challenges in phoneme-level recognition motivate a higher unit level of articulatory recognition, for example, word-level or sentence-level.", "labels": [], "entities": [{"text": "phoneme-level recognition", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.7461633682250977}]}, {"text": "Although sentence-level recognition accuracy is high, it lacks the scalability of phoneme-and word-level recognition because all sentences are required to be known prior to prediction.", "labels": [], "entities": [{"text": "sentence-level recognition", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7219824194908142}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9808657169342041}, {"text": "word-level recognition", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.6847723871469498}]}, {"text": "Word-level recognition may have better scalability than sentence-level recognition and the potential for higher accuracy than phoneme-level recognition.", "labels": [], "entities": [{"text": "Word-level recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6810084879398346}, {"text": "sentence-level recognition", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6895494014024734}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9978973865509033}]}, {"text": "Word-level recognition from acoustic data has outperformed monophone recognition by approximately 25%.", "labels": [], "entities": [{"text": "Word-level recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7357174456119537}, {"text": "monophone recognition", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.744139164686203}]}, {"text": "However, whole-word recognition has rarely been investigated in articulatory data probably due to logistic difficulty of collecting articulatory data.", "labels": [], "entities": [{"text": "whole-word recognition", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.865420788526535}]}, {"text": "Online word recognition from continuous articulatory movements can be extremely challenging because word boundaries (onset and offset) are difficult to identify.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7055522501468658}]}, {"text": "Recent works have shown offline word classification (word boundaries are known) accuracy can be greater than 90% fora small vocabulary.", "labels": [], "entities": [{"text": "offline word classification", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6208489040533701}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9992573857307434}]}, {"text": "However, because of word segmentation issues, online recognition accuracy can be significantly lower than offline classification accuracy.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7937992215156555}, {"text": "online recognition", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6228301227092743}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.94085294008255}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.5878808498382568}]}, {"text": "Online word segmentation based on articulatory movements has rarely been attempted.", "labels": [], "entities": [{"text": "Online word segmentation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6478492120901743}]}, {"text": "A threshold (e.g., 2 SD) of the articulatory movements has been successfully used for isolated word datasets.", "labels": [], "entities": []}, {"text": "Such amplitude-based segmentation may not be well suited for words produced in a continuous sequence because of co-articulation (illustrated in) or for words within sentences (connected speech).", "labels": [], "entities": []}, {"text": "Co-articulation is an effect characterized by a sound is affected by its adjacent sounds.", "labels": [], "entities": []}, {"text": "illustrates the articulatory movements fora word sequence with co-articulation produced by one of the participants.", "labels": [], "entities": []}, {"text": "The top panel shows the continuous motion of sensors (y and z coordinates, where y is vertical and z is frontback) attached on the tongue and lips.", "labels": [], "entities": []}, {"text": "T1, T2, T3, and T4 are four sensors attached on the midsaggital line of the tongue, from tip to back; UL is upper lip; LL is lower lip.", "labels": [], "entities": [{"text": "T1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8509535193443298}]}, {"text": "Details of the coordinate system and the labels of the sensors are provided in Section 4.", "labels": [], "entities": []}, {"text": "The bottom panel shows the synchronously recorded audio.", "labels": [], "entities": []}, {"text": "The goal of this project was to investigate word recognition from continuous articulatory movements.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.768611341714859}]}, {"text": "A novel, real-time algorithm for word recognition from continuous stream of articulatory movements has been recently proposed.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8351214826107025}]}, {"text": "The algorithm was designed to solve the online segmentation and recognition problems simultaneously.", "labels": [], "entities": [{"text": "online segmentation and recognition", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.6005213782191277}]}, {"text": "The algorithm is characterized by the following: recognition is at the word level rather than the phoneme-or sentence-level; recognition employs a dynamic thresholding technique based on patterns in the probability change returned by a classifier; and the algorithm is extensible (i.e., it can be embedded with a variety of classifiers).", "labels": [], "entities": []}, {"text": "The algorithm has been tested on the minimally processed articulatory movements.", "labels": [], "entities": []}, {"text": "Although the results were promising (missing only 1.93 words on a sequence with twenty-five words), false positives caused a relatively low overall accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9974449872970581}]}, {"text": "The current project implemented the following three strategies for improving word recognition accuracy: (1) using symbolic aggregation approximation (SAX) representation to reduce the local variation in the original articulatory movement time-series data, (2) adding a look-back strategy to handle a situation in which two words are so close that the onset of the second word may not be accurately identified, and (3) using speaker-dependent thresholds to determine the word candidates during online recognition.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.7928658425807953}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.8115373253822327}]}, {"text": "A phonetically-balanced and isolated word dataset of tongue and lip movements was collected using electromagnetic articulograph and used to evaluate the effectiveness and efficiency of the improved algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "Recognition accuracy and processing time were used to evaluate the performance of the word recognition algorithm.", "labels": [], "entities": [{"text": "Recognition", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9367163777351379}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.7527627944946289}, {"text": "word recognition algorithm", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.8806575934092203}]}, {"text": "A word prediction is correct if the expected word is identified within half a second of its actual occurrence time.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 2, "end_pos": 17, "type": "TASK", "confidence": 0.7154898792505264}]}, {"text": "That is, both missing values and wrongly predicted occurrence times are considered as errors.", "labels": [], "entities": []}, {"text": "A false positive is a word that is recognized at a time point where there is actually no word.", "labels": [], "entities": []}, {"text": "illustrates the word probability distribution on a selected sequence.", "labels": [], "entities": []}, {"text": "In this example, all twenty-five words were correctly recognized.", "labels": [], "entities": []}, {"text": "Two measures were used to evaluate the efficiency of this algorithm: prediction location offset (machine-independent) and prediction processing time, or latency (machinedependent).", "labels": [], "entities": [{"text": "latency", "start_pos": 153, "end_pos": 160, "type": "METRIC", "confidence": 0.966042160987854}]}, {"text": "Prediction location offset was defined as the difference in location on a sequence between where a word is actually spoken and where it is recognized.", "labels": [], "entities": [{"text": "Prediction location offset", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.8979534109433492}]}, {"text": "The prediction location offset provides an estimate of how much information is needed for predicting a word.", "labels": [], "entities": [{"text": "predicting a word", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.860251784324646}]}, {"text": "Latency is the actual CPU time needed for predicting a word.", "labels": [], "entities": [{"text": "Latency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9768505096435547}, {"text": "predicting a word", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8877783219019572}]}], "tableCaptions": [{"text": " Table 1. Summary of the performances of current and  the original algorithm.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9093863368034363}]}]}