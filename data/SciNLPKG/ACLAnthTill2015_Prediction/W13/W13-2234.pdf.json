{"title": [{"text": "Generating English Determiners in Phrase-Based Translation with Synthetic Translation Options", "labels": [], "entities": [{"text": "Phrase-Based Translation", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7732723951339722}, {"text": "Synthetic Translation Options", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.7976961334546407}]}], "abstractContent": [{"text": "We propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation options-phrasal translations that are generated by auxiliary translation and post-editing processes-to augment the default phrase inventory learned from parallel data.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7235099375247955}]}, {"text": "We apply our technique to the problem of producing English deter-miners when translating from Russian and Czech, languages that lack definiteness morphemes.", "labels": [], "entities": []}, {"text": "Our approach augments the English side of the phrase table using a classifier to predict where English articles might plausibly be added or removed, and then we decode as usual.", "labels": [], "entities": []}, {"text": "Doing so, we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier.", "labels": [], "entities": [{"text": "quality", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9689190983772278}]}], "introductionContent": [{"text": "Phrase-based translation works as follows.", "labels": [], "entities": [{"text": "Phrase-based translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8912075757980347}]}, {"text": "A set of candidate translations for an input sentence is created by matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (.", "labels": [], "entities": []}, {"text": "This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones, improving the identification of phrase pairs in parallel data (, and formal generalizations to gapped rules and rich nonterminal types).", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9636219143867493}]}, {"text": "This paper proposes a different mechanism for improving phrase-based translation: the use of synthetic translation options to supplement the standard phrasal inventory used in phrase-based translation systems.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.843178004026413}]}, {"text": "In the following, we argue that phrase tables acquired in usual way will be expected to have gaps in their coverage in certain language pairs and that supplementing these with synthetic translation options is a priori preferable to alternative techniques, such as post processing, for generalizing beyond the translation pairs observable in training data ( \u00a72).", "labels": [], "entities": []}, {"text": "As a case study, we consider the problem of producing English definite/indefinite articles (the, a, and an) when translating from Russian and Czech, two languages that lack overt definiteness morphemes ( \u00a73).", "labels": [], "entities": []}, {"text": "We develop a classifier that predicts the presence and absence of English articles ( \u00a74).", "labels": [], "entities": []}, {"text": "This classifier is used to generate synthetic translation options that are used to augment phrase tables used the usual way ( \u00a75).", "labels": [], "entities": []}, {"text": "We evaluate their performance relative to post-processing approach and to a baseline phrase-based system, finding that synthetic translation options reliably outperform the other approaches ( \u00a76).", "labels": [], "entities": []}, {"text": "We then discuss how our approach relates to previous work ( \u00a77) and conclude by discussing further applications of our technique ( \u00a78).", "labels": [], "entities": []}, {"text": "standard phrase-based translation approaches, and why this technique might be better than some alternative proposals that been made for generalizing beyond translation examples directly observable in the training data.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.6431792229413986}]}, {"text": "In language pairs that are typologically similar (i.e., when both languages lexicalize the same kinds of semantic and syntactic information), words and phrases map relatively directly from source to target languages, and the standard approach to learning phrase pairs is quite effective.", "labels": [], "entities": []}, {"text": "However, in language pairs in which individual source language words have many different possible translations (e.g., when the target language word could have many different inflections or could be surrounded by different function words that have no direct correspondence in the source language), we can expect the standard phrasal inventory to be incomplete, except when very large quantities of parallel data are available or for very frequent words.", "labels": [], "entities": []}, {"text": "There simply will not be enough examples from which to learn the ideal set of translation options.", "labels": [], "entities": []}, {"text": "Therefore, since phrase based translation can only generate input/output word pairs that were directly observed in the training corpus, the decoder's only hope for producing a good output is to find a fluent, meaningpreserving translation using incomplete translation lexicons.", "labels": [], "entities": [{"text": "phrase based translation", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.6461176574230194}]}, {"text": "Synthetic translation option generation seeks to fill these gaps using secondary generation processes that produce possible phrase translation alternatives that are not directly extractable from the training data.", "labels": [], "entities": [{"text": "Synthetic translation option generation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8085687011480331}, {"text": "phrase translation", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7385205626487732}]}, {"text": "We hypothesize that by filling in gaps in the translation options, discriminative translation models will be more effective (leading to better translation quality).", "labels": [], "entities": []}, {"text": "The creation of synthetic translation options can be understood as a kind of translation or postediting of phrasal units/translations.", "labels": [], "entities": []}, {"text": "This raises a question: if we have the ability to post-edit a phrasal translation or retranslate a source phrase so as to fill in gaps in the phrasal inventory, we should be able to use the same technique to translate the sentence; why not do this?", "labels": [], "entities": []}, {"text": "While the effectiveness of this approach will ultimately be assessed empirically, translation option generation is appealing because the translation option synthesizer need not produce only single-best guesses-1 When translating from a language with a richer lexical inventory to a simpler one, approximate matching or backing off to (e.g.) morphologically simpler forms likewise reliably produces good translations.", "labels": [], "entities": [{"text": "translation option generation", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.8800234993298849}]}, {"text": "Since Russian lacks a definiteness morpheme the determiners a, the must be part of a translation option containing \u00f3\u00e2\u00e8\u00e4\u00e5\u00eb or \u00ea\u00ee\u00f8\u00ea\u00f3 in order to be present in the right place in the English output.", "labels": [], "entities": []}, {"text": "Translation options that are in dashed boxes should exist but were not observed in the training data.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.937188982963562}]}, {"text": "This work seeks to produce such missing translation options synthetically.", "labels": [], "entities": []}, {"text": "if multiple possibilities appear to be equally good (say, multiple inflections of a translated lemma), then multiple translation options maybe synthesized.", "labels": [], "entities": []}, {"text": "Ultimately, of course, the global translation model must select one translation for every phrase it uses, but the decoder will have access to global information that it can use to pick better translation options.", "labels": [], "entities": [{"text": "global translation", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6684738397598267}]}], "datasetContent": [{"text": "We employ the creg regression modeling framework to train a ternary logistic regression classifier.", "labels": [], "entities": []}, {"text": "All features were computed for the targetside of the Russian-English TED corpus (; from 117,527 sentences we removed 5K sentences used as tuning and test sets in the MT system.", "labels": [], "entities": [{"text": "Russian-English TED corpus", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.6325699190298716}, {"text": "MT", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.9010133147239685}]}, {"text": "We extract statistical features from monolingual English corpora released for WMT-11).", "labels": [], "entities": [{"text": "WMT-11", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.8871151804924011}]}, {"text": "In the training corpus there are 65,075 I instances, 114,571 D instances, and 2,435,287 N instances.", "labels": [], "entities": []}, {"text": "To create a balanced training set we randomly sample 65K instances from each set of collected instances.", "labels": [], "entities": []}, {"text": "8 This training set of feature vectors has 142,604 features and 285,210 parameters.", "labels": [], "entities": []}, {"text": "To minimize the number of free parameters in our model we use 1 regularization.", "labels": [], "entities": []}, {"text": "We perform 10-fold cross validation experiments with various feature combinations, evaluating the classifier accuracy for all classes and for each class independently.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9560071229934692}]}, {"text": "The performance of the classifier on individual classes and consolidated results for all classes are listed in.", "labels": [], "entities": []}, {"text": "We observe that morphosyntactic and lexical features are highly significant, reducing the error rate of statistical features by 25%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9880522191524506}]}, {"text": "A combi-: 10-fold cross validation accuracy of the classifier overall and by class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9465177059173584}]}, {"text": "nation of morphosyntactic, lexical, and statistical features is also helpful, reducing 13% more errors.", "labels": [], "entities": [{"text": "nation", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9379075765609741}]}, {"text": "Semantic features do not contribute to the classifier accuracy (we believe, mainly due to the feature sparsity).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.984955370426178}]}, {"text": "Our experimental workflow includes the following steps.", "labels": [], "entities": []}, {"text": "First, we select a phrase table PT source from which we generate synthetic phrases.", "labels": [], "entities": []}, {"text": "To synthesize phrases, we employ two different techniques: LM-based and classifier-based.", "labels": [], "entities": []}, {"text": "We use a LM for one-or two-word phrases or an auxiliary classifier for longer phrases and create anew phrase in which we insert, remove or substitute an article between each adjacent pair of words in the original phrase.", "labels": [], "entities": []}, {"text": "Such distinction between short and longer phrases has clear motivation: phrases without context may allow alternative, equally plausible options for article selection, therefore we can just rely on a LM, trained on large monolingual corpora, to identify phrases unobserved in MT training corpus.", "labels": [], "entities": [{"text": "article selection", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.8307051658630371}, {"text": "MT training", "start_pos": 276, "end_pos": 287, "type": "TASK", "confidence": 0.8215705752372742}]}, {"text": "Longer context restricts determiners usage and statistical model decisions are less prone to generating ungrammatical synthetic phrases.", "labels": [], "entities": []}, {"text": "LM-based method is applied to phrases shorter than three words.", "labels": [], "entities": []}, {"text": "These phrases are numerous, roughly 20% of a phrase table, and extracted from many sites in the training data.", "labels": [], "entities": []}, {"text": "For each short (target) phrase we add all possible alternative entries observed in the LM and not observed in the original translation model.", "labels": [], "entities": []}, {"text": "For example, fora short target phrase a cat we extract the cat.", "labels": [], "entities": []}, {"text": "We apply an auxiliary classifier to longer phrases, containing three or more words.", "labels": [], "entities": []}, {"text": "Based on the classifier prediction, we use the maximally probable class to insert, remove or substitute an article between each adjacent pair of words in the original phrase.", "labels": [], "entities": []}, {"text": "Synthetic phrases are generated by linguistically-informed features and can introduce alternative grammatically-correct translations of source phrases by adding or removing existing articles (since the English article selection in a local context is often ambiguous and not categorical).", "labels": [], "entities": []}, {"text": "We add a synthetic phrase only if the phrase pair not observed in the original model.", "labels": [], "entities": []}, {"text": "We compare two possible applications of a classifier: one-pass and iterative prediction.", "labels": [], "entities": [{"text": "iterative prediction", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.7035419940948486}]}, {"text": "With one-pass prediction we decide on the prediction for each position independently of other decisions.", "labels": [], "entities": []}, {"text": "With iterative update we adopt the best first (greedy) strategy, selecting in each iteration the update-location in which the classifier obtains highest confidence score.", "labels": [], "entities": []}, {"text": "In each iteration we incorporate a prediction in a target phrase, and in the next iteration the best first decision is made on an updated phrase.", "labels": [], "entities": []}, {"text": "Iterative prediction stops when no updates are introduced.", "labels": [], "entities": [{"text": "Iterative prediction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7674134373664856}]}, {"text": "Synthetic phrases are added to a phrase table with the five standard phrasal translation features that were found in the source phrase, and with several new features.", "labels": [], "entities": []}, {"text": "First, we add a boolean feature indicating the origin of a phrase: synthetic or original.", "labels": [], "entities": []}, {"text": "Second, we experiment with a posterior probability of a classifier averaged overall locations where it could be extracted from the training data.", "labels": [], "entities": []}, {"text": "The next feature is derived from this score: it is a boolean feature indicating a confidence of the classifier: the feature value is 1 iff the average classifier score is higher than some threshold.", "labels": [], "entities": []}, {"text": "Consider again a phrase I saw a cat discussed in Section 1.", "labels": [], "entities": []}, {"text": "Synthetic entry generation from the original phrase table entry is illustrated in", "labels": [], "entities": [{"text": "Synthetic entry generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9216066002845764}]}], "tableCaptions": [{"text": " Table 1: 10-fold cross validation accuracy of the  classifier over all and by class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9806628823280334}]}, {"text": " Table 2: WER (lower is better) of reference trans- lations without articles and of post-processed ref- erence translations. Both one-pass and iterative  approaches are effective in the task of determin- ers prediction.", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9989181756973267}, {"text": "determin- ers prediction", "start_pos": 194, "end_pos": 218, "type": "TASK", "confidence": 0.6628867760300636}]}]}