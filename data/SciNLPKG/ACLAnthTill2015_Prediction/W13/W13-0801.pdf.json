{"title": [{"text": "A Semantic Evaluation of Machine Translation Lexical Choice", "labels": [], "entities": [{"text": "Machine Translation Lexical", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8131310939788818}]}], "abstractContent": [{"text": "While automatic metrics of translation quality are invaluable for machine translation research , deeper understanding of translation errors require more focused evaluations designed to target specific aspects of translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8082985877990723}]}, {"text": "We show that Word Sense Disam-biguation (WSD) can be used to evaluate the quality of machine translation lexical choice, by applying a standard phrase-based SMT system on the SemEval2010 Cross-Lingual WSD task.", "labels": [], "entities": [{"text": "machine translation lexical choice", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.8141943886876106}, {"text": "SemEval2010 Cross-Lingual WSD task", "start_pos": 175, "end_pos": 209, "type": "TASK", "confidence": 0.5856762230396271}]}, {"text": "This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9919971823692322}]}], "introductionContent": [{"text": "Much research has focused on automatically evaluating the quality of Machine Translation (MT) by comparing automatic translations to human translations on samples of a few thousand sentences.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.8690509796142578}]}, {"text": "Many metrics (;, for instance) have been proposed to estimate the adequacy and fluency of machine translation and evaluated based on their correlatation with human judgements of translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7234542816877365}]}, {"text": "While these metrics have proven invaluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors.", "labels": [], "entities": [{"text": "MT research", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.9335325658321381}]}, {"text": "When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9876701831817627}, {"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9980733394622803}, {"text": "TER scores", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9625565707683563}]}, {"text": "Error analysis can of course be done manually), but it is often too slow and expensive to be performed as often as needed during system development.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9029792845249176}]}, {"text": "Several metrics have been recently proposed to evaluate specific aspects of translation quality such as word order.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8784357309341431}]}, {"text": "While word order is indirectly taken into account by BLEU, TER or METEOR scores, dedicated metrics provide a direct evaluation that lets us understand whether a given system's reordering performance improved during system development.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9988322854042053}, {"text": "TER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9854871034622192}, {"text": "METEOR", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9549368023872375}]}, {"text": "Word order metrics provide a complementary tool for targeting evaluation and analysis to a specific aspect of machine translation quality.", "labels": [], "entities": [{"text": "machine translation quality", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.8146349589029948}]}, {"text": "There has not been as much work on evaluating the lexical choice performance of MT: does a MT system preserve the meaning of words in translation?", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9758133888244629}]}, {"text": "This is of course measured indirectly by commonly used global metrics, but a more focused evaluation can help us gain a better understanding of the behavior of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.986652135848999}]}, {"text": "In this paper, we show that MT lexical choice can be framed and evaluated as a standard Word Sense Disambiguation (WSD) task.", "labels": [], "entities": [{"text": "MT lexical choice", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.9519396424293518}, {"text": "Word Sense Disambiguation (WSD) task", "start_pos": 88, "end_pos": 124, "type": "TASK", "confidence": 0.7743198190416608}]}, {"text": "We leverage existing WSD shared tasks in order to evaluate whether word meaning is preserved in translation.", "labels": [], "entities": [{"text": "WSD shared tasks", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.8331477443377177}]}, {"text": "Let us emphasize that, just like reordering metrics, our WSD evaluation is meant to complement global metrics of translation quality.", "labels": [], "entities": [{"text": "WSD", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9680961966514587}]}, {"text": "In previous work, intrinsic evaluations of lexical choice have been performed using either semi-automatically constructed data sets based on MT reference translations (, or manually constructed word sense disambiguation test beds that do not exactly match MT lexical choice).", "labels": [], "entities": [{"text": "MT reference translations", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.6898549993832906}]}, {"text": "We will show how existing Cross-Lingual Word Sense Disambiguation tasks ( can be directly seen as machine translation lexical choice (Section 2): their sense inventory is based on translations in a second language rather than arbitrary sense representations used in other WSD tasks; unlike in MT evaluation settings, human annotators can more easily provide a complete representation of all correct meanings of a word.", "labels": [], "entities": [{"text": "Cross-Lingual Word Sense Disambiguation", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.6788867861032486}, {"text": "machine translation lexical choice", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.7995456680655479}, {"text": "WSD tasks", "start_pos": 272, "end_pos": 281, "type": "TASK", "confidence": 0.9197341501712799}, {"text": "MT evaluation", "start_pos": 293, "end_pos": 306, "type": "TASK", "confidence": 0.9221758544445038}]}, {"text": "Second, we show how using this task for evaluating the lexical choice performance of several phrase-based SMT systems (PB-SMT) gives some insights into their strengths and weaknesses (Section 5).", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.7159025073051453}]}], "datasetContent": [{"text": "In order to make comparison with other systems possible, we follow the standard evaluation framework defined for the task and score the output of all our systems using four different metrics, computed using the scoring tool made available by the organizers.", "labels": [], "entities": []}, {"text": "The difference between system predictions and gold standard annotations are quantified using precision and recall scores 1 , defined as follows.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9994345307350159}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9980320334434509}]}, {"text": "Given a set T of test items and a set H of annotators, H i is the set of translation proposed by all annotators h for instance i \u2208 T . Each translation type res in H i has an associated frequency freq res , which represents the number of human annotators which selected res as one of their top 3 translations.", "labels": [], "entities": []}, {"text": "Given a set of system answers A of items i \u2208 T such that the system provides at least one answer, a i : i \u2208 A is is the set of answers from the system for instance i.", "labels": [], "entities": []}, {"text": "For each i, the scorer computes the intersection of the system answers a i and the gold standard H i . Systems propose as many answers as deemed nec-essary, but the scores are divided by the number of guesses in order not to favor systems that output many answers per instance.", "labels": [], "entities": []}, {"text": "We also report Mode Precision and Mode Recall scores: instead of comparing system answers to the full set of gold standard translations H i for an instance i \u2208 T , the Mode Precision and Recall scores only use a single gold translation, which is the translation chosen most frequently by the human annotators.", "labels": [], "entities": []}, {"text": "In addition, we compute the 1-gram precision component of the BLEU score (), denoted as BLEU1 in the result tables 2 . In contrast with the official CLWSD evaluation scores described above, BLEU1 gives equal weight to all translation candidates, which can be seen as multiple references.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9822962880134583}, {"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9767796099185944}, {"text": "BLEU1", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9975546002388}, {"text": "BLEU1", "start_pos": 190, "end_pos": 195, "type": "METRIC", "confidence": 0.996459424495697}]}], "tableCaptions": [{"text": " Table 2: Main CLWSD results: PBSMT yields com- petitive results, but WSD outperforms PBSMT", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.5647412538528442}]}, {"text": " Table 3: Impact of source and target context models  on PBSMT performance", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 57, "end_pos": 62, "type": "TASK", "confidence": 0.804427444934845}]}, {"text": " Table 4: Impact of reordering models: lexicalized  reodering does not hurt lexical choice only when hi- erarchical models are used", "labels": [], "entities": []}, {"text": " Table 6: Impact of training corpus on PBSMT per- formance: adding news parallel sentences helps Pre- cision and Recall, but does not match WSD on the  Europarl only.", "labels": [], "entities": [{"text": "Pre", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9846622943878174}, {"text": "Recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9506088495254517}, {"text": "WSD", "start_pos": 140, "end_pos": 143, "type": "DATASET", "confidence": 0.5704526901245117}, {"text": "Europarl", "start_pos": 152, "end_pos": 160, "type": "DATASET", "confidence": 0.977068305015564}]}]}