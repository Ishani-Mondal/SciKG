{"title": [{"text": "Multi-Step Regression Learning for Compositional Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a model for compositional distributional semantics related to the framework of Co-ecke et al.", "labels": [], "entities": []}, {"text": "(2010), and emulating formal semantics by representing functions as tensors and arguments as vectors.", "labels": [], "entities": []}, {"text": "We introduce anew learning method for tensors, generalising the approach of Ba-roni and Zamparelli (2010).", "labels": [], "entities": []}, {"text": "We evaluate it on two benchmark data sets, and find it to outperform existing leading methods.", "labels": [], "entities": []}, {"text": "We argue in our analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face.", "labels": [], "entities": []}], "introductionContent": [{"text": "The staggering amount of machine readable text available on today's Internet calls for increasingly powerful text and language processing methods.", "labels": [], "entities": []}, {"text": "This need has fuelled the search for more subtle and sophisticated representations of language meaning, and methods for learning such models.", "labels": [], "entities": []}, {"text": "Two well-researched but prima-facie orthogonal approaches to this problem are formal semantic models and distributional semantic models, each complementary to the other in its strengths and weaknesses.", "labels": [], "entities": []}, {"text": "Formal semantic models generally implement the view of Frege (1892)-that the semantic content of an expression is its logical form-by defining a systematic passage from syntactic rules to the composition of parts of logical expressions.", "labels": [], "entities": []}, {"text": "This allows us to derive the logical form a of sentence from its syntactic structure.", "labels": [], "entities": []}, {"text": "These models are fully compositional, whereby the meaning of a phrase is a function of the meaning of its parts; however, as they reduce meaning to logical form, they are not necessarily adapted to all language processing applications such as paraphrase detection, classification, or search, where topical and pragmatic relations maybe more relevant to the task than equivalence of logical form or truth value.", "labels": [], "entities": [{"text": "paraphrase detection, classification", "start_pos": 243, "end_pos": 279, "type": "TASK", "confidence": 0.7183748781681061}]}, {"text": "Furthermore, reducing meaning to logical form presupposes the provision of a logical model and domain in order for the semantic value of expressions to be determined, rendering such models essentially a priori.", "labels": [], "entities": []}, {"text": "In contrast, distributional semantic models, suggested by, implement the linguistic philosophy of stating that meaning is associated with use, and therefore meaning can be learned through the observation of linguistic practises.", "labels": [], "entities": []}, {"text": "In practical terms, such models learn the meaning of words by examining the contexts of their occurrences in a corpus, where 'context' is generally taken to mean the tokens with which words co-occur within a sentence or frame of n tokens.", "labels": [], "entities": []}, {"text": "Such models have been successfully applied to various tasks such as thesaurus extraction and essay grading.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7061547785997391}, {"text": "essay grading", "start_pos": 93, "end_pos": 106, "type": "TASK", "confidence": 0.7580388784408569}]}, {"text": "However, unlike their formal semantics counterparts, distributional models have no explicit canonical composition operation, and provide noway to integrate syntactic information into word meaning combination to produce sentence meanings.", "labels": [], "entities": []}, {"text": "In this paper, we present anew approach to the development of compositional distributional semantic models, based on earlier work by, and, combining features from the compositional distributional framework of the latter two with the learning methods of the former.", "labels": [], "entities": []}, {"text": "In Section 2 we outline a brief history of approaches to compositional distributional semantics.", "labels": [], "entities": []}, {"text": "In Section 3 we overview a tensor-based compositional distributional model resembling traditional formal semantic models.", "labels": [], "entities": []}, {"text": "In Section 4 we present anew multi-step regression algorithm for learning the tensors in this model.", "labels": [], "entities": []}, {"text": "Sections 5-7 present the experimental setup and results of two experiments evaluating our model against other known approaches to compositionality in distributional semantics, followed by an analysis of these results in Section 8.", "labels": [], "entities": []}, {"text": "We conclude in Section 9 by suggesting future work building on the success of the model presented in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the test set of, consisting of 180 pairs of simple sentences made of a subject and an intransitive verb.", "labels": [], "entities": []}, {"text": "The stimuli were constructed so as to ensure that there would be pairs where the sentences have high similarity (the fire glowed vs. the fire burned) and cases where the sentences are dissimilar while having a comparable degree of lexical overlap (the face glowed vs. the face burned).", "labels": [], "entities": []}, {"text": "The sentence pairs were rated for similarity by 49 subjects on a 1-7 scale.", "labels": [], "entities": [{"text": "similarity", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9725243449211121}]}, {"text": "Following Mitchell and Lapata, we evaluate each composition method by the Spearman correlation of the cosines of the sentence pair vectors, as predicted by the method, with the individual ratings produced by the subjects for the corresponding sentence pairs.", "labels": [], "entities": []}, {"text": "The results in table 1(a) show that the Regression-based model achieves the best correlation when applied to SVD space, confirming that the approach proposed by Baroni and Zamparelli for adjectivenoun constructions can be successfully extended to subject-verb composition.", "labels": [], "entities": [{"text": "Regression-based", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.9480584859848022}]}, {"text": "The Regression model also achieves good performance in NMF space, where it is comparable to Multiply.", "labels": [], "entities": [{"text": "Regression", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9383042454719543}]}, {"text": "Multiply was found to be the best model by Mitchell and Lapata, and we confirm their results here (recall that Multiply can also be seen as the natural extension of Kronecker to the intransitive setting).", "labels": [], "entities": []}, {"text": "The correlations attained by Add and Verb are considerably lower than those of the other methods.: Spearman correlation of composition methods with human similarity intuitions on two sentence similarity data sets (all correlations significantly above chance).", "labels": [], "entities": []}, {"text": "The multiplication-based Multiply and Kronecker methods are not well-suited for the SVD space (see Section 5.1) and their performance is reported in NMF space only.", "labels": [], "entities": []}, {"text": "Kronecker is only defined for the transitive case, Multiply functioning also as its intransitive-case equivalent (see Section 5.2).", "labels": [], "entities": []}, {"text": "We use the test set of, which was constructed with the same criteria that Mitchell and Lapata applied, but here the sentences have a simple transitive structure.", "labels": [], "entities": []}, {"text": "An example of a high-similarity pair is table shows result vs. table expresses result; whereas map shows location vs. map expresses location is a low-similarity pair.", "labels": [], "entities": []}, {"text": "Grefenstette and Sadrzadeh had 25 subjects rating each sentence.", "labels": [], "entities": []}, {"text": "Model evaluation proceeds like in the intransitive case.", "labels": [], "entities": []}, {"text": "As the results in table 1(b) show, the Regression model performs very well again, better than any other methods in NMF space, and with a further improvement when SVD is used, similarly to the first experiment.", "labels": [], "entities": [{"text": "Regression", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.97299724817276}]}, {"text": "The Kronecker model is also competitive, confirming the results of Grefenstette and Sadrzadeh's experiments.", "labels": [], "entities": []}, {"text": "Neither Add nor Verb achieve very good results, although even for them the correlation with human ratings is significant.", "labels": [], "entities": [{"text": "Verb", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9112873673439026}]}], "tableCaptions": [{"text": " Table 1: Spearman correlation of composition methods with human similarity intuitions on two sentence  similarity data sets (all correlations significantly above chance). Humans is inter-annotator correlation.  The multiplication-based Multiply and Kronecker methods are not well-suited for the SVD space (see  Section 5.1) and their performance is reported in NMF space only. Kronecker is only defined for the  transitive case, Multiply functioning also as its intransitive-case equivalent (see Section 5.2).", "labels": [], "entities": []}]}