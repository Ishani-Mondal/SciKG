{"title": [{"text": "Automatic Extraction of Linguistic Metaphor with LDA Topic Modeling", "labels": [], "entities": [{"text": "Automatic Extraction of Linguistic Metaphor", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.710391366481781}]}], "abstractContent": [{"text": "We aim to investigate cross-cultural patterns of thought through cross-linguistic investigation of the use of metaphor.", "labels": [], "entities": []}, {"text": "As a first step, we produce a system for locating instances of metaphor in English and Spanish text.", "labels": [], "entities": [{"text": "locating instances of metaphor in English and Spanish text", "start_pos": 41, "end_pos": 99, "type": "TASK", "confidence": 0.801984945933024}]}, {"text": "In contrast to previous work which relies on resources like syntactic parsing and WordNet, our system is based on LDA topic modeling, enabling its application even to low-resource languages, and requires no labeled data.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7562485933303833}, {"text": "WordNet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9414478540420532}]}, {"text": "We achieve an F-score of 59% for English.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.999352753162384}]}], "introductionContent": [{"text": "Patterns in the use of metaphors can provide a great deal of insight into a culture.", "labels": [], "entities": []}, {"text": "Cultural differences expressed linguistically as metaphor can play a role in matters as complex and important as diplomatic relations.", "labels": [], "entities": []}, {"text": "For instance, discusses the different metaphors that are used in the context of security in French and British coverage of two major post-cold-war summit meetings.", "labels": [], "entities": []}, {"text": "Example metaphors such as \"the cornerstone of the new security structure,\" \"structures for defence and security cooperation,\" and \"the emerging shape of Europe,\" exemplify the English use of the source concept structure in describing the target concept of security.", "labels": [], "entities": []}, {"text": "In contrast, the metaphors \"des r\u00e8gles de s\u00e9curit\u00e9 nouvelles (new rules of security)\", \"une r\u00e9vision fondamentale des dispositions de s\u00e9curit\u00e9 (a fundamental revision of security provisions)\", and \"un syst\u00e8me de s\u00e9curit\u00e9 europ\u00e9en (a system of European security)\" exemplify the French use of the more abstract source concept system to describe the same target concept.", "labels": [], "entities": []}, {"text": "As Thornborrow notes, the implied British conception of security as \"concrete, fixed, and immobile\" contrasts deeply with the French conception of security as \"a system as a series of processes.\"", "labels": [], "entities": []}, {"text": "Our ultimate goal is to use metaphor to further our knowledge of how different cultures understand complex topics.", "labels": [], "entities": []}, {"text": "Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text.", "labels": [], "entities": [{"text": "find instances of metaphor in English and Spanish text", "start_pos": 69, "end_pos": 123, "type": "TASK", "confidence": 0.749755409028795}]}, {"text": "Most existing work on metaphor identification;) 1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet) and FrameNet (.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.8918035328388214}, {"text": "WordNet", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.959679126739502}]}, {"text": "This limits the approaches to languages with rich linguistic resources.", "labels": [], "entities": []}, {"text": "As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages.", "labels": [], "entities": []}, {"text": "Instead, we apply LDA topic modeling () which requires only an adequate amount of raw text in the target language.", "labels": [], "entities": [{"text": "LDA topic modeling", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7052412231763204}]}, {"text": "This work is similar to, in which an SVM model is trained with LDA-based features to recognize metaphorical text.", "labels": [], "entities": []}, {"text": "There the work is framed as a classification task, and supervised methods are used to label metaphorical and literal text.", "labels": [], "entities": []}, {"text": "Here, the task is one of recognition, and we use heuristic-based, unsu-pervised methods to identify the presence of metaphor in unlabeled text.", "labels": [], "entities": []}, {"text": "We hope to eliminate the need for labeled data which, as discussed in and elsewhere, is very difficult to produce for metaphor recognition.", "labels": [], "entities": [{"text": "metaphor recognition", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.9233099520206451}]}], "datasetContent": [{"text": "We collected a domain-specific corpus in each language.", "labels": [], "entities": []}, {"text": "We curated a set of news websites and governance-relevant blogs in English and Spanish and then collected data from these websites over the course of several months.", "labels": [], "entities": []}, {"text": "For each language, we ran our system over this corpus (all steps in an American political movement Section 5), produced a set of linguistic metaphors for each topic-aligned source concept (the target concept was always governance), and ranked them by the final score (Section 4.4).", "labels": [], "entities": []}, {"text": "Below, we will refer to the set of all linguistic metaphors sharing the same source and target concept as a conceptual metaphor.", "labels": [], "entities": []}, {"text": "For this evaluation, we selected the top five examples for each conceptual metaphor.", "labels": [], "entities": []}, {"text": "If the same sentence was selected by multiple conceptual metaphors, it was kept for only the highest scoring one.", "labels": [], "entities": []}, {"text": "We then added enough of the highest-ranked unselected metaphors to create a full set of 300.", "labels": [], "entities": []}, {"text": "We then added random sentences from the corpus that were not selected as metaphorical by the system to bring the total to 600.", "labels": [], "entities": []}, {"text": "Our Spanish annotators were unavailable at the time this evaluation took place, so we are only able to report results for English in this case.", "labels": [], "entities": []}, {"text": "For each of these instances, two annotators were asked the question, \"Is there a metaphor about governance in this example?\"", "labels": [], "entities": []}, {"text": "These annotators had previous experience in identifying metaphors for this study, both by searching manually in online texts and evaluating previous versions of our system.", "labels": [], "entities": []}, {"text": "Over time we have given them feedback on what does and does not constitute a metaphor.", "labels": [], "entities": []}, {"text": "In this case, the annotators were given neither the system's concept-word association annotations nor the source concept associated with the instance.", "labels": [], "entities": []}, {"text": "In one way, the evaluation was generous, because any metaphor in the extracted sentence would benefit precision even if it was not the metaphor found by our system.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9987234473228455}]}, {"text": "On the other hand, the same is true for the random sentences; while the system will only extract metaphors with source concepts in our list, the annotators had no such restriction.", "labels": [], "entities": []}, {"text": "This causes the recall score to suffer.", "labels": [], "entities": [{"text": "recall score", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.9885401725769043}]}, {"text": "The annotation task was difficult, with a -score of 0.48.", "labels": [], "entities": [{"text": "-score", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.8414615988731384}]}, {"text": "The resulting scores are given in.", "labels": [], "entities": []}, {"text": "The examples given in Section 5 illustrate the error classes found among the false positives identified by the human annotators.", "labels": [], "entities": []}, {"text": "There are many cases where the source-concept associated terms are used literally rather than metaphorically, and many cases where the system-found metaphor is not about governance.", "labels": [], "entities": []}, {"text": "Some text processing issues, such as a bug in our sentence breaking script, as well as the noisy nature of blog and blog comment input, caused some of the examples to be difficult to interpret or evaluate.", "labels": [], "entities": []}, {"text": "Annotator   We did a second evaluation of both English and Spanish using a different paradigm.", "labels": [], "entities": [{"text": "Annotator", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.871935248374939}]}, {"text": "For each language, we selected the 250 highest-ranked linguistic metaphor instances in the corpus.", "labels": [], "entities": []}, {"text": "Subjects on Amazon Mechanical Turk were shown instances with the system-predicted concept-associated words highlighted and asked if the highlighted words were being used metaphorically (options were yes and no).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 12, "end_pos": 34, "type": "DATASET", "confidence": 0.9553744395573934}]}, {"text": "Each subject was randomly asked about roughly a quarter of the data.", "labels": [], "entities": []}, {"text": "We paid the subjects $10 per hour.", "labels": [], "entities": []}, {"text": "We added catch trial sentences which asked the subject to simply answer yes or no as away of excluding those not actually reading the sentences.", "labels": [], "entities": []}, {"text": "Subjects answering these questions incorrectly were excluded (17 in English, 25 in Spanish).", "labels": [], "entities": []}, {"text": "We defined the metaphoricity of an instance to be the fraction of subjects who answered yes for that instance.", "labels": [], "entities": []}, {"text": "We define the metaphoricity of a conceptual metaphor as the average metaphoricity of its groundings among the instances in this evaluation set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3.  The examples given in Section 5 illustrate the error  classes found among the false positives identified", "labels": [], "entities": []}, {"text": " Table 3: Simple English Evaluation", "labels": [], "entities": [{"text": "Simple English Evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5588244001070658}]}]}