{"title": [{"text": "Using a Random Forest Classifier to recognise translations of biomedical terms across languages", "labels": [], "entities": [{"text": "recognise translations of biomedical terms", "start_pos": 36, "end_pos": 78, "type": "TASK", "confidence": 0.7644458651542664}]}], "abstractContent": [{"text": "We present a novel method to recognise semantic equivalents of biomedical terms in language pairs.", "labels": [], "entities": [{"text": "recognise semantic equivalents of biomedical terms in language pairs", "start_pos": 29, "end_pos": 97, "type": "TASK", "confidence": 0.7670512066947089}]}, {"text": "We hypothesise that biomedical term are formed by semantically similar textual units across languages.", "labels": [], "entities": []}, {"text": "Based on this hypothesis, we employ a Random Forest (RF) classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples.", "labels": [], "entities": []}, {"text": "We apply our method on two language pairs: one that uses the same character set and another with a different script, English-French and English-Chinese, respectively.", "labels": [], "entities": []}, {"text": "We show that English-French pairs of terms are highly transliterated in contrast to the English-Chinese pairs.", "labels": [], "entities": []}, {"text": "Nonetheless, our method performs robustly on both cases.", "labels": [], "entities": []}, {"text": "We evaluate RF against a state-of-the-art alignment method, GIZA++, and we report a statistically significant improvement.", "labels": [], "entities": [{"text": "RF", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.6205072402954102}]}, {"text": "Finally , we compare RF against Support Vector Machines and analyse our results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a term in a source language and term in a target language the task of this paper is to classify this pair as a translation or not.", "labels": [], "entities": []}, {"text": "We investigate the performance of the proposed classifier by applying it on a balanced classification problem, i.e. our experimental datasets contain an equal number of positive and negative examples.", "labels": [], "entities": []}, {"text": "The proposed classification model can be used as a component of a larger system that automatically compiles bilingual dictionaries of technical terms across languages.", "labels": [], "entities": []}, {"text": "Bilingual dictionaries of terms are important resources for many Natural Language Processing (NLP) applications including Statistical Machine Translation (SMT), CrossLanguage Information Retrieval ( and Question Answering systems.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 122, "end_pos": 159, "type": "TASK", "confidence": 0.8100989361604055}, {"text": "CrossLanguage Information Retrieval", "start_pos": 161, "end_pos": 196, "type": "TASK", "confidence": 0.6818417410055796}, {"text": "Question Answering", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.8457351326942444}]}, {"text": "Especially in the biomedical domain, manually creating and more importantly updating such resources is an expensive process, due to the vast amount of neologisms, i.e. newly introduced terms ().", "labels": [], "entities": []}, {"text": "The UMLS metathesaurus which is one the most popular hub of multilingual resources in the biomedical domain, contains technical terms in 21 languages that are linked together using a concept identifier.", "labels": [], "entities": [{"text": "UMLS metathesaurus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9453809559345245}]}, {"text": "In Spanish, the second most popular language in UMLS, only 16.44% of the 7.6M English terms are covered while other languages fluctuate between 0.0052% (for Hebrew terms) to 3.26% (for Japanese terms).", "labels": [], "entities": []}, {"text": "Hence, these lexica are far for complete and methods that semiautomatically (i.e., in a post-processing step, curators can manually remove erroneous dictionary entries) discover pairs of terms across languages are needed to enrich such multilingual resources.", "labels": [], "entities": []}, {"text": "Our method can be applied to parallel, aligned corpora, where we expect approximately the same, balanced classification problem.", "labels": [], "entities": []}, {"text": "However, in comparable corpora the search space of candidate alignments is of vast size, i.e., quadratic the the size of the input data.", "labels": [], "entities": []}, {"text": "To cope with this heavily unbalanced classification problem, we would need to narrow down the number of negative instances before classification.", "labels": [], "entities": []}, {"text": "We hypothesise that there are language independent rules that apply to biomedical terms across many languages.", "labels": [], "entities": []}, {"text": "Often the same or similar textual units (e.g., morphemes and suffixes) are concatenated to realise the same terms in different languages.", "labels": [], "entities": []}, {"text": "For example, illustrates how a morpheme expressing pain (ache in English) is used to realise the same terms in English, Chinese and French.", "labels": [], "entities": []}, {"text": "The realisations of the term \"head-English Morpheme: -ache Chinese Morpheme:: An example of English, Chinese and French terms consisting of the same morphemes ache\" is expected to consist of the units for \"head\" and \"ache\" regardless of the language of realisation.", "labels": [], "entities": []}, {"text": "Hence, knowing the translations of \"head\" and \"ache\" allows the reconstruction \"headache\" in a target language.", "labels": [], "entities": []}, {"text": "In our method, we use a Random Forest (RF) classifier) to learn the underlying rules according to which terms are being constructed across languages.", "labels": [], "entities": []}, {"text": "An RF is an ensemble of Decision Trees voting for the most popular class.", "labels": [], "entities": []}, {"text": "RF classifiers are popular in the biomedical domain for various tasks: classification of microarray data), compound classification in cheminformatics (, classification of microRNA data) and protein-protein interactions in Systems Biology (Chen and).", "labels": [], "entities": [{"text": "RF classifiers", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7678066790103912}, {"text": "compound classification", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.735772043466568}, {"text": "Systems Biology", "start_pos": 222, "end_pos": 237, "type": "TASK", "confidence": 0.8246535062789917}]}, {"text": "In NLP, RF classifiers have been used for: Language Modelling () and semantic parsing ().", "labels": [], "entities": [{"text": "Language Modelling", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.6958464235067368}, {"text": "semantic parsing", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7505391836166382}]}, {"text": "To the best of the authors' knowledge, this is the first attempt to employ RF for identifying translation equivalents of biomedical terms.", "labels": [], "entities": [{"text": "identifying translation equivalents of biomedical terms", "start_pos": 82, "end_pos": 137, "type": "TASK", "confidence": 0.8182769119739532}]}, {"text": "We prefer RF over other traditional machine learning approaches such as Support Vector Machines (SVMs) fora number of reasons.", "labels": [], "entities": [{"text": "RF", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9567232728004456}]}, {"text": "Firstly, RF is able to automatically construct correlation paths from the feature space, i.e. decision rules that correspond to the translation rules that we intend to capture.", "labels": [], "entities": []}, {"text": "Secondly, RF is considered one of the most accurate classifier available.", "labels": [], "entities": [{"text": "RF", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9759261012077332}]}, {"text": "Finally, RF is reported to cope well with datasets where the number of features is larger than the number of observations).", "labels": [], "entities": [{"text": "RF", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.7755513787269592}]}, {"text": "In our dataset, the number of features is almost four times more than that of the observations.", "labels": [], "entities": []}, {"text": "We represent pairs of terms using character gram features (i.e., first order features).", "labels": [], "entities": []}, {"text": "Such shallow features have been proven effective in a number of NLP applications including: Named Entity Recognition (, Multilingual Named Entity Transliteration ( and predicting authorship).", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.649040162563324}, {"text": "predicting authorship", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.8837718963623047}]}, {"text": "In addition, by selecting character n-grams instead of word n-grams, one avoids to segment words in Chinese which has been proven to be a challenging topic.", "labels": [], "entities": []}, {"text": "We evaluate our proposed method on two datasets of biomedical terms (English-French and English-Chinese) that contain equal numbers of positive and negative instances.", "labels": [], "entities": []}, {"text": "RF achieves higher classification performance than baseline methods.", "labels": [], "entities": [{"text": "RF", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.584690272808075}]}, {"text": "To boost SVM's performance further, we used a second order feature space to represent the data.", "labels": [], "entities": []}, {"text": "It consists of pairs of character grams that co-occur in translation pairs.", "labels": [], "entities": []}, {"text": "In the second order feature space, the performance of SVMs improved significantly.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present previous approaches in identifying translation equivalents of terms or named entities.", "labels": [], "entities": [{"text": "identifying translation equivalents of terms or named entities", "start_pos": 48, "end_pos": 110, "type": "TASK", "confidence": 0.8526294827461243}]}, {"text": "In Section 3, we define the classification problem, we formulate the RF classifier and we discuss the first and second order feature space that we use to represent pairs of terms.", "labels": [], "entities": []}, {"text": "In Section 4, we show that RF achieves superior classification performance.", "labels": [], "entities": [{"text": "RF", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.6973792314529419}]}, {"text": "In Section 5, we overview our method and we discuss how it can be used to compile large-scale bilingual dictionaries of terms from comparable corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss the employed datasets of biomedical terms in English-French and English-Chinese and three baseline methods.", "labels": [], "entities": []}, {"text": "We compare and discuss RF and SVMs trained on the first order and second order features.", "labels": [], "entities": []}, {"text": "Finally, we report results of all classification methods evaluated on the same datasets.", "labels": [], "entities": []}, {"text": "For our experiments, we used an online bilingual dictionary 2 for English-Chinese terms and the UMLS metathesaurus 3 for English-French terms.", "labels": [], "entities": [{"text": "UMLS metathesaurus 3", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.9254930218060812}]}, {"text": "The former contains 31, 700 entries while the latter is a much larger dictionary containing 84, 000 entries.", "labels": [], "entities": []}, {"text": "For training, we used the same number of instances for both language pairs (i.e., 21, 000 entries) in order not to bias the performance towards the larger English-French dataset.", "labels": [], "entities": []}, {"text": "The remaining instances were used for testing (i.e., 10, 7000 and 63, 000 English-Chinese and English-French respectively).", "labels": [], "entities": []}, {"text": "In the case where a source term corresponded to more that one target terms according to the seed dictionary, we randomly selected only one translation.", "labels": [], "entities": []}, {"text": "Negative instances were created by randomly matching non-translation pairs of terms.", "labels": [], "entities": []}, {"text": "Since we are dealing with a balanced clas-: Example of a term construction rule as a branch in a decision tree.", "labels": [], "entities": []}, {"text": "Input pair of English-French terms : (e 1 , e 2 , e 3 , f 1 , f 2 , f 3 ) English first order French first order Second order: Example of first and second order features using a predefined n-gram size of 2.", "labels": [], "entities": []}, {"text": "sification problem, we created as many negative instances as the positive ones in all our datasets.", "labels": [], "entities": []}, {"text": "In all experiments we performed a 3-fold crossvalidation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance", "labels": [], "entities": [{"text": "RF", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9057319760322571}, {"text": "GIZA", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9713049530982971}, {"text": "Levenshtein Distance", "start_pos": 62, "end_pos": 82, "type": "METRIC", "confidence": 0.6717333793640137}]}]}