{"title": [{"text": "A Search Task Dataset for German Textual Entailment", "labels": [], "entities": [{"text": "German Textual Entailment", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6797489921251932}]}], "abstractContent": [{"text": "We present the first freely available large German dataset for Textual Entailment (TE).", "labels": [], "entities": [{"text": "Textual Entailment (TE)", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.808268016576767}]}, {"text": "Our dataset builds on posts from German online forums concerned with computer problems and models the task of identifying relevant posts for user queries (i.e., descriptions of their computer problems) through TE.", "labels": [], "entities": [{"text": "TE", "start_pos": 210, "end_pos": 212, "type": "DATASET", "confidence": 0.45936059951782227}]}, {"text": "We use a sequence of crowdsourcing tasks to create realistic problem descriptions through summarisation and paraphrasing of forum posts.", "labels": [], "entities": []}, {"text": "The dataset is represented in RTE-5 Search task style and consists of 172 positive and over 2800 negative pairs.", "labels": [], "entities": [{"text": "RTE-5", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.6209790110588074}]}, {"text": "We analyse the properties of the created dataset and evaluate its difficulty by applying two TE algorithms and comparing the results with results on the English RTE-5 Search task.", "labels": [], "entities": [{"text": "TE", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9900755286216736}, {"text": "English RTE-5 Search task", "start_pos": 153, "end_pos": 178, "type": "DATASET", "confidence": 0.7089422047138214}]}, {"text": "The results show that our dataset is roughly comparable to the RTE-5 data in terms of both difficulty and balancing of positive and negative entailment pairs.", "labels": [], "entities": [{"text": "RTE-5 data", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8992056548595428}, {"text": "difficulty", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9601860642433167}]}, {"text": "Our approach to create task-specific TE datasets can be transferred to other domains and languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Textual Entailment (TE) is a binary relation between two utterances, a Text T and a Hypothesis H, which holds if \"a human reading T would infer that H is most likely true\").", "labels": [], "entities": [{"text": "Textual Entailment (TE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8169296443462372}]}, {"text": "Example 1 shows a positive entailment (T entails H 1 ) and a negative entailment (T does not entail H 2 ).", "labels": [], "entities": []}, {"text": "(1) T: Yoko Ono unveiled a bronze statue of her late husband, John Lennon, to complete the official renaming of England's Liverpool Airport as Liverpool John Lennon Airport.", "labels": [], "entities": []}, {"text": "H 1 : Yoko Ono is John Lennon's widow.", "labels": [], "entities": []}, {"text": "H 2 : John Lennon renamed Liverpool Airport.", "labels": [], "entities": [{"text": "Liverpool Airport", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9369959235191345}]}, {"text": "The appeal of Textual Entailment is that it can arguably meet a substantial part of the semantic processing requirements of a range of language processing tasks such as Question Answering (), Information Extraction (), or Summarisation (.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8141853213310242}, {"text": "Question Answering", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.8096494078636169}, {"text": "Information Extraction", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.7937749028205872}, {"text": "Summarisation", "start_pos": 222, "end_pos": 235, "type": "TASK", "confidence": 0.9865735769271851}]}, {"text": "Consequently, there is now a research community that works on and improves Textual Entailment technology.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8499231934547424}]}, {"text": "In this spirit, the main TE forum, the yearly Recognising Textual Entailment (RTE) Challenge, has created a number of datasets that incorporate the properties of particular tasks, such as Semantic Search in RTE-5 ( ) or Novelty Detection in RTE-7).", "labels": [], "entities": [{"text": "TE", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9000079035758972}, {"text": "Recognising Textual Entailment (RTE) Challenge", "start_pos": 46, "end_pos": 92, "type": "TASK", "confidence": 0.7174110753195626}, {"text": "RTE-5", "start_pos": 207, "end_pos": 212, "type": "DATASET", "confidence": 0.8021997213363647}, {"text": "Novelty Detection in RTE-7", "start_pos": 220, "end_pos": 246, "type": "TASK", "confidence": 0.6891024261713028}]}, {"text": "At the same time, work on RTE on has focused almost exclusively on English.", "labels": [], "entities": [{"text": "RTE on", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.7830497026443481}]}, {"text": "There is at most a handful of studies on Textual Entailment in other languages, notably German and Italian () as well as a study on cross-lingual entailment.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7792614996433258}]}, {"text": "Consequently, virtually no TE technology is available for non-English languages.", "labels": [], "entities": [{"text": "TE", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.6364452242851257}]}, {"text": "What is more, it is not clear how well existing algorithms for English RTE carryover to other languages, which might show very different types of surface variation from English.", "labels": [], "entities": []}, {"text": "The same limitation exists in terms of genre/register.", "labels": [], "entities": []}, {"text": "Virtually all existing datasets have been created from \"clean\" corpora -that is, properly tokenised, grammatical text, notably Wikipedia.", "labels": [], "entities": []}, {"text": "Again, the question arises how well TE algorithms would do on noisier genres like transcribed speech or user-generated content.", "labels": [], "entities": [{"text": "TE", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.7850830554962158}]}, {"text": "Arguably, it would benefit the community to have a larger variety of datasets at hand for such investigations.", "labels": [], "entities": []}, {"text": "This paper reports our creation and analysis of a German dataset for TE that is derived from social media data, as is produced everyday on a large scale by of non-professional web users.", "labels": [], "entities": [{"text": "German dataset for TE", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.729791447520256}]}, {"text": "This type of data respects linguistic norms such as spelling and grammar less than traditional textual entailment datasets (, which present challenges to semantic processing.", "labels": [], "entities": []}, {"text": "We concentrate on a search task on a computer user forum that deals with computer problems: given a problem statement formulated by a user, identify all relevant forum threads that describe this problem.", "labels": [], "entities": []}, {"text": "We created queries fora sample of forum threads by crowdsourcing.", "labels": [], "entities": []}, {"text": "We asked annotators to summarise the threads and to paraphrase the summaries to achieve high syntactic and lexical variability.", "labels": [], "entities": []}, {"text": "The resulting summaries can be understood as queries (problem statements) corresponding to the original posts.", "labels": [], "entities": []}, {"text": "The search for relevant posts given a query can be phrased as a TE problem as follows: queries are hypotheses that are entailed by forum posts (texts) T iff the forum post is relevant for the query.", "labels": [], "entities": []}, {"text": "Section 2 defines the task in more detail and describes the rationale behind our definition of the crowdsourcing tasks.", "labels": [], "entities": []}, {"text": "Section 3 provides a detailed analysis of the queries that were produced by crowdsourcing.", "labels": [], "entities": []}, {"text": "Section 4 assesses the difficulty of the dataset by modelling it with the RTE system EDITS (.", "labels": [], "entities": [{"text": "RTE system EDITS", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.68719349304835}]}, {"text": "Finally we relate our study to prior work and sum up.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each T/H pair, Task 3 provides us with three judgements on an ordinal scale with three categories: perfect summary (\"ps\"), incomplete summary (\"is\"), no summary (\"ns\").", "labels": [], "entities": []}, {"text": "The next question is how to select cases of true entailment and true non-entailment from this dataset.", "labels": [], "entities": []}, {"text": "As for entailment, we start by discarding all pairs that were tagged as \"ns\" by at least one rater.", "labels": [], "entities": []}, {"text": "The situation is less clear for \"is\" cases: on one hand, hypotheses can drop information Assessments ps-ps-ps ps-ps-is ps-is-is is-is-is ns-ns-ns ns-ns-is ns-is-: Association between AMT assessments and final entailment relations present in the text while preserving entailment; on the other hand, the absence of important information in the summary can indicate difficulties with the original text or the summary.", "labels": [], "entities": [{"text": "AMT", "start_pos": 183, "end_pos": 186, "type": "TASK", "confidence": 0.952113151550293}]}, {"text": "Thus, to keep precision high, we decided to manually check all \"is\"/\"ps\" T/H pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9995540976524353}]}, {"text": "The left-hand part of shows that in fact, the ratio of proper entailments trails off from almost 100% for \"ps-ps-ps\" to about one third for \"is-is-is\".", "labels": [], "entities": []}, {"text": "In total, we obtained 127 positive entailment pairs in this manner.", "labels": [], "entities": []}, {"text": "During the extraction, we noted that one of the 23 forum posts did not yield reliable assessments for any of its generated hypotheses and discarded it.", "labels": [], "entities": []}, {"text": "Negative entailment pairs come from two sources.", "labels": [], "entities": [{"text": "Negative entailment", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9173619151115417}]}, {"text": "First, \"ns\" T/H pairs are cases where turkers missed the semantic core of the original text.", "labels": [], "entities": []}, {"text": "These cases might be particularly informative non-entailment pairs because they are near the decision boundary.", "labels": [], "entities": []}, {"text": "For example, one author asks whether a virus can settle down on the motherboard.", "labels": [], "entities": []}, {"text": "The corresponding generated hypothesis turned the question into a fact, stating that \"My BIOS has been infected by a virus.\".", "labels": [], "entities": []}, {"text": "Again, we checked all pairs with at least one \"ns\" judgement by hand.", "labels": [], "entities": []}, {"text": "As the right-hand side of shows, we find the same pattern as for positive pairs: perfect non-entailment for instances with perfect agreement on \"ns\", and lower non-entailment ratio for increasing \"is\" ratio.", "labels": [], "entities": []}, {"text": "Rejected pairs are e.g. very generic and fuzzy summaries or refer only to a minor aspect of the problem described in the forum.", "labels": [], "entities": []}, {"text": "Unfortunately, this strategy only results in 10 negative entailment T/H pairs.", "labels": [], "entities": []}, {"text": "The second source of negative pairs are combinations of verified Hs with \"other\" Ts, that is, Ts from which they were not created.", "labels": [], "entities": []}, {"text": "In fact, we can pair each of the 137 validated distinct Hs with all other Ts, resulting in 21 * 137 = 2877 additional non-entailment T/H pairs.", "labels": [], "entities": []}, {"text": "However, since the domain of computer problems is relatively narrow, a few post topics are so close to each other that generated hypotheses are entailed by multiple texts.", "labels": [], "entities": []}, {"text": "While this effect is usually ignored in machine learning (, our goal is a clean dataset.", "labels": [], "entities": []}, {"text": "Therefore, we manually checked all cross-pairs with similar topics (e.g. virus attacks) for positive entailment relations.", "labels": [], "entities": []}, {"text": "Indeed, we found hypotheses which were general enough to match other texts.", "labels": [], "entities": []}, {"text": "We removed 45 such pairs from the negative entailment pairs and added them to the set of positive pairs.", "labels": [], "entities": []}, {"text": "In total, we obtained 172 positive and 2842 negative entailment T/H pairs for 22 Ts and 137 distinct Hs.", "labels": [], "entities": []}, {"text": "At a cost of 82 USD, this corresponds to an average of 50 cents for each explicitly generated positive pair, but just 3 cents for each T/H pair in the complete dataset.", "labels": [], "entities": []}, {"text": "From the 226 AMT-generated pairs, we use 56% as positive pairs and 4% as negative pairs.", "labels": [], "entities": []}, {"text": "We discard the remaining, inconsistently judged, 40%.", "labels": [], "entities": []}, {"text": "In order to evaluate the difficulty of the dataset that we have created, we performed experiments with two different TE engines.", "labels": [], "entities": []}, {"text": "We split our dataset into a development and a test set.", "labels": [], "entities": []}, {"text": "Both sets are identical in terms of size (1507 T/H pairs) and amount of positive and negative pairs (86 and 1421 pairs, respectively).", "labels": [], "entities": []}, {"text": "The first system is EDITS (Negri et al., 2009), version 3.0. 3 EDITS uses string edit distance as a proxy of semantic similarity between T and H and classifies pairs as entailing if their normalised edit distance is below a threshold \u03b8 which can be optimised on a development set.", "labels": [], "entities": []}, {"text": "While additional entailment knowledge can be included, no such knowledge is currently available for German and we use the default weights.", "labels": [], "entities": []}, {"text": "The second system is a simple word overlap strategy which approximates semantic similarity through the fraction of H words that also occur in T (Monz and de).", "labels": [], "entities": [{"text": "word overlap", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.7143460363149643}]}, {"text": "Again, pairs are classified as entailing if this fraction is larger than a threshold \u03b8.", "labels": [], "entities": []}, {"text": "We preprocessed the data by lemmatising it with TreeTagger ( and removing stop words, employing a German stop word list which includes keywords from the social media domain.", "labels": [], "entities": []}, {"text": "The thresholds \u03b8 for both systems were set by optimising the F 1 score for positive entailment on the train set.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9850718379020691}]}, {"text": "shows the results for the word overlap model and EDITS.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.7211378365755081}, {"text": "EDITS", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.8345717787742615}]}, {"text": "The very high accuracy values merely reflect the predominance of the negative entailment class; we therefore concentrate on the F-score statistics for positive entailment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9983891248703003}, {"text": "F-score", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.9927829504013062}]}, {"text": "We find that edit distance outperforms word overlap with F 1 scores of .44 and .38, respectively.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.7487221360206604}, {"text": "F 1 scores", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9895921945571899}]}, {"text": "Since the main difference between the two approaches is that edit distance is sensitive to word order, order information appears to be indeed informative: reordering between T and H do not incur costs in the word overlap model, but they do in the edit distance model.", "labels": [], "entities": []}, {"text": "Example 3 shows a T/H pair with high word overlap, but negative entailment.", "labels": [], "entities": []}, {"text": "It is correctly classified by EDITS, but misclassified by the word overlap model.", "labels": [], "entities": [{"text": "EDITS", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.920839250087738}]}], "tableCaptions": [{"text": " Table 1: Confusion matrix for pairs of AMT validation annotations", "labels": [], "entities": [{"text": "AMT validation annotations", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.8942349751790365}]}, {"text": " Table 2: Association between AMT assessments and final entailment relations", "labels": [], "entities": [{"text": "AMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9698160290718079}]}, {"text": " Table 5: Test set results on social media dataset for two simple Textual Entailment algorithms", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7785064876079559}]}]}