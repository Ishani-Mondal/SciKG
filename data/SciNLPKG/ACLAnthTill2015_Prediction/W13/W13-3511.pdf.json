{"title": [], "abstractContent": [{"text": "Hidden Markov tree models for semantic class induction\u00c9douard induction\u00b4induction\u00c9douard Grave Abstract In this paper, we propose anew method for semantic class induction.", "labels": [], "entities": [{"text": "semantic class induction", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.672575851281484}]}, {"text": "First, we introduce a generative model of sentences, based on dependency trees and which takes into account homonymy.", "labels": [], "entities": []}, {"text": "Our model can thus be seen as a generalization of Brown clustering.", "labels": [], "entities": []}, {"text": "Second, we describe an efficient algorithm to perform inference and learning in this model.", "labels": [], "entities": []}, {"text": "Third, we apply our proposed method on two large datasets (10 8 tokens, 10 5 words types), and demonstrate that classes induced by our algorithm improve performance over Brown clustering on the task of semi-supervised supersense tagging and named entity recognition.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.6967944502830505}, {"text": "named entity recognition", "start_pos": 241, "end_pos": 265, "type": "TASK", "confidence": 0.5967814028263092}]}], "introductionContent": [{"text": "Most competitive learning methods for computational linguistics are supervised, and thus require labeled examples, which are expensive to obtain.", "labels": [], "entities": []}, {"text": "Moreover, those techniques suffer from data scarcity: many words only appear a small number of time, or even not at all, in the training data.", "labels": [], "entities": []}, {"text": "It thus helps a lotto first learn word clusters on a large amount of unlabeled data, which are cheap to obtain, and then to use this clusters as features for the supervised task.", "labels": [], "entities": []}, {"text": "This scheme has proven to be effective for various tasks such as named entity recognition, syntactic chunking ( or syntactic dependency parsing (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.6621437867482504}, {"text": "syntactic chunking", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7558551132678986}, {"text": "syntactic dependency parsing", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.6900462508201599}]}, {"text": "It was also successfully applied for transfer learning of multilingual structure by.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.9447981119155884}]}, {"text": "The most commonly used clustering method for semi-supervised learning is the one proposed by, and known as Brown clustering.", "labels": [], "entities": []}, {"text": "While still being one of the most efficient word representation method (, Brown clustering has two limitations we want to address in this work.", "labels": [], "entities": [{"text": "word representation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7426797747612}]}, {"text": "First, since it is a hard clustering method, homonymy is ignored.", "labels": [], "entities": []}, {"text": "Second, it does not take into account syntactic relations between words, which seems crucial to induce semantic classes.", "labels": [], "entities": []}, {"text": "Our goal is thus to propose a method for semantic class induction which takes into account both syntax and homonymy, and then to study their effects on semantic class learning.", "labels": [], "entities": [{"text": "semantic class induction", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7743102510770162}, {"text": "semantic class learning", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.6537791887919108}]}, {"text": "In this paper, we start by introducing anew unsupervised method for semantic classes induction.", "labels": [], "entities": [{"text": "semantic classes induction", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.6852797667185465}]}, {"text": "This is achieved by defining a generative model of sentences with latent variables, which aims at capturing semantic roles of words.", "labels": [], "entities": []}, {"text": "We require our method to be scalable, in order to learn models on large datasets containing tens of millions of sentences.", "labels": [], "entities": []}, {"text": "More precisely, we make the following contributions: \u2022 We introduce a generative model of sentences, based on dependency trees, which can be seen as a generalization of Brown clustering, \u2022 We describe a fast approximate inference algorithm, based on message passing and online EM for scaling to large datasets.", "labels": [], "entities": []}, {"text": "It allowed us to learn models with 512 latent states on a dataset with hundreds of millions of tokens in less than two days on a single core, \u2022 We learn models on two datasets, Wikipedia articles about musicians and the NYT corpus, and evaluate them on two semi-supervised tasks, namely supersense tagging and named entity recognition.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 220, "end_pos": 230, "type": "DATASET", "confidence": 0.9702582359313965}, {"text": "supersense tagging", "start_pos": 287, "end_pos": 305, "type": "TASK", "confidence": 0.7863418757915497}, {"text": "named entity recognition", "start_pos": 310, "end_pos": 334, "type": "TASK", "confidence": 0.6467861235141754}]}], "datasetContent": [{"text": "In this section, we present the datasets used for the experiments, and the two semi-supervised tasks on which we evaluate our models: named entity recognition and supersense tagging.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.6301716566085815}, {"text": "supersense tagging", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.7855990827083588}]}, {"text": "We considered two datasets: the first one, which we refer to as the music dataset, corresponds to all the Wikipedia articles refering to a musical artist.", "labels": [], "entities": []}, {"text": "They were extracted using the Freebase database . This dataset comprises 2.22 millions sentences and 56 millions tokens.", "labels": [], "entities": [{"text": "Freebase database", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9773266315460205}]}, {"text": "We choose this dataset because it corresponds to a restricted domain.", "labels": [], "entities": []}, {"text": "The second dataset are the articles of the NYT corpus corresponding to the period 1987-1997 and labeled as news.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.930379182100296}]}, {"text": "This dataset comprises 14.7 millions sentences and 310 millions tokens.", "labels": [], "entities": []}, {"text": "We parsed both datasets using the Stanford parser, and converted parse trees to dependency trees).", "labels": [], "entities": []}, {"text": "We decided to discard sentences longer than 50 tokens, for parsing time reasons, and then lemmatized tokens using Wordnet.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.988010823726654}]}, {"text": "Each word of our vocabulary is then a pair of lemma and its associated part-of-speech.", "labels": [], "entities": []}, {"text": "This means that the noun attack and the verb attack are two different words.", "labels": [], "entities": []}, {"text": "Finally, we introduced a special token, -*-, for infrequent (lemma, part-of-speech) pairs, in order to perform smoothing.", "labels": [], "entities": []}, {"text": "For the music dataset, we kept the 25 000 most frequent words, while for the NYT corpus, we kept the 100 000 most frequent words.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9708080291748047}]}, {"text": "For the music dataset we set the number of latent states to 256, while we set it to 512 for the NYT corpus.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9742238223552704}]}], "tableCaptions": [{"text": " Table 3: Randomly selected semantic classes corresponding to the news dataset.", "labels": [], "entities": [{"text": "news dataset", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.7367882281541824}]}, {"text": " Table 4: Results of semi-supervised named entity  recognition.", "labels": [], "entities": [{"text": "semi-supervised named entity  recognition", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.5794127136468887}]}]}