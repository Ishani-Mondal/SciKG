{"title": [{"text": "Relation Annotation for Understanding Research Papers", "labels": [], "entities": [{"text": "Relation Annotation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9114610850811005}]}], "abstractContent": [{"text": "We describe anew annotation scheme for formalizing relation structures in research papers.", "labels": [], "entities": []}, {"text": "The scheme has been developed through the investigation of computer science papers.", "labels": [], "entities": []}, {"text": "Using the scheme, we are building a Japanese corpus to help develop information extraction systems for digital libraries.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7149029076099396}]}, {"text": "We report on the outline of the annotation scheme and on annotation experiments conducted on research abstracts from the IPSJ Journal.", "labels": [], "entities": [{"text": "IPSJ Journal", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.9832937121391296}]}], "introductionContent": [{"text": "Present day researchers need services for searching research papers.", "labels": [], "entities": []}, {"text": "Search engines and publishing companies provide specialized search services, such as Google Scholar, Microsoft Academic Search, and Science Direct.", "labels": [], "entities": []}, {"text": "Academic societies provide archives of journal articles and/or conference proceedings such as the ACL Anthology.", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9404264986515045}]}, {"text": "These services focus on simple keywordbased searches as well as extralinguistic relations among research papers, authors, and research topics.", "labels": [], "entities": []}, {"text": "However, because contemporary research is becoming increasingly complicated and interrelated, intelligent content-based search systems are desired.", "labels": [], "entities": []}, {"text": "A typical query in computational linguistics could be what tasks have CRFs been used for?, which includes the elements of atypical schema for searching research papers; researchers want to find relationships between a technique and its applications).", "labels": [], "entities": []}, {"text": "Answers to this query can be found in various forms in published papers, for example, (1) CRF-based POS tagging has achieved state-ofthe-art accuracy.", "labels": [], "entities": [{"text": "CRF-based POS tagging", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.5596917867660522}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9976747632026672}]}, {"text": "(2) CRFs have been successfully applied to sequence labeling problems including POS tagging and named entity recognition.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.8572509586811066}, {"text": "named entity recognition", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7049870093663534}]}, {"text": "(3) We apply feature reduction to CRFs and show its effectiveness in POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.8865481913089752}]}, {"text": "(4) This study proposes anew method for the efficient training of CRFs.", "labels": [], "entities": []}, {"text": "The proposed method is evaluated for POS tagging tasks.", "labels": [], "entities": [{"text": "POS tagging tasks", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8987705906232198}]}, {"text": "Note that the same semantic relation, i.e., the use of CRFs for POS tagging, is expressed by various syntactic constructs: internal structures of the phrase in (1), clause-level structures in (2), interclause structures in (3), and discourse-level structures in (4).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.8236654996871948}]}, {"text": "This implies that an integrated framework is required to represent semantic relations for phrase-level, clause-level, inter-clause level, and discourse-level structures.", "labels": [], "entities": []}, {"text": "Another interesting fact is that we can recognize various fragments of information from single texts.", "labels": [], "entities": []}, {"text": "For example, from sentence (1), we can identify CRF is applied to POS tagging, state-of-the-art accuracy is achieved for POS tagging, and CRFs achieve high POS tagging accuracy, all of which is valuable content for different search requests.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.739487886428833}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9958173632621765}, {"text": "POS tagging", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.7233712375164032}, {"text": "POS tagging", "start_pos": 156, "end_pos": 167, "type": "TASK", "confidence": 0.6533020734786987}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.8368698954582214}]}, {"text": "This indicates that we need a framework that can cover (almost) all content in a text.", "labels": [], "entities": []}, {"text": "In this paper we describe anew annotation scheme for formalizing typical schemas for representing relations among concepts in research papers, such as techniques, resources, and effects.", "labels": [], "entities": []}, {"text": "Our study aims to establish a framework for representing the semantics of research papers to help construct intelligent search systems.", "labels": [], "entities": []}, {"text": "In particular, we focus on the formalization of typical schemas that we believe exemplify common query characteristics.", "labels": [], "entities": []}, {"text": "From the above observations, we have developed the following criteria for our proposed framework: use the same scheme for annotating contents in all levels of linguistic structures, annotate (almost) all contents presented in texts, and capture relations necessary for surveying research papers.", "labels": [], "entities": []}, {"text": "We investigated 71 computer science abstracts (498 sentences) and defined an annotation scheme comprising 16 types of semantic relations.", "labels": [], "entities": []}, {"text": "Computer science is particularly suitable for our purpose because it is primarily concerned with abstract concepts rather than concrete entities, which are typically the primary focus of empirical sciences such as physics and biology.", "labels": [], "entities": []}, {"text": "In addition, computer and computational methods can be applied to an extraordinarily wide range of topics; computer science papers might discuss a bus timetable (for automatic optimization), a person's palm (as a device for projecting images), or looking over another person!G s shoulder (to obtain passwords).", "labels": [], "entities": []}, {"text": "Therefore, to annotate all computer science papers, we cannot develop predefined entity ontologies, which is the typical approach taken in biomedical text mining).", "labels": [], "entities": [{"text": "biomedical text mining", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.66424760222435}]}, {"text": "However, most computer science papers have characteristic schemata: the papers describe a problem, postulate a method, apply the method to the problem using particular data or devices, and perform experiments to evaluate the method.", "labels": [], "entities": []}, {"text": "The typical schemata clearly represent the structure of interests in this research field.", "labels": [], "entities": []}, {"text": "Therefore, we can focus on typical schemata, such as application of a method to a problem and evaluation of a method fora task.", "labels": [], "entities": []}, {"text": "As we will demonstrate in this paper, the proposed annotation scheme can cover almost all content, from phrase levels to discourse levels, in computer science papers.", "labels": [], "entities": []}, {"text": "Note that this does not necessarily mean that our framework can only be applied to computer science literature.", "labels": [], "entities": []}, {"text": "The characteristics of the schemata described above are universal in contemporary science and engineering, and many other activities inhuman society.", "labels": [], "entities": []}, {"text": "Thus, the framework presented in this study can be viewed as a starting point for research focusing on representative schemata of human activities.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted an experiment on another 30 abstracts (197 sentences) from the IPSJ Journal.", "labels": [], "entities": [{"text": "IPSJ Journal", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.9606004059314728}]}, {"text": "The two annotators who participated in the development of the guidelines annotated the abstracts independently, and inter-annotator discrepancy was checked.", "labels": [], "entities": []}, {"text": "The annotation was performed manually using the brat annotation tool(.", "labels": [], "entities": []}, {"text": "No automatic preprocessing was performed.", "labels": [], "entities": []}, {"text": "shows the annotation results for the abstract shown in.", "labels": [], "entities": []}, {"text": "The 30 pairs of annotation results were aligned automatically; The results are shown in, and 5.", "labels": [], "entities": []}, {"text": "shows the matches between the two annotators.", "labels": [], "entities": []}, {"text": "The differences in the span and direction are ignored.", "labels": [], "entities": [{"text": "span", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9785712361335754}]}, {"text": "Agreement in F-score calculated in the same manner as in for each relation is shown in column F, with the overall (micro-average) Fscore shown in the bottom row of column F.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9899821877479553}, {"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9914814233779907}, {"text": "Fscore", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.8895336985588074}]}, {"text": "If we assume the number of cases that none of  the annotators recognized (the value of the cell X in the tables) to be zero, the observed agreement and Cohen's \u03ba coefficient are 90.3% and 70.0% for entities, and 49.3% and 43.5% for relations, respectively.", "labels": [], "entities": [{"text": "agreement", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9357381463050842}, {"text": "Cohen's \u03ba coefficient", "start_pos": 152, "end_pos": 173, "type": "METRIC", "confidence": 0.6505958959460258}]}, {"text": "If we ignore the count for the cases where one annotator did not recognize the entity/relation (\"None\" rows and columns in the tables), the observed agreement and \u03ba are 96.1% and 89.3% for entities, and 76.1% and 74.3% for relations, respectively.", "labels": [], "entities": []}, {"text": "The latter statistics indicate the agreement on types for entities/relations that both annotators recognized.", "labels": [], "entities": []}, {"text": "These results show that entity annotation was consistent between the annotators but the agreement for relation annotation varied, depending on the relation type.", "labels": [], "entities": []}, {"text": "shows that agreement for DESTINATION, ORIGIN, EVALUATE, and SPLIT was reasonably high, but was low for CONDITION and TARGET.", "labels": [], "entities": [{"text": "agreement", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9974426031112671}, {"text": "DESTINATION", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.5662120580673218}, {"text": "ORIGIN", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9672917127609253}, {"text": "EVALUATE", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9328135251998901}, {"text": "SPLIT", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.8061166405677795}, {"text": "TARGET", "start_pos": 117, "end_pos": 123, "type": "DATASET", "confidence": 0.505024254322052}]}, {"text": "The rise in agreement (simple and \u03ba) by excluding cases where only one annotator recognized the relation indicate that the problem is recognition, rather than classification, of relations 4 . From the investigation of the annotated text, the following was found: (1) ATTRIBUTE/CONDITION decision was inconsistent in phrases involving EVALUATE relation, such as the disk space is smaller for the image ().", "labels": [], "entities": [{"text": "ATTRIBUTE", "start_pos": 267, "end_pos": 276, "type": "METRIC", "confidence": 0.9958034157752991}]}, {"text": "The EVALUATE relation between the disk space and smaller was agreed; however, the two annotators recognized different relations between the image and other words.", "labels": [], "entities": [{"text": "EVALUATE", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9961715340614319}]}, {"text": "One annota- The same observation was true for entities tor recognized the ATTRIBUTE relation between the disk space and the image (\"the disk space as a feature of the image is smaller\").", "labels": [], "entities": [{"text": "ATTRIBUTE", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9972449541091919}]}, {"text": "The other recognized the CONDITION relation between the image and smaller (\"the disk space is smaller in the case of the image\").", "labels": [], "entities": [{"text": "CONDITION", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9959442019462585}]}, {"text": "(2) We were not incomplete agreement about skipping phrases that directly represent a relation.", "labels": [], "entities": []}, {"text": "The expressions to be skipped in the 71 trial abstracts were listed in the guidelines; however, it is difficult to exhaust all such expressions.", "labels": [], "entities": []}, {"text": "(3) In the case of some verbs, an argument can be INPUT and OUTPUT simultaneously (Section 3.1).", "labels": [], "entities": [{"text": "INPUT", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.987999439239502}, {"text": "OUTPUT", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9477171301841736}]}, {"text": "We agreed that an object that undergoes alteration in a process should be tagged as both INPUT and OUTPUT but one that does not undergo alteration or which is just moved is the TARGET.", "labels": [], "entities": [{"text": "INPUT", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9894038438796997}, {"text": "OUTPUT", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.8248505592346191}, {"text": "TARGET", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.5975368022918701}]}, {"text": "Conflicts occurred for verbs that denote prevention of some situations such as prevent, avoid, and suppress, as illustrated in.", "labels": [], "entities": []}, {"text": "One annotator claimed that the possibility of DoS attacks is reduced to zero; hence the argument of the verb should be annotated with INPUT and OUTPUT.", "labels": [], "entities": [{"text": "INPUT", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.9060519933700562}, {"text": "OUTPUT", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.7062466740608215}]}, {"text": "The other claims that since the DoS attack itself does not change, it is a TARGET.", "labels": [], "entities": [{"text": "DoS", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.8241214752197266}, {"text": "TARGET", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.8673858046531677}]}, {"text": "(4) Ina coordination expression, logical inference maybe implicitly stated.", "labels": [], "entities": []}, {"text": "For example, in it requires the linguistic knowledge and is costly, the reason for costly is likely to be the need for linguistic knowledge, i.e., employment of an expert linguist.", "labels": [], "entities": []}, {"text": "However, the relation is not readily apparent.", "labels": [], "entities": []}, {"text": "We wanted to capture the relation in such cases, but the disagreement shows that it is difficult to judge such a relation consistently.", "labels": [], "entities": []}, {"text": "(5) The decision on whether to split expressions like XX dekiru and XX kanou (can/able to XX) was also problematic.", "labels": [], "entities": []}, {"text": "The guideline was to split them.", "labels": [], "entities": []}, {"text": "This contradicts the decision for the compound words in general that we do not split them; however, we determined that dekiru/kanou cases had APP ATT COMP COND DEST EQU EVAL IN ORIG OUT PER RES SPL STA SUB TAR None Total F     Unfortunately, confusion about splitting them remains.", "labels": [], "entities": [{"text": "APP ATT COMP COND DEST EQU EVAL IN ORIG OUT PER RES SPL STA SUB TAR None Total", "start_pos": 142, "end_pos": 220, "type": "METRIC", "confidence": 0.7636310193273756}, {"text": "F", "start_pos": 221, "end_pos": 222, "type": "METRIC", "confidence": 0.5306392312049866}]}], "tableCaptions": [{"text": " Table 4: Confusion Matrix for Entity", "labels": [], "entities": []}, {"text": " Table 5: Confusion Matrix for Relation", "labels": [], "entities": [{"text": "Relation", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.967689037322998}]}]}