{"title": [{"text": "Multi-document multilingual summarization and evaluation tracks in ACL 2013 MultiLing Workshop", "labels": [], "entities": [{"text": "Multi-document multilingual summarization", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5733177165190378}, {"text": "ACL 2013 MultiLing Workshop", "start_pos": 67, "end_pos": 94, "type": "DATASET", "confidence": 0.7139634788036346}]}], "abstractContent": [{"text": "The MultiLing 2013 Workshop of ACL 2013 posed a multilingual , multi-document summarization task to the summarization community, aiming to quantify and measure the performance of multilingual , multi-document summa-rization systems across languages.", "labels": [], "entities": [{"text": "MultiLing 2013 Workshop of ACL 2013", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.5489791134993235}, {"text": "summarization community", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.8978173732757568}]}, {"text": "The task was to create a 240-250 word summary from 10 news articles, describing a given topic.", "labels": [], "entities": []}, {"text": "The texts of each topic were provided in 10 languages (Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian, Spanish) and each participant generated summaries for at least 2 languages.", "labels": [], "entities": []}, {"text": "The evaluation of the summaries was performed using automatic and manual processes.", "labels": [], "entities": []}, {"text": "The participating systems submitted over 15 runs, some providing summaries across all languages.", "labels": [], "entities": []}, {"text": "An automatic evaluation task was also added to this year's set of tasks.", "labels": [], "entities": []}, {"text": "The evaluation task meant to determine whether automatic measures of evaluation can function well in the multilingual domain.", "labels": [], "entities": []}, {"text": "This paper provides a brief description related to the data of both tasks, the evaluation methodology, as well as an overview of participation and corresponding results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The MultiLing Pilot introduced in TAC 2011 was a combined community effort to present and promote multi-document summarization apporaches that are (fully or partly) language-neutral.", "labels": [], "entities": [{"text": "multi-document summarization apporaches", "start_pos": 98, "end_pos": 137, "type": "TASK", "confidence": 0.6101553738117218}]}, {"text": "This year, in the MultiLing 2013 Workshop of ACL 2013, the effort grew to include a total of 10 languages in a multi-lingual, multi-document summarization corpus: Arabic, Czech, English, French, Greek, Hebrew, Hindi from the old corpus, plus Chinese, Romanian and Spanish as new additions.", "labels": [], "entities": [{"text": "MultiLing 2013 Workshop of ACL 2013", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.6496340334415436}]}, {"text": "Furthermore, the document set in existing languages was extended by 5 new topics.", "labels": [], "entities": []}, {"text": "We also added anew track aiming to work on evaluation measures related to multi-document summarization, similarly to the AESOP task of the recent Text Analysis Conferences.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.6270376145839691}, {"text": "AESOP", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.7994104027748108}, {"text": "Text Analysis Conferences", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.7896535595258077}]}, {"text": "This document describes: \u2022 the tasks and the data of the multi-document multilingual summarization track; \u2022 the evaluation methodology of the participating systems (Section 2.3); \u2022 the evaluation track of MultiLing (Section 3).", "labels": [], "entities": []}, {"text": "\u2022 The document is concluded (Section 4) with a summary and future steps related to this specific task.", "labels": [], "entities": []}, {"text": "The first track aims at the real problem of summarizing news topics, parts of which maybe described or happen in different moments in time.", "labels": [], "entities": [{"text": "summarizing news topics", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.9303300182024637}]}, {"text": "The implications of including multiple aspects of the same event, as well as time relations at a varying level (from consequtive days to years), are still difficult to tackle in a summarization context.", "labels": [], "entities": []}, {"text": "Furthermore, the requirement for multilingual applicability of the methods, further accentuates the difficulty of the task.", "labels": [], "entities": []}, {"text": "The second track, summarization evaluation, is related the corresponding, prominent research problem of how to automatically evaluate a summary.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.9575697183609009}]}, {"text": "While commonly used methods build upon a few human summaries to be able to judge automatic summaries (e.g.,), there also exist works on fully automatic evaluation of summaries, without human\"model\" summaries ().", "labels": [], "entities": []}, {"text": "The Text Analysis Conference has a separate track, named AESOP (e.g. see) aiming to test and evaluate different automatic evaluation methods of summarization systems.", "labels": [], "entities": [{"text": "Text Analysis Conference", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7985093394915262}, {"text": "AESOP", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9563547372817993}, {"text": "summarization", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.9683161377906799}]}, {"text": "We perform a similar task, but in a multilingual setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of results was perfromed both automatically and manually.", "labels": [], "entities": []}, {"text": "The manual evaluation was based on the Overall Responsiveness () of a text.", "labels": [], "entities": [{"text": "Overall Responsiveness", "start_pos": 39, "end_pos": 61, "type": "METRIC", "confidence": 0.7376946210861206}]}, {"text": "For the manual evaluation the human evaluators were provided the following guidelines: Each summary is to be assigned an integer grade from 1 to 5, related to the overall responsiveness of the summary.", "labels": [], "entities": []}, {"text": "We consider a text to be worth a 5, if it appears to coverall the important aspects of the corresponding document set using fluent, readable language.", "labels": [], "entities": []}, {"text": "A text should be assigned a 1, if it is either unreadable, nonsensical, or contains only trivial information from the document set.", "labels": [], "entities": []}, {"text": "We consider the content and the quality of the language to be equally important in the grading.", "labels": [], "entities": []}, {"text": "The automatic evaluation was based on human, model summaries provided by fluent speakers of each corresponding language (native speakers in the general case).", "labels": [], "entities": []}, {"text": "ROUGE variations (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4)) and the AutoSummENG-MeMoG () and NPowER ( methods were used to automatically evaluate the summarization systems.", "labels": [], "entities": [{"text": "ROUGE-4", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.883460521697998}]}, {"text": "Within this paper we provide results based on ROUGE-2 and MeMoG methods.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.6632897853851318}]}, {"text": "In the next paragraphs we describe the task, the corpus and the evaluation methodology related to the automatic summary evaluation track of MultiLing 2013.", "labels": [], "entities": [{"text": "MultiLing 2013", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.6567040532827377}]}, {"text": "This task aims to examine how well automated systems can evaluate summaries from different languages.", "labels": [], "entities": []}, {"text": "This task takes as input the summaries generated from automatic systems and humans in the Summarization Task.", "labels": [], "entities": [{"text": "Summarization Task", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8893144130706787}]}, {"text": "The output should be a grading of the summaries.", "labels": [], "entities": []}, {"text": "Ideally, we would want the automatic evaluation to maximally correlate to human judgement.", "labels": [], "entities": []}, {"text": "Based on the Source Document Set, a number of human summarizers and several automatic systems submitted summaries for the different topics in different languages.", "labels": [], "entities": []}, {"text": "The human summaries were considered model summaries and were provided, together with the source texts and the automatic summaries, as input to summary evaluation systems.", "labels": [], "entities": []}, {"text": "There were a total of 405 model summaries and 929 automatic summaries (one system did not submit summaries for all the topics).", "labels": [], "entities": []}, {"text": "Each topic in each language was mapped to 3 model summaries.", "labels": [], "entities": []}, {"text": "The question posed in the multi-lingual context is whether an automatic measure is enough to provide a ranking of systems.", "labels": [], "entities": []}, {"text": "In order to answer this question we used the ROUGE-2 score, as well as the \"n-gram graph\"-based methods (AutoSummENG, MeMoG, NPowER) to grade summaries.", "labels": [], "entities": [{"text": "ROUGE-2 score", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9700332880020142}]}, {"text": "We used ROUGE-2 because it has been robust and highly used for several years in the DUC See http://multiling.iit.demokritos.gr and TAC communities.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9231674075126648}, {"text": "DUC", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.9374192953109741}, {"text": "TAC communities", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.8227555751800537}]}, {"text": "There was only one additional participating measure for the evaluation track -namely the Coverage measure -in addition to the above methods.", "labels": [], "entities": [{"text": "Coverage measure", "start_pos": 89, "end_pos": 105, "type": "METRIC", "confidence": 0.7774445712566376}]}, {"text": "In order to measure correlation we used Kendall's Tau, to see whether grading with the automatic or the manual grades would cause different rankings (and how different).", "labels": [], "entities": [{"text": "Kendall's Tau", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.5952868262926737}]}, {"text": "The results of the correlation per language are indicated in.", "labels": [], "entities": []}, {"text": "Unfortunately, the Hebrew evaluation data were not fully available at the time of writing and, thus, they could not be used.", "labels": [], "entities": []}, {"text": "Please check the technical report tha twill be available after the completion of the Workshop for more information 4 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Participation per language. An asterisk indicates a contributor system, with early access to  corpus data.", "labels": [], "entities": []}, {"text": " Table 2: Arabic: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 18, "end_pos": 40, "type": "DATASET", "confidence": 0.79474937915802}]}, {"text": " Table 3: Arabic: Tukey's HSD test OR groups", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.8099267035722733}, {"text": "OR", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.4467838406562805}]}, {"text": " Table 4: Chinese: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.78218754529953}]}, {"text": " Table 5: Chinese: Tukey's HSD test OR groups", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.8017251193523407}, {"text": "OR", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.41542088985443115}]}, {"text": " Table 6: Czech: Tukey's HSD test MeMoG groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 17, "end_pos": 39, "type": "DATASET", "confidence": 0.7947867631912231}]}, {"text": " Table 7: Czech: Tukey's HSD test OR groups", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.7869771867990494}, {"text": "OR", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.6171703934669495}]}, {"text": " Table 8: English: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.7843469738960266}]}, {"text": " Table 9: English: Tukey's HSD test OR groups", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.7970026582479477}, {"text": "OR", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.4069525897502899}]}, {"text": " Table 10: French: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.7645823121070862}]}, {"text": " Table 11: Greek: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 18, "end_pos": 40, "type": "DATASET", "confidence": 0.8033852577209473}]}, {"text": " Table 12: Greek: Tukey's HSD test OR groups", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.7727987468242645}, {"text": "OR", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.4835842251777649}]}, {"text": " Table 13: Hebrew: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.7250800728797913}]}, {"text": " Table 14: Hindi: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 18, "end_pos": 40, "type": "DATASET", "confidence": 0.7731914401054383}]}, {"text": " Table 15: Romanian: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.7881738901138305}]}, {"text": " Table 16: Romanian: Tukey's HSD test OR groups", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.7601853460073471}, {"text": "OR", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.6607397794723511}]}, {"text": " Table 17: Spanish: Tukey's HSD test MeMoG  groups", "labels": [], "entities": [{"text": "Tukey's HSD test MeMoG", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.7671987771987915}]}, {"text": " Table 18: Spanish: Tukey's HSD test OR groups", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.7458311021327972}, {"text": "OR", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.4985099136829376}]}, {"text": " Table 19: Correlation (Kendall's Tau) Between Gradings. Note: statistically significant results, with  p-value < 0.05, in bold.", "labels": [], "entities": [{"text": "Correlation (Kendall's Tau)", "start_pos": 11, "end_pos": 38, "type": "METRIC", "confidence": 0.730416069428126}, {"text": "Gradings", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.6216685175895691}]}]}