{"title": [], "abstractContent": [{"text": "This paper presents the Arborator, an online tool for collaborative dependency annotation together with a case study of crowdsourcing in a pedagogical university context.", "labels": [], "entities": []}, {"text": "In greater detail, we explore what generally distinguishes dependency annotation tools from phrase structure annotation tools and we introduce existing tools for dependency annotation as well as the distinctive features and design choices of our tool.", "labels": [], "entities": []}, {"text": "Finally we show how to setup a crowdsourced dependency annotation experiment as an exercise for university students.", "labels": [], "entities": []}, {"text": "We explore constraints, results, and conclusions to draw.", "labels": [], "entities": []}], "introductionContent": [{"text": "The importance of treebanks in today's datadriven linguistics cannot be overrated.", "labels": [], "entities": []}, {"text": "All datadriven approaches to syntax require gold-standard annotations, and the need for (possibly machine-aided) hand annotation tools is more urgent than ever, as researchers want to go beyond the eternal Penn Treebank derivatives, because of interests in different languages, annotation levels, and theoretical backgrounds underlying the research.", "labels": [], "entities": [{"text": "Penn Treebank derivatives", "start_pos": 206, "end_pos": 231, "type": "DATASET", "confidence": 0.9888371229171753}]}, {"text": "In recent years, dependency treebanks have become the near-standard representation of annotation schemes in computational linguistics.", "labels": [], "entities": []}, {"text": "However, the inherently non-local structure of dependencies make graphical annotation tools more difficult to develop and commonly less intuitive to use.", "labels": [], "entities": []}, {"text": "This paper presents an online annotation tool named Arborator, its features, and how it can be used in an educational surrounding at the same time for pedagogical purposes as well as with the goal to develop high-quality dependency treebanks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The second part of this paper addresses the question of how good the dependency annotation of non-professional annotators can become, if we use a rover, i.e. a voting system (Fiscus 1997), to establish the best annotation among a series of annotations produced by semi-trained students.", "labels": [], "entities": []}, {"text": "This is interesting as many linguistic departments lack resources to train and pay professional annotators, but don't lack students with the desire to learn syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7502408921718597}]}, {"text": "When using one part of the trees for evaluation of the students, and constructing an optimal tree on the remaining sentences we obtain the following results.", "labels": [], "entities": []}, {"text": "At the present state we always split into a first part for computing the students' scores and a second part which are the remaining sentences.", "labels": [], "entities": []}, {"text": "Successive studies will try different jackknifing techniques.", "labels": [], "entities": []}, {"text": "The construction of an optimal tree is slightly complicated by the graph structure of the analysis, i.e. the possibility of double governors, as explained above.", "labels": [], "entities": []}, {"text": "So the first step of the different voting systems is to decide on the number of governors, 1 most of the time, but sometimes 0 (errors in segmentation) or 2 (only relative pronouns with our annotation guidelines for French).", "labels": [], "entities": []}, {"text": "The Scoring voting system works as follows: For every node, every proposal of a governor node gets the score the annotator obtained in the evaluation.", "labels": [], "entities": [{"text": "Scoring voting", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9121728241443634}]}, {"text": "Then the governor (or the two governors, if the first vote decided on two governors) with the highest score is chosen for the tree.", "labels": [], "entities": []}, {"text": "Note that this does not include explicit coherence tests (like non-circularity etc.) but we have not discovered any circular tree with our data.", "labels": [], "entities": []}, {"text": "In this first version, only students can take part in the vote that have annotated \u00bc of the trees that are used for evaluation.", "labels": [], "entities": []}, {"text": "Looking on these numbers, the first astonishing fact is the stability of the results independently of the number of sentences that are used for evaluation.", "labels": [], "entities": []}, {"text": "Put differently: With only one tree to annotate, we already get a reasonable estimate of the student's capacities.", "labels": [], "entities": []}, {"text": "We also checked whether the threshold (of taking only evaluations into account that are based on a reasonable number of annotated sentences) has an impact on the results, but in fact the differences are very slight.", "labels": [], "entities": []}, {"text": "This is astonishing when looking at the annotation quality seen in section 4.4, but can be explained by the stabilizing factor that most students try to do a good job.", "labels": [], "entities": []}, {"text": "Unsurprisingly, not voting but just taking the best student for each tree gives quite unstable results, depending on the number of sentences annotated by the best students.", "labels": [], "entities": []}, {"text": "The results are partly better, partly worse than the previous results.", "labels": [], "entities": []}, {"text": "Of course it is unrealistic to have this many annotations per sentence.", "labels": [], "entities": []}, {"text": "This leads us naturally to the exploration of how many annotations we actually need to keep up reasonable results.", "labels": [], "entities": []}], "tableCaptions": []}