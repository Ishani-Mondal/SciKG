{"title": [], "abstractContent": [], "introductionContent": [{"text": "The KBGen 2013 natural language generation challenge 1 was intended to survey and compare the performance of various systems which perform tasks in the content realization stage of generation (.", "labels": [], "entities": [{"text": "KBGen 2013 natural language generation challenge", "start_pos": 4, "end_pos": 52, "type": "TASK", "confidence": 0.764365608493487}]}, {"text": "Given a set of relations which form a coherent unit, the task is to generate complex sentences which are grammatical and fluent in English.", "labels": [], "entities": []}, {"text": "The relations for this year's challenge were selected from the AURA knowledge base (KB) (.", "labels": [], "entities": [{"text": "AURA knowledge base (KB)", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.9582320253054301}]}, {"text": "In this paper we give an overview of the KB, describe our methodology for selecting sets of relations from the KB to provide input-output pairs for the challenge, and give details of the development and test data set that was provided to participating teams.", "labels": [], "entities": []}, {"text": "Three teams have submitted system outputs for this year's challenge.", "labels": [], "entities": []}, {"text": "In this paper we show BLEU and NIST scores for outputs generated by the teams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9991325736045837}, {"text": "NIST", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.6835457682609558}]}, {"text": "The full results of our evaluation, including human judgements, as well as the development and test data set are available at http://www.kbgen.org.", "labels": [], "entities": []}], "datasetContent": [{"text": "Participants submitted two sets of outputs: (1) outputs generated by their system as is (modulo including the lexicon provided in the test data set) (2) outputs generated 6 days later, during which time teams had a chance to make improvements.", "labels": [], "entities": []}, {"text": "Each team was allowed to submit a set of 5 ranked outputs for each input.", "labels": [], "entities": []}, {"text": "We have evaluated all of the submitted outputs using BLEU and NIST scores and we are currently in the process of collecting human judgements for the final system outputs that were ranked first.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9979414343833923}, {"text": "NIST scores", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.814805418252945}]}, {"text": "shows the overall results of automatic evaluation on both the initial and final data sets for our three teams 3 , as well as the coverage of the individual systems over the 72 test inputs.", "labels": [], "entities": []}, {"text": "More detail including the full results of our evaluation can be found at http: //www.kbgen.org, along with a link to download", "labels": [], "entities": []}], "tableCaptions": []}