{"title": [], "abstractContent": [{"text": "We present three ways of inducing probability distributions on derivation trees produced by Minimalist Grammars, and give their maximum likelihood estimators.", "labels": [], "entities": []}, {"text": "We argue that a parameterization based on locally normalized log-linear models balances competing requirements for mod-eling expressiveness and computational tractability.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammars that define not just sets of trees or strings but probability distributions over these objects have many uses both in natural language processing and in psycholinguistic models of such tasks as sentence processing and grammar acquisition.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.6872672438621521}, {"text": "sentence processing", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.7571822106838226}, {"text": "grammar acquisition", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.7250831574201584}]}, {"text": "Minimalist Grammars (MGs)) provide a computationally explicit formalism that incorporates the basic elements of one of the most common modern frameworks adopted by theoretical syntacticians, but these grammars have not often been put to use in probabilistic settings.", "labels": [], "entities": [{"text": "Minimalist Grammars (MGs))", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6647289037704468}]}, {"text": "In the few cases where they have (e.g.), distributions over MG derivations have been over-parametrized in a manner that follows straightforwardly from a conceptualization of the derivation trees as those generated by a particular context-free grammar, but which does not respect the characteristic perspective of the underlying MG derivation.", "labels": [], "entities": []}, {"text": "We propose an alternative approach with a smaller number of parameters that are straightforwardly interpretable in terms that relate to the theoretical primitives of the MG formalism.", "labels": [], "entities": []}, {"text": "This improved parametrization opens up new possibilities for probabilistically-based empirical evaluation of MGs as a cognitive hypothesis about the discrete primitives of natural language grammars, and for the use of MGs in applied natural language processing.", "labels": [], "entities": []}, {"text": "In Section 2 we present MGs and their equivalence to MCFGs, which provides a contextfree characterization of MG derivation trees.", "labels": [], "entities": []}, {"text": "We demonstrate the problems with the straightforward method of supplementing a MG with probabilities that this equivalence permits in Section 3, and then introduce our proposed reparametrization that solves these problems in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 concludes and outlines some suggestions for future related work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Selected feature values for a sample of MCFG rules. The first four rules are the ones that  illustrated the problems with the naive parametrization in Section 3.3.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of probability estimators.", "labels": [], "entities": []}]}