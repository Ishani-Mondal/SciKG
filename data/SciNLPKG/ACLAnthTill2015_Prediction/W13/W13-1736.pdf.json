{"title": [{"text": "Identifying the L1 of non-native writers: the CMU-Haifa system", "labels": [], "entities": [{"text": "CMU-Haifa system", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.934834361076355}]}], "abstractContent": [{"text": "We show that it is possible to learn to identify, with high accuracy, the native language of English test takers from the content of the essays they write.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.993763267993927}]}, {"text": "Our method uses standard text classification techniques based on multiclass logistic regression, combining individually weak indicators to predict the most probable native language from a set of 11 possibilities.", "labels": [], "entities": [{"text": "text classification", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7292343378067017}]}, {"text": "We describe the various features used for classification, as well as the settings of the classifier that yielded the highest accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.9589011073112488}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9964371919631958}]}], "introductionContent": [{"text": "The task we address in this work is identifying the native language (L1) of non-native English (L2) authors.", "labels": [], "entities": []}, {"text": "More specifically, given a dataset of short English essays ( ), composed as part of the Test of English as a Foreign Language (TOEFL) by authors whose native language is one out of 11 possible languages-Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, or Turkish-our task is to identify that language.", "labels": [], "entities": []}, {"text": "This task has a clear empirical motivation.", "labels": [], "entities": []}, {"text": "Nonnative speakers make different errors when they write English, depending on their native language); understanding the different types of errors is a prerequisite for correcting them (, and systems such as the one we describe here can shed interesting light on such errors.", "labels": [], "entities": []}, {"text": "Tutoring applications can use our system to identify the native language of students and offer better-targeted advice.", "labels": [], "entities": []}, {"text": "Forensic linguistic applications are sometimes required to determine the L1 of authors ().", "labels": [], "entities": []}, {"text": "Additionally, we believe that the task is interesting in and of itself, providing a better understanding of non-native language.", "labels": [], "entities": []}, {"text": "We are thus equally interested in defining meaningful features whose contribution to the task can be linguistically interpreted.", "labels": [], "entities": []}, {"text": "Briefly, our features draw heavily on prior work in general text classification and authorship identification, those used in identifying so-called translationese (Volansky et al., forthcoming), and a class of features that involves determining what minimal changes would be necessary to transform the essays into \"standard\" English (as determined by an n-gram language model).", "labels": [], "entities": [{"text": "general text classification", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6301125486691793}, {"text": "authorship identification", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.6933367103338242}, {"text": "identifying so-called translationese", "start_pos": 125, "end_pos": 161, "type": "TASK", "confidence": 0.6003706256548563}]}, {"text": "We address the task as a multiway textclassification task; we describe our data in \u00a73 and classification model in \u00a74.", "labels": [], "entities": []}, {"text": "As in other author attribution tasks), the choice of features for the classifier is crucial; we discuss the features we define in \u00a75.", "labels": [], "entities": []}, {"text": "We report our results in \u00a76 and conclude with suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training set statistics.", "labels": [], "entities": []}, {"text": " Table 3: Dev set accuracy with main feature groups,  added cumulatively. The number of parameters is always  a multiple of 11 (the number of classes). Only 2 regular- ization was used for these experiments; the penalty was  tuned on the dev set as well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9808079600334167}]}, {"text": " Table 4: Dev set accuracy with main features plus addi- tional feature groups, added independently. 2 regulariza- tion was tuned as in table 3 (two values, 1.0 and 5.0, were  tried for each configuration; more careful tuning might  produce slightly better accuracy). Results are sorted by  accuracy; only three groups exhibited independent im- provements over the main feature set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9926815629005432}, {"text": "accuracy", "start_pos": 257, "end_pos": 265, "type": "METRIC", "confidence": 0.9903469681739807}, {"text": "accuracy", "start_pos": 291, "end_pos": 299, "type": "METRIC", "confidence": 0.9989890456199646}]}, {"text": " Table 5: Official test set confusion matrix with the full model. Accuracy is 81.5%.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9998202919960022}]}]}