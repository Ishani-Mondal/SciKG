{"title": [{"text": "Investigation of annotator's behaviour using eye-tracking data", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an analysis of an anno-tator's behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems.", "labels": [], "entities": [{"text": "machine learning-based NLP", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.5942214131355286}]}, {"text": "Since an annota-tor's behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal useful information for NLP tasks, in particular semantic and discourse processing that require deeper language understanding.", "labels": [], "entities": []}, {"text": "We conducted an experiment for collecting annotator actions and eye gaze during the annotation of predicate-argument relations in Japanese texts.", "labels": [], "entities": []}, {"text": "Our analysis of the collected data suggests that obtained insight into human annotation behaviour is useful for exploring effective linguistic features in machine learning-based approaches .", "labels": [], "entities": []}], "introductionContent": [{"text": "Text annotation is essential for machine learning (ML)-based natural language processing (NLP) where annotated texts are used for both training and evaluating supervised systems.", "labels": [], "entities": [{"text": "machine learning (ML)-based natural language processing (NLP)", "start_pos": 33, "end_pos": 94, "type": "TASK", "confidence": 0.7072595953941345}]}, {"text": "This annotation-then-learning approach has been broadly applied to various NLP tasks, ranging from shallow processing tasks, such as POS tagging and NP chunking, to tasks requiring deeper linguistic information, such as coreference resolution and discourse relation classification, and has been largely successful for shallow NLP tasks in particular.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.6784097403287888}, {"text": "NP chunking", "start_pos": 149, "end_pos": 160, "type": "TASK", "confidence": 0.6476793587207794}, {"text": "coreference resolution", "start_pos": 220, "end_pos": 242, "type": "TASK", "confidence": 0.9291890859603882}, {"text": "discourse relation classification", "start_pos": 247, "end_pos": 280, "type": "TASK", "confidence": 0.6669152677059174}]}, {"text": "The key to this success is how useful information can be effectively introduced into ML algorithms as features.", "labels": [], "entities": [{"text": "ML algorithms", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.9053539335727692}]}, {"text": "With shallow NLP tasks, surface information like words and their POS within a window of a certain size can be easily employed as useful features.", "labels": [], "entities": []}, {"text": "In contrast, in semantic and discourse processing, such as coreference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.9354784190654755}, {"text": "discourse structure analysis", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.670606384674708}]}, {"text": "In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet and linguistic theories such as Centering Theory (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 156, "end_pos": 163, "type": "DATASET", "confidence": 0.9346392750740051}]}, {"text": "They partially succeed in improving performance, but there is still room for further improvement).", "labels": [], "entities": []}, {"text": "Unlike past attempts relying on heuristic feature engineering, we take a cognitive science approach to improving system performance.", "labels": [], "entities": []}, {"text": "In stead of employing existing resources and theories, we look into human behaviour during annotation and elicit useful information for NLP tasks requiring deeper linguistic knowledge.", "labels": [], "entities": []}, {"text": "Particularly we focus on annotator eye gaze during annotation.", "labels": [], "entities": []}, {"text": "Because of recent developments in eye-tracking technology, eye gaze data has been widely used in various research fields, including psycholinguistics and problem solving.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.9003793001174927}]}, {"text": "There have been a number of studies on the relations between eye gaze and language comprehension/production).", "labels": [], "entities": []}, {"text": "Compared to the studies on language and eye gaze, the role of gaze in general problem solving settings has been less studied).", "labels": [], "entities": []}, {"text": "Since our current interest, text annotation, can be considered a problem solving as well as language comprehension task, we refer to them when defining our prob-lem setting.", "labels": [], "entities": [{"text": "text annotation", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7739299833774567}, {"text": "problem solving", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7721075713634491}]}, {"text": "Through analysis of annotators' eyetracking data, we aim at finding useful information which can be employed as features in ML algorithms.", "labels": [], "entities": [{"text": "ML algorithms", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.9341531991958618}]}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the details of the experiment for collecting annotator behavioural data during annotation as well as details on the collected data.", "labels": [], "entities": []}, {"text": "Section 3 explains the structure of the annotation process fora single annotation instance.", "labels": [], "entities": []}, {"text": "Section 4 provides a detailed analysis of human annotation processes, suggesting usages of those results in NLP.", "labels": [], "entities": []}, {"text": "Section 5 reviews the related work and Section 6 concludes and discusses future research directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results of annotation by each annotator", "labels": [], "entities": []}, {"text": " Table 3: Distribution of cases over Near/Far", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9479280710220337}]}, {"text": " Table 4: Distribution of arguments across four cat- egories", "labels": [], "entities": [{"text": "Distribution of arguments", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8768194317817688}]}, {"text": " Table 5: Relation of argument modifiers and gaze  dispersal", "labels": [], "entities": [{"text": "Relation of argument modifiers", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7666577100753784}, {"text": "gaze  dispersal", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.6980470567941666}]}]}