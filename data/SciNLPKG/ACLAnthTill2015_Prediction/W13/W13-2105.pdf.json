{"title": [], "abstractContent": [{"text": "In this paper, we focus on the task of generating elliptic sentences.", "labels": [], "entities": []}, {"text": "We extract from the data provided by the Surface Realisa-tion (SR) Task (Belz et al., 2011) 2398 input whose corresponding output sentence contain an ellipsis.", "labels": [], "entities": []}, {"text": "We show that 9% of the data contains an ellipsis and that both coverage and BLEU score markedly decrease for elliptic input (from 82.3% coverage for non-elliptic sentences to 65.3% for ellip-tic sentences and from 0.60 BLEU score to 0.47).", "labels": [], "entities": [{"text": "coverage", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9987989664077759}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9996592998504639}, {"text": "BLEU score", "start_pos": 219, "end_pos": 229, "type": "METRIC", "confidence": 0.9833419620990753}]}, {"text": "We argue that elided material should be represented using phonetically empty nodes and we introduce a set of rewrite rules which permits adding these empty categories to the SR data.", "labels": [], "entities": []}, {"text": "Finally, we evaluate an existing surface realiser on the resulting dataset.", "labels": [], "entities": []}, {"text": "We show that, after rewriting, the generator achieves a coverage of 76% and a BLEU score of 0.74 on the elliptical data.", "labels": [], "entities": [{"text": "coverage", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9929779171943665}, {"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9850291311740875}]}], "introductionContent": [{"text": "To a large extent, previous work on generating ellipsis has assumed a semantically fully specified input).", "labels": [], "entities": []}, {"text": "Given such input, elliptic sentences are then generated by first producing full sentences and second, deleting from these sentences substrings that were identified to obey deletion constraints.", "labels": [], "entities": []}, {"text": "In contrast, recent work on generation often assumes input where repeated material has already been elided.", "labels": [], "entities": []}, {"text": "This includes work on sentence compression which regenerates sentences from surface dependency trees derived from parsing the initial text (; Surface realisation approaches which have produced results for regenerating from the Penn Treebank;; and more recently, the Surface Realisation (SR)) which has proposed dependency trees and graphs derived from the Penn Treebank (PTB) as a common ground input representation for testing and comparing existing surface realisers.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7458831965923309}, {"text": "Penn Treebank", "start_pos": 227, "end_pos": 240, "type": "DATASET", "confidence": 0.9955579936504364}, {"text": "Surface Realisation (SR))", "start_pos": 266, "end_pos": 291, "type": "TASK", "confidence": 0.7364933013916015}, {"text": "Penn Treebank (PTB)", "start_pos": 356, "end_pos": 375, "type": "DATASET", "confidence": 0.9689111113548279}]}, {"text": "In all these approaches, repeated material is omitted from the representation that is input to surface realisation.", "labels": [], "entities": []}, {"text": "As shown in the literature, modelling the interface between the empty phonology and the syntactic structure of ellipses is a difficult task.", "labels": [], "entities": []}, {"text": "For parsing,, and propose either to modify the derivation process of Tree Adjoining Grammar or to introduce elementary trees anchored with empty category in asynchronous TAG to accommodate elliptic coordinations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9717544317245483}]}, {"text": "In HPSG (HeadDriven Phrase Structure Grammar), introduce a neutralisation mechanism to account for unlike constituent coordination ; in LFG (Lexical Functional Grammar), employ set values to model coordination; in CCG (Combinatory Categorial Grammar,), it is the nonstandard notion of constituency assumed by the approach which permits accounting for coordinated structures; finally, in TLCG (Type-Logical Categorial Grammar), gapping is treated as likecategory constituent coordinations (.", "labels": [], "entities": []}, {"text": "In this paper, we focus on how surface realisation handles elliptical sentences given an input where repeated material is omitted.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7625623643398285}]}, {"text": "We extract from the SR data 2398 input whose corresponding output sentence contain an ellipsis.", "labels": [], "entities": [{"text": "SR data 2398 input", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.9523206353187561}]}, {"text": "Based on previous work on how to annotate and to represent ellipsis, we argue that elided material should be represented using phonetically empty nodes (Section 3) and we introduce a set of rewrite rules which permits adding these empty categories to the SR data (Section 4).", "labels": [], "entities": []}, {"text": "We then evaluate our surface realiser) on the resulting dataset (Section 5) and we show that, on this data, the generator achieves a coverage of 76% and a BLEU score, for the generated sentences, of 0.74.", "labels": [], "entities": [{"text": "coverage", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9853723645210266}, {"text": "BLEU score", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9826730191707611}]}, {"text": "Section 6 discusses related work on generating elliptic coordination.", "labels": [], "entities": [{"text": "generating elliptic coordination", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.6529763837655386}]}], "datasetContent": [{"text": "We ran the surface realiser on the SR input data both before and after rewriting elliptic coordinations; on the sentences estimated to contain ellipsis; on sentences devoid of ellipsis; and on all sentences.", "labels": [], "entities": [{"text": "SR input data", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8257261912027994}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "They indicate coverage and BLEU score before and after rewriting.", "labels": [], "entities": [{"text": "coverage", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9994361996650696}, {"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.98296058177948}]}, {"text": "BLEU score is given both with respect to covered sentences (COV) i.e., the set of input for which generation succeeds; and for all sentences (ALL).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9756141304969788}]}, {"text": "We evaluate both with respect to the SR test data and with respect to the SR training  The impact of ellipsis on coverage and precision.", "labels": [], "entities": [{"text": "SR test data", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.7296083569526672}, {"text": "coverage", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9562212824821472}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9993425011634827}]}, {"text": "Previous work on parsing showed that coordination was a main source of parsing failure.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9840482473373413}, {"text": "parsing", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.9686748385429382}]}, {"text": "Similarly, ellipses is an important source of failure for the TAG generator.", "labels": [], "entities": [{"text": "TAG generator", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.6485757529735565}]}, {"text": "Ellipses are relatively frequent with 9% of the sentences in the training data containing an elliptic structure and performance markedly decreases in the presence of ellipsis.", "labels": [], "entities": []}, {"text": "Thus, before rewriting, coverage decreases from 82.3% for non-elliptic sentences to 80.75% on all sentences (elliptic and non elliptic sentences) and to 65.3% on the set of elliptic sentences.", "labels": [], "entities": [{"text": "coverage", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9986069798469543}]}, {"text": "Similarly, BLEU score decreases from 0.60 for non elliptical sentences to 0.58 for all sentences and to 0.47 for elliptic sentences.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9678707420825958}]}, {"text": "In sum, both coverage and BLEU score decrease as the number of elliptic input increases.", "labels": [], "entities": [{"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9994280934333801}, {"text": "BLEU score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.979862630367279}]}, {"text": "The impact of the input representation on coverage and precision.", "labels": [], "entities": [{"text": "coverage", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8399542570114136}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9988139867782593}]}, {"text": "Recent work on treebank annotation has shown that the annotation schema adopted for coordination impacts parsing.", "labels": [], "entities": []}, {"text": "In particular, propose revised annotation guidelines for coordinations in the Penn Treebank whose aim is to facilitate the detection of coordinations.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9929313957691193}]}, {"text": "And show that treebank annotations which include phonetically empty material for representing elided material allows for better parsing results.", "labels": [], "entities": []}, {"text": "Similarly, shows that the way in which ellipsis is represented in the input data has a strong impact on generation.", "labels": [], "entities": []}, {"text": "Thus rewriting the input data markedly extends coverage with an overall improvement of 11 points (from 65% to 76%) for elliptic sentences and of almost 1 point for all sentences.", "labels": [], "entities": [{"text": "coverage", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9900938868522644}]}, {"text": "As detailed in though, there are important differences between the different types of elliptic constructs: coverage increases by 68 points for NCC and 64 points for gapping against only 15, 13 and 5 points for RNR, mixed RNR-Shared Subject and Shared Subject respectively.", "labels": [], "entities": [{"text": "coverage", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9971452355384827}, {"text": "NCC", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.8655927181243896}]}, {"text": "The reason for this is that sentences are generated for many input containing the latter types of constructions (RNR and Shared Subject) even without rewriting.", "labels": [], "entities": [{"text": "RNR", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.6604111790657043}]}, {"text": "In fact, generation succeeds on the non rewritten input fora majority of RNR (66% PASS), Shared Subject (70% PASS) and mixed RNR-Shared Subject (61% PASS) constructions whereas it fails for almost all cases of gapping (3% PASS) and of NCC (5% PASS).", "labels": [], "entities": []}, {"text": "The reason for this difference is that, while the grammar cannot cope with headless constructions such as gapping and NCC constructions, it can often provide a derivation for shared subject sentences by using the finite verb form in the source sentence and the corresponding infinitival form in the target.", "labels": [], "entities": []}, {"text": "Since the infinitival does not require a subject, the target sentence is generated.", "labels": [], "entities": []}, {"text": "Similarly, RNR constructions can be generated when the verb in the source clause has both a transitive and an intransitive form: the transitive form is used to generate the source clause and the intransitive for the target clause.", "labels": [], "entities": [{"text": "RNR constructions", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.9150654375553131}]}, {"text": "In short, many sentences containing a RNR or a shared subject construction can be generated without rewriting because the grammar overgenerates i.e., it produces sentences which are valid sentences of English but whose phrase structure tree is incorrect.", "labels": [], "entities": []}, {"text": "Precision is measured using the BLEU score.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9563428163528442}, {"text": "BLEU score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9589643180370331}]}, {"text": "For each input, we take the best score obtained within the 5 derivations 4 produced by the generator.", "labels": [], "entities": []}, {"text": "Since the BLEU score reflects the degree to which a sentence generated by the system matches the corresponding Penn Treebank sentence, it is impacted not just by elliptic coordination but also by all linguistic constructions present in the sentence.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9789878129959106}, {"text": "Penn Treebank sentence", "start_pos": 111, "end_pos": 133, "type": "DATASET", "confidence": 0.9780676166216532}]}, {"text": "Nonetheless, the results show that rewriting consistently improves the BLEU score with an overall increase of 0.09 points on the set of elliptic sentences.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9765316545963287}]}, {"text": "Moreover, the consistent improvement in terms of BLEU score for generated sentences (COV column) shows that rewriting simultaneously improves both coverage and precision that is, that for those sentences that are generated, rewriting consistently improves precision.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9850243330001831}, {"text": "coverage", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9885014295578003}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9972731471061707}, {"text": "precision", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.9977685213088989}]}, {"text": "Analysing the remaining failure cases.", "labels": [], "entities": []}, {"text": "To better assess the extent to which rewriting and the FB-LTAG generation system succeed in generating elliptic coordinations, we performed error mining on the elliptic data using our error miner described in.", "labels": [], "entities": [{"text": "FB-LTAG", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.7861284017562866}]}, {"text": "This method permits highlighting the most likely sources of error given two datasets: a set of successful cases and a set of failure cases.", "labels": [], "entities": []}, {"text": "In this case, the successful cases is the subset of rewritten input data for elliptic coordination cases for which generation succeeds . The failure cases is the subset for which generation fails.", "labels": [], "entities": []}, {"text": "If elliptic coordination was still a major source of errors, input nodes or edges labelled with labels related to elliptic coordination (e.g., the COORD and the GAP-X dependency relations or the CONJ part of speech tag) would surface as most suspicious forms.", "labels": [], "entities": [{"text": "COORD", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.7765575051307678}]}, {"text": "In practice however, we found that the 5 top sources of errors highlighted by error mining all include the DEP relation, an unknown dependency relation used by the Pennconverter when it fails to assign a label to a dependency edge.", "labels": [], "entities": [{"text": "Pennconverter", "start_pos": 164, "end_pos": 177, "type": "DATASET", "confidence": 0.9577341675758362}]}, {"text": "In other words, most of the remaining elliptic cases for which generation fails, fails for reasons unrelated to ellipsis.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Generation results on elliptical data be- fore and after input rewriting (SS: Shared Subject,  NCC: Non Constituent Coordination, RNR: Right  Node Raising). The number in brackets in the first  column is the number of cases. Pass stands for  the coverage of the generator. COV and ALL in  BLEU scores column stand for BLEU scores for  the covered and the total input data.", "labels": [], "entities": [{"text": "RNR: Right  Node Raising", "start_pos": 140, "end_pos": 164, "type": "TASK", "confidence": 0.5930243909358979}, {"text": "Pass", "start_pos": 235, "end_pos": 239, "type": "METRIC", "confidence": 0.9911946058273315}, {"text": "COV", "start_pos": 283, "end_pos": 286, "type": "METRIC", "confidence": 0.8324437141418457}, {"text": "ALL", "start_pos": 291, "end_pos": 294, "type": "METRIC", "confidence": 0.9940527081489563}, {"text": "BLEU", "start_pos": 299, "end_pos": 303, "type": "METRIC", "confidence": 0.9682765603065491}, {"text": "BLEU", "start_pos": 328, "end_pos": 332, "type": "METRIC", "confidence": 0.9970122575759888}]}, {"text": " Table 2: Generation results on SR test and SR  training data before and after input rewriting (+E  stands for elliptical data, -E for non elliptical data  and T for total.)", "labels": [], "entities": [{"text": "SR test", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.8174163103103638}]}]}