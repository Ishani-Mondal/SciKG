{"title": [{"text": "Using Integer Linear Programming for Content Selection, Lexicalization, and Aggregation to Produce Compact Texts from OWL Ontologies", "labels": [], "entities": [{"text": "Content Selection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7041009217500687}]}], "abstractContent": [{"text": "We present an Integer Linear Programming model of content selection, lexical-ization, and aggregation that we developed fora system that generates texts from OWL ontologies.", "labels": [], "entities": [{"text": "content selection", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7313387989997864}]}, {"text": "Unlike pipeline archi-tectures, our model jointly considers the available choices in these three text generation stages, to avoid greedy decisions and produce more compact texts.", "labels": [], "entities": []}, {"text": "Experiments with two ontologies confirm that it leads to more compact texts, compared to a pipeline with the same components, with no deterioration in the perceived quality of the generated texts.", "labels": [], "entities": []}, {"text": "We also present an approximation of our model, which allows longer texts to be generated efficiently.", "labels": [], "entities": []}], "introductionContent": [{"text": "Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations).", "labels": [], "entities": [{"text": "Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations", "start_pos": 0, "end_pos": 103, "type": "TASK", "confidence": 0.7497374690496005}]}, {"text": "With the emergence of the Semantic Web (Berners-;, interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (, a form of description logic (), in natural language (;).", "labels": [], "entities": [{"text": "Berners-;", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9384329120318095}]}, {"text": "NLG systems typically employ a pipeline architecture.", "labels": [], "entities": []}, {"text": "They usually start by selecting the logical facts (axioms, in the case of an OWL ontology) to be expressed.", "labels": [], "entities": []}, {"text": "The purpose of the next stage, text planning, ranges from simply ordering the facts to be expressed to making more complex decisions about the rhetorical structure of the text.", "labels": [], "entities": [{"text": "text planning", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.8604827523231506}]}, {"text": "Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence.", "labels": [], "entities": []}, {"text": "Sentence aggregation may then combine shorter sentences to form longer ones.", "labels": [], "entities": [{"text": "Sentence aggregation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9360654056072235}]}, {"text": "Another component generates appropriate referring expressions, and surface realization produces the final text.", "labels": [], "entities": []}, {"text": "Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified.", "labels": [], "entities": []}, {"text": "This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent.", "labels": [], "entities": []}, {"text": "For example, decisions made during content selection may maximize importance measures, but may produce facts that are difficult to turn into a coherent text; also, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities.", "labels": [], "entities": [{"text": "content selection", "start_pos": 164, "end_pos": 181, "type": "TASK", "confidence": 0.7373965382575989}]}, {"text": "Some of these problems can be addressed by overgenerating at each stage (e.g., producing several alternative sets of facts at the end of content selection, several alternative lexicalizations etc.) and employing a final ranking component to select the best combination ().", "labels": [], "entities": []}, {"text": "This overgenerate and rank approach, however, may also fail to find an optimal solution, and it generates an exponentially large number of candidate solutions when several components are pipelined.", "labels": [], "entities": []}, {"text": "In this paper, we present an Integer Linear Programming (ILP) model that combines content selection, lexicalization, and sentence aggregation.", "labels": [], "entities": [{"text": "content selection", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7432584762573242}]}, {"text": "Our model does not consider directly text planning, nor referring expression generation, which we hope to include in future work, but it is combined with an external simple text planner and an external referring expression generation component; we also do not discuss surface realization.", "labels": [], "entities": [{"text": "text planning", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.720968097448349}, {"text": "referring expression generation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.6398851275444031}]}, {"text": "Unlike pipeline architectures, our model jointly examines the possible choices in the three NLG stages it considers, to avoid greedy local decisions.", "labels": [], "entities": []}, {"text": "Given an individual (entity) or class of an OWL ontology and a set of facts (axioms) about the individual or class, we aim to produce a compact text that expresses as many facts in as few words as possible.", "labels": [], "entities": []}, {"text": "This is desirable when space is limited or expensive, e.g., when displaying product descriptions on smartphones, or when including advertisements in Web search results.", "labels": [], "entities": []}, {"text": "If an importance score is available for each fact, our model can take it into account to prefer expressing important facts, again using as few words as possible.", "labels": [], "entities": []}, {"text": "The model itself, however, does not produce importance scores, i.e., we assume that the scores are produced by a separate process (, not included in our content selection.", "labels": [], "entities": []}, {"text": "In the experiments of this article, we treat all the facts as equally important.", "labels": [], "entities": []}, {"text": "Although the search space of our model is very large and ILP problems are in general NP-hard, offthe-shelf ILP solvers can be used, which can be very fast in practice and guarantee finding a global optimum.", "labels": [], "entities": [{"text": "ILP solvers", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.7370168268680573}]}, {"text": "Experiments with two ontologies show that our ILP model outperforms, in terms of expressed facts per word, an NLG system that uses the same components connected in a pipeline, with no deterioration in perceived text quality; the ILP model may actually lead to texts of higher quality, compared to those of the pipeline, when there are many facts to express.", "labels": [], "entities": []}, {"text": "We also present an approximation of our ILP model, which is more efficient when larger numbers of facts need to be expressed.", "labels": [], "entities": []}, {"text": "Section 2 discusses previous related work.", "labels": [], "entities": []}, {"text": "Section 3 defines our ILP model.", "labels": [], "entities": []}, {"text": "Section 4 presents our experimentals.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used NaturalOWL (), an NLG system for OWL ontologies that relies on a pipeline of content selection, text planning, lexicalization, aggregation, referring expression generation, and surface realization components.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 148, "end_pos": 179, "type": "TASK", "confidence": 0.7146921555201212}]}, {"text": "We modified the content selection, lexicalization, and aggregation components to use our ILP model, maintaining the aggregation rules of the original system.", "labels": [], "entities": []}, {"text": "For referring expressions and surface realization, the new system, called ILPNLG, invokes the corresponding components of the original system.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7766071259975433}]}, {"text": "We use branch-andcut to solve the ILP problems.", "labels": [], "entities": []}, {"text": "The original system, hereafter called PIPELINE, assumes that each relation has been mapped to a topical section, as in ILPNLG.", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9423642754554749}, {"text": "ILPNLG", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.9275833368301392}]}, {"text": "It also assumes that a manually specified order of the sections and the relations of each section is available, which is used by the text planner to order the selected facts (by their relations).", "labels": [], "entities": []}, {"text": "The subsequent components of the pipeline are not allowed to change the order of the facts, and aggregation operates only on sentence plans of adjacent facts from the same section.", "labels": [], "entities": []}, {"text": "In ILPNLG, the manually specified order of sections and relations is used to order the sentences of each subset s j (before aggregating them), the aggregated sentences in each section (each aggregated sentence inherits the minimum order of its constituents), and the sections (with their sentences).", "labels": [], "entities": []}, {"text": "Ina first set of experiments, we used the Wine Ontology, which had also been used in previous experiments with PIPELINE ().", "labels": [], "entities": [{"text": "Wine Ontology", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9269683957099915}]}, {"text": "The ontology contains 63 wine classes, 52 wine individuals, a total of 238 classes and individuals (including wineries, regions, etc.), and 14 properties.", "labels": [], "entities": []}, {"text": "We kept the 2 topical sections, the ordering of sections and relations, and the sentence plans of the previous experiments, but we added more sentence plans to ensure that 3 sentence plans were available per relation.", "labels": [], "entities": []}, {"text": "We generated English texts for the 52 wine individuals All the software and data that we used will be freely available from http://nlp.cs.aueb.gr/ software.html.", "labels": [], "entities": []}, {"text": "We use version 2 of NaturalOWL.", "labels": [], "entities": []}, {"text": "We use the branch-and-cut implementation of GLPK with mixed integer rounding, mixed cover, and clique cuts; see sourceforge.net/projects/winglpk/.", "labels": [], "entities": [{"text": "GLPK", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8728318810462952}]}, {"text": "6 See www..org/TR/owl-guide/wine.rdf. of the ontology; we did not experiment with texts describing classes, because we could not think of multiple alternative sentence plans for many of their axioms.", "labels": [], "entities": []}, {"text": "For each wine individual, there were 5 facts on average and a maximum of 6 facts.", "labels": [], "entities": []}, {"text": "We set the importance scores imp(f i ) of all the facts f i to 1, to make the decisions of PIPELINE and ILPNLG easier to understand; both systems use the same importance scores.", "labels": [], "entities": [{"text": "importance scores imp(f i )", "start_pos": 11, "end_pos": 38, "type": "METRIC", "confidence": 0.9243000660623822}, {"text": "PIPELINE", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.5545157790184021}, {"text": "ILPNLG", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.8019394874572754}]}, {"text": "PIPELINE does not provide any mechanism to estimate the importance scores, assuming that they are provided manually.", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8615003228187561}]}, {"text": "PIPELINE has a parameter M specifying the maximum number of facts it is allowed to report per text.", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.6723326444625854}]}, {"text": "When M is smaller than the number of available facts (|F |) and all the facts are treated as equally important, as in our experiments, it selects randomly M of the available facts.", "labels": [], "entities": []}, {"text": "We repeated the generation of PIPELINE's texts for the 52 individuals for M = 2, 3, 4, 5, 6.", "labels": [], "entities": [{"text": "PIPELINE's texts", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8787664373715719}, {"text": "M = 2", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.8792575200398763}]}, {"text": "For each M , the texts of PIPELINE for the 52 individuals were generated three times, each time using one of the different alternative sentence plans of each relation.", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8446308970451355}]}, {"text": "We also generated the texts using a variant of PIPELINE, dubbed PIPELINESHORT, which always selects the shortest (in elements) sentence plan among the available ones.", "labels": [], "entities": []}, {"text": "In all cases, PIPELINE and PIPELINESHORT were allowed to form aggregated sentences containing up to B max = 22 distinct elements, which was the number of distinct elements of the longest aggregated sentence in the previous experiments, where PIPELINE was allowed to aggregate up to 3 original sentences.", "labels": [], "entities": []}, {"text": "With ILPNLG, we repeated the generation of the texts of the 52 individuals using different values of \u03bb 1 (\u03bb 2 = 1 \u2212 \u03bb 1 ), which led to texts expressing from zero to all of the available facts.", "labels": [], "entities": [{"text": "ILPNLG", "start_pos": 5, "end_pos": 11, "type": "DATASET", "confidence": 0.9468681812286377}]}, {"text": "We set the maximum number of fact subsets tom = 3, which was the maximum number of (aggregated) sentences in the texts of PIPELINE and PIPELI-NESHORT.", "labels": [], "entities": []}, {"text": "Again, we set B max = 22.", "labels": [], "entities": [{"text": "B max", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.982393205165863}]}, {"text": "We compared ILPNLG to PIPELINE and PIPELI-NESHORT by measuring the average number of facts they reported divided by the average text length (in words).", "labels": [], "entities": []}, {"text": "shows this ratio as a function of the average number of reported facts, along with 95% confidence intervals (of sample means).", "labels": [], "entities": []}, {"text": "PIPELINESHORT achieved better results than PIPELINE, but the differences were small.", "labels": [], "entities": [{"text": "PIPELINESHORT", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.6469036340713501}]}, {"text": "For \u03bb 1 < 0.2, ILPNLG produces empty texts, We modified the two pipeline systems to count elements.", "labels": [], "entities": []}, {"text": "because it focuses on minimizing the number of distinct elements of each text.", "labels": [], "entities": []}, {"text": "For \u03bb 1 \u2265 0.225, it performs better than the other systems.", "labels": [], "entities": []}, {"text": "For \u03bb 1 \u2248 0.3, it obtains the highest fact/words ratio by selecting the facts and sentence plans that lead to the most compressive aggregations.", "labels": [], "entities": []}, {"text": "For greater values of \u03bb 1 , it selects additional facts whose sentence plans do not aggregate that well, which is why the ratio declines.", "labels": [], "entities": []}, {"text": "For small numbers of facts, the two pipeline systems select facts and sentence plans that offer few aggregation opportunities; as the number of selected facts increases, some more aggregation opportunities arise, which is why the facts/words ratio of the two systems improves.", "labels": [], "entities": []}, {"text": "In all the experiments, the ILP solver was very fast (average: 0.08 sec, worst: 0.14 sec per text).", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.7798491418361664}]}, {"text": "We show below texts produced by PIPELINE (M = 4) and ILPNLG (\u03bb 1 = 0.3).", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.551004946231842}, {"text": "ILPNLG", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.6120724081993103}]}, {"text": "In the first pair, PIPELINE uses different verbs for the grapes and producer, whereas ILPNLG uses the same verb, which leads to a more compressive aggregation; both texts describe the same wine and report 4 facts.", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.5064697861671448}, {"text": "ILPNLG", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.7992369532585144}]}, {"text": "In the second pair, ILPNLG has chosen to express the sweetness instead of the producer, and uses the same verb (\"be\") for all the facts, leading to a shorter sentence; again both texts describe the same wine and report 4 facts.", "labels": [], "entities": [{"text": "ILPNLG", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.8921034336090088}]}, {"text": "In both examples, some facts are not aggregated because they belong in different sections.", "labels": [], "entities": []}, {"text": "We also wanted to investigate the effect that the higher facts/words ratio of ILPNLG has on the perceived quality of the generated texts, compared to the texts of the pipeline.", "labels": [], "entities": [{"text": "ILPNLG", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.8772998452186584}]}, {"text": "We were concerned that the more compressive aggregations of ILPNLG  might lead to sentences that soundless fluent or unnatural, though aggregation often helps produce more natural texts.", "labels": [], "entities": [{"text": "ILPNLG", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9346199035644531}]}, {"text": "We were also concerned that the more compact texts of ILPNLG might be perceived as being more difficult to understand (less clear) or less well-structured.", "labels": [], "entities": [{"text": "ILPNLG", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.8554385900497437}]}, {"text": "To investigate these issues, we showed the 52 \u00d7 2 = 104 texts of PIPELINESHORT (M = 4) and ILPNLG (\u03bb 1 = 0.3) to 6 computer science students not involved in the work of this article; they were all fluent, though not native, English speakers.", "labels": [], "entities": [{"text": "PIPELINESHORT", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.5674432516098022}, {"text": "ILPNLG", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.6042730212211609}]}, {"text": "Each one of the 104 texts was given to exactly one student.", "labels": [], "entities": []}, {"text": "Each student was given approximately 9 randomly selected texts of each system.", "labels": [], "entities": []}, {"text": "The OWL statements that the texts were generated from were not shown, and the students did not know which system had generated each text.", "labels": [], "entities": [{"text": "OWL statements", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8511060774326324}]}, {"text": "Each student was shown all of his/her texts in random order, regardless of the system that generated them.", "labels": [], "entities": []}, {"text": "The students were asked to score each text by stating how strongly they agreed or disagreed with statements S 1 -S 3 below.", "labels": [], "entities": []}, {"text": "A scale from 1 to 5 was used (1: strong disagreement, 3: ambivalent, 5: strong agreement).", "labels": [], "entities": []}, {"text": "(S1) Sentence fluency: The sentences of the text are fluent, i.e., each sentence on its own is grammatical and sounds natural.", "labels": [], "entities": [{"text": "Sentence fluency", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.8535558879375458}]}, {"text": "When two or more smaller sentences are combined to form a single, longer sentence, the resulting longer sentence is also grammatical and sounds natural.", "labels": [], "entities": []}, {"text": "(S2) Text structure: The order of the sentences is appropriate.", "labels": [], "entities": [{"text": "Text structure", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.6920627951622009}]}, {"text": "The text presents information by moving reasonably from one topic to another.", "labels": [], "entities": []}, {"text": "(S3) Clarity: The text is easy to understand, provided that the reader is familiar with basic wine terms.", "labels": [], "entities": [{"text": "Clarity", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.884972870349884}]}, {"text": "The students were also asked to provide an overall score (1-5) per text.", "labels": [], "entities": []}, {"text": "We did not score referring expressions, since both systems use the same component to generate them.", "labels": [], "entities": []}, {"text": "shows the average scores of the two systems with 95% confidence intervals (of sample means).", "labels": [], "entities": []}, {"text": "For each criterion, the best score is shown in bold.", "labels": [], "entities": []}, {"text": "The sentence fluency and overall scores of ILPNLG are slightly higher than those of PIPELINESHORT, whereas PIPELINESHORT obtained a slightly higher score for text structure and clarity.", "labels": [], "entities": [{"text": "text structure", "start_pos": 158, "end_pos": 172, "type": "TASK", "confidence": 0.6824111640453339}]}, {"text": "The differences, however, are very small, especially in clarity, and there is no statistically significant difference between the two systems in any of the criteria.", "labels": [], "entities": [{"text": "clarity", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9950236082077026}]}, {"text": "8 Hence, there was no evidence in these experiments that the highest facts/words ratio of ILPNLG comes at the expense of lower perceived text quality.", "labels": [], "entities": []}, {"text": "We investigated these issues further in a second set of experiments, discussed next, where the generated texts were longer.", "labels": [], "entities": []}, {"text": "In the second set of experiments, we used the Consumer Electronics Ontology, which had also been used in previous work with PIPELINE.", "labels": [], "entities": [{"text": "Consumer Electronics Ontology", "start_pos": 46, "end_pos": 75, "type": "DATASET", "confidence": 0.8857964674631754}, {"text": "PIPELINE", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.8082923293113708}]}, {"text": "The ontology comprises 54 classes and 441 individuals (e.g., printer types, paper sizes), but no information about particular products.", "labels": [], "entities": []}, {"text": "In previous work, 30 individuals (10 digital cameras, 10 camcorders, 10 printers) were added to the ontology; they were randomly selected from a publicly available dataset of 286 digital cameras, 613 camcorders, and 58 printers, whose instances comply with the Consumer Electronics Ontology.", "labels": [], "entities": [{"text": "Consumer Electronics Ontology", "start_pos": 261, "end_pos": 290, "type": "DATASET", "confidence": 0.9236869613329569}]}, {"text": "We kept the 6 topical sections, the ordering of sections and relations, and the sentence plans of the previous work, but we added more sentence plans to ensure that 3 sentence plans were available for almost every relation; for some relations we could not think of enough sentence plans.", "labels": [], "entities": []}, {"text": "Again, we set the importance scores of all the facts to 1.", "labels": [], "entities": [{"text": "importance scores", "start_pos": 18, "end_pos": 35, "type": "METRIC", "confidence": 0.9767925441265106}]}, {"text": "We generated texts with PIPELINE and PIPELI-NESHORT for the 30 individuals, for M = 3, 6, 9, . .", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9693326950073242}, {"text": "PIPELI-NESHORT", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.9142026305198669}]}, {"text": "Again for each M , the texts of PIPELINE were generated three times, each time using one of the different alternative sentence plans of each relation.", "labels": [], "entities": []}, {"text": "PIPELINE and PIPELI-NESHORT were allowed to form aggregated sentences containing up to B max = 39 distinct elements, which was the number of distinct elements of the longest aggregated sentence in the previous work with this ontology, where PIPELINE was allowed to aggregate up to 3 original sentences.", "labels": [], "entities": []}, {"text": "We also set B max = 39 in ILPNLG.", "labels": [], "entities": [{"text": "B max", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9862456023693085}, {"text": "ILPNLG", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.9550771713256836}]}, {"text": "There are 14 facts (F ) on average and a maximum of 21 facts for each one of the 30 individuals, compared to the 5 facts on average and the maximum of 6 facts of the experiments with the Wine Ontology.", "labels": [], "entities": [{"text": "F )", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8735738694667816}, {"text": "Wine Ontology", "start_pos": 187, "end_pos": 200, "type": "DATASET", "confidence": 0.9539060294628143}]}, {"text": "Hence, the texts of the Consumer: Average solver times for ILPNLG for different maximum numbers of fact subsets (m).", "labels": [], "entities": [{"text": "solver", "start_pos": 42, "end_pos": 48, "type": "TASK", "confidence": 0.8917138576507568}]}, {"text": "Electronics Ontology are much longer, when they report all the available facts.", "labels": [], "entities": [{"text": "Electronics Ontology", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7470150589942932}]}, {"text": "To generate texts for the 30 individuals with ILPNLG, we would have to set the maximum number of fact subsets tom = 10, which was the maximum number of (aggregated) sentences in the texts of PIPELINE and PIPELINESHORT.", "labels": [], "entities": []}, {"text": "The number of variables of our ILP model, however, grows exponentially tom and the number of available facts |F |.", "labels": [], "entities": []}, {"text": "shows the average time the ILP solver took for different values of min the experiments with the Consumer Electronics ontology; the results are also averaged for \u03bb 1 = 0.4, 0.5, 0.6 (\u03bb 2 = 1 \u2212 \u03bb 1 ).", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.6937372088432312}, {"text": "Consumer Electronics ontology", "start_pos": 96, "end_pos": 125, "type": "DATASET", "confidence": 0.8471077084541321}]}, {"text": "For m = 4, the solver took 1 minute and 47 seconds on average per text; recall that |F | is also much larger now, compared to the experiments of the previous section.", "labels": [], "entities": [{"text": "solver", "start_pos": 15, "end_pos": 21, "type": "TASK", "confidence": 0.9704382419586182}, {"text": "F", "start_pos": 85, "end_pos": 86, "type": "METRIC", "confidence": 0.9919924736022949}]}, {"text": "For m = 5, the solver was so slow that we aborted the experiment.", "labels": [], "entities": [{"text": "solver", "start_pos": 15, "end_pos": 21, "type": "TASK", "confidence": 0.982671320438385}]}, {"text": "shows the average solver time for different numbers of available facts |F |, form = 3; in this case, we modified the set of available facts (F ) of every individual to contain 3, 6, 9, 12, 15, 18, 21 facts; the results are averaged for \u03bb 1 = 0.4, 0.5, 0.6.", "labels": [], "entities": [{"text": "solver", "start_pos": 18, "end_pos": 24, "type": "TASK", "confidence": 0.9296678900718689}]}, {"text": "Although the times of also grow exponentially, they remain under 4 seconds, showing that the main problem for ILPNLG ism, the number of fact subsets, which is also the maximum allowed number of (aggregated) sentences of each text.", "labels": [], "entities": []}, {"text": "To be able to efficiently generate texts with larger m values, we use a variant of ILPNLG, called ILPNLGAPPROX, which considers each fact subset separately.", "labels": [], "entities": []}, {"text": "ILPNLGAPPROX starts with the full set of available facts (F ) and uses our ILP model (Section 3) with m = 1 to produce the first (aggregated) sentence of the text.", "labels": [], "entities": [{"text": "ILPNLGAPPROX", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8498094081878662}]}, {"text": "It then removes the facts expressed by the first (aggregated) sentence from F , and uses the ILP model, again with  m = 1, to produce the second (aggregated) sentence etc.", "labels": [], "entities": []}, {"text": "This process is repeated until we produce the maximum number of allowed aggregated sentences, or until we run out of facts.", "labels": [], "entities": []}, {"text": "ILPNLGAP-PROX is an approximation of ILPNLG, in the sense that it does not consider all the fact subsets jointly and, hence, does not guarantee finding a globally optimal solution for the entire text.", "labels": [], "entities": [{"text": "ILPNLGAP-PROX", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9169273376464844}]}, {"text": "show the average solver times of ILPNLGAPPROX for different values of m and |F |; all the other settings are as in.", "labels": [], "entities": [{"text": "solver", "start_pos": 17, "end_pos": 23, "type": "TASK", "confidence": 0.9474559426307678}, {"text": "F", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9526536464691162}]}, {"text": "The solver times of ILPNLGAPPROX grow approximately linearly tom and |F | and are under 0.3 seconds in all cases.", "labels": [], "entities": [{"text": "solver", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9500073790550232}, {"text": "F", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9928520321846008}]}, {"text": "shows the average facts/words ratio of ILPNLGAPPROX (m = 10), PIPELINE and PIPELI-NESHORT, along with 95% confidence intervals (of sample means), for the texts of the 30 individuals.", "labels": [], "entities": [{"text": "ILPNLGAPPROX", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.7714419364929199}, {"text": "PIPELINE", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.990009605884552}, {"text": "PIPELI-NESHORT", "start_pos": 75, "end_pos": 89, "type": "METRIC", "confidence": 0.9364904761314392}]}, {"text": "Again, PIPELINESHORT achieves slightly better results than PIPELINE, but the differences are now smaller (cf.).", "labels": [], "entities": [{"text": "PIPELINESHORT", "start_pos": 7, "end_pos": 20, "type": "METRIC", "confidence": 0.5535716414451599}]}, {"text": "ILPNLGAPPROX behaves very similarly to ILPNLG in the Wine Ontology experiments (cf; for \u03bb 1 \u2264 0.35, it produces empty texts, while for \u03bb 1 \u2265 0.4 it performs better than the other systems.", "labels": [], "entities": []}, {"text": "ILPNLGAPPROX obtains  the highest facts/words ratio for \u03bb 1 = 0.45, where it selects the facts and sentence plans that lead to the most compressive aggregations.", "labels": [], "entities": []}, {"text": "For greater values of \u03bb 1 , it selects additional facts whose sentence plans do not aggregate that well, which is why the ratio declines.", "labels": [], "entities": []}, {"text": "The two pipeline systems select facts and sentence plans that offer very few aggregation opportunities; as the number of selected facts increases, some more aggregation opportunities arise, which is why the facts/words ratio of the two systems improves slightly, though the improvement is now hardly noticeable.", "labels": [], "entities": []}, {"text": "We show below two example texts produced by PIPELINE (M = 6) and ILPNLGAPPROX (\u03bb 1 = 0.45).", "labels": [], "entities": [{"text": "PIPELINE", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.459716260433197}, {"text": "M", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.887327253818512}, {"text": "ILPNLGAPPROX", "start_pos": 65, "end_pos": 77, "type": "METRIC", "confidence": 0.8006223440170288}]}, {"text": "Both texts report 6 facts, but ILPNLGAP-PROX has selected facts and sentence plans that allow more compressive aggregations.", "labels": [], "entities": [{"text": "ILPNLGAP-PROX", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.8812358975410461}]}, {"text": "Recall that we treat all the facts as equally important.", "labels": [], "entities": []}, {"text": "0.45) to the same 6 students, as in Section 4.1.", "labels": [], "entities": [{"text": "Section 4.1", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8666349649429321}]}, {"text": "Again, each text was given to exactly one student.", "labels": [], "entities": []}, {"text": "Each student was given approximately 5 randomly selected texts of each system.", "labels": [], "entities": []}, {"text": "The OWL statements that the texts were generated from were not shown, and the students did not know which system had generated each text.", "labels": [], "entities": [{"text": "OWL statements", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8511060774326324}]}, {"text": "Each student was shown all of his/her texts in random order, regardless of the system that generated them.", "labels": [], "entities": []}, {"text": "The students were asked to score each text by stating how strongly they agreed or disagreed with statements S 1 -S 3 , as in Section 4.1.", "labels": [], "entities": []}, {"text": "They were also asked to provide an overall score (1-5) per text.", "labels": [], "entities": []}, {"text": "shows the average scores of the two systems with 95% confidence intervals (of sample means).", "labels": [], "entities": []}, {"text": "For each criterion, the best score is shown in bold; the confidence interval of the best score is also shown in bold, if it does not overlap with the confidence interval of the other system.", "labels": [], "entities": [{"text": "confidence interval", "start_pos": 57, "end_pos": 76, "type": "METRIC", "confidence": 0.9768540561199188}]}, {"text": "Unlike the Wine Ontology experiments, the scores of our ILP approach are now higher than those of the pipeline in all of the criteria, and the differences are also larger, though the differences are statistically significant only for clarity and overall quality.", "labels": [], "entities": [{"text": "clarity", "start_pos": 234, "end_pos": 241, "type": "METRIC", "confidence": 0.9882121682167053}]}, {"text": "We attribute these differences to the fact that the texts are now longer and the sentence plans more varied, which often makes the texts of the pipeline sound verbose and, hence, more difficult to follow, compared to the more compact texts of ILPNLGAPPROX, which sound more concise.", "labels": [], "entities": []}, {"text": "Overall, the human scores of the experiments with the two ontologies suggest that the higher facts/words ratio of our ILP approach does not come at the expense of lower perceived text quality.", "labels": [], "entities": []}, {"text": "On the contrary, the texts of the ILP approach maybe perceived as clearer and overall better than those of the pipeline, when the texts are longer.", "labels": [], "entities": []}, {"text": "avoid greedy local decisions and produce more compact texts.", "labels": [], "entities": []}, {"text": "The model has been embedded in NaturalOWL, a NLG system for OWL ontologies, which used a pipeline architecture in its original form.", "labels": [], "entities": []}, {"text": "Experiments with two ontologies confirmed that our approach leads to expressing more facts per word, with no deterioration in the perceived text quality; the ILP approach may actually lead to texts perceived as clearer and overall better, compared to the pipeline, when there are many facts to express.", "labels": [], "entities": []}, {"text": "We also presented an approximation of our ILP model, which allows longer texts to be generated efficiently.", "labels": [], "entities": []}, {"text": "We plan to extend our model to include text planning, referring expression generation, and mechanisms to obtain importance scores.", "labels": [], "entities": [{"text": "text planning", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.8125311136245728}, {"text": "referring expression generation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.766026238600413}]}], "tableCaptions": [{"text": " Table 1: Human scores for Wine Ontology texts.", "labels": [], "entities": [{"text": "Wine Ontology texts", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.682955930630366}]}, {"text": " Table 2: Human scores for Consumer Electronics.", "labels": [], "entities": []}]}