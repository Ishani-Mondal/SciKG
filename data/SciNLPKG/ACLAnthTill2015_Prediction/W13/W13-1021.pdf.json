{"title": [{"text": "Construction of English MWE Dictionary and its Application to POS Tagging", "labels": [], "entities": [{"text": "English MWE Dictionary", "start_pos": 16, "end_pos": 38, "type": "DATASET", "confidence": 0.668256680170695}, {"text": "POS Tagging", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.7345037162303925}]}], "abstractContent": [{"text": "This paper reports our ongoing project for constructing an English multiword expression (MWE) dictionary and NLP tools based on the developed dictionary.", "labels": [], "entities": []}, {"text": "We extracted functional MWEs from the English part of Wik-tionary, annotated the Penn Treebank (PTB) with MWE information, and conducted POS tagging experiments.", "labels": [], "entities": [{"text": "Wik-tionary", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.5227752327919006}, {"text": "Penn Treebank (PTB)", "start_pos": 81, "end_pos": 100, "type": "DATASET", "confidence": 0.9782697439193726}, {"text": "POS tagging", "start_pos": 137, "end_pos": 148, "type": "TASK", "confidence": 0.7890472412109375}]}, {"text": "We report how the MWE annotation is done on PTB and the results of POS and MWE tagging experiments.", "labels": [], "entities": [{"text": "MWE annotation", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7857876420021057}, {"text": "PTB", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9757317900657654}, {"text": "MWE tagging", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.6660983264446259}]}], "introductionContent": [{"text": "While there have been a great progress in POS tagging and parsing of natural language sentences thanks to the advancement of statistical and corpusbased methods, there still remains difficulty in sentence processing stemming from syntactic discrepancies.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9410661160945892}, {"text": "parsing of natural language sentences", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.8240643262863159}, {"text": "sentence processing", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.7149229943752289}]}, {"text": "One of such discrepancies is caused by multiword expressions (MWEs), which are known and defined as expressions having \"idiosyncratic interpretations that crossword boundaries (or spaces)\" ().", "labels": [], "entities": []}, {"text": "classifies MWEs largely into the following categories: \u2022 Lexicalized phrases -fixed expressions: Those having fixed word order and form (e.g. by and large).", "labels": [], "entities": []}, {"text": "-semi-fixed expressions: Those having fixed word order with lexical variation such as inflection, determiner selection, etc.", "labels": [], "entities": [{"text": "determiner selection", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7868644297122955}]}, {"text": "(e.g. come up with).", "labels": [], "entities": []}, {"text": "-syntactically flexible expressions: Those having a wide range of syntactic variability (e.g. phrasal verbs that take an NP argument between or following the verb and the particle).", "labels": [], "entities": []}, {"text": "\u2022 Institutionalized phrases -Phrases that are semantically and syntactically compositional, such as collocations (e.g. traffic light).", "labels": [], "entities": []}, {"text": "This paper reports our ongoing project for developing an English MWE dictionary of abroad coverage and MWE-aware natural language processing tools.", "labels": [], "entities": [{"text": "MWE-aware natural language processing", "start_pos": 103, "end_pos": 140, "type": "TASK", "confidence": 0.7670233845710754}]}, {"text": "The main contributions of this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "Construction of an English MWE dictionary (mainly consisting of functional expressions) through extraction from Wiktionary 1 . 2. Annotation of MWEs in the Penn Treebank (PTB).", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 156, "end_pos": 175, "type": "DATASET", "confidence": 0.9777291536331176}]}, {"text": "3. Implementation of an MWE-aware POS tagger and evaluation of its performance.", "labels": [], "entities": [{"text": "MWE-aware POS tagger", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8005171219507853}]}], "datasetContent": [{"text": "We conduct POS tagging experiments on the MWEannotated PTB, using sections 0-18 for training and sections 22-24 for test as usual.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.8771644234657288}, {"text": "MWEannotated PTB", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.8652780652046204}]}, {"text": "For the experiments, we use four versions of PTB with the following POS annotations.", "labels": [], "entities": [{"text": "PTB", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9590638279914856}]}, {"text": "Each word is annotated with the following in- shows sample annotations of MWE \"about to\" in each of the four versions of PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.9636971950531006}]}, {"text": "In (a), \"about/RB\" is annotated incorrectly, which is corrected in (b).", "labels": [], "entities": [{"text": "about/RB\"", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.7083083093166351}]}, {"text": "In (c), \"-B\" indicates the beginning token of an MWE and \"-I\" indicates an inside position of an MWE.", "labels": [], "entities": []}, {"text": "In (d), \"about to\" is annotated as an RB (we omit the POS tags for its internal words, which are IN and TO).", "labels": [], "entities": [{"text": "RB", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9595702290534973}, {"text": "IN", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9806693196296692}, {"text": "TO", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.8890742063522339}]}, {"text": "We use a CRF-based tagger for training and test on all the four PTB versions.", "labels": [], "entities": [{"text": "PTB", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9214436411857605}]}, {"text": "Our CRF can handle \"words with spaces\" (e.g. \"about to\" as a single token as well as separated tokens) as shown in.", "labels": [], "entities": []}, {"text": "This extension is only relevant to the case of the (d) MWE version.", "labels": [], "entities": []}, {"text": "summarizes the set of feature templates used in the experiments.", "labels": [], "entities": []}, {"text": "In, \"Head POS\" means the POS tag of the beginning token of an MWE.", "labels": [], "entities": [{"text": "POS", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9634745717048645}]}, {"text": "In the same way, \"Tail POS\" means the POS tag of the last token of an MWE.", "labels": [], "entities": [{"text": "Tail POS", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.911372721195221}, {"text": "POS", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9673960208892822}]}, {"text": "For example, for \"a lot of /DT\", its Head POS is DT and its Tail POS is IN.", "labels": [], "entities": [{"text": "IN", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9119672179222107}]}, {"text": "We evaluate POS tagging accuracy and MWE recognition accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.7140871286392212}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9759494066238403}, {"text": "MWE recognition", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8845473229885101}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9623304009437561}]}, {"text": "In POS evaluation, each token receives a tag in the cases of (a), (b) and (c), so the tagging accuracy is straightforwardly calculated.", "labels": [], "entities": [{"text": "POS evaluation", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.9209316968917847}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9836249351501465}]}, {"text": "MWE recognition accuracy is evaluated for the cases of (c) and (d).", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9097499251365662}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9677574634552002}]}, {"text": "For the purpose of comparison, we employ a simple baseline as well.", "labels": [], "entities": []}, {"text": "This baseline assigns each occurrence of an MWE its most frequent usage in the training part of PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.8854982852935791}]}, {"text": "Evaluation of MWE recognition accuracy is shown in precision, recall and F-measure.", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.969253420829773}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9608159065246582}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9997777342796326}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9996354579925537}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9986332058906555}]}, {"text": "We use the standard set of features based on unigram/bi-gram of words/POS.", "labels": [], "entities": []}, {"text": "For our MWE version, we add the word forms and POS tags of the first and the last internal words of MWEs as shown in Table 3.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9623934328556061}]}, {"text": "shows the results of MWE recognition.", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9758310616016388}]}, {"text": "Our MWE-aware CRF model (d) shows the best results.", "labels": [], "entities": [{"text": "MWE-aware CRF", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.5617532730102539}]}, {"text": "While the BIO model (c) significantly outperforms the baseline, it gives significantly lower results than our model.", "labels": [], "entities": [{"text": "BIO", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9061377048492432}]}, {"text": "We investigated errors in (d) and categorized them into three types.", "labels": [], "entities": []}, {"text": "\u2022 False Positive: System finds an MWE, while it is actually literal.", "labels": [], "entities": [{"text": "False", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9881349802017212}]}, {"text": "\u2022 False Negative: System misses to identify an MWE.", "labels": [], "entities": [{"text": "False Negative", "start_pos": 2, "end_pos": 16, "type": "METRIC", "confidence": 0.9282504022121429}]}, {"text": "\u2022 Misrecognition: System finds an MWE wrongly (correct answer is another MWE).", "labels": [], "entities": []}, {"text": "shows number of recognition errors of MWEs.", "labels": [], "entities": []}, {"text": "An example of the False Positive is \"a bit /RB\" in, which actually is a literal usage and should be tagged as \"a /DT, bit /NN\".", "labels": [], "entities": [{"text": "False", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9284487962722778}]}, {"text": "An example of the False Negative is \"in black and white /RB\", which is not recognized as an MWE.", "labels": [], "entities": [{"text": "False", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.8963380455970764}]}, {"text": "One reason of this type of errors is low or zero frequency of such MWEs in training data.", "labels": [], "entities": []}, {"text": "\"after all /RB\" (in is another False Negative example.", "labels": [], "entities": [{"text": "RB", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.6074532270431519}]}, {"text": "One example of Misrecognition errors stems from ambiguous MWEs.", "labels": [], "entities": [{"text": "Misrecognition", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.9532017111778259}]}, {"text": "For example, while \"how much\" only has MWE usages as RB, there are two RB usages of \"how much\" that have different POS tag sequences for the internal words.", "labels": [], "entities": []}, {"text": "Other examples of Misrecognition are due to zero or low frequency MWEs, whose substrings also matches shorter MWEs: \"quite/RB, a few/PRP\" while correct analysis is \"quite a few/RB\", and \"the hell /RB, out of /IN\" while the correct analysis is \"the hell out of /RB\".", "labels": [], "entities": [{"text": "Misrecognition", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.960968554019928}, {"text": "IN", "start_pos": 209, "end_pos": 211, "type": "METRIC", "confidence": 0.9545726776123047}]}], "tableCaptions": [{"text": " Table 1: Number of MWE types in Wiktionary and Penn Treebank", "labels": [], "entities": [{"text": "Wiktionary and Penn Treebank", "start_pos": 33, "end_pos": 61, "type": "DATASET", "confidence": 0.8164156824350357}]}, {"text": " Table 4: Per token accuracy (precision)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.668053388595581}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9964309930801392}]}, {"text": " Table 5: Recognition performance of MWEs", "labels": [], "entities": [{"text": "Recognition", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8163269758224487}, {"text": "MWEs", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.8944119215011597}]}, {"text": " Table 6: Recognition error of MWEs", "labels": [], "entities": [{"text": "Recognition error", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8264835178852081}, {"text": "MWEs", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.626213550567627}]}]}