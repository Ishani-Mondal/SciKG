{"title": [{"text": "Subtopic Annotation in a Corpus of News Texts: Steps Towards Automatic Subtopic Segmentation", "labels": [], "entities": [{"text": "Subtopic Annotation in a Corpus of News Texts", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8845500722527504}, {"text": "Subtopic Segmentation", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.6908059269189835}]}], "abstractContent": [{"text": "Subtopic segmentation aims at finding the boundaries among text passages that represent different subtopics, which usually develop a main topic in a text.", "labels": [], "entities": [{"text": "Subtopic segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8721149265766144}]}, {"text": "Being capable of automatically detecting subtopics is very useful for several Natural Language Processing applications.", "labels": [], "entities": []}, {"text": "This paper describes subtopic annotation in a corpus of news texts written in Brazilian Portuguese.", "labels": [], "entities": []}, {"text": "In particular, we focus on answering the main scientific questions regarding corpus annotation, aiming at both discussing and dealing with important annotation decisions and making available a reference corpus for research on subtopic structuring and segmentation.", "labels": [], "entities": []}, {"text": "Segmenta\u00e7\u00e3o topical visa segmentar um texto em passagens que representam subt\u00f3picos diferentes, os quais desenvolvem um t\u00f3pico principal de um texto.", "labels": [], "entities": []}, {"text": "A identifica\u00e7\u00e3o de subt\u00f3picos \u00e9 \u00fatil para diversas aplica\u00e7\u00f5es de Processamento de Linguagem Natural.", "labels": [], "entities": []}, {"text": "Este artigo descreve a anota\u00e7\u00e3o de subt\u00f3picos em um c\u00f3rpus de textos jornal\u00edsticos em Portugu\u00eas do Brasil.", "labels": [], "entities": []}, {"text": "Em particular, foca-se em responder as quest\u00f5es cientificas a respeito da anota\u00e7\u00e3o do c\u00f3rpus, visando discutir e lidar com quest\u00f5es importantes de anota\u00e7\u00e3o e disponibiliza\u00e7\u00e3o de um c\u00f3rpus de refer\u00eancia para pesquisas sobre estrutura\u00e7\u00e3o e segmenta\u00e7\u00e3o topical.", "labels": [], "entities": []}], "introductionContent": [{"text": "Subtopic segmentation aims at finding the boundaries among text passages that represent different subtopics, which usually develop the main topic in a text.", "labels": [], "entities": [{"text": "Subtopic segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8713260591030121}]}, {"text": "For example, a text about a football match would have \"football\" as the main topic and passages that might represent the subtopics \"preparation and training for the match\", \"moves of the match and final score\", and \"schedule of next matches\".", "labels": [], "entities": []}, {"text": "This task is useful for many important applications in Natural Language Processing (NLP), such as automatic summarization, question answering, and information retrieval and extraction.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.6943156818548838}, {"text": "summarization", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.7535704374313354}, {"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.9105970859527588}, {"text": "information retrieval and extraction", "start_pos": 147, "end_pos": 183, "type": "TASK", "confidence": 0.7936539649963379}]}, {"text": "For instance, explain that information retrieval with the identification of subtopics in the retrieved texts may provide the user with text fragments that are semantically and topically related to a given query.", "labels": [], "entities": []}, {"text": "This makes it easier for the user to quickly find the information of interest.", "labels": [], "entities": []}, {"text": "suggest that a question answering system, which aims to answer a question/query submitted by the user, may link this query to the subtopics in a text in order to increase the accuracy of the identification of the answer.", "labels": [], "entities": [{"text": "question answering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7390527427196503}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9971264004707336}]}, {"text": "says that, given some subtopic segmentation, automatic summarization may produce summaries that select different aspects from the collection of texts, producing better summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9509557485580444}]}, {"text": "Given its usefulness, it is common to prepare a reference segmentation that supports not only the study and understanding of the phenomenon, but also the development and evaluation of systems for automatic subtopic segmentation.", "labels": [], "entities": [{"text": "automatic subtopic segmentation", "start_pos": 196, "end_pos": 227, "type": "TASK", "confidence": 0.6798257033030192}]}, {"text": "As the construction of corpora is a time consuming and very expensive task, it is necessary to follow procedures to systematize it and to ensure a reliable annotation, in order to produce a scientifically sound resource.", "labels": [], "entities": []}, {"text": "claim that it is necessary to be concerned with the reliability, validity, and consistency of the corpus annotation process.", "labels": [], "entities": [{"text": "reliability", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.9714856743812561}, {"text": "validity", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9516192078590393}, {"text": "consistency", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9839056134223938}]}, {"text": "Because of this, many researchers, with such procedure in mind, suggest some methodological research questions, which maybe summarized in the following 7 steps: (1) choosing the phenomenon to annotate and the underlying theory, (2) selecting the appropriate corpus, (3) selecting and training the annotators, (4) specifying the annotation procedure, (5) designing the annotation interface, (6) choosing and applying the evaluation measures, and (7) delivering and maintaining the product.", "labels": [], "entities": []}, {"text": "In this paper, we report the subtopic annotation of a corpus of news texts written in Brazilian Portuguese.", "labels": [], "entities": []}, {"text": "The corpus, called CSTNews 1), was originally designed for multi-document processing and contains 50 clusters, with each cluster having 2 or 3 texts on the same topic.", "labels": [], "entities": []}, {"text": "In particular, we focus on answering the 7 research questions cited above, aiming at both discussing and dealing with important annotation decisions and making available a reference corpus for research on automatic subtopic structuring and segmentation.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized into two main sections.", "labels": [], "entities": []}, {"text": "Section 2 shows a brief discussion about relevant issues related to corpus annotation and some work on subtopic segmentation.", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7484717071056366}, {"text": "subtopic segmentation", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.7023498117923737}]}, {"text": "Section 3 describes our annotation under the light of the 7 annotation questions, including the description of our dataset and the quality evaluation of the subtopic segmentation.", "labels": [], "entities": []}, {"text": "Section 4 presents final remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The underlying premise of an annotation is that if people cannot agree enough, then either the theory is wrong (or badly stated or instantiated), or the process itself is flawed.", "labels": [], "entities": []}, {"text": "At first, it may seem intuitive to determine possible subtopic boundaries, but the task is very subjective and levels of agreement among humans tend to below (see, e.g., Hearst, 1997;.", "labels": [], "entities": []}, {"text": "Agreement also varies depending on the text genre/type that is segmented.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.976416826248169}]}, {"text": "For example, technical reports have headings and subheadings, while other genres, such as news texts, have little demarcation.", "labels": [], "entities": []}, {"text": "The quality of annotation may refer to the agreement and consistency with which it is applied.", "labels": [], "entities": [{"text": "consistency", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9594703316688538}]}, {"text": "As adopted by, we used the traditional kappa measure, which is better than simple percent agreement measures because it subtracts from the counts the expected chance agreement among judges.", "labels": [], "entities": []}, {"text": "The kappa measure produces results up to 1, when agreement is perfect.", "labels": [], "entities": []}, {"text": "It is assumed in the area that a 0.60 value is enough to the annotation to be reliable and conclusions maybe drawn; however, it is known that such value highly depends on the subjectivity of the task at hand.", "labels": [], "entities": []}, {"text": "In our case, we expect a lower value, since determining subtopics boundaries is a difficult task.", "labels": [], "entities": []}, {"text": "From left to right, shows the days of annotation, the groups of annotators (represented by A and B letters), the number of annotators in each group, the number of texts that were annotated in each group per day, and the obtained agreement value.", "labels": [], "entities": []}, {"text": "For instance, we see that the first day produced the best agreement among annotators, with a 0.656 agreement value for group A and 0.566 for group B.", "labels": [], "entities": [{"text": "agreement", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9742529392242432}]}, {"text": "On the other hand, the lowest agreement was in the second day, with 0.458 for group A and 0.447 for group B.", "labels": [], "entities": [{"text": "agreement", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9973288774490356}]}, {"text": "It is difficult to explain the fluctuations according to the day, but it maybe the case that, on the first day, the annotators benefitted from the recent training, and that, on the second day, their confidence faltered as they encountered new cases.", "labels": [], "entities": [{"text": "fluctuations", "start_pos": 31, "end_pos": 43, "type": "METRIC", "confidence": 0.9604588150978088}]}, {"text": "Agreement measures after the second day seem to improve, with some fluctuation.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8488804697990417}]}, {"text": "The average agreement was 0.560.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9702412486076355}]}, {"text": "Since this task is very subjective, such agreement is considered satisfactory.", "labels": [], "entities": []}, {"text": "However, it is a bit lower than the agreement score obtained by Hearst (1997) in her seminal work, which was 0.647.", "labels": [], "entities": [{"text": "agreement score", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.9920512437820435}]}, {"text": "This maybe explained by the fact that her work used fewer texts (only 12), which were expository texts, where subtopic boundaries are usually more clearly indicated.", "labels": [], "entities": []}, {"text": "From the annotated texts, the reference segmentation was established.", "labels": [], "entities": [{"text": "reference segmentation", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.5932495296001434}]}, {"text": "We computed the opinion of the majority (half plus one) in the boundaries.", "labels": [], "entities": []}, {"text": "We adopted this strategy because we were looking for more course-grained subtopic boundaries, and we believed that these would have the consensus of most of the annotators.", "labels": [], "entities": []}, {"text": "It is interesting that we observed only two cases with total agreement and others with more or less variation on segmentation.", "labels": [], "entities": []}, {"text": "The variation in segmentation is related to factors such as the interpretation of the text and prior knowledge about the subject, mainly.", "labels": [], "entities": []}, {"text": "As an example of segmentation, shows different segmentations for the text in.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.9794846177101135}]}, {"text": "The rows numbered from 1 to 5 represent the segmentation made by each of the five annotators.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9567063450813293}]}, {"text": "Each box represents a sentence and the segmentation is indicated by vertical lines.", "labels": [], "entities": []}, {"text": "The last line, labeled \"Final\", represents the reference segmentation, obtained from the majority of the annotators.", "labels": [], "entities": []}, {"text": "One may see, for instance, that the first annotator did not place any subtopic boundary.", "labels": [], "entities": []}, {"text": "The last boundary, after the last sentence, is expected, since the text ends.", "labels": [], "entities": []}, {"text": "The second annotator placed boundaries after the fifth and sixth sentences, besides the one in the end of the text.", "labels": [], "entities": []}, {"text": "These boundaries were the most indicated by the annotators and, therefore, were considered the ideal segmentation for the text, as shown in the last row.", "labels": [], "entities": []}, {"text": "shows the number of subtopics in the reference segmentation.", "labels": [], "entities": []}, {"text": "It maybe seen that there were eight texts (6% of the corpus) with only one subtopic, 24 texts (17%) with 2 subtopics, 50 texts (36%) with 3 subtopics, 32 texts (23%) with 4 subtopics, 18 texts (13%) with 5 subtopics, 4 texts (3%) with 6 subtopics, 2 texts (1%) with 7 subtopics, and 2 texts (1%) with 8 subtopics.", "labels": [], "entities": []}, {"text": "Overall, the average number of subtopic boundaries in a text is 3.", "labels": [], "entities": []}, {"text": "Most of them (99%) happen among paragraphs.", "labels": [], "entities": []}, {"text": "This is related to how people structure their writings, using paragraphs as basic discourse units for the organization of the text.", "labels": [], "entities": []}, {"text": "The descriptions given by each judge after a topic boundary were not used to define the final annotation.", "labels": [], "entities": []}, {"text": "In this study, the descriptions were used only for better understanding the annotators' decisions.", "labels": [], "entities": []}], "tableCaptions": []}