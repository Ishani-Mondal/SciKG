{"title": [{"text": "Improving English-Russian sentence alignment through POS tagging and Damerau-Levenshtein distance", "labels": [], "entities": [{"text": "English-Russian sentence alignment", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.5785531004269918}, {"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.75809645652771}]}], "abstractContent": [{"text": "The present paper introduces approach to improve English-Russian sentence alignment , based on POS-tagging of automatically aligned (by HunAlign) source and target texts.", "labels": [], "entities": [{"text": "English-Russian sentence alignment", "start_pos": 49, "end_pos": 83, "type": "TASK", "confidence": 0.5710812509059906}]}, {"text": "The initial hypothesis is tested on a corpus of bitexts.", "labels": [], "entities": []}, {"text": "Sequences of POS tags for each sentence (exactly, nouns, adjectives, verbs and pronouns) are processed as \"words\" and Damerau-Levenshtein distance between them is computed.", "labels": [], "entities": []}, {"text": "This distance is then normalized by the length of the target sentence and is used as a threshold between supposedly mis-aligned and \"good\" sentence pairs.", "labels": [], "entities": []}, {"text": "The experimental results show precision 0.81 and recall 0.8, which allows the method to be used as additional data source in parallel corpora alignment.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.999028205871582}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.999422550201416}, {"text": "parallel corpora alignment", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.6317036549250284}]}, {"text": "At the same time, this leaves space for further improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parallel multilingual corpora have long ago become a valuable resource both for academic and for industrial computational linguistics.", "labels": [], "entities": []}, {"text": "They are employed for solving problems of machine translation, for research in comparative language studies and many more.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8114253282546997}, {"text": "comparative language studies", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.8183397253354391}]}, {"text": "One of difficult tasks in parallel multilingual corpora building is alignment of its elements with each other, that is establishing a set of links between words and phrases of source and target language segments.", "labels": [], "entities": [{"text": "multilingual corpora building", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6052846113840739}]}, {"text": "Alignment can be done on the level of words, sentences, paragraphs or whole documents in text collection.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9686532616615295}]}, {"text": "Most widely used are word and sentence alignment, and the present paper deals with the latter one.", "labels": [], "entities": [{"text": "word and sentence alignment", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.675139881670475}]}, {"text": "Word alignment is an essential part of statistical machine translation workflow.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7583138048648834}, {"text": "statistical machine translation workflow", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.7718696221709251}]}, {"text": "However, usually it can only be done after sentence alignment is already present.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7084977775812149}]}, {"text": "Accordingly, there have been extensive research on the ways to improve it.", "labels": [], "entities": []}, {"text": "Basic algorithm of sentence alignment simply links sentences from source and target text in order of their appearance in the texts.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7285417467355728}]}, {"text": "E.g., sentence number 1 in the source corresponds to sentence number 1 in the target etc.", "labels": [], "entities": []}, {"text": "But this scheme by design can't handle one-to-many, many-to-one and many-to-many links (a sentence translated by two sentences, two sentences translated by one, etc) and is sensitive to omissions in source or translated text.", "labels": [], "entities": []}, {"text": "Mainstream ways of coping with these problems and increasing alignment quality include considering sentence length ( and using bilingual dictionaries) or cognates ( to estimate the possibility of sentences being linked.", "labels": [], "entities": []}, {"text": "showed that these ways provide generally good results for Russian as well.", "labels": [], "entities": []}, {"text": "But often this is not enough.", "labels": [], "entities": []}, {"text": "Sentence length can vary in translation, especially when translation language is typologically different from the source one.", "labels": [], "entities": []}, {"text": "As for bilingual dictionaries, it is sometimes problematic to gather and compile a useful set of them.", "labels": [], "entities": []}, {"text": "Thus, various additional methods were proposed, among them using part-of speech data from both source and target texts.", "labels": [], "entities": []}, {"text": "It is rather commonplace in word alignment).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7604745030403137}]}, {"text": "Using part-of speech tagging to improve sentence alignment for ChineseEnglish parallel corpus is presented in.", "labels": [], "entities": [{"text": "part-of speech tagging", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.6654158433278402}, {"text": "sentence alignment", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7043623626232147}, {"text": "ChineseEnglish parallel corpus", "start_pos": 63, "end_pos": 93, "type": "DATASET", "confidence": 0.9119012157122294}]}, {"text": "In the current paper we propose to use similar approach in aligning English-Russian translations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the part-of-speech based approach to improve quality of sentence alignment in our parallel corpus of learner translations available at http://rus-ltc.org.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7471668124198914}]}, {"text": "Only English to Russian translations were selected, as of now.", "labels": [], "entities": []}, {"text": "The workflow was as follows.", "labels": [], "entities": []}, {"text": "All source and target texts were automatically aligned with the help of HunAlign software () together with its wrapper LF Aligner by Andr\u00e1s Farkas (http://sourceforge.net/projects/aligner).", "labels": [], "entities": [{"text": "HunAlign", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.8840899467468262}]}, {"text": "The choice of aligner was based on high estimation by researchers (cf. () and its open-source code.", "labels": [], "entities": [{"text": "estimation", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9857178926467896}]}, {"text": "Sentence splitting was done with a tool from Europarl v3 Preprocessing Tools (http://www.statmt.org/europarl) written by Philipp Koehn and Josh Schroeder.", "labels": [], "entities": [{"text": "Sentence splitting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9642046689987183}, {"text": "Europarl v3 Preprocessing Tools", "start_pos": 45, "end_pos": 76, "type": "DATASET", "confidence": 0.918239414691925}]}, {"text": "Proper lists of non-breaking prefixes were used for both Russian and English.", "labels": [], "entities": []}, {"text": "HunAlign uses both bilingual dictionaries and Gale-Church sentence-length information.", "labels": [], "entities": [{"text": "HunAlign", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9440955519676208}]}, {"text": "Its results are quite good, considering the noisiness of our material.", "labels": [], "entities": []}, {"text": "However, about 30 percent of sentences are still mis-aligned.", "labels": [], "entities": []}, {"text": "The reasons behind this are different, but mostly it is sentence splitter errors (notwithstanding its preparation for Russian), omissions or number of sentences changing during translation.", "labels": [], "entities": [{"text": "sentence splitter", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7097409665584564}]}, {"text": "Here is atypical example: \"And these two fuels are superior to ethanol, Liao says, because they have a higher energy density, do not attract water, and are noncorrosive\".", "labels": [], "entities": []}, {"text": "||| \"\u042d\u0442\u0438 \u0434\u0432\u0430 \u0432\u0438\u0434\u0430 \u0442\u043e\u043f\u043b\u0438\u0432\u0430 \u044f\u0432\u043d\u043e \u043f\u0440\u0435\u0432\u043e\u0441\u0445\u043e\u0434\u044f\u0442 \u044d\u0442\u0430\u043d\u043e\u043b \u043f\u043e \u0441\u0432\u043e\u0438\u043c \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430\u043c.\"", "labels": [], "entities": []}, {"text": "0 ||| \"\u041f\u043e \u0441\u043b\u043e\u0432\u0430\u043c \u041b\u044f\u043e, \u043e\u043d\u0438 \u043e\u0431\u043b\u0430\u0434\u0430\u044e\u0442 \u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u043e\u0439 \u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u044c\u044e, \u043d\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0432\u043e\u0434\u0443, \u0430 \u0437\u043d\u0430\u0447\u0438\u0442 \u043d\u0435\u043a\u043e\u0440\u0440\u043e\u0437\u0438\u0439\u043d\u044b\u0435.\"", "labels": [], "entities": []}, {"text": "The translator transformed one English sentence into two Russian sentences.", "labels": [], "entities": []}, {"text": "Consequently, aligner linked the first Russian sentence to the source one, and the second sentence is left without its source counterpart (null link).", "labels": [], "entities": []}, {"text": "It should be said that in many cases HunAlign manages to cope with such problems, but not always, as we can see in the example above.", "labels": [], "entities": []}, {"text": "The cases of mis-alignment must be human corrected, which is very time-expensive, especially because there is noway to automatically assess the quality of alignment.", "labels": [], "entities": []}, {"text": "HunAlign's internal measure of quality is often not very helpful.", "labels": [], "entities": [{"text": "HunAlign", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9404040575027466}]}, {"text": "For example, for the first row of the table above it assigned rather high quality mark of 0.551299.", "labels": [], "entities": [{"text": "quality mark", "start_pos": 74, "end_pos": 86, "type": "METRIC", "confidence": 0.9437304437160492}]}, {"text": "Trying to predict alignment correctness with the help of Hun quality mark only for the whole our data set resulted in precision 0.727 and recall 0.548, which is much lower than our results presented below.", "labels": [], "entities": [{"text": "alignment correctness", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.8281449675559998}, {"text": "Hun quality mark", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.5931752324104309}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9992440938949585}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9993380904197693}]}, {"text": "We hypothesize that source and target sentence should inmost cases correspond in the number and order of content parts of speech (POS).", "labels": [], "entities": []}, {"text": "This data can be used to trace mis-aligned sentences and perhaps to find correct equivalents for them.", "labels": [], "entities": []}, {"text": "In order to test this hypothesis, our source and target texts were POS-tagged using Freeling 3.0 suite of language analyzers.", "labels": [], "entities": []}, {"text": "Freeling gives comparatively good results in English and Russian POS-tagging, using Markov trigram scheme trained on large disambiguated corpus.", "labels": [], "entities": []}, {"text": "Freeling tag set for English follows that of Penn TreeBank, while Russian tag set, according to Freeling manual, corresponds to EAGLES recommendations for morphosyntactic annotation of corpora described on http://www.ilc.cnr.it/EAGLES96/home.html.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9920938313007355}]}, {"text": "It is not trivial to project one scheme onto another completely, except for the main content words -nouns, verbs and adjectives.", "labels": [], "entities": []}, {"text": "Moreover, these three parts of speech are the ones used in the paper by, mentioned above.", "labels": [], "entities": []}, {"text": "So, the decision was made to take into consideration only the aforementioned lexical classes, with optional inclusion of pronouns (in real translations they often replace nouns and vice versa).", "labels": [], "entities": []}, {"text": "Thus, each sentence was assigned a \"POS watermark\", indicating number and order of content words in it.", "labels": [], "entities": []}, {"text": "Cf. the following sentence: \"Imagine three happy people each win $1 million in the lottery.\" and its \"POS watermark\": VANVNN, where N is noun, A is adjective and V is verb.", "labels": [], "entities": [{"text": "POS", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9811286926269531}, {"text": "VANVNN", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9949911236763}]}, {"text": "Here is the same analysis for its Russian translation counterpart: \"\u041f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u043c \u0441\u0435\u0431\u0435 \u0442\u0440\u0435\u0445 \u0441\u0447\u0430\u0441\u0442\u043b\u0438\u0432\u044b\u0445 \u043b\u044e\u0434\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u0438\u0433\u0440\u0430\u043b\u0438 \u0432 \u043b\u043e\u0442\u0435\u0440\u0435\u044e \u043f\u043e \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u0443 \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432.\"", "labels": [], "entities": []}, {"text": "Corresponding \"POS watermark\": VPANVNNN, where N is noun, V is verb, A is adjective and P is pronoun.", "labels": [], "entities": [{"text": "VPANVNNN", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.6126125454902649}]}, {"text": "Nouns and verbs are marked identically in Penn and EAGLES schemes.", "labels": [], "entities": [{"text": "Penn", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.911375105381012}, {"text": "EAGLES", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.657574474811554}]}, {"text": "Adjectives in Penn are marked as JJ, so this mark was corrected to A, which is also the mark for adjectives in EAGLES.", "labels": [], "entities": [{"text": "Penn", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.9661689400672913}, {"text": "A", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.9955452680587769}, {"text": "EAGLES", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.915406346321106}]}, {"text": "We considered to be 'pronouns' (P) those words which are marked as \"E\" in EAGLES and \"PRP\" in Penn.", "labels": [], "entities": [{"text": "EAGLES", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.6749383807182312}, {"text": "Penn", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.981859028339386}]}, {"text": "Thus, each content word is represented as one letter strictly corresponding to one lexical class.", "labels": [], "entities": []}, {"text": "Therefore our \"POS watermark\" can bethought of as a kind of \"word\".", "labels": [], "entities": [{"text": "POS watermark", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.7626067698001862}]}, {"text": "The difference between these \"words\" is computed using Damerau-Levenshtein distance.", "labels": [], "entities": [{"text": "Damerau-Levenshtein distance", "start_pos": 55, "end_pos": 83, "type": "METRIC", "confidence": 0.6417189538478851}]}, {"text": "Basically, it is the number of corrections, deletions, additions and transpositions needed to transform one character sequence into another.", "labels": [], "entities": []}, {"text": "We employ Python implementation of this algorithm by Michael Homer (published at http://mwh.geek.nz/2009/04/26/pythondamerau-levenshtein-distance).", "labels": [], "entities": []}, {"text": "According to it, the distance between POS watermarks of two sentence above is 2.", "labels": [], "entities": []}, {"text": "It means we need only two operations -adding one pronoun and one noun -to get target POS structure from source POS structure.", "labels": [], "entities": []}, {"text": "At the same time, the distance between VPVNANNNNNNNNNNVN and NVNNANANANN is as high as 10, which means that POS structures of these sentences are quite different.", "labels": [], "entities": []}, {"text": "Indeed, the sentences which generated these structures are obviously mis-aligned: \"If a solar panel ran its extra energy into a vat of these bacteria, which could use the energy to create biofuel, then the biofuel effectively becomes away to store solar energy that otherwise would have gone to waste.\"", "labels": [], "entities": []}, {"text": "||| \"\u041e\u0434\u043d\u0430\u043a\u043e \u043e\u043d\u0438 \u0432\u044b\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044e\u0442 \u044d\u043d\u0435\u0440\u0433\u0438\u0438 \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f.\"", "labels": [], "entities": []}, {"text": "One can suppose that there is correlation between Damerau-Levenshtein distance and the quality of alignment: the more is the distance the more is the possibility that the alignment of these two sentences has failed in one or the other way.", "labels": [], "entities": []}, {"text": "In the following chapter we present the results of the preliminary experiment on our parallel texts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Overall performance of pairs classifier  depending on the method.", "labels": [], "entities": []}]}