{"title": [{"text": "UdS at the CoNLL 2013 Shared Task", "labels": [], "entities": [{"text": "CoNLL 2013 Shared Task", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.8746545016765594}]}], "abstractContent": [{"text": "This paper describes our submission for the CoNLL 2013 Shared Task, which aims to to improve the detection and correction of the five most common grammatical error types in English text written by non-native speakers.", "labels": [], "entities": [{"text": "CoNLL 2013 Shared Task", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.8352356553077698}, {"text": "detection and correction of the five most common grammatical error types in English text written by non-native speakers", "start_pos": 97, "end_pos": 216, "type": "TASK", "confidence": 0.5843784477975633}]}, {"text": "Our system concentrates only on two of them; it employs machine learning classifiers for the ArtOrDet-, and a fully deterministic rule based workflow for the SVA error type.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction is not anew task in Natural Language Processing field.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7762176990509033}]}, {"text": "Many previous research was done to solve the problem.", "labels": [], "entities": []}, {"text": "Most of these works focus on article and preposition correction.", "labels": [], "entities": [{"text": "article and preposition correction", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.6125708222389221}]}, {"text": "In this paper we present our implementation of our system that participated in the CoNLL 2013 Shared Task for grammatical error correction.", "labels": [], "entities": [{"text": "CoNLL 2013 Shared Task", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.8283821493387222}, {"text": "grammatical error correction", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.7203264037768046}]}, {"text": "Out of the 28 annotated error types in the training data, this year's task focuses on 5 error types: article or determiner (ArtOrDet), preposition (Prep), noun number (Nn), verb form (Vform) and subject-verb agreement (SVA).", "labels": [], "entities": []}, {"text": "This error proportion can be seen in SVA mistakes only.", "labels": [], "entities": [{"text": "error proportion", "start_pos": 5, "end_pos": 21, "type": "METRIC", "confidence": 0.9714999794960022}]}, {"text": "The remaining part of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Chapters refcorp and 3 describe the data and system architecture.", "labels": [], "entities": []}, {"text": "Chapter 4.2 explains the ArtOrDet classification task.", "labels": [], "entities": [{"text": "ArtOrDet classification task", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.7500362793604533}]}, {"text": "Our experimental setup for ArtOrDet error is presented in Section 4.3.", "labels": [], "entities": []}, {"text": "Chapter 4.4 describes the results from our experiments and some analysis regarding the results.", "labels": [], "entities": []}, {"text": "Chapters 5.1 and 5.1.1 describe the task and issues respectively, Chapter 5.2 explains the how the subject-verb pairs are extracted, Chapter 5.3 is about the evaluation of the pairs.", "labels": [], "entities": []}, {"text": "Lastly, Chapter 8 will conclude our work.", "labels": [], "entities": []}], "datasetContent": [{"text": "After defining the error types, we split the corpus into training and testing dataset.", "labels": [], "entities": []}, {"text": "We select 50 documents from the corpus as a held-out test data and the rest is used for the training data.", "labels": [], "entities": []}, {"text": "For the training part, we extract the NP (which is not headed by pronoun) using the information from constituent parse tree and POS tags.", "labels": [], "entities": []}, {"text": "Each NP that is extracted represents one training example.", "labels": [], "entities": []}, {"text": "Thus, if an NP is incorrect then we label it to one of the label from Table 2.", "labels": [], "entities": []}, {"text": "We consider this task as a multi-class classification task, that one NP finds a mapping f : x \u2192 {c 1 , c 2 , . .", "labels": [], "entities": []}, {"text": ", c 8 } that maps x \u2208 NP s into one of the 8 labels.", "labels": [], "entities": []}, {"text": "For the first experiment, we select two well known).", "labels": [], "entities": []}, {"text": "Both of these methods are trained using the same training data and features which we are going to discuss in Subsection 4.3.5.", "labels": [], "entities": []}, {"text": "In the testing part, our classifier will predict a label for each NP.", "labels": [], "entities": []}, {"text": "If the classifier predicts that the observed NP is already corrector it needs to add article a then we apply a rule-based approach to make sure it puts the right article (a/an).", "labels": [], "entities": []}, {"text": "This rule-based will utilize CMU pronouncing dictionary from NLTK to do the checking and put conditional constraints such as checking whether this NP is an acronym or not.", "labels": [], "entities": []}, {"text": "The second and third experiments are inspired by).", "labels": [], "entities": []}, {"text": "We realize that the proportion of observed NP without article error outnumbers the observed NP with an article error (see).", "labels": [], "entities": [{"text": "article error", "start_pos": 103, "end_pos": 116, "type": "METRIC", "confidence": 0.953141838312149}]}, {"text": "Therefore, this huge proportion of correct NP may affect the classifier accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9794298410415649}]}, {"text": "To justify this claim, we will utilize error inflation method for the second experiment and do re-sampling and undersampling NP as the third experiment.", "labels": [], "entities": [{"text": "error inflation", "start_pos": 39, "end_pos": 54, "type": "METRIC", "confidence": 0.9442651867866516}]}, {"text": "After the pairing is complete, only the pairs which include VBP 2 /VBZ 3 tags for the verbs, or verb forms in the past tense of the copula (was/were) are retained for the agreement evaluation.", "labels": [], "entities": [{"text": "VBP 2 /VBZ", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.6609390377998352}]}, {"text": "If the number of the subject and verb don't agree, the verb form gets corrected.", "labels": [], "entities": []}, {"text": "2 plural verb form 3 third person singular verb form", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Error types in NUCLE corpus", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9482766389846802}, {"text": "NUCLE", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.8794431686401367}]}, {"text": " Table 5: ArtOrDet accuracy using error inflation", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.942055881023407}]}, {"text": " Table 7: Error Correction distribution", "labels": [], "entities": [{"text": "Error Correction", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8572335839271545}]}, {"text": " Table 4. The training for this classifier comes  from all NP with ArtOrDet error. Our result proves that  79% of the ArtOrDet can be corrected (see", "labels": [], "entities": []}]}