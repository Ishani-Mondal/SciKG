{"title": [], "abstractContent": [{"text": "We present Minimum Description Length techniques for learning the structure of weighted languages.", "labels": [], "entities": []}, {"text": "MDL is already widely used both for segmentation and classification tasks, and here we show it can be used to formalize further important tools in the descriptive linguists' toolbox, including the distinction between accidental and systematic gaps in the data, the detection of ambiguity, the selective discarding of data, and the merging of categories.", "labels": [], "entities": [{"text": "segmentation and classification tasks", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.7018143534660339}]}], "introductionContent": [{"text": "The Minimum Description Length (MDL, see framework is primarily about data compression: if we are given some data D, our goal is to find a model M, and a correction term E, such that the model output and the correction term together describe the data, and transmitting M and E takes fewer bits than transmitting any competing M and E . From the very beginning, starting with P\u00af an . ini, linguists have put a premium on brevity.", "labels": [], "entities": [{"text": "data compression", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7420856952667236}]}, {"text": "The hope is that the shortest theory is the best theory (see, at least if we are willing to posit a theory of Universal Grammar (UG) that will let us specify M briefly, since we can assume UG to be amortized over many languages.", "labels": [], "entities": []}, {"text": "In this paper we study the problem of compressing weighted languages by presenting them via weighted finite state automata (WFSA).", "labels": [], "entities": []}, {"text": "The theoretical approach we discuss here has along history: the founding paper of Kolmogorov complexity,, already studied the problem of inferring a grammar from data, and uses MDL to infer CFGs from corpora, there conceived of as long strings over a finite alphabet.", "labels": [], "entities": []}, {"text": "It is fair to say that this theory has not had much impact on computational practice, where grammatical inference is dominated by the standard n-gram based language modeling methods, see for an excellent summary of the basic ideas and techniques, most of which are still in wide use.", "labels": [], "entities": [{"text": "grammatical inference", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7128589600324631}]}, {"text": "While the two approaches may coincide in certain cases (see, and in theory ngram models are just a special case of the general WFSA, in practice they are divided by a fundamental difference in modeling unseen data.", "labels": [], "entities": []}, {"text": "From an engineering standpoint, are entirely right in saying: No matter how much data we have, we never have enough.", "labels": [], "entities": []}, {"text": "Linguists, starting perhaps with, draw a bright line between accidentally and systematically missing data, and would prefer to restrict backoff techniques to the accidental gaps.", "labels": [], "entities": []}, {"text": "The distinction is often lost in applied work, because the models need to be builtin a noisy environment, where frequent typos like *teh and similar performance errors can easily overwhelm genuine items like boisterous or mopeds by an order of magnitude or even more.", "labels": [], "entities": []}, {"text": "In the eyes of many linguists, this observation alone is sufficient to rob probabilistic models of grammatical content, since this makes it impossible to define a single threshold g such that all and only strings with weight greater than g are grammatical.", "labels": [], "entities": []}, {"text": "Aside from this subtle but important distinction between accidental and systematic gaps, both kinds of language modeling can be cast in the same formal terms: we fit a model M that minimizes some function E (typically, the squared sum) of the error E.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7412801384925842}]}, {"text": "Obviously, the more parameters M has, the better fit we can obtain.", "labels": [], "entities": []}, {"text": "Much of contemporary computational linguistics follows the route of training simple models such as Hidden Markov Models (HMMs) and probabilistic context-free grammars (PCFGs) with very many parameters, and stops adding more only when compressing the memory footprint is of paramount importance.", "labels": [], "entities": []}, {"text": "As ( notes, applications like the contextual speller of Microsoft Office simply could not ship without keeping the language model within reasonable size limits.", "labels": [], "entities": []}, {"text": "In such cases, we are quite willing to trade in E for gains in the size M of M, and considerations of optimizing the sum of the two are simply irrelevant.", "labels": [], "entities": []}, {"text": "In contrast, our strategy is to search for model which measures both M and E in bits, and optimizes the sum M + E, not because we put such a premium on data compression, but rather because we follow in P\u00af an . ini's footsteps.", "labels": [], "entities": []}, {"text": "Our goal is finding structural models capable of distinguishing structurally excluded (ungrammatical) strings like furiously sleep ideas green colorless from low probability but grammatical strings like colorless green ideas sleep furiously.", "labels": [], "entities": []}, {"text": "For this more ambitious goal comparing models with different number of parameters is a key issue, and this is precisely where MDL is helpful.", "labels": [], "entities": []}, {"text": "The rest of this Introduction provides the basic definitions, notation, and terminology, all fairly standard except for the use of Moore rather than Mealy machines -the significance of this choice will be discussed in Section 2.", "labels": [], "entities": []}, {"text": "In Section 1 we bring a fundamental idea of signal processing, quantization error, to bear on the problem of model selection, illustrating the issue on areal example, the proquant system of Hungarian.", "labels": [], "entities": [{"text": "model selection", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.723719984292984}]}, {"text": "In Section 2 we show how one of the most powerful tools at disposal of the linguist, ambiguity, can be detected by MDL, bringing another standard idea, signal to noise ratio to bear.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss another real example, Hungarian morphotactics, and show that two methods widely (but shamefacedly) used in practice, discarding data and merging descriptive categories, can be used on a principled basis within MDL.", "labels": [], "entities": []}, {"text": "Our goal is to show that by consistent application of MDL principles we can automatically setup the kind of models that linguists would setup.", "labels": [], "entities": []}, {"text": "Ultimately, both man and machine work toward the same goal, optimization of grammar elegance or, what is the same, brevity.", "labels": [], "entities": []}, {"text": "Given some finite alphabet \u03a3, a weighted language p over this alphabet is defined as a mapping p : \u03a3 * \u2192 R taking non-negative values such that \u03b1\u2208\u03a3 * p(\u03b1) = 1.", "labels": [], "entities": []}, {"text": "This is less general than the standard notion of noncommutative power series with weights taken in arbitrary semirings) but will suffice here.", "labels": [], "entities": []}, {"text": "The stringset {\u03b1|p(\u03b1) > 0} is called the support of p and will be denoted by S(p).", "labels": [], "entities": []}, {"text": "Given two weighted languages p and q, we say the Kullback-Leibler (KL) approximation error Q of q relative top is \u03b1\u2208S(q) p(\u03b1) log(p(\u03b1)/q(\u03b1)).", "labels": [], "entities": [{"text": "approximation error Q", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.8327181140581766}]}, {"text": "The entropy of p is defined as \u2212 \u03b1\u2208S(p) p(\u03b1) log(p(\u03b1)).", "labels": [], "entities": []}, {"text": "A WFSA M is defined by a square transition matrix M whose element m ij give the probability of transition from state i to state j, an emission list h that gives a string hi \u2208 \u03a3 * for each i = 0, and an acceptance vector a whose i-th component is 1 if i is an accepting state and 0 otherwise.", "labels": [], "entities": [{"text": "WFSA M", "start_pos": 2, "end_pos": 8, "type": "TASK", "confidence": 0.7985280454158783}]}, {"text": "There is a unique initial state which starts the state numbering at 0, and we permit states with empty outputs.", "labels": [], "entities": []}, {"text": "Rows of M must sum to 1.", "labels": [], "entities": []}, {"text": "Thus we have defined WFSA as normalized probabilityweighted nondeterministic Moore machines.", "labels": [], "entities": [{"text": "WFSA", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.7744386792182922}]}, {"text": "The weight a WFSA assigns to a generation path is the product of the weights on the edges traversed, and the weight it assigns to a string \u03b1 is the sum of the weights assigned to all paths that generate \u03b1.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Frequencies of proquants in the Hungarian Webcorpus", "labels": [], "entities": [{"text": "Frequencies of proquants in the Hungarian Webcorpus", "start_pos": 10, "end_pos": 61, "type": "DATASET", "confidence": 0.5593375265598297}]}, {"text": " Table 2: List models with character-based string  encoding", "labels": [], "entities": [{"text": "character-based string  encoding", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.6835220456123352}]}, {"text": " Table 3: Hub models with/out ambiguous hogy", "labels": [], "entities": []}, {"text": " Table 4. Some letters are quite rare, in particu- lar, I makes up less than 0.06% of O and 0.013%  of Y . Columns KL O and KL Y show the KL di- vergence of O and Y from models obtained by by  discarding words containing the letter in question,  columns P O and P Y show the weight of the strings  that are getting discarded.", "labels": [], "entities": [{"text": "KL di- vergence", "start_pos": 138, "end_pos": 153, "type": "METRIC", "confidence": 0.6303478628396988}]}, {"text": " Table 4: Divergence caused by discarding data", "labels": [], "entities": [{"text": "Divergence", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9848339557647705}]}]}