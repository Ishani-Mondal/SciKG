{"title": [{"text": "Generating Natural Language Questions to Support Learning On-Line", "labels": [], "entities": [{"text": "Generating Natural Language Questions", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7383427172899246}]}], "abstractContent": [{"text": "When instructors prepare learning materials for students, they frequently develop accompanying questions to guide learning.", "labels": [], "entities": []}, {"text": "Natural language processing technology can be used to automatically generate such questions but techniques used have not fully leveraged semantic information contained in the learning materials or the full context in which the question generation task occurs.", "labels": [], "entities": [{"text": "question generation task", "start_pos": 227, "end_pos": 251, "type": "TASK", "confidence": 0.8059450586636862}]}, {"text": "We introduce a sophisticated template-based approach that incorporates semantic role labels into a system that automatically generates natural language questions to support online learning.", "labels": [], "entities": []}, {"text": "While we have not yet incorporated the full learning context into our approach, our preliminary evaluation and evaluation methodology indicate our approach is a promising one for supporting learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ample research (e.g., shows that learners learn more, and more deeply, if they are prompted to examine their learning materials while and after they study.", "labels": [], "entities": []}, {"text": "Often, these prompts consist of questions related to the learning materials.", "labels": [], "entities": []}, {"text": "After reading a given passage or section of text, learners are familiar with learning exercises which consist of questions they need to answer.", "labels": [], "entities": []}, {"text": "Questioning is one of the most common and intensively studied instructional strategies used by teachers (.", "labels": [], "entities": []}, {"text": "Questions embedded in text, or presented while learners are studying text, are hypothesized to promote selfexplanation which is known to increase comprehension and enhance transfer of learning (e.g.,).", "labels": [], "entities": []}, {"text": "Traditionally, these questions have been constructed by educators.", "labels": [], "entities": []}, {"text": "Recent research, though, has investigated how natural language processing techniques can be used to automatically generate these questions (.", "labels": [], "entities": []}, {"text": "While the automated approaches have generally focussed on syntactic features, we propose an approach that also takes semantic features into account, in conjunction with domain dependent and domain independent templates motivated by educational research.", "labels": [], "entities": []}, {"text": "After introducing our question generation system, we will provide a preliminary analysis of the performance of the system on educational material, and then outline our future plans to tailor the questions to the needs of specific learners and specific learning outcomes.", "labels": [], "entities": [{"text": "question generation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7156089693307877}]}], "datasetContent": [{"text": "There remains no standard set of evaluation metrics for assessing the quality of question generation output.", "labels": [], "entities": []}, {"text": "Some present no evaluation at all).", "labels": [], "entities": []}, {"text": "Among those who do perform an evaluation, there does appear to be a consensus that some form of human evaluation is necessary.", "labels": [], "entities": []}, {"text": "Despite this agreement in principle, approaches tend to diverge thereafter.", "labels": [], "entities": []}, {"text": "There are differences in the evaluation criteria and the evaluation procedure.", "labels": [], "entities": []}, {"text": "Most previous efforts in QG have not gone beyond manual evaluation.", "labels": [], "entities": [{"text": "QG", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.93397057056427}]}, {"text": "While some have gone a step further and built models for ranking based on the probability of a question being acceptable), these models have not had a strong basis in pedagogy.", "labels": [], "entities": []}, {"text": "While a question that is both syntactically and semantically wellformed is considered acceptable in some evaluation schemes, such questions can greatly outnumber the questions that we can reasonably expect a student would want or have time to answer.", "labels": [], "entities": []}, {"text": "We implement a classifier that attempts to identify the questions that are the most pedagogically useful.", "labels": [], "entities": []}, {"text": "For our initial evaluation of the performance of our QG system, we selected a subset of 10 documents from the collection described in the previous section.", "labels": [], "entities": []}, {"text": "On average, each document contained 25 sentences.", "labels": [], "entities": []}, {"text": "From the 10 documents, our system generated 1472 questions in total, an average of 5.9 questions per sentence.", "labels": [], "entities": []}, {"text": "Due to the educational nature of this material, we needed evaluators with educational training rather than naive ones.", "labels": [], "entities": []}, {"text": "Accordingly, the questions we generated were evaluated by a graduate student from the Faculty of Education.", "labels": [], "entities": []}, {"text": "She was asked to give binary judgements for grammaticality, semantic validity, vagueness, answerability, and learning value.", "labels": [], "entities": []}, {"text": "For each question, two aspects of answerability were evaluated.", "labels": [], "entities": []}, {"text": "The first aspect was whether the question was answerable from the source sentence from which it was generated.", "labels": [], "entities": []}, {"text": "The second was whether the question was answerable given the source document as a whole.", "labels": [], "entities": []}, {"text": "The evaluator was given no predetermined guidelines regarding the relationships among the evaluation criteria (e.g., the influence of vagueness and answerability on learning value).", "labels": [], "entities": []}, {"text": "This aspect of the evaluation was left to her discretion as an educator.", "labels": [], "entities": []}, {"text": "She found that 85% of the questions were grammatical, with 66% of them actually making sense.", "labels": [], "entities": []}, {"text": "It was determined that 14% of the questions were answerable from the sentence used to generate them, while 20% of them were answerable from the document.", "labels": [], "entities": []}, {"text": "Finally, she determined that 17% of the questions had learning value according to the prescribed learning outcomes for the curriculum being modeled.", "labels": [], "entities": []}, {"text": "Aside from performing this evaluation, the evaluator was not involved in this research.", "labels": [], "entities": []}, {"text": "Given this evaluation, we then built a classifier which used logistic regression (L2 regularized log-likelihood) to classify on learning value.", "labels": [], "entities": []}, {"text": "We used length, language model, SRL, named entity, glossary, and syntax features.", "labels": [], "entities": []}, {"text": "Length and language model features measure the token count and grammaticality of a question and the sentence from which it was generated.", "labels": [], "entities": [{"text": "Length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9302504658699036}]}, {"text": "SRL features include the token count of each semantic role in the generating predicate frame, whether each role is required by the matching template, and whether each role's text is used.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7419600486755371}]}, {"text": "Named entity features indicate the presence of each of nine named entity types in both the source sentence and generated question.", "labels": [], "entities": []}, {"text": "Glossary features note the number of glossary terms that appear in a sentence and question and a measure of the average importance of each term, which we calculated from a simple in-terms-of graph ( we constructed from the glossary.", "labels": [], "entities": []}, {"text": "This graph has directed edges between each glossary term and the terms that appear in its gloss.", "labels": [], "entities": []}, {"text": "Syntax features identify the depth of the generating predicate frame in the source sentence and the POS tag of its predicate.", "labels": [], "entities": []}, {"text": "Without adding noise, the training set had 217 questions with learning value and 1101 questions without learning value.", "labels": [], "entities": []}, {"text": "The classifier obtained precision and recall scores of 0.47 and 0.22 respectively for questions with learning value, along with scores of 0.79 and 0.92 for questions with no learning value.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9995723366737366}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.997848629951477}]}, {"text": "We then added noise to the training set by relabelling any grammatical question that made sense as having learning value.", "labels": [], "entities": []}, {"text": "This relabelling resulted in a training set of 778 questions with learning value and only 540 questions without learning value.", "labels": [], "entities": []}, {"text": "The classifier trained on this noisy set showed a precision score on learning value questions decreased to 0.29 but a dramatic increase in recall to 0.81.", "labels": [], "entities": [{"text": "precision score", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.982893705368042}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9994210004806519}]}, {"text": "For questions with no learning value, the precision increased slightly to 0.86 which was offset by a dramatic decrease in recall to 0.38.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9997103810310364}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9991432428359985}]}, {"text": "So when the system generates a poor quality question, we have a high probability of knowing that it is a poor question which allows us to then filter or discard it.", "labels": [], "entities": []}], "tableCaptions": []}