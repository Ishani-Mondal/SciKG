{"title": [{"text": "A Hybrid Model for Grammatical Error Correction", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.9035459359486898}]}], "abstractContent": [{"text": "This paper presents a hybrid model for the CoNLL-2013 shared task which focuses on the problem of grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.6330073177814484}]}, {"text": "This year's task includes determiner, preposition, noun number, verb form, and subject-verb agreement errors which is more comprehensive than previous error correction tasks.", "labels": [], "entities": []}, {"text": "We correct these five types of errors in different modules where either machine learning based or rule-based methods are applied.", "labels": [], "entities": []}, {"text": "Pre-processing and post-processing procedures are employed to keep idiomatic phrases from being corrected.", "labels": [], "entities": [{"text": "Pre-processing", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9570320844650269}]}, {"text": "We achieved precision of 35.65%, recall of 16.56%, F 1 of 22.61% in the official evaluation and precision of 41.75%, recall of 20.29%, F 1 of 27.3% in the revised version.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9998045563697815}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9997836947441101}, {"text": "F 1", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9968782365322113}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.999777615070343}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.999680757522583}, {"text": "F 1", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9960250854492188}]}, {"text": "Some further comparisons employing different strategies are made in our experiments .", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic Grammatical Error Correction (GEC) for non-native English language learners has attracted more and more attention with the development of natural language processing, machine learning and big-data techniques.", "labels": [], "entities": [{"text": "Automatic Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7878923501287188}]}, {"text": "\uf02a The CoNLL-2013 shared task focuses on the problem of GEC in five different error types including determiner, preposition, noun number, verb form, and subject-verb agreement which is more complicated and challenging than previous correction tasks.", "labels": [], "entities": [{"text": "GEC", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9034274220466614}]}, {"text": "Other than most previous works which concentrate most on determiner and preposition errors, more error types introduces the possibility of correcting multiple interacting errors such as de-\uf02a Corresponding author terminer vs. noun number and preposition vs. verb form.", "labels": [], "entities": []}, {"text": "Generally, for GEC on annotated data such as the NUCLE corpus () in this year's shared task which contains both original errors and human annotations, there are two main types of approaches.", "labels": [], "entities": [{"text": "GEC", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9494061470031738}, {"text": "NUCLE corpus", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9590157270431519}]}, {"text": "One of them is the employment of external language materials.", "labels": [], "entities": []}, {"text": "Although there are minor differences on strategies, the main idea of this approach is to use frequencies as a filter, such as n-gram counts, and take those phrases that have relatively high frequencies as the correct ones.", "labels": [], "entities": []}, {"text": "Typical works are shown in ( and.", "labels": [], "entities": []}, {"text": "Similar methods also exist in HOO shared tasks 1 such as the web 1TB n-gram features used by and the large-scale ngram model described by.", "labels": [], "entities": []}, {"text": "The other type is machine learning based approach which considers most on local context including syntactic and semantic features.", "labels": [], "entities": []}, {"text": "take maximum entropy as their classifier and apply some simple parameter tuning methods.", "labels": [], "entities": []}, {"text": "present their classifier-based models together with a few representative features.", "labels": [], "entities": []}, {"text": "invite a meta-learning approach and show its effectiveness.", "labels": [], "entities": []}, {"text": "introduce an alternating structure optimization based approach.", "labels": [], "entities": []}, {"text": "Most of the works mentioned above focus on determiner and preposition errors.", "labels": [], "entities": [{"text": "determiner and preposition errors", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8103296309709549}]}, {"text": "Besides, propose a method to correct verb form errors through combining the features of parse trees and n-gram counts.", "labels": [], "entities": []}, {"text": "To our knowledge, no one focused on noun form errors in specific researches.", "labels": [], "entities": [{"text": "noun form errors", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.749669631322225}]}, {"text": "In this paper, we propose a hybrid model to solve the problem of GEC for five error types.", "labels": [], "entities": [{"text": "GEC", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8042634725570679}]}, {"text": "Machine learning based methods are applied to solve determiner (ArtOrDet), preposition (Prep) and noun form (Nn) problems while rule-based methods are proposed for subject-verb agreement (SVA) and verb form (Vform) problems.", "labels": [], "entities": []}, {"text": "We treat corrections of errors in each type as individual sub problems the results of which are combined through a result combination module.", "labels": [], "entities": []}, {"text": "Solutions on interacting error corrections were considered originally but dropped at last because of the bad effects brought about by them such as the accumulation of errors which lead to a very low performance.", "labels": [], "entities": [{"text": "interacting error corrections", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6520288288593292}]}, {"text": "We perform feature selection and confidence tuning in machine learning based modules which contribute a lotto our performance.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7210990637540817}]}, {"text": "Also, pre-processing and post-processing procedures are employed to keep idiomatic phrases from being corrected.", "labels": [], "entities": []}, {"text": "Through experiments, we found that the result of the system was affected by many factors such as the selection of training samples and features, and the settings of confidence parameters in classifiers.", "labels": [], "entities": []}, {"text": "Some of the factors make the whole system too sensitive that it can easily be trapped into a local optimum.", "labels": [], "entities": []}, {"text": "Some comparisons are shown in our experiments section.", "labels": [], "entities": []}, {"text": "No other external language materials are included in our model except for several NLP tools which will be introduced in \u00a75.2.", "labels": [], "entities": []}, {"text": "We achieved precision of 35.65%, recall of 16.56% and F 1 of 22.61% in the official score of our submitted result.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9998629093170166}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9998506307601929}, {"text": "F 1", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9965253174304962}]}, {"text": "However, it was far from satisfactory mainly due to the ill settings of confidence parameters.", "labels": [], "entities": []}, {"text": "Trying to find out a set of optimal confidence parameters, our model is able to reach an upper bound of precision of 34.23%, recall of 25.56% and F 1 of 29.27% on the official test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9997013211250305}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9998282194137573}, {"text": "F 1", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.996625542640686}, {"text": "official test set", "start_pos": 167, "end_pos": 184, "type": "DATASET", "confidence": 0.7653189500172933}]}, {"text": "For the revised version, we achieved precision of 41.75%, recall of 20.29%, and F 1 of 27.3%.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9998449087142944}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.999863862991333}, {"text": "F 1", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9958575963973999}]}, {"text": "The remainder of this paper is arranged as follows.", "labels": [], "entities": []}, {"text": "The next section introduces our system architecture.", "labels": [], "entities": []}, {"text": "Section 3 describes machine learning based modules.", "labels": [], "entities": []}, {"text": "Section 4 shows rule based modules.", "labels": [], "entities": []}, {"text": "Experiments and analysis are arranged in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we give our discussion and conclusion in Section 6 and 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The performance of each machine learning module is affected by the selection of training samples, features and confidence tuning for the maximum entropy classifier.", "labels": [], "entities": []}, {"text": "All these factors contribute more or less to the final performance and need to be carefully developed.", "labels": [], "entities": []}, {"text": "In our experiments, we focus on machine learning based modules and make comparisons on sample selection, confidence tuning and feature selection and list a series of results before and after applying our strategies.", "labels": [], "entities": []}, {"text": "In our experiment, the performance is measured with precision, recall and F 1 -measure where Precision is the amount of predicted corrections that are also corrected by the manual annotators divided by the whole amount of predicted corrections.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9997192025184631}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9997774958610535}, {"text": "F 1 -measure", "start_pos": 74, "end_pos": 86, "type": "METRIC", "confidence": 0.9871918261051178}, {"text": "Precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9986647367477417}]}, {"text": "Recall has the same numerator as precision while its denominator is the amount of manually corrected errors.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7021397948265076}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9993368983268738}]}, {"text": "They are in accordance with those measurements generated by the official m2scorer) to a great extent and easily to be integrated in our program.", "labels": [], "entities": [{"text": "m2scorer", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.700856626033783}]}, {"text": "As we have mentioned in Section 3, we don't employ all samples but make use of all with (with errors and annotations) instances and sample the without ones (without errors) for training.", "labels": [], "entities": []}, {"text": "And the sampling for without type is totally random without loss of generality.", "labels": [], "entities": []}, {"text": "We apply the same strategy in all of these three error types (ArtOrDet, Prep and Nn) and try several ratios of with-without to find out whether this ratio has great impact on the final result and which ratio performs best.", "labels": [], "entities": [{"text": "Prep", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.7984489798545837}]}, {"text": "We use the 80%-10%-10% data (mentioned in \u00a75.1) for our experiments and make comparisons of different ratios on developing data.", "labels": [], "entities": []}, {"text": "The experimental results are described in detail in.", "labels": [], "entities": []}, {"text": "Confidence tuning is applied in all these three error types which contributes most to the final performance in our model.", "labels": [], "entities": []}, {"text": "We compare the results before and after tuning in all sample ratios that we designed and they are also depicted in.", "labels": [], "entities": []}, {"text": ".4 .5 .6 presision before and after tuning recall before and after tuning F 1 before and after tuning From the three groups of data in, we notice that the ratio of samples has little impact on F 1 . This phenomenon shows that our conclusion goes against the previous work by.", "labels": [], "entities": [{"text": "presision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9783855080604553}]}, {"text": "We believe it is mainly due to our confidence tuning which makes the parameters vary much under different sample ratios, that is, if given the same parameters, the effect of sample ratio selection may become obvious.", "labels": [], "entities": []}, {"text": "Unfortunately, we didn't do such a systematic comparison in our work.", "labels": [], "entities": []}, {"text": "The improvement under confidence tuning can be seen clearly in all ratios of with-without samples.", "labels": [], "entities": []}, {"text": "The confidence tuning algorithm employed in our work is better than the traditional tuning methods that assign a fixed threshold for each category or for all categories (about 1%~2% better measured by F 1 ).", "labels": [], "entities": []}, {"text": "However, although we are able to pick out the training data with a high F 1 through confidence tuning for the developing data, it is difficult for us to choose a set of confidence parameters that also fits the test data well.", "labels": [], "entities": [{"text": "F 1 through confidence tuning", "start_pos": 72, "end_pos": 101, "type": "METRIC", "confidence": 0.943003797531128}]}, {"text": "Given several close F 1 s, the numerical values of denominators and numerators which determine the precision and recall can vary a lot.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9993973970413208}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9988915324211121}]}, {"text": "For example, one set that has a high precision and low recall may share the similar F 1 with another set that has a low precision and high recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9980061650276184}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.993937611579895}, {"text": "F 1", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9841648638248444}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9917495250701904}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.990558385848999}]}, {"text": "Our work lacked of the development on how to control the number of proposed errors to make leverage on the performance between developing set and testing set.", "labels": [], "entities": []}, {"text": "It resulted in that the developing set and the testing set were not balanced at all, and our model was notable to keep the sample distribution as the training set.", "labels": [], "entities": []}, {"text": "This is the main factor that leads to a low performance in our submitted result which can be clearly seen in.", "labels": [], "entities": []}, {"text": "The upper bound performance of our system achieves precision of 34.23%, recall of 25.56% and F 1 of 29.27%, in which the F 1 goes 7% beyond our submitted system.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9997231364250183}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9998094439506531}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9974201023578644}, {"text": "F 1", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9892864227294922}]}, {"text": "We notice that results of all metrics of the three error types where machine learning algorithms are applied improve with the simultaneous increase of numerators and denominators.", "labels": [], "entities": []}, {"text": "This is especially noticeable in Prep.", "labels": [], "entities": [{"text": "Prep.", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8207766711711884}]}, {"text": "For the other two types SVA and Vform, we just apply several heuristic rules to solve a subset of problems and the case of Vform has not been solved well such as tense and voice.", "labels": [], "entities": []}, {"text": "Genetic Algorithm (GA) is applied to process feature reduction and subset selection.", "labels": [], "entities": [{"text": "Genetic Algorithm (GA)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5836503446102143}, {"text": "process feature reduction", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6278665562470754}, {"text": "subset selection", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7052237391471863}]}, {"text": "This is done in ArtOrDet type in which we extract as many as 350,000 binary features.", "labels": [], "entities": []}, {"text": "For error type Prep and Nn, the feature dimensionalities we constructed were not as high as that in ArtOrDet, and the improvements under GA were not obvious which we would not discuss in this work.", "labels": [], "entities": [{"text": "error type Prep", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.6519813736279806}]}, {"text": "Through experiments on a few sample ratios, we notice that feature selection using genetic algorithm is able to reduce the feature dimensionality to about 170,000 which greatly lowers down the downstream computational complexity.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7258732914924622}]}, {"text": "However, the improvement contributed by GA after confidence tuning is not obvious as that before confidence tuning.", "labels": [], "entities": [{"text": "GA", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9415349960327148}]}, {"text": "We think it is partly because of the bad initialization of GA which is to be improved in our future work.", "labels": [], "entities": [{"text": "initialization", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.9378188848495483}, {"text": "GA", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.5026025772094727}]}, {"text": "The unfixed parameters may also lead to such a result which we didn't discuss enough in our work.", "labels": [], "entities": []}, {"text": "The comparison before and after GA is described in.", "labels": [], "entities": [{"text": "GA", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.4037029445171356}]}, {"text": "Pre-processing and post-processing we propose also contribute to some extent which we could see from.", "labels": [], "entities": []}, {"text": "Some idiomatic phrases are excluded from being corrected in preprocessing which enhances precision while some are being modified in post-processing to improve recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9990984201431274}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9960297346115112}]}, {"text": "Without pre-processing and post-processing% Final%.", "labels": [], "entities": []}, {"text": "Comparison with and without preprocessing and post-processing.", "labels": [], "entities": []}, {"text": "We didn't do much on the interacting errors problem since we didn't workout perfect plans to solve it.", "labels": [], "entities": []}, {"text": "So, in the result combination module, we just simply combine the result of each part together..", "labels": [], "entities": []}, {"text": "Comparisons before and after Genetic Algorithm on ArtOrDet error type.", "labels": [], "entities": []}, {"text": "ME, GA, and Tuning stand for Maximum Entropy, Genetic Algorithm and confidence tuning.", "labels": [], "entities": [{"text": "ME", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9586761593818665}, {"text": "GA", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.896349310874939}]}, {"text": "In the revised version, under further corrections for the gold annotations, our model achieves precision of 41.75%, recall of 20.19% and F 1 of 27.3%.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9997950196266174}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9998457431793213}, {"text": "F 1", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9962583184242249}]}], "tableCaptions": [{"text": " Table 1. Different performances according to dif- ferent confidence parameters. Det stands for Ar- tOrDet.", "labels": [], "entities": [{"text": "Ar", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9945064187049866}]}, {"text": " Table 2. Comparison with and without pre- processing and post-processing.", "labels": [], "entities": []}]}