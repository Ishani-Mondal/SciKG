{"title": [{"text": "Grammatical Error Correction as Multiclass Classification with Single Model *", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8069899082183838}, {"text": "Multiclass Classification", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6514691412448883}]}], "abstractContent": [{"text": "This paper describes our system in the shared task of CoNLL-2013.", "labels": [], "entities": [{"text": "CoNLL-2013", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.8502357006072998}]}, {"text": "We illustrate that grammatical error detection and correction can be transformed into a multiclass classification task and implemented as a single-model system regardless of various error types with the aid of maximum entropy modeling.", "labels": [], "entities": [{"text": "grammatical error detection and correction", "start_pos": 19, "end_pos": 61, "type": "TASK", "confidence": 0.6645064353942871}, {"text": "multiclass classification task", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.7970895171165466}]}, {"text": "Our system achieves the F1 score of 17.13% on the standard test set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9829496741294861}]}], "introductionContent": [], "datasetContent": [{"text": "The evaluation is done by calculating the M 2 precission, recall and F1 score between the system output and golden annotation.", "labels": [], "entities": [{"text": "M 2 precission", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.8933860858281454}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.989270806312561}, {"text": "F1 score", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9816768169403076}]}, {"text": "All the error types are evaluated jointly.", "labels": [], "entities": []}, {"text": "Only one run of a team is permitted to be submitted.", "labels": [], "entities": []}, {"text": "shows our result on our DEV data set and the official test data set.", "labels": [], "entities": [{"text": "DEV data set", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9896864692370096}, {"text": "official test data set", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.8182266801595688}]}], "tableCaptions": [{"text": " Table 6: All labels after relabeling", "labels": [], "entities": []}]}