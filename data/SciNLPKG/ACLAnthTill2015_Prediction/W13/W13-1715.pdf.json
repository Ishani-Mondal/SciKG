{"title": [{"text": "Recognizing English Learners' Native Language from Their Writings", "labels": [], "entities": [{"text": "Recognizing English Learners' Native Language from Their Writings", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.9184670299291611}]}], "abstractContent": [{"text": "Native Language Identification (NLI), which tries to identify the native language (L1) of a second language learner based on their writings , is helpful for advancing second language learning and authorship profiling in forensic linguistics.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7592701564232508}, {"text": "authorship profiling", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.7988884150981903}]}, {"text": "With the availability of relevant data resources, much work has been done to explore the native language of a foreign language learner.", "labels": [], "entities": []}, {"text": "In this report, we present our system for the first shared task in Native Language Identification (NLI).", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 67, "end_pos": 103, "type": "TASK", "confidence": 0.8045138070980707}]}, {"text": "We use a linear SVM classifier and explore features of words, word and character n-grams, style, and metadata.", "labels": [], "entities": []}, {"text": "Our official system achieves accuracy of 0.773, which ranks it 18 th among the 29 teams in the closed track.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9997798800468445}]}], "introductionContent": [{"text": "Native Language Identification (NLI), which tries to identify the native language (L1) of a second language learner based on their writings, is expected to be helpful for advancing second language learning and authorship profiling in forensic linguistics.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7562128951152166}, {"text": "authorship profiling", "start_pos": 210, "end_pos": 230, "type": "TASK", "confidence": 0.8049469888210297}]}, {"text": "With the availability of relevant data resources, much work has been done to explore the effective way to identify the native language of a foreign language learner ().", "labels": [], "entities": []}, {"text": "To evaluate different techniques and approaches to Native Language Identification with the same setting, the first shared task in Native Language Identification (NLI) was organized by researchers from Nuance Communications and Educational Testing Service (.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.6446443994839987}, {"text": "Native Language Identification (NLI)", "start_pos": 130, "end_pos": 166, "type": "TASK", "confidence": 0.7845642566680908}]}, {"text": "A larger and more reliable data set, TOEFL11 , was used in this open evaluation.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.6396810412406921}]}, {"text": "This paper reports our NLI2013 shared task system that we built at the Department of Computer Science, Henan University of Technology, China.", "labels": [], "entities": []}, {"text": "To be involved in this evaluation, we would like to obtain a more thorough knowledge of the research on native language identification and its state-ofthe-art, as we may focus on authorship attribution ( problems in the near future.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.6459338466326395}, {"text": "authorship attribution", "start_pos": 179, "end_pos": 201, "type": "TASK", "confidence": 0.7062574476003647}]}, {"text": "The NLI2013 shared task is framed as a supervised text classification problem where the set of native languages (L1s), i.e. categories, is known, which includes Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish.", "labels": [], "entities": [{"text": "text classification", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7273079752922058}]}, {"text": "A system is given a large part of the TOEFL11 dataset for training a detection model, and then makes predictions on the test writing samples.", "labels": [], "entities": [{"text": "TOEFL11 dataset", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9507604837417603}]}, {"text": "Inspired by our experience of dealing with different text classification problems, we decide to employ a linear support vector machine (SVM) in our NLI2013 system.", "labels": [], "entities": [{"text": "text classification", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7333971709012985}]}, {"text": "We plan to take this system as a starting point, and may explore other complex classifiers in the future.", "labels": [], "entities": []}, {"text": "Although in-depth syntac-tic features maybe helpful for this kind of tasks (, we decide to explore the effectiveness of the traditional word and character features, as well as style features, in our system.", "labels": [], "entities": []}, {"text": "We would like to verify on the first open available large dataset whether these traditional features work and how good they are..", "labels": [], "entities": []}, {"text": "We submitted four runs with different feature sets.", "labels": [], "entities": []}, {"text": "The run with all the features achieved the best accuracy of 0.773, which ranks our system 18th among the 29 systems in the closed track.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9996339082717896}]}, {"text": "In the rest of this paper we describe the detail of our system and analyze the results.", "labels": [], "entities": [{"text": "detail", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9574642777442932}]}, {"text": "Section 2 gives the overview of our system, while Section 3 discusses the various features in-depth.", "labels": [], "entities": []}, {"text": "We present our experiments and discussions in Section 4, and conclude in Section 5.", "labels": [], "entities": []}, {"text": "gives the architecture of our NLI2013 system, which takes machine learning framework.", "labels": [], "entities": []}, {"text": "At the training stage, annotated data is first processed through preprocessing and feature extraction, then fed to the classifier learning module, and we can finally obtain a NLI model.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.707134872674942}]}, {"text": "At the testing stage, each test sample goes through the same preprocessing and feature extraction modules, and is assigned a category with the learned NLI model.", "labels": [], "entities": []}, {"text": "Data Preprocessing: this module aims at transforming the original data into a suitable format for the system, e.g. inserting the category information into the individual writing sample and attaching metadata to essays.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset of the NLI2013 shared task contains 12,100 English essays from the Test of English as a Foreign Language (TOEFL).", "labels": [], "entities": []}, {"text": "Educational Testing Service (ETS) published the dataset through the LDC with the motivation to create a larger and more reliable data set for researchers to conduct Native Language Identification experiments on.", "labels": [], "entities": [{"text": "Educational Testing Service (ETS) published the dataset through the LDC", "start_pos": 0, "end_pos": 71, "type": "DATASET", "confidence": 0.8831380903720856}, {"text": "Native Language Identification", "start_pos": 165, "end_pos": 195, "type": "TASK", "confidence": 0.6457592745622}]}, {"text": "This dataset, henceforth TOEFL11, comprises 11 native languages (L1s) with 1,000 essays per language.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.8874613046646118}]}, {"text": "The 11 covered native languages are: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish.", "labels": [], "entities": []}, {"text": "In addition, each essay in the TOEFL11 is marked with an English language proficiency level (high, medium, or low) based on the judgments of human assessment specialists.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9081041216850281}]}, {"text": "The essays are usually 300 to 400 words long.", "labels": [], "entities": []}, {"text": "9,900 essays of this set are chosen as the training data, 1,100 are for development and the rest 1,100 as test data..", "labels": [], "entities": []}, {"text": "Official results of our system..", "labels": [], "entities": []}, {"text": "Performance of our official runs.", "labels": [], "entities": []}, {"text": "We did 10-fold cross validation on the training and development data with the same setting as the HAUTCS-1 run.", "labels": [], "entities": [{"text": "HAUTCS-1 run", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.8875766098499298}]}, {"text": "The data splitting is given by the organizers.", "labels": [], "entities": []}, {"text": "Accuracies of the 10 runs are show in table 2.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.990517795085907}]}, {"text": "The overall accuracy 0.799 is better than that on the test data..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997358918190002}]}, {"text": "Results of 10-fold cross validation on the training and development data.", "labels": [], "entities": []}, {"text": "To check how metadata features work, we did another run HAUTCS-5, which uses only words as features.", "labels": [], "entities": [{"text": "HAUTCS-5", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9031654596328735}]}, {"text": "This run got the same overall accuracy 0.756 on the old test data as HAUTCS-4 did, which demonstrates that those metadata features may not provide much useful information for native language identification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9989540576934814}, {"text": "HAUTCS-4", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.891204833984375}, {"text": "native language identification", "start_pos": 175, "end_pos": 205, "type": "TASK", "confidence": 0.6253248353799185}]}], "tableCaptions": [{"text": " Table 1. Official results of our system.", "labels": [], "entities": []}, {"text": " Table 2. Results of 10-fold cross validation on the train- ing and development data.", "labels": [], "entities": []}]}