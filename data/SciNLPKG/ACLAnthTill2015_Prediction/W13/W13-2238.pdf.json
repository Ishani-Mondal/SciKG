{"title": [], "abstractContent": [{"text": "We present an iterative technique to generate phrase tables for SMT, which is based on force-aligning the training data with a modified translation decoder.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9954820871353149}]}, {"text": "Different from previous work, we completely avoid the use of a word alignment or phrase extraction heuristics, moving towards a more principled phrase generation and probability estimation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.7143639177083969}, {"text": "phrase extraction heuristics", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.8001932601133982}, {"text": "phrase generation", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.727422371506691}, {"text": "probability estimation", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.656858041882515}]}, {"text": "During training , we allow the decoder to generate new phrases on-the-fly and increment the maximum phrase length in each iteration.", "labels": [], "entities": []}, {"text": "Experiments are carried out on the IWSLT 2011 Arabic-English task, where we are able to reach moderate improvements on a state-of-the-art baseline with our training method.", "labels": [], "entities": [{"text": "IWSLT 2011 Arabic-English task", "start_pos": 35, "end_pos": 65, "type": "DATASET", "confidence": 0.8645377308130264}]}, {"text": "The resulting phrase table shows only a small overlap with the heuristically extracted one, which demonstrates the re-strictiveness of limiting phrase selection by a word alignment or heuristics.", "labels": [], "entities": []}, {"text": "By interpolating the heuristic and the trained phrase table, we can improve over the baseline by 0.5% BLEU and 0.5% TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.999530553817749}, {"text": "TER", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9977039694786072}]}], "introductionContent": [{"text": "Most state-of-the-art SMT systems get the statistics from which the different component models are estimated via heuristics using a Viterbi word alignment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9900004267692566}]}, {"text": "The word alignment is usually generated with tools like GIZA++ (, that apply the EM algorithm to estimate the alignment with the HMM or IBM-4 translation models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.6924756169319153}, {"text": "EM", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.8602399230003357}]}, {"text": "This is also the case for the phrases or rules which serve as translation units for the decoder.", "labels": [], "entities": []}, {"text": "All phrases that do not violate the word alignment are extracted and their probabilities are estimated as relative frequencies (.", "labels": [], "entities": []}, {"text": "A number of different approaches have tried to do away with the heuristics and close this gap between the phrase table generation and translation decoding.", "labels": [], "entities": [{"text": "phrase table generation", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.6890188852945963}, {"text": "translation decoding", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.9296976923942566}]}, {"text": "However, most of these approaches either fail to achieve state-of-the-art performance or still make use of the word alignment or the extraction heuristics, e.g. as a prior in discriminative training or to initialize a generative or generatively inspired training procedure and are thus biased by their weaknesses.", "labels": [], "entities": [{"text": "generative or generatively inspired training", "start_pos": 218, "end_pos": 262, "type": "TASK", "confidence": 0.6844407796859742}]}, {"text": "Here, we aim at moving towards the ideal situation, where a unified framework induces the phrases based on the same models as in decoding.", "labels": [], "entities": []}, {"text": "We train the phrase table without using a word alignment or the extraction heuristics.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7119379788637161}]}, {"text": "Different from previous work, we are able to generate all possible phrase pairs on-the-fly during the training procedure.", "labels": [], "entities": []}, {"text": "A further advantage of our proposed algorithm is that we use basically the same beam search as in translation.", "labels": [], "entities": []}, {"text": "This makes it easy to re-implement by modifying any translation decoder, and makes sure that training and translation are consistent.", "labels": [], "entities": []}, {"text": "In principle, we apply the forced decoding approach described in) with cross-validation to prevent over-fitting, but we initialize the phrase table with IBM-1 lexical probabilities) instead of heuristically extracted relative frequencies.", "labels": [], "entities": []}, {"text": "The algorithm is extended with the concept of backoff phrases, so that new phrase pairs can be generated at training time.", "labels": [], "entities": []}, {"text": "The size of the newly generated phrases is incremented over the training iterations.", "labels": [], "entities": []}, {"text": "By introducing fallback decoding runs, we are able to successfully align the complete training data.", "labels": [], "entities": []}, {"text": "Local language models are used for better phrase pair pre-selection.", "labels": [], "entities": []}, {"text": "The experiments are carried out on the IWSLT 2011 Arabic-English shared task.", "labels": [], "entities": [{"text": "IWSLT 2011 Arabic-English shared task", "start_pos": 39, "end_pos": 76, "type": "DATASET", "confidence": 0.9108187317848205}]}, {"text": "We are able to show that it is possible and feasible to reach stateof-the-art performance without the need to wordalign the bilingual training data.", "labels": [], "entities": []}, {"text": "The small overlap of 18.5% between the trained and the heuristically extracted phrase table demonstrates the limitations of previous work, where training is initialized by the baseline phrase table or phrase selection is restricted by a word alignment.", "labels": [], "entities": []}, {"text": "With a linear interpolation of phrase tables an improvement of 0.5% BLEU and 0.5% TER over the baseline can be achieved.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9995837807655334}, {"text": "TER", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9991447925567627}]}, {"text": "The result in BLEU is statistically significant on the test set with 90% confidence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9985969662666321}]}, {"text": "Further, we can confirm the observation of previous work, that phrases with near-zero entropies seem to be a disadvantage for translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.8920302391052246}]}, {"text": "Although we use a phrase-based decoder here, the principles of our work can be applied to any statistical machine translation paradigm.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.6179219186306}]}, {"text": "The software used for our experiments is available under a non-commercial open source licence.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We review related work in Section 2.", "labels": [], "entities": []}, {"text": "The decoder and its features are described in Section 3 and we give an overview of the training procedure in Section 4.", "labels": [], "entities": []}, {"text": "The complete algorithm is described in Section 5 and experiments are presented in Section 6.", "labels": [], "entities": []}, {"text": "We conclude with Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics for the IWSLT 2011 Arabic- English data. The out-of-vocabulary words are de- noted as OOVs.", "labels": [], "entities": [{"text": "IWSLT 2011 Arabic- English data", "start_pos": 29, "end_pos": 60, "type": "DATASET", "confidence": 0.9184579650561014}]}, {"text": " Table 2: BLEU and TER scores of the baseline,  phrase training with leave-one-out and length- incremental training after 12 iterations, as well as  a linear interpolation of the baseline with length- incremental phrase table. Results marked with  \u2020  are statistically significant with 90% confidence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994427561759949}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.998397171497345}, {"text": "length", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9674028158187866}]}]}