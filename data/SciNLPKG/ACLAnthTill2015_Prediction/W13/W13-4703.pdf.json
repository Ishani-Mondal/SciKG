{"title": [{"text": "A Three-Layer Architecture for Automatic Post-Editing System Using Rule-Based Paradigm", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a post-editing model in which our three-level rule-based automatic post-editing engine called Grafix is presented to refine the output of machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.6971547603607178}]}, {"text": "The type of corrections on sentences varies from lexical transformation to complex syntactical rearrangement.", "labels": [], "entities": []}, {"text": "The experimental results both in manual and automatic evaluations show that the proposed system is able to improve the quality of our state-of-the-art English-Persian SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 167, "end_pos": 170, "type": "TASK", "confidence": 0.8583870530128479}]}], "introductionContent": [{"text": "The overall success of well-designed statistical machine translation (SMT) systems has made SMT one of the most popular machine translation (MT) approaches.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.7851458191871643}, {"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9878137111663818}, {"text": "machine translation (MT)", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.8574141502380371}]}, {"text": "Currently however, MT output is often seriously grammatically incorrect.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9887867569923401}]}, {"text": "This is more often the casein SMT than other approaches due to the absence of linguistic rules for the language pair on which it is being applied.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8011592626571655}]}, {"text": "Grammatical error not only weakens the fluency of the translation, but in certain cases it completely changes the meaning of a sentence.", "labels": [], "entities": []}, {"text": "In morphologically rich languages, grammatical accuracy is of more significance, as the interpretation of syntactic relations depends heavily on morphological agreements within the sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9554299116134644}]}, {"text": "Since our system's approach is SMT, and deals with Persian, a morphologically rich language, post-editing the output is an important step in maintaining the fluency of the translation, and as we will show, this can yield to higher evaluation scores and more fluent translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9828051924705505}]}, {"text": "Due to the repetitive nature of machine translation mistakes and the similarity of automatic post-editing (APE) process to machine translation process  certain MT systems can carryout the task of an APE component.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.6799046993255615}, {"text": "MT", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.9415102601051331}]}, {"text": "The SMT approach to MT is useful in that it operates on numerical data extracted from parallel corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9923898577690125}, {"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9914763569831848}]}, {"text": "This approach tends to reduce human cost at translation stage.", "labels": [], "entities": [{"text": "translation", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9723232388496399}]}, {"text": "However, the search cost is expensive, and the system has no linguistic background (although generally it is this that enables the system to be applied to any language pair after training on language-specific data).", "labels": [], "entities": []}, {"text": "Most systems of this approach also encounter difficulties when capturing long distance phenomena.", "labels": [], "entities": []}, {"text": "RBMT approaches are categorized under three different types: Direct Systems, Transfer RBMT Systems that employ morphological and syntactical analysis, and Interligual RBMT Systems that use an abstract meaning.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9259225130081177}]}, {"text": "The Grafix APE system follows the pattern of Transfer-based systems.", "labels": [], "entities": []}, {"text": "The aim of development of Grafix system is to correct some grammatical SMT system errors frequently occur in English-to-Persian translations.", "labels": [], "entities": [{"text": "SMT system", "start_pos": 71, "end_pos": 81, "type": "TASK", "confidence": 0.8733010292053223}]}], "datasetContent": [{"text": "We used both BLEU and NIST to evaluate the effect of APE system on translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9845402836799622}, {"text": "NIST", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.9488092064857483}, {"text": "APE", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.5149062275886536}, {"text": "translation", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9548171162605286}]}, {"text": "The results of translation before and after APE are shown in.", "labels": [], "entities": [{"text": "APE", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.4409318268299103}]}, {"text": "As also demonstrate, the results generally show increases in both metrics.", "labels": [], "entities": []}, {"text": "The greatest increase in BLEU score due to the APE was achieved in test set #3, with an increase of about 0.15 BLEU, while the greatest NIST score increase was in test set #1, with a 0.16 increase.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9777469038963318}, {"text": "APE", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8823031783103943}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9982364177703857}, {"text": "NIST score increase", "start_pos": 136, "end_pos": 155, "type": "METRIC", "confidence": 0.6889005303382874}]}, {"text": "However, in certain test sets the scoring metrics report a decrease in output quality, the worst BLEU score being at a difference of -0.0151, and the worst NIST at -0.27.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9795272052288055}, {"text": "NIST", "start_pos": 156, "end_pos": 160, "type": "DATASET", "confidence": 0.6993908286094666}]}, {"text": "The main reason behind weakened results is lack of training data for the Transliterator module, as it scripted some proper names and terms incorrectly in Persian.", "labels": [], "entities": []}, {"text": "The large difference between the BLEU scores of data sets is due to each data set genre and the type of training data set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9914649128913879}]}, {"text": "The quality of statistical translation (in terms of BLEU metric score) affects the APE module directly.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7680723071098328}, {"text": "BLEU metric score", "start_pos": 52, "end_pos": 69, "type": "METRIC", "confidence": 0.9567407568295797}]}, {"text": "The test set is in the news story domain, the same domain as the parallel corpus used.", "labels": [], "entities": []}, {"text": "In test set #4, in the religious genre, the decrease in both BLEU and NIST is attributed to alack of data in this genre.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9986880421638489}, {"text": "NIST", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.7966040372848511}]}, {"text": "As suggested by), grammatical correctness of sentences cannot be measured appropriately with BLEU metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9967254400253296}]}, {"text": "Because of this, we evaluated the output manually.", "labels": [], "entities": []}, {"text": "The same test sets in automatic evaluation containing 153 sentences are evaluated here.", "labels": [], "entities": []}, {"text": "We assigned the APE output of test sentences to two separate annotators, instructing them to rank the APE output sentences based on whether the output showed improvement, decrease, or no change in fluency compared to the original SMT output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 230, "end_pos": 233, "type": "TASK", "confidence": 0.9614291787147522}]}, {"text": "Both annotators, who completed the evaluation without discussing or consulting with each other, had very similar judgment of the APE system's output.", "labels": [], "entities": [{"text": "APE system's output", "start_pos": 129, "end_pos": 148, "type": "DATASET", "confidence": 0.7868760079145432}]}, {"text": "The results show that the APE system has been successful in improving the quality of the baseline SMT system output by 29.4%.", "labels": [], "entities": [{"text": "APE", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.5200998783111572}, {"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9888513684272766}]}, {"text": "The current developed rules for the APE system are effective for about 37% of the SMT translated sentences.", "labels": [], "entities": [{"text": "APE", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8349385261535645}, {"text": "SMT translated sentences", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.9291920065879822}]}, {"text": "We also identified both annotator agreements on ranked sentences.", "labels": [], "entities": []}, {"text": "Based on their agreement, the quality improvement from the APE system is 25% and weakened by 3%.", "labels": [], "entities": [{"text": "quality improvement", "start_pos": 30, "end_pos": 49, "type": "METRIC", "confidence": 0.9474671483039856}, {"text": "APE system", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.6561508029699326}]}, {"text": "The comparison of the average percent of manual scores is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Manual evaluation scores for 153 test sentences", "labels": [], "entities": []}, {"text": " Table 1. Scores of APE based on SMT Joshua version 4.0", "labels": [], "entities": [{"text": "APE", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.5288383364677429}, {"text": "SMT Joshua version 4.0", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.853737473487854}]}]}