{"title": [], "abstractContent": [{"text": "This paper reports on our work in the NLI shared task 2013 on Native Language Identification.", "labels": [], "entities": [{"text": "NLI shared task 2013 on Native Language Identification", "start_pos": 38, "end_pos": 92, "type": "TASK", "confidence": 0.7517899200320244}]}, {"text": "The task is to automatically detect the native language of the TOEFL essays authors in a set of given test documents in Eng-lish.", "labels": [], "entities": [{"text": "TOEFL essays authors in a set of given test documents in Eng-lish", "start_pos": 63, "end_pos": 128, "type": "DATASET", "confidence": 0.6752850512663523}]}, {"text": "The task was solved by a system that used the PPM compression algorithm based on an n-gram statistical model.", "labels": [], "entities": [{"text": "PPM compression", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.749315470457077}]}, {"text": "We submitted four runs; word-based PPMC algorithm with normalization and without, character-based PPMC algorithm with normalization and without.", "labels": [], "entities": []}, {"text": "The worst result was obtained on training and testing data during the evaluation procedure using the character-based PPM method and normalization: accuracy = 31.9%; the best one was macroaverage F-measure = 0.708 with the word-based PPMC algorithm without normalization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9995311498641968}, {"text": "macroaverage F-measure", "start_pos": 182, "end_pos": 204, "type": "METRIC", "confidence": 0.6965617835521698}]}], "introductionContent": [{"text": "With the emergence of user-generated web content, text author profiling is being increasingly studied by the NLP community.", "labels": [], "entities": [{"text": "text author profiling", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.6715418696403503}]}, {"text": "Various works describe experiments aiming to automatically discover hidden attributes of text which reveal author's gender, age, personality and others.", "labels": [], "entities": []}, {"text": "While English remains one of the main global languages used for communication, interchange of information and ideas, English texts written by different language speakers differ considerably.", "labels": [], "entities": [{"text": "interchange of information and ideas", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.8531170606613159}]}, {"text": "This is yet another characteristic of the author that can be learned from a text.", "labels": [], "entities": []}, {"text": "While a great number of works have presented investigations in this area there was no common ground to evaluate different techniques and approaches to Native Language Identification.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 151, "end_pos": 181, "type": "TASK", "confidence": 0.6407804687817892}]}, {"text": "NLI shared task 2013 on Native Language Identification provides a playground and a corpus for such an evaluation.", "labels": [], "entities": [{"text": "NLI shared task 2013", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.920378252863884}, {"text": "Native Language Identification", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.8272417783737183}]}, {"text": "We participated in this shared task with the PPM compression algorithm based on a character-based and word-based n-gram statistical model.", "labels": [], "entities": [{"text": "PPM compression", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.903880774974823}]}], "datasetContent": [{"text": "Three sets of experiments were carried out during the NLI shared task event.", "labels": [], "entities": [{"text": "NLI shared task event", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.5541860833764076}]}, {"text": "The first one was performed on the training and development data released in January.", "labels": [], "entities": []}, {"text": "The second set consisted of evaluation runs on test data released in March and the results for these experiments were provided by the organizers.", "labels": [], "entities": []}, {"text": "The third set was 10-fold crossvalidation on training + development data requested by the organizers.", "labels": [], "entities": []}, {"text": "The first set of experiments was carried out on the first set of data released by the organizers: TOEFL essays written by 11 native languages speakers.", "labels": [], "entities": [{"text": "TOEFL essays written by 11 native languages speakers", "start_pos": 98, "end_pos": 150, "type": "TASK", "confidence": 0.5407952219247818}]}, {"text": "9,900 essays of this set were sequestered as the training data and 1,100 were for the development set.", "labels": [], "entities": []}, {"text": "Thus, we trained our model on 900 files for each native language speakers, for each class.", "labels": [], "entities": []}, {"text": "Next, we attributed classes to 1,100 development texts.", "labels": [], "entities": []}, {"text": "We carried out four experiments.", "labels": [], "entities": []}, {"text": "The first two were done on the basis of the character-based PPMC5 method with and without the normalization procedure described earlier.", "labels": [], "entities": []}, {"text": "The second two experiments were done with the word-based PPMC1 method with and without the normalization.", "labels": [], "entities": []}, {"text": "The Precision, Recall and F-measure for these four experiments are presented in are confusion tables for the worst and for the best cases of the four experiments..", "labels": [], "entities": [{"text": "Precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995391368865967}, {"text": "Recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9937746524810791}, {"text": "F-measure", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9974884986877441}]}, {"text": "Confusion table for 1,100 development files for the first PPMC1 word-based experiment without normalization.", "labels": [], "entities": []}, {"text": "The second set of experiments was done on the 1,100 test files during the evaluation phase of the challenge.", "labels": [], "entities": []}, {"text": "The results of these experiments were provided by the organizers.", "labels": [], "entities": []}, {"text": "Again, we carried out four experiments: character-based PPMC5 method with and without normalization and word-based PPMC1 method with and without normalization.", "labels": [], "entities": []}, {"text": "Confusion tables 4 and 5 presents the worst and the best results.", "labels": [], "entities": []}, {"text": "The overall accuracies for these experiments are: Character-based PPMC5 method without normalization -37.4%; Character-based PPMC5 method with normalization -31.9%; Word-based PPMC1 method without normalization -62.5%; Word-based PPMC1 method with normalization -62.2%.", "labels": [], "entities": []}, {"text": "The third set of the experiments was done at the organizers' request on the basis of training + development data.", "labels": [], "entities": []}, {"text": "10-fold cross-validation was made on this data with exactly the same splitting used in.", "labels": [], "entities": []}, {"text": "The results of these experiments are presented in. are confusion tables for the worst and the best cases among all 10 folds and four experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Results obtained on character-based and letter-based PPM models with and without normalization.", "labels": [], "entities": []}, {"text": " Table 2. Confusion table for 1,100 development files for the first PPMC5 character-based experiment with normali- zation.", "labels": [], "entities": []}, {"text": " Table 3. Confusion table for 1,100 development files for the first PPMC1 word-based experiment without normali- zation.", "labels": [], "entities": []}, {"text": " Table 4. Confusion table for 1,100 test files for the PPMC5 character-based experiment with normalization.  The overall accuracy is 31.9%.", "labels": [], "entities": [{"text": "PPMC5 character-based experiment", "start_pos": 55, "end_pos": 87, "type": "DATASET", "confidence": 0.8005110422770182}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9993854761123657}]}, {"text": " Table 5. Confusion table for 1,100 test files for the PPMC1 word-based experiment without normalization.  The overall accuracy is 62.5%.", "labels": [], "entities": [{"text": "Confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9710649847984314}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9994485974311829}]}, {"text": " Table 6. Results obtained on character-based and letter-based PPM models with and without normalization on the  basis of training + development data.", "labels": [], "entities": []}, {"text": " Table 7. Confusion table for the worst case in the third set of experiments; 10-fold cross-validation, fold 9, PPMC5  character-based, with normalization.", "labels": [], "entities": [{"text": "Confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9480127096176147}]}, {"text": " Table 8. Confusion table for the best case in the third set of experiments; 10-fold cross-validation, fold 3, PPMC1", "labels": [], "entities": [{"text": "Confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9729002714157104}, {"text": "PPMC1", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.8051343560218811}]}]}