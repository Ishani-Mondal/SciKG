{"title": [{"text": "Improvements to Syntax-based Machine Translation using Ensemble Dependency Parsers", "labels": [], "entities": [{"text": "Syntax-based Machine Translation", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.7772616545359293}]}], "abstractContent": [{"text": "Dependency parsers are almost ubiquitously evaluated on their accuracy scores, these scores say nothing of the complexity and usefulness of the resulting structures.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7866953611373901}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.998777449131012}]}, {"text": "The structures may have more complexity due to their coordination structure or attachment rules.", "labels": [], "entities": []}, {"text": "As dependency parses are basic structures in which other systems are built upon, it would seem more reasonable to judge these parsers down the NLP pipeline.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.740350604057312}]}, {"text": "We show results from 7 individual parsers, including dependency and constituent parsers, and 3 ensemble parsing techniques with their overall effect on a Machine Translation system, Treex, for En-glish to Czech translation.", "labels": [], "entities": []}, {"text": "We show that parsers' UAS scores are more correlated to the NIST evaluation metric than to the BLEU Metric, however we see increases in both metrics.", "labels": [], "entities": [{"text": "UAS", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.7208589315414429}, {"text": "NIST", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8231997489929199}, {"text": "BLEU Metric", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.9449154436588287}]}], "introductionContent": [{"text": "Ensemble learning) has been used fora variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success.", "labels": [], "entities": [{"text": "Ensemble learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7652151882648468}, {"text": "dependency parsing", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8745840489864349}]}, {"text": "() showed a successful combination of parse trees through a linear combination of trees with various weighting formulations.", "labels": [], "entities": []}, {"text": "To keep their tree constraint, they applied Eisner's algorithm for reparsing.", "labels": [], "entities": []}, {"text": "Parser combination with dependency trees has been examined in terms of accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9993706345558167}]}, {"text": "Other methods of parser combinations have shown to be successful such as using one parser to generate features for another parser.", "labels": [], "entities": []}, {"text": "This was shown in, in which Malt Parser was used as a feature to MST Parser.", "labels": [], "entities": [{"text": "MST Parser", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.89076167345047}]}, {"text": "The result was a successful combination of a transition-based and graph-based parser, but did not address adding other types of parsers into the framework.", "labels": [], "entities": []}, {"text": "We will use three ensemble approaches.", "labels": [], "entities": []}, {"text": "First a fixed weight ensemble approach in which edges are added together in a weighted graph.", "labels": [], "entities": []}, {"text": "Second, we added the edges using weights learned through fuzzy clustering based on POS errors.", "labels": [], "entities": []}, {"text": "Third, we will use a meta-classifier that uses an SVM to predict the correct model for edge using only model agreements without any linguistic information added.", "labels": [], "entities": []}, {"text": "Parsing accuracy and machine translation has been examined in terms of BLEU score).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9014372229576111}, {"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7268020808696747}, {"text": "BLEU score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9821189641952515}]}, {"text": "However, we believe our work is the first to examine the NLP pipeline for ensemble parsing for both dependency and constituent parsers as well as examining both BLEU and NIST scores' relationship to their Unlabeled Accuracy Score(UAS).", "labels": [], "entities": [{"text": "ensemble parsing", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.6102979779243469}, {"text": "BLEU", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9830012321472168}, {"text": "Unlabeled Accuracy Score(UAS)", "start_pos": 205, "end_pos": 234, "type": "METRIC", "confidence": 0.8065330336491267}]}], "datasetContent": [{"text": "For Machine Translation we report two automatic evaluation scores, BLEU and NIST.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8684544563293457}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9990608096122742}, {"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9089238047599792}]}, {"text": "We examine parser accuracy using UAS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9223173260688782}, {"text": "UAS", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.46872514486312866}]}, {"text": "This paper compares a machine translation system integrating 10 different parsing systems against each other, using the below metrics.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7541152536869049}]}, {"text": "The BLEU (BiLingual Evaluation Understudy) and NIST(from the National Institute of Standards and Technology), are automatic scoring mechanisms for machine translation that are quick and can be reused as benchmarks across machine translation tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9945788383483887}, {"text": "NIST", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.8521372079849243}, {"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.8133518099784851}, {"text": "machine translation tasks", "start_pos": 221, "end_pos": 246, "type": "TASK", "confidence": 0.7885317007700602}]}, {"text": "BLEU and NIST are calculated as the geometric mean of n-grams multiplied by a brevity penalty, comparing a machine translation and a reference text ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9796604514122009}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.6993568539619446}]}, {"text": "NIST is based upon the BLEU n-gram approach however it is also weighted towards discovering more \"informative\" n-grams.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9442170858383179}, {"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9849228858947754}]}, {"text": "The more rare an n-gram is, the higher the weight fora correct translation of it will be.", "labels": [], "entities": []}, {"text": "Made a standard in the CoNLL shared tasks competition, UAS studies the structure of a dependency tree and assesses how often the output has the correct head and dependency arcs).", "labels": [], "entities": []}, {"text": "We report UAS scores for each parser on section 23 of the WSJ.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.765049159526825}, {"text": "section 23 of the WSJ", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.699612808227539}]}, {"text": "We selected 200 sentences at random from our annotations and they were given to 7 native Czech speakers.", "labels": [], "entities": []}, {"text": "77 times the reviewers preferred the SVM system, 48 times they preferred the MST system, and 57 times they said there was no difference between the sentences.", "labels": [], "entities": []}, {"text": "On average each reviewer looked at 26 sentences with a median of 30 sentences.", "labels": [], "entities": []}, {"text": "Reviewers were allowed three options: sentence 1 is better, sentence 2 is better, both sentences are of equal quality.", "labels": [], "entities": []}, {"text": "Sentences were displayed in a random order and the systems were randomly shuffled for each question and for each user.", "labels": [], "entities": []}, {"text": "+ = -+ 12 12 0 = 3 7 -7: Agreement for sentences with 2 or more annotators for our baseline and SVM systems.", "labels": [], "entities": []}, {"text": "(-,-) all annotators agreed the baseline was better, (+,+) all annotators agreed the SVM system was better, (+,-) the annotators disagreed with each other indicates that the SVM system was preferred.", "labels": [], "entities": []}, {"text": "When removing annotations marked as equal, we see that the SVM system was preferred 24 times to the Baseline's 14.", "labels": [], "entities": []}, {"text": "Although a small sample, this shows that using the ensemble parser will at worse give you equal results and at best a much improved result.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pearson correlation coefficients for each  year and each metric when measured against UAS.  Statistics are taken from the WMT results in", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8929900825023651}, {"text": "UAS", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9305559396743774}, {"text": "WMT", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.4891357123851776}]}, {"text": " Table  1. Overall NIST has the stronger correlation to  UAS scores, however both NIST and BLEU show  a strong relationship.", "labels": [], "entities": [{"text": "NIST", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.9022830724716187}, {"text": "UAS", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.7364791631698608}, {"text": "NIST", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.9247972369194031}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9966831803321838}]}]}