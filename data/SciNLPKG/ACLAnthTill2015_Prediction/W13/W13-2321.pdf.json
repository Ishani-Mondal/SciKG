{"title": [{"text": "Automatic Named Entity Pre-Annotation for Out-of-Domain Human Annotation", "labels": [], "entities": [{"text": "Out-of-Domain Human Annotation", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.5193011164665222}]}], "abstractContent": [{"text": "Automatic pre-annotation is often used to improve human annotation speed and accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.8016512989997864}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9962312579154968}]}, {"text": "We address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneficial in this setting.", "labels": [], "entities": []}, {"text": "Our study design includes two different corpora, three pre-annotation schemes linked to two annotation levels, both expert and novice an-notators, a questionnaire-based subjective assessment and a corpus-based quantitative assessment.", "labels": [], "entities": []}, {"text": "We observe that pre-annotation helps in all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome.", "labels": [], "entities": [{"text": "speed", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9944846034049988}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9989894032478333}]}], "introductionContent": [{"text": "Human corpus annotation is a difficult, timeconsuming, and hence costly process.", "labels": [], "entities": [{"text": "Human corpus annotation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5827691853046417}]}, {"text": "This motivates research into methods which reduce this cost (.", "labels": [], "entities": []}, {"text": "One such method consists of automatically pre-annotating the corpus () using an existing system, e.g., a POS tagger, syntactic parser, named entity recognizer, according to the task for which the annotations aim to provide a gold standard.", "labels": [], "entities": []}, {"text": "The pre-annotations are then corrected by the human annotators.", "labels": [], "entities": []}, {"text": "The underlying hypothesis is that this should reduce annotation time while possibly at the same time increasing annotation completeness and consistency.", "labels": [], "entities": [{"text": "annotation time", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.9034176468849182}, {"text": "consistency", "start_pos": 140, "end_pos": 151, "type": "METRIC", "confidence": 0.9890698194503784}]}, {"text": "We study here corpus pre-annotation in a specific setting, out-of-domain named entity annotation, in which we examine specific questions that we present below.", "labels": [], "entities": []}, {"text": "We produced corpora and annotation guidelines for named entities which are both hierarchical and compositional ), and which we used in contrastive studies of news texts in French ( . We want to rely on the same named entity definitions for studies on two types of data we did not cover: parliament debates (Europarl corpus) and regional, contemporary written news (L'Est R\u00e9publicain), both in French.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 307, "end_pos": 322, "type": "DATASET", "confidence": 0.9180780947208405}]}, {"text": "To help the annotation process we could reuse our system (), but needed first to examine whether a system trained on one type of text (our first Broadcast News data) could be used to produce a useful pre-annotation for different types of text (our two corpora).", "labels": [], "entities": [{"text": "Broadcast News data", "start_pos": 145, "end_pos": 164, "type": "DATASET", "confidence": 0.9358514149983724}]}, {"text": "We therefore setup the present study in which we aim to answer the following questions linked to this point and to related annotation issues: \u2022 can a system trained on data from one specific domain be useful on data from another domain in a pre-annotation task?", "labels": [], "entities": []}, {"text": "\u2022 does this pre-annotation help human annotators or bias them?", "labels": [], "entities": []}, {"text": "\u2022 what importance can we give to the annotators' subjective assessment of the usefulness of the pre-annotation?", "labels": [], "entities": []}, {"text": "\u2022 can we observe differences in the use of preannotation depending on the level of expertise of human annotators?", "labels": [], "entities": []}, {"text": "Moreover, as the aforementioned annotation scheme is based on two annotation levels (entities and components), we want to answer these questions taking into account these two levels.", "labels": [], "entities": []}, {"text": "We first examine related work on pre-annotation (Section 2), then present our corpora and annotation task (Section 3).", "labels": [], "entities": []}, {"text": "We describe and discuss experiments in Section 4, and make subjective and quantitative observations in Sections 5 and 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude and present some perspectives in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the protocol we designed to study the usefulness of pre-annotation under different conditions, and its overall results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: General description of the reference an- notations: number of components, types, entities  (the sum of components and types), and words", "labels": [], "entities": []}, {"text": " Table 2: F-measure and Slot Error Rate achieved  by the automatic system on each kind of annota- tion and on in-domain broadcast data", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9977814555168152}, {"text": "Slot Error Rate", "start_pos": 24, "end_pos": 39, "type": "METRIC", "confidence": 0.9008723696072897}]}, {"text": " Table 4: Sentence summary of the three corpora", "labels": [], "entities": [{"text": "Sentence summary", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.924338310956955}]}, {"text": " Table 5: Pre-annotation errors and comparison  with in-domain (Broadcast News) data", "labels": [], "entities": [{"text": "Broadcast News) data", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.9494708478450775}]}, {"text": " Table 7: Mean F-measure of experts and novices,  for each pre-annotation scheme", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9764485359191895}, {"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.6496121287345886}]}, {"text": " Table 8: Mean duration (in minutes) of annotation  for experts and novices, for each pre-annotation  scheme (two corpus quarters)", "labels": [], "entities": [{"text": "Mean duration", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9154374599456787}]}]}