{"title": [{"text": "Reinforcement Learning of Two-Issue Negotiation Dialogue Policies", "labels": [], "entities": [{"text": "Reinforcement Learning of Two-Issue Negotiation Dialogue Policies", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.8695545707430158}]}], "abstractContent": [{"text": "We use hand-crafted simulated negotiators (SNs) to train and evaluate dialogue policies for two-issue negotiation between two agents.", "labels": [], "entities": []}, {"text": "These SNs differ in their goals and in the use of strong and weak arguments to persuade their counterparts.", "labels": [], "entities": [{"text": "SNs", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9621575474739075}]}, {"text": "They may also make irrational moves, i.e., moves not consistent with their goals, to generate a variety of negotiation patterns.", "labels": [], "entities": []}, {"text": "Different versions of these SNs interact with each other to generate corpora for Reinforcement Learning (RL) of argumentation dialogue policies for each of the two agents.", "labels": [], "entities": []}, {"text": "We evaluate the learned policies against hand-crafted SNs similar to the ones used for training but with the modification that these SNs no longer make irrational moves and thus are harder to beat.", "labels": [], "entities": []}, {"text": "The learned policies generally do as well as, or better than the hand-crafted SNs showing that RL can be successfully used for learning argumentation dialogue policies in two-issue negotiation scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "The dialogue policy of a dialogue system decides on what dialogue move (also called action) the system should make given the dialogue context (also called dialogue state).", "labels": [], "entities": []}, {"text": "Building hand-crafted policies is a hard task, and there is no guarantee that the resulting policies will be optimal.", "labels": [], "entities": []}, {"text": "This issue has motivated the dialogue community to use statistical methods for automatically learning dialogue policies, the most popular of which is Reinforcement Learning (RL).", "labels": [], "entities": []}, {"text": "To date, RL has been used mainly for learning dialogue policies for slot-filling applications such as restaurant recommendations), flight reservations (, sightseeing recommendations (, appointment scheduling, technical support, etc., largely ignoring other types of dialogue.", "labels": [], "entities": [{"text": "RL", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.760955810546875}, {"text": "flight reservations", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.789056122303009}]}, {"text": "RL has also been applied to question-answering ( ) and tutoring domains).", "labels": [], "entities": [{"text": "RL", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8048845529556274}]}, {"text": "There has also been some work on applying RL to the more difficult problem of learning negotiation policies, which is the topic of this paper.", "labels": [], "entities": [{"text": "RL", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9564732313156128}, {"text": "learning negotiation policies", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.6833044687906901}]}, {"text": "In negotiation dialogue the system and the user have opinions about the optimal outcomes and try to reach a joint decision.", "labels": [], "entities": []}, {"text": "Dialogue policy decisions are typically whether to present, accept, or reject a proposal, whether to compromise, etc.", "labels": [], "entities": []}, {"text": "Rewards may depend on the type of policy that we want to learn.", "labels": [], "entities": []}, {"text": "For example, a cooperative policy should be rewarded for accepting proposals.", "labels": [], "entities": []}, {"text": "Recently, Georgila and Traum (2011a; 2011b) learned argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario.", "labels": [], "entities": []}, {"text": "We extend this work by learning argumentation policies in a twoissue negotiation setting.", "labels": [], "entities": []}, {"text": "We aim to learn system (or agent) policies that will persuade their interlocutor (a human user or another agent) to agree on the system's preferences.", "labels": [], "entities": []}, {"text": "Our research contribution is two-fold: First, to our knowledge this is the first study that uses RL for learning argumentation policies in a two-issue negotiation scenario and one of the few studies on using RL for negotiation.", "labels": [], "entities": []}, {"text": "Second, for the first time, we learn policies for agents with different degrees of persuasion skills, i.e., agents that provide strong or weak arguments.", "labels": [], "entities": []}, {"text": "Section 2 introduces RL, and section 3 describes our two-issue negotiation domain and our learning methodology.", "labels": [], "entities": [{"text": "RL", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.8393440246582031}]}, {"text": "Section 4 presents our evaluation results and section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each policy of Agent 1 resulting from atrial is evaluated against two hand-crafted SNs for Agent 2, one where Agent 2 provides strong arguments (Agent 2 S) and one where Agent 2 provides weak arguments (Agent 2 W).", "labels": [], "entities": []}, {"text": "So for the condition \"Agent 1 with strong arguments trained against Agent 2 with strong arguments (Agent 1 S(S))\" we have 5 policies, each of which interacts with \"Agent 2 S\" (or \"Agent 2 W\").", "labels": [], "entities": []}, {"text": "We calculate the averages of the earned points for each of the agents, of the number of actions per dialogue of each agent, and of the number of turns per dialogue of each agent, over 10,000 dialogues per policy.", "labels": [], "entities": []}, {"text": "Likewise for the policies of Agent 2.", "labels": [], "entities": []}, {"text": "Note that the SNs used in the evaluation do not behave irrationally like the ones used for training, and thus are harder to beat.", "labels": [], "entities": []}, {"text": "In we can seethe results for the policy of Agent 1.", "labels": [], "entities": []}, {"text": "Results for the policy of Agent 2 are similar given that the goals of Agent 2 mirror the goals of Agent 1.", "labels": [], "entities": []}, {"text": "As we can see, the policy of Agent 1 with strong arguments learned to provide the appropriate arguments and make Agent 2 agree on \"Thai\" and \"Friday\" or \"Saturday\".", "labels": [], "entities": [{"text": "Thai", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.8238129615783691}]}, {"text": "When the policy of Agent 1 provides only weak arguments it cannot get day \"Friday\" but it can secure a tradeoff.", "labels": [], "entities": []}, {"text": "This is because both the learned policies and the SNs usually accept trade-off offers (due to the way the hand-crafted SNs were constructed).", "labels": [], "entities": []}, {"text": "We also performed tests with SNs that did not propose or accept as many trade-offs.", "labels": [], "entities": []}, {"text": "This arrangement favored the policy of Agent 1 with strong arguments, and hurt the performance of the policy of Agent 1 with weak arguments playing against Agent 2 with strong arguments.", "labels": [], "entities": []}, {"text": "This shows that trade-offs help the weaker negotiators.", "labels": [], "entities": []}, {"text": "Furthermore, we experimented with testing on semi-rational SNs similar to the ones used for training and the results were better for the policy of Agent 1 with weak arguments and worse for the policy of Agent 1 with strong arguments.", "labels": [], "entities": []}, {"text": "So like trade-offs a semi-rational SN favors the weaker negotiators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Rewards for Agents 1 and 2.", "labels": [], "entities": []}, {"text": " Table 3: Results of different training and testing combinations for learned policies of Agent 1 and rational  SNs for Agent 2.", "labels": [], "entities": []}]}