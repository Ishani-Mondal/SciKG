{"title": [{"text": "Towards Weakly Supervised Resolution of Null Instantiations", "labels": [], "entities": [{"text": "Weakly Supervised Resolution of Null Instantiations", "start_pos": 8, "end_pos": 59, "type": "TASK", "confidence": 0.8024692684412003}]}], "abstractContent": [{"text": "This paper addresses the task of finding antecedents for locally uninstantiated arguments.", "labels": [], "entities": []}, {"text": "To resolve such null instantiations, we develop a weakly supervised approach that investigates and combines a number of linguistically motivated strategies that are inspired by work on semantic role labeling and corefence resolution.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.6271158158779144}, {"text": "corefence resolution", "start_pos": 212, "end_pos": 232, "type": "TASK", "confidence": 0.8516692519187927}]}, {"text": "The performance of the system is competitive with the current state-of-the-art supervised system.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is a growing interest in developing algorithms for resolving locally unrealized semantic arguments, so-called null instantiations (NIs).", "labels": [], "entities": []}, {"text": "Null instantiations are frequent in natural discourse; only a relatively small proportion of the theoretically possible semantic arguments tend to be locally instantiated in the same clause or sentence as the target predicate.", "labels": [], "entities": [{"text": "Null instantiations", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8363986313343048}]}, {"text": "This even applies to core arguments of a predicate i.e., those that express participants which are necessarily present in the situation which the predicate evokes.", "labels": [], "entities": []}, {"text": "However, null instantiated arguments can often be 'recovered' from the surrounding context.", "labels": [], "entities": []}, {"text": "Consider example (1) below (taken from Arthur Conan Doyle's \"The Adventure of Wisteria Lodge\").", "labels": [], "entities": [{"text": "Arthur Conan Doyle's \"The Adventure of Wisteria Lodge\")", "start_pos": 39, "end_pos": 94, "type": "DATASET", "confidence": 0.6640307713638652}]}, {"text": "Ina frame-semantic analysis of (1), interesting evokes the Mental stimulus stimulus focus (Mssf) frame.", "labels": [], "entities": [{"text": "Mental stimulus stimulus focus (Mssf) frame", "start_pos": 59, "end_pos": 102, "type": "METRIC", "confidence": 0.6581802852451801}]}, {"text": "This frame has two core semantic arguments, EXPERIENCER and STIMULUS, as well as eight peripheral arguments, such as TIME, MANNER, DEGREE.", "labels": [], "entities": [{"text": "EXPERIENCER", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9533601999282837}, {"text": "STIMULUS", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.856564998626709}, {"text": "TIME", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9612286686897278}, {"text": "MANNER", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.8411180973052979}, {"text": "DEGREE", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9524357914924622}]}, {"text": "Of the two core arguments, neither is realized in the same sentence.", "labels": [], "entities": []}, {"text": "Only the peripheral argument DEGREE (DEG) is instantiated and realized by most.", "labels": [], "entities": [{"text": "peripheral argument DEGREE (DEG)", "start_pos": 9, "end_pos": 41, "type": "METRIC", "confidence": 0.7612146039803823}]}, {"text": "To fully comprehend the sentence, it is necessary to infer the fillers of the EXPERI-ENCER and STIMULUS roles, i.e., the reader needs to make an assumption about what is interesting and to whom.", "labels": [], "entities": [{"text": "EXPERI-ENCER", "start_pos": 78, "end_pos": 90, "type": "METRIC", "confidence": 0.9692494869232178}, {"text": "STIMULUS", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9230119585990906}]}, {"text": "For humans this inference is easy to make since the EXPERIENCER (EXP) and STIMULUS (STIM) roles are actually filled by he and a white cock in the previous sentence.", "labels": [], "entities": [{"text": "EXPERIENCER (EXP)", "start_pos": 52, "end_pos": 69, "type": "METRIC", "confidence": 0.8876287788152695}]}, {"text": "Similarly, in (2) right evokes the Correctness (Corr) frame, which has four core arguments, only one of which is filled locally, namely SOURCE (SRC), which is realized by You (and co-referent with Mr. Holmes).", "labels": [], "entities": [{"text": "SOURCE (SRC)", "start_pos": 136, "end_pos": 148, "type": "METRIC", "confidence": 0.9140378534793854}]}, {"text": "However, another argument, INFORMATION (INF), is filled by the preceding sentence (spoken by a different speaker, namely Holmes), which provides details of the fact about which Holmes was right.", "labels": [], "entities": [{"text": "INFORMATION (INF)", "start_pos": 27, "end_pos": 44, "type": "METRIC", "confidence": 0.8611273169517517}]}, {"text": "Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.833033154408137}]}, {"text": "Previous attempts to resolve null instantiated arguments have obtained mixed results.", "labels": [], "entities": []}, {"text": "While obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the) are much lower.", "labels": [], "entities": [{"text": "NI resolution", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9120573699474335}, {"text": "FrameNet-based data set", "start_pos": 125, "end_pos": 148, "type": "DATASET", "confidence": 0.943598210811615}]}, {"text": "This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles.", "labels": [], "entities": [{"text": "Semantic role labelling", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.8028130332628886}]}, {"text": "Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by consists of annotated examples sentences for just a few predicates.", "labels": [], "entities": [{"text": "SemEval 2010 data set", "start_pos": 12, "end_pos": 33, "type": "DATASET", "confidence": 0.8159150928258896}]}, {"text": "This makes the latter data set easier as there are fewer predicates to deal with and more examples per predicate to learn from.", "labels": [], "entities": []}, {"text": "However, this set-up is somewhat artificial and unrealistic.", "labels": [], "entities": []}, {"text": "Independently of whether the NI annotation is done on individual predicates or running texts, it is unlikely that we will ever have sufficient amounts of annotated data to address large-scale NI resolution in a purely supervised fashion.", "labels": [], "entities": [{"text": "NI resolution", "start_pos": 192, "end_pos": 205, "type": "TASK", "confidence": 0.8167597651481628}]}, {"text": "In this paper, we present a system that uses only a minimal amount of supervision.", "labels": [], "entities": []}, {"text": "It combines various basic NI resolvers that exploit different types of linguistic knowledge.", "labels": [], "entities": [{"text": "NI resolvers", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.8112002015113831}]}, {"text": "Most of the basic resolvers employ heuristics; however, we make use of semantic representations of roles learnt from FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.9058694243431091}]}, {"text": "Note that the system does not require data annotated with NI information, only data annotated with overt semantic roles (i.e., FrameNet).", "labels": [], "entities": []}, {"text": "Our paper is largely exploratory; we aim to shed light on what types of information are useful for this task.", "labels": [], "entities": []}, {"text": "Similarly to , we focus mainly on NI resolution, i.e., we assume that it is known whether an argument is missing, which argument is missing, and whether the missing argument has a definite or indefinite interpretation (DNI vs. INI, see Section 2 for details).", "labels": [], "entities": [{"text": "NI resolution", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.8425121307373047}]}], "datasetContent": [{"text": "We first applied all individual resolvers as well as the combination of the four informed resolvers (by majority vote) to the Wisteria data set.", "labels": [], "entities": [{"text": "Wisteria data set", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.9562908411026001}]}, {"text": "As shows, the string and the participant (part) resolvers behave similarly as well as the semantic type (stres) and vector (vec) resolvers: the former two have a relatively high precision but very low recall, while the latter two obtain a higher recall and f-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9970546960830688}, {"text": "recall", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9988387227058411}, {"text": "recall", "start_pos": 246, "end_pos": 252, "type": "METRIC", "confidence": 0.9992952346801758}, {"text": "f-score", "start_pos": 257, "end_pos": 264, "type": "METRIC", "confidence": 0.9062904119491577}]}, {"text": "This is not surprising since string and part on the one hand and stres and vec on the other hand model very similar types of information.", "labels": [], "entities": []}, {"text": "Moreover, the string and part resolvers suffer more from sparse data since they are based on information about argument structures seen before.", "labels": [], "entities": [{"text": "part resolvers", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7380593121051788}]}, {"text": "The more strongly semantic resolvers stres and vec are more robust.", "labels": [], "entities": []}, {"text": "The combination of all resolvers by majority voting outperforms each individual resolver.", "labels": [], "entities": []}, {"text": "However, the difference is not huge, which suggests that there is a certain amount of overlap between the resolvers, i.e. they are not disjoint.", "labels": [], "entities": []}, {"text": "We experimented with other voting schemes besides majority voting, however none led to significant improvements.", "labels": [], "entities": []}, {"text": "As expected, the baseline resolver performs fairly poorly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the SemEval-10 Task-10 corpus", "labels": [], "entities": [{"text": "SemEval-10 Task-10", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8722659051418304}]}, {"text": " Table 2: Results for the individual resolvers on Wisteria", "labels": [], "entities": [{"text": "resolvers", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.8731985688209534}, {"text": "Wisteria", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.667395293712616}]}, {"text": " Table 3: STRES performance on training data for frequent DNI-FEs (forward-and backward-looking)", "labels": [], "entities": [{"text": "STRES", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9785355925559998}]}, {"text": " Table 4: STRES performance on training data for frequent DNI-FEs (backward-looking only)", "labels": [], "entities": [{"text": "STRES", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9760951995849609}]}, {"text": " Table 5: Distribution of DNI instances across targets of different POS in the training data", "labels": [], "entities": []}, {"text": " Table 6: Performance of the semantic type-based resolver for major POS types in the training data", "labels": [], "entities": []}, {"text": " Table 7. Our intuition is that there may be differences  between the domains. For instance, as suggested by the example of the GOAL FE in the Arriving  frame (discussed in 6.1 above) Source and Goal FEs in motion-related frames may be relatively difficult  to resolve. However, the differences between the domains are not statistically significant on the amount  of data we have: the p-value of a Fisher's exact test using the Freeman-Halton extension is 0.17537655.", "labels": [], "entities": [{"text": "GOAL FE", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.8538378179073334}]}, {"text": " Table 7: Resolution performance of STRES for three well-represented domains", "labels": [], "entities": [{"text": "Resolution", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8733308911323547}, {"text": "STRES", "start_pos": 36, "end_pos": 41, "type": "TASK", "confidence": 0.7926084399223328}]}, {"text": " Table 11: Results on Hound Chapter 14 (non gold)", "labels": [], "entities": []}]}