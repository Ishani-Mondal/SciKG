{"title": [{"text": "The AI-KU System at the SPMRL 2013 Shared Task : Unsupervised Features for Dependency Parsing", "labels": [], "entities": [{"text": "SPMRL 2013 Shared Task", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6143491864204407}, {"text": "Dependency Parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.6910855174064636}]}], "abstractContent": [{"text": "We propose the use of the word categories and embeddings induced from raw text as auxiliary features in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.8367480039596558}]}, {"text": "To induce word features, we make use of contex-tual, morphologic and orthographic properties of the words.", "labels": [], "entities": []}, {"text": "To exploit the contextual information , we make use of substitute words, the most likely substitutes for target words, generated by using a statistical language model.", "labels": [], "entities": []}, {"text": "We generate morphologic and orthographic properties of word types in an unsupervised manner.", "labels": [], "entities": []}, {"text": "We use a co-occurrence model with these properties to embed words onto a 25-dimensional unit sphere.", "labels": [], "entities": []}, {"text": "The AI-KU system shows improvements for some of the languages it is trained on for the first Shared Task of Statistical Parsing of Morphologically Rich Languages.", "labels": [], "entities": [{"text": "Statistical Parsing of Morphologically Rich Languages", "start_pos": 108, "end_pos": 161, "type": "TASK", "confidence": 0.8862901926040649}]}], "introductionContent": [{"text": "For the first shared task of Workshop on Statistical Parsing of Morphologically Rich Languages, we propose to use unsupervised features as auxillary features for dependency parsing.", "labels": [], "entities": [{"text": "Statistical Parsing of Morphologically Rich Languages", "start_pos": 41, "end_pos": 94, "type": "TASK", "confidence": 0.897603323062261}, {"text": "dependency parsing", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.8442310392856598}]}, {"text": "We induce the unsupervised features using contextual, morphological and orthographic properties of the words.", "labels": [], "entities": []}, {"text": "We use possible substitutes of the target word which are generated by a statistical language model to exploit the contextual information.", "labels": [], "entities": []}, {"text": "We induce morphological features with a HMMbased model).", "labels": [], "entities": []}, {"text": "We combine contextual, morphological and orthographic features of co-occurring words within the co-occurrence data embedding framework).", "labels": [], "entities": []}, {"text": "The framework embeds word types sharing similar context, morphological and orthographic properties closely on a 25-dimensional sphere.", "labels": [], "entities": []}, {"text": "Thus, it provides the word embeddings on a 25 dimensional sphere.", "labels": [], "entities": []}, {"text": "We conduct experiments using these word embeddings with MaltParser ( and MaltOptimizer (.", "labels": [], "entities": []}, {"text": "In addition to CONLL features (, they are added as additional features and the parsers are configured such that they are able to exploit these additional features.", "labels": [], "entities": []}, {"text": "As a first step we use real valued word embeddings as they are.", "labels": [], "entities": []}, {"text": "Secondly, we discretize the real valued word embeddings.", "labels": [], "entities": []}, {"text": "Finally, we cluster them and find fine-grained word categories for word types.", "labels": [], "entities": []}, {"text": "Our experiments show that, the AI-KU system leads to better results than the baseline experiments for some languages.", "labels": [], "entities": []}, {"text": "We claim that with the correct parameter settings, these unsupervised features could be useful for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8895844221115112}]}, {"text": "In the following sections, we introduce the related work, the algorithm, experiments, results and provide a conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments using MaltParser ( and MaltOptimizer (Ballesteros and Nivre, 2012) with features provided in CONLL format and the additional unsupervised features that we generated with default settings of the parsers.", "labels": [], "entities": []}, {"text": "To make use of additional features, we need to modify MaltParser accordingly.", "labels": [], "entities": []}, {"text": "shows that how we use MaltOptimizer and MaltParser with new features.", "labels": [], "entities": []}, {"text": "In order to handle auxiliary features, the feature model file is modified in two different ways.", "labels": [], "entities": []}, {"text": "We handle new features with feature functions Input and Stack . We should note that other feature functions should also be experimented as a future work.", "labels": [], "entities": [{"text": "Input", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9502878189086914}]}, {"text": "The following subsections explain the details of the experiments.", "labels": [], "entities": []}, {"text": "Our first approach was trying to use word embeddings as they are with the MaltParser.", "labels": [], "entities": []}, {"text": "For each token in the training and the test set, we added the corresponding 25-dimensional word vector from the word embeddings file to the training and test sets.", "labels": [], "entities": []}, {"text": "If the word type is not present in the word embeddings, then, we use the unknown word vector.", "labels": [], "entities": []}, {"text": "The  The second approach is discretizing the real valued vectors.", "labels": [], "entities": []}, {"text": "For each dimension of word embeddings, we separate b equal sized bins.", "labels": [], "entities": []}, {"text": "Then, for each vector's dimensions, we assign their corresponding bin numbers.", "labels": [], "entities": []}, {"text": "The third approach is clustering the word embeddings.", "labels": [], "entities": []}, {"text": "We use a modified k-means algorithm.", "labels": [], "entities": []}, {"text": "We experiment with varying number of clusters k.", "labels": [], "entities": []}, {"text": "For each token in training and test file, we use word type's cluster id as an auxiliary feature.", "labels": [], "entities": []}, {"text": "Again, if the token is not in the word embeddings file, we used the unknown word's cluster id.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Substitute Vector for \"thought\" in above sen- tence.", "labels": [], "entities": []}, {"text": " Table 2: Results on German with MaltParser of Development Set with Default Settings", "labels": [], "entities": []}, {"text": " Table 3: Results on German with MaltOptimizer of Development Set", "labels": [], "entities": []}, {"text": " Table 4: Results on French", "labels": [], "entities": [{"text": "French", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.974790096282959}]}, {"text": " Table 5: Results on German", "labels": [], "entities": [{"text": "German", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.9425985813140869}]}, {"text": " Table 6: Results on Hebrew", "labels": [], "entities": []}, {"text": " Table 7: Results on Hungarian", "labels": [], "entities": [{"text": "Hungarian", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.9421650767326355}]}, {"text": " Table 8: Results on Polish", "labels": [], "entities": [{"text": "Polish", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.9629995226860046}]}, {"text": " Table 9: Results on Swedish", "labels": [], "entities": [{"text": "Swedish", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9516528844833374}]}, {"text": " Table 10: Results of Multi Word Expressions on French", "labels": [], "entities": [{"text": "Multi Word Expressions on French", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.630553025007248}]}]}