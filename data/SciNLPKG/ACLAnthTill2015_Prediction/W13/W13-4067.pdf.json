{"title": [{"text": "A Simple and Generic Belief Tracking Mechanism for the Dialog State Tracking Challenge: On the believability of observed information", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.7491454780101776}]}], "abstractContent": [{"text": "This paper presents a generic dialogue state tracker that maintains beliefs over user goals based on a few simple domain-independent rules, using basic probability operations.", "labels": [], "entities": [{"text": "dialogue state tracker", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.6891538302103678}]}, {"text": "The rules apply to observed system actions and partially observable user acts, without using any knowledge obtained from external resources (i.e. without requiring training data).", "labels": [], "entities": []}, {"text": "The core insight is to maximise the amount of information directly gainable from an error-prone dialogue itself, so as to better lower-bound one's expectations on the performance of more advanced statistical techniques for the task.", "labels": [], "entities": []}, {"text": "The proposed method is evaluated in the Dialog State Tracking Challenge, where it achieves comparable performance in hypothesis accuracy to machine learning based systems.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.7839827090501785}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9555768966674805}]}, {"text": "Consequently , with respect to different scenarios for the belief tracking problem, the potential superiority and weakness of machine learning approaches in general are investigated .", "labels": [], "entities": [{"text": "belief tracking problem", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.8602625131607056}]}], "introductionContent": [{"text": "Spoken dialogue system (SDS) can be modelled as a decision process, in which one of the main problems researchers try to overcome is the uncertainty in tracking dialogue states due to errorprone outputs from automatic speech recognition (ASR) and spoken language understanding (SLU) components.", "labels": [], "entities": [{"text": "Spoken dialogue system (SDS)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8469756444295248}, {"text": "automatic speech recognition (ASR)", "start_pos": 208, "end_pos": 242, "type": "TASK", "confidence": 0.7812803586324056}]}, {"text": "Recent advances in SDS have demonstrated that maintaining a distribution over a set of possible (hidden) dialogue states and optimising dialogue policies with respect to long term expected rewards can significantly improve the interaction performance ().", "labels": [], "entities": [{"text": "SDS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9868541955947876}]}, {"text": "Such methods are usually developed under a partially observable Markov decision process (POMDP) framework (, where the distribution over dialogue states is called a 'belief' and is modelled as a posterior updated every turn given an observation.", "labels": [], "entities": []}, {"text": "Furthermore, instead of simply taking the most probable (or highest confidence score) hypothesis of the user act as in 'traditional' handcrafted systems, the observation here may consist of an n-best list of the SLU hypotheses (dialogue acts) with (normalised) confidence scores.", "labels": [], "entities": []}, {"text": "See () for more details of POMDP-based SDS.", "labels": [], "entities": [{"text": "POMDP-based SDS", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.5031541734933853}]}, {"text": "It is understandable that beliefs more accurately estimating the true dialogue states will ease the tuning of dialogue policies, and hence can result in better overall system performance.", "labels": [], "entities": []}, {"text": "The accuracy of belief tracking has been studied in depth by Williams (2012) based on two SDS in public use.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.99826580286026}, {"text": "belief tracking", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7589798271656036}]}, {"text": "Here the effects of several mechanisms are analysed, which can alter the 'most-believed' dialogue state hypothesis (computed using a generative POMDP model) from the one derived directly from an observed top SLU hypothesis.", "labels": [], "entities": []}, {"text": "Williams's work comprehensively explores how and why a machine learning approach (more specifically the generative model proposed in) functions in comparison with a naive baseline.", "labels": [], "entities": []}, {"text": "However, we target a missing intermediate analysis in this work: how much information one can gain purely from the SLU n-best lists (and the corresponding confidence scores), without any prior knowledge either being externally learned (using data-driven methods) or designed (based on domain-specific strategies), but beyond only considering the top SLU hypotheses.", "labels": [], "entities": []}, {"text": "We explain this idea in greater detail as follows.", "labels": [], "entities": []}, {"text": "Firstly, we can view the belief update procedure in previous models as re-constructing the hidden dialogue states (or user goals) based on the previous belief, a current observation (normally an SLU n-best list), and some prior knowledge.", "labels": [], "entities": []}, {"text": "The prior knowledge can be observation probabilities given a hidden state, the previous system action and/or dialogue histories (, or probabilistic domain-specific ontologies (, where the probabilities can be either trained on a collection of dialogue examples or manually assigned by human experts.", "labels": [], "entities": []}, {"text": "In such models, a common strategy is to use the confidence scores in the observed n-best list as immediate information substituted into the model for belief computation, which implies that the performance of such belief tracking methods to a large extent depends on the reliability of the confidence scores.", "labels": [], "entities": [{"text": "belief computation", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.8373961746692657}]}, {"text": "On the other hand, since the confidence scores may reflect the probabilities of the occurrences of corresponding user acts (SLU hypotheses), a belief can also be maintained based on basic probability operations on those events (as introduced in this paper).", "labels": [], "entities": []}, {"text": "Such a belief will advance the estimation obtained from top SLU hypotheses only, and can serve as a baseline to justify how much further improvement is actually contributed by the use of prior knowledge.", "labels": [], "entities": []}, {"text": "Note that the fundamental method in this paper relies on the assumption that confidence scores carry some useful information, and their informativeness will affect the performance of the proposed method as will be seen in our experiments (Section 5).", "labels": [], "entities": []}, {"text": "Therefore, this paper presents a generic belief tracker that maintains beliefs over user goals only using information directly observable from the dialogue itself, including SLU n-best list confidence scores and user and system behaviours, such as a user not disconfirming an implicit confirmation of the system, or the system explicitly rejecting a query (since no matching item exists), etc.", "labels": [], "entities": [{"text": "SLU n-best list confidence scores", "start_pos": 174, "end_pos": 207, "type": "METRIC", "confidence": 0.5727555096149445}]}, {"text": "The belief update is based on simple probability operations and a few very general domainindependent rules.", "labels": [], "entities": []}, {"text": "The proposed method was evaluated in the Dialog State Tracking Challenge (DSTC) ( . A systematic analysis is then conducted to investigate the extent to which machine learning can advance this naive strategy.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge (DSTC)", "start_pos": 41, "end_pos": 79, "type": "TASK", "confidence": 0.7873583691460746}]}, {"text": "Moreover, the results show the performance of the proposed method to be comparable to other machine learning based approaches, which, in consideration of the simplicity of its implementation, suggests that another practical use of the proposed method could be as a module in an initial system installation to collect training data for machine learning techniques, in addition to functioning as a baseline for further analysing them.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews some basic mathematical background, based on which Section 3 introduces the proposed belief tracker.", "labels": [], "entities": [{"text": "belief tracker", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.6592837274074554}]}, {"text": "Section 4 briefly describes the DSTC task.", "labels": [], "entities": [{"text": "DSTC task", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.8833584487438202}]}, {"text": "The evaluation results and detailed analysis are illustrated in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we further discuss in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The method proposed in this paper corresponds to Team 2, Entry 1 in the DSTC submissions.", "labels": [], "entities": [{"text": "DSTC submissions", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.9511628448963165}]}, {"text": "In the following analysis, we will compare it with the 26 machine learning models submitted by the other 8 anonymised participant teams plus a baseline system (Team 0, Entry 1) that only considers the top SLU result.", "labels": [], "entities": []}, {"text": "Each team can submit up to 5 systems, whilst the systems from a same team may differ from each other in either the statistical model or the training data selection (or both of them).", "labels": [], "entities": []}, {"text": "There is a brief description of each system available after the challenge.", "labels": [], "entities": []}, {"text": "For the convenience of analysis and illustration, on each test set we categorise these systems into the following groups: in-domainsystems trained only using the data sets which are similar (including the 'to-some-extent-similar' ones) to the particular test set, out-of-domainsystems trained on the data sets which are totally different from the particular test set, mixeddomain -systems trained on a mixture of the indomain and out-of-domain data, and ensemblesystems combining multiple models to generate their final output.", "labels": [], "entities": []}, {"text": "(The ensemble systems here are all trained on the mixed-domain data.)", "labels": [], "entities": []}, {"text": "Note that, for test4 there are no in-domain data available, so all those non-ensemble systems are merged into one group.", "labels": [], "entities": []}, {"text": "Detailed system categorisation on each test set can be found in Appendix A.", "labels": [], "entities": []}], "tableCaptions": []}