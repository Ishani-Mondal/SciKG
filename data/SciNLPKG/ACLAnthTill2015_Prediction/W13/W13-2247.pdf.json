{"title": [{"text": "LORIA System for the WMT13 Quality Estimation Shared Task", "labels": [], "entities": [{"text": "LORIA", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8791190981864929}, {"text": "WMT13 Quality Estimation Shared Task", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.7515906453132629}]}], "abstractContent": [{"text": "In this paper we present the system we submitted to the WMT13 shared task on Quality Estimation.", "labels": [], "entities": [{"text": "WMT13 shared task", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.5585328141848246}, {"text": "Quality Estimation", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.6663632392883301}]}, {"text": "We participated in the Task 1.1.", "labels": [], "entities": []}, {"text": "Each translated sentence is given a score between 0 and 1.", "labels": [], "entities": []}, {"text": "The score is obtained by using several numerical or boolean features calculated according to the source and target sentences.", "labels": [], "entities": []}, {"text": "We perform a linear regression of the feature space against scores in the range [0..1].", "labels": [], "entities": []}, {"text": "To this end, we use a Support Vector Machine with 66 features.", "labels": [], "entities": []}, {"text": "In this paper, we propose to increase the size of the training corpus.", "labels": [], "entities": []}, {"text": "For that, we use the post-edited and reference corpora during the training step.", "labels": [], "entities": []}, {"text": "We assign a score to each sentence of these corpora.", "labels": [], "entities": []}, {"text": "Then, we tune these scores on a development corpus.", "labels": [], "entities": []}, {"text": "This leads to an improvement of 10.5% on the development corpus, in terms of Mean Average Error, but achieves only a slight improvement on the test corpus.", "labels": [], "entities": [{"text": "Mean Average Error", "start_pos": 77, "end_pos": 95, "type": "METRIC", "confidence": 0.9364642898241679}]}], "introductionContent": [{"text": "In the scope of Machine Translation (MT), Quality Estimation (QE) is the task consisting to evaluate the translation quality of a sentence or a document.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.8658355712890625}, {"text": "Quality Estimation (QE)", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.6126223027706146}]}, {"text": "This process maybe useful for post-editors to decide or not to revise a sentence produced by a MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9612671136856079}]}, {"text": "Moreover, it can be useful to decide if a translated document can be broadcasted or not.", "labels": [], "entities": []}, {"text": "The most obvious way to give a score to a translated sentence consists in using a machine learning approach.", "labels": [], "entities": []}, {"text": "This approach is supervised: experts are asked to score translated sentences and with the obtained material, one learns a prediction model of scores.", "labels": [], "entities": []}, {"text": "The main drawback of the machine learning approach is that it is supervised and requires huge data.", "labels": [], "entities": []}, {"text": "To score a sentence is time-consuming.) dealt with this issue by proposing unsupervised similarity measures.", "labels": [], "entities": []}, {"text": "In fact, the score of a translated sentence is defined by a measure giving the distance between it and the contents of an external corpus.", "labels": [], "entities": []}, {"text": "The authors improve the results of the supervised approach but this method can be used only in the ranking task.) proposed a method to add errors in reference sentences (deletion, substitution, insertion).", "labels": [], "entities": []}, {"text": "By this way, they build additional corpus in which each word can be associated with a label correct/not correct.", "labels": [], "entities": []}, {"text": "But, it is not possible to predict the translation quality of sentences including these erroneous words.", "labels": [], "entities": []}, {"text": "In this paper, we propose to increase the size of the training corpus.", "labels": [], "entities": []}, {"text": "For that, we use the score given by experts to evaluate additional sentences from the post-edited and reference corpora.", "labels": [], "entities": []}, {"text": "Practically, we extract from source and target sentences numerical vectors (features) and we learn a prediction model of the scores.", "labels": [], "entities": []}, {"text": "Then, we apply this model to predict the scores of the post-edited and the reference sentences.", "labels": [], "entities": []}, {"text": "And finally, we tune the predicted scores on a development corpus.", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we give an overview of our machine learning approach and of the features we use.", "labels": [], "entities": []}, {"text": "Then, in Sections 3 and 4 we describe the corpora and how we increase the size of the training corpus by a partlyunsupervised approach.", "labels": [], "entities": []}, {"text": "In section 5, we give results about this method and we end by a conclusion and perspectives.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance in terms of MAE without  increasing the training corpus", "labels": [], "entities": [{"text": "MAE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9645869731903076}, {"text": "training", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.969575822353363}]}, {"text": " Table 2: Statistics on HTER for the three sets of  sentences used in the training corpus", "labels": [], "entities": [{"text": "HTER", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.563148558139801}]}, {"text": " Table 3: Statistics on HTER for the three sets of  sentences used in the training corpus. Nb is the  number of sentences", "labels": [], "entities": [{"text": "HTER", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.6780429482460022}]}, {"text": " Table 4: Performance in terms of MAE of the fea- tures with and without increasing the training cor- pus", "labels": [], "entities": [{"text": "MAE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9992454051971436}, {"text": "training cor- pus", "start_pos": 88, "end_pos": 105, "type": "METRIC", "confidence": 0.7164684981107712}]}, {"text": " Table 5: Statistics on HTER for the post and ref  sets of sentences used in the training corpus, after  tuning", "labels": [], "entities": [{"text": "HTER", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.623627245426178}]}, {"text": " Table 6: Statistics on HTER for the post and ref  sets of sentences used in the training corpus, after  tuning. Nb is the number of sentences.", "labels": [], "entities": []}]}