{"title": [{"text": "A Comparison of Smoothing Techniques for Bilingual Lexicon Extraction from Comparable Corpora", "labels": [], "entities": [{"text": "Bilingual Lexicon Extraction from Comparable Corpora", "start_pos": 41, "end_pos": 93, "type": "TASK", "confidence": 0.7225113113721212}]}], "abstractContent": [{"text": "Smoothing is a central issue in language modeling and a prior step in different natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "Smoothing", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9694184064865112}, {"text": "language modeling", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7301911264657974}]}, {"text": "However, less attention has been given to it for bilingual lexicon extraction from comparable corpora.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6470799744129181}]}, {"text": "If a first work to improve the extraction of low frequency words showed significant improvement while using distance-based averaging (Pekar et al., 2006), no investigation of the many smoothing techniques has been carried out so far.", "labels": [], "entities": []}, {"text": "In this paper , we present a study of some widely-used smoothing algorithms for language n-gram modeling (Laplace, Good-Turing, Kneser-Ney...).", "labels": [], "entities": [{"text": "language n-gram modeling", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.6473178168137869}]}, {"text": "Our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach (Fung, 1998) traditionally used for bilingual lexicon extraction.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 163, "end_pos": 191, "type": "TASK", "confidence": 0.6873166163762411}]}, {"text": "We show that using smoothing as a pre-processing step of the standard approach increases its performance significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cooccurrences play an important role in many corpus based approaches in the field of naturallanguage processing ().", "labels": [], "entities": []}, {"text": "They represent the observable evidence that can be distilled from a corpus and are employed fora variety of applications such as machine translation (, information retrieval, word sense disambiguation (, etc.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.79604771733284}, {"text": "information retrieval", "start_pos": 152, "end_pos": 173, "type": "TASK", "confidence": 0.7976312041282654}, {"text": "word sense disambiguation", "start_pos": 175, "end_pos": 200, "type": "TASK", "confidence": 0.7190767526626587}]}, {"text": "In bilingual lexicon extraction from comparable corpora, frequency counts for word pairs often serve as a basis for distributional methods, such as the standard approach) which compares the cooccurrence profile of a given source word, a vector of association scores for its translated cooccurrences, with the profiles of all words of the target language.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6952751676241556}]}, {"text": "The distance between two such vectors is interpreted as an indicator of their semantic similarity and their translational relation.", "labels": [], "entities": []}, {"text": "If using association measures to extract word translation equivalents has shown a better performance than using a raw cooccurrence model, the latter remains the core of any statistical generalisation).", "labels": [], "entities": [{"text": "word translation equivalents", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.7781022389729818}]}, {"text": "As has been known, words and other type-rich linguistic populations do not contain instances of all types in the population, even the largest samples.", "labels": [], "entities": []}, {"text": "Therefore, the number and distribution of types in the available sample are not reliable estimators, especially for small comparable corpora.", "labels": [], "entities": []}, {"text": "The literature suggests two major approaches for solving the data sparseness problem: smoothing and class-based methods.", "labels": [], "entities": []}, {"text": "Smoothing techniques are often used to better estimate probabilities when there is insufficient data to estimate probabilities accurately.", "labels": [], "entities": []}, {"text": "They tend to make distributions more uniform, by adjusting low probabilities such as zero probabilities upward, and high probabilities downward.", "labels": [], "entities": []}, {"text": "Generally, smoothing methods not only prevent zero probabilities, but they also attempt to improve the accuracy of the model as a whole).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9988079071044922}]}, {"text": "Class-based models () use classes of similar words to distinguish between unseen cooccurrences.", "labels": [], "entities": []}, {"text": "The relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones.", "labels": [], "entities": []}, {"text": "They presented a method that makes analogies between each specific unseen cooccurrence and other cooccurrences that contain similar words.", "labels": [], "entities": []}, {"text": "The analogies are based on the assumption that similar word cooccurrences have similar values of mutual information.", "labels": [], "entities": []}, {"text": "Their method has shown significant improvement for both: word sense disambiguation in machine translation and data recovery tasks.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.7800509730974833}, {"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8032816052436829}, {"text": "data recovery tasks", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8508640130360922}]}, {"text": "() employed the nearest neighbor variety of the previous approach to extract translation equivalents for low frequency words from comparable corpora.", "labels": [], "entities": []}, {"text": "They used a distance-based averaging technique for smoothing).", "labels": [], "entities": []}, {"text": "Their method yielded a significant improvement in relation to low frequency words.", "labels": [], "entities": []}, {"text": "Starting from the assumption that smoothing improves the accuracy of the model as a whole), we believe that smoothed context vectors should lead to better performance for bilingual terminology extraction from comparable corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9981822967529297}, {"text": "bilingual terminology extraction", "start_pos": 171, "end_pos": 203, "type": "TASK", "confidence": 0.5895224710305532}]}, {"text": "In this work we carryout an empirical comparison of the most widely-used smoothing techniques, including additive smoothing, Good-Turing estimate, Jelinek-Mercer, and kneser-Ney smoothing (.", "labels": [], "entities": [{"text": "additive smoothing", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.722574383020401}]}, {"text": "Unlike (), the present work does not investigate unseen words.", "labels": [], "entities": []}, {"text": "We only concentrate on observed cooccurrences.", "labels": [], "entities": []}, {"text": "We believe it constitutes the most systematic comparison made so far with different smoothing techniques for aligning translation equivalents from comparable corpora.", "labels": [], "entities": []}, {"text": "We show that using smoothing as a pre-processing step of the standard approach, leads to significant improvement even without considering unseen cooccurrences.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we present in Section 2, the different smoothing techniques.", "labels": [], "entities": []}, {"text": "The steps of the standard approach and our extended method are then described in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental setup and our resources.", "labels": [], "entities": []}, {"text": "Section 5 presents the experiments and comments on several results.", "labels": [], "entities": []}, {"text": "We finally discuss the results in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the smoothing techniques, several resources and parameters are needed.", "labels": [], "entities": []}, {"text": "We present hereafter the experiment data and the parameters of the standard approach.", "labels": [], "entities": []}, {"text": "Three major parameters need to beset to the standard approach, namely the similarity measure, the association measure defining the entry vectors and the size of the window used to build the context vectors.", "labels": [], "entities": []}, {"text": "() carried out a complete study of the influence of these parameters on the quality of bilingual alignment.", "labels": [], "entities": []}, {"text": "As a similarity measure, we chose to use Weighted Jaccard Index ( and Cosine similarity.", "labels": [], "entities": [{"text": "Weighted Jaccard Index", "start_pos": 41, "end_pos": 63, "type": "METRIC", "confidence": 0.8611905574798584}, {"text": "Cosine similarity", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.9504103660583496}]}, {"text": "The entries of the context vectors were determined by the log-likelihood, mutual information and the discounted Odds-ratio (.", "labels": [], "entities": [{"text": "Odds-ratio", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.6918443441390991}]}, {"text": "We also chose a 7-window size.", "labels": [], "entities": []}, {"text": "Other combinations of parameters were assessed but the previous parameters turned out to give the best performance.", "labels": [], "entities": []}, {"text": "We note that 'Top k' means that the correct translation of a given word is present in the k first candidates of the list returned by the standard approach.", "labels": [], "entities": []}, {"text": "We use also the mean average precision MAP ( which represents the quality of the system.", "labels": [], "entities": [{"text": "precision MAP", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.7516701519489288}]}, {"text": "where |Q| is the number of terms to be translated, mi is the number of reference translations for the i th term (always 1 in our case), and P (R ik ) is 0 if the reference translation is not found for the i th term or 1/r if it is (r is the rank of the reference translation in the translation candidates).", "labels": [], "entities": []}, {"text": "We conducted a set of three experiments on two specialized comparable corpora.", "labels": [], "entities": []}, {"text": "We carried out a comparison between the standard approach (SA) and the smoothing techniques presented in Section 2 namely : additive smoothing (Add1), GoodTuring smoothing (GT), the Jelinek-Mercer technique (JM), the Katz-Backoff (Katz) and kneserNey smoothing (Kney).", "labels": [], "entities": []}, {"text": "Experiment 1 shows the results for the breast cancer corpus.", "labels": [], "entities": []}, {"text": "Experiment 2 shows the results for the wind energy corpus and finally experiment 3 presents a comparison of the best configurations on both corpora.", "labels": [], "entities": []}, {"text": "shows the results of the experiments on the breast cancer corpus.", "labels": [], "entities": [{"text": "breast cancer corpus", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.5890003939469656}]}, {"text": "The first observation concerns the standard approach (SA).", "labels": [], "entities": [{"text": "standard approach (SA", "start_pos": 35, "end_pos": 56, "type": "METRIC", "confidence": 0.6916884034872055}]}, {"text": "The best results are obtained using the Log-Jac parameters with a MAP = 27.9%.", "labels": [], "entities": [{"text": "MAP", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9989973902702332}]}, {"text": "We can also notice that for this configuration, only the Additive smoothing significantly improves the performance of the standard approach with a MAP = 30.6%.", "labels": [], "entities": [{"text": "MAP", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.9979795813560486}]}, {"text": "The other smoothing techniques even degrade the results.", "labels": [], "entities": []}, {"text": "The second observation concerns the Odds-Cos parameters where none of the smoothing techniques significantly improved the performance of the baseline (SA).", "labels": [], "entities": []}, {"text": "Although Good-Turing and Katz-Backoff smoothing give slightly better results with respectively a MAP = 25.2 % and MAP = 25.3 %, these results are not significant.", "labels": [], "entities": [{"text": "MAP", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9971572160720825}, {"text": "MAP", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9972255825996399}]}, {"text": "The most notable result concerns the PMI-COS parameters.", "labels": [], "entities": []}, {"text": "We can notice that four of the five smoothing techniques improve the performance of the baseline.", "labels": [], "entities": []}, {"text": "The best smoothing is the Jelinek-Mercer technique which reaches a MAP = 29.5% and improves the Top1 precision of 6% and the Top10 precision of 10.3%.", "labels": [], "entities": [{"text": "MAP", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9995759129524231}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.7241250276565552}, {"text": "Top10", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.5545076727867126}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.6867129802703857}]}, {"text": "shows the results of the experiments on the wind energy corpus.", "labels": [], "entities": []}, {"text": "Generally the results exhibit the same behaviour as the previous experiment.", "labels": [], "entities": []}, {"text": "The best results of the standard approach are obtained using the Log-Jac parameters: Results of the experiments on the \"Breast cancer\" corpus (except the Odds-Cos configuration, the improvements indicate a significance at the 0.05 level using the Student t-test).: Results of the experiments on the \"Wind Energy\" corpus (except the Odds-Cos configuration, the improvements indicate a significance at the 0.05 level using the Student t-test).", "labels": [], "entities": [{"text": "Breast cancer\" corpus", "start_pos": 120, "end_pos": 141, "type": "DATASET", "confidence": 0.5337419360876083}]}, {"text": "In this experiment, we would like to investigate whether the smoothing techniques are more efficient for frequent translation equivalents or less frequent ones.", "labels": [], "entities": []}, {"text": "For that purpose, we split the breast cancer reference list of 321 entries into two sets of translation pairs.", "labels": [], "entities": [{"text": "breast cancer reference list of 321 entries", "start_pos": 31, "end_pos": 74, "type": "DATASET", "confidence": 0.8072571924754551}]}, {"text": "A set of 133 frequent pairs named : High-test set and a set of 188 less frequent pairs called Low-test set.", "labels": [], "entities": []}, {"text": "The initial reference list of 321 pairs is the Full-test set.", "labels": [], "entities": [{"text": "Full-test set", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.8891235589981079}]}, {"text": "We consider frequent pairs those of a frequency higher than 100.", "labels": [], "entities": []}, {"text": "We chose to analyse the two configurations that provided the best performance that is : Log-Jac and Pmi-Cos parameters according to the Full-test, High-test and Low-test sets.", "labels": [], "entities": []}, {"text": "shows the results using the LogJac configuration.", "labels": [], "entities": []}, {"text": "We can see that the additive smoothing always outperforms the standard approach for all the test sets.", "labels": [], "entities": []}, {"text": "The other smoothing techniques are always under the baseline and behave approximately the same way.", "labels": [], "entities": []}, {"text": "shows the results using the PMI-COS configuration.", "labels": [], "entities": []}, {"text": "We can see that except the Kneser-Ney smoothing, all the smoothing techniques outperform the standard approach for all the test sets.", "labels": [], "entities": []}, {"text": "We can also notice that the Jelinek-Mercer smoothing improves more notably the High-test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of the experiments on the \"Breast  cancer\" corpus (except the Odds-Cos configura- tion, the improvements indicate a significance at  the 0.05 level using the Student t-test).", "labels": [], "entities": [{"text": "Breast  cancer\" corpus", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.6689759716391563}, {"text": "Odds-Cos configura- tion", "start_pos": 80, "end_pos": 104, "type": "METRIC", "confidence": 0.8914869129657745}]}, {"text": " Table 4: Results of the experiments on the \"Wind  Energy\" corpus (except the Odds-Cos configura- tion, the improvements indicate a significance at  the 0.05 level using the Student t-test).", "labels": [], "entities": [{"text": "Wind  Energy\" corpus", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.6632731184363365}]}]}