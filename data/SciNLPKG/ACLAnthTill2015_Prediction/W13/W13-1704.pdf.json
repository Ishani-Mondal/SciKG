{"title": [{"text": "Developing and testing a self-assessment and tutoring system", "labels": [], "entities": []}], "abstractContent": [{"text": "Automated feedback on writing maybe a useful complement to teacher comments in the process of learning a foreign language.", "labels": [], "entities": []}, {"text": "This paper presents a self-assessment and tutoring system which combines an holistic score with detection and correction of frequent errors and furthermore provides a qualitative assessment of each individual sentence, thus making the language learner aware of potentially problematic areas rather than providing a panacea.", "labels": [], "entities": []}, {"text": "The system has been tested by learners in a range of educational institutions, and their feedback has guided its development.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to write a foreign language well requires a considerable amount of practice and appropriate feedback.", "labels": [], "entities": []}, {"text": "Good teachers are essential, but their time is limited.", "labels": [], "entities": []}, {"text": "As recently shown in a study by) conducted amongst first-year students of English at a Taiwanese university, automated writing evaluation can lead to increased learner autonomy and higher writing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9561706781387329}]}, {"text": "In this paper, we investigate the merits of a self-assessment and tutoring (SAT) system specifically aimed at intermediate learners of English, at around B2 level in the Common European Framework of Reference for Languages (CEFR)).", "labels": [], "entities": [{"text": "Common European Framework of Reference for Languages (CEFR))", "start_pos": 170, "end_pos": 230, "type": "DATASET", "confidence": 0.5873014628887177}]}, {"text": "There area large number of students at this level, and they should have sufficient knowledge of the language to benefit from the system whilst at the same time committing errors which can be identified reliably.", "labels": [], "entities": []}, {"text": "The system provides automated feedback on learners' writing at three different levels of granularity: an overall assessment of their proficiency, a score for each individual sentence, highlighting well-written passages as well as ones requiring more work, and specific comments on local issues including spelling and word choice.", "labels": [], "entities": [{"text": "word choice", "start_pos": 317, "end_pos": 328, "type": "TASK", "confidence": 0.6675392240285873}]}, {"text": "Computer-based writing tools have been around fora longtime, with Criterion (, which also provides a number of features for teachers) and ESL Assistant (, not currently available) aimed specifically at secondlanguage learners, but the idea of indicating the relative quality of different parts of a text (sentences in our case) has, to the best of our knowledge, not been implemented previously.", "labels": [], "entities": []}, {"text": "This kind of non-specific feedback does not provide a precise diagnosis or immediate cure, but might have the advantage of fostering learning.", "labels": [], "entities": []}, {"text": "In addition to describing the SAT system itself, we present a series of three trials in which learners of English in a number of educational contexts used the system as a tool to work on written responses to specific tasks and improve their writing skills.", "labels": [], "entities": []}], "datasetContent": [{"text": "The second component of the SAT system automatically assesses and scores the quality of individual sentences, independently of their context.", "labels": [], "entities": []}, {"text": "The challenge of assessing intra-sentential quality lies in the limited linguistic evidence that can be extracted automatically from relatively short sentences for them to be assessed reliably, in addition to the difficulty in acquiring annotated data, since rating a response sentence by sentence is not something examiners typically do and would therefore require an additional and expensive manual annotation effort.", "labels": [], "entities": []}, {"text": "Previous work has primarily focused on automatic content scoring of short answers, ranging from a few words to a few sentences).", "labels": [], "entities": [{"text": "automatic content scoring of short answers", "start_pos": 39, "end_pos": 81, "type": "TASK", "confidence": 0.7713166375954946}]}, {"text": "On the other hand, scoring of individual sentences with respect to their linguistic quality, specifically in learner texts, has received considerably less attention.", "labels": [], "entities": []}, {"text": "devised guidelines for the manual annotation of sentences in learner texts, and evaluated a rule-based approach that classifies sentences with respect to clarity of expression based on grammar, mechanics and word usage errors; however, their system performs binary classification, whereas we are focusing on scoring sentences.", "labels": [], "entities": [{"text": "manual annotation of sentences in learner texts", "start_pos": 27, "end_pos": 74, "type": "TASK", "confidence": 0.7743200744901385}]}, {"text": "Writing instruction tools, such as Criterion (, give advice on stylistic and organisational issues and automatically detect a variety of errors in the text, though they do not explicitly allow for an overall evaluation of sentences with respect to various writing aspects.", "labels": [], "entities": []}, {"text": "The latter, used in combination with an error feedback component (see Section 2.3), can be a useful instrument informing learners about the severity of their mistakes; for example, although sentences may contain some errors, they may still maintain a certain level of acceptability that does not impede communication.", "labels": [], "entities": []}, {"text": "Moreover, indicating problematic regions maybe better from a pedagogic point of view than detecting and correcting all errors identified in the text.", "labels": [], "entities": []}, {"text": "To date, there is no publically available annotated dataset consisting of sentences marked with a score representing their linguistic quality.", "labels": [], "entities": []}, {"text": "Manual annotation is typically expensive and time-consuming, and a certain amount of annotator training is generally required.", "labels": [], "entities": []}, {"text": "Instead, we exploit already available annotated data -scores and error annotation in the FCE dataset -and evaluate various approaches, two of which are: a) to use the script-level model (see Section 2.1) to predict sentence quality scores, and b) to use the script-level score divided by the total number of (manually annotated) errors in a sentence as pseudo-gold labels to train a sentence-level model.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9653270542621613}]}, {"text": "As the models above are expected to contain a certain amount of noise, it is imperative that we identify evaluation measures that are indicative of our application -that is, assign higher scores to highquality sentences compared to low-quality onesand not only depend on the labels they have been trained on.", "labels": [], "entities": []}, {"text": "More specifically, we use correlation with pseudo-gold scores (r g and \u03c1 g ; not applicable to the script-level model), correlation with the scriptlevel scores by first averaging predicted sentencelevel scores (r sand \u03c1 s ), correlation with error counts (r e and \u03c1 e ), average precision (AP) and pairwise accuracy.", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 271, "end_pos": 293, "type": "METRIC", "confidence": 0.8347245693206787}, {"text": "accuracy", "start_pos": 307, "end_pos": 315, "type": "METRIC", "confidence": 0.9563458561897278}]}, {"text": "AP is a measure used in information retrieval to evaluate systems that return a ranked list of documents.", "labels": [], "entities": [{"text": "AP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9387969374656677}, {"text": "information retrieval", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7318834662437439}]}, {"text": "Herein, sentences are ranked by their predicted scores, precision is calculated at each correct sentence (that is, containing no errors), and averaged overall correct sentences (in other words, we treat sentences with no errors as the 'relevant documents').", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.999561607837677}]}, {"text": "Pairwise accuracy is calculated based on the number of times the corrected sentence (available through the error annotation in the FCE dataset) is ranked higher than the original one written by the candidate, ignoring sentences without errors.", "labels": [], "entities": [{"text": "Pairwise", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9350244998931885}, {"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8547242283821106}, {"text": "FCE dataset", "start_pos": 131, "end_pos": 142, "type": "DATASET", "confidence": 0.9491879642009735}]}, {"text": "Correlation with error counts, average precision and pairwise accuracy are particularly important as they reflect more directly the extent to which good and bad sentences are discriminated.", "labels": [], "entities": [{"text": "error counts", "start_pos": 17, "end_pos": 29, "type": "METRIC", "confidence": 0.8938428163528442}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.848772406578064}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9366030097007751}]}, {"text": "Again, in both cases, we employ a linear ranking perceptron.", "labels": [], "entities": []}, {"text": "We conducted a series of experiments on a separate development set to evaluate the performance of features beyond the ones used in the script-level model.", "labels": [], "entities": []}, {"text": "The final results, reported in, are calculated on the FCE test set (.", "labels": [], "entities": [{"text": "FCE test set", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9492677450180054}]}, {"text": "The texts were parsed using RASP ().", "labels": [], "entities": [{"text": "RASP", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.483730286359787}]}, {"text": "Model a, the script-level model, does notwork as well at the sentence level.", "labels": [], "entities": []}, {"text": "However, it does perform better when evaluated against script-level scores (r sand \u03c1 s ), and this is expected given that it is trained directly on gold script-level scores.", "labels": [], "entities": []}, {"text": "On the other hand, this evaluation measure is not as indicative of good performance in our application as the others, as it does not take into account the varying quality of individual sentences within a script.", "labels": [], "entities": []}, {"text": "Training the script-level model with different feature sets (including those utilised in the sentencelevel model) did not yield an improvement in performance (the results are omitted due to space restrictions).", "labels": [], "entities": []}, {"text": "Additional experiments were conducted to investigate the effect of training the sentence-level model with different pseudo-gold labels (e.g., additive/subtractive pseudo-gold scores rather than divisive/multiplicative), but the results are not reported here as the difference in performance was not substantial.", "labels": [], "entities": []}, {"text": "shows that better performance can be achieved with our pseudo-gold labels, used to train a model at the sentence level, rather than gold la- bels at the script level.", "labels": [], "entities": []}, {"text": "To evaluate this further, we trained a sentence-level model using the scriptlevel scores as labels (that is, sentences within the same script are all assigned the same label/score).", "labels": [], "entities": []}, {"text": "However, this did not improve performance (again, the results are omitted due to space restrictions).", "labels": [], "entities": []}, {"text": "We also point out that the best-performing feature space (described above) is based on text properties that are more likely to be present in relatively short sentences (e.g., the presence of main verbs), compared to those used for script-level models in previous work (Yannakoudakis et al., 2011), such as word and part-of-speech bigrams and trigrams, which maybe too sparse fora sentence-level model.", "labels": [], "entities": []}, {"text": "Analogously to what we did to present the overall score, we developed a sentence score feedback view to indicate the general quality of the sentences, as given by our best model, by highlighting each of them with a background colour ranging from green fora well-written sentence, via yellow and orange fora sentence which the system thinks is acceptable, to dark orange and red fora sentence which may have a few problems.", "labels": [], "entities": []}, {"text": "shows how the SAT system evaluates and colour-codes a few authentic student-written sentences containing errors, as well as their corrected counterparts based on the error-coding in the FCE test set.", "labels": [], "entities": [{"text": "FCE test set", "start_pos": 186, "end_pos": 198, "type": "DATASET", "confidence": 0.9672200282414755}]}, {"text": "Overall, the system correctly identifies correct and incorrect versions of each sentence, attributing a higher score (greener colour) to the corrected sentence in each pair.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the FCE test set for the script-level  model (a) and our model (b).", "labels": [], "entities": [{"text": "FCE test set", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9330983360608419}]}, {"text": " Table 2: Number of revisions per task response.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9727492332458496}]}, {"text": " Table 3: Average feedback scores on a scale from 1 (strongly disagree) to 5 (strongly agree).", "labels": [], "entities": []}, {"text": " Table 4: Number of words per submission.", "labels": [], "entities": []}]}