{"title": [{"text": "A Virtual Manipulative for Learning Log-Linear Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an open-source virtual ma-nipulative for conditional log-linear models.", "labels": [], "entities": []}, {"text": "This web-based interactive visual-ization lets the user tune the probabilities of various shapes-which grow and shrink accordingly-by dragging sliders that correspond to feature weights.", "labels": [], "entities": []}, {"text": "The visualization displays a regularized training objective; it supports gradient ascent by optionally displaying gradients on the sliders and providing \"Step\" and \"Solve\" buttons.", "labels": [], "entities": [{"text": "gradient ascent", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.9078783988952637}]}, {"text": "The user can sample parameters and datasets of different sizes and compare their own parameters to the truth.", "labels": [], "entities": []}, {"text": "Our web-site, http://cs.jhu.edu/ \u02dc jason/ tutorials/loglin/, guides the user through a series of interactive lessons and provides auxiliary readings, explanations, practice problems and resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "We argue that if one is going to teach only a single machine learning technique in a computational linguistics course, it should be conditional loglinear modeling.", "labels": [], "entities": [{"text": "conditional loglinear modeling", "start_pos": 132, "end_pos": 162, "type": "TASK", "confidence": 0.6204537351926168}]}, {"text": "Such models are pervasive in natural language processing.", "labels": [], "entities": []}, {"text": "They have the form where f extracts a feature vector from context x and outcome y \u2208 Y(x).", "labels": [], "entities": []}, {"text": "The set of possible outcomes Y(x) might depend on the context x.", "labels": [], "entities": []}, {"text": "We then present an interactive web visualization that guides students through playing with loglinear models and their estimation.", "labels": [], "entities": []}, {"text": "This opensource tool, available at http://cs.jhu.", "labels": [], "entities": []}, {"text": "edu/ \u02dc jason/tutorials/loglin/, is intended to develop intuitions, so that basic loglinear models can be then taken for granted in future lectures.", "labels": [], "entities": []}, {"text": "It can be used near the start of a course, perhaps after introducing probability notation and n-gram models.", "labels": [], "entities": []}, {"text": "We used the tool in our Natural Language Processing (NLP) class and received very positive feedback.", "labels": [], "entities": []}, {"text": "Students were excited by it, with some saying the tool helped develop their \"physical intuition\" for log-linear models.", "labels": [], "entities": []}, {"text": "Other test users with no technical background also enjoyed working through the introductory lessons and found that they began to understand the model.", "labels": [], "entities": []}, {"text": "The app includes 18 ready-to-use lessons for individual or small-group study or classroom use.", "labels": [], "entities": []}, {"text": "Each lesson, e.g., guides the student to fit a probability model p \u03b8 (y | x) over some collection Y of shapes, words, or other images such as parse trees.", "labels": [], "entities": []}, {"text": "Each lesson is peppered with questions; students can be asked to answer some of these questions in writing.", "labels": [], "entities": []}, {"text": "Ambitious instructors can add new lessons or edit existing ones by writing configuration files (see section 5.3).", "labels": [], "entities": []}, {"text": "This is useful for emphasizing specific concepts or applications.", "labels": [], "entities": []}, {"text": "Section 8 provides some history and applications of log-linear modeling, as well as assignment ideas.", "labels": [], "entities": []}, {"text": "2 Why Teach With Log-Linear Models?", "labels": [], "entities": []}, {"text": "Log-linear models are very handy in NLP.", "labels": [], "entities": []}, {"text": "They can be used throughout a course, when one needs \u2022 a global classifier for an applied task, such as detecting sentiment, topic, spam, or gender; \u2022 a local classifier for structure annotation, such as tags or segment boundaries; \u2022 a local classifier to be applied repeatedly in sequential decision-making; \u2022 a local conditional probability within some generative process, such as an n-gram model, HMM, PCFG, probabilistic FSA or FST, noisy-channel MT model, or Bayes net; \u2022 a global structured prediction method.", "labels": [], "entities": [{"text": "detecting sentiment, topic, spam, or gender", "start_pos": 104, "end_pos": 147, "type": "TASK", "confidence": 0.8449487951066759}]}, {"text": "Here y is a complete structured object such as a tagging, segmentation, parse, alignment, or translation.", "labels": [], "entities": []}, {"text": "Then p(y | x) is a Markov random field or a conditional random field, depending on whether x is empty or not.", "labels": [], "entities": []}, {"text": "Log-linear models over discrete variables are also sufficiently expressive for an NLP course.", "labels": [], "entities": []}, {"text": "Students may experiment freely with adding their own creative model features that refer to salient attributes or properties of the data, since the probability (1) may consider any number of informative features of the (x, y) pair.", "labels": [], "entities": []}, {"text": "Estimation of the parameter weights \u03b8 from a set of fully observed (x, y) pairs is simply a convex optimization problem.", "labels": [], "entities": []}, {"text": "Maximizing the regularized conditional loglikelihood is a simple, uniform training principle that can be used throughout the course.", "labels": [], "entities": []}, {"text": "The scaled regularizer C \u00b7 R( \u03b8) prevents overfitting on sparse features.", "labels": [], "entities": []}, {"text": "This is arguably more straightforward than the traditional NLP smoothing methods for estimating probabilities from sparse data, which require applying various ad hoc formulas to counts, and which do not generalize well to settings where there is not a natural sequence of backoff models.", "labels": [], "entities": []}, {"text": "There exist fast and usable tools that students can use to train their log-linear models, including, among others, MegaM), and NLTK (.", "labels": [], "entities": [{"text": "MegaM", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.7937781810760498}]}, {"text": "Formally, log-linear models area good gateway to a more general understanding of undirected graphical models and the exponential family, including globally normalized joint or conditional distributions over trees and sequences.", "labels": [], "entities": []}, {"text": "One reason that log-linear models are both versatile and pedagogically useful is that they do not just make predictions, but explicitly model probabilities.", "labels": [], "entities": []}, {"text": "These can be \u2022 combined with other probabilities using the usual rules of probability; \u2022 marginalized attest time to obtain the probability that the outcome y has a particular property (e.g., one can sum over alignments); \u2022 marginalized at training time in the case of incomplete data y (e.g., the training data may not include alignments); \u2022 used to choose among possible decisions by computing their expected loss (risk).", "labels": [], "entities": []}, {"text": "The training procedure also takes a probabilistic view.", "labels": [], "entities": []}, {"text": "Equation (2) helps illustrate important statistical principles such as maximum likelihood, 4 regularization (the bias-variance tradeoff), and cross-validation, as well as optimization principles such as gradient ascent.", "labels": [], "entities": []}, {"text": "Log-linear models also provide natural extensions of commonly taught NLP methods.", "labels": [], "entities": []}, {"text": "For example, under a probabilistic context-free grammar (PCFG), 5 p(parse tree | sentence) is proportional to a product of rule probabilities.", "labels": [], "entities": []}, {"text": "Simply replacing each rule probability with an arbitrary non-negative potential-an exponentiated weight, or sum of weights of features of that rule-gives an instance of (1).", "labels": [], "entities": []}, {"text": "The same parsing algorithms still apply without modification, as does the same inside-outside approach to computing the posterior expectation of rule counts and feature counts.", "labels": [], "entities": []}, {"text": "Immediate variants include CRF, in which the rule features become position-dependent and sentence-dependent, and log-linear PCFGs, in which the feature-rich rule potentials are locally renormalized into rule probabilities via (1).", "labels": [], "entities": []}, {"text": "For all these reasons, we recommend log-linear models as one's \"go-to\" machine learning technique when teaching.", "labels": [], "entities": []}, {"text": "Other linear classifiers, such as perceptrons and SVMs, similarly choose y given x based on a linear score f \u00b7 \u03b8(x, y)-but these scores have no probabilistic interpretation, and the procedures for training \u03b8 are harder to understand or to justify.", "labels": [], "entities": []}, {"text": "Thus, they can be taught as variants later on or in another course.", "labels": [], "entities": []}, {"text": "Further reading includes (Smith, 2011).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}