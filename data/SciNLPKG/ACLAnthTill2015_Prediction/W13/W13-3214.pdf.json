{"title": [{"text": "Recurrent Convolutional Neural Networks for Discourse Compositionality", "labels": [], "entities": [{"text": "Recurrent Convolutional Neural Networks", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8370743691921234}, {"text": "Discourse Compositionality", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7430187463760376}]}], "abstractContent": [{"text": "The compositionality of meaning extends beyond the single sentence.", "labels": [], "entities": []}, {"text": "Just as words combine to form the meaning of sentences , so do sentences combine to form the meaning of paragraphs, dialogues and general discourse.", "labels": [], "entities": []}, {"text": "We introduce both a sentence model and a discourse model corresponding to the two levels of composi-tionality.", "labels": [], "entities": []}, {"text": "The sentence model adopts con-volution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network.", "labels": [], "entities": []}, {"text": "The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker.", "labels": [], "entities": []}, {"text": "The discourse model is able to capture both the sequen-tiality of sentences and the interaction between different speakers.", "labels": [], "entities": []}, {"text": "Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment.", "labels": [], "entities": [{"text": "dialogue act classification", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.6614979108174642}]}], "introductionContent": [{"text": "There are at least two levels at which the meaning of smaller linguistic units is composed to form the meaning of larger linguistic units.", "labels": [], "entities": []}, {"text": "The first level is that of sentential compositionality, where the meaning of words composes to form the meaning of the sentence or utterance that contains them.", "labels": [], "entities": [{"text": "sentential compositionality", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.7885274291038513}]}, {"text": "The second level extends beyond the first and involves general discourse compositionality, where the meaning of multiple sentences or utterances composes to form the meaning of the paragraph, document or dialogue that comprises them ().", "labels": [], "entities": [{"text": "discourse compositionality", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.8125228583812714}]}, {"text": "The problem of discourse compositionality is the problem of modelling how the meaning of general discourse composes from the meaning of the sentences involved and, since the latter in turn stems from the meaning of the words, how the meaning of discourse composes from the words themselves.", "labels": [], "entities": [{"text": "discourse compositionality", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7595891952514648}]}, {"text": "Tackling the problem of discourse compositionality promises to be central to a number of different applications.", "labels": [], "entities": [{"text": "discourse compositionality", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7163611501455307}]}, {"text": "These include sentiment or topic classification of single sentences within the context of a longer discourse, the recognition of dialogue acts within a conversation, the classification of a discourse as a whole and the attainment of general unsupervised or semi-supervised representations of a discourse for potential use in dialogue tracking and question answering systems and machine translation, among others.", "labels": [], "entities": [{"text": "sentiment or topic classification of single sentences within the context of a longer discourse", "start_pos": 14, "end_pos": 108, "type": "TASK", "confidence": 0.775480044739587}, {"text": "dialogue tracking", "start_pos": 325, "end_pos": 342, "type": "TASK", "confidence": 0.7272774875164032}, {"text": "question answering", "start_pos": 347, "end_pos": 365, "type": "TASK", "confidence": 0.8218270838260651}, {"text": "machine translation", "start_pos": 378, "end_pos": 397, "type": "TASK", "confidence": 0.7353183329105377}]}, {"text": "To this end much work has been done on modelling the meaning of single words byway of semantic vectors and the latter have found applicability in areas such as information retrieval).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 160, "end_pos": 181, "type": "TASK", "confidence": 0.8225978016853333}]}, {"text": "With regard to modelling the meaning of sentences and sentential compositionality, recent proposals have included simple additive and multiplicative models that do not take into account sentential features such as word order or syntactic structure, matrix-vector based models that do take into account such features but are limited to phrases of a specific syntactic type and structured models that fully capture such features) and are embedded within a deep neural architecture.", "labels": [], "entities": [{"text": "sentential compositionality", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.7692561149597168}]}, {"text": "It is notable that the additive and multiplicative models as well as simple, noncompositional bag of n-grams and word vector averaging models have equalled or outperformed the structured models at certain phrase similarity and sentiment classifica-tion tasks).", "labels": [], "entities": [{"text": "word vector averaging", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.6435428857803345}]}, {"text": "With regard to discourse compositionality, most of the proposals aimed at capturing semantic aspects of paragraphs or longer texts have focused on bag of n-grams or sentence vector averaging approaches ().", "labels": [], "entities": [{"text": "discourse compositionality", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7255177795886993}, {"text": "sentence vector averaging", "start_pos": 165, "end_pos": 190, "type": "TASK", "confidence": 0.6776860555013021}]}, {"text": "In addition, the recognition of dialogue acts within dialogues has largely been treated in non-compositional ways byway of language models coupled to hidden Markov sequence models ().", "labels": [], "entities": [{"text": "recognition of dialogue acts within dialogues", "start_pos": 17, "end_pos": 62, "type": "TASK", "confidence": 0.8188993235429128}]}, {"text": "Principled approaches to discourse compositionality have largely been unexplored.", "labels": [], "entities": [{"text": "discourse compositionality", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.73509681224823}]}, {"text": "We introduce a novel model for sentential compositionality.", "labels": [], "entities": [{"text": "sentential compositionality", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.8650862574577332}]}, {"text": "The composition operation is based on a hierarchy of one dimensional convolutions.", "labels": [], "entities": []}, {"text": "The convolutions are applied feature-wise, that is they are applied across each feature of the word vectors in the sentence.", "labels": [], "entities": []}, {"text": "The weights adopted in each convolution are different for each feature, but do not depend on the different words being composed.", "labels": [], "entities": []}, {"text": "The hierarchy of convolution operations involves a sequence of convolution kernels of increasing sizes.", "labels": [], "entities": []}, {"text": "This allows for the composition operation to be applied to sentences of any length, while keeping the model at a depth of roughly \u221a 2l where l is the length of the sentence.", "labels": [], "entities": []}, {"text": "The hierarchy of feature-wise convolution operations followed by sigmoid non-linear activation functions results in a hierarchical convolutional neural network (HCNN) based on a convolutional architecture ().", "labels": [], "entities": []}, {"text": "The HCNN shares with the structured models the aspect that it is sensitive to word order and adopts a hierarchical architecture, although it is not based on explicit syntactic structure.", "labels": [], "entities": [{"text": "HCNN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9371297359466553}]}, {"text": "We also introduce a novel model for discourse compositionality.", "labels": [], "entities": [{"text": "discourse compositionality", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.7236223518848419}]}, {"text": "The discourse model is based on a recurrent neural network (RNN) architecture that is a powerful model for sequences.", "labels": [], "entities": []}, {"text": "The model aims at capturing two central aspects of discourse and its meaning: the sequentiality of the sentences or utterances in the discourse and, where applicable, the interactions between the different speakers.", "labels": [], "entities": []}, {"text": "The underlying RNN has its recurrent and output weights conditioned on the respective speaker, while simultaneously taking as input at every turn the sentence vector for the current sentence generated through the sentence model.", "labels": [], "entities": []}, {"text": "The compositionality of meaning extends beyond the single sentence.", "labels": [], "entities": []}, {"text": "Just as words combine to form the meaning of an sentence, so do sentences in turn combine sequentially to form the meaning of general discourse.", "labels": [], "entities": []}, {"text": "Discourse may take the form of paragraphs, soliloqui or conversations between multiple speakers.", "labels": [], "entities": []}, {"text": "The problem of cross-sentential compositionality is the problem of modelling how the meaning of the various forms of discourse arises from the meaning of the utterances and the words involved.", "labels": [], "entities": [{"text": "cross-sentential compositionality", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.7727832198143005}]}, {"text": "We here introduce 6 General Instructions Manuscripts must be in two-column format.", "labels": [], "entities": []}, {"text": "Exceptions to the two-column format include the title, authors' names and complete addresses, which must be centered at the top of the first page, and any full-width figures or tables (see the guidelines in Subsection 6.5", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Most frequent dialogue act labels with examples and frequencies in train and test data.", "labels": [], "entities": []}, {"text": " Table 3: SwDA dialogue act tagging accuracies.  The LM-HMM results are from (", "labels": [], "entities": [{"text": "SwDA dialogue act tagging", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.9056490361690521}]}]}