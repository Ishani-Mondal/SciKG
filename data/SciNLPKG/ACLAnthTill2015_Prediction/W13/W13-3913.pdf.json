{"title": [{"text": "A Self Learning Vocal Interface for Speech-impaired Users", "labels": [], "entities": [{"text": "Self Learning Vocal Interface", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.6279999911785126}]}], "abstractContent": [{"text": "In this work we describe research aimed at developing an assis-tive vocal interface for users with a speech impairment.", "labels": [], "entities": []}, {"text": "In contrast to existing approaches, the vocal interface is self-learning, which means it is maximally adapted to the end-user and can be used with any language, dialect, vocabulary and grammar.", "labels": [], "entities": []}, {"text": "The paper describes the overall learning framework and the vocabulary acquisition technique, and proposes a novel grammar induction technique based on weakly supervised hidden Markov model learning.", "labels": [], "entities": [{"text": "vocabulary acquisition", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.888479083776474}, {"text": "grammar induction", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.6923332363367081}]}, {"text": "We evaluate early implementations of these vocabulary and grammar learning components on two datasets: recorded sessions of a vocally guided card game by non-impaired speakers and speech-impaired users engaging in a home automation task.", "labels": [], "entities": []}], "introductionContent": [{"text": "These days, vocal user interfaces (VUIs) allow us to control computers, smart phones, car navigation systems and domestic devices by voice.", "labels": [], "entities": []}, {"text": "While still generally perceived as a luxury, assistive technology employing a VUI can make a prominent difference in the lives of individuals with a physical disability for whom operating and controlling devices would require exhaustive physical effort.", "labels": [], "entities": []}, {"text": "Unfortunately, even state-of-the-art speech recognition systems offer little, if any, robustness to dialectic or dysarthric speech (often encountered with disabled users), and are often restricted in their vocabulary and grammar.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.766872763633728}]}, {"text": "In practice, it is not feasible to design speech interfaces featuring custom acoustic and language models that cater to the dialectic and/or pathological speech of individual users, and adaptation of existing acoustic models is limited to only very mild speech pathologies.", "labels": [], "entities": []}, {"text": "Moreover, the user's voice may changeover time due to progressive speech impairments.", "labels": [], "entities": []}, {"text": "Our aim is to build a VUI that is trained by the end-user himself, which means that it is maximally adapted to thepossibly dysarthric -speech of the user, and can be used with any vocabulary and grammar.", "labels": [], "entities": []}, {"text": "The challenge is to learn both acoustics and grammar from a small number of examples, with as only supervisory information coarse annotation in the form of associated actions.", "labels": [], "entities": []}, {"text": "For example, the annotation of the command \"Turn on the television please\", accompanied by a button press, would only be annotated at the utterance level with a device label (television) and an action label (turn on).", "labels": [], "entities": []}, {"text": "Our learning approach consists of two components that interact.", "labels": [], "entities": []}, {"text": "Vocabulary acquisition first builds recurrent acoustic patterns representing words or parts of spoken commands, while grammar induction attempts to model the relationships between these patterns.", "labels": [], "entities": [{"text": "Vocabulary acquisition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9166576564311981}, {"text": "grammar induction", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7148814797401428}]}, {"text": "For vocabulary acquisition, we build on existing work on child language learning modeling with non-negative matrix factorisation (NMF).", "labels": [], "entities": [{"text": "vocabulary acquisition", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8980061709880829}]}, {"text": "For grammar induction, we propose the use of a weakly supervised Hidden Markov Model (HMM).", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8977530300617218}]}, {"text": "In short, we first use NMF to find recurrent acoustic patterns by mining utterance-level acoustic representations, supervised with relevant information about the action that was performed, such as a 'television' device and a 'turn on' action.", "labels": [], "entities": []}, {"text": "Building on these, we then use the temporal occurrence of these patterns in the training data as observation features to train a multi-label version of a discrete HMM.", "labels": [], "entities": []}, {"text": "In the HMM, the hidden states represent the collection of possible values in the data structures (devices and actions in the example).", "labels": [], "entities": []}, {"text": "By mining the temporal occurrence of the NMF-based observations and the commonalities and differences across commands, the HMM is able to discover temporal structure in the commands, related to the data structures representing the actions.", "labels": [], "entities": []}, {"text": "The goals of our work are similar to those of in that we aim to discover acoustic patterns that recur in utterances and ground these by linking them to other modalities.", "labels": [], "entities": []}, {"text": "However, to accommodate pathological voices, our work does not rely on pre-trained models, but they are learned from the speakerspecific acoustic data.", "labels": [], "entities": []}, {"text": "In that sense, it shows similarities to the work in, but we learn form continuous speech and do not model low-level acoustics with an HMM.", "labels": [], "entities": []}, {"text": "In terms of grammar learning, our task approaches unsupervised grammar induction, but on a restricted domain with a small vocabulary.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.780076414346695}]}, {"text": "We evaluate our learning framework on two databases: PAT-COR, recorded sessions of a vocally guided card game by nonimpaired speakers, and DOMOTICA-2, speech-impaired users engaging in a home automation task.", "labels": [], "entities": [{"text": "PAT-COR", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.5762995481491089}]}, {"text": "The users were free to choose their own words and grammatical constructs to address the systems during the recording sessions.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "In section 2, we present an overview of the learning framework, describe the acoustic representations and introduce the NMF and HMM learning approaches.", "labels": [], "entities": []}, {"text": "In section 3, the experimental setup is explained and in sections 4 and 5 the experimental results are presented and discussed.", "labels": [], "entities": []}, {"text": "We conclude with our conclusions and thoughts for future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we give a description of the databases used for evaluation, the evaluation procedure and metrics.", "labels": [], "entities": []}, {"text": "For each utterance in the databases, we have a manually constructed gold standard frame description, which is used as a reference for system evaluation.", "labels": [], "entities": []}, {"text": "In this reference frame description, the slot values that are expressed in the utterance, are filled in.", "labels": [], "entities": []}, {"text": "The system was evaluated by comparing the automatically induced frame descriptions to the gold standard reference frames.", "labels": [], "entities": []}, {"text": "The used metric is the slot F \u03b2=1 -score, which is the harmonic mean of the slot precision and the slot recall.", "labels": [], "entities": [{"text": "slot F \u03b2=1 -score", "start_pos": 23, "end_pos": 40, "type": "METRIC", "confidence": 0.8581477318491254}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.7844984531402588}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9575417041778564}]}, {"text": "These metrics are commonly used for the evaluation of frame-based systems for spoken language understanding.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.7107831835746765}]}, {"text": "The following formulas were used for calculation: slot precision = # correctly filled slots # total filled slots in induced frame slot recall = # correctly filled slots # total filled slots in reference frame slot F \u03b2=1 -score = 2 \u00b7 slot precision \u00b7 slot recall slot precision + slot recall This means that only slots that are filled with a correct value are rewarded, and both slots that are falsely filled and slots that are falsely left empty are penalised.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.611504077911377}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.88690185546875}, {"text": "F \u03b2=1 -score", "start_pos": 214, "end_pos": 226, "type": "METRIC", "confidence": 0.6993210514386495}]}, {"text": "When an induced frame is of another type than the corresponding reference frame, the filled slots in the induced frame and in the reference frame are consequently different, which automatically results in a relatively large drop in the slot F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 241, "end_pos": 248, "type": "METRIC", "confidence": 0.9401261210441589}]}, {"text": "It should be noted that the reported F-scores aggregate slot counts overall five folds.", "labels": [], "entities": [{"text": "F-scores aggregate slot", "start_pos": 37, "end_pos": 60, "type": "METRIC", "confidence": 0.9534838795661926}]}], "tableCaptions": [{"text": " Table 1: parameters of the speech databases", "labels": [], "entities": []}, {"text": " Table 2: F-scores for NMF word learning for all speakers of DOMOTICA-2 and all training set sizes", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9972522854804993}, {"text": "NMF word learning", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7773549358050028}, {"text": "DOMOTICA-2", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.8659334182739258}]}, {"text": " Table 3: F-scores for HMM grammar induction for all speakers of DOMOTICA-2 and all training set sizes", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963943362236023}, {"text": "HMM grammar induction", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.9364848931630453}]}]}