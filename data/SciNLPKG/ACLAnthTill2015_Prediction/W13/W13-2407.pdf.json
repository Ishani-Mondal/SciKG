{"title": [{"text": "GPKEX: Genetically Programmed Keyphrase Extraction from Croatian Texts", "labels": [], "entities": [{"text": "GPKEX", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8901553750038147}, {"text": "Genetically Programmed Keyphrase Extraction from Croatian Texts", "start_pos": 7, "end_pos": 70, "type": "TASK", "confidence": 0.7138290234974453}]}], "abstractContent": [{"text": "We describe GPKEX, a keyphrase extraction method based on genetic programming.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7529995441436768}]}, {"text": "We represent keyphrase scoring measures as syntax trees and evolve them to produce rankings for keyphrase candidates extracted from text.", "labels": [], "entities": [{"text": "keyphrase scoring", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7766759693622589}]}, {"text": "We apply and evaluate GPKEX on Croatian newspaper articles.", "labels": [], "entities": [{"text": "GPKEX", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.819155752658844}, {"text": "Croatian newspaper articles", "start_pos": 31, "end_pos": 58, "type": "DATASET", "confidence": 0.8370068073272705}]}, {"text": "We show that GPKEX can evolve simple and interpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for Croatian.", "labels": [], "entities": [{"text": "GPKEX", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.9158222079277039}]}], "introductionContent": [{"text": "Keyphrases are an effective way of summarizing document contents, useful for text categorization, document management, and search.", "labels": [], "entities": [{"text": "summarizing document contents", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.9077199101448059}, {"text": "text categorization", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7273154556751251}, {"text": "document management", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7726283669471741}]}, {"text": "Unlike keyphrase assignment, in which documents are assigned keyphrases from a predefined taxonomy, keyphrase extraction selects phrases from the text of the document.", "labels": [], "entities": [{"text": "keyphrase assignment", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8059283792972565}, {"text": "keyphrase extraction selects phrases from the text of the document", "start_pos": 100, "end_pos": 166, "type": "TASK", "confidence": 0.7990819036960601}]}, {"text": "Extraction is preferred in cases when a taxonomy is not available or when its construction is not feasible, e.g., if the set of possible keyphrases is too large or changes often.", "labels": [], "entities": []}, {"text": "Manual keyphrase extraction is extremely tedious and inconsistent, thus methods for automatic keyphrase extraction have attracted a lot of research interest.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.7421835660934448}, {"text": "keyphrase extraction", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.6855030655860901}]}, {"text": "In this paper we describe GPKEX, a keyphrase extraction method based on genetic programming (GP), an evolutionary optimization technique inspired by biological evolution (.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7177747339010239}]}, {"text": "GP is similar to genetic algorithms except that the individual solutions are expressions, rather than values.", "labels": [], "entities": []}, {"text": "We use GP to evolve keyphrase scoring measures, represented as abstract syntax trees.", "labels": [], "entities": []}, {"text": "The advantage of using GP over black-box machine learning methods is in the interpretability of the results: GP yields interpretable expressions, revealing the relevant features and their relationships, thus offering some insight into keyphrase usage.", "labels": [], "entities": []}, {"text": "Furthermore, GP can evolve simple scoring measures, providing an efficient alternative to more complex machine learning methods.", "labels": [], "entities": []}, {"text": "We apply GPKEX to Croatian language and evaluate it on a dataset of newspaper articles with manually extracted keyphrases.", "labels": [], "entities": []}, {"text": "Our results show that GPKEX performs comparable to previous supervised and unsupervised approaches for Croatian, but has the advantage of generating simple and interpretable keyphrase scoring measures.", "labels": [], "entities": [{"text": "GPKEX", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.7629903554916382}]}], "datasetContent": [{"text": "We use the dataset developed by Miji\u00b4c, comprising 1020 Croatian newspaper articles provided by the Croatian News Agency.", "labels": [], "entities": [{"text": "Croatian News Agency", "start_pos": 100, "end_pos": 120, "type": "DATASET", "confidence": 0.84102863073349}]}, {"text": "The articles have been manually annotated by expert annotators, i.e., each document has an associated list of keyphrases.", "labels": [], "entities": []}, {"text": "The number of extracted keyphrases per document varies between 1 and 7 (3.4 on average).", "labels": [], "entities": []}, {"text": "The dataset is divided in two parts: 960 documents each annotated by a single annotator and 60 documents independently annotated by eight annotators.", "labels": [], "entities": []}, {"text": "We use the first part for training and the second part for testing.", "labels": [], "entities": []}, {"text": "Based on dataset analysis, we chose the following POS patterns for keyphrase candidate filtering: N, AN, NN, NSN, V, U (N -noun, A -adjective, S -preposition, V -verb, U -unknown).", "labels": [], "entities": [{"text": "keyphrase candidate filtering", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.6377831598122915}]}, {"text": "Although a total of over 200 patterns would be needed to coverall keyphrases from the training set, we use only the six most frequent ones in order to reduce the number of candidates.", "labels": [], "entities": []}, {"text": "These patterns account for cca.", "labels": [], "entities": []}, {"text": "70% of keyphrases, while reducing the number of candidates by cca.", "labels": [], "entities": []}, {"text": "Note that we chose to only extract keyphrases of three words or less, thereby covering 93% of keyphrases.", "labels": [], "entities": []}, {"text": "For lemmatization and (ambiguous) POS tagging, we use the inflectional lexicon from\u0160najderfrom\u02c7from\u0160najder et al., with additional suffix removal after lemmatization.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.7615343332290649}]}, {"text": "Keyphrase extraction is a highly subjective task and there is no agreedupon evaluation methodology.", "labels": [], "entities": [{"text": "Keyphrase extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.921824187040329}]}, {"text": "Annotators are often inconsistent: they extract different keyphrases and also keyphrases of varying length.", "labels": [], "entities": []}, {"text": "What is more, an omission of a keyphrase by one of the annotators does not necessarily mean that the keyphrase is incorrect; it may merely indicate that it is less relevant.", "labels": [], "entities": []}, {"text": "To account for this, we use rank-based evaluation measures.", "labels": [], "entities": []}, {"text": "As our method produces a ranked list of keyphrases for each document, we can compare this list against a gold-standard keyphrase ranking for each document.", "labels": [], "entities": []}, {"text": "We obtain the latter by aggregating the judgments of all annotators; the more annotators have extracted a keyphrase, the higher its ranking will be.", "labels": [], "entities": []}, {"text": "Following, we consider the morphological variants when matching the keyphrases; however, we do not consider partial matches.", "labels": [], "entities": []}, {"text": "To evaluate a ranked list of extracted keyphrases, we use the generalized average precision (GAP) measure proposed by.", "labels": [], "entities": [{"text": "generalized average precision (GAP) measure", "start_pos": 62, "end_pos": 105, "type": "METRIC", "confidence": 0.8234338760375977}]}, {"text": "GAP generalizes average precision to multi-grade relevance judgments: it takes into account both precision (all correct items are ranked before all incorrect ones) and the quality of ranking (more relevant items are ranked before less relevant ones).", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.8395248651504517}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9993407130241394}]}, {"text": "Another way of evaluating against keyphrases extracted by multiple annotators is to consider the different levels of agreement.", "labels": [], "entities": []}, {"text": "We consider as strong agreement the cases in which a keyphrase is extracted by at least five annotators, and as weak agreement the cases in which at least two annotators have extracted a keyphrase.", "labels": [], "entities": []}, {"text": "For both agreement levels separately, we compare the extracted keyphrases against the manually extracted keyphrases using rank-based IR measures of Precision at Rank 10 (P@10) and Recall at Rank 10 (R@10).", "labels": [], "entities": [{"text": "Precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9959945678710938}, {"text": "Recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9963589310646057}]}, {"text": "Because GP is a stochastic algorithm, to account for randomness we made 30 runs of each experiment and report the average scores.", "labels": [], "entities": []}, {"text": "On these samples, we use the unpaired t-test to determine the significance in performance differences.", "labels": [], "entities": []}, {"text": "As baseline to compare against GPKEX, we use keyphrase extraction based on tf-idf scores (with the same preprocessing and filtering setup as for GPKEX).", "labels": [], "entities": [{"text": "GPKEX", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9341587424278259}, {"text": "keyphrase extraction", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7292210161685944}]}, {"text": "We tested four evolution configurations.", "labels": [], "entities": []}, {"text": "Configuration A uses the parameter setting described in Section 3.2, but without parsimony pressure.", "labels": [], "entities": []}, {"text": "Configurations B and C use parsimony pressure defined by (2), with \u03b1 = 1000 Results.", "labels": [], "entities": [{"text": "parsimony pressure", "start_pos": 27, "end_pos": 45, "type": "METRIC", "confidence": 0.9655744731426239}]}, {"text": "Configurations A and B perform similarly across all evaluation measures (pairwise differences are not significant at p<0.05, except for R@10) and outperform the baseline (differences are significant at p<0.01).", "labels": [], "entities": []}, {"text": "Configuration C is outperformed by configuration A (differences are significant at p<0.05).", "labels": [], "entities": [{"text": "Configuration", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9344874620437622}]}, {"text": "Configuration D outperforms the baseline, but is outperformed by other configurations (pairwise differences in GAP are significant at p<0.05), indicating that conservative POS filtering is beneficial.", "labels": [], "entities": [{"text": "POS filtering", "start_pos": 172, "end_pos": 185, "type": "TASK", "confidence": 0.8288105130195618}]}, {"text": "Since A and B perform similar, we conclude that applying parsimony pressure in our case only marginally improved GAP (although it has reduced KSM size from an average 30 nodes for configuration A to an average of 20 and 9 nodes for configurations B and C, respectively).", "labels": [], "entities": [{"text": "GAP", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.7513708472251892}]}, {"text": "We believe there are two reasons for this: first, the increase in KSM complexity also increases the probability that the KSM will be discarded as not computable (e.g., the right subtree of a '/' node evaluates to zero).", "labels": [], "entities": []}, {"text": "Secondly, our fitness function is perhaps not fine-grained enough to allow more complex KSMs to emerge gradually, as small changes in keyphrase scores do not immediately affect the value of the fitness function.", "labels": [], "entities": []}, {"text": "In absolute terms, GAP values are rather low.", "labels": [], "entities": [{"text": "GAP", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.8553059101104736}]}, {"text": "This is mostly due to wrong ranking, rather than the omission of correct phrases.", "labels": [], "entities": []}, {"text": "Furthermore, the precision for strong agreement is considerably lower than for weak agreement.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9997263550758362}]}, {"text": "This indicates that GP-KEX often assigns high scores to less relevant keyphrases.", "labels": [], "entities": []}, {"text": "Both deficiencies maybe attributed to the fact that we do not learn to rank, but train on dataset with binary relevance judgments.", "labels": [], "entities": []}, {"text": "The best-performing KSM from configuration A is shown in.", "labels": [], "entities": [{"text": "KSM", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9053200483322144}]}, {"text": "Length is the length of the phrase, First is the position of the first occurrence, and Rare is the number of discriminative words in a phrase (cf. Section 3.1).", "labels": [], "entities": [{"text": "Length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9731578230857849}, {"text": "Rare", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9967836141586304}]}, {"text": "Tfidf, First, and Rare features seem to be positively correlated with keyphraseness.", "labels": [], "entities": [{"text": "Tfidf", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9786251187324524}, {"text": "First", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9283257722854614}]}, {"text": "This particular KSM extracts on average three correct keyphrases (weak agreement) within the first 10 results.", "labels": [], "entities": [{"text": "KSM", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.947393000125885}]}, {"text": "Our results are not directly comparable to previous work for Croatian ( because we use a different dataset and/or evaluation methodology.", "labels": [], "entities": []}, {"text": "However, to allow for an indirect comparison, we re-evaluated the results of unsupervised keyphrase extraction (UKE) from Saratlija et al.; we show the result in the last row of.", "labels": [], "entities": [{"text": "keyphrase extraction (UKE)", "start_pos": 90, "end_pos": 116, "type": "METRIC", "confidence": 0.5814451575279236}]}, {"text": "GPKEX (configuration A) outperforms UKE in terms of precision (GAP and P@10), but performs worse in terms of recall.", "labels": [], "entities": [{"text": "GPKEX", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9416648745536804}, {"text": "UKE", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.5389171838760376}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9994539618492126}, {"text": "GAP", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.8753175735473633}, {"text": "P@10)", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9257174432277679}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9988247752189636}]}, {"text": "In terms of F1@10 (harmonic mean of P@10 and R@10), GPKEX performs better than UKE at the strong agreement level (12.9 vs. 9.9), but worse at the weak agreement level (13.0 vs. 15.6).", "labels": [], "entities": [{"text": "F1", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9992650151252747}, {"text": "GPKEX", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.848887026309967}, {"text": "UKE", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.6479455828666687}]}, {"text": "For comparison, report UKE to be comparable to supervised method from, but better than the tf-idf extraction method from Miji\u00b4c.", "labels": [], "entities": [{"text": "UKE", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9621871113777161}, {"text": "Miji\u00b4c", "start_pos": 121, "end_pos": 127, "type": "DATASET", "confidence": 0.951886773109436}]}], "tableCaptions": [{"text": " Table 1: Keyphrase ranking results.", "labels": [], "entities": []}]}