{"title": [{"text": "On Named Entity Recognition in Targeted Twitter Streams in Polish", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6394790609677633}]}], "abstractContent": [{"text": "This paper reports on some experiments aiming at tuning a rule-based NER system designed for detecting names in Polish online news to the processing of targeted Twitter streams.", "labels": [], "entities": [{"text": "detecting names in Polish online news", "start_pos": 93, "end_pos": 130, "type": "TASK", "confidence": 0.6928820908069611}]}, {"text": "In particular, one explores whether the performance of the baseline NER system can be improved through the incremental application of knowledge-poor methods for name matching and guessing.", "labels": [], "entities": [{"text": "name matching", "start_pos": 161, "end_pos": 174, "type": "TASK", "confidence": 0.8385233879089355}]}, {"text": "We study various settings and combinations of the methods and present evaluation results on five corpora gathered from Twitter, centred around major events and known individuals.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, Twitter emerged as an important social medium providing most up-to-date information and comments on current events of any kind.", "labels": [], "entities": []}, {"text": "This results in an ever-growing interest of various organizations in tools for real-time monitoring of Twitter streams to collect their businessspecific information therefrom for analysis purposes.", "labels": [], "entities": []}, {"text": "Since monitoring the entire Twitter stream appears to be unfeasible due to the high volume of published tweets, one usually monitors targeted Twitter streams, i.e., streams of tweets potentially satisfying specific information needs.", "labels": [], "entities": []}, {"text": "Applications for monitoring Twitter streams usually require named entity recognition (NER) capacity.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.8073365390300751}]}, {"text": "However, due to the nature of Twitter messages, i.e., being short, noisy, written in an informal style, lacking punctuation and capitalization, containing misspellings, non-standard abbreviations, and non grammatically correct sentences, the application of even basic NLP tools (trained on formal texts) on tweets usually results in poor performances.", "labels": [], "entities": []}, {"text": "In the case of well-formed texts such as online news, exploitation of contextual clues is crucial to named entity identification and classification (e.g.,Mayor of ' in the left context of a capitalized token is a reliable pattern to classify it as city name).", "labels": [], "entities": [{"text": "named entity identification", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.6391480167706808}]}, {"text": "Such external evidence is often missing in tweets, and entity names are frequently incomplete, abbreviated or glued with other words.", "labels": [], "entities": []}, {"text": "Furthermore, deployment of supervised ML-based techniques for NER from tweets is challenging due to the dynamic nature of Twitter.", "labels": [], "entities": [{"text": "NER from tweets", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.8958011865615845}]}, {"text": "In this paper, we report on experiments aiming at tuning a rule-based NER system, initially designed for detecting names in Polish online news, to the processing of targeted Twitter streams.", "labels": [], "entities": [{"text": "detecting names in Polish online news", "start_pos": 105, "end_pos": 142, "type": "TASK", "confidence": 0.7314158876736959}]}, {"text": "In particular, we explore whether the performance of the baseline NER system can be improved through the utilization of knowledge-poor methods (based on string distance metrics) for name matching and name guessing.", "labels": [], "entities": [{"text": "name matching", "start_pos": 182, "end_pos": 195, "type": "TASK", "confidence": 0.8401654362678528}, {"text": "name guessing", "start_pos": 200, "end_pos": 213, "type": "TASK", "confidence": 0.8116837739944458}]}, {"text": "In comparison to English, Polish is a free-word order and highly inflective language, with particularly complex declension paradigm of proper names, which makes NER for Polish a more difficult task.", "labels": [], "entities": [{"text": "NER", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9695234894752502}]}, {"text": "The remaining part of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "First, Section 2 provides information on related work.", "labels": [], "entities": []}, {"text": "Next, Section 3 describes the baseline NER system and the knowledge-poor enhancements.", "labels": [], "entities": [{"text": "NER", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9777640104293823}]}, {"text": "Subsequently, Section 4 presents the evaluation results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 gives a summary and an outlook as regards future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each tweet collection was extracted using simple queries, e.g., \"zamach AND (Boston OR Bostonie)\" (\"attack\" AND \"'Boston\"' either in nominative of locative form) for collecting tweets on the Boston bombings.", "labels": [], "entities": []}, {"text": "From each collection a subset of randomly chosen tweets was selected for evaluation purposes.", "labels": [], "entities": []}, {"text": "We will refer to the latter as the test corpus, whereas the entire tweet collections will be referred to as the stream corpus.", "labels": [], "entities": []}, {"text": "In the stream corpus, we computed for each tweet: (a) the text-like fraction of its body, i.e., the fraction of the body which contains text, and (b) the lexical validity, i.e., the percentage of tokens in the text-like part of the body of the tweet which are valid word forms in Polish . show the histograms for text-like fraction and lexical validity of the tweets in each collection in the stream corpus.", "labels": [], "entities": []}, {"text": "We can observe that large portion of the tweets contains significant text-like part, which is also lexically valid.", "labels": [], "entities": []}, {"text": "Interestingly, the random collection exhibits lower lexical validity, which is due to more colloquial language used in the tweets in this collection.", "labels": [], "entities": [{"text": "validity", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.8859267234802246}]}, {"text": "We built the test corpus by randomly selecting tweets whose text-like fraction of the body was \u2265 80%, additionally checking the language and removing duplicates.", "labels": [], "entities": []}, {"text": "These tweets were afterwards manually annotated with person, location and organization names, according to the following guidelines: consideration of unigram entities, non-inclusion of titles, functions and alike, non-inclusion of spurious punctuation marks and exclusion of names starting with '@', since their recognition as names is trivial.", "labels": [], "entities": []}, {"text": "The test corpus statistics are provided in Table 1.", "labels": [], "entities": []}, {"text": "We provide in brackets the number of tweets in the corresponding tweet collections in the entire stream corpus.", "labels": [], "entities": []}, {"text": "In this test corpus, 86,7% of the annotated names are word unigrams, whereas bigrams constitute 12,7% of the annotated names and 3-and 4-grams account only fora tiny fraction (0,6%); this is inline with the characteristics of the Twitter language, which favours quick and simple expressions.", "labels": [], "entities": []}, {"text": "For each collection, we computed the name diversity as the ratio between entity occurrences and unique entities, as well as the average number of entities per tweet . Targeted stream corpora show a medium name diversity (except for Boston and Gmyz collections, centred on a very specific location and person name resp.) and a high rate of entity per tweet (around 2.2), in contrast with random corpus which shows a high name diversity (0.79) fora low average number of entity per tweets.", "labels": [], "entities": [{"text": "Boston and Gmyz collections", "start_pos": 232, "end_pos": 259, "type": "DATASET", "confidence": 0.862907350063324}]}, {"text": "Reported to the limited number of characters in tweets (140), the important significant number of entity per tweet in targeted streams accounts, on the one hand, for the usefulness of working on targeted streams and, on the other, for the importance of NER in tweets.", "labels": [], "entities": []}, {"text": "In our experiments we evaluated the performance of (i) the NER grammar (BASE), a combination thereof with (ii) different name matching strategies (MATCH) and (iii) different variants of the name guesser and, finally, (iv) the combinations of all techniques.", "labels": [], "entities": [{"text": "BASE", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.6735414266586304}]}, {"text": "Within the MATCH configuration, we experimented all string distance metrics presented in 3.2 but since Jaro, Jaro-Winkler and SmithWaterman metrics performed on average worse than the others, we did not consider them in further experiments.", "labels": [], "entities": []}, {"text": "We selected the best performing metric, LCS 10 , as the one used by the name guesser (CLUSTERING) in subsequent experiments.", "labels": [], "entities": [{"text": "LCS 10", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.7944498658180237}, {"text": "CLUSTERING", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.7535308003425598}]}, {"text": "As a complement, we measured the performance of the name guesser alone to compare it with BASE.", "labels": [], "entities": [{"text": "name guesser", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.7078745365142822}, {"text": "BASE", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9644664525985718}]}, {"text": "Furthermore, name matching and name guessing algorithms were using the tweet collections in the stream corpus (as quasi 'Twitter stream window') in order to gather knowledge for matching/guessing 'new' names in the test corpus.", "labels": [], "entities": [{"text": "name matching", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8415500819683075}, {"text": "name guessing", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.7680453658103943}]}, {"text": "We measured the performance of the different configurations in terms of Precision (P), Recall (R) and F-measure (F), according to two different schemes: exact match, where entity types and both boundaries should match perfectly, and fuzzy match, which allows for one name boundary returned by the system to be different from the reference, i.e., either too short or too long on the left or on the right, but not on both.", "labels": [], "entities": [{"text": "Precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9908117055892944}, {"text": "Recall (R)", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9368866682052612}, {"text": "F-measure (F)", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.9511528760194778}, {"text": "exact match", "start_pos": 153, "end_pos": 164, "type": "METRIC", "confidence": 0.9580288231372833}]}, {"text": "Furthermore, since the clustering-based name guesser described in 3.3 does not classify names, for any settings with this technique we only evaluated name detection performance, i.e., no distinction between name types was made.", "labels": [], "entities": [{"text": "clustering-based name guesser", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6389723519484202}, {"text": "name detection", "start_pos": 150, "end_pos": 164, "type": "TASK", "confidence": 0.7008738964796066}]}, {"text": "The overall summary of the results for the entire pool of tweet collections, is presented in.", "labels": [], "entities": []}, {"text": "In the context of the CLUSTERING algorithm we explored various settings as regards the minimum frequency of an n-gram to be considered as cluster seed (\u03c6 parameter -see Section 3.3).", "labels": [], "entities": []}, {"text": "More precisely, we tested values in the range of 1 to 30 for all corpora and system settings which included CLUSTERING, and compared the resulting P/R and F figures.", "labels": [], "entities": [{"text": "CLUSTERING", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.820030927658081}, {"text": "P/R", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.8511380354563395}, {"text": "F", "start_pos": 155, "end_pos": 156, "type": "METRIC", "confidence": 0.6646431684494019}]}, {"text": "An example of a curve with P/R values (exact match) of BASE-CLUSTERING algorithm applied on the 'Boston' corpus with varying values of \u03c6 is given in.", "labels": [], "entities": [{"text": "exact match", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9321366548538208}, {"text": "BASE-CLUSTERING", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.9791208505630493}, {"text": "Boston' corpus", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9846343000729879}]}, {"text": "One can observe and hypothesize that the frequency threshold does not impact much the performance.", "labels": [], "entities": []}, {"text": "Suchlike curves for other settings were of a similar nature.", "labels": [], "entities": []}, {"text": "Therefore we decided to set the \u03c6 to 1 in all settings reported in.", "labels": [], "entities": [{"text": "\u03c6", "start_pos": 32, "end_pos": 33, "type": "METRIC", "confidence": 0.9921765923500061}]}], "tableCaptions": [{"text": " Table 1: Test corpus statistics.", "labels": [], "entities": []}, {"text": " Table 2: Precision/recall figures for person, or- ganization and location name recognition (exact  match) with BASE.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9903162121772766}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.8074981570243835}, {"text": "location name recognition", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.614399254322052}, {"text": "BASE", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9843341708183289}]}, {"text": " Table 3: Precision, Recall and F-measure figures for exact (top) and fuzzy match (bottom). The best  results are highlighted in bold.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9991819262504578}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9973588585853577}, {"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9985764026641846}, {"text": "exact", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.998050332069397}]}]}