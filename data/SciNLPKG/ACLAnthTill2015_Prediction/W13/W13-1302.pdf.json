{"title": [{"text": "Generating Natural-Language Video Descriptions Using Text-Mined Knowledge", "labels": [], "entities": [{"text": "Generating Natural-Language Video Descriptions", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6486973538994789}]}], "abstractContent": [{"text": "We present a holistic data-driven technique that generates natural-language descriptions for videos.", "labels": [], "entities": []}, {"text": "We combine the output of state-of-the-art object and activity detectors with \"real-world\" knowledge to select the most probable subject-verb-object triplet for describing a video.", "labels": [], "entities": []}, {"text": "We show that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification.", "labels": [], "entities": [{"text": "triplet selection", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7179957926273346}, {"text": "activity identification", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.6976219564676285}]}, {"text": "Unlike previous methods, our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus.", "labels": [], "entities": []}, {"text": "We evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61% of the time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Combining natural-language processing (NLP) with computer vision to to generate English descriptions of visual data is an important area of active research).", "labels": [], "entities": []}, {"text": "We present a novel approach to generating a simple sentence for describing a short video that: 1.", "labels": [], "entities": []}, {"text": "Identifies the most likely subject, verb and object (SVO) using a combination of visual object and activity detectors and text-mined knowledge to judge the likelihood of SVO triplets.", "labels": [], "entities": []}, {"text": "From a natural-language generation * Indicates equal contribution (NLG) perspective, this is the content planning stage.", "labels": [], "entities": [{"text": "Indicates equal contribution (NLG)", "start_pos": 37, "end_pos": 71, "type": "METRIC", "confidence": 0.863108883301417}]}, {"text": "2. Given the selected SVO triplet, it uses a simple template-based approach to generate candidate sentences which are then ranked using a statistical language model trained on web-scale data to obtain the best overall description.", "labels": [], "entities": []}, {"text": "This is the surface realization stage.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7429257035255432}]}, {"text": "Our approach can be viewed as a holistic data-driven threestep process where we first detect objects and activities using state-of-the-art visual recognition algorithms.", "labels": [], "entities": []}, {"text": "Next, we combine these often noisy detections with an estimate of real-world likelihood, which we obtain by mining SVO triplets from largescale web corpora.", "labels": [], "entities": []}, {"text": "Finally, these triplets are used to generate candidate sentences which are then ranked for plausibility and grammaticality.", "labels": [], "entities": []}, {"text": "The resulting natural-language descriptions can be usefully employed in applications such as semantic video search and summarization, and providing video interpretations for the visually impaired.", "labels": [], "entities": [{"text": "semantic video search", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7542270620663961}, {"text": "summarization", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.9517441391944885}]}, {"text": "Using vision models alone to predict the best subject and object fora given activity is problematic, especially while dealing with challenging real-world YouTube videos as shown in Figures 4 and 5, as it requires a large annotated video corpus of similar SVO triplets ().", "labels": [], "entities": []}, {"text": "We are interested in annotating arbitrary short videos using off-the-shelf visual detectors, without the engineering effort required to build domain-specific activity models.", "labels": [], "entities": []}, {"text": "Our main contribution is incorporating the pragmatics of various entities' likelihood of being the subject/object of a given activity, learned from web-scale text corpora.", "labels": [], "entities": []}, {"text": "For example, animate objects like people and dogs are more likely to be subjects compared to inanimate objects like balls or TV monitors.", "labels": [], "entities": []}, {"text": "Likewise, certain objects are more likely to function as subjects/objects of certain activities, e.g., \"riding a horse\" vs. \"riding a house.\"", "labels": [], "entities": []}, {"text": "Selecting the best verb may also require recognizing activities for which no explicit training data has been provided.", "labels": [], "entities": []}, {"text": "For example, consider a video with a man walking his dog.", "labels": [], "entities": []}, {"text": "The object detectors might identify the man and dog; however the action detectors may only have the more general activity, \"move,\" in their training data.", "labels": [], "entities": []}, {"text": "In such cases, realworld pragmatics is very helpful in suggesting that \"walk\" is best used to describe a man \"moving\" with his dog.", "labels": [], "entities": []}, {"text": "We refer to this process as verb expansion.", "labels": [], "entities": [{"text": "verb expansion", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7411757707595825}]}, {"text": "After describing the details of our approach, we present experiments evaluating it on a real-world corpus of YouTube videos.", "labels": [], "entities": []}, {"text": "Using a variety of methods for judging the output of the system, we demonstrate that it frequently generates useful descriptions of videos and outperforms a purely vision-based approach that does not utilize text-mined knowledge.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the English portion of the YouTube data collected by, consisting of short videos each with multiple natural-language descriptions.", "labels": [], "entities": [{"text": "YouTube data collected", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.8108308911323547}]}, {"text": "This data was previously used by, and like them, we ensured that the test data only contained videos in which we can potentially detect objects.", "labels": [], "entities": []}, {"text": "We used the object detector by as it achieves the state-of-the-art performance on the PASCAL Visual Object Classes (VOC) Challenge.", "labels": [], "entities": [{"text": "PASCAL Visual Object Classes (VOC) Challenge", "start_pos": 86, "end_pos": 130, "type": "TASK", "confidence": 0.6174850389361382}]}, {"text": "As such, we selected test videos whose subjects and objects belong to the 20 VOC object classes -aeroplane, car, horse, sheep, bicycle, cat, sofa, bird, chair, motorbike, train, boat, cow, person, tv monitor, bottle, dining table, bus, dog, potted plant.", "labels": [], "entities": [{"text": "VOC object classes -aeroplane, car, horse, sheep, bicycle, cat, sofa, bird, chair, motorbike, train, boat, cow, person, tv monitor, bottle, dining table, bus, dog", "start_pos": 77, "end_pos": 239, "type": "Description", "confidence": 0.8241612945878228}]}, {"text": "During this filtering, we also allow synonyms of these object names by including all words with a Lesk similarity (as implemented by) of at least 0.5. 1 Using this approach, we chose 235 potential test videos; the remaining 1,735 videos were reserved for training.", "labels": [], "entities": []}, {"text": "All the published activity recognition methods that work on datasets such as KTH (),) and UCF50 (Reddy and Shah, 2012) have a very limited recognition vocabulary of activity classes.", "labels": [], "entities": [{"text": "activity recognition", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7594842612743378}, {"text": "KTH", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8543561697006226}, {"text": "UCF50", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.9628869891166687}]}, {"text": "Since we did not have explicit activity la- bels for our YouTube videos, we followed Motwani and Mooney (2012)'s approach to automatically discover activity clusters.", "labels": [], "entities": []}, {"text": "We first parsed the training descriptions using Stanford's dependency parser) to obtain the set of verbs describing each video.", "labels": [], "entities": []}, {"text": "We then clustered these verbs using Hierarchical Agglomerative Clustering (HAC) using the res metric from WordNet::Similarity by to measure the distance between verbs.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9322691559791565}]}, {"text": "By manually cutting the resulting hierarchy at a desired level (ensuring that each cluster has at least 9 videos), we discovered the 58 activity clusters shown in.", "labels": [], "entities": []}, {"text": "We then filtered the training and test sets to ensure that all verbs belonged to these 58 activity clusters.", "labels": [], "entities": []}, {"text": "The final data contains 185 test and 1,596 training videos.", "labels": [], "entities": []}, {"text": "Turk Given the limitations of metrics like BLEU and METEOR, we also asked human judges to evaluate the quality of the sentences generated by our ap-: Examples where we outperform the baseline: Examples where we underperform the baseline proach compared to those generated by the baseline system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9966822266578674}, {"text": "METEOR", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.944057285785675}]}, {"text": "For each of the 185 test videos, we asked 9 unique workers (with >95% HIT approval rate and who had worked on more than 1000 HITs) on Amazon Mechanical Turk to pick which sentence better described the video.", "labels": [], "entities": [{"text": "HIT approval rate", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.918287436167399}, {"text": "Amazon Mechanical Turk", "start_pos": 134, "end_pos": 156, "type": "DATASET", "confidence": 0.8768999775250753}]}, {"text": "We also gave them a \"none of the above two sentences\" option in case neither of the sentences were relevant to the video.", "labels": [], "entities": []}, {"text": "Quality was controlled by also including in each HIT a gold-standard example generated from the human descriptions, and discarding judgements of workers who incorrectly answered this gold-standard item.", "labels": [], "entities": [{"text": "Quality", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9734436869621277}]}, {"text": "Overall, when they expressed a preference, humans picked our descriptions to that of the baseline  61.04% of the time.", "labels": [], "entities": []}, {"text": "Out of the 84 videos where the majority of judges had a clear preference, they chose our descriptions 65.48% of the time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora used to Mine SVO Triplets", "labels": [], "entities": [{"text": "SVO Triplets", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.6471655070781708}]}, {"text": " Table 1.  Using the dependency parses for these corpora,  we mined SVO triplets. Specifically, we looked for  subject-verb relationships using nsubj dependencies  and verb-object relationships using dobj and prep  dependencies. The prep dependency ensures that  we account for intransitive verbs with prepositional  objects. Synonyms of subjects and objects and con- jugations of verbs were reduced to their base forms  (20 object classes, 58 activity clusters) while form- ing triplets. If a subject, verb or object not belonging", "labels": [], "entities": []}, {"text": " Table 2: SVO Triplet accuracy: Binary metric", "labels": [], "entities": [{"text": "SVO", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.4433203637599945}, {"text": "Triplet", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.8146907687187195}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.5030354261398315}]}, {"text": " Table 3: SVO Triplet accuracy: WUP metric", "labels": [], "entities": [{"text": "SVO", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.4037056863307953}, {"text": "Triplet", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.852068305015564}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.6387646198272705}, {"text": "WUP", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.575954020023346}]}, {"text": " Table 4: Automatic evaluation of sentence quality", "labels": [], "entities": []}, {"text": " Table 5: Effect of training corpus on SVO binary accu- racy", "labels": [], "entities": []}, {"text": " Table 6: Effect of training corpus on SVO WUP accuracy", "labels": [], "entities": [{"text": "SVO WUP", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.487998366355896}]}]}