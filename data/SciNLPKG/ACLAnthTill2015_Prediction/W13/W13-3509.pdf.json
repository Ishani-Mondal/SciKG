{"title": [{"text": "Learning Adaptable Patterns for Passage Reranking", "labels": [], "entities": [{"text": "Passage Reranking", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.9472125172615051}]}], "abstractContent": [{"text": "This paper proposes passage reranking models that (i) do not require manual feature engineering and (ii) greatly preserve accuracy, when changing application domain.", "labels": [], "entities": [{"text": "passage reranking", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8431348204612732}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9990949630737305}]}, {"text": "Their main characteristic is the use of relational semantic structures representing questions and their answer passages.", "labels": [], "entities": []}, {"text": "The relations are established using information from automatic classifiers, i.e., question category (QC) and focus classifiers (FC) and Named Entity Recog-nizers (NER).", "labels": [], "entities": []}, {"text": "This way (i) effective structural relational patterns can be automatically learned with kernel machines; and (ii) structures are more invariant w.r.t. different domains, thus fostering adaptability.", "labels": [], "entities": []}], "introductionContent": [{"text": "A critical issue for implementing Question Answering (QA) systems is the need of designing answer search and extraction modules specific to the target application domain.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.839041268825531}, {"text": "answer search and extraction", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.7863166481256485}]}, {"text": "These modules encode handcrafted rules based on syntactic patterns that detect the relations between a question and its candidate answers in text fragments.", "labels": [], "entities": []}, {"text": "Such rules are triggered when patterns in the question and the passage are found.", "labels": [], "entities": []}, {"text": "For example, given a question 1 : What is Mark Twain's real name? and a relevant passage, e.g., retrieved by a search engine: Samuel Langhorne Clemens, better known as Mark Twain.", "labels": [], "entities": []}, {"text": "the QA engineers typically apply a syntactic parser to obtain the parse trees of the above two sentences, from which, they extract rules like: if the pattern \"What is NP 2 's ADJ name\" is in the question and the pattern \"NP 1 better known as NP 2 \" is in the answer passage then associate the passage with a high score 2 . Machine learning has made easier the task of QA engineering by enabling the automatic learning of answer extraction modules.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 421, "end_pos": 438, "type": "TASK", "confidence": 0.7380067408084869}]}, {"text": "However, new features and training data have to be typically developed when porting a QA system from a domain to another.", "labels": [], "entities": []}, {"text": "This is even more critical considering that effective features tend to be as much complex and similar as traditional handcrafted rules.", "labels": [], "entities": []}, {"text": "To reduce the burden of manual feature engineering for QA, we proposed structural models based on kernel methods, ( with passages limited to one sentence.", "labels": [], "entities": []}, {"text": "Their main idea is to: (i) generate question and passage pairs, where the text passages are retrieved by a search engine; (ii) assuming those containing the correct answer as positive instance pairs and all the others as negative ones; (iii) represent such pairs with two syntactic trees; and (ii) learn to rank answer passages by means of structural kernels applied to two trees.", "labels": [], "entities": []}, {"text": "This enables the automatic engineering of structural/lexical semantic patterns.", "labels": [], "entities": [{"text": "automatic engineering of structural/lexical semantic patterns", "start_pos": 17, "end_pos": 78, "type": "TASK", "confidence": 0.6061850674450397}]}, {"text": "More recently, we showed that such models can be learned for passages constituted by multiple sentences on very large-scale.", "labels": [], "entities": []}, {"text": "For this purpose, we designed a shallow syntactic representation of entire paragraphs by also improving the pair representation using relational tags.", "labels": [], "entities": []}, {"text": "In this paper, we firstly use our model in as the current baseline and compare it with more advanced structures derived from dependency trees.", "labels": [], "entities": []}, {"text": "Secondly, we enrich the semantic representation of QA pairs with the categorical information provided by automatic classifiers, i.e., question category (QC) and focus classifiers (FC) and Named Entity Recognizers (NER).", "labels": [], "entities": []}, {"text": "FC determines the constituent of the question to be linked to the named entities (NEs) of the answer passage.", "labels": [], "entities": []}, {"text": "The target NEs are selected based on their compatibility with the category of the question, e.g., an NE of type PERSON is compatible with a category of a question asking fora human (HUM).", "labels": [], "entities": []}, {"text": "Thirdly, we tested our models in a cross-domain setting since we believe that: (i) the enriched representation is supposed to increase the capability of learning effective structural relational patterns through kernel machines; and (ii) such structural features are more invariant with respect to different domains, fostering their adaptability.", "labels": [], "entities": []}, {"text": "Finally, the results show that our methods greatly improve on IR baseline, e.g., BM25, by 40%, and on previous reranking models, up to 10%.", "labels": [], "entities": [{"text": "BM25", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.6079884767532349}]}, {"text": "In particular, differently from our previous work such models can effectively use NERs and the output of different automatic modules.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows, Sec.", "labels": [], "entities": []}, {"text": "2 describes our kernel-based reranker, Sec.", "labels": [], "entities": []}, {"text": "3 illustrates our question/answer relational structures; Sec.", "labels": [], "entities": []}, {"text": "5 briefly describes the feature vectors, and finally Sec.", "labels": [], "entities": []}, {"text": "6 reports the experimental results on TREC and Answerbag data.", "labels": [], "entities": [{"text": "TREC", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.4627872109413147}, {"text": "Answerbag data", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.8890535831451416}]}, {"text": "2 Learning to rank with kernels 2.1 QA system Our QA system is based on a rather simple reranking framework as displayed in: given a query question a search engine retrieves a list of candidate passages ranked by their relevancy.", "labels": [], "entities": []}, {"text": "Various NLP components embedded in the pipeline as UIMA annotators are then used to analyze each question together with its candidate answers, e.g., part-of-speech tagging, chunking, named entity recognition, constituency and dependency parsing, etc.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.7241821587085724}, {"text": "named entity recognition", "start_pos": 183, "end_pos": 207, "type": "TASK", "confidence": 0.5934387147426605}, {"text": "constituency and dependency parsing", "start_pos": 209, "end_pos": 244, "type": "TASK", "confidence": 0.5602897480130196}]}, {"text": "These annotations are then used to produce structural models (described in Sec.", "labels": [], "entities": []}, {"text": "3), which are further used by a question focus detector and question type classifiers to establish relational links fora given question/answer pair.", "labels": [], "entities": []}, {"text": "The resulting tree pairs are then used to train a kernel-based reranker, which outputs the model to refine the initial ordering of the retrieved answer passages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report the results on two QA collections: factoid open-domain QA corpus from TREC and a community QA corpus Answerbag.", "labels": [], "entities": [{"text": "TREC", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.5748409628868103}]}, {"text": "Since we focus on passage reranking we do not carryout answer extraction.", "labels": [], "entities": [{"text": "passage reranking", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.982932448387146}, {"text": "answer extraction", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.9387939870357513}]}, {"text": "The goal is to rank the passage containing the right answer in the top position.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Structural representations on TREC QA.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.7083933353424072}]}, {"text": " Table 3: Accuracy (%) of focus classifiers.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983104467391968}]}, {"text": " Table 4: Accuracy (%) of question classifiers.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984258413314819}]}, {"text": " Table 5: Cross-domain experiment: training on Answerbag", "labels": [], "entities": [{"text": "Answerbag", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.5312196612358093}]}]}