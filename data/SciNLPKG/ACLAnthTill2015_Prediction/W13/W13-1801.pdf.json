{"title": [{"text": "Computing the Most Probable String with a Probabilistic Finite State Machine", "labels": [], "entities": []}], "abstractContent": [{"text": "The problem of finding the consensus / most probable string fora distribution generated by a probabilistic finite automaton or a hidden Markov model arises in a number of natural language processing tasks: it has to be solved in several transducer related tasks like optimal decoding in speech, or finding the most probable translation of an input sentence.", "labels": [], "entities": []}, {"text": "We provide an algorithm which solves these problems in time polynomial in the inverse of the probability of the most probable string, which in practise makes the computation tractable in many cases.", "labels": [], "entities": []}, {"text": "We also show that this exact computation compares favourably with the traditional Viterbi computation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic finite state machines are used to define distributions over sets of strings, to model languages, help with decoding or for translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.9199703335762024}]}, {"text": "These machines come under various names, with different characteristics: probabilistic (generating) finite state automata, weighted machines, hidden Markov models (HMMs) or finite state transducers...", "labels": [], "entities": []}, {"text": "An important and common problem in all the settings is that of computing the most probable event generated by the machine, possibly under a constraint over the input string or the length.", "labels": [], "entities": []}, {"text": "The typical way of handling this question is by using the Viterbi algorithm, which extracts the most probable path/parse given the requirements.", "labels": [], "entities": []}, {"text": "If in certain cases finding the most probable parse is what is seeked, in others this is computed under the generally accepted belief that the computation of the most probable string, also called the consensus string, is untractable and that the Viterbi score is an acceptable approximation.", "labels": [], "entities": []}, {"text": "But the probability of the string is obtained by summing over the different parses, so there is no strong reason that the string with the most probable parse is also the most probable one.", "labels": [], "entities": []}, {"text": "The problem of finding the most probable string was addressed by a number of authors, in computational linguistics, pattern recognition and bio-informatics: the problem was proved to be N P-hard; the associated decision problem is N Pcomplete in limited cases only, because the most probable string can be exponentially long in the number of states of the finite state machine (a construction can be found in [de la).", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.9058150053024292}]}, {"text": "As a corollary, finding the most probable translation (or decoding) of some input string, when given a finite state transducer, is intractable: the set of possible transductions, with their conditional probabilities can be represented as a PFA.", "labels": [], "entities": []}, {"text": "argue that the Viterbi algorithm does not allow to solve the decoding problem in cases where there is not a oneto-one relationship between derivations and parses.", "labels": [], "entities": []}, {"text": "In automatic translation proposes to compute the top n translations from word graphs, which is possible when these are deterministic.", "labels": [], "entities": [{"text": "automatic translation", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7232958078384399}]}, {"text": "But when they are not, an alternative in statistical machine translation is to approximate these thanks to the Viterbi algorithm.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6882823805014292}]}, {"text": "In speech recognition, the optimal decoding problem consists in finding the most probable sequence of utterances.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.744499534368515}]}, {"text": "Again, if the model is non-deterministic, this will usually be achieved by computing the most probable path instead.", "labels": [], "entities": []}, {"text": "In the before mentioned results the weight of each individual transition is between 0 and 1 and the score can be interpreted as a probability.", "labels": [], "entities": []}, {"text": "An interesting variant, in the framework of multiplicity automata or of accepting probabilistic finite automata (also called Rabin automata), is the question, known as the cut-point emptiness problem, of the existence of a string whose weight is above a specific threshold; this problem is known to be undecidable.", "labels": [], "entities": []}, {"text": "Ina recent analysis, de la Higuera and Oncina solved an associated decision problem: is there a string whose probability is above a given threshold?", "labels": [], "entities": []}, {"text": "The condition required is that we are given an upper bound to the length of the most probable string and a lower bound to its probability.", "labels": [], "entities": []}, {"text": "These encouraging results do not provide the means to actually compute the consensus string.", "labels": [], "entities": []}, {"text": "In this paper we provide three main results.", "labels": [], "entities": []}, {"text": "The first (Section 3) relates the probability of a string with its length; as a corollary, given any fraction p, either all strings have probability less than p, or there is a string whose probability is at least p and is of length at most (n+1) 2 p where n is the number of states of the corresponding PFA.", "labels": [], "entities": []}, {"text": "The second result (Section 4) is an algorithm that can effectively compute the consensus string in time polynomial in the inverse of the probability of this string.", "labels": [], "entities": []}, {"text": "Our third result (Section 5) is experimental: we show that our algorithm works well, and also that in highly ambiguous settings, the traditional approach, in which the Viterbi score is used to return the string with the most probable parse, will return sub-optimal results.", "labels": [], "entities": []}], "datasetContent": [{"text": "From the theoretical analysis it appears that the new algorithm will be able to compute the consensus string.", "labels": [], "entities": []}, {"text": "The goal of the experiments is therefore to show how the algorithm scales up: there are domains in natural language processing where the probabilities are very small, and the alphabets very large.", "labels": [], "entities": []}, {"text": "In others this is not the case.", "labels": [], "entities": []}, {"text": "How well does the algorithm adapt to small probabilities?", "labels": [], "entities": []}, {"text": "A second line of experiments consists in measuring the quality of the most probable parse for the consensus string.", "labels": [], "entities": []}, {"text": "This could obviously not be measured up to now (because of the lack of an algorithm for the most probable string).", "labels": [], "entities": []}, {"text": "Finding out how far (both in value and in rank) the string returned by the Viterbi algorithm is from the consensus string is of interest.", "labels": [], "entities": []}, {"text": "A more extensive experimentation may consist in building a collection of random PFA, in the line of what has been done in HMM/PFA learning competitions, for instance.", "labels": [], "entities": []}, {"text": "A connected graph is built, the arcs are transformed into transitions by adding labels and weights.", "labels": [], "entities": []}, {"text": "A normalisation phase takes place so as to end up with a distribution of probabilities.", "labels": [], "entities": []}, {"text": "The main drawback of this procedure is that, most often, the consensus string will end up by being the empty string (or a very short string) and any algorithm will find it easily.", "labels": [], "entities": []}, {"text": "Actually, the same will happen when testing on more realistic data: a language model built from ngrams will often have as most probable string the empty string.", "labels": [], "entities": []}, {"text": "An extensive experimentation should also compare this algorithm with alternative techniques which have been introduced: A first extension of the Viterbi approximation is called crunching]: instead of just computing fora string the probability of the best path, with a bit more effort, the value associated to a string is the sum of the probabilities of then best paths.", "labels": [], "entities": []}, {"text": "Another approach is variational decoding: in this method and alternatives like Minimum Risk Decoding, a best approximation by n-grams of the distribution is used, and the most probable string is taken as the one which maximizes the probability with respect to this approximation.", "labels": [], "entities": [{"text": "Minimum Risk Decoding", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.6090774635473887}]}, {"text": "These techniques are shown to give better results than the Viterbi decoding, but are notable to cope with long distance dependencies.", "labels": [], "entities": []}, {"text": "Coarse-to-fine parsing] is a strategy that reduces the complexity of the search involved in finding the best parse.", "labels": [], "entities": [{"text": "Coarse-to-fine parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5378822833299637}]}, {"text": "It defines a sequence of increasingly more complex Probabilistic Context-Free grammars (PCFG), and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG.", "labels": [], "entities": []}], "tableCaptions": []}