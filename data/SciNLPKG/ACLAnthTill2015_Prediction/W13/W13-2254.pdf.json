{"title": [{"text": "MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame based MT evaluation metric", "labels": [], "entities": [{"text": "MEANT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9338502883911133}, {"text": "WMT 2013", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.8516070544719696}, {"text": "MT evaluation", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.946372926235199}]}], "abstractContent": [{"text": "The linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semantic frame similarity between MT output and reference translations.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9459165930747986}, {"text": "MT evaluation", "start_pos": 148, "end_pos": 161, "type": "TASK", "confidence": 0.7834690809249878}, {"text": "MT output and reference translations", "start_pos": 218, "end_pos": 254, "type": "TASK", "confidence": 0.7878017067909241}]}, {"text": "In this paper , we describe HKUST's submission to the WMT 2013 metrics evaluation task, MEANT and UMEANT.", "labels": [], "entities": [{"text": "HKUST", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.8884442448616028}, {"text": "WMT 2013 metrics evaluation task", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.7760541915893555}, {"text": "MEANT", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9819213151931763}, {"text": "UMEANT", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.5877688527107239}]}, {"text": "MEANT is optimized by tuning a small number of weights-one for each semantic role label-so as to maximize correlation with human adequacy judgment on a development set.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5085923075675964}]}, {"text": "UMEANT is an unsuper-vised version where weights for each semantic role label are estimated via an inexpensive unsupervised approach, as opposed to MEANT's supervised method relying on more expensive grid search.", "labels": [], "entities": [{"text": "UMEANT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8688043355941772}]}, {"text": "In this paper, we present a battery of experiments for optimizing MEANT on different development sets to determine the set of weights that maximize MEANT's accuracy and stability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9979185461997986}]}, {"text": "Evaluated on test sets from the WMT 2012/2011 metrics evaluation , both MEANT and UMEANT achieve competitive correlations with human judgments using nothing more than a monolin-gual corpus and an automatic shallow semantic parser.", "labels": [], "entities": [{"text": "WMT 2012/2011 metrics evaluation", "start_pos": 32, "end_pos": 64, "type": "DATASET", "confidence": 0.9129114051659902}, {"text": "MEANT", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9209420680999756}]}], "introductionContent": [{"text": "We evaluate in the context of WMT 2013 the MEANT ( ) and UMEANT (Lo and Wu, 2012) semantic machine translation (MT) evaluation metrics-tunable, simple yet highly effective, fully-automatic semantic frame based objective functions that score the degree of similarity between the MT output and the reference translations via semantic role labels (SRL).", "labels": [], "entities": [{"text": "MEANT", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9152494668960571}, {"text": "UMEANT (Lo and Wu, 2012) semantic machine translation (MT) evaluation", "start_pos": 57, "end_pos": 126, "type": "TASK", "confidence": 0.7179847458998362}]}, {"text": "Recent studies (  show that tuning MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9691827297210693}, {"text": "MEANT", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.8939116597175598}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9957481026649475}, {"text": "TER", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9432804584503174}]}, {"text": "In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (, which assume that a good translation is one that shares the same lexical choices as the reference translation.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.864915007352829}, {"text": "MT evaluation", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.8742829859256744}, {"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9984834790229797}]}, {"text": "Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely;).", "labels": [], "entities": []}, {"text": "Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure-\"who did what to whom, when, where and why\").", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9935310482978821}, {"text": "MT evaluation", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9124331176280975}]}, {"text": "show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), CDER (), WER (), and TER).", "labels": [], "entities": [{"text": "MEANT", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9917117357254028}, {"text": "MT evaluation", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.8945702016353607}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9983217120170593}, {"text": "NIST", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.7473310828208923}, {"text": "METEOR", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9382324814796448}, {"text": "WER", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.9763315916061401}, {"text": "TER", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9966098666191101}]}, {"text": "Recent studies ) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9643581509590149}, {"text": "MEANT", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.8637269139289856}, {"text": "BLEU", "start_pos": 212, "end_pos": 216, "type": "METRIC", "confidence": 0.9545757174491882}]}, {"text": "These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.981230616569519}, {"text": "MT evaluation", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9122980833053589}, {"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9897247552871704}]}, {"text": "As described in, the pa-rameters in MEANT, i.e. the weight for each semantic role label, could be estimated using simple grid search to optimize the correlation with human adequacy judgments.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.8397429585456848}]}, {"text": "Later, Lo and Wu (2012) described an unsupervised approach for estimating the parameters of MEANT using relative frequency of each semantic role label in the reference translations under the situation when the human judgments for the development set are unavailable.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.6562475562095642}]}, {"text": "In this paper, we refer the version of MEANT using the unsupervised approach of weight estimation as UMEANT.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.7814521193504333}, {"text": "UMEANT", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.6627346873283386}]}, {"text": "In this paper, we present a battery of experiments for optimizing MEANT on different development sets to determine the set of weights that maximizes MEANT's accuracy and stability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9974068999290466}]}, {"text": "Evaluated on the test sets of WMT 2012/2011 metrics evaluation, MEANT and UMEANT achieve a competitive correlation score with human judgments by nothing more than a monolingual corpus and an automatic shallow semantic parser.", "labels": [], "entities": [{"text": "WMT 2012/2011 metrics evaluation", "start_pos": 30, "end_pos": 62, "type": "DATASET", "confidence": 0.8901727696259817}, {"text": "MEANT", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9272241592407227}, {"text": "UMEANT", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.5788211822509766}]}], "datasetContent": [{"text": "We tune the 12 weights for the set of semantic role labels in MEANT using grid search to maximize the correlation with human judgment on 6 development sets.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.5328205227851868}]}, {"text": "Following the protocol in WMT12 metrics evaluation task, we use Kendall's correlation coefficient for the sentence-level correlation with human judgments.", "labels": [], "entities": [{"text": "WMT12 metrics evaluation task", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.5747319310903549}, {"text": "Kendall's correlation coefficient", "start_pos": 64, "end_pos": 97, "type": "METRIC", "confidence": 0.708105057477951}]}, {"text": "The GALE development set consists of 40 sentences randomly drawn from the DARPA GALE P2.5 Chinese-English evaluation set along with the outputs from 3 participating MT systems and the corresponding human adequacy judgments.", "labels": [], "entities": [{"text": "GALE development set", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7879287004470825}, {"text": "DARPA GALE P2.5 Chinese-English evaluation set", "start_pos": 74, "end_pos": 120, "type": "DATASET", "confidence": 0.8653653462727865}]}, {"text": "The WMT12-A development set consists of 800 sentences randomly drawn from the Czech-English test set in WMT12 metrics evaluation task along with the output from 5 participating systems and the corresponding human judgments.", "labels": [], "entities": [{"text": "WMT12-A development set", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8479468027750651}, {"text": "Czech-English test set", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.8998880982398987}]}, {"text": "Similarly, each of the WMT12-B, WMT12-C and WMT12-D development sets consists of 800 randomly drawn sentences from the WMT12 metrics evaluation test set on German-English, Spanish-English and French-English respectively.", "labels": [], "entities": [{"text": "WMT12-B", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9236440062522888}, {"text": "WMT12-C", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.8282235860824585}, {"text": "WMT12-D development sets", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.8965173363685608}, {"text": "WMT12 metrics evaluation test set", "start_pos": 119, "end_pos": 152, "type": "DATASET", "confidence": 0.8448657989501953}]}, {"text": "The WMT12-E development set consists of 800 sentences out of which 200 sentences were randomly drawn from each of WMT12-A, WMT12-B, WMT12-C and WMT12-D data set.", "labels": [], "entities": [{"text": "WMT12-E development set", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.9265040556589762}, {"text": "WMT12-A", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9693217277526855}, {"text": "WMT12-B", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.7842856049537659}, {"text": "WMT12-C", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.8252899646759033}, {"text": "WMT12-D data set", "start_pos": 144, "end_pos": 160, "type": "DATASET", "confidence": 0.954945703347524}]}, {"text": "We evaluated MEANT and UMEANT on 3 groups of test sets.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.8731213808059692}, {"text": "UMEANT", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.5265726447105408}]}, {"text": "The first group is the original (without partition) test data for each language pair (translated in English) in WMT12.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.9711319804191589}]}, {"text": "This group of test sets is used for comparing MEANT's performance with the reported results from other participants of WMT12.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.8062703609466553}, {"text": "WMT12", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.7195006608963013}]}, {"text": "The second group is the held out subset of the test data for each language pair in WMT12.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9551329612731934}]}, {"text": "The third group is the original set of test data for each language pair in WMT11.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.946716845035553}]}, {"text": "The latter 2 groups are used for determining which set of tuned weights maximize the accuracy and stability of MEANT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9993712306022644}, {"text": "MEANT", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.4741212725639343}]}, {"text": "shows that the best and the worst sentencelevel correlations reported in on the original WMT12 test sets (without partitioning) for translations into English, together the sentence-level correlation of MEANT tuned on different development sets and UMEANT.", "labels": [], "entities": [{"text": "WMT12 test sets", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.964073896408081}, {"text": "MEANT", "start_pos": 202, "end_pos": 207, "type": "METRIC", "confidence": 0.9758317470550537}, {"text": "UMEANT", "start_pos": 248, "end_pos": 254, "type": "DATASET", "confidence": 0.8740803003311157}]}, {"text": "The grey boxes mark the results of experiments in which there was an overlap between parts of the development data and the test data.", "labels": [], "entities": []}, {"text": "Given the fact that MEANT employs significantly less expensive linguistic resources and less sophisticated machine learning algorithm in tuning the parameters, the performance of MEANT is very competitive with other participants last year.", "labels": [], "entities": []}, {"text": "shows the sentence-level correlation on the WMT12 held-out test sets and the original WMT11 test sets of MEANT tuned on different development sets and UMEANT together with the average sentence-level correlation on all test sets.", "labels": [], "entities": [{"text": "WMT12 held-out test sets", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.9364776015281677}, {"text": "WMT11 test sets", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9689400394757589}, {"text": "MEANT", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9476526379585266}, {"text": "UMEANT", "start_pos": 151, "end_pos": 157, "type": "DATASET", "confidence": 0.7781698703765869}]}, {"text": "The results show that MEANT tuning on WMT12-C development set achieve the highest sentencelevel correlation with human judgments on average.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9168480038642883}, {"text": "WMT12-C development set", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.906600554784139}]}, {"text": "UMEANT, the unsupervised wight estimated version of MEANT, achieves a very competitive correlation score when compared with MEANT tuned on different development sets.", "labels": [], "entities": [{"text": "UMEANT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8633431196212769}, {"text": "MEANT", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.6014891266822815}, {"text": "correlation", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.967538595199585}]}, {"text": "As a result,: The best and the worst sentence-level correlation reported in on the original WMT12 test sets (without partitioning) for translations into English together the sentence-level correlation of MEANT tuned on different development sets and UMEANT.", "labels": [], "entities": [{"text": "WMT12 test sets", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9679842591285706}, {"text": "MEANT", "start_pos": 204, "end_pos": 209, "type": "METRIC", "confidence": 0.9417414665222168}, {"text": "UMEANT", "start_pos": 250, "end_pos": 256, "type": "DATASET", "confidence": 0.8342314958572388}]}, {"text": "The grey box marked results of experiments in which parts of the development data and the test data are overlapped.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Sentence-level correlation on the WMT12 held-out test sets and the original WMT11 test sets  of MEANT tuned on different development sets and UMEANT together with the average sentence-level  correlation on all test sets.", "labels": [], "entities": [{"text": "WMT12 held-out test sets", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.8355408012866974}, {"text": "WMT11 test sets", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9661561846733093}, {"text": "MEANT", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.8677693009376526}, {"text": "UMEANT", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.7756505012512207}]}]}