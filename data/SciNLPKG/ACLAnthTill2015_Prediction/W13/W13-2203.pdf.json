{"title": [{"text": "The Feasibility of HMEANT as a Human MT Evaluation Metric", "labels": [], "entities": [{"text": "MT Evaluation Metric", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8347169359525045}]}], "abstractContent": [{"text": "There has been a recent surge of interest in semantic machine translation, which standard automatic metrics struggle to evaluate.", "labels": [], "entities": [{"text": "semantic machine translation", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7382474939028422}]}, {"text": "A family of measures called MEANT has been proposed which uses semantic role labels (SRL) to overcome this problem.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.7194365859031677}]}, {"text": "The human variant, HMEANT, has largely been evaluated using correlation with human contrastive evaluations, the standard human evaluation metric for the WMT shared tasks.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.6940200328826904}, {"text": "WMT shared tasks", "start_pos": 153, "end_pos": 169, "type": "TASK", "confidence": 0.8754428426424662}]}, {"text": "In this paper we claim that fora human metric to be useful, it needs to be evaluated on intrinsic properties.", "labels": [], "entities": []}, {"text": "It needs to be reliable; it needs to work across different language pairs; and it needs to be lightweight.", "labels": [], "entities": []}, {"text": "Most importantly , however, a human metric must be discerning.", "labels": [], "entities": []}, {"text": "We conclude that HMEANT is a step in the right direction, but has some serious flaws.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.6680155992507935}]}, {"text": "The reliance on verbs as heads of frames, and the assumption that annotators need minimal guidelines are particularly problematic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human evaluation is essential in machine translation (MT) research because it is the ultimate way to judge system quality.", "labels": [], "entities": [{"text": "Human evaluation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7136192619800568}, {"text": "machine translation (MT) research", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.8648024449745814}]}, {"text": "Furthermore, human evaluation is used to evaluate automatic metrics which are necessary for tuning system parameters.", "labels": [], "entities": []}, {"text": "Unfortunately, there is no clear consensus on which evaluation strategy is best.", "labels": [], "entities": []}, {"text": "Humans have been asked to judge if translations are correct, to grade them and to rank them.", "labels": [], "entities": []}, {"text": "But it is often very difficult to decide how good a translation is, when there are so many possible ways of translating a sentence.", "labels": [], "entities": []}, {"text": "Another problem is that different types of evaluation might be useful for different purposes.", "labels": [], "entities": []}, {"text": "If the MT is going to be the basis of a human translator's work-flow, then post-editing effort seems like a natural fit.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9625935554504395}]}, {"text": "However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9722641110420227}]}, {"text": "We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention.", "labels": [], "entities": [{"text": "meaning retention", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7041523903608322}]}, {"text": "In particular, we consider HMEANT (), a metric that uses semantic role labels to measure how much of the \"who, why, when, where\" has been preserved.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.949148416519165}]}, {"text": "For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames.", "labels": [], "entities": [{"text": "HMEANT evaluation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8676914572715759}]}, {"text": "Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation.", "labels": [], "entities": []}, {"text": "Ina series of papers,) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations.", "labels": [], "entities": []}, {"text": "Given the drawbacks of those evaluation measures, which we discuss in Sec.", "labels": [], "entities": []}, {"text": "2, they could just as well have been evaluating the human adequacy and contrastive judgements using HMEANT.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.6954450011253357}]}, {"text": "Human evaluation metrics need to be judged on other intrinsic qualities, which we describe below.", "labels": [], "entities": []}, {"text": "The aim of this paper is to evaluate the effectiveness of HMEANT, with the goal of using it to judge the relative merits of different MT systems, for example in the shared task of the Workshop on Machine Translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 134, "end_pos": 136, "type": "TASK", "confidence": 0.9794033169746399}, {"text": "Machine Translation", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.7742976546287537}]}, {"text": "In order to be useful, an MT evaluation metric must be reliable, be language independent, have discriminatory power, and be efficient.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9024496674537659}]}, {"text": "We address each of these criteria as follows: Reliability We produce extensive IAA (Interannotator agreement) for HMEANT, breaking it down into the different stages of annotation.", "labels": [], "entities": [{"text": "IAA (Interannotator agreement)", "start_pos": 79, "end_pos": 109, "type": "METRIC", "confidence": 0.8825724959373474}, {"text": "HMEANT", "start_pos": 114, "end_pos": 120, "type": "DATASET", "confidence": 0.8068094849586487}]}, {"text": "Our experimental results show that whilst the IAA for HMEANT is acceptable at the individual stages of the annotation, the compounding effect of disagreement at each stage of the pipeline greatly reduces the effective overall IAA -to 0.44 on role alignment for German, and, only slightly better, 0.59 for English.", "labels": [], "entities": [{"text": "IAA", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9985104203224182}, {"text": "HMEANT", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.740149736404419}, {"text": "IAA -", "start_pos": 226, "end_pos": 231, "type": "METRIC", "confidence": 0.9961267709732056}, {"text": "role alignment", "start_pos": 242, "end_pos": 256, "type": "TASK", "confidence": 0.7626834809780121}]}, {"text": "This raises doubts about the reliability of HMEANT in its current form.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.6791238188743591}]}, {"text": "Discriminatory Power We consider output of three types of MT system (Phrase-based, Syntaxbased and Rule-based) to attempt to gain insight into the different types of semantic information preserved by the different systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9508236050605774}]}, {"text": "The Syntaxbased system seems to have a slight edge overall, but since IAA is so low, this result has to betaken with a grain of salt.", "labels": [], "entities": [{"text": "IAA", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.996273398399353}]}, {"text": "Language Independence We apply HMEANT to both English and German translation outputs, showing that the guidelines can be adapted to the new language.", "labels": [], "entities": [{"text": "Language Independence", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.632227897644043}, {"text": "HMEANT", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.8480671048164368}]}, {"text": "Efficiency Whilst HMEANT evaluation will never be as fast as, for example, the contrastive judgements used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9923803806304932}, {"text": "WMT shared task", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.7831621766090393}]}, {"text": "On average, annotators evaluated about 10 sentences per hour.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Cased BLEU on the full newstest2013  test set for the systems used in this study", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9884065985679626}, {"text": "newstest2013  test set", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.9623571634292603}]}, {"text": " Table 3: IAA for role identification. This is calcu- lated by considering exact endpoint matches on all  spans (predicates and arguments).", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8012027740478516}, {"text": "role identification", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.9087709188461304}]}, {"text": " Table 4: IAA for role classification. We only con- sider cases where annotators had marked the same  span in the same frame.", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7387073040008545}, {"text": "role classification", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.9377244114875793}]}, {"text": " Table 5: Most common role type disagreements,  for German", "labels": [], "entities": []}, {"text": " Table 6: Most common role type disagreements,  for English", "labels": [], "entities": []}, {"text": " Table 7: IAA for action identification.", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8829250335693359}, {"text": "action identification", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.8416588008403778}]}, {"text": " Table 8: IAA for action alignment, collapsing par- tial and full alignment", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9600955247879028}, {"text": "action alignment", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7755125463008881}]}, {"text": " Table 9: IAA for role alignment.", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9028105735778809}, {"text": "role alignment", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.9148201644420624}]}, {"text": " Table 10: Scores assigned by each annotator pair.  The numbers in brackets after the HMEANT scores  show the relative ranking assigned by each anno- tator.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9482569098472595}]}, {"text": " Table 11: Comparison of mean HMEANT and  (smoothed sentence) BLEU for the three systems.", "labels": [], "entities": [{"text": "mean", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9418269991874695}, {"text": "HMEANT", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.5070841312408447}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9564765095710754}]}]}