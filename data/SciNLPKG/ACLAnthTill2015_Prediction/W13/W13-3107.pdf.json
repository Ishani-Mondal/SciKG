{"title": [], "abstractContent": [{"text": "The paper describes our participation in the Multi-document summarization task of Multiling-2013.", "labels": [], "entities": [{"text": "Multi-document summarization task", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.8195915023485819}]}, {"text": "The community initiative was born as a pilot task for the Text Analysis Conference in 2011.", "labels": [], "entities": [{"text": "Text Analysis Conference in 2011", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.8211107850074768}]}, {"text": "This year the corpus was extended by new three languages and another five topics, covering in total 15 topics in 10 languages.", "labels": [], "entities": []}, {"text": "Our summariser is based on latent semantic analysis and it is in principle language independent.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.642320990562439}]}, {"text": "Its results on the Multiling-2011 corpus were promising.", "labels": [], "entities": [{"text": "Multiling-2011 corpus", "start_pos": 19, "end_pos": 40, "type": "DATASET", "confidence": 0.804131269454956}]}, {"text": "The generated summaries were ranked first in several languages based on various metrics.", "labels": [], "entities": []}, {"text": "The summariser with minor changes was run on the updated 2013 corpus.", "labels": [], "entities": [{"text": "updated 2013 corpus", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.7611484924952189}]}, {"text": "Although we do not have the manual evaluation results yet the ROUGE-2 score indicates good results again.", "labels": [], "entities": [{"text": "ROUGE-2 score", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9765572845935822}]}, {"text": "The summariser produced best summaries in 6 from 10 considered languages according to the ROUGE-2 metric.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.7416890263557434}]}], "introductionContent": [{"text": "Multi-document summarization has received increasing attention during the last decade.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.771361380815506}]}, {"text": "This was mainly due to the requirement of news monitoring to reduce the big bulk of highly redundant news data.", "labels": [], "entities": []}, {"text": "More and more interest arises for approaches that will be able to be applied on a variety of languages.", "labels": [], "entities": []}, {"text": "The summariser should be of high quality.", "labels": [], "entities": []}, {"text": "However, when applied in a highly multilingual environment, it has to be enough language-independent to guarantee similar performance across languages.", "labels": [], "entities": []}, {"text": "Given the lack of multilingual summarisation evaluation resources, the summarisation community started to discuss the topic at Text Analysis Conference (TAC 1 ).", "labels": [], "entities": [{"text": "summarisation evaluation", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8822657465934753}, {"text": "summarisation", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9723846912384033}, {"text": "Text Analysis Conference (TAC 1 )", "start_pos": 127, "end_pos": 160, "type": "TASK", "confidence": 0.7769221493176052}]}, {"text": "It resulted in the http://www.nist.gov/tac/ first multilingual shared task organised as part of.", "labels": [], "entities": []}, {"text": "Each group took an active role in the creation of their language subcorpus.", "labels": [], "entities": []}, {"text": "Because no freely available parallel corpus suitable for multidocument summarisation was found, news clusters from WikiNews (in English) needed to be first translated to six other languages.", "labels": [], "entities": [{"text": "multidocument summarisation", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.551488995552063}]}, {"text": "Three model summaries for each cluster were then written and both model and peer summaries were manually evaluated.", "labels": [], "entities": []}, {"text": "For Multiling-2013, three new languages were added (Chinese, Romanian and Spanish) and 5 new topics (news clusters) were added to the corpus.", "labels": [], "entities": []}, {"text": "This article contains the description of our system based on latent semantic analysis (LSA) which participated in Multiling-2013.", "labels": [], "entities": []}, {"text": "We first briefly discuss the multi-document task in section 2.", "labels": [], "entities": []}, {"text": "Then we show our summarisation approach based on LSA (Section 3).", "labels": [], "entities": [{"text": "summarisation", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.9799654483795166}, {"text": "LSA", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.7050540447235107}]}, {"text": "The next section (4) compares the participating systems based on the ROUGE-2 score.", "labels": [], "entities": [{"text": "ROUGE-2 score", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9567018449306488}]}, {"text": "Manually assigned scores were not available at the time of creation of this report.", "labels": [], "entities": []}, {"text": "We conclude by a discussion of possible improvements of the method which require language-specific resources.", "labels": [], "entities": []}, {"text": "2 Multi-document summarisation task at Multiling'13 MultiLing-2013 is a community effort, a set of research tasks and a corresponding workshop which covers three summarisation tasks, focused on the multilingual aspect.", "labels": [], "entities": [{"text": "Multi-document summarisation task", "start_pos": 2, "end_pos": 35, "type": "TASK", "confidence": 0.6240646640459696}]}, {"text": "It aims to evaluate the application of (partially or fully) language-independent summarization algorithms on a variety of languages.", "labels": [], "entities": []}, {"text": "The annotation part consisted of four phases.", "labels": [], "entities": []}, {"text": "The first phase was to select English WikiNews articles about the same event and to create the topics.", "labels": [], "entities": []}, {"text": "The articles were then manually translated to the other languages.", "labels": [], "entities": []}, {"text": "Model summaries were created separately for each language by native speakers.", "labels": [], "entities": []}, {"text": "Ina certain time frame, participating groups ran their summarisers and the automatic summaries were then evaluated, both manually (on a 5-to-1 scale) and automatically by ROUGE) and the AutoSummENG metric (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 171, "end_pos": 176, "type": "METRIC", "confidence": 0.9974880218505859}]}, {"text": "We participated with our summariser in the main multi-document task, which requires to generate a single, fluent, representative summary from a set of 10 documents describing an event sequence.", "labels": [], "entities": []}, {"text": "The language of the document set (topic) was within a given range of 10 languages (Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian and Spanish) and all documents in a set share the same language.", "labels": [], "entities": []}, {"text": "The output summary should be of the same language as its source documents.", "labels": [], "entities": []}, {"text": "The output summary should be 250 words at most.", "labels": [], "entities": []}, {"text": "The corpus was extended to 15 topics (Chinese, French and Hindi subcorpora contained only 10 topics).", "labels": [], "entities": []}], "datasetContent": [{"text": "Although the approach works only with term cooccurrence, and thus it is completely languageindependent, pre-processing plays an important role and greatly affects the performance.", "labels": [], "entities": []}, {"text": "When generating the summaries for Multiling-2013 each article was split into sentences.", "labels": [], "entities": []}, {"text": "We used the old DUC sentence splitter 3 , although a different sentence-splitting character was used for Chinese.", "labels": [], "entities": [{"text": "DUC sentence splitter", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.8468438386917114}]}, {"text": "It was a simplification because the sentence splitter should be adapted for each language (e.g. a different list of abbreviations should be used or language specific features should be added).", "labels": [], "entities": [{"text": "sentence splitter", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7345684915781021}]}, {"text": "If LSA is applied on a large matrix stopwords can be found in the first linear combination which could be then filtered out.", "labels": [], "entities": []}, {"text": "However, in our case we apply it on rather small matrices and stopwords could affect negatively the topic distribution.", "labels": [], "entities": []}, {"text": "Thus the safer option is to filter them out.", "labels": [], "entities": []}, {"text": "This brings a dependency on a language but, on the other hand, acquiring lists of stop-words for various languages is not difficult.", "labels": [], "entities": []}, {"text": "Filtering these insignificant terms does not also slowdown the system.", "labels": [], "entities": []}, {"text": "The stopwords were filtered out for all the languages of Multiling.", "labels": [], "entities": [{"text": "Multiling", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9225525259971619}]}, {"text": "The approach discussed in section 3 was then used to select sentences until the required summary length: ROUGE-2 scores of the average model and paricipating systems.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9960079193115234}]}, {"text": "Our LSA-based system is ID4 and we report its rank from the total number of systems which submitted summaries for the particular language.", "labels": [], "entities": []}, {"text": "We included the baseline (the start of a centroid article) and excluded the topline which uses model sentences.", "labels": [], "entities": []}, {"text": "much attention has to be given to sentence ordering because some topics contained articles spread over along period, even 5 years.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7941797971725464}]}, {"text": "We did not perform any temporal analysis at sentence level.", "labels": [], "entities": []}, {"text": "The sentences in the summary were ordered based on the date of the article they came from.", "labels": [], "entities": []}, {"text": "Sentences from the same article followed their order in the full text.", "labels": [], "entities": []}, {"text": "Even if they were sometimes out of context, when extracted, the adjacent sentences at least dealt with the same (or temporary close) event.", "labels": [], "entities": []}, {"text": "We analysed ROUGE scores which we received from the organisers.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.959709882736206}]}, {"text": "We discuss here ROUGE-2 (bigram) score, a traditionally used metric in summarisation evaluation.", "labels": [], "entities": [{"text": "ROUGE-2 (bigram) score", "start_pos": 16, "end_pos": 38, "type": "METRIC", "confidence": 0.8612324833869934}, {"text": "summarisation evaluation", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.970070093870163}]}, {"text": "ROUGE-2 ranked our summariser on the top of the list for 6 from 10 languages (Arabic, Czech, English, French, Romanian, Spanish).", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7948404550552368}]}, {"text": "System ID11 performed better twice (Hebrew and Hindi), there were three better systems in Greek and the baseline won in Chinese.", "labels": [], "entities": []}, {"text": "In the following, we will discuss the results for each language separately.", "labels": [], "entities": []}, {"text": "For Arabic, our system received the best ROUGE-2 score.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9981208443641663}]}, {"text": "It was significantly better (at confidence 95%) then 5 other systems, including baseline.", "labels": [], "entities": []}, {"text": "It performed on the same level as models.", "labels": [], "entities": []}, {"text": "It was our first attempt to run the summariser on Chinese.", "labels": [], "entities": []}, {"text": "We did not use any specific wordsplitting tool and we considered each character to be a context feature for LSA.", "labels": [], "entities": []}, {"text": "The ROUGE results say that the summariser was not that successful compared to the others.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9894223809242249}]}, {"text": "It was significantly better than one system and worse than two and the baseline which received suspiciously high score.", "labels": [], "entities": []}, {"text": "We annotated the Czech part of the corpus, and therefore the result of our system can be considered only as another baseline for this language.", "labels": [], "entities": [{"text": "Czech part of the corpus", "start_pos": 17, "end_pos": 41, "type": "DATASET", "confidence": 0.7050718426704407}]}, {"text": "It received the largest ROUGE-2 score, however, there was no significant difference among the top four systems.", "labels": [], "entities": [{"text": "ROUGE-2 score", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.9758945107460022}]}, {"text": "For English, our system together with the following systems ID1 and ID11 were significantly better than the rest.", "labels": [], "entities": []}, {"text": "A similar conclusion can be driven by observing the French results.", "labels": [], "entities": []}, {"text": "In the case of Greek only baseline performed poorly.", "labels": [], "entities": [{"text": "baseline", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9633976817131042}]}, {"text": "Our approach was ranked fourth although there were marginal differences between the systems.", "labels": [], "entities": []}, {"text": "For Hebrew and Hindi system ID11 performed the best, followed by our system.", "labels": [], "entities": []}, {"text": "For Romanian, a newly introduced language this year, our system received a high score, however, a larger confidence interval did not show much significance.", "labels": [], "entities": []}, {"text": "For another newly-introduced language, Spanish, only system ID11 was not significantly worse than our system.", "labels": [], "entities": []}, {"text": "As a try to compare the systems across languages, an average rank was computed.", "labels": [], "entities": []}, {"text": "(Computing an average of absolute ROUGE-2 scores did not seem to have sense.)", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9093675017356873}]}, {"text": "Our system and system ID11 received the best average rank: 1.9.", "labels": [], "entities": []}, {"text": "For several languages (Arabic, French, Hebrew), our summaries were better (not significantly) then the average model according to ROUGE-2.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.6664191484451294}]}, {"text": "The AutoSummENG method gave results similar to those of ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.8040123581886292}]}, {"text": "The only difference was in Chi-nese: ROUGE-2 ranked our system 5th, AutoSummENG 1st.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9787988662719727}, {"text": "AutoSummENG", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.7121795415878296}]}, {"text": "One question remains: are the ROUGE scores correlated with human grades?", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9890185594558716}]}, {"text": "Unfortunately, the human grades were not available at the time of the system reports submission.", "labels": [], "entities": []}, {"text": "However, because we were managing annotation of the Czech subcorpus we had access to human grades for that language.", "labels": [], "entities": [{"text": "Czech subcorpus", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9330761432647705}]}, {"text": "The system ranking provided by ROUGE mostly agree with the human grades, reaching Pearson correlation of .97 for the systems-only scenario.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.4904196858406067}, {"text": "Pearson correlation", "start_pos": 82, "end_pos": 101, "type": "METRIC", "confidence": 0.980275571346283}]}, {"text": "The human grades ranked our system as significantly better than any other submission in the case of Czech.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE-2 scores of the average model and paricipating systems. Our LSA-based system is ID4  and we report its rank from the total number of systems which submitted summaries for the particular  language. We included the baseline (the start of a centroid article) and excluded the topline which uses  model sentences.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9906125068664551}]}]}