{"title": [{"text": "Building the Chinese Open Wordnet (COW): Starting from Core Synsets", "labels": [], "entities": [{"text": "Chinese Open Wordnet (COW)", "start_pos": 13, "end_pos": 39, "type": "DATASET", "confidence": 0.8711272974809011}]}], "abstractContent": [{"text": "Princeton WordNet (PWN) is one of the most influential resources for semantic descriptions, and is extensively used in natural language processing.", "labels": [], "entities": [{"text": "Princeton WordNet (PWN)", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.9433165788650513}, {"text": "semantic descriptions", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7097053229808807}]}, {"text": "Based on PWN, three Chinese wordnets have been developed: Sinica Bilingual Ontological Wordnet (BOW), Southeast University WordNet (SEW), and Taiwan University WordNet (CWN).", "labels": [], "entities": [{"text": "PWN", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.895490825176239}, {"text": "Taiwan University WordNet (CWN)", "start_pos": 142, "end_pos": 173, "type": "DATASET", "confidence": 0.7399530410766602}]}, {"text": "We used SEW to sense-tag a corpus, but found some issues with coverage and precision.", "labels": [], "entities": [{"text": "coverage", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9735037684440613}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9966576099395752}]}, {"text": "We decided to make anew Chinese wordnet based on SEW to increase the coverage and accuracy.", "labels": [], "entities": [{"text": "SEW", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9558553099632263}, {"text": "coverage", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9843432903289795}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9979175925254822}]}, {"text": "In addition, a small scale Chinese wordnet was constructed from open multilingual wordnet (OMW) using data from Wiktionary (WIKT).", "labels": [], "entities": []}, {"text": "We then merged SEW and WIKT.", "labels": [], "entities": [{"text": "SEW", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9325113296508789}, {"text": "WIKT", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.9572895765304565}]}, {"text": "Starting from core synsets, we formulated guidelines for the new Chinese Open Wordnet (COW).", "labels": [], "entities": [{"text": "Chinese Open Wordnet (COW)", "start_pos": 65, "end_pos": 91, "type": "DATASET", "confidence": 0.8948979179064432}]}, {"text": "We compared the five Chinese wordnets, which shows that COW is currently the best, but it still has room for further improvement, especially with polysemous words.", "labels": [], "entities": [{"text": "COW", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.6885555386543274}]}, {"text": "It is clear that building an accurate semantic resource fora language is not an easy task, but through consistent efforts, we will be able to achieve it.", "labels": [], "entities": []}, {"text": "COW is released under the same license as the PWN, an open license that freely allows use, adaptation and redistribution.", "labels": [], "entities": [{"text": "COW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9452852606773376}, {"text": "PWN", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9532496929168701}]}], "introductionContent": [{"text": "Semantic descriptions of languages are useful fora variety of tasks.", "labels": [], "entities": []}, {"text": "One of the most influential such resources is the Princeton WordNet (PWN), an English lexical database created at the Cognitive Science Laboratory of Princeton University.", "labels": [], "entities": [{"text": "Princeton WordNet (PWN)", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.9371186137199402}]}, {"text": "It is widely used in natural language processing tasks, such as word sense disambiguation, information retrieval and text classification.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.6922107338905334}, {"text": "information retrieval", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.8187051713466644}, {"text": "text classification", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7840398550033569}]}, {"text": "PWN has greatly improved the performance of these tasks.", "labels": [], "entities": [{"text": "PWN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.927668571472168}]}, {"text": "Based on PWN, three Chinese wordnets have been developed.", "labels": [], "entities": [{"text": "PWN", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.9461544156074524}]}, {"text": "Sinica Bilingual Ontological Wordnet (BOW) was created through a bootstrapping method.", "labels": [], "entities": [{"text": "Sinica Bilingual Ontological Wordnet (BOW)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.6283670238086155}]}, {"text": "Southeast University Chinese WordNet (SEW) was automatically constructed by implementing three approaches, including Minimum Distance, Intersection and Words Cooccurrence (Xu,; Taiwan University and Academia Sinica also developed a Chinese WordNet (CWN).", "labels": [], "entities": []}, {"text": "We used SEW to sense-tag NTU corpus data.", "labels": [], "entities": [{"text": "SEW", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.7451117634773254}, {"text": "NTU corpus data", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.804551512002945}]}, {"text": "However, its mistakes and its coverage hinder the progress of the sense-tagged corpus.", "labels": [], "entities": []}, {"text": "Moreover, the open multilingual wordnet project (OMW) 1 created wordnet data for many languages, including Chinese.", "labels": [], "entities": []}, {"text": "Based on OMW, we created a small scale Chinese wordnet from Wiktionary (WIKT).", "labels": [], "entities": [{"text": "OMW", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8920367956161499}, {"text": "Wiktionary (WIKT)", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.7480102628469467}]}, {"text": "All of these wordnets have some flaws and, when we started our project, none of them were available under an open license.", "labels": [], "entities": []}, {"text": "A high-quality and freely available wordnet would bean important resource for the community.", "labels": [], "entities": []}, {"text": "Therefore, we have started work on yet another Chinese wordnet in Nanyang Technological University (NTU COW), aiming to produce one with even better accuracy and coverage.", "labels": [], "entities": [{"text": "Nanyang Technological University (NTU COW)", "start_pos": 66, "end_pos": 108, "type": "DATASET", "confidence": 0.9537022709846497}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9988056421279907}, {"text": "coverage", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9261528253555298}]}, {"text": "Core synsets 2 are the most common ones ranked according to word frequency in British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 78, "end_pos": 101, "type": "DATASET", "confidence": 0.9184121092160543}]}, {"text": "There are 4,960 synsets after mapping to WordNet 3.0.", "labels": [], "entities": []}, {"text": "These synsets are more salient than others, so we began with them.", "labels": [], "entities": []}, {"text": "In this paper we compared all the five wordnets (COW, BOW, SEW, WIKT, and CWN), and showed their strengths and weaknesses.", "labels": [], "entities": [{"text": "BOW", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9861097931861877}, {"text": "SEW", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9048455953598022}, {"text": "WIKT", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.8166834115982056}]}, {"text": "The following sections are organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 elaborates on the four Chinese wordnets built based on PWN.", "labels": [], "entities": [{"text": "PWN", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9639238715171814}]}, {"text": "Section 3 introduces the guidelines in building COW.", "labels": [], "entities": []}, {"text": "Section 4 compares the core synsets of different wordnets.", "labels": [], "entities": []}, {"text": "Finally the conclusion and future work are stated in Section 5..", "labels": [], "entities": []}, {"text": "PWN has been a very important resource in computer science, psychology, and language studies.", "labels": [], "entities": [{"text": "PWN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9023498892784119}]}, {"text": "Hence many languages followed up and multilingual wordnets were either under construction or have been built.", "labels": [], "entities": []}, {"text": "PWN is the mother of all wordnets.", "labels": [], "entities": [{"text": "PWN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9499551057815552}]}, {"text": "IA chooses the intersection of the translated words.", "labels": [], "entities": [{"text": "IA", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.5897361040115356}]}, {"text": "WCA put an English word and a Chinese word as a group to get the co-occurrence results from Google.", "labels": [], "entities": [{"text": "WCA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9047872424125671}]}, {"text": "IA has the highest precision, but the lowest recall.", "labels": [], "entities": [{"text": "IA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7346422672271729}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9996340274810791}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9993054866790771}]}, {"text": "WCA has highest recall but lowest recall.", "labels": [], "entities": [{"text": "WCA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8869620561599731}, {"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9995686411857605}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9991692304611206}]}, {"text": "Considering the pros and cons of each approach, they then integrated them into an integrated one called MIWA.", "labels": [], "entities": [{"text": "MIWA", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.8319458961486816}]}, {"text": "They first chose IA to process the whole English WordNet then MDA to deal with the remaining synsets of WordNet; finally adopt WCA for the rest.", "labels": [], "entities": [{"text": "MDA", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.7597211003303528}]}, {"text": "Following this order, MIWA got a high translation precision and increased the number of synsets that can be translated.", "labels": [], "entities": [{"text": "MIWA", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.516982913017273}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9222613573074341}]}, {"text": "SEW is free for research, but cannot be redistributed.", "labels": [], "entities": [{"text": "SEW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9376364946365356}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Size of SEW, BOW, CWN, and WIKT", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8429303765296936}, {"text": "SEW", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8216400742530823}, {"text": "BOW", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9937283992767334}, {"text": "WIKT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9054213762283325}]}, {"text": " Table 2. Revision of the wordnet", "labels": [], "entities": [{"text": "wordnet", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.5829445123672485}]}, {"text": " Table 3. Error rate of entries by POS", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9786829352378845}, {"text": "POS", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.6701318025588989}]}, {"text": " Table 5. Loose gold standard", "labels": [], "entities": []}, {"text": " Table 6. Strict gold standard", "labels": [], "entities": []}]}