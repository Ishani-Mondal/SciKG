{"title": [{"text": "Event-Centered Information Retrieval Using Kernels on Event Graphs", "labels": [], "entities": [{"text": "Event-Centered Information Retrieval", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5807628333568573}]}], "abstractContent": [{"text": "Traditional information retrieval models assume keyword-based queries and use unstruc-tured document representations.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7228541076183319}]}, {"text": "There is an abundance of event-centered texts (e.g., breaking news) and event-oriented information needs that often involve structure that cannot be expressed using keywords.", "labels": [], "entities": []}, {"text": "We present a novel retrieval model that uses a struc-tured event-based representation.", "labels": [], "entities": []}, {"text": "We structure queries and documents as graphs of event mentions and employ graph kernels to measure the query-document similarity.", "labels": [], "entities": []}, {"text": "Experimental results on two event-oriented test collections show significant improvements over state-of-the-art keyword-based models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The purpose of an information retrieval (IR) system is to retrieve the documents relevant to user's information need expressed in the form of a query.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.8586191654205322}]}, {"text": "Many information needs are event-oriented, while at the same time there exists an abundance of event-centered texts (e.g., breaking news, police reports) that could satisfy these needs.", "labels": [], "entities": []}, {"text": "Furthermore, event-oriented information needs often involve structure that cannot easily be expressed with keyword-based queries (e.g., \"What are the countries that President Bush has visited and in which has his visit triggered protests?\").", "labels": [], "entities": []}, {"text": "Traditional IR models () rely on shallow unstructured representations of documents and queries, making no use of syntactic, semantic, or discourse level information.", "labels": [], "entities": [{"text": "IR", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9737840294837952}]}, {"text": "On the other hand, models utilizing structured event-based representations have not yet proven useful in IR.", "labels": [], "entities": [{"text": "IR", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9940156936645508}]}, {"text": "However, significant advances in event extraction have been achieved in the last decade as the result of standardization efforts ( and shared evaluation tasks (, renewing the interest in structured event-based text representations.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8489860892295837}]}, {"text": "In this paper we present a novel retrieval model that relies on structured event-based representation of text and addresses event-centered queries.", "labels": [], "entities": []}, {"text": "We define an event-oriented query as a query referring to one or more real-world events, possibly including their participants, the circumstances under which the events occurred, and the temporal relations between the events.", "labels": [], "entities": []}, {"text": "We account for such queries by structuring both documents and queries into event graphs (Glava\u0161 and\u0160najderand\u02c7and\u0160najder, 2013b).", "labels": [], "entities": []}, {"text": "The event graphs are built from individual event mentions extracted from text, capturing their protagonists, times, locations, and temporal relations.", "labels": [], "entities": []}, {"text": "To measure the query-document similarity, we compare the corresponding event graphs using graph kernels.", "labels": [], "entities": []}, {"text": "Experimental results on two news story collections show significant improvements over stateof-the-art keyword-based models.", "labels": [], "entities": []}, {"text": "We also show that our models are especially suitable for retrieval from collections containing topically similar documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "To the best of our knowledge, there is no standard test collection available for event-centered IR that we could use to evaluate our models.", "labels": [], "entities": [{"text": "IR", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.844098687171936}]}, {"text": "Thus, we decided to build two such test collections, with 50 queries each: (1) a general collection of topically diverse news stories and (2) a topic-specific collection of news on Syria crisis.", "labels": [], "entities": []}, {"text": "The first collection contains 25,948 news stories obtained from EMM News Brief, an online news clustering service.", "labels": [], "entities": [{"text": "EMM News Brief", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9568810065587362}]}, {"text": "1 For the topic-specific collection, we selected from the general collection 1387 documents that contain the word \"Syria\" or its derivations.", "labels": [], "entities": []}, {"text": "For each collection we asked an annotator to compile 50 queries.", "labels": [], "entities": []}, {"text": "She was instructed to select at random a document from the collection, read the document carefully, and compile at least one query consisting of at least two event mentions, in such away that the selected document is relevant for the query.", "labels": [], "entities": []}, {"text": "Example queries are shown in.", "labels": [], "entities": []}, {"text": "For instance, query q1 (whose corresponding event graph is shown in was created based on the following document (whose event graph is shown in): Google Inc.", "labels": [], "entities": [{"text": "Google Inc.", "start_pos": 145, "end_pos": 156, "type": "DATASET", "confidence": 0.9216525554656982}]}, {"text": "won approval from Chinese regulators for its $12.5 billion purchase of Motorola Mobility Holdings Inc., clearing a final hurdle fora deal that boosts its patents portfolio.", "labels": [], "entities": [{"text": "Motorola Mobility Holdings Inc.", "start_pos": 71, "end_pos": 102, "type": "DATASET", "confidence": 0.9498169720172882}]}, {"text": "To create relevance judgments, we use the standard IR pooling method with two baseline retrieval models -a TF-IDF weighted vector space model (VSM) and a language model.", "labels": [], "entities": [{"text": "relevance judgments", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8289911150932312}, {"text": "IR pooling", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.937480241060257}]}, {"text": "Our graph-based model was not used for pooling because of time limitations (note that this favors the baseline models because pool-based evaluation is biased against models not contributing to the pool  clusters contain less than 50 news stories, we estimate that there are at most 50 relevant documents per query.", "labels": [], "entities": []}, {"text": "To get an even better estimate of recall, for each query we pooled the union of top 75 documents retrieved by each of the two baseline models.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9995137453079224}]}, {"text": "One annotator made the relevance judgments for all queries.", "labels": [], "entities": []}, {"text": "We asked another annotator to provide judgments for two randomly chosen queries and obtained perfect agreement, which confirmed our intuition that determining relevance for complex eventcentered queries is not difficult.", "labels": [], "entities": [{"text": "agreement", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9773764610290527}]}, {"text": "The average number of relevant documents per query in the general and topic-specific collection is 12 and 8, respectively.", "labels": [], "entities": []}, {"text": "shows the mean average precision (MAP) on both test collections for four graph kernel-based models (tensor/conormal product and with/without Mah\u00e9 extension).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9107774694760641}]}, {"text": "We compare our models to baselines from the three traditional IR paradigms: a TF-IDF-weighted cosine VSM, the language model of, and the bestperforming models from the probabilistic Divergence from Randomness (DFR) framework (In expC2 and DFR BM25)).", "labels": [], "entities": [{"text": "DFR BM25", "start_pos": 239, "end_pos": 247, "type": "DATASET", "confidence": 0.8459071516990662}]}, {"text": "We evaluate these models using the Terrier IR platform.", "labels": [], "entities": [{"text": "Terrier IR platform", "start_pos": 35, "end_pos": 54, "type": "DATASET", "confidence": 0.7240950465202332}]}, {"text": "Overall, all models perform worse on the topicspecific collection, in which all documents are topically related.", "labels": [], "entities": []}, {"text": "Our graph kernel models outperform all baseline models (p<0.01 for tensor models and p<0.05 for conormal models; paired student's t-test) on both collections, with a wider margin on topicspecific than on the general collection.", "labels": [], "entities": []}, {"text": "This result suggests that the graph-based models are especially suitable for retrieval over topic-specific collections.", "labels": [], "entities": []}, {"text": "There is no significant difference between the tensor product and conormal product models, indicating that the conormal product introduces spurious edges more often than it remedies for incorrect extraction of temporal relations.", "labels": [], "entities": []}, {"text": "The performance differences due to Mah\u00e9 extension are not significant, providing no conclusive evidence on the effect of tottering.", "labels": [], "entities": [{"text": "Mah\u00e9 extension", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.5493816137313843}]}, {"text": "To gain more insights into the performance of our event graph-based model, we analyzed per query differences in average precision between our bestperforming model (Tensor) and the best-performing baseline (In expC2) on queries from the general collection.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9172804951667786}]}, {"text": "shows the histogram of differences.", "labels": [], "entities": []}, {"text": "Our graph kernel-based model outperforms the baseline on 42 out of 50 queries.", "labels": [], "entities": []}, {"text": "A closer inspection of the eight queries on which our model performs worse than the baseline reveals that this is due to (1) an important event mention not being extracted from the query (2 cases) or a (2) failure in coreference resolution between an event mention from the query and a mention from the document (6 cases).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 217, "end_pos": 239, "type": "TASK", "confidence": 0.947293609380722}]}], "tableCaptions": [{"text": " Table 2: Retrieval performance (MAP)", "labels": [], "entities": []}]}