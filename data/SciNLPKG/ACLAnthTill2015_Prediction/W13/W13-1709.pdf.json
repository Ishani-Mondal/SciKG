{"title": [{"text": "Automated Content Scoring of Spoken Responses in an Assessment for Teachers of English", "labels": [], "entities": [{"text": "Automated Content Scoring of Spoken Responses", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7771607935428619}]}], "abstractContent": [{"text": "This paper presents and evaluates approaches to automatically score the content correctness of spoken responses in anew language test for teachers of English as a foreign language who are non-native speakers of English.", "labels": [], "entities": []}, {"text": "Most existing tests of English spoken proficiency elicit responses that are either very constrained (e.g., reading a passage aloud) or are of a predominantly spontaneous nature (e.g., stating an opinion on an issue).", "labels": [], "entities": []}, {"text": "However, the assessment discussed in this paper focuses on essential speaking skills that English teachers need in order to be effective communicators in their classrooms and elicits mostly responses that fall in between these extremes and are moderately predictable.", "labels": [], "entities": []}, {"text": "In order to automatically score the content accuracy of these spoken responses , we propose three categories of robust features, inspired from flexible text matching, n-grams, as well as string edit distance met-rics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9686586260795593}]}, {"text": "The experimental results indicate that even based on speech recognizer output, most of the feature correlations with human expert rater scores are in the range of r = 0.4 tor = 0.5, and further, that a scoring model for predicting human rater proficiency scores that includes our content features can significantly outperform a baseline without these features (r = 0.56 vs. r = 0.33).", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.6931294947862625}]}], "introductionContent": [{"text": "With the increased need for instruction of international learners of English as a foreign language (EFL), there is a concomitant rise in demand to assess the language competence of English teachers who are non-native speakers of English.", "labels": [], "entities": [{"text": "international learners of English as a foreign language (EFL)", "start_pos": 43, "end_pos": 104, "type": "TASK", "confidence": 0.6393684988672083}]}, {"text": "This situation arises because it is neither possible nor affordable for countries where English is not spoken as a native language to employ only or even mostly native speakers of English as EFL teachers.", "labels": [], "entities": []}, {"text": "Moreover, as the language of instruction increasingly becomes English inmost classrooms, teachers' competence in the productive language modality of speaking becomes substantially more important than in the past.", "labels": [], "entities": []}, {"text": "In order to meet this demand for assessing the English language proficiency of teachers of English, anew test, English Teachers Language Assessment (ETLA), was developed recently and piloted in 2012.", "labels": [], "entities": [{"text": "English Teachers Language Assessment (ETLA)", "start_pos": 111, "end_pos": 154, "type": "METRIC", "confidence": 0.5522264242172241}]}, {"text": "The test comprises items for all four main language modalities: reading, listening, writing and speaking.", "labels": [], "entities": []}, {"text": "While reading and listening items use a multiple-choice paradigm, test items for speaking and writing elicit open responses.", "labels": [], "entities": []}, {"text": "For cost and efficiency reasons, we aim to employ automated scoring of written and spoken responses in this test.", "labels": [], "entities": []}, {"text": "This paper is concerned in particular with the conceptualization, implementation and evaluation of features that can assess one aspect of English speaking proficiency: the content correctness of a test taker's response.", "labels": [], "entities": []}, {"text": "Our automated speech scoring system, SpeechRater The speaking items in ETLA range in complexity from reading a text passage aloud to more challenging tasks requiring multi-sentence responses related to typical teaching situations.", "labels": [], "entities": []}, {"text": "The items, therefore, elicit speech in which predictability ranges from high (e.g., reading aloud) to medium (e.g., open responses based on teaching material).", "labels": [], "entities": []}, {"text": "(, also has features addressing other aspects of speaking proficiency, such as fluency or pronunciation, but the details of these features will not be discussed as part of this paper.", "labels": [], "entities": []}, {"text": "To illustrate what an ETLA speaking item may look like, we provide a relatively simple example here.", "labels": [], "entities": [{"text": "ETLA speaking item", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7758761246999105}]}, {"text": "Suppose the test taker (i.e., an English language teacher) is asked to request that the class open their textbooks on page 55.", "labels": [], "entities": []}, {"text": "We could see a range of responses, from \"perfect\" (score level 3, e.g., \"Please open your textbooks on page 55.\" or \"Please open your textbooks and turn to page 55.\"), to \"good\" (score level 2, e.g., \"Please open the books on the page 55.\") and to \"poor\" (score level 1, e.g., \"Open book page 55.\").", "labels": [], "entities": []}, {"text": "Again, note that for this paper we are not interested in potential issues with fluency, such as long pauses or speaking rate, nor with pronunciation or prosody.", "labels": [], "entities": []}, {"text": "We just look at the content of the test takers' responses, either in idealized form by means of a human transcription of what a test taker actually said, or in a realistic operational scenario, where we look at the output of an ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 228, "end_pos": 231, "type": "TASK", "confidence": 0.9763439893722534}]}, {"text": "In both cases, we consider the sequence of words only (i.e., a textual representation of the test takers' spoken responses).", "labels": [], "entities": []}, {"text": "One important aspect of any features used for content scoring is that they have to be robust with respect to speech recognition errors.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.8006460070610046}]}, {"text": "Robustness is necessary because we are using an automatic speech recognition (ASR) system as a front end, and the average word error rate of the system is around 27% for moderately predictable item responses.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.7800746858119965}, {"text": "word error rate", "start_pos": 122, "end_pos": 137, "type": "METRIC", "confidence": 0.7414317528406779}]}, {"text": "In order to investigate the effectiveness of candidate content features in a short-term development cycle before a larger amount of pilot data would be available, we first conducted a small scale in-house A test item is a basic element of a test, consisting of stimulus material, such as text and/or visuals, and a prompt (test question) that elicits a response from the test taker.", "labels": [], "entities": []}, {"text": "data collection effort focusing on the moderately predictable spoken items in ETLA.", "labels": [], "entities": []}, {"text": "Based on the analysis of this mini-corpus, several different categories of promising features were selected for potential operational use and then evaluated on the pilot data.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 provides an overview on related work; Section 3 describes the in-house data set, the pilot data and the ASR system; the developed features are presented in Section 4; Section 5 presents our experiments; we then discuss our findings in Section 6 and we conclude the paper in Section 7.", "labels": [], "entities": [{"text": "ASR", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.946008563041687}]}], "datasetContent": [{"text": "BLEU () is one of the most popular metrics for automatic evaluation of machine translation, where the score is calculated based on the modified n-gram precision.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9854645133018494}, {"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8190603256225586}]}, {"text": "In this study, the BLEU score is introduced to evaluate the content quality of a test response, where three different gold standard reference corpora are extracted from the training set according to each score level.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9773438274860382}]}, {"text": "Similar to the edit distance and WER features described below, three BLEU scores are calculated by comparing them with reference responses from each score level (i.e., bleu_1, bleu_2 and bleu_3).", "labels": [], "entities": [{"text": "WER", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9905909895896912}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9985722303390503}]}, {"text": "We decide to use the following two features for our experiments below: bleu_3 and bleu_score, the score level which receives the maximum BLEU score.", "labels": [], "entities": [{"text": "bleu_score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.8909999330838522}, {"text": "BLEU score", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9742075800895691}]}, {"text": "considerations, seven content features from three categories are selected and will be evaluated on a larger scale on ETLA pilot data in the next section: re_match (A1), num_fragments (A2), percent_sub_keywords (A3), bleu_3 (B1), ed_score (C1), wer_3 (C2) and wer_score (C3).", "labels": [], "entities": [{"text": "ETLA pilot data", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.8971307078997294}, {"text": "re_match (A1)", "start_pos": 154, "end_pos": 167, "type": "METRIC", "confidence": 0.8964705864588419}]}, {"text": "This section first describes experiments related to the performance of the seven selected content features on a larger corpus from an ETLA pilot administration (described above in Section 3.2).", "labels": [], "entities": [{"text": "ETLA pilot administration", "start_pos": 134, "end_pos": 159, "type": "DATASET", "confidence": 0.8835365176200867}]}, {"text": "Then, a similar analysis is conducted based on human rater analytic content scores on a subset of this data.", "labels": [], "entities": []}, {"text": "Finally, the selected content features are combined with other features related to pronunciation, prosody and fluency to build a scoring model for the prediction of human scores.", "labels": [], "entities": []}, {"text": "A construct is the set of knowledge, skills and abilities measured by a test.", "labels": [], "entities": []}, {"text": "The term \"construct considerations\" in the context of feature selection refers to the process of ensuring that the selected feature set obtains a high coverage of all aspects of the relevant construct.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7187942713499069}]}, {"text": "In the following experiments, we use the asrTrain set to train the content features.", "labels": [], "entities": []}, {"text": "Then these features are examined on the smTrain and smEval data sets.", "labels": [], "entities": [{"text": "smEval data sets", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.8707079887390137}]}, {"text": "In order to extract the edit distance, WER-and BLEU-related features for each item, three text reference corpora according to different score levels, are needed.", "labels": [], "entities": [{"text": "WER-and", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9890220165252686}, {"text": "BLEU-related", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.8522673845291138}]}, {"text": "Duplicate reference responses with the same content are removed within each score level.", "labels": [], "entities": []}, {"text": "Furthermore, we improve two RegEx features using the reference responses from the highest score level 3 in the asrTrain set.", "labels": [], "entities": [{"text": "RegEx", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.7335422039031982}]}, {"text": "(1) Since the previously obtained re_match feature based on the inhouse data may not be able to match multiple content-correct responses in the pilot data, we need to augment the set of RegEx for this feature based on correct responses from score level 3 in the asrTrain set.", "labels": [], "entities": [{"text": "RegEx", "start_pos": 186, "end_pos": 191, "type": "DATASET", "confidence": 0.9112608432769775}]}, {"text": "(2) Since the maximum number of candidate fragments varies across different ETLA items, the num_fragments feature values are not comparable across items.", "labels": [], "entities": []}, {"text": "Therefore, we redesign this feature by assigning a list of manually selected keywords for each fragment.", "labels": [], "entities": []}, {"text": "During feature extraction, we count the number of distinct keywords associated with all the matched fragments and divide this number by the number of predefined keywords for each item (as in AII.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7784024178981781}]}, {"text": "Keyword Detection), which results in another feature: perc_fragment_kw (A2).", "labels": [], "entities": [{"text": "Keyword Detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7003688365221024}]}, {"text": "Based on the ASR output of smTrain and smEval data sets, seven content features are extracted and their Pearson correlation coefficients with the holistic human scores are calculated and shown in..", "labels": [], "entities": [{"text": "ASR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.7450031042098999}, {"text": "smEval data sets", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.8766019344329834}, {"text": "Pearson correlation", "start_pos": 104, "end_pos": 123, "type": "METRIC", "confidence": 0.9639838635921478}]}, {"text": "Pearson correlation coefficients between content features and human holistic scores, based on both the transcription and the ASR output of smTrain and smEval.", "labels": [], "entities": [{"text": "ASR", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.5558033585548401}]}, {"text": "The intercorrelation for content analytic scores was 0.79.", "labels": [], "entities": [{"text": "intercorrelation", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9914138317108154}]}, {"text": "1,410 responses from the smTrain set and 1,402 responses from the smEval set received such analytic content scores.", "labels": [], "entities": [{"text": "smTrain set", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.8041329383850098}]}, {"text": "On this subset, table 5 shows the Pearson correlation coefficients between the content features and the analytic content scores, as well as the holistic scores, for comparison.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.9660035371780396}]}], "tableCaptions": [{"text": " Table 1. All these spoken responses have been  manually transcribed and scored with holistic  scores from 1 to 3 by trained human expert raters.  For the smTrain and smEval partitions, there were  6,367 responses receiving double annotation, and  the inter-rater correlation was 0.73. Furthermore,  the average length of responses from smTrain and  smEval sets was 10.5 words, and the correspond- ing vocabulary size was 855 (not including partial  words).", "labels": [], "entities": [{"text": "correspond- ing vocabulary size", "start_pos": 386, "end_pos": 417, "type": "METRIC", "confidence": 0.8472077012062073}]}, {"text": " Table 1. Number of speakers and number of responses  included within each data partition.", "labels": [], "entities": []}, {"text": " Table 2, only including moderately pre- dictable responses.", "labels": [], "entities": []}, {"text": " Table 3. Pearson correlation coefficients (r) of content  features with human holistic scores.", "labels": [], "entities": [{"text": "Pearson correlation coefficients (r)", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.932876189549764}]}, {"text": " Table 4. Pearson correlation coefficients between con- tent features and human holistic scores, based on both  the transcription and the ASR output of smTrain and  smEval. 5", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9231024980545044}, {"text": "ASR", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.5861532092094421}]}, {"text": " Table 5. Pearson correlation coefficients between con- tent features and human analytic content scores as well  as human holistic scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8985379934310913}]}]}