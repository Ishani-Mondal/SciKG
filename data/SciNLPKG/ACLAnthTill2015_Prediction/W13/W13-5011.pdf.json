{"title": [{"text": "A Graph-Based Approach to Skill Extraction from Text *", "labels": [], "entities": [{"text": "Skill Extraction from Text", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.9130354821681976}]}], "abstractContent": [{"text": "This paper presents a system that performs skill extraction from text documents.", "labels": [], "entities": [{"text": "skill extraction from text documents", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.8462882161140441}]}, {"text": "It outputs a list of professional skills that are relevant to a given input text.", "labels": [], "entities": []}, {"text": "We argue that the system can be practical for hiring and management of personnel in an organization.", "labels": [], "entities": []}, {"text": "We make use of the texts and the hyperlink graph of Wikipedia, as well as a list of professional skills obtained from the LinkedIn social network.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.9028906226158142}]}, {"text": "The system is based on first computing similarities between an input document and the texts of Wikipedia pages and then using a biased, hub-avoiding version of the Spreading Activation algorithm on the Wikipedia graph in order to associate the input document with skills.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most difficult tasks of an employer can be the recruitment of anew employee out of along list of applicants.", "labels": [], "entities": []}, {"text": "Another challenge of the employer is to keep track of the skills and know-how of their employees in order to direct the right people to work on things they know.", "labels": [], "entities": []}, {"text": "In the scientific community, editors of journals and committees of conferences always face the task of assigning suitable reviewers fora tall pile of submitted papers.", "labels": [], "entities": []}, {"text": "The tasks described above are example problems of expertise retrieval (.", "labels": [], "entities": [{"text": "expertise retrieval", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7464953064918518}]}, {"text": "It is a subfield of information retrieval that focuses on inferring associations between people, expertise and information content, such as text documents.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7273827642202377}]}, {"text": "* Part of this work has been funded by projects with the \"R\u00e9gion wallonne\".", "labels": [], "entities": []}, {"text": "We thank this institution forgiving us the opportunity to conduct both fundamental and applied research.", "labels": [], "entities": []}, {"text": "In addition, we thank Laurent Genard and St\u00e9phane Dessy for their contributions for the In this paper, we propose a method that makes a step towards a solution of these problems.", "labels": [], "entities": []}, {"text": "We describe an approach for the extraction of professional skills associated with a text or its author.", "labels": [], "entities": []}, {"text": "The goal of our system is to automatically extract a set of skills from an input text, such as a set of articles written by a person.", "labels": [], "entities": []}, {"text": "Such technology can be potentially useful in various contexts, such as the ones mentioned above, along with expertise management in a company, analysis of professional blogs, automatic meta-data extraction, etc.", "labels": [], "entities": [{"text": "meta-data extraction", "start_pos": 185, "end_pos": 205, "type": "TASK", "confidence": 0.7352845966815948}]}, {"text": "For succeeding in our goal, we exploit Wikipedia, a list of skills obtained from the LinkedIn social network and the mapping between them.", "labels": [], "entities": []}, {"text": "Our method consists of two phases.", "labels": [], "entities": []}, {"text": "First, we analyze a query document with a vector space model or a topic model in order to associate it with Wikipedia articles.", "labels": [], "entities": []}, {"text": "Then, using these initial pages, we use the Spreading Activation algorithm on the hyperlink graph of Wikipedia in order to find articles that correspond to LinkedIn skills and are related or central to the initial pages.", "labels": [], "entities": []}, {"text": "One difficulty with this approach is that it often results in some skills, which can be identified as hubs of the Wikipedia graph, constantly being retrieved, regardless of what the input is.", "labels": [], "entities": [{"text": "Wikipedia graph", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.9160682559013367}]}, {"text": "In order to avoid this pitfall, we bias the activation to avoid spreading to general, or popular nodes.", "labels": [], "entities": []}, {"text": "We try different measures of node popularity to redirect the spreading and perform evaluative experiments which show that this biasing in fact improves retrieval results.", "labels": [], "entities": []}, {"text": "We have built a web service that enables anyone to test our skill extraction system.", "labels": [], "entities": [{"text": "skill extraction", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.7190065830945969}]}, {"text": "The name of the system is Elisit, an abbreviation from \"Expertise Localization from Informal Sources and Information Technologies\" and conveying the idea of trying to elicit, i.e. draw forth latent information about expertise in a target text.", "labels": [], "entities": []}, {"text": "According to the best of our knowledge, we are the first to propose such a system and describe openly the method behind it.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compare the four text similarity functions, we collected p = 200, 000 pairs of semantically related documents from the \"See also\" sections of Wikipedia articles.", "labels": [], "entities": []}, {"text": "A good model is supposed to assign a high similarity to these pairs.", "labels": [], "entities": []}, {"text": "However, since the distribution of similarity scores depends on the model, one cannot simply compare the mean similarity \u00af s over the set of pairs.", "labels": [], "entities": []}, {"text": "Thus, we used a z-score as evaluation metric.", "labels": [], "entities": []}, {"text": "The z-scores are computed as wher\u00ea \u00b5 and\u02c6\u03c3and\u02c6 and\u02c6\u03c3 are sample estimates of mean and standard deviation of similarity scores fora given model.", "labels": [], "entities": []}, {"text": "These sample estimates have been calculated from a set of 1,000,000 randomly selected pairs of articles.", "labels": [], "entities": []}, {"text": "presents the results of this experiment.", "labels": [], "entities": []}, {"text": "It appears that more complex models (LSA, LDA) are outperformed on this task by the simpler vector space models (TF-IDF, LogEntropy).", "labels": [], "entities": []}, {"text": "This can be just a special case with this experimental setting and perhaps another choice of the number of topics could give better results.", "labels": [], "entities": []}, {"text": "Thus, further meta-parameter optimization of LSA and LDA is one approach for improving the performance of the text2wiki module.", "labels": [], "entities": []}, {"text": "In order to find the optimal strategy of applying Spreading Activation, we designed an evaluation protocol relying on related skills listed on each LinkedIn skill page.", "labels": [], "entities": []}, {"text": "These are automatically selected by computing similarities between skills from user profiles (.", "labels": [], "entities": []}, {"text": "Each skill page contains at most 20 related skills.", "labels": [], "entities": []}, {"text": "For the evaluation procedure, we choose an initial node i, corresponding to a LinkedIn skill, and activate it by setting a(0) = e i , that is a vector containing 1 in its i-th element and zeros elsewhere.", "labels": [], "entities": []}, {"text": "Then, we compute a(T ) with some spreading strategy and for some number of steps T , filter out the skill nodes and rank them according to their final activations.", "labels": [], "entities": []}, {"text": "To measure how well the related skills are represented in this ranked list of skills, we use Precision at 1, 5 and 10, and R-Precision to evaluate the accuracy of the first ranked results and Recall at 100 to see how well the algorithm manages to activate all of the re-lated skills.", "labels": [], "entities": [{"text": "Precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9924288392066956}, {"text": "R-Precision", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.9783927202224731}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9991838335990906}, {"text": "Recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.9957529306411743}]}, {"text": "There are many LinkedIn skills that are not well represented in the Wikipedia graph, because of ambiguity issues, for instance.", "labels": [], "entities": [{"text": "Wikipedia graph", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.8860065340995789}]}, {"text": "To prevent these anomalies from causing misguiding results, we selected a fixed set of 16 representative skills for the evaluation.", "labels": [], "entities": []}, {"text": "These skills were \"Statistics\", \"Hidden Markov Models\", \"Telecommunications\", \"MeeGo\", \"Digital Printing\", \"OCR\", \"Linguistics\", \"Speech Synthesis\", \"Classical\", \"Impressionist\", \"Education\", \"Secondary Education\", \"Cinematography\", \"Executive producer\", \"Social Sciences\", \"Political Sociology\".", "labels": [], "entities": [{"text": "Political Sociology", "start_pos": 275, "end_pos": 294, "type": "TASK", "confidence": 0.6988701224327087}]}, {"text": "Developing a completely automatic optimisation scheme for this model selection task would be difficult because of the number of different parameters, the size of the Wikipedia graph and the heuristic nature of the whole methodology.", "labels": [], "entities": [{"text": "Wikipedia graph", "start_pos": 166, "end_pos": 181, "type": "DATASET", "confidence": 0.9284954965114594}]}, {"text": "Thus, we decided to rely on a manual evaluation of the results.", "labels": [], "entities": []}, {"text": "Exploring the whole space of algorithms spanned by eq.", "labels": [], "entities": []}, {"text": "(1) would be too demanding as well.", "labels": [], "entities": []}, {"text": "That is why we have so far tested only a few models.", "labels": [], "entities": []}, {"text": "In the preliminary experiments that we conducted with the system, we observed that using a friction factor \u03bb smaller than one had little effect on the results, and thus we decided to always use \u03bb = 1.", "labels": [], "entities": []}, {"text": "Otherwise, we experimented with three models, which we will simply refer to as models 1, 2 and 3 and which we define as follows \u2022 model 1: \u03b3 = 0 and c(t) = 0; \u2022 model 2: \u03b3 = 1 and c(t) = 0; \u2022 model 3: \u03b3 = 0 and c(t) = a(0).", "labels": [], "entities": []}, {"text": "In model 1, activation is not conserved in anode but only depends on the activation it has received from its neighbors after each pulse.", "labels": [], "entities": []}, {"text": "In contrast, the activation that anode receives is completely conserved in model 2.", "labels": [], "entities": []}, {"text": "Model 3 corresponds to the Random Walk with Restart model, where the initial activation is fed to the system at each pulse.", "labels": [], "entities": []}, {"text": "Models 1 and 2 eventually converge to a stationary distribution that is independent of the initial activation vector.", "labels": [], "entities": []}, {"text": "This can be beneficial in situations where some of the initially activated nodes are noisy, or irrelevant, because it allows the initial activation to die out, or at least become lower than the activation of other, possibly more relevant nodes.", "labels": [], "entities": []}, {"text": "With Model 3, the initially activated nodes remain always among the most activated nodes, which is not necessarily a robust choice.", "labels": [], "entities": []}, {"text": "The outcomes of the experiments demonstrated that model 2 and model 3 perform equally well.", "labels": [], "entities": []}, {"text": "Indeed, these models are very similar, and apparently their small differences do not affect the results much.", "labels": [], "entities": []}, {"text": "However, model 1 provided constantly worse results than the two other models.", "labels": [], "entities": []}, {"text": "Thus, we decided to use model 3, corresponding to the Random Walk with Restart model, in the system and in selecting the rest of the spreading strategy.", "labels": [], "entities": [{"text": "spreading", "start_pos": 133, "end_pos": 142, "type": "TASK", "confidence": 0.9656522870063782}]}, {"text": "We also evaluated different settings for the link weighting scheme.", "labels": [], "entities": []}, {"text": "Here, we faced a startling result, namely that increasing the bidirectional link weight \u03b4 all the way up to the value \u03b4 = 15 kept improving the results according to almost all evaluation measures.", "labels": [], "entities": [{"text": "bidirectional link weight \u03b4", "start_pos": 62, "end_pos": 89, "type": "METRIC", "confidence": 0.6963815614581108}]}, {"text": "This would indicate that links that exist in only one direction do not convey a lot of semantic relatedness.", "labels": [], "entities": []}, {"text": "However, we assume that this is a phenomenon caused by the nature of the experiment and the small subset of skills used in it, and not necessarily a general phenomenon for the whole Wikipedia graph.", "labels": [], "entities": [{"text": "Wikipedia graph", "start_pos": 182, "end_pos": 197, "type": "DATASET", "confidence": 0.9281787574291229}]}, {"text": "In our experiments, the improvement was more drastic in the range \u03b4 \u2208 after which a damping effect can be observed.", "labels": [], "entities": []}, {"text": "For this reason, we decided to set the bidirectional link weight in the Elisit system to \u03b4 = 5.", "labels": [], "entities": []}, {"text": "We observed a similar phenomenon for the number of pulses T . Increasing its value up to T = 8 improved constantly the results.", "labels": [], "entities": [{"text": "T", "start_pos": 89, "end_pos": 90, "type": "METRIC", "confidence": 0.9624983072280884}]}, {"text": "However, again, there was no substantial change in the results in the range T \u2208.", "labels": [], "entities": []}, {"text": "In the web service, the number of pulses of the spreading activation can be determined by the user.", "labels": [], "entities": []}, {"text": "In addition to the parameters discussed above, the link weighting involves the popularity index \u03c0 j and the biasing parameter \u03b1.", "labels": [], "entities": [{"text": "popularity index \u03c0 j", "start_pos": 79, "end_pos": 99, "type": "METRIC", "confidence": 0.9169297069311142}]}, {"text": "An overview of the effect of these two choices can be seen in, which presents the results with the different evaluation measures.", "labels": [], "entities": []}, {"text": "These results were obtained by setting parameters as described earlier in this section.", "labels": [], "entities": []}, {"text": "First, we can see from this table that using negative values for \u03b1 in the weighting improves results compared to the natural random walk, i.e. the case \u03b1 = 0.", "labels": [], "entities": []}, {"text": "This indicates that our strategy of biasing the spreading of activation to avoid popular nodes indeed improves the results.", "labels": [], "entities": []}, {"text": "We can also see  We adapted the evaluation procedure used for the wiki2skill module, described in the previous section, in order to test the whole Elisit system.", "labels": [], "entities": []}, {"text": "This time, instead of activating the node of a given skill, we activated the nodes found by the text2wiki module when fed with the Wikipedia article corresponding to the skill.", "labels": [], "entities": []}, {"text": "We run the Spreading Activation algorithm with the setup presented in the previous section.", "labels": [], "entities": [{"text": "Spreading Activation", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8245365619659424}]}, {"text": "To make the evaluation more realistic, the initial activation of the target skill node is set to zero (instead of 1, i.e. the cosine of a vector with itself).", "labels": [], "entities": []}, {"text": "The system allows its user to set the number of initially activated nodes.", "labels": [], "entities": []}, {"text": "We investigated the effect of this choice by measuring Precision and Recall according to the related skills, and by looking at the average rank of the target skill on the list of final activations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9937554597854614}, {"text": "Recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8421963453292847}]}, {"text": "However, there was no clear trend in the results when testing with 1-200 initially activated nodes.", "labels": [], "entities": []}, {"text": "Nevertheless, we have noticed that using more than 20 initially activated nodes rarely improves the results.", "labels": [], "entities": []}, {"text": "We must also emphasize that the choice of the number of initially activated nodes depends on the query, especially its length.", "labels": [], "entities": []}, {"text": "We also wanted to compare the different VSM's  of the text2wiki module when using the whole Elisit system.", "labels": [], "entities": []}, {"text": "We did this by comparing Precision and Recall at different ranks w.r.t. the related skills of the target skill found on LinkedIn.", "labels": [], "entities": [{"text": "Precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9760233163833618}, {"text": "Recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9000705480575562}]}, {"text": "Thus, this experiment combines the experiments introduced in sections 5.1, where the evaluation was based on the \"See also\" pages, and 5.2, where we used a set of 16 target skills and their related skills.", "labels": [], "entities": []}, {"text": "reports the Precision and Recall values obtained with the different VSM's.", "labels": [], "entities": [{"text": "Precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9989261031150818}, {"text": "Recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9869458079338074}, {"text": "VSM", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.43963152170181274}]}, {"text": "These values result from an average over 12 different numbers of initially activated nodes.", "labels": [], "entities": []}, {"text": "They confirm the conclusion drawn from the experiment in section 5.1, namely that the LogEntropy and TF-IDF models outperform LSA and LDA models for this task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of different text similarity functions  on the Wikipedia \"See also\" dataset.", "labels": [], "entities": []}, {"text": " Table 2: The effect of the biasing parameter \u03b1 and the choice of popularity index on the results in the evaluation of the  wiki2skill module.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of the different models of the  text2wiki module in the performance of the whole  Elisit system.", "labels": [], "entities": []}]}