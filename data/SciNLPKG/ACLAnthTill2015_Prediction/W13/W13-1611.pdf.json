{"title": [{"text": "From newspaper to microblogging: What does it take to find opinions?", "labels": [], "entities": []}], "abstractContent": [{"text": "We compare the performance of two lexicon-based sentiment systems-SentiStrength (Thelwall et al., 2012) and SO-CAL (Taboada et al., 2011)-on the two genres of newspaper text and tweets.", "labels": [], "entities": []}, {"text": "While SentiStrength has been geared specifically toward short social-media text, SO-CAL was built for general, longer text.", "labels": [], "entities": []}, {"text": "After the initial comparison, we successively enrich the SO-CAL-based analysis with tweet-specific mechanisms and observe that in some cases, this improves the performance.", "labels": [], "entities": []}, {"text": "A qualitative error analysis then identifies classes of typical problems the two systems have with tweets.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Having computed the success rates, we then performed a small qualitative evaluation: What are the main reasons for the misclassifications on tweets?", "labels": [], "entities": []}, {"text": "In addition, we wanted to know why the Qantas corpus yielded much worse results than the Sanders corpus, and thus we looked into its results.", "labels": [], "entities": [{"text": "Qantas corpus", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.8103231191635132}, {"text": "Sanders corpus", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.8645385503768921}]}], "tableCaptions": [{"text": " Table 1: Distribution of tweets and labels across subcor- pora", "labels": [], "entities": [{"text": "Distribution of tweets and labels", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8500747323036194}]}, {"text": " Table 1.  According to the annotation guidelines, positive  and negative labels were only assigned to clear cases  of sentiment. Ambigious tweets have been anno- tated as neutral.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy on MPQA sentences", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978476762771606}, {"text": "MPQA", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.8876357674598694}]}, {"text": " Table 4: SO-CAL error types on 120 Sanders tweets", "labels": [], "entities": [{"text": "SO-CAL error types", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6931201020876566}]}, {"text": " Table 5: Error types on 75 Qantas tweets", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9901817440986633}]}]}