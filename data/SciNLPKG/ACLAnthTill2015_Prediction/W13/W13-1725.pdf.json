{"title": [{"text": "Using Other Learner Corpora in the 2013 NLI Shared Task", "labels": [], "entities": [{"text": "2013 NLI Shared Task", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.7129892110824585}]}], "abstractContent": [{"text": "Our efforts in the 2013 NLI shared task fo-cused on the potential benefits of external corpora.", "labels": [], "entities": []}, {"text": "We show that including training data from multiple corpora is highly effective at robust , cross-corpus NLI (i.e. open-training task 1), particularly when some form of domain adaptation is also applied.", "labels": [], "entities": []}, {"text": "This method can also be used to boost performance even when training data from the same corpus is available (i.e. open-training task 2).", "labels": [], "entities": []}, {"text": "However, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our participation in the 2013 NLI shared task) follows on our recent work exploring cross-corpus evaluation, i.e. using distinct corpora for training and testing, an approach that is now becoming fairly standard alternative in relevant work.", "labels": [], "entities": [{"text": "cross-corpus evaluation", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.6960033774375916}]}, {"text": "Our promotion of crosscorpus evaluation in NLI was partially motivated by serious issues with the most popular corpus for native language identification workup to now, the International Corpus of Learner English ().", "labels": [], "entities": [{"text": "crosscorpus evaluation", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7733897864818573}, {"text": "native language identification", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.7278286616007487}, {"text": "International Corpus of Learner English", "start_pos": 172, "end_pos": 211, "type": "DATASET", "confidence": 0.9604371786117554}]}, {"text": "The new TOEFL-11 () used for this NLI shared task addresses some of the problems with the ICLE (most glaringly, the fact that some topics in the ICLE appeared only in some L1 backgrounds), but, from the perspective of topic, proficiency, and particularly genre, it is necessarily limited in scope (perhaps even more so than the ICLE); in short, it addresses only a small portion of the space of learner texts.", "labels": [], "entities": [{"text": "TOEFL-11", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.5222325921058655}]}, {"text": "Our interest, then, continues to be in robust models for NLI that are not restricted to utility in a particular corpus, and in our participation in this task we have focused our efforts on the open-training tasks which allow the use of corpora beyond the TOEFL-11.", "labels": [], "entities": [{"text": "TOEFL-11", "start_pos": 255, "end_pos": 263, "type": "DATASET", "confidence": 0.9423585534095764}]}, {"text": "Since participation in these tasks was low relative to the closed-training task, fewer papers will address them, making our emphasis here all the more relevant.", "labels": [], "entities": []}, {"text": "The models built for all of three of the tasks are extensions of the model used in our recent work; we will discuss the aspects of this model common to all tasks in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 is a brief review of our methodology and results in the closed-training task, which was focused exclusively on testing features (both new and old); we found almost nothing that improved on our best feature set from previous work, and most features actually hurt performance.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss the corpora we used for the open-training tasks, some of which we collected and/or have not been applied to NLI before.", "labels": [], "entities": []}, {"text": "Our approach to the open-training task 2 using these corpora is presented in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6, we discuss how we used domain adaption methods and our various external corpora to create the (winning) model for the opentraining task 1, which did not permit usage of the TOEFL-11; we also present some post hoc testing (now that TOEFL-11 is no longer off limits).", "labels": [], "entities": [{"text": "TOEFL-11", "start_pos": 187, "end_pos": 195, "type": "DATASET", "confidence": 0.8575103878974915}]}, {"text": "In Section 7 we offer conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Feature testing for closed-training task, previ- ously investigated features; best result is in bold.", "labels": [], "entities": []}, {"text": " Table 2: Feature frequency cutoff testing for closed- training task; best result is in bold.", "labels": [], "entities": []}, {"text": " Table 3: Feature testing for closed-training task, new fea- tures; best result is in bold.", "labels": [], "entities": []}, {"text": " Table 6: Corpus testing for open-training task; best result  is in bold.", "labels": [], "entities": []}, {"text": " Table 8: ICLE testing for Open-training task 1; best result  is in bold.", "labels": [], "entities": [{"text": "ICLE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.5740880966186523}]}, {"text": " Table 9: ICNALE testing for open-training task 1; best  result is in bold.", "labels": [], "entities": [{"text": "ICNALE", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.47304558753967285}]}, {"text": " Table 11: 11-language testing on TOEFL-11 sets for open-training task 1; best result is in bold, best submitted run is  in italics.", "labels": [], "entities": [{"text": "TOEFL-11 sets", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8915969729423523}]}]}