{"title": [{"text": "Identifying Metaphorical Word Use with Tree Kernels", "labels": [], "entities": [{"text": "Identifying Metaphorical Word Use", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9072294235229492}]}], "abstractContent": [{"text": "A metaphor is a figure of speech that refers to one concept in terms of another, as in \"He is such a sweet person\".", "labels": [], "entities": []}, {"text": "Metaphors are ubiquitous and they present NLP with a range of challenges for WSD, IE, etc.", "labels": [], "entities": [{"text": "WSD", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9153454303741455}, {"text": "IE", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.8867817521095276}]}, {"text": "Identifying metaphors is thus an important step in language understanding.", "labels": [], "entities": [{"text": "Identifying metaphors", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9464071094989777}, {"text": "language understanding", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7115845233201981}]}, {"text": "However, since almost any word can serve as a metaphor, they are impossible to list.", "labels": [], "entities": []}, {"text": "To identify metaphorical use, we assume that it results in unusual semantic patterns between the metaphor and its dependencies.", "labels": [], "entities": []}, {"text": "To identify these cases, we use SVMs with tree-kernels on a balanced corpus of 3872 instances, created by bootstrapping from available metaphor lists.", "labels": [], "entities": []}, {"text": "1 We outper-form two baselines, a sequential and a vector-based approach, and achieve an F1-score of 0.75.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9996403455734253}]}], "introductionContent": [{"text": "A metaphor is a figure of speech used to transfer qualities of one concept to another, as in \"He is such a sweet person\".", "labels": [], "entities": []}, {"text": "Here, the qualities of \"sweet\" (the source) are transferred to a person (the target).", "labels": [], "entities": []}, {"text": "Traditionally, linguistics has modeled metaphors as a mapping from one domain to another.", "labels": [], "entities": []}, {"text": "Metaphors are ubiquitous in normal language and present NLP with a range of challenges.", "labels": [], "entities": []}, {"text": "First, due to their very nature, they cannot be interpreted at face value, with consequences for WSD, IE, etc.", "labels": [], "entities": [{"text": "WSD", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.7703136801719666}, {"text": "IE", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.5386787056922913}]}, {"text": "Second, metaphors are very productive constructions, and almost any word can be used metaphorically (e.g., \"This is the Donald Trump of sandwiches.\").", "labels": [], "entities": []}, {"text": "This property makes them impossible to pre-define or list.", "labels": [], "entities": []}, {"text": "Third, repeated use of a metaphor eventually solidifies it into a fixed expression with the metaphorical meaning now accepted as just another sense, no longer recognized as metaphorical at all.", "labels": [], "entities": []}, {"text": "This gradient makes it hard to determine a boundary between literal and metaphorical use of some expressions.", "labels": [], "entities": []}, {"text": "Identifying metaphors is thus a difficult but important step in language understanding.", "labels": [], "entities": [{"text": "Identifying metaphors", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.955201655626297}, {"text": "language understanding", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7026650160551071}]}, {"text": "Since many words can be productively used as new metaphors, approaches that try to identify them based on lexical features alone are bound to be unsuccessful.", "labels": [], "entities": []}, {"text": "Some approaches have therefore suggested considering distributional properties and \"abstractness\" of the phrase).", "labels": [], "entities": []}, {"text": "This nicely captures the contextual nature of metaphors, but their ubiquity makes it impossible to find truly \"clean\" data to learn the separate distributions of metaphorical and literal use for each word.", "labels": [], "entities": []}, {"text": "Other approaches have used pre-defined mappings from a source to a target domain, as in \"X is like Y\", e.g., \"emotions are like temperature\").", "labels": [], "entities": []}, {"text": "These approaches tend to do well on the defined mappings, but they do not generalize to new, creative metaphors.", "labels": [], "entities": []}, {"text": "It is doubtful that it is feasible to list all possible mappings, so these approaches remain brittle.", "labels": [], "entities": []}, {"text": "In contrast, we do not assume any predefined mappings.", "labels": [], "entities": []}, {"text": "We hypothesize instead that if we interpreted every word literally, metaphors will manifest themselves as unusual semantic compositions.", "labels": [], "entities": []}, {"text": "Since these compositions most frequently occur in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical \"You will have to eat your words\", the food-related verb heads a noun of communication.", "labels": [], "entities": []}, {"text": "In contrast, with the literal sense of \"eat\" in \"You will have to eat your peas\", it heads a food noun.", "labels": [], "entities": []}, {"text": "This intuition is the basis of the approaches in ().", "labels": [], "entities": []}, {"text": "We generalize this intuition beyond preference selections of verbs and relational nouns.", "labels": [], "entities": []}, {"text": "Given enough labeled examples of a word, we expect to find distinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships.", "labels": [], "entities": []}, {"text": "If we can learn to detect such differences/anomalies, we can reliably identify metaphors.", "labels": [], "entities": []}, {"text": "Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors.", "labels": [], "entities": []}, {"text": "The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem.", "labels": [], "entities": []}, {"text": "Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations ().", "labels": [], "entities": []}, {"text": "Our contributions in this paper are: \u2022 we annotate and release a corpus of 3872 instances for supervised metaphor classification \u2022 we are the first to use tree kernels for metaphor identification \u2022 our approach achieves an F1-score of 0.75, the best score of of all systems tested.", "labels": [], "entities": [{"text": "supervised metaphor classification", "start_pos": 94, "end_pos": 128, "type": "TASK", "confidence": 0.6772683560848236}, {"text": "metaphor identification", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.7604561746120453}, {"text": "F1-score", "start_pos": 223, "end_pos": 231, "type": "METRIC", "confidence": 0.9992231130599976}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracy, precision, recall, and F1 for various  systems on the held-out test set. Values significantly bet- ter than baseline at p < .02 are marked  *  (two-tailed t- test).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993301630020142}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.999688982963562}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.99980229139328}, {"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9998600482940674}]}]}