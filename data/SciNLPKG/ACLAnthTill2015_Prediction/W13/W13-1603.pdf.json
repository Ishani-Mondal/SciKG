{"title": [{"text": "Fine-Grained Emotion Recognition in Olympic Tweets Based on Human Computation", "labels": [], "entities": [{"text": "Fine-Grained Emotion Recognition", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6090164581934611}]}], "abstractContent": [{"text": "In this paper, we detail a method for domain specific, multi-category emotion recognition , based on human computation.", "labels": [], "entities": [{"text": "multi-category emotion recognition", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.6996659437815348}]}, {"text": "We create an Amazon Mechanical Turk 1 task that elicits emotion labels and phrase-emotion associations from the participants.", "labels": [], "entities": []}, {"text": "Using the proposed method, we create an emotion lexicon , compatible with the 20 emotion categories of the Geneva Emotion Wheel.", "labels": [], "entities": [{"text": "Geneva Emotion Wheel", "start_pos": 107, "end_pos": 127, "type": "DATASET", "confidence": 0.9630969762802124}]}, {"text": "GEW is the first computational resource that can be used to assign emotion labels with such a high level of granularity.", "labels": [], "entities": [{"text": "GEW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9305616021156311}]}, {"text": "Our emotion annotation method also produced a corpus of emotion labeled sports tweets.", "labels": [], "entities": []}, {"text": "We compared the cross-validated version of the lexicon with existing resources for both the positive/negative and multi-emotion classification problems.", "labels": [], "entities": [{"text": "multi-emotion classification", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.7193599939346313}]}, {"text": "We show that the presented domain-targeted lexicon outperforms the existing general purpose ones in both settings.", "labels": [], "entities": []}, {"text": "The performance gains are most pronounced for the fine-grained emotion classification, where we achieve an accuracy twice higher than the benchmark.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.6374603509902954}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9991844296455383}]}], "introductionContent": [{"text": "Social media platforms such as Twitter.com have become a common way for people to share opinions and emotions.", "labels": [], "entities": []}, {"text": "Sports events are traditionally accompanied by strong emotions and the 2012 summer Olympic Games in London were not an exception.", "labels": [], "entities": []}, {"text": "In this paper we describe methods to analyze and data mine the emotional content of tweets about this 1 www.mturk.com The corpus and the lexicon are available upon email request event using human computation.", "labels": [], "entities": []}, {"text": "Our goal is to create an emotion recognition method, capable of classifying domain specific emotions with a high emotion granularity.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.73097063601017}]}, {"text": "In the stated case, domain specificity refers not only to the sport event, but also to the Twitter environment.", "labels": [], "entities": []}, {"text": "We focus on the categorical representation of emotions because it allows a more fine-grained analysis and it is more natural for humans.", "labels": [], "entities": []}, {"text": "In daily life we use emotion names to describe specific feelings rather than give numerical evaluations or specify polarity.", "labels": [], "entities": []}, {"text": "So far, the multi-item emotion classification problem has received much less attention.", "labels": [], "entities": [{"text": "multi-item emotion classification", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.7977919379870096}]}, {"text": "One reason is that high quality training corpora are difficult to construct largely due to the cost of human annotators.", "labels": [], "entities": []}, {"text": "Further, if emotion representation is not carefully designed, the annotator agreement can be very low.", "labels": [], "entities": [{"text": "emotion representation", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7670564353466034}]}, {"text": "The higher the number of considered emotions is, the more difficult it is for humans to agree on a label fora given text.", "labels": [], "entities": []}, {"text": "Low quality labeling leads to difficulties in extracting powerful classification features.", "labels": [], "entities": []}, {"text": "This problem is further compounded in parsimonious environments, like Twitter, where the short text leads to alack of emotional cues.", "labels": [], "entities": []}, {"text": "All this presents challenges in developing a high-quality emotion recognition system operating with a fine-grained emotion category set within a chosen domain.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7367312610149384}]}, {"text": "In this paper, we show how to tackle the above challenges through human computation, using an online labor market such as the Amazon Mechanical Turk or AMT (.", "labels": [], "entities": []}, {"text": "To overcome the possible difficulties in annotation we employ a well-designed emotion assessment tool, the Geneva Emotion Wheel (GEW) (.", "labels": [], "entities": [{"text": "Geneva Emotion Wheel (GEW)", "start_pos": 107, "end_pos": 133, "type": "DATASET", "confidence": 0.9100026488304138}]}, {"text": "Having 20 separate emotion categories, it provides a desirable high level of emotion granularity.", "labels": [], "entities": []}, {"text": "Ina given task, we show the annotators the tweets, related to the aforementioned sports event, and ask them to classify the tweets' emotional content into one of the provided emotion categories.", "labels": [], "entities": []}, {"text": "The action sequence requires them to both label the tweets and to specify the textual constructs that support their decision.", "labels": [], "entities": []}, {"text": "We view the selected textual constructs as probable classification features.", "labels": [], "entities": []}, {"text": "The proposed method thus simultaneously produces an emotion annotated corpus of tweets and creates an emotion lexicon.", "labels": [], "entities": []}, {"text": "The resulting weighted emotion lexicon is a list of phrases indicative of emotion presence.", "labels": [], "entities": []}, {"text": "It consists solely of ones selected by respondents, while their weights were learnt based on their occurrence in the constructed Sports-Related Emotion Corpus (SREC).", "labels": [], "entities": [{"text": "Sports-Related Emotion Corpus (SREC)", "start_pos": 129, "end_pos": 165, "type": "DATASET", "confidence": 0.67847540974617}]}, {"text": "We show that the human-based lexicon is well suited for the particularities of the chosen environment, and also for an emotion model with a high number of categories.", "labels": [], "entities": []}, {"text": "Firstly, we show that domain specificity matters, and that non-specialists, using their commonsense, can extract features that are useful in classification.", "labels": [], "entities": []}, {"text": "We use the resulting lexicon, OlympLex, in a binary polarity classification problem on the domain data and show that it outperforms several traditional lexicons.", "labels": [], "entities": [{"text": "binary polarity classification", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.7220814426740011}]}, {"text": "In multi-emotion classification, we show that it is highly accurate in classifying tweets into 20 emotion categories of the Geneva Emotion Wheel (GEW) (.", "labels": [], "entities": [{"text": "multi-emotion classification", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.8102952837944031}, {"text": "Geneva Emotion Wheel (GEW)", "start_pos": 124, "end_pos": 150, "type": "DATASET", "confidence": 0.9304259320100149}]}, {"text": "As a baseline for comparison we use the Geneva wheel compatible lexicon, the Geneva Affect Label Coder (GALC).", "labels": [], "entities": [{"text": "Geneva wheel compatible lexicon", "start_pos": 40, "end_pos": 71, "type": "DATASET", "confidence": 0.9277643114328384}, {"text": "Geneva Affect Label Coder (GALC)", "start_pos": 77, "end_pos": 109, "type": "DATASET", "confidence": 0.829932417188372}]}, {"text": "The experiments show that OlympLex significantly outperforms this baseline.", "labels": [], "entities": [{"text": "OlympLex", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.6232983469963074}]}, {"text": "Such a detailed emotion representation allows us to create an accurate description of the sentiment the chosen event evokes in its viewers.", "labels": [], "entities": []}, {"text": "For instance, we find that Pride is the dominant emotion, and that it is 2.3 times more prevalent than Anger.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our lexicon on the SREC corpus as a classifier, using ten-fold cross-validation to avoid possible overfitting.", "labels": [], "entities": [{"text": "SREC corpus", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.8230071663856506}]}, {"text": "The precompiled universal lexicons were used for benchmarking.", "labels": [], "entities": []}, {"text": "As no training is required, we tested them over the full data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Basic statistics on the data collected over the annotation iterations.", "labels": [], "entities": []}, {"text": " Table 2: The results of polarity classification evaluation.  P=precision, R=recall, F1 = F1-score, A=accuracy  *A lexicon employing several emotion categories", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7436960935592651}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9970876574516296}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9721847772598267}, {"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.995785653591156}, {"text": "F1-score", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.7853418588638306}, {"text": "A", "start_pos": 100, "end_pos": 101, "type": "METRIC", "confidence": 0.9615280032157898}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.8097594380378723}]}, {"text": " Table 3: Results of multi-label evaluation. P=precision,  R=recall, F1 = F1-score, A=accuracy", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9941757917404175}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9584519267082214}, {"text": "F1", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9988505840301514}, {"text": "F1-score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8845444917678833}, {"text": "A=", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9410026967525482}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.8015342950820923}]}, {"text": " Table 4: Evaluation results at per-category level. P=precision, R=recall, F1 = F1-score", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.976757824420929}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9280972480773926}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9995588660240173}, {"text": "F1-score", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9529164433479309}]}]}