{"title": [{"text": "Edinburgh's Machine Translation Systems for European Language Pairs", "labels": [], "entities": [{"text": "Edinburgh", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9528041481971741}, {"text": "Machine Translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8317452967166901}]}], "abstractContent": [{"text": "We validated various novel and recently proposed methods for statistical machine translation on 10 language pairs, using large data resources.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.7441393136978149}]}, {"text": "We saw gains from optimizing parameters, training with sparse features, the operation sequence model, and domain adaptation techniques.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.6787233799695969}]}, {"text": "We also report on utilizing a huge language model trained on 126 billion tokens.", "labels": [], "entities": []}, {"text": "The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.8613985578219095}, {"text": "ACL Workshop on Statistical Machine Translation", "start_pos": 95, "end_pos": 142, "type": "TASK", "confidence": 0.806415339310964}, {"text": "machine translation", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.7624412178993225}]}, {"text": "Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications , led to a strong showing in the evaluation campaign.", "labels": [], "entities": [{"text": "Moses open source toolkit", "start_pos": 67, "end_pos": 92, "type": "DATASET", "confidence": 0.8957023024559021}]}, {"text": "We would like to stress especially two contributions: the use of the new operation sequence model (Section 3) within Moses, and-in a separate unconstraint track submission-the use of a huge language model trained on 126 billion tokens with anew training tool (Section 4).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We explored a number of other settings and features, but did not observe any gains.", "labels": [], "entities": []}, {"text": "\u2022 Using HMM alignment instead of IBM Model 4 leads to losses of -.01 to -.27.", "labels": [], "entities": []}, {"text": "\u2022 An earlier check of modified Moore-Lewis filtering (see also below in Section 3) gave very inconsistent results.", "labels": [], "entities": []}, {"text": "filtering) leads to losses of -.19 to -.63.", "labels": [], "entities": []}, {"text": "\u2022 Throwing out phrase pairs with direct translation probability \u03c6(\u00af e| \u00af f ) of less than 10 \u22125 has almost no effect.", "labels": [], "entities": [{"text": "direct translation probability \u03c6", "start_pos": 33, "end_pos": 65, "type": "METRIC", "confidence": 0.5597349479794502}]}, {"text": "\u2022 Double-checking the contribution of the sparse lexical features in the final setup, we observe an average losses of -.07 when dropping these features.", "labels": [], "entities": []}, {"text": "\u2022 For the German-English language pairs we saw some benefits to using sparse lexical features over POS tags instead of words, so we used this in the final system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tuning with k-best MIRA instead of MERT  (cased BLEU scores with length ratio)", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9759837985038757}, {"text": "MIRA", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9686353802680969}, {"text": "MERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9972301125526428}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9882934093475342}, {"text": "length ratio", "start_pos": 75, "end_pos": 87, "type": "METRIC", "confidence": 0.9504513740539551}]}, {"text": " Table 2: Translation model smoothing with Kneser-Ney", "labels": [], "entities": [{"text": "Translation model smoothing", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8957269191741943}]}, {"text": " Table 4: Sparse domain features", "labels": [], "entities": [{"text": "Sparse domain", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.4680922329425812}]}, {"text": " Table 5: Combining domain and other sparse features", "labels": [], "entities": []}, {"text": " Table 6: Tuning settings (number of iterations, size of n-best  list, and cube pruning pop limit)", "labels": [], "entities": []}, {"text": " Table 7: Maximum phrase length, reduced from baseline", "labels": [], "entities": []}, {"text": " Table 8: Language models without singleton pruning", "labels": [], "entities": []}, {"text": " Table 11: Overall improvements per language pair", "labels": [], "entities": []}, {"text": " Table 12: Training with new data (newstest2012 scores)", "labels": [], "entities": [{"text": "newstest2012 scores", "start_pos": 35, "end_pos": 54, "type": "DATASET", "confidence": 0.8277580738067627}]}, {"text": " Table 13: Comparison of phrase-table interpolation (two  methods) with baseline (on newstest2012). The baselines are  as", "labels": [], "entities": []}, {"text": " Table 14: Comparison of MML filtering and weighting with  baseline. The MML uses monolingual news as in-domain,  and selects from all training data after alignment.The weight- ing uses the MML weights, optionally downscaled by 10,  then exponentiated. Baselines are as Table 13.", "labels": [], "entities": [{"text": "MML filtering", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9149999022483826}]}, {"text": " Table 15: Step-wise Generation of Figure 1", "labels": [], "entities": []}, {"text": " Table 16: Results using the OSM Feature", "labels": [], "entities": [{"text": "OSM", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.6841617226600647}]}, {"text": " Table 17: Counts of unique n-grams (m for millions) for the  5 orders in the unconstrained language model", "labels": [], "entities": [{"text": "Counts", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.965146541595459}]}, {"text": " Table 18. Improvement from large lan- guage models is not a new result (", "labels": [], "entities": []}, {"text": " Table 18: Gain on newstest2013 from the unconstrained lan- guage model. Our time on shared machines with 1 TB is  limited so Russian-English was run after the deadline and  German-English was not ready in time.", "labels": [], "entities": [{"text": "newstest2013", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9587059617042542}]}]}