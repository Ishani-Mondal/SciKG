{"title": [{"text": "A Maximum Entropy Approach to Chinese Spelling Check", "labels": [], "entities": [{"text": "Chinese Spelling Check", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7938358783721924}]}], "abstractContent": [{"text": "Spelling check identifies incorrect writing words in documents.", "labels": [], "entities": [{"text": "Spelling check", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.7313782870769501}]}, {"text": "For the reason of input methods, Chinese spelling check is much different from English and it is still a challenging work.", "labels": [], "entities": [{"text": "Chinese spelling check", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.686633030573527}]}, {"text": "For the past decade years, most of the methods in detecting errors in documents are lexicon-based or probability-based, and much progress are made.", "labels": [], "entities": [{"text": "detecting errors in documents", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.8488173484802246}]}, {"text": "In this paper, we propose anew method in Chinese spelling check by using maximum entropy (ME).", "labels": [], "entities": [{"text": "Chinese spelling check", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6664720078309377}, {"text": "maximum entropy (ME)", "start_pos": 73, "end_pos": 93, "type": "METRIC", "confidence": 0.8115004658699035}]}, {"text": "Experiment shows that by importing a large raw corpus, maximum entropy can build a well-trained model to detect spelling errors in Chinese documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Because of the popularity of computers, more and more documents are produced.", "labels": [], "entities": []}, {"text": "For the carelessness of human or errors of OCR image recognition, many spelling errors occur in documents, which seriously interferes documents quality.", "labels": [], "entities": [{"text": "OCR image recognition", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.884581466515859}]}, {"text": "Proofreading by human to correct the errors is laborious and expensive, so an automatic approach is badly in need.", "labels": [], "entities": []}, {"text": "Automatic spelling check can identify incorrect writing words in documents, which plays an important role in documents writing and OCR post-processing.", "labels": [], "entities": [{"text": "documents writing", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7131490260362625}, {"text": "OCR post-processing", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.907592386007309}]}, {"text": "Research on automatic spelling check of English documents began in the 1960s (Damerau F.J., 1964) , many studies have been proposed and quite good results have been obtained.", "labels": [], "entities": [{"text": "automatic spelling check of English documents", "start_pos": 12, "end_pos": 57, "type": "TASK", "confidence": 0.7761842012405396}]}, {"text": "While spelling check of Chinese is still a challenging work due to some special processing difficulties arising from Chinese writing, which hardly occur in spelling check of English.", "labels": [], "entities": [{"text": "spelling check of Chinese", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.8649236410856247}]}, {"text": "In English writing, each word is directly input by Latin letters, so the spelling errors are only the situation that one letter is mistaken written to another, such as writing \"bcg\" instead of \"bag\", or \"son glasses\" instead of \"sun glasses\".", "labels": [], "entities": []}, {"text": "The former is a non-word spelling error, meaning the form of input word is definitely incorrect and latter is a real-word spelling error, meaning the form of input word can be found in the dictionary but incorrectly used.", "labels": [], "entities": []}, {"text": "In Chinese writing, unlike English, all legal characters (we call them hanzi) have been stored in a font lib and Chinese input system builds an effective map between Latin letters and hanzi fonts.", "labels": [], "entities": []}, {"text": "For the reason of input methods, Chinese characters would not take the non-word errors such as missing or adding apart of character to form an illegal character in the dictionary.", "labels": [], "entities": []}, {"text": "That is, all Chinese spelling errors are real-word errors.", "labels": [], "entities": []}, {"text": "The treatment of real-word errors needs analyzing the context, which is much harder than the treatment of non-word errors.", "labels": [], "entities": []}, {"text": "Chinese spelling check is still a challenging work.", "labels": [], "entities": [{"text": "Chinese spelling check", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6969883044560751}]}, {"text": "In this paper, we propose anew but simple method in Chinese spelling check by using maximum entropy (ME) models.", "labels": [], "entities": [{"text": "Chinese spelling check", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7239827315012614}]}, {"text": "We train a maximum entropy model for each Chinese character based on a large raw corpus and use the model to detect the spelling errors in documents.", "labels": [], "entities": []}, {"text": "Tentative experiment in the bakeoff shows the simple strategy works.", "labels": [], "entities": []}, {"text": "However, further refinement and methodology combinations seem still needed to produce state-of-arts results.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: In section 2, we give a brief introduction to the Chinese spelling check.", "labels": [], "entities": []}, {"text": "In section 3 we introduce our approach to Chinese spelling check using maximum entropy model.", "labels": [], "entities": [{"text": "Chinese spelling check", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6810549596945444}]}, {"text": "Section 4 is description and discussion of our experiments.", "labels": [], "entities": []}, {"text": "Section 5 is the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Spelling check performance is evaluated by Fscore F=2RP/(R + P).", "labels": [], "entities": [{"text": "Spelling check", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8716822564601898}, {"text": "Fscore F", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9813532531261444}]}, {"text": "The recall R is the ratio of the correctly identified spelling error sentences of the checker's output to all spelling error sentences in the gold-standard and the precision P refers to the ratio of the correctly identified spelling error sentences of the checker's output to all identified error sentences of the checker's output.", "labels": [], "entities": [{"text": "recall R", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.987303227186203}, {"text": "precision P", "start_pos": 164, "end_pos": 175, "type": "METRIC", "confidence": 0.9749433100223541}]}, {"text": "Moreover, False-Alarm Rate and Detection Accuracy are also introduced to evaluate spelling check.", "labels": [], "entities": [{"text": "False-Alarm Rate", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9862849712371826}, {"text": "Detection Accuracy", "start_pos": 31, "end_pos": 49, "type": "METRIC", "confidence": 0.7797863483428955}, {"text": "spelling check", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.6403807401657104}]}, {"text": "The former is the ratio of the checker's output to all spelling error sentences with false positive error detection results to testing sentences without errors in the gold-standard, and the latter is the ratio of the checker's output to all spelling sentences with correctly detected results to all testing sentences.", "labels": [], "entities": []}, {"text": "From the result, we achieve a relative better Detection Recall.", "labels": [], "entities": [{"text": "Detection", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.8857148289680481}, {"text": "Recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.5863170027732849}]}, {"text": "As the maximum entropy can storage the knowledge of characters appearing together, most of the illegal continuous characters can be detected, and they are highly likely incorrectly written characters.", "labels": [], "entities": []}, {"text": "However, the Detection Precision is relative not high, as the maximum entropy mistaken classifies many single characters with high probabilities of the wrong character category such as \"\u6211\", \"\u7684\", \"\u662f\", \"\u4e0d\", \"\u5728\" and soon.", "labels": [], "entities": [{"text": "Detection Precision", "start_pos": 13, "end_pos": 32, "type": "METRIC", "confidence": 0.764741063117981}]}, {"text": "These characters are high frequency characters, almost appearing in every sentence.", "labels": [], "entities": []}, {"text": "Even though the maximum entropy can classify over 99% of these characters correctly, the rest 1% mistaken classified would pull down the Detection Precision.", "labels": [], "entities": [{"text": "Detection Precision", "start_pos": 137, "end_pos": 156, "type": "METRIC", "confidence": 0.7843217551708221}]}], "tableCaptions": [{"text": " Table 1: Performance of the final test", "labels": [], "entities": []}]}