{"title": [{"text": "Association for Computational Linguistics From Global to Local Similarities: A Graph-Based Contextualization Method using Distributional Thesauri", "labels": [], "entities": [{"text": "Computational Linguistics From Global to Local Similarities", "start_pos": 16, "end_pos": 75, "type": "TASK", "confidence": 0.6282867108072553}]}], "abstractContent": [{"text": "After recasting the computation of a distribu-tional thesaurus in a graph-based framework for term similarity, we introduce anew con-textualization method that generates, for each term occurrence in a text, a ranked list of terms that are semantically similar and compatible with the given context.", "labels": [], "entities": []}, {"text": "The framework is instantiated by the definition of term and context , which we derive from dependency parses in this work.", "labels": [], "entities": []}, {"text": "Evaluating our approach on a standard data set for lexical substitution, we show substantial improvements over a strong non-contextualized baseline across all parts of speech.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7486208081245422}]}, {"text": "In contrast to comparable approaches, our framework defines an unsupervised gener-ative method for similarity in context and does not rely on the existence of lexical resources as a source for candidate expansions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Following () we consider two distinct viewpoints: syntagmatic relations consider the assignment of values to a linear sequence of terms, and the associative (also: paradigmatic) viewpoint assigns values according to the commonalities and differences to other terms in the reader's memory.", "labels": [], "entities": []}, {"text": "Based on these notions, we automatically expand terms in the linear sequence with their paradigmatically related terms.", "labels": [], "entities": []}, {"text": "Using the distributional hypothesis, and operationalizing similarity of terms, it became possible to compute term similarities fora large vocabulary.", "labels": [], "entities": []}, {"text": "Lin (1998) computed a distributional thesaurus (DT) by comparing context features defined over grammatical dependencies with an appropriate similarity measure for all reasonably frequent words in a large collection of text, and evaluated these automatically computed word similarities against lexical resources.", "labels": [], "entities": []}, {"text": "Entries in the DT consist of a ranked list of the globally most similar terms fora target term.", "labels": [], "entities": []}, {"text": "While the similarities are dependent on the instantiation of the context feature as well as on the underlying text collection, they are global in the sense that the DT aggregates overall occurrences of target and its similar elements.", "labels": [], "entities": []}, {"text": "In our work, we will use a DT in a graph representation and move from a global notion of similarity to a contextualized version, which performs context-dependent text expansion for all word nodes in the DT graph.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of contextualizing the thesaurus (CT) was performed using the LexSub dataset, introduced in the Lexical Substitution task at.", "labels": [], "entities": [{"text": "LexSub dataset", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9919447004795074}]}, {"text": "Following the setup provided by the task organizers, we tuned our approach on the 300 trial sentences, and evaluate it on the official remaining 1710 test sentences.", "labels": [], "entities": []}, {"text": "For the evaluation we used the out often (oot) precision and oot mode precision.", "labels": [], "entities": [{"text": "out often (oot) precision", "start_pos": 31, "end_pos": 56, "type": "METRIC", "confidence": 0.8113397558530172}, {"text": "oot mode precision", "start_pos": 61, "end_pos": 79, "type": "METRIC", "confidence": 0.7874597907066345}]}, {"text": "Both measures calculate the number of detected substitutions within ten guesses over the complete subset.", "labels": [], "entities": []}, {"text": "Whereas entries in the oot precision measures are considered correct if they match the gold standard, without penalizing non-matching entries, the oot mode precision includes also a weighting as given in the gold standard . For comparison, we use the results of the DT as a baseline to evaluate the contextualization.", "labels": [], "entities": [{"text": "oot mode precision", "start_pos": 147, "end_pos": 165, "type": "METRIC", "confidence": 0.5687772532304128}]}, {"text": "The DT was computed based on newspaper corpora (120 million sentences), taken from the Leipzig Corpora Collection () and the Gigaword corpus).", "labels": [], "entities": [{"text": "DT", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.4665237367153168}, {"text": "Leipzig Corpora Collection", "start_pos": 87, "end_pos": 113, "type": "DATASET", "confidence": 0.9687753717104594}, {"text": "Gigaword corpus", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.9482419490814209}]}, {"text": "Our holing system uses collapsed Stanford parser dependencies) as context features.", "labels": [], "entities": []}, {"text": "The contextualization uses only context features that contain words with part-of-speech prefixes V,N,J,R.", "labels": [], "entities": []}, {"text": "Furthermore, we use a threshold for the significance value of the LMI values of 50.0, p=1000, and the most similar 30 terms from the DT entries.", "labels": [], "entities": [{"text": "significance", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9613384008407593}, {"text": "DT entries", "start_pos": 133, "end_pos": 143, "type": "DATASET", "confidence": 0.8616616129875183}]}], "tableCaptions": [{"text": " Table 1: Results of the LexSub test dataset.", "labels": [], "entities": [{"text": "LexSub test dataset", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9856426119804382}]}]}