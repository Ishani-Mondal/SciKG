{"title": [{"text": "The RWTH Aachen Machine Translation System for WMT 2013", "labels": [], "entities": [{"text": "RWTH Aachen Machine Translation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8099674433469772}, {"text": "WMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8490211963653564}]}], "abstractContent": [{"text": "This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8113233149051666}, {"text": "translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013)", "start_pos": 115, "end_pos": 209, "type": "TASK", "confidence": 0.8373555392026901}]}, {"text": "We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions.", "labels": [], "entities": []}, {"text": "Both hierarchical and phrase-based SMT systems are applied.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8704500198364258}]}, {"text": "A number of different techniques are evaluated, including hierarchical phrase reordering, translation model interpolation, domain adaptation techniques, weighted phrase extraction, word class language model, continuous space language model and system combination.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.6764170974493027}, {"text": "translation model interpolation", "start_pos": 90, "end_pos": 121, "type": "TASK", "confidence": 0.8990357915560404}, {"text": "domain adaptation", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.7145865112543106}, {"text": "weighted phrase extraction", "start_pos": 153, "end_pos": 179, "type": "TASK", "confidence": 0.6149857640266418}, {"text": "system combination", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.7225258946418762}]}, {"text": "By application of these methods we achieve considerable improvements over the respective baseline systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the WMT 2013 shared translation task 1 RWTH utilized state-of-the-art phrase-based and hierarchical translation systems as well as an inhouse system combination framework.", "labels": [], "entities": [{"text": "WMT 2013 shared translation task", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.7300471901893616}]}, {"text": "We give a survey of these systems and the basic methods they implement in Section 2.", "labels": [], "entities": []}, {"text": "For both the French-English (Section 3) and the GermanEnglish (Section 4) language pair, we investigate several different advanced techniques.", "labels": [], "entities": [{"text": "GermanEnglish (Section 4) language", "start_pos": 48, "end_pos": 82, "type": "DATASET", "confidence": 0.8635610640048981}]}, {"text": "We concentrate on specific research directions for each of the translation tasks and present the respective techniques along with the empirical results they yield: For the French\u2192English task (Section 3.2), we apply a standard phrase-based system with up to five language models including a word class language model.", "labels": [], "entities": []}, {"text": "In addition, we employ translation model interpolation and hierarchical phrase reordering.", "labels": [], "entities": []}, {"text": "For the English\u2192French task (Section 3.1), we train translation models on different training data sets and augment the phrase-based system with a hierarchical reordering model, a word class language model, a discriminative word lexicon and a insertion and deletion model.", "labels": [], "entities": []}, {"text": "For the German\u2192English (Section 4.3) and English\u2192German (Section 4.4) tasks, we utilize morpho-syntactic analysis to preprocess the data (Section 4.1), domain-adaptation (Section 4.2) and a hierarchical reordering model.", "labels": [], "entities": []}, {"text": "For the German\u2192English task, an augmented hierarchical phrase-based system is setup and we rescore the phrase-based baseline with a continuous space language model.", "labels": [], "entities": []}, {"text": "Finally, we perform a system combination.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the English\u2192French task, separate translation models (TMs) were trained for each of the five data sets and fed to the decoder.", "labels": [], "entities": []}, {"text": "Four additional indicator features are introduced to distinguish the different TMs.", "labels": [], "entities": [{"text": "TMs", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.8594022989273071}]}, {"text": "Further, we applied the hierarchical reordering model, the word class language model, the discriminative word lexicon, and the insertion and deletion model.", "labels": [], "entities": []}, {"text": "shows the results of our experiments.", "labels": [], "entities": []}, {"text": "As a development set for MERT, we use newstest2010 in all setups.", "labels": [], "entities": [{"text": "MERT", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.5590702295303345}]}, {"text": "For the French\u2192English task, a translation model (TM) was trained on all available parallel data.", "labels": [], "entities": []}, {"text": "For the baseline, we interpolated this TM with an in-domain TM trained on EPPS+NC and employed the hierarchical reordering model.", "labels": [], "entities": [{"text": "EPPS+NC", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.8716896176338196}]}, {"text": "Moreover, three language models were used: The first language model was trained on the English side of all available parallel data, the second one on EPPS and NC and the third LM on the News Shuffled data.", "labels": [], "entities": [{"text": "EPPS and NC", "start_pos": 150, "end_pos": 161, "type": "DATASET", "confidence": 0.7184250950813293}, {"text": "News Shuffled data", "start_pos": 186, "end_pos": 204, "type": "DATASET", "confidence": 0.9058691064516703}]}, {"text": "The baseline was improved by adding a fourth LM trained on the Gigaword corpus (Version 5) and a 5-gram word class language model trained on News Shuffled data.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9600471258163452}, {"text": "News Shuffled data", "start_pos": 141, "end_pos": 159, "type": "DATASET", "confidence": 0.7949026425679525}]}, {"text": "For the WCLM, we used 50 word classes clustered with the tool mkcls).", "labels": [], "entities": []}, {"text": "All results are presented in Table 3.", "labels": [], "entities": []}, {"text": "For the German\u2192English task, the baseline is trained on all available parallel data and includes the hierarchical reordering model.", "labels": [], "entities": []}, {"text": "The results of the various filtering and weighting experiments are summarized in.", "labels": [], "entities": [{"text": "filtering and weighting", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.6604124704996744}]}, {"text": "For filtering, we use the 800K best sentences from the whole training corpora, as this selection performed best on the dev set among 100K,200K,400K,800K,1600K setups.", "labels": [], "entities": [{"text": "filtering", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9621896743774414}]}, {"text": "Filtering seems to mainly improve on the TER scores, BLEU scores are virtually unchanged in comparison to the baseline.", "labels": [], "entities": [{"text": "TER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9983579516410828}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9995074272155762}]}, {"text": "LM+M1 filtering improves further on TER in comparison to LM-based filtering.", "labels": [], "entities": [{"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9772959351539612}]}, {"text": "The weighted phrase extraction performs best in our experiments, where the weights from the LM+M1 scoring method are used.", "labels": [], "entities": [{"text": "weighted phrase extraction", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.624732087055842}]}, {"text": "Improvements in both BLEU and TER are achieved, with BLEU improvements ranging from +0.4% up-to +0.6% and TER improvements from -0.9% and up-to -1.1%.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9988654851913452}, {"text": "TER", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9958288073539734}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9994900226593018}, {"text": "TER", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9989166259765625}]}, {"text": "As a final step, we added the English Gigaword corpus to the LM (+GW).", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 30, "end_pos": 53, "type": "DATASET", "confidence": 0.6668088634808859}]}, {"text": "This resulted in further improvements of the systems.", "labels": [], "entities": []}, {"text": "In addition, the system as described above was tuned on newstest2009.", "labels": [], "entities": [{"text": "newstest2009", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9578720927238464}]}, {"text": "Using this development set results in worse translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9619225263595581}]}, {"text": "Furthermore, we rescored the SCSS baseline tuned on newstest2009 with a continuous space language model (CSLM) as described in ().", "labels": [], "entities": [{"text": "SCSS baseline", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.794748455286026}, {"text": "newstest2009", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9015848636627197}]}, {"text": "The CSLM was trained on the europarl and news-commentary corpora.", "labels": [], "entities": [{"text": "europarl", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9695463180541992}]}, {"text": "For rescoring, we used the newstest2011 set as tuning set and re-optimized the parameters with MERT on 1000-best lists.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9518615007400513}, {"text": "newstest2011 set", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.9510452151298523}, {"text": "MERT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9494547247886658}]}, {"text": "This results in an improvement of up to 0.8 points in BLEU compared to the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.999182403087616}]}, {"text": "We compared the phrase-based setups with a hierarchical translation system, which was augmented with preference grammars, soft stringto-dependency features, discriminative reordering extensions, DWL, IDM, and discriminative re-ordering extensions.", "labels": [], "entities": []}, {"text": "The phrase table of the hierarchical setup has been extracted from News Commentary and Europarl parallel data only (not from Common Crawl).", "labels": [], "entities": [{"text": "News Commentary", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.9312961399555206}, {"text": "Europarl parallel data", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.8811692198117574}]}, {"text": "Finally, three setups were joined in a system combination and we gained an improvement of up to 0.5 points in BLEU compared to the best single system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9977602958679199}]}, {"text": "The results for the English\u2192German task are shown in.", "labels": [], "entities": []}, {"text": "While the LM-based filtering led to almost no improvement over the baseline, the LM+M1 filtering brought some improvements in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9963957667350769}]}, {"text": "In addition to the sentence filtering, we tried to combine the translation model trained on NC+EPPS with a TM trained on Common Crawl using the ifelse combination . This combination scheme concatenates both TMs and assigns the probabilities of the in-domain TM if it contains the phrase, else it uses the probabilities of the out-of-domain TM.", "labels": [], "entities": [{"text": "sentence filtering", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7444339990615845}, {"text": "NC+EPPS", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.934933602809906}]}, {"text": "Appling this method, we achieved further improvements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics of the preprocessed  French-English parallel training data. EPPS de- notes Europarl, NC denotes News Commentary,  CC denotes Common Crawl. In the data, numeri- cal quantities have been replaced by a single cate- gory symbol.", "labels": [], "entities": [{"text": "French-English parallel training data", "start_pos": 49, "end_pos": 86, "type": "DATASET", "confidence": 0.5722476243972778}, {"text": "EPPS", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8195429444313049}, {"text": "Europarl", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.43080267310142517}]}, {"text": " Table 2: Results for the English\u2192French task (truecase). newstest2010 is used as development set.  BLEU and TER are given in percentage.", "labels": [], "entities": [{"text": "newstest2010", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9229875206947327}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9990156888961792}, {"text": "TER", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9940264225006104}]}, {"text": " Table 3: Results for the French\u2192English task (truecase). newstest2010 is used as development set.  BLEU and TER are given in percentage.", "labels": [], "entities": [{"text": "newstest2010", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9260197281837463}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9990542531013489}, {"text": "TER", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.994156002998352}]}, {"text": " Table 5: German-English results (truecase). BLEU and TER are given in percentage. Corresponding  development set is marked with *.  \u2020 labels the single systems selected for the system combination.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9991670846939087}, {"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9957531690597534}]}, {"text": " Table 6: English-German results (truecase). newstest2009 was used as development set. BLEU and TER  are given in percentage.", "labels": [], "entities": [{"text": "newstest2009", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9335315227508545}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9989172220230103}, {"text": "TER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9956804513931274}]}]}