{"title": [{"text": "Social Metaphor Detection via Topical Analysis", "labels": [], "entities": [{"text": "Social Metaphor Detection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7351834177970886}, {"text": "Topical Analysis", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.8810451030731201}]}], "abstractContent": [{"text": "With massive social media data, e.g., comments, blog articles, or tweets, become available, there is a rising interest towards automatic metaphor detection from open social text.", "labels": [], "entities": [{"text": "automatic metaphor detection", "start_pos": 127, "end_pos": 155, "type": "TASK", "confidence": 0.6898832519849142}]}, {"text": "One of the most well-known approaches is detecting the violation of selectional preference.", "labels": [], "entities": [{"text": "detecting the violation of selectional preference", "start_pos": 41, "end_pos": 90, "type": "TASK", "confidence": 0.7750065128008524}]}, {"text": "The idea of selectional preference is that verbs tend to have semantic preferences of their arguments.", "labels": [], "entities": []}, {"text": "If we find that in some text, any arguments of these predicates are not of their preferred semantic classes, and it's very likely to be a metaphor.", "labels": [], "entities": []}, {"text": "However, previously only few papers have focuses on leveraging topical analysis techniques in metaphor detection.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.9647895693778992}]}, {"text": "Intuitively, both predicates and arguments exhibit strong tendencies towards a few specific topics, and this topical information provides additional evidence to facilitate identification of selectional preference among text.", "labels": [], "entities": []}, {"text": "In this paper, we study how the metaphor detection technique can be influenced by topical analysis techniques based on our proposed threestep approach.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.9602577686309814}]}, {"text": "We formally define the problem, and propose our approach for metaphor detection, and then we conduct experiments on a real-world data set.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9491345882415771}]}, {"text": "Though our experimental result shows that topics do not have strong impacts on the metaphor detection techniques, we analyze the result and present some insights based on our study.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8573523163795471}]}], "introductionContent": [{"text": "With massive social media data, e.g., comments, blog articles, or tweets, become available, there is a rising interest towards automatic metaphor detection from open social text.", "labels": [], "entities": [{"text": "automatic metaphor detection", "start_pos": 127, "end_pos": 155, "type": "TASK", "confidence": 0.6898825863997141}]}, {"text": "One of the most well-known approaches is detecting the violation of selectional preference.", "labels": [], "entities": [{"text": "detecting the violation of selectional preference", "start_pos": 41, "end_pos": 90, "type": "TASK", "confidence": 0.7750065128008524}]}, {"text": "The idea of selectional preference is that the predicates (mostly verbs) tend to have semantic preferences of their arguments.", "labels": [], "entities": []}, {"text": "For instance, the verb \"flex\" has a strong preference of \"muscle\" and \"bone\" as its object.", "labels": [], "entities": []}, {"text": "If we find that in some text, the object of \"flex\" is not of the semantic class of \"muscle\" and \"bone\", it's very likely to be a metaphorical use.", "labels": [], "entities": []}, {"text": "Previously, researchers have studies metaphor identification by modeling selectional preference, while only few papers have focused on leveraging topical analysis techniques in metaphor detection.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.9187477827072144}, {"text": "metaphor detection", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.8457770645618439}]}, {"text": "The intuition behind combining metaphor identification and topic analysis is that both verbs and arguments exhibit strong tendencies towards a few specific topics, and this topical information provides additional evidence to facilitate identification of selectional preference among text.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7339287996292114}, {"text": "topic analysis", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7783785760402679}]}, {"text": "For instance, in the topic of sport, the subjects of \"flex\" are mostly humans; but in the topic of finance or politics, the subjects of \"flex\" are mostly organizations or countries, e.g., \"China to flex its financial muscles at US meeting.\"", "labels": [], "entities": []}, {"text": "In this paper, we aim to study how the metaphor detection technique can be influenced based on topical analysis techniques.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.9299083352088928}]}, {"text": "The problem of automatic social metaphor detection via topical analysis poses several challenges: First, as social media data is usually noisy, how to effectively preprocess the input texts before an actual detection component is employed should be carefully studied.", "labels": [], "entities": [{"text": "social metaphor detection", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7330856422583262}]}, {"text": "We should empirically estimate the performance of existing NLP tools, especially lemmatizers and POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.7350720167160034}]}, {"text": "Second, how we can automatically discover the topical distribution for each term (including verbs and nouns) within open text is not a trivial problem.", "labels": [], "entities": []}, {"text": "Moreover, we also need to study how to leverage topical distribution of each verb and noun to metaphor detection.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8973764777183533}]}, {"text": "Finally, how to apply and evaluate the proposed approach on areal world data set is not straight-forward, as there is hardly existing data set nor benchmark to evaluate metaphor detection, we need to create a benchmark that can effectively show that the performance difference.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.8287878930568695}]}, {"text": "In this paper, we formally define the problem, and propose our 3-step approach for metaphor detection, specifically, we first preprocess the input text by extracting tokens and further clustering nouns, and then we detect selectional association outlier, finally, we apply a selectional preference strength filter to extract metaphorembedded text snippets.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8816875219345093}]}, {"text": "We then conduct experiments on a real-world social media data set.", "labels": [], "entities": []}, {"text": "The LDA model is applied to partition the input corpus based on topics, and we adopt the 3-step approach both on the whole corpus and every single topic data partitions, respectively.", "labels": [], "entities": []}, {"text": "Finally, we compare the metaphor detection results between that with and without influences of topics, and to observe which one performs better.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7918122708797455}]}, {"text": "The rest of the paper is organized as follows: In Section 2, we briefly summarize related work for metaphor detection based on selectional preference detection.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.9745952785015106}, {"text": "selectional preference detection", "start_pos": 127, "end_pos": 159, "type": "TASK", "confidence": 0.639438807964325}]}, {"text": "In Section 3, we formally define the problem of automatic social metaphor detection.", "labels": [], "entities": [{"text": "automatic social metaphor detection", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.7075618132948875}]}, {"text": "Then, in Section 4, we first conduct a preliminary test to compare two technologies for metaphor detection, and choose one to establish the 3-step framework describe in Section 5.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.9321674108505249}]}, {"text": "In Section 6, we further discuss the details of topic analysis.", "labels": [], "entities": [{"text": "topic analysis", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.8640218079090118}]}, {"text": "Finally, we demonstrate the experiment in Section 7, discuss the results in Section 8, and conclude the whole work in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since labeling metaphor embedded sentences are effort consuming, we conduct experiment on a benchmark corpus, which contains 122 sentences extracted from the Web, where 61 (50%) of them contain metaphor, and 61 of them don't contain metaphor.", "labels": [], "entities": [{"text": "labeling metaphor embedded sentences", "start_pos": 6, "end_pos": 42, "type": "TASK", "confidence": 0.8156463503837585}]}, {"text": "We apply both approaches on this data set.", "labels": [], "entities": []}, {"text": "For the selectional association outlier detection, the best resulting F-1 score is 0.58 with precision of 0.60, and recall of 0.56.", "labels": [], "entities": [{"text": "selectional association outlier detection", "start_pos": 8, "end_pos": 49, "type": "TASK", "confidence": 0.6431689336895943}, {"text": "F-1 score", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9753888249397278}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9990248680114746}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9997430443763733}]}, {"text": "On the other hand, for the semantic outlier word detection, regardless of which value of threshold we set, the performance remains very low.", "labels": [], "entities": [{"text": "semantic outlier word detection", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.5813009738922119}]}, {"text": "This method returns huge amount of false positive semantic outliers.", "labels": [], "entities": []}, {"text": "Mainly due to two reasons: First, the semantic coherence can be easily affected by very general words, which usually have very high similarities and also occur very often.", "labels": [], "entities": []}, {"text": "If one sentence has more than one very general context words, e.g., \"take\", \"put\", or \"get\", the semantic coherences of all other words could be systematically increased, and thus fail to represent the outlier words.", "labels": [], "entities": []}, {"text": "We believe that's the main reason why this method cannot detect the semantic outlier we expected.", "labels": [], "entities": []}, {"text": "Second, the measure of semantic similarity between word pairs is not very reliable for low frequency words.", "labels": [], "entities": []}, {"text": "The similarities calculations which are based on the text of big corpus usually have this problem: It's reliable on high frequency words, but not on low frequency words, which exactly are what we aim to capture.", "labels": [], "entities": []}, {"text": "To conclude, the selectional association outlier detection method obviously outperform the semantic outlier word detection in the preliminary test.", "labels": [], "entities": [{"text": "selectional association outlier detection", "start_pos": 17, "end_pos": 58, "type": "TASK", "confidence": 0.5658251568675041}, {"text": "semantic outlier word detection", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.664457380771637}]}, {"text": "Therefore, in this paper, we only focus on selectional association to develop our technology.", "labels": [], "entities": []}], "tableCaptions": []}