{"title": [{"text": "Influence of preprocessing on dependency syntax annotation: speed and agreement", "labels": [], "entities": [{"text": "speed", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9961715340614319}, {"text": "agreement", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9192344546318054}]}], "abstractContent": [{"text": "When creating anew resource, prepro-cessing the source texts before annotation is both ubiquitous and obvious.", "labels": [], "entities": []}, {"text": "How the preprocessing affects the annotation effort for various tasks is for the most part an open question, however.", "labels": [], "entities": []}, {"text": "In this paper, we study the effects of preprocessing on the annotation of dependency corpora and how annotation speed varies as a function of the quality of three different parsers and compare with the speed obtained when starting from a least-processed baseline.", "labels": [], "entities": []}, {"text": "We also present preliminary results concerning the effects on agreement based on a small subset of sentences that have been doubly-annotated.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is commonly accepted wisdom in treebanking that it is preferable to preprocess data before PoS and syntax annotation, rather than having annotators work from raw text.", "labels": [], "entities": []}, {"text": "However, the impact of preprocessing is not well studied and factors such as the lower bound on performance for preprocessing to be useful and the return on investment of increased performance are largely unknown.", "labels": [], "entities": []}, {"text": "Corpora and applications based on dependency syntax have become increasingly popular in recent years, and many new corpora are being created.", "labels": [], "entities": []}, {"text": "In this work we investigate the task of syntactic annotation based on dependency grammar, and how annotation speed and inter-annotator agreement are influenced by parser performance.", "labels": [], "entities": []}, {"text": "Our study is performed in the context of the annotation effort currently underway at the national library of Norway, tasked with creating a freely available syntactically annotated corpus of Norwegian.", "labels": [], "entities": []}, {"text": "It is the first widely available such corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we outline the key methodological choices made for our experiments.", "labels": [], "entities": []}, {"text": "First we discuss what timing data we collect and the texts annotated, before describing the preprocessors used.", "labels": [], "entities": []}, {"text": "Environment For our experiments, four different texts were chosen for annotation: two from the Aftenposten (AP 06 & AP 08), and two from Dagbladet (DB 12 & DB 13), both daily newspapers.", "labels": [], "entities": [{"text": "Aftenposten (AP 06 & AP 08)", "start_pos": 95, "end_pos": 122, "type": "DATASET", "confidence": 0.8835378289222717}, {"text": "Dagbladet (DB 12 & DB 13)", "start_pos": 137, "end_pos": 162, "type": "DATASET", "confidence": 0.9075781181454659}]}, {"text": "Key statistics for the four texts are given in.", "labels": [], "entities": []}, {"text": "The annotation effort uses the TRED tool 2 , originally created for the Prague Dependency Treebank project.", "labels": [], "entities": [{"text": "TRED", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7472819089889526}, {"text": "Prague Dependency Treebank project", "start_pos": 72, "end_pos": 106, "type": "DATASET", "confidence": 0.9691885709762573}]}, {"text": "It is easily extended, and thus we used these facilities to collect the timing data.", "labels": [], "entities": []}, {"text": "To minimise interference with the annotators, we simply recorded the time a sentence was shown onscreen and accounted for outliers caused by breaks and interruptions in the analysis.", "labels": [], "entities": []}, {"text": "The annotation work is done by two annotators, Odin and Thor.", "labels": [], "entities": []}, {"text": "Both are trained linguists, and: Statistics of the annotated texts.", "labels": [], "entities": []}, {"text": "n number of sentences, \u00b5 mean length, s length standard deviation. are full-time employees of the National Library tasked with annotating the corpus.", "labels": [], "entities": [{"text": "National Library", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.9832352101802826}]}, {"text": "The only additional instruction given to the annotators in conjunction with the experiment was that they try to close the TRED program when they know that they were going away fora longtime, in order to minimise the number of outliers.", "labels": [], "entities": [{"text": "TRED", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.7531545758247375}]}, {"text": "The actual annotation proceeded as normal according to the annotation guidelines . Thor annotated AP 08 and DB 13, while Odin annotated AP 06 and DB 12 as well as the first 400 sentences of DB 13 for the purposes of measuring annotator agreement.", "labels": [], "entities": []}, {"text": "Preprocessing In our experiments, we consider three different statistical parsers as preprocessors and compare these to a minimally preprocessed baseline.", "labels": [], "entities": []}, {"text": "Unfortunately, it was impossible to get timing data for completely unannotated data, as TRED requires its input to be a dependency tree.", "labels": [], "entities": []}, {"text": "For this reason our minimal preprocessing, we call it the caterpillar strategy, is attaching each word to the previous word, labelled with the most frequent dependency relation.", "labels": [], "entities": []}, {"text": "Of the three statistical parsers, one is trained directly on already annotated Norwegian data released by the treebank project (version 0.2) and the other two are cross-lingual parsers trained on converted Swedish and Danish data using the techniques described in.", "labels": [], "entities": [{"text": "Norwegian data released by the treebank project", "start_pos": 79, "end_pos": 126, "type": "DATASET", "confidence": 0.7706900877611977}]}, {"text": "In brief, this technique involves mapping the PoS and dependency relation tagsets of the source corpora into the corresponding tagsets of the target representation, and applying structural transformations to bring the syntactic analyses into as close a correspondence as possible with the target analyses.", "labels": [], "entities": []}, {"text": "It was also shown that for languages as closely related as Norwegian, Danish and Swedish, not delexicalising, contrary to the  standard procedure in cross-lingual parsing, yields a non-negligible boost in performance.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 149, "end_pos": 170, "type": "TASK", "confidence": 0.7206836938858032}]}, {"text": "All three parsers are trained using MaltParser () using the liblinear learner and the nivreeager parsing algorithm with default settings.", "labels": [], "entities": []}, {"text": "The Norwegian parser is trained on the first 90% of the version 0.2 release of the Norwegian dependency treebank with the remaining 10% held out for evaluation, while the cross-lingual parsers are trained on the training sets of Talbanken05 () and the Danish Dependency Treebank ( as distributed for the CoNLL-X shared task.", "labels": [], "entities": [{"text": "Norwegian dependency treebank", "start_pos": 83, "end_pos": 112, "type": "DATASET", "confidence": 0.7711841265360514}, {"text": "Danish Dependency Treebank", "start_pos": 252, "end_pos": 278, "type": "DATASET", "confidence": 0.881190299987793}]}, {"text": "The parser trained on Swedish data is lexicalised, while the one trained on Danish used a delexicalised corpus.", "labels": [], "entities": []}, {"text": "The performance of the four different preprocessing strategies is summarised in.", "labels": [], "entities": []}, {"text": "The numbers are mostly inline with those reported in, with a drop of a few percentage points in both LAS and UAS for all parsers, except fora gain of more than 5 points LAS for the Danish parser, due to the fixed relation labels.", "labels": [], "entities": [{"text": "LAS", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9909858703613281}, {"text": "UAS", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.7732669115066528}, {"text": "LAS", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9897677302360535}, {"text": "Danish", "start_pos": 181, "end_pos": 187, "type": "DATASET", "confidence": 0.9284195899963379}]}, {"text": "There are three reasons for the differences: First of all, the test corpus is different; used the version 0.1 release of the Norwegian corpus, while we use version 0.2.", "labels": [], "entities": [{"text": "Norwegian corpus", "start_pos": 125, "end_pos": 141, "type": "DATASET", "confidence": 0.9638204276561737}]}, {"text": "Secondly, TRED requires that its input trees only have a single child of the root node, while MaltParser will attach unconnected subgraphs to the root node if the graph produced after consuming the whole input isn't connected.", "labels": [], "entities": []}, {"text": "Finally, TRED validates dependency relation labels strictly, which revealed a few bugs in the conversion script for the Danish data.", "labels": [], "entities": [{"text": "TRED", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.6041185259819031}, {"text": "Danish data", "start_pos": 120, "end_pos": 131, "type": "DATASET", "confidence": 0.9409943521022797}]}, {"text": "A postprocessing script corrects the invalid relations and attaches multiple children of the root node to the most appropriate child of the root.", "labels": [], "entities": []}, {"text": "The texts given to the annotators were an amalgam of the outputs of the four parsers, such that each block often sentences comes from the same parser.", "labels": [], "entities": []}, {"text": "Each chunk was randomly assigned to a parser, in such away that 5 chunks were parsed with the baseline strategy and the remaining chunks were evenly distributed between the remaining three parsers.", "labels": [], "entities": []}, {"text": "This strategy ensures as even a distribution between parsers as possible, while keeping the annotators blind to parser assignments.", "labels": [], "entities": []}, {"text": "We avoid the annotators knowing which parser was used, as this could subconciously bias their behaviour.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the annotated texts. n num- ber of sentences, \u00b5 mean length, s length standard  deviation.", "labels": [], "entities": [{"text": "\u00b5 mean length", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.7937360803286234}, {"text": "s length standard  deviation", "start_pos": 85, "end_pos": 113, "type": "METRIC", "confidence": 0.6579409763216972}]}, {"text": " Table 2: Parser performance. Labelled (LAS) and  unlabelled (UAS) attachment scores.", "labels": [], "entities": [{"text": "Labelled (LAS) and  unlabelled (UAS) attachment scores", "start_pos": 30, "end_pos": 84, "type": "METRIC", "confidence": 0.7019802033901215}]}, {"text": " Table 3: Annotator agreement. n sentences, unla- belled (UAS) and labelled (LAS) attachment.", "labels": [], "entities": [{"text": "unla- belled (UAS) and labelled (LAS) attachment", "start_pos": 44, "end_pos": 92, "type": "METRIC", "confidence": 0.6863463347156843}]}]}