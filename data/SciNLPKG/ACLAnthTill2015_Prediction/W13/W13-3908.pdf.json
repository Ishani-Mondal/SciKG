{"title": [{"text": "Improving Continuous Sign Language Recognition: Speech Recognition Techniques and System Design", "labels": [], "entities": [{"text": "Improving Continuous Sign Language Recognition", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8953038811683655}, {"text": "Speech Recognition", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7123237997293472}]}], "abstractContent": [{"text": "Automatic sign language recognition (ASLR) is a special case of automatic speech recognition (ASR) and computer vision (CV) and is currently evolving from using artificial lab-generated data to using 'real-life' data.", "labels": [], "entities": [{"text": "Automatic sign language recognition (ASLR)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8008244207927159}, {"text": "automatic speech recognition (ASR)", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.8452282349268595}]}, {"text": "Although ASLR still struggles with feature extraction, it can benefit from techniques developed for ASR.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.9701305031776428}, {"text": "feature extraction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7577637434005737}, {"text": "ASR", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9715662598609924}]}, {"text": "We present a large-vocabulary ASLR system that is able to recognize sentences in continuous sign language and uses features extracted from standard single-view video cameras without using additional equipment.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9391074180603027}]}, {"text": "ASR techniques such as the multi-layer-perceptron (MLP) tandem approach, speaker adaptation, pronunciation modelling, and parallel hidden Markov models are investigated.", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9699004888534546}, {"text": "speaker adaptation", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.8456045091152191}, {"text": "pronunciation modelling", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.9513268768787384}]}, {"text": "We evaluate the influence of each system component on the recognition performance.", "labels": [], "entities": []}, {"text": "On two publicly available large vocabulary databases representing lab-data (25 signer, 455 sign vocabulary, 19k sentence) and unconstrained 'real-life' sign language (1 signer, 266 sign vocabulary, 351 sentences) we can achieve 22.1% respectively 38.6% WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 253, "end_pos": 256, "type": "METRIC", "confidence": 0.9991326928138733}]}], "introductionContent": [{"text": "Sign languages are natural languages that develop in communities of deaf people around the world and vary from region to region.", "labels": [], "entities": []}, {"text": "A sign consists of manual and non-manual components that partly occur in parallel but are not perfectly synchronous.", "labels": [], "entities": []}, {"text": "Manual components comprise hand configuration, place of articulation, hand movement and hand orientation while non-manual components include body pose and facial expression.", "labels": [], "entities": [{"text": "hand orientation", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.6882463991641998}]}, {"text": "ASLR is a subfield of CV and ASR allowing methods of both worlds to be deployed but it also inherits their respective challenges.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.5962603688240051}]}, {"text": "Large inter-/intra-personal signing variability, strong coarticulation effects, context dependent classifier gestures, no agreed written form or phoneme-like definition in conjunction with partly parallel information streams, high signing speed inducing motion blur, missing features and the need for automatic hand and face tracking make video-based ASLR a notoriously challenging research field.", "labels": [], "entities": [{"text": "hand and face tracking", "start_pos": 311, "end_pos": 333, "type": "TASK", "confidence": 0.5685862153768539}]}, {"text": "Although ASLR is starting to tackle 'real-life' data, the majority of work in the community still focusses on the recognition of isolated signs, particularly in the context of gesture recognition.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.9769516587257385}, {"text": "gesture recognition", "start_pos": 176, "end_pos": 195, "type": "TASK", "confidence": 0.8273724615573883}]}, {"text": "Deng and Tsui and use parallel HMMs to recognize isolated signs in American Sign Language or Chinese Sign Language, respectively, achieving recognition accuracies over 90%.", "labels": [], "entities": [{"text": "recognition accuracies", "start_pos": 140, "end_pos": 162, "type": "METRIC", "confidence": 0.8958707749843597}]}, {"text": "Ong et al. use boosted sequential pattern trees to recognize isolated signs in British sign language (BSL) allowing to combine partly parallel, not perfectly synchronous, automatically mined phoneme-like units in the recognition process.", "labels": [], "entities": [{"text": "British sign language (BSL)", "start_pos": 79, "end_pos": 106, "type": "DATASET", "confidence": 0.8567647536595663}]}, {"text": "Pitsikalis et al. extract subunit definitions from linguistic annotation in HamNoSys, whereas Koller et al. employ an open SignWriting dictionary to produce and align linguistically meaningful subunits to signs in German sign language (GSL).", "labels": [], "entities": [{"text": "HamNoSys", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9716041684150696}]}, {"text": "However, in real tasks ASLR is more likely to face continuous signing, that is what this work focusses on.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.970987856388092}]}, {"text": "In this context, Cooper et al. compare boosted sequential pattern trees to HMMs using linguistically inspired subunits and 3D tracking information finding that the trees outperform HMMs for BSL.", "labels": [], "entities": [{"text": "BSL", "start_pos": 190, "end_pos": 193, "type": "TASK", "confidence": 0.8664306998252869}]}, {"text": "Forster et al. investigate techniques to combine not perfectly synchronous information streams within an HMMbased ASLR system finding that synchronization just at word boundaries improves the recognition performance.", "labels": [], "entities": []}, {"text": "Recognizing a sign language sentence by spotting individual signs has been investigated by several authors reporting promising results.", "labels": [], "entities": [{"text": "Recognizing a sign language sentence", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8538995504379272}]}, {"text": "Finally Yang et al. use a nested dynamic programming approach to handle coarticulation movements between signs.", "labels": [], "entities": []}, {"text": "Given the cited work and the works described in the survey on sign language recognition by Ong and Ranganath, two approaches to ASLR are observable.", "labels": [], "entities": [{"text": "sign language recognition", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.66569717725118}, {"text": "ASLR", "start_pos": 128, "end_pos": 132, "type": "TASK", "confidence": 0.9887233972549438}]}, {"text": "On the one hand, ASLR is viewed as a pure CV problem neglecting the natural language processing nature of the task and focussing on developing tailor-made solutions for gestures.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.9849337935447693}]}, {"text": "However, we believe to be soon able to tackle real-world problems, ASLR should much more be seen as application of ASR, exploiting previous knowledge gained in that area.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 67, "end_pos": 71, "type": "TASK", "confidence": 0.9724059700965881}, {"text": "ASR", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9841264486312866}]}, {"text": "Following that track, we provide systematically gathered knowledge on how to create a large vocabulary ASLR system for continuous SL evaluating which techniques from ASR are applicable.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 103, "end_pos": 107, "type": "TASK", "confidence": 0.9316422343254089}, {"text": "continuous SL", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.547466903924942}]}, {"text": "Specifically, we investigate the impact of CV and ASR techniques on the recognition performance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9632189273834229}]}, {"text": "Among others, the impact of the performance of automatic hand tracking on the recognition performance is investigated.", "labels": [], "entities": [{"text": "hand tracking", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.7417750656604767}]}, {"text": "Tackling the question of suitable features for nonrigid objects such as the hands, HoG3D features proposed in the area of action recognition, appearance based features and learned MLP features used in ASR are investigated.", "labels": [], "entities": [{"text": "action recognition", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7524467706680298}, {"text": "ASR", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.9856842756271362}]}, {"text": "Addressing inter signer variability, the technique of automatic signer adaptation is adopted from ASR (speaker adaptation) and tested within our proposed large-vocabulary, HMM-based sign language recognition system.", "labels": [], "entities": [{"text": "signer adaptation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6954735368490219}, {"text": "HMM-based sign language recognition", "start_pos": 172, "end_pos": 207, "type": "TASK", "confidence": 0.6445638164877892}]}, {"text": "Additionally, techniques to combine Pr(w N 1 ) Figure 1: Bayes' decision rule used in ASLR.", "labels": [], "entities": [{"text": "Pr", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9385106563568115}, {"text": "ASLR", "start_pos": 86, "end_pos": 90, "type": "TASK", "confidence": 0.8514009714126587}]}, {"text": "partly parallel information streams/modalities are presented and evaluated.", "labels": [], "entities": []}, {"text": "The system and its components are tested in the context of continuous ASLR for two publicly available, largevocabulary databases.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 70, "end_pos": 74, "type": "TASK", "confidence": 0.9164020419120789}]}, {"text": "One database represents lab-data created for pattern recognition purposes and one database represents 'real-life' data recorded from German public TV.", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.9073701500892639}]}, {"text": "Comparing findings on lab-data and 'real-life' data we investigate which findings on lab-data generalize to 'real-life' scenarios.", "labels": [], "entities": []}], "datasetContent": [{"text": "The SIGNUM database contains lab recordings of 25 signers wearing black long-sleeve clothes in front of a dark blue background signing predefined sentences.", "labels": [], "entities": [{"text": "SIGNUM database", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.740112692117691}]}, {"text": "Videos are recorded at 780 \u00d7 580 Pixel and 30 frames per second (fps).", "labels": [], "entities": []}, {"text": "Each signer signs the 603 unique training and 177 testing sentences once, whereas they are signed thrice in the single signer setup.", "labels": [], "entities": []}, {"text": "3.6% of the glosses are out of vocabulary (OOV).", "labels": [], "entities": [{"text": "OOV", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.8450083136558533}]}, {"text": "shows statistics of the single signer setup only.", "labels": [], "entities": []}, {"text": "The multi signer setup has the same vocabulary and OOV rate but 15k sentences (92k running glosses) for training and 4.4k sentences (23k running glosses) for testing.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.98943892121315}]}, {"text": "If not stated explicitly otherwise, all presented SIGNUM results refer to the single signer setup.", "labels": [], "entities": []}, {"text": "The PHOENIX database contains 'real-life' sign language footage recorded from weather-forecasts aired by the show the interpreter wearing dark clothes in front of an artificial gray gradient background and pose a strong challenge to CV and ASLR due to high signing speed (majority of signs spans less than 10 frames), strong coarticulation effects and more than 30% of the vocabulary being singletons.", "labels": [], "entities": [{"text": "PHOENIX database", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7817394733428955}]}, {"text": "Statistics of both databases are shown in.", "labels": [], "entities": []}, {"text": "The system is trained using maximum likelihood and the EM-algorithm.", "labels": [], "entities": []}, {"text": "The number of Gaussian densities and the LM-scale are optimized.", "labels": [], "entities": [{"text": "LM-scale", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8345834612846375}]}, {"text": "For PHOENIX, the system uses 1433 emission distributions with a total of 4k Gaussians and a globally pooled covariance matrix.", "labels": [], "entities": [{"text": "PHOENIX", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.708351731300354}]}, {"text": "The same applies to SIGNUM, but the numbers are 1366 emission distributions with 24k Gaussians for single signer and 198k for multi-signer.", "labels": [], "entities": []}, {"text": "Recognition uses word-conditioned tree search and Viterbi approximation.", "labels": [], "entities": []}, {"text": "Basic Features: In order to build a well performing ASLR system, the feature selection plays a crucial role.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.9600501656532288}]}, {"text": "The full video images can be seen as a global descriptor of manual and non-manual parameters and are, thus, a good starting point.", "labels": [], "entities": []}, {"text": "As the hands are known to carry the most information in signing, tracked and cutout hand patches have often been preferred overfull frames.", "labels": [], "entities": [{"text": "signing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.9661608934402466}]}, {"text": "Comparing both features, hand patches outperform full images on both databases (see, Row 1).", "labels": [], "entities": []}, {"text": "Model length estimation: In ASR, the HMM model of a word is formed by the linked models of the word's subunit HMMs.", "labels": [], "entities": [{"text": "Model length estimation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6341051558653513}, {"text": "ASR", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9708501100540161}]}, {"text": "Thereby, the typical temporal length of a word is modelled.", "labels": [], "entities": []}, {"text": "This approach is not yet possible in ASLR because the definition and extraction of subunits is still an open research question.", "labels": [], "entities": [{"text": "ASLR", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.9707061648368835}, {"text": "definition and extraction", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7016566495100657}]}, {"text": "PHOENIX includes word boundary annotations from which the number of segments for each gloss HMM can been estimated by choosing the median of the lengths minus 20% and adjusting the length in case the adapted median is shorter than the shortest utterance of the gloss.", "labels": [], "entities": [{"text": "PHOENIX", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8900400400161743}]}, {"text": "The hand patch baseline presented above uses this approach.", "labels": [], "entities": []}, {"text": "Using uniform length for all glosses, the recognition result is 60.8% instead of 55.0% WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9953438639640808}]}, {"text": "'Bootstrapping' the initial system alignment using the word boundary ground truth, we achieve 57.5% WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.999163031578064}]}, {"text": "No word boundary ground truth is available for SIGNUM.", "labels": [], "entities": [{"text": "SIGNUM", "start_pos": 47, "end_pos": 53, "type": "TASK", "confidence": 0.7089579105377197}]}, {"text": "Model length estimation is performed using statistics on the We have manually annotated the variants with regard to the visual appearance and the motion of the hand yielding on average 2.7 variants per gloss and a total of 711 different variants.", "labels": [], "entities": [{"text": "Model length estimation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6069895724455515}]}, {"text": "Using these annotations, each variant is modelled by a distinct HMM with model length estimation achieving 56.5% WER in contrast to the baseline of 55.0%.", "labels": [], "entities": [{"text": "model length estimation", "start_pos": 73, "end_pos": 96, "type": "METRIC", "confidence": 0.6259419520696005}, {"text": "WER", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9981198906898499}]}, {"text": "Further, both systems outperform the 62.2% WER of a 'nearest-neighbor' style system where each gloss occurrence is modelled independently.", "labels": [], "entities": [{"text": "WER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9937188625335693}]}, {"text": "Apparently, increasing the number of dedicated HMMs per gloss worsens recognition.", "labels": [], "entities": [{"text": "recognition", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9162945747375488}]}, {"text": "Coherent manual definition of variants is likely to be a problematic factor, as well as the HMMs not generalizing well over unseen data because of the reduction in training data per HMM and strong coarticulation effects.", "labels": [], "entities": []}, {"text": "Tracking Influence: The presented hand patch baselines rely on tracking to localize the hands of the signer.", "labels": [], "entities": []}, {"text": "Tracking is not perfect and errors propagate through the recognition system.", "labels": [], "entities": [{"text": "Tracking", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8317779898643494}]}, {"text": "shows the impact of tracking quality measured in tracking error rate (TrackEr) counting a tracked position as wrong if it differs by more than 20 Pixel from ground truth on ASLR for PHOENIX.", "labels": [], "entities": [{"text": "tracking error rate (TrackEr)", "start_pos": 49, "end_pos": 78, "type": "METRIC", "confidence": 0.8515677054723104}]}, {"text": "The TrackEr of 0 at 48.3% WER refers to using ground truth tracking annotation (see, Row 9).", "labels": [], "entities": [{"text": "TrackEr", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9910303354263306}, {"text": "WER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9915317893028259}]}, {"text": "HoG3D: HoG3D features encode the shape and its changeover time of a tracked hand.", "labels": [], "entities": [{"text": "HoG3D", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9211928844451904}]}, {"text": "The latter aspect is not covered by hand patch features.", "labels": [], "entities": []}, {"text": "Further, HoG3D features are more compact than hand patches, and robust against local illumination changes.", "labels": [], "entities": [{"text": "HoG3D", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.8855347633361816}]}, {"text": "Comparing to the hand patch baselines, recognition results are improved from 55.0% to 49.7% WER for PHOENIX and from 16.0% to 12.5% WER for SIGNUM.", "labels": [], "entities": [{"text": "recognition", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9565104246139526}, {"text": "WER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9971350431442261}, {"text": "WER", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9955961108207703}]}, {"text": "The result on PHOENIX is almost as good as using ground truth tracking information for the hand patches.", "labels": [], "entities": [{"text": "PHOENIX", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.498176634311676}]}, {"text": "Temporal Context: The temporal context of a feature includes information that cannot easily be learned by an HMM system but has been shown to improve results in ASR.", "labels": [], "entities": [{"text": "Temporal Context", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7939390242099762}, {"text": "ASR", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9806641340255737}]}, {"text": "Although HoG3D features already incorporate temporal context, we find that including additional context benefits the recognition, as can be seen in.", "labels": [], "entities": []}, {"text": "More context than \u00b12 frames degrades recognition accuracy on PHOENIX, capturing too much information of the following glosses.", "labels": [], "entities": [{"text": "recognition", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.8993672728538513}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9415832757949829}, {"text": "PHOENIX", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.8411152362823486}]}, {"text": "On SIGNUM, we observe only marginal recognition improvement indicating that the context included in HoG3D is sufficient.", "labels": [], "entities": [{"text": "HoG3D", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9390969276428223}]}, {"text": "The chosen system defaults are at \u00b12 frames for PHOENIX and \u00b14 frames for SIGNUM and are, thus, well chosen for both cases.", "labels": [], "entities": [{"text": "PHOENIX", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.8202492594718933}]}, {"text": "Modalities: In addition to the body pose (full image) and the right hand (HoG3D), we evaluate the performance using facial expressions (POIAAM), the left hand (HoG3D) and the movement of the right hand (Traj).", "labels": [], "entities": [{"text": "HoG3D", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9123924374580383}, {"text": "POIAAM", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9356998205184937}, {"text": "HoG3D", "start_pos": 160, "end_pos": 165, "type": "DATASET", "confidence": 0.9110130071640015}, {"text": "Traj)", "start_pos": 203, "end_pos": 208, "type": "METRIC", "confidence": 0.9711639583110809}]}, {"text": "For both databases, the left hand tracking quality is worse than the right hand.", "labels": [], "entities": []}, {"text": "Henceforth ground truth tracking annotations are used for PHOENIX to avoid tracking bias.", "labels": [], "entities": [{"text": "ground truth tracking", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6145539581775665}]}, {"text": "Thus, the HoG3D baseline improves to 45.2% WER.", "labels": [], "entities": [{"text": "HoG3D baseline", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.6779420375823975}, {"text": "WER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9969273209571838}]}, {"text": "Using left hand features 63.9% respectively 51.0% WER are achieved for PHOENIX and SIGNUM.", "labels": [], "entities": [{"text": "WER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9994435906410217}, {"text": "PHOENIX", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.7804746031761169}]}, {"text": "The stronger recognition degradation for PHOENIX reflects the difficulty of the database.", "labels": [], "entities": [{"text": "PHOENIX", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.8107931613922119}]}, {"text": "With facial features, the recognition result is 62.6% respectively 89.3% WER for PHOENIX and SIGNUM.", "labels": [], "entities": [{"text": "recognition", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9711597561836243}, {"text": "WER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9996299743652344}, {"text": "PHOENIX", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.7290706634521484}]}, {"text": "The high WER for SIGNUM is due to the fact that hardly any facial expressions are present here.", "labels": [], "entities": [{"text": "WER", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9996243715286255}, {"text": "SIGNUM", "start_pos": 17, "end_pos": 23, "type": "TASK", "confidence": 0.6146078705787659}]}, {"text": "Concatenating movement trajectory and right hand HoG3D, results are improved for PHOENIX but not for SIGNUM (Table 2, Row 3).", "labels": [], "entities": [{"text": "HoG3D", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7920166850090027}, {"text": "PHOENIX", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.7098355293273926}]}, {"text": "Using synchronous, Row 4) and asynchronous (Table 2, Row 5) modality combination techniques, recognition results for both databases are improved if the respectively best single modalities are combined.", "labels": [], "entities": []}, {"text": "For a full overview of modality combination techniques and results refer to.", "labels": [], "entities": []}, {"text": "Gap Models: The SIGNUM database is designed to contain only one-handed signs and no switching of the hand.", "labels": [], "entities": []}, {"text": "Contrarily, in PHOENIX signers partly switch hands and use the left hand for signing while holding the right.", "labels": [], "entities": [{"text": "PHOENIX signers", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.5996926426887512}, {"text": "signing", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9678230285644531}]}, {"text": "This effect introduces missing features in the information stream of the right and left hand.", "labels": [], "entities": []}, {"text": "One way to remedy this problem is to borrow the idea of noise models from ASR and to augment the system's vocabulary by two such models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9054767489433289}]}, {"text": "One model subsuming signs performed by the left hand only and one for long gaps between signs of more than five frames that are part of the sentence but do not belong to either neighboring sign.", "labels": [], "entities": []}, {"text": "The training data annotation is automatically augmented by labels for both aspects using ground truth annotation.", "labels": [], "entities": []}, {"text": "Using these gap models, the WER is improved from 42.1% to 39.8% on PHOENIX, due to the models only being populated with clean and complete data.", "labels": [], "entities": [{"text": "WER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9767469167709351}, {"text": "PHOENIX", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.8679749965667725}]}, {"text": "Further, we observe an improved feature to HMM state alignment (measured as distance to the ground truth annotation).", "labels": [], "entities": [{"text": "HMM state alignment", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.671526829401652}]}], "tableCaptions": [{"text": " Table 1: Statistics for SIGNUM single signer and PHOENIX  SIGNUM  PHOENIX  Train  Test Train Test  # sentences  1809  531  304  47  # running glosses  11,109 2805  3309  487  vocabulary size  455  - 266  - # singletons  0  - 90  - # OOV [%]  - 3.6  - 1.6  perplexity (3-gram)  17.8  72.2  15.9 34.9", "labels": [], "entities": [{"text": "SIGNUM single signer", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7153013944625854}, {"text": "PHOENIX  SIGNUM  PHOENIX  Train  Test Train Test  # sentences  1809  531  304", "start_pos": 50, "end_pos": 127, "type": "DATASET", "confidence": 0.7330201603472233}, {"text": "OOV", "start_pos": 234, "end_pos": 237, "type": "METRIC", "confidence": 0.8670868873596191}]}, {"text": " Table 2: WERs for competing features (Rows 1.-6.), WERs  without and with specific techniques (Rows 7.-11.). '+' denotes  a synchronous, asynchronous or feature combination. Please  see corresponding text parts for explanations. HoG3D uses  tracked hand locations. For PHOENIX, in rows 3.-5., manual  ground truth annotation has been used instead.", "labels": [], "entities": [{"text": "WERs", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9535754323005676}]}, {"text": " Table 3: Class LM results for PHOENIX. Error rates in %.  Class  del/ins  WER  None  20.7/4.5  39.8  Orientation  18.1/5.3  39.2  Numbers  19.3/4.1  38.8  + Orientation 16.2/6.2  38.6", "labels": [], "entities": [{"text": "PHOENIX", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.6524881720542908}, {"text": "Error", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9968867897987366}, {"text": "WER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.885714590549469}]}]}