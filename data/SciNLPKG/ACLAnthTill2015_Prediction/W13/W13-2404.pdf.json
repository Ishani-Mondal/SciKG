{"title": [{"text": "Aspect-Oriented Opinion Mining from User Reviews in Croatian", "labels": [], "entities": [{"text": "Aspect-Oriented Opinion Mining", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7383063435554504}]}], "abstractContent": [{"text": "Aspect-oriented opinion mining aims to identify product aspects (features of products) about which opinion has been expressed in the text.", "labels": [], "entities": [{"text": "Aspect-oriented opinion mining", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8185121019681295}]}, {"text": "We present an approach for aspect-oriented opinion mining from user reviews in Croatian.", "labels": [], "entities": [{"text": "aspect-oriented opinion mining", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.6286738614241282}]}, {"text": "We propose methods for acquiring a domain-specific opinion lexicon, linking opinion clues to product aspects, and predicting polarity and rating of reviews.", "labels": [], "entities": [{"text": "predicting polarity and rating of reviews", "start_pos": 114, "end_pos": 155, "type": "TASK", "confidence": 0.7644243538379669}]}, {"text": "We show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions .", "labels": [], "entities": []}], "introductionContent": [{"text": "For companies, knowing what customers think of their products and services is essential.", "labels": [], "entities": []}, {"text": "Opinion mining is being increasingly used to automatically recognize opinions about products in natural language texts.", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7685637176036835}, {"text": "recognize opinions about products in natural language texts", "start_pos": 59, "end_pos": 118, "type": "TASK", "confidence": 0.6470695473253727}]}, {"text": "Numerous approaches to opinion mining have been proposed, ranging from domainspecific) to cross-domain approaches, and from lexicon-based methods () to machine learning approaches (.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.8641016483306885}]}, {"text": "While early attempts focused on classifying overall document opinion), more recent approaches identify opinions expressed about individual product aspects (.", "labels": [], "entities": []}, {"text": "Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization for individual aspects.", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.7106576263904572}]}, {"text": "Furthermore, opinionated aspects maybe useful for predicting overall review polarity and rating.", "labels": [], "entities": []}, {"text": "While many opinion mining systems and resources have been developed for major languages, there has been considerably less development for less prevalent languages, such as Croatian.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7183169424533844}]}, {"text": "In this paper we present a method for domain-specific, aspect-oriented opinion mining from user reviews in Croatian.", "labels": [], "entities": [{"text": "aspect-oriented opinion mining", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.6627295116583506}]}, {"text": "We address two tasks: (1) identification of opinion expressed about individual product aspects and (2) predicting the overall opinion expressed by a review.", "labels": [], "entities": [{"text": "identification of opinion expressed about individual product", "start_pos": 26, "end_pos": 86, "type": "TASK", "confidence": 0.8451571805136544}]}, {"text": "We assume that solving the first task successfully will help improve the performance on the second task.", "labels": [], "entities": []}, {"text": "We propose a simple semi-automated approach for acquiring domainspecific lexicon of opinion clues and prominent product aspects.", "labels": [], "entities": []}, {"text": "We use supervised machine learning to detect the links between opinion clues (e.g., excellent, horrible) and product aspects (e.g., pizza, delivery).", "labels": [], "entities": []}, {"text": "We conduct preliminary experiments on restaurant reviews and show that our method can successfully pair opinion clues with the targeted aspects.", "labels": [], "entities": []}, {"text": "Furthermore, we show that the extracted clues and opinionated aspects help classify review polarity and predict user-assigned ratings.", "labels": [], "entities": []}], "datasetContent": [{"text": "For experimental evaluation, we acquired a domain-specific dataset of restaurant reviews 2 from (HR) Zaista za svaku pohvalu!", "labels": [], "entities": []}, {"text": "Jelo su nam dostavili 15 minuta ranije.", "labels": [], "entities": []}, {"text": "Naru\u010dili smo pizzu koja je bila prepuna dodataka, dobro pe\u010dena, i vrlo ukusna.", "labels": [], "entities": []}, {"text": "(EN) Really laudable!", "labels": [], "entities": []}, {"text": "Food was delivered 15 minutes early.", "labels": [], "entities": []}, {"text": "We ordered pizza which was filled with extras, well-baked, and very tasteful.", "labels": [], "entities": []}, {"text": "Rating: 6/6 . We use these userassigned ratings as gold-standard labels for supervised learning.", "labels": [], "entities": []}, {"text": "shows an example of a review (clues are bolded and aspects are underlined).", "labels": [], "entities": []}, {"text": "We split the dataset into a development and a test set (7:3 ratio) and use the former for lexicon acquisition and model training.", "labels": [], "entities": [{"text": "lexicon acquisition", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7332250773906708}]}, {"text": "Experiment 1: Opinionated aspects.", "labels": [], "entities": []}, {"text": "To build a set on which we can train the aspect-clue pairing model, we sampled 200 reviews from the development set and extracted from each sentence all possible aspect-clue pairs.", "labels": [], "entities": []}, {"text": "We obtained 1406 aspect-clue instances, which we then manually labeled as either paired or not paired.", "labels": [], "entities": []}, {"text": "Similarly for the test set, we annotated 308 aspect-clue instances extracted from a sample of 70 reviews.", "labels": [], "entities": []}, {"text": "Among the extracted clues, 77% are paired with at least one aspect and 23% are unpaired (the aspect is implicit).", "labels": [], "entities": []}, {"text": "We trained a support vector machine (SVM) with radial basis kernel and features described in Section 3.", "labels": [], "entities": []}, {"text": "We optimized the model using 10-fold crossvalidation on the training set.", "labels": [], "entities": []}, {"text": "The baseline assigns to each aspect the closest opinion clue within the same sentence.", "labels": [], "entities": []}, {"text": "We use stratified shuffling test) to determine statistical significance of performance differences.", "labels": [], "entities": []}, {"text": "All of our supervised models significantly outperform the closest clue baseline (p < 0.01).", "labels": [], "entities": []}, {"text": "The Basic+Lex+POS+Synt model outperforms Basic model (F-score difference is statistically significant at p < 0.01), while the F-score differences between Basic and both Basic+Lex and Basic+Lex+POS are pairwise significant at p < 0.05.", "labels": [], "entities": [{"text": "F-score difference", "start_pos": 54, "end_pos": 72, "type": "METRIC", "confidence": 0.9853718876838684}, {"text": "F-score", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9932841062545776}]}, {"text": "The F-score: Review polarity and rating performance differences between Basic+Lex, Basic+Lex+POS, and Basic+Lex+POS+Synt are pairwise not statistically significant (p < 0.05).", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9980644583702087}, {"text": "Review", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9880123138427734}, {"text": "Basic+Lex", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.8571658134460449}]}, {"text": "This implies that linguistic features increase the classification performance, but there are no significant differences between models employing different linguistic feature sets.", "labels": [], "entities": []}, {"text": "We also note that improvements over the Basic model are not as large as we expected; we attribute this to the noisy user-generated text and the limited size of the training set.", "labels": [], "entities": []}, {"text": "Experiment 2: Overall review opinion.", "labels": [], "entities": []}, {"text": "We considered two models: a classification model for predicting review polarity and a regression model for predicting user-assigned rating.", "labels": [], "entities": [{"text": "predicting review polarity", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.8359173734982809}]}, {"text": "We trained the models on the full development set (2276 reviews) and evaluated on the full test set (1034 reviews).", "labels": [], "entities": []}, {"text": "For the classification task, we consider reviews rated lower than 2.5 as negative and those rated higher than 4 as positive.", "labels": [], "entities": [{"text": "classification", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.9609211087226868}]}, {"text": "Ratings between 2.5 and 4 are mostly inconsistent (assigned to both positive and negative reviews), thus we did not consider reviews with these ratings.", "labels": [], "entities": []}, {"text": "For classification, we used SVM with radial basis kernel, while for regression we used support vector regression (SVR) model.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9629982113838196}]}, {"text": "We optimized both models using 10-fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": "shows performance of models with different feature sets.", "labels": [], "entities": []}, {"text": "The model with bag-of-words features (BoW) is the baseline.", "labels": [], "entities": []}, {"text": "For polarity classification, we report F1-scores for positive and negative class.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8705751001834869}, {"text": "F1-scores", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9985511898994446}]}, {"text": "For rating prediction, we report Pearson correlation (r) and mean average error (MAE).", "labels": [], "entities": [{"text": "rating prediction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9200995862483978}, {"text": "Pearson correlation (r)", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.9427204132080078}, {"text": "mean average error (MAE)", "start_pos": 61, "end_pos": 85, "type": "METRIC", "confidence": 0.951011727253596}]}, {"text": "The models that use opinion clue features (BoW+E+C) or opinionated aspect features (BoW+E+A and BoW+E+A+C) outperform the baseline model (difference in classification and regression performance is significant at p < 0.05 and p < 0.01, respectively; tested using stratified shuffling test).", "labels": [], "entities": []}, {"text": "This confirms our assumption that opinion clues and opinionated aspects improve the prediction of overall review opinion.", "labels": [], "entities": []}, {"text": "Performance on negative reviews is consistently lower than for positive reviews; this can be ascribed to the fact that the dataset is biased toward positive reviews.", "labels": [], "entities": []}, {"text": "Models BoW+E+A and BoW+E+C perform similarly (the difference is not statistically significant at p < 0.05), suggesting that opinion clues improve the performance just as much as opinionated aspects.", "labels": [], "entities": []}, {"text": "We believe this is due to (1) the existence of a considerable number (23%) of unpaired opinion clues (e.g., u\u017easno (terrible) in \"Bilo je u\u017easno!\" (\"It was terrible!\")) and (2) the fact that most opinionated aspects inherit the prior polarity of the clue that targets them (also supported by the fact the BoW+E+A+C model does not significantly outperform the BoW+E+C nor the BoW+E+A models).", "labels": [], "entities": [{"text": "BoW+E+A+C", "start_pos": 305, "end_pos": 314, "type": "DATASET", "confidence": 0.8880419731140137}, {"text": "BoW+E+C", "start_pos": 359, "end_pos": 366, "type": "DATASET", "confidence": 0.9205676317214966}, {"text": "BoW+E+A", "start_pos": 375, "end_pos": 382, "type": "DATASET", "confidence": 0.9110251069068909}]}, {"text": "Moreover, note that, in general, user-assigned ratings may deviate from the opinions expressed in text (e.g., because some users chose to comment only on some aspects).", "labels": [], "entities": []}, {"text": "However, the issue of annotation quality is out of scope and we leave it for future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Aspect-clue pairing performance", "labels": [], "entities": [{"text": "Aspect-clue pairing", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6650467664003372}]}, {"text": " Table 3: Review polarity and rating performance", "labels": [], "entities": [{"text": "Review", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9480488300323486}]}]}