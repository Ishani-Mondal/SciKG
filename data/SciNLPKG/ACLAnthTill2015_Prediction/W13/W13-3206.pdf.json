{"title": [{"text": "General estimation and evaluation of compositional distributional semantic models", "labels": [], "entities": []}], "abstractContent": [{"text": "In recent years, there has been widespread interest in compositional distributional semantic models (cDSMs), that derive meaning representations for phrases from their parts.", "labels": [], "entities": []}, {"text": "We present an evaluation of alternative cDSMs under truly comparable conditions.", "labels": [], "entities": []}, {"text": "In particular, we extend the idea of Baroni and Zamparelli (2010) and Guevara (2010) to use corpus-extracted examples of the target phrases for parameter estimation to the other models proposed in the literature, so that all models can be tested under the same training conditions.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.6516098231077194}]}, {"text": "The linguistically motivated functional model of Baroni and Zamparelli (2010) and Coecke et al.", "labels": [], "entities": []}, {"text": "(2010) emerges as the winner in all our tests.", "labels": [], "entities": []}], "introductionContent": [{"text": "The need to assess similarity in meaning is central to many language technology applications, and distributional methods are the most robust approach to the task.", "labels": [], "entities": []}, {"text": "These methods measure word similarity based on patterns of occurrence in large corpora, following the intuition that similar words occur in similar contexts.", "labels": [], "entities": []}, {"text": "More precisely, vector space models, the most widely used distributional models, represent words as high-dimensional vectors, where the dimensions represent (functions of) context features, such as co-occurring context words.", "labels": [], "entities": []}, {"text": "The relatedness of two words is assessed by comparing their vector representations.", "labels": [], "entities": []}, {"text": "The question of assessing meaning similarity above the word level within the distributional paradigm has received a lot of attention in recent years.", "labels": [], "entities": []}, {"text": "A number of compositional frameworks have been proposed in the literature, each of these defining operations to combine word vectors into representations for phrases or even entire sentences.", "labels": [], "entities": []}, {"text": "These range from simple but robust methods such as vector addition to more advanced methods, such as learning function words as tensors and composing constituents through inner product operations.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7744815647602081}]}, {"text": "Empirical evaluations in which alternative methods are tested in comparable settings are thus called for.", "labels": [], "entities": []}, {"text": "This is complicated by the fact that the proposed compositional frameworks package together a number of choices that are conceptually distinct, but difficult to disentangle.", "labels": [], "entities": []}, {"text": "Broadly, these concern (i) the input representations fed to composition; (ii) the composition operation proper; (iii) the method to estimate the parameters of the composition operation.", "labels": [], "entities": []}, {"text": "have recently highlighted the importance of teasing apart the different aspects of a composition framework, presenting an evaluation in which different input vector representations are crossed with different composition methods.", "labels": [], "entities": []}, {"text": "However, two out of three composition methods they evaluate are parameter-free, so that they can side-step the issue of fixing the parameter estimation method.", "labels": [], "entities": []}, {"text": "In this work, we evaluate all composition methods we know of, excluding a few that lag be-hind the state of the art or are special cases of those we consider, while keeping the estimation method constant.", "labels": [], "entities": []}, {"text": "This evaluation is made possible by our extension to all target composition models of the corpus-extracted phrase approximation method originally proposed in ad-hoc settings by and.", "labels": [], "entities": []}, {"text": "For the models for which it is feasible, we compare the phrase approximation approach to supervised estimation with crossvalidation, and show that phrase approximation is competitive, thus confirming that we are not comparing models under poor training conditions.", "labels": [], "entities": [{"text": "phrase approximation", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7308743894100189}, {"text": "phrase approximation", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.7629913091659546}]}, {"text": "Our tests are conducted over three tasks that involve different syntactic constructions and evaluation setups.", "labels": [], "entities": []}, {"text": "Finally, we consider a range of parameter settings for the input vector representations, to insure that our results are not too brittle or parameter-dependent.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the composition methods on three phrase-based benchmarks that test the models on a variety of composition processes and similaritybased tasks.", "labels": [], "entities": []}, {"text": "Intransitive sentences The first dataset, introduced by, focuses on simple sentences consisting of intransitive verbs and their noun subjects.", "labels": [], "entities": []}, {"text": "It contains a total of 120 sentence pairs together with human similarity judgments on a 7-point scale.", "labels": [], "entities": []}, {"text": "For example, conflict erupts/conflict bursts is scored 7, skin glows/skin burns is scored 1.", "labels": [], "entities": []}, {"text": "On average, each pair is rated by 30 participants.", "labels": [], "entities": []}, {"text": "Rather than evaluating against mean scores, we use each rating as a separate data point, as done by Mitchell and Lapata.", "labels": [], "entities": []}, {"text": "We report Spearman correlations between human-assigned scores and model cosine scores.", "labels": [], "entities": []}, {"text": "Adjective-noun phrases Turney (2012) introduced a dataset including both noun-noun compounds and adjective-noun phrases (ANs).", "labels": [], "entities": []}, {"text": "We focus on the latter, and we frame the task differently from Turney's original definition due to data sparsity issues.", "labels": [], "entities": []}, {"text": "In our version, the dataset contains 620 ANs, each paired with a singlenoun paraphrase.", "labels": [], "entities": []}, {"text": "Examples include: archaeological site/dig, spousal relationship/marriage and dangerous undertaking/adventure.", "labels": [], "entities": []}, {"text": "We evaluate a model by computing the cosine of all 20K nouns in our semantic space with the target AN, and looking at the rank of the correct paraphrase in this list.", "labels": [], "entities": []}, {"text": "The lower the rank, the better the model.", "labels": [], "entities": []}, {"text": "We report median rank across the test items.", "labels": [], "entities": [{"text": "median rank", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9466489255428314}]}, {"text": "Determiner phrases The last dataset, introduced in, focuses on a class of grammatical terms (rather than content: Fulllex estimation problem. words), namely determiners.", "labels": [], "entities": []}, {"text": "It is a multiplechoice test where target nouns (e.g., amnesia) must be matched with the most closely related determiner(-noun) phrases (DPs) (e.g., no memory).", "labels": [], "entities": []}, {"text": "The task differs from the previous one also because here the targets are single words, and the related items are composite.", "labels": [], "entities": []}, {"text": "There are 173 target nouns in total, each paired with one correct DP response, as well as 5 foils, namely the determiner (no) and noun (memory) from the correct response and three more DPs, two of which contain the same noun as the correct phrase (less memory, all memory), the third the same determiner (no repertoire).", "labels": [], "entities": []}, {"text": "Other examples of targets/relatedphrases are polysemy/several senses and trilogy/three books.", "labels": [], "entities": []}, {"text": "The models compute cosines between target noun and responses and are scored based on their accuracy at ranking the correct phrase first.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9984297156333923}]}, {"text": "We begin with some remarks pertaining to the overall quality of and motivation for corpusphrase-based estimation.", "labels": [], "entities": [{"text": "corpusphrase-based estimation", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.7412284016609192}]}, {"text": "In seven out of nine comparisons of this unsupervised technique with fully supervised crossvalidation (3 \"simple\" models -Add, Dil and Mult-times 3 test sets), there was no significant difference between the two estimation methods.", "labels": [], "entities": []}, {"text": "10 Supervised estimation outperformed the corpus-phrase-based method only for Dil on the intransitive sentence and AN benchmarks, but crossvalidated Dil was outperformed by at least one phrase-estimated simple model on both benchmarks.", "labels": [], "entities": []}, {"text": "The rightmost boxes in the panels of Figure 2 depict the performance distribution for using phrase vectors directly extracted from the corpus to tackle the various tasks.", "labels": [], "entities": []}, {"text": "This noncompositional approach outperforms all compositional methods in two tasks over three, and it is one of the best approaches in the third, although Significance assessed through Tukey Honestly Significant Difference tests (, \u03b1 = 0.05.", "labels": [], "entities": []}, {"text": "in all cases even its top scores are far from the theoretical ceiling.", "labels": [], "entities": []}, {"text": "Still, performance is impressive, especially in light of the fact that the noncompositional approach suffers of serious datasparseness problems.", "labels": [], "entities": []}, {"text": "Performance on the intransitive task is above state-of-the-art despite the fact that for almost half of the cases one test phrase is not in the corpus, resulting in 0 vectors and consequently 0 similarity pairs.", "labels": [], "entities": []}, {"text": "The other benchmarks have better corpus-phrase coverage (nearly perfect AN coverage; for DPs, about 90% correct phrase responses are in the corpus), but many target phrases occur only rarely, leading to unreliable distributional vectors.", "labels": [], "entities": [{"text": "AN coverage", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9169873893260956}]}, {"text": "We interpret these results as a good motivation for corpus-phrase-based estimation.", "labels": [], "entities": []}, {"text": "On the one hand they show how good these vectors are, and thus that they are sensible targets of learning.", "labels": [], "entities": []}, {"text": "On the other hand, they do not suffice, since natural language is infinitely productive and thus no corpus can provide full phrase coverage, justifying the whole compositional enterprise.", "labels": [], "entities": []}, {"text": "The other boxes in report the performance of the composition methods trained by corpus phrase approximation.", "labels": [], "entities": [{"text": "corpus phrase approximation", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6401472290356954}]}, {"text": "Nearly all models are significantly above chance in all tasks, except for Fulladd on intransitive sentences.", "labels": [], "entities": []}, {"text": "To put AN median ranks into perspective, consider that a median rank as high as 8,300 has near-0 probability to occur by chance.", "labels": [], "entities": []}, {"text": "For DP accuracy, random guessing gets 0.17% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.8656178116798401}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9985305070877075}]}, {"text": "Lexfunc emerges consistently as the best model.", "labels": [], "entities": [{"text": "Lexfunc", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9548004865646362}]}, {"text": "On intransitive constructions, it significantly outperforms all other models except Mult, but the difference approaches significance even with respect to the latter (p = 0.071).", "labels": [], "entities": [{"text": "Mult", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.5843755006790161}]}, {"text": "On this task, Lexfunc's median correlation (0.26) is nearly equivalent to the best correlation across a wide range of parameters reported by.", "labels": [], "entities": [{"text": "Lexfunc's median correlation", "start_pos": 14, "end_pos": 42, "type": "METRIC", "confidence": 0.654305711388588}]}, {"text": "In the AN task, Lexfunc significantly outperforms Fulllex and Dil and, visually, its distribution is slightly more skewed towards lower (better) ranks than any other model.", "labels": [], "entities": [{"text": "AN task", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9084950983524323}, {"text": "Fulllex", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9289908409118652}]}, {"text": "In the DP task, Lexfunc significantly outperforms Add and Mult and, visually, most of its distribution lies above that of the other models.", "labels": [], "entities": [{"text": "DP task", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.8806270062923431}, {"text": "Mult", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.6611111760139465}]}, {"text": "Most importantly, Lexfunc is the only model that is consistent across the three tasks, with all other models displaying instead a brittle performance pattern.", "labels": [], "entities": [{"text": "Lexfunc", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9155873656272888}]}, {"text": "Still, the top-performance range of all models: Boxplots displaying composition model performance distribution on three benchmarks, across input vector settings (6 datapoints for Mult, 12 for all other models).", "labels": [], "entities": [{"text": "Mult", "start_pos": 179, "end_pos": 183, "type": "DATASET", "confidence": 0.8539126515388489}]}, {"text": "For intransitive sentences, figure of merit is Spearman correlation, for ANs median rank of correct paraphrase, and for DPs correct response accuracy.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 47, "end_pos": 67, "type": "METRIC", "confidence": 0.9076967537403107}, {"text": "ANs median rank", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.9606993993123373}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.6061266660690308}]}, {"text": "The boxplots display the distribution median as a thick horizontal line within a box extending from first to third quartile.", "labels": [], "entities": []}, {"text": "Whiskers cover 1.5 of interquartile range in each direction from the box, and extreme outliers outside this extended range are plotted as circles.", "labels": [], "entities": []}, {"text": "on the three tasks is underwhelming, and none of them succeeds in exploiting compositionality to do significantly better than using whatever phrase vectors can be extracted from the corpus directly.", "labels": [], "entities": []}, {"text": "Clearly, much work is still needed to develop truly successful cDSMs.", "labels": [], "entities": []}, {"text": "The AN results might look particularly worrying, considering that even the top (lowest) median ranks are above 100.", "labels": [], "entities": [{"text": "AN", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9970917701721191}]}, {"text": "A qualitative analysis, however, suggests that the actual performance is not as bad as the numerical scores suggest, since often the nearest neighbours of the ANs to be paraphrased are nouns that are as strongly related to the ANs as the gold standard response (although not necessarily proper paraphrases).", "labels": [], "entities": []}, {"text": "For example, the gold response to colorimetric analysis is colorimetry, whereas the Lexfunc (NMF, 300 dimensions) nearest neighbour is chromatography; the gold response to heavy particle is baryon, whereas Lexfunc proposes muon; for melodic phrase the gold is tune and Lexfunc has appoggiatura; for indoor garden, the gold is hothouse but Lexfunc proposes glasshouse (followed by the more sophisticated orangery!), and soon and so forth.", "labels": [], "entities": [{"text": "Lexfunc", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.8984319567680359}]}], "tableCaptions": []}