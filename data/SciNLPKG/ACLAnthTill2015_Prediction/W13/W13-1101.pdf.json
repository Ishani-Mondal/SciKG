{"title": [{"text": "Does Size Matter? Text and Grammar Revision for Parsing Social Media Data", "labels": [], "entities": [{"text": "Text and Grammar Revision", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.5699011832475662}]}], "abstractContent": [{"text": "We explore improving parsing social media and other web data by altering the input data, namely by normalizing web text, and by revising output parses.", "labels": [], "entities": [{"text": "parsing social media and other web", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.8868138094743093}]}, {"text": "We find that text normal-ization improves performance, though spell checking has more of a mixed impact.", "labels": [], "entities": [{"text": "spell checking", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.8409757614135742}]}, {"text": "We also find that a very simple tree reviser based on grammar comparisons performs slightly but significantly better than the baseline and well outperforms a machine learning model.", "labels": [], "entities": []}, {"text": "The results also demonstrate that, more than the size of the training data, the goodness of fit of the data has a great impact on the parser.", "labels": [], "entities": [{"text": "goodness of fit", "start_pos": 80, "end_pos": 95, "type": "METRIC", "confidence": 0.8970290223757426}]}], "introductionContent": [], "datasetContent": [{"text": "We report three major sets of experiments: the first set compares the two parse revision strategies; the second looks into text normalization strategies; and the third set investigates whether the size of the training set or its similarity to the target domain is more important.", "labels": [], "entities": [{"text": "parse revision", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.8117718398571014}, {"text": "text normalization", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7701217830181122}]}, {"text": "Since we are interested in parsing in these experiments, we use gold POS tags as input for the parser, in order to exclude any unwanted interaction between POS tagging and parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9757590293884277}]}], "tableCaptions": [{"text": " Table 1: Error detection results for MST confidence scores (\u2264 0.5) for different conditions and normalization settings.  Number of tokens and errors below the threshold are reported.", "labels": [], "entities": [{"text": "MST confidence scores", "start_pos": 38, "end_pos": 59, "type": "METRIC", "confidence": 0.7889908750851949}]}, {"text": " Table 2: Results of comparing a machine learning reviser (DeSR) with a tree anomaly model (APS), with base parser  MST (* = sig. at the 0.05 level, as compared to row 2).", "labels": [], "entities": []}, {"text": " Table 4: Results of different training data sets and normalization patterns on parsing the EWT test data. (Significance  tested for APS versions as compared to the corresponding MST: * = sig. at the 0.05 level, ** = sig. at the 0.01 level)", "labels": [], "entities": [{"text": "EWT test data", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.950088103612264}]}]}