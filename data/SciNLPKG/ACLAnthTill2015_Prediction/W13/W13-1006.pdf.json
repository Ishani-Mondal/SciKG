{"title": [{"text": "Determining Compositionality of Word Expressions Using Word Space Models", "labels": [], "entities": [{"text": "Determining Compositionality of Word Expressions", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8734209060668945}]}], "abstractContent": [{"text": "This research focuses on determining semantic compositionality of word expressions using word space models (WSMs).", "labels": [], "entities": []}, {"text": "We discuss previous works employing WSMs and present differences in the proposed approaches which include types of WSMs, corpora, preprocess-ing techniques, methods for determining com-positionality, and evaluation testbeds.", "labels": [], "entities": []}, {"text": "We also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components.", "labels": [], "entities": []}, {"text": "The vectors were obtained by Latent Semantic Analysis (LSA) applied to the ukWaC corpus.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9954336881637573}]}, {"text": "Our results outperform those of all the participants in the Distributional Semantics and Composi-tionality (DISCO) 2011 shared task.", "labels": [], "entities": [{"text": "Distributional Semantics and Composi-tionality (DISCO) 2011 shared task", "start_pos": 60, "end_pos": 131, "type": "TASK", "confidence": 0.7018742144107819}]}], "introductionContent": [{"text": "A word expression is semantically compositional if its meaning can be understood from the literal meaning of its components.", "labels": [], "entities": []}, {"text": "Therefore, semantically compositional expressions involve e.g. \"small island\" or \"hot water\"; on the other hand, semantically non-compositional expressions are e.g. \"red tape\" or \"kick the bucket\".", "labels": [], "entities": []}, {"text": "The notion of compositionality is closely related to idiomacy -the higher the compositionality the lower the idiomacy and vice versa (;.", "labels": [], "entities": []}, {"text": "Non-compositional expressions are often referred to as Multiword Expressions (MWEs).", "labels": [], "entities": []}, {"text": "differentiate the following sub-types of compositionality: lexical, syntactic, semantic, pragmatic, and statistical.", "labels": [], "entities": []}, {"text": "This paper is concerned with semantic compositionality.", "labels": [], "entities": [{"text": "semantic compositionality", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.8039716482162476}]}, {"text": "Compositionality as a feature of word expressions is not discrete.", "labels": [], "entities": []}, {"text": "Instead, expressions populate a continuum between two extremes: idioms and free word combinations ().", "labels": [], "entities": []}, {"text": "Typical examples of expressions between the two extremes are \"zebra crossing\" or \"blind alley\".", "labels": [], "entities": [{"text": "zebra crossing", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7241694927215576}]}, {"text": "Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by, who utilized MWEs in Information Retrieval (IR).", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 253, "end_pos": 279, "type": "TASK", "confidence": 0.8417603492736816}]}, {"text": "Besides that, there are other NLP applications that can benefit from knowing the degree of compositionality of expressions such as machine translation, lexicography, word sense disambiguation (, part-of-speech (POS) tagging and parsing as listed in.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7755862474441528}, {"text": "word sense disambiguation", "start_pos": 166, "end_pos": 191, "type": "TASK", "confidence": 0.6653097669283549}, {"text": "part-of-speech (POS) tagging", "start_pos": 195, "end_pos": 223, "type": "TASK", "confidence": 0.6577737092971802}]}, {"text": "The main goal of this paper is to present an analysis of previous approaches using WSMs for determining the semantic compositionality of expressions.", "labels": [], "entities": []}, {"text": "The analysis can be found in Section 2.", "labels": [], "entities": []}, {"text": "A special attention is paid to the evaluation of the proposed models that is described in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents our first intuitive experimental setup and results of LSA applied to the DISCO 2011 task.", "labels": [], "entities": [{"text": "DISCO 2011 task", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.8334946632385254}]}, {"text": "Section 5 concludes the paper., show the ability of methods based on WSMs to capture the degree of semantic compositionality of word expressions.", "labels": [], "entities": [{"text": "semantic compositionality of word expressions", "start_pos": 99, "end_pos": 144, "type": "TASK", "confidence": 0.7798802852630615}]}, {"text": "We analyse the proposed methods and discuss their differences.", "labels": [], "entities": []}, {"text": "As further described in detail and summarized in, the approaches differ in the type of WSMs, corpora, preprocessing techniques, methods for determining the compositionality, datasets for evaluation, and methods of evaluation itself.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 87, "end_pos": 91, "type": "TASK", "confidence": 0.9132720232009888}]}, {"text": "Our understanding of WSM is in agreement with Sahlgren (2006): \"The word space model is a computational model of word meaning that utilizes the distributional patterns of words collected overlarge text data to represent semantic similarity between words in terms of spatial proximity\".", "labels": [], "entities": [{"text": "WSM", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9318872094154358}]}, {"text": "For more information on WSMs, see e.g.,, or Sahlgren (2006).", "labels": [], "entities": [{"text": "WSMs", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.7196369171142578}]}, {"text": "WSMs and their parameters WSMs can be built by different algorithms including LSA, Hyperspace Analogue to Language (HAL), Random Indexing (RI), and Correlated Occurrence Analogue to Lexical Semantics (COALS) ().", "labels": [], "entities": [{"text": "Random Indexing (RI)", "start_pos": 122, "end_pos": 142, "type": "METRIC", "confidence": 0.8557411670684815}]}, {"text": "Every algorithm has its own specifics and can be configured in different ways.", "labels": [], "entities": []}, {"text": "The configuration usually involves e.g. the choice of context size, weighting functions, or normalizing functions.", "labels": [], "entities": []}, {"text": "While, , and Methods We have identified three basic methods for determining semantic compositionality: 1) The substitutability-based methods exploit the fact that replacing components of noncompositional expressions by words which are similar leads to anti-collocations).", "labels": [], "entities": []}, {"text": "Then, frequency or mutual information of such expressions (anti-collocations) is compared with the frequency or mutual information of the original expressions.", "labels": [], "entities": []}, {"text": "For example, consider expected occurrence counts of \"hot dog\" and its anti-collocations such as \"warm dog\" or \"hot terrier\".", "labels": [], "entities": []}, {"text": "2) The component-based methods, utilized for example by, compare the distributional characteristics of expressions and their components.", "labels": [], "entities": []}, {"text": "The context vectors expected to be different from each other are e.g. the vector representing the expression \"hot dog\" and the vector representing the word \"dog\".", "labels": [], "entities": []}, {"text": "3) The compositionality-based methods compare two vectors of each analysed expression: the true co-occurrence vector of an expression and the vector obtained from vectors corresponding to the components of the expression using a compositionality function ().", "labels": [], "entities": []}, {"text": "The most common compositionality functions are vector addition or pointwise vector multiplication).", "labels": [], "entities": []}, {"text": "For example, the vectors for \"hot dog\" and \"hot\"\u2295\"dog\" are supposed to be different.", "labels": [], "entities": []}, {"text": "Evaluation datasets There is still no consensus on how to evaluate models determining semantic compositionality.", "labels": [], "entities": []}, {"text": "However, by examining the discussed papers, we have observed an increasing ten-   dency to exploit manually annotated data from a specific corpus, ranging from semantically compositional to non-compositional expressions ().", "labels": [], "entities": []}, {"text": "This approach, as opposed to the methods based on dictionaries of MWEs (idioms) or Wordnet, has the following advantages: Firstly, the classification of a manually annotated data is not binary but finer-grained, enabling the evaluation to be more detailed.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9543344974517822}]}, {"text": "Secondly, the lowcoverage problem of dictionaries, which originates for example due to the facts that new MWEs still arise or are domain specific, is avoided.", "labels": [], "entities": []}, {"text": "1 For example, Lin (1999),, used Wordnet or other dictionarytype resources.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9729026556015015}]}, {"text": "Our evaluation is based on the English part of the manually annotated datasets DISCO 2011 (, further referred to as DISCO-EnGold.", "labels": [], "entities": [{"text": "English part of the manually annotated datasets DISCO 2011", "start_pos": 31, "end_pos": 89, "type": "DATASET", "confidence": 0.7438613507482741}, {"text": "DISCO-EnGold", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.9295356273651123}]}, {"text": "Disco-En-Gold consists of 349 expressions divided into training (TrainD), validation (ValD), and test data (TestD) manually assigned scores from 0 to 100, indicating the level of compositionality (the lower the score the lower the compositionality and vice versa).", "labels": [], "entities": [{"text": "validation (ValD)", "start_pos": 74, "end_pos": 91, "type": "METRIC", "confidence": 0.7897159606218338}]}, {"text": "The expressions are of the following types: adjective-noun (AN), verb-object (VO), and subject-verb (SV).", "labels": [], "entities": []}, {"text": "Based on the numerical scores, the expressions are also classified into three disjoint classes (coarse scores): low, medium, and high compositional.", "labels": [], "entities": []}, {"text": "A sample of the Disco-En-Gold data is presented in.", "labels": [], "entities": [{"text": "Disco-En-Gold data", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.9572222828865051}]}, {"text": "Comparison of evaluation methods The purpose of the DISCO workshop was to find the best methods for determining semantic compositionality.", "labels": [], "entities": [{"text": "determining semantic compositionality", "start_pos": 100, "end_pos": 137, "type": "TASK", "confidence": 0.5886377890904745}]}, {"text": "The participants were asked to create systems capable of assigning the numerical values closest to the ones assigned by the annotators (Gold values).", "labels": [], "entities": []}, {"text": "The proposed APD evaluation measure is calculated as the mean difference between the particular systems' val-   Evaluation based on ranking can be realized by measuring ranked correlations (Spearman and Kendall) or Precision/Recall scores and curves commonly used e.g. in IR (.", "labels": [], "entities": [{"text": "Precision/Recall scores", "start_pos": 215, "end_pos": 238, "type": "METRIC", "confidence": 0.8538189828395844}, {"text": "IR", "start_pos": 272, "end_pos": 274, "type": "TASK", "confidence": 0.7337219715118408}]}, {"text": "In IR, Precision is defined as the ratio of found relevant documents to all the retrieved documents with regards to a user's query.", "labels": [], "entities": [{"text": "IR", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9662381410598755}, {"text": "Precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9914811849594116}]}, {"text": "Recall is defined as the ratio of found relevant documents to all the relevant documents in a test set to the user's query.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9387872815132141}]}, {"text": "The Precision/Recall curve is a curve depicting the dependency of Precision upon Recall.", "labels": [], "entities": []}, {"text": "Analogously, the scheme can be used for evaluation of the methods finding semantically non-compositional expressions.", "labels": [], "entities": []}, {"text": "However, estimation of Recall is not possible without knowledge of the correct class 3 for every expression in a corpus.", "labels": [], "entities": [{"text": "Recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.5079489350318909}]}, {"text": "To bypass this, calculates Recall with respect to the set of annotated data divided into non-compositional and compositional classes.", "labels": [], "entities": [{"text": "Recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9756917953491211}]}, {"text": "The Precision/nBest, Recall/nBest, and Precision/Recall curves for the LSA experiment described in the following section are depicted in.", "labels": [], "entities": [{"text": "Recall/nBest", "start_pos": 21, "end_pos": 33, "type": "METRIC", "confidence": 0.8896540602048238}, {"text": "Precision/Recall", "start_pos": 39, "end_pos": 55, "type": "METRIC", "confidence": 0.7732867995897929}]}, {"text": "Evert's (2005) curves allow us to visually compare the results of the methods in more detail.", "labels": [], "entities": []}, {"text": "To facilitate comparison of several methods, we also suggest using average precision (AP) adopted from Pecina (2009), which reduces information provided by a single Precision/Recall curve to one value.", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 67, "end_pos": 89, "type": "METRIC", "confidence": 0.9163030624389649}]}, {"text": "AP is defined as a mean Precision at all the values of Recall different from zero.", "labels": [], "entities": [{"text": "AP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9736927151679993}, {"text": "Precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9633417129516602}, {"text": "Recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9927178621292114}]}], "datasetContent": [{"text": "LSA is WSM based on the Singular Value Decomposition (SVD) factorization) applied to the co-occurrence matrix.", "labels": [], "entities": [{"text": "WSM", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9140768051147461}]}, {"text": "In the matrix, the numbers of word occurrences in specified contexts are stored.", "labels": [], "entities": []}, {"text": "The row vectors of the matrix capture the word meanings.", "labels": [], "entities": []}, {"text": "The idea of using SVD is to project vectors corresponding to the words into a lower-dimensional space and thus bring the vectors of words with similar meaning near to each other.", "labels": [], "entities": []}, {"text": "We built LSA WSM and applied the componentbased method to Disco-En-Gold.", "labels": [], "entities": [{"text": "LSA WSM", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.80099156498909}]}, {"text": "We used our own modification of the LSA algorithm originally implemented in the S-Space package.", "labels": [], "entities": []}, {"text": "The modification lies in treating expressions and handling stopwords.", "labels": [], "entities": []}, {"text": "Specifically, we added vectors for the examined expressions to WSM in such away that the original vectors for words were preserved.", "labels": [], "entities": [{"text": "WSM", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.6815411448478699}]}, {"text": "This differentiates our approach e.g. from who label the expressions ahead of time and build WSMs treating them as single words.", "labels": [], "entities": []}, {"text": "Treating the expressions as the single words affects the WSM vectors of their constituents.", "labels": [], "entities": [{"text": "WSM", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8342388868331909}]}, {"text": "As an example, consider the replacement of occurrences of \"short distance\" by e.g. the EXP#123 label.", "labels": [], "entities": [{"text": "EXP#123 label", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9011930078268051}]}, {"text": "This affects the WSM vectors of \"short\" and \"distance\" since the numbers of their occurrences and the numbers of contexts they occur in drops.", "labels": [], "entities": [{"text": "WSM", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8806653022766113}]}, {"text": "Consequently, this also affects the methods for determining the compositionality which are based upon using the vectors of expressions' constituents.", "labels": [], "entities": []}, {"text": "As for treating stopwords, we mapped the trigram expressions containing the determiners \"the\", \"a\", or \"an\" as the middle word to the corresponding bigram expressions without the determiners.", "labels": [], "entities": []}, {"text": "The intuition is to extract more precise co-occurrence vectors for the VO expressions often containing some intervening determiner.", "labels": [], "entities": []}, {"text": "As an example, compare the occurrences of \"reinvent wheel\" and \"reinvent (determiner) wheel\" in the ukWaC corpus which are 27 and 623, respectively, or the occurrences of \"cross bridge\" and \"cross (determiner) bridge\" being 50 and 1050, respectively.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.9954160451889038}]}, {"text": "We built LSA WSM from the whole ukWaC POS-tagged corpus for all the word lemmas concatenated with their POS tags excluding stopwords.", "labels": [], "entities": [{"text": "LSA WSM", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.6819291412830353}, {"text": "ukWaC POS-tagged corpus", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.945885439713796}]}, {"text": "We treated the following strings as stopwords: the lemmas with frequency below 50 (omitting lowfrequency words), the strings containing two adjacent non-letter characters (omitting strings such as web addresses and sequences of e.g. star symbols), and lemmas with a different POS tag from noun, proper noun, adjective, verb, and adverb (omitting closed-class words).", "labels": [], "entities": []}, {"text": "As contexts, the entire documents were used.", "labels": [], "entities": []}, {"text": "The co-occurrence matrix for words was normalized by applying the log-entropy transformation and reduced to 300 dimensions.", "labels": [], "entities": []}, {"text": "Using these settings, obtained the best results.", "labels": [], "entities": []}, {"text": "Finally, the co-occurrence vectors of expressions were expressed in the lower-dimensional space of words in a manner analogous to how a user's query is being expressed in lower-dimensional space of documents in IR (.", "labels": [], "entities": []}, {"text": "The DiscoEn-Gold expressions were sorted in ascending order by the average cosine similarity between the vectors corresponding to the expressions and the vectors corresponding to their components.", "labels": [], "entities": [{"text": "DiscoEn-Gold", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9387317299842834}]}, {"text": "Evaluation We have not tried to find the optimal parameter settings for the LSA-based model yet.", "labels": [], "entities": []}, {"text": "Therefore, we present the results on the concatenation of TrainD with ValD giving us TrainValD and on TestD.", "labels": [], "entities": [{"text": "TrainValD", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.8586094379425049}]}, {"text": "The expressions \"leading edge\" and \"broken link\" were removed from TestD because they occur in the ukWaC corpus assigned with the required POS tags less than 50 times.", "labels": [], "entities": [{"text": "TestD", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9167850613594055}, {"text": "ukWaC corpus", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.9845212399959564}]}, {"text": "APs with the Spearman and Kendall correlations between the compositionality values assigned by the LSA-based model and the Gold values are depicted in.", "labels": [], "entities": []}, {"text": "The Spearman correlations of the LSA model applied to the whole TrainValD and TestD are highly significant with p-values < 0.001.", "labels": [], "entities": [{"text": "Spearman correlations", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.6328345835208893}, {"text": "TrainValD", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.9350742101669312}, {"text": "TestD", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.7401705980300903}]}, {"text": "For the AP evaluation, the expressions with numerical values lessor equal to 50 were classified as non-compositional 7 , giving us the ratio of non-compositional expressions in TrainValD and TestD equal to 0.26 and 0.20, respectively.", "labels": [], "entities": [{"text": "TrainValD", "start_pos": 177, "end_pos": 186, "type": "DATASET", "confidence": 0.806755542755127}]}, {"text": "The Precision/nBest and Recall/nBest graphs corresponding to the LSA-based model applied to TestD are depicted in.", "labels": [], "entities": [{"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9576357007026672}]}, {"text": "The Precision/Recall graphs corresponding to the LSA-based model applied to TrainD and TestD are depicted in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9716481566429138}, {"text": "Recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.5136124491691589}]}, {"text": "For comparison, the graphs in Figures 1 and 2 also show the curves corresponding to the evaluation of Pointwise Mutual Information (PMI).", "labels": [], "entities": [{"text": "evaluation of Pointwise Mutual Information (PMI)", "start_pos": 88, "end_pos": 136, "type": "TASK", "confidence": 0.6175785548985004}]}, {"text": "The cooccurrence statistics of the expressions in Disco-EnGold was extracted from the window of size three, sliding through the whole lemmatized ukWaC corpus.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.9815553426742554}]}, {"text": "Discussion As suggested in Section 3, we compare the results of the methods using Spearman and Kendall correlations, AP, and Everts' curves.", "labels": [], "entities": [{"text": "AP", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.9986924529075623}]}, {"text": "We present the results of the LSA and PMI models alongside the results of the best performing models participating in the DISCO task.", "labels": [], "entities": []}, {"text": "Namely, presents the correlation values of our models, the best performing WSM-based model (, the best performing model based upon association measures, and random baseline models.", "labels": [], "entities": []}, {"text": "The poor results achieved by employing PMI are similar to the results of random baselines and in accordance with those of participants of the DISCO workshop).", "labels": [], "entities": [{"text": "PMI", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9490286111831665}, {"text": "DISCO workshop", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.8569634556770325}]}, {"text": "We hypothesize that the PMI-based model incorrectly assigns low values of semantic compositionality (high val-  ues of PMI) to frequently occurring fixed expressions.", "labels": [], "entities": [{"text": "val-  ues", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9057358702023824}]}, {"text": "For example, we observed that the calculated values of PMI for \"international airport\" and \"religious belief\" were high.", "labels": [], "entities": [{"text": "PMI", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9917761087417603}]}, {"text": "To the contrary, our results achieved by employing the LSA model are statistically significant and better than those of all the participants of the DISCO workshop.", "labels": [], "entities": [{"text": "DISCO workshop", "start_pos": 148, "end_pos": 162, "type": "DATASET", "confidence": 0.8513639271259308}]}, {"text": "However, the data set is probably not large enough to provide statistically reliable comparison of the methods and it is not clear how reliable the dataset itself is (the interannotator agreement was not analyzed) and therefore we cannot make any hard conclusions.", "labels": [], "entities": []}], "tableCaptions": []}