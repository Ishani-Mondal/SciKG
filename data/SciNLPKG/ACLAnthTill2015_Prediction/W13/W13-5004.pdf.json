{"title": [], "abstractContent": [{"text": "Distance metric learning from high (thou-sands or more) dimensional data with hundreds or thousands of classes is intractable but in NLP and IR, high dimensionality is usually required to represent data points, such as in modeling semantic similarity.", "labels": [], "entities": [{"text": "Distance metric learning", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7854823668797811}]}, {"text": "This paper presents algorithms to scale up learning of a Mahalanobis distance metric from a large data graph in a high dimensional space.", "labels": [], "entities": []}, {"text": "Our novel contributions include random projection that reduces dimensionality and anew objective function that regularizes intra-class and inter-class distances to handle a large number of classes.", "labels": [], "entities": [{"text": "random projection", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.711614266037941}]}, {"text": "We show that the new objective function is convex and can be efficiently optimized by a stochastic-batch subgradient descent method.", "labels": [], "entities": []}, {"text": "We applied our algorithm to two different domains; semantic similarity of documents collected from the Web, and phenotype descriptions in genomic data.", "labels": [], "entities": []}, {"text": "Experiments show that our algorithm can handle the high-dimensional big data and outperform competing approximations in both domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "According to, distance metric learning learns a distance metric from data sets that consists of pairs of points of the same or different classes while at the same time preserving the adjacency relations among the data points.", "labels": [], "entities": [{"text": "distance metric learning", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.7400408089160919}]}, {"text": "Usually, it is easier to let the user label whether a set of data is in the same class than directly assign a distance between each pair or classify whether a pair of data points is a match or not.", "labels": [], "entities": []}, {"text": "Learning a good distance metric in the feature space is essential in many realworld NLP and IR applications.", "labels": [], "entities": []}, {"text": "For example, Web news article clustering applying hierarchical clustering or k-means requires that the distance between the two feature vectors extracted from the news articles faithfully reflect the semantic similarity between them for these algorithms to perform well.", "labels": [], "entities": [{"text": "Web news article clustering", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6508919894695282}]}, {"text": "Studies on distance metric learning over the past few years show that the learned metric can outperform Euclidean distance metric.", "labels": [], "entities": [{"text": "distance metric learning", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.7587556838989258}]}, {"text": "The constraints of training examples in learning usually comes from the global or local adjacency information.", "labels": [], "entities": []}, {"text": "Data points with the same class labels are supposed to be connected while those with different classes labels disconnected.", "labels": [], "entities": []}, {"text": "Supervised algorithms aim to learn the distance metric to make the adjacency relationships in the training examples preserved.", "labels": [], "entities": []}, {"text": "One of the most common approaches to distance metric learning is to learn a Mahalanobis distance metric.", "labels": [], "entities": [{"text": "distance metric learning", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.805627445379893}, {"text": "Mahalanobis distance metric", "start_pos": 76, "end_pos": 103, "type": "DATASET", "confidence": 0.807536800702413}]}, {"text": "Example algorithms to learn a Mahalanobis distance metric include (.", "labels": [], "entities": []}, {"text": "A common limitation shared by these algorithms is that they fail to scale up to high dimensional data sets.", "labels": [], "entities": []}, {"text": "When those algorithms run on high dimensional data sets, they usually run out of memory.", "labels": [], "entities": []}, {"text": "However, many NLP applications depend on tens of thousands of features to perform well.", "labels": [], "entities": []}, {"text": "Dimensionality reduction and approximation have been suggested, but they usually degrade performance.", "labels": [], "entities": [{"text": "Dimensionality reduction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7329279035329819}, {"text": "approximation", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.6194605827331543}]}, {"text": "Other issues occur when the data sets consists of a large number of disjoint classes.", "labels": [], "entities": []}, {"text": "In this case, the learned distance metric must map the data points to a space where the data points cluster unevenly into a large number of small groups, which makes the learning problem harder and may require special treatments.", "labels": [], "entities": []}, {"text": "In this paper, we present anew scalable approach to distance metric learning that addresses the scalability issues mentioned above.", "labels": [], "entities": [{"text": "distance metric learning", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7283865412076315}]}, {"text": "To deal with high dimensionality, one approach is to factorize the metric matrix in the Mahalanobis distance metric into low-rank matrix.", "labels": [], "entities": [{"text": "Mahalanobis distance metric", "start_pos": 88, "end_pos": 115, "type": "DATASET", "confidence": 0.8690936962763468}]}, {"text": "This reduces the number of parameters that must be learned during the learning phase.", "labels": [], "entities": []}, {"text": "However, the learning problem becomes nonconvex.", "labels": [], "entities": []}, {"text": "Different initializations may result in drastically different performance due to local optima.", "labels": [], "entities": []}, {"text": "We solve this problem by introducing random projection, which projects data points to a low dimensional space before learning a low dimensional fullrank metric matrix.", "labels": [], "entities": []}, {"text": "We show that this strategy not only is more robust than the low-rank approximation, but also outperforms the Principal Component Analysis (PCA), a common approach to dimensionality reduction.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.7359289079904556}]}, {"text": "Another contribution that our approach offers is new regularization terms in the objective function of learning.", "labels": [], "entities": []}, {"text": "The new terms specify that one should learn to minimize the distance for data points in the same classes and maximize those in different ones.", "labels": [], "entities": []}, {"text": "We found that minimization but not maximization would lead to the best performance, so we kept only the minimization term.", "labels": [], "entities": []}, {"text": "We evaluated our new approach with data sets from two problem domains.", "labels": [], "entities": []}, {"text": "One domain is about learning semantic similarity between Web pages.", "labels": [], "entities": [{"text": "learning semantic similarity between Web pages", "start_pos": 20, "end_pos": 66, "type": "TASK", "confidence": 0.8197659353415171}]}, {"text": "This domain was studied in) and involves moderately high dimensional data sets of bag-of-words.", "labels": [], "entities": []}, {"text": "The other is about matching semantically related phenotype variables across different genome-wide association studies (GWAS) ().", "labels": [], "entities": []}, {"text": "This problem domain requires extremely high dimensional data fora learner to perform well.", "labels": [], "entities": []}, {"text": "Our experimental results show that our new algorithm consistently outperform the previous ones in both the domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied our approach in two different problem domains.", "labels": [], "entities": []}, {"text": "One involves a small amount of data points with moderately high dimensions (more than 1,000 and less than 10,000); the other involves a large number of data points with very high dimensions (more than 10,000).", "labels": [], "entities": []}, {"text": "The results show that our approach can perform well in both cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Wikipedia pages dataset", "labels": [], "entities": [{"text": "Wikipedia pages dataset", "start_pos": 10, "end_pos": 33, "type": "DATASET", "confidence": 0.9144037564595541}]}, {"text": " Table 2: Performance Comparison on Wikipedia Docu- ments", "labels": [], "entities": [{"text": "Wikipedia Docu- ments", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.9090816080570221}]}, {"text": " Table 3. We trained  our models on connected and disconnected training  pairs in two folds and then evaluated with the pairs  in the other fold. Again, after we used the trained  model to predict the distances of all test pairs, we  ranked them and compared the quality of the rank- ing using the ROC curves as described earlier. The  evaluation metric is the average of their AUC scores  for the three folds.", "labels": [], "entities": [{"text": "ROC", "start_pos": 298, "end_pos": 301, "type": "METRIC", "confidence": 0.7594321966171265}, {"text": "AUC", "start_pos": 378, "end_pos": 381, "type": "METRIC", "confidence": 0.9915803074836731}]}, {"text": " Table 4: Performance Comparison on Phenotype Data", "labels": [], "entities": []}]}