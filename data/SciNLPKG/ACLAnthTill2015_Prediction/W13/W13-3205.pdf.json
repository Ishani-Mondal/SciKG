{"title": [{"text": "Transducing Sentences to Syntactic Feature Vectors: an Alternative Way to \"Parse\"?", "labels": [], "entities": [{"text": "Transducing Sentences to Syntactic Feature Vectors", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8114693562189738}, {"text": "Parse\"", "start_pos": 75, "end_pos": 81, "type": "TASK", "confidence": 0.9205791652202606}]}], "abstractContent": [{"text": "Classification and learning algorithms use syntactic structures as proxies between source sentences and feature vectors.", "labels": [], "entities": []}, {"text": "In this paper, we explore an alternative path to use syntax in feature spaces: the Distributed Representation \"Parsers\" (DRP).", "labels": [], "entities": []}, {"text": "The core of the idea is straightforward: DRPs directly obtain syntactic feature vectors from sentences without explicitly producing symbolic syntactic interpretations.", "labels": [], "entities": []}, {"text": "Results show that DRPs produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information .", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic processing is widely considered an important activity in natural language understanding.", "labels": [], "entities": [{"text": "Syntactic processing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.930135190486908}, {"text": "natural language understanding", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6723264853159586}]}, {"text": "Research in natural language processing (NLP) exploits this hypothesis in models and systems.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8176305294036865}]}, {"text": "Syntactic features improve performance in high level tasks such as question answering, semantic role labeling (), paraphrase detection, and textual entailment recognition (;).", "labels": [], "entities": [{"text": "question answering", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.850450873374939}, {"text": "semantic role labeling", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6643909811973572}, {"text": "paraphrase detection", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.9009442031383514}, {"text": "textual entailment recognition", "start_pos": 140, "end_pos": 170, "type": "TASK", "confidence": 0.7606345613797506}]}, {"text": "Classification and learning algorithms are key components in the above models and in current NLP systems, but these algorithms cannot directly use syntactic structures.", "labels": [], "entities": []}, {"text": "The relevant parts of phrase structure trees or dependency graphs are explicitly or implicitly stored in feature vectors.", "labels": [], "entities": []}, {"text": "To fully exploit syntax in learning classifiers, kernel machines) use graph similarity algorithms (e.g.,) for trees) as structural kernels.", "labels": [], "entities": []}, {"text": "These structural kernels allow to exploit high-dimensional spaces of syntactic tree fragments by concealing their complexity.", "labels": [], "entities": []}, {"text": "These feature spaces, although hidden, still exist.", "labels": [], "entities": []}, {"text": "Then, even in kernel machines, symbolic syntactic structures act only as proxies between the source sentences and the syntactic feature vectors.", "labels": [], "entities": []}, {"text": "In this paper, we explore an alternative way to use syntax in feature spaces: the Distributed Representation Parsers (DRP).", "labels": [], "entities": []}, {"text": "The core of the idea is straightforward: DRPs directly bridge the gap between sentences and syntactic feature spaces.", "labels": [], "entities": []}, {"text": "DRPs act as syntactic parsers and feature extractors at the same time.", "labels": [], "entities": []}, {"text": "We leverage on the distributed trees recently introduced by Zanzotto&Dell'Arciprete (2012) and on multiple linear regression models.", "labels": [], "entities": []}, {"text": "Distributed trees are small vectors that encode the large vectors of the syntactic tree fragments underlying the tree kernels ().", "labels": [], "entities": []}, {"text": "These vectors effectively represent the original vectors and lead to performances in NLP tasks similar to tree kernels.", "labels": [], "entities": []}, {"text": "Multiple linear regression allows to learn linear DRPs from training data.", "labels": [], "entities": []}, {"text": "We experiment with the Penn Treebank data set.", "labels": [], "entities": [{"text": "Penn Treebank data set", "start_pos": 23, "end_pos": 45, "type": "DATASET", "confidence": 0.9954781234264374}]}, {"text": "Results show that DRPs produce distributed trees significantly better than those obtained by existing methods, in the same non-lexicalized conditions, and competitive with those obtained by existing methods with lexical information.", "labels": [], "entities": []}, {"text": "Finally, DRPs are extremely faster than existing methods.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we present the background of our idea (Sec. 2).", "labels": [], "entities": []}, {"text": "Second, we fully describe our model (Sec. 3).", "labels": [], "entities": []}, {"text": "Then, we report on the experiments (Sec. 4).", "labels": [], "entities": []}, {"text": "Finally, we draw some conclusions and outline future work (Sec. 5)", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated three issues for assessing DRP models: the performance of DRPs in reproducing oracle distributed trees (Sec.", "labels": [], "entities": []}, {"text": "4.2); the quality of the topology of the vector spaces of distributed trees induced by DRPs (Sec. 4.3); and the computation run time of DRPs (Sec. 4.4).", "labels": [], "entities": []}, {"text": "Section 4.1 describes the experimental set-up.", "labels": [], "entities": []}, {"text": "Data We derived the data sets from the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (, using a standard data split for training (sections 2-21 PT train with 39,832 trees) and for testing (section 23 PT 23 with 2,416 trees).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion", "start_pos": 39, "end_pos": 72, "type": "DATASET", "confidence": 0.9490685292652675}, {"text": "English Penn Treebank data set", "start_pos": 80, "end_pos": 110, "type": "DATASET", "confidence": 0.9301817178726196}, {"text": "PT", "start_pos": 170, "end_pos": 172, "type": "METRIC", "confidence": 0.9477132558822632}]}, {"text": "We used section 24 PT 24 with 1,346 trees for parameter estimation.", "labels": [], "entities": [{"text": "PT 24", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.7718672156333923}]}, {"text": "We produced the final data sets of distributed trees with three different \u03bb values: \u03bb=0, \u03bb=0.2, and \u03bb=0.4.", "labels": [], "entities": []}, {"text": "For each \u03bb, we have two versions of the data sets: a non-lexicalized version (no lex), where syntactic trees are considered without words, and a lexicalized version (lex), where words are considered.", "labels": [], "entities": []}, {"text": "Oracle trees tare transformed into oracle distributed trees ; o using the Distributed Tree Encoder DT (see).", "labels": [], "entities": []}, {"text": "We experimented with two sizes of the distributed trees space Rd : 4096 and 8192.", "labels": [], "entities": []}, {"text": "We have designed the data sets to determine how DRPs behave with \u03bb values relevant for syntax-sensitive NLP tasks.", "labels": [], "entities": []}, {"text": "Both tree kernels and distributed tree kernels have the best performances in tasks such as question classification, semantic role labeling, or textual entailment recognition with \u03bb values in the range 0-0.4.", "labels": [], "entities": [{"text": "question classification", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.8412265777587891}, {"text": "semantic role labeling", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.7110853791236877}, {"text": "textual entailment recognition", "start_pos": 143, "end_pos": 173, "type": "TASK", "confidence": 0.7403254012266794}]}], "tableCaptions": [{"text": " Table 1: Average similarity on P T 23 of the  DRPs (with different j) and the DSP on the non- lexicalized data sets with different \u03bbs and with the  two dimensions of the distributed tree space (4096  and 8192).  \u2021 indicates significant difference wrt.  DSP no lex (p << .005 computed with the Stu- dent's t test)", "labels": [], "entities": [{"text": "similarity", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.6683512330055237}]}, {"text": " Table 2: Average similarity on P T 23 of the DRP 3  and the DSP lex on the lexicalized data sets with  different \u03bbs on the distributed tree space with 4096  dimensions", "labels": [], "entities": [{"text": "Average similarity", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7148345112800598}, {"text": "DRP 3", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9414076209068298}, {"text": "DSP lex", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9309997260570526}]}, {"text": " Table 3: Average Spearman's Correlation: dim  4096 between the oracle's vector space and the  systems' vector spaces (100 trials on lists of 1000  sentence pairs).", "labels": [], "entities": [{"text": "Average Spearman's Correlation", "start_pos": 10, "end_pos": 40, "type": "METRIC", "confidence": 0.657549150288105}, {"text": "dim", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9643286466598511}]}]}