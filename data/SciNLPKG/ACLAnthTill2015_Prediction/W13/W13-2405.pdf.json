{"title": [{"text": "Frequently Asked Questions Retrieval for Croatian Based on Semantic Textual Similarity", "labels": [], "entities": [{"text": "Croatian Based on Semantic Textual Similarity", "start_pos": 41, "end_pos": 86, "type": "TASK", "confidence": 0.5951485087474188}]}], "abstractContent": [{"text": "Frequently asked questions (FAQ) are an efficient way of communicating domain-specific information to the users.", "labels": [], "entities": [{"text": "Frequently asked questions (FAQ)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.601496547460556}]}, {"text": "Unlike general purpose retrieval engines, FAQ retrieval engines have to address the lexical gap between the query and the usually short answer.", "labels": [], "entities": [{"text": "FAQ retrieval", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.6342397630214691}]}, {"text": "In this paper we describe the design and evaluation of a FAQ retrieval engine for Croatian.", "labels": [], "entities": [{"text": "FAQ retrieval", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.6036518216133118}]}, {"text": "We frame the task as a binary classification problem , and train a model to classify each FAQ as either relevant or not relevant fora given query.", "labels": [], "entities": []}, {"text": "We use a variety of semantic textual similarity features, including term overlap and vector space features.", "labels": [], "entities": []}, {"text": "We train and evaluate on a FAQ test collection built specifically for this purpose.", "labels": [], "entities": [{"text": "FAQ test collection", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.8955324093500773}]}, {"text": "Our best-performing model reaches 0.47 of mean reciprocal rank, i.e., on average ranks the relevant answer among the top two returned answers.", "labels": [], "entities": [{"text": "mean reciprocal rank", "start_pos": 42, "end_pos": 62, "type": "METRIC", "confidence": 0.8987874388694763}]}], "introductionContent": [{"text": "The amount of information available online is growing at an exponential rate.", "labels": [], "entities": []}, {"text": "It is becoming increasingly difficult to navigate the vast amounts of data and isolate relevant pieces of information.", "labels": [], "entities": []}, {"text": "Thus, providing efficient information access for clients can be essential for many businesses.", "labels": [], "entities": []}, {"text": "Frequently asked questions (FAQ) databases area popular way to present domain-specific information in the form of expert answers to users questions.", "labels": [], "entities": []}, {"text": "Each FAQ consists of a question and an answer, possibly complemented with additional metadata (e.g., keywords).", "labels": [], "entities": []}, {"text": "A FAQ retrieval engine provides an interface to a FAQ database.", "labels": [], "entities": [{"text": "FAQ retrieval engine", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.5907294154167175}, {"text": "FAQ database", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.8813163936138153}]}, {"text": "Given a user query in natural language as input, it retrieves a ranked list of FAQs relevant to the query.", "labels": [], "entities": []}, {"text": "FAQ retrieval can be considered halfway between traditional document retrieval and question answering (QA).", "labels": [], "entities": [{"text": "FAQ retrieval", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7320206761360168}, {"text": "document retrieval", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.6920842975378036}, {"text": "question answering (QA)", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.7985545992851257}]}, {"text": "Unlike in full-blown QA, in FAQ retrieval the questions and the answers are already extracted.", "labels": [], "entities": [{"text": "FAQ retrieval", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.7373732626438141}]}, {"text": "On the other hand, unlike in document retrieval, FAQ queries are typically questions and the answers are typically much shorter than documents.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7318043112754822}, {"text": "FAQ queries", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.654388964176178}]}, {"text": "While FAQ retrieval can be approached using simple keyword matching, the performance of such systems will be severely limited due to the lexical gap -a lack of overlap between the words that appear in a query and words from a FAQ pair.", "labels": [], "entities": [{"text": "FAQ retrieval", "start_pos": 6, "end_pos": 19, "type": "TASK", "confidence": 0.9377496540546417}]}, {"text": "As noted by, there are two causes for this.", "labels": [], "entities": []}, {"text": "Firstly, the FAQ database creators in general do not know the user questions in advance.", "labels": [], "entities": [{"text": "FAQ database creators", "start_pos": 13, "end_pos": 34, "type": "DATASET", "confidence": 0.8778555790583292}]}, {"text": "Instead, they must guess what the likely questions would be.", "labels": [], "entities": []}, {"text": "Thus, it is very common that users' information needs are not fully covered by the provided questions.", "labels": [], "entities": []}, {"text": "Secondly, both FAQs and user queries are generally very short texts, which diminishes the chances of a keyword match.", "labels": [], "entities": []}, {"text": "In this paper we describe the design and the evaluation of a FAQ retrieval engine for Croatian.", "labels": [], "entities": [{"text": "FAQ retrieval", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.5132884085178375}]}, {"text": "To address the lexical gap problem, we take a supervised learning approach and train a model that predicts the relevance of a FAQ given a query.", "labels": [], "entities": []}, {"text": "Motivated by the recent work on semantic textual similarity (, we use as model features a series of similarity measures based on word overlap and semantic vector space similarity.", "labels": [], "entities": []}, {"text": "We train and evaluate the model on a FAQ dataset from a telecommunication domain.", "labels": [], "entities": [{"text": "FAQ dataset", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.8404688835144043}]}, {"text": "On this dataset, our best performing model achieves 0.47 of mean reciprocal rank, i.e., on average ranks the relevant FAQ among the top two results.", "labels": [], "entities": [{"text": "mean reciprocal rank", "start_pos": 60, "end_pos": 80, "type": "METRIC", "confidence": 0.889513889948527}, {"text": "FAQ", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.8447204828262329}]}, {"text": "In summary, the contribution of this paper is twofold.", "labels": [], "entities": []}, {"text": "Firstly, we propose and evaluate a FAQ retrieval model based on supervised machine learning.", "labels": [], "entities": [{"text": "FAQ retrieval", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.7249070107936859}]}, {"text": "To the best of our knowledge, no previ-ous work exists that addresses IR for Croatian in a supervised setting.", "labels": [], "entities": [{"text": "IR", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9807680249214172}]}, {"text": "Secondly, we build a freely available FAQ test collection with relevance judgments.", "labels": [], "entities": [{"text": "FAQ test collection", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.7589570780595144}]}, {"text": "To the best of our knowledge, this is the first IR test collection for Croatian.", "labels": [], "entities": [{"text": "IR test collection", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.832142174243927}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section we give an overview of related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the FAQ test collection, while in Section 4 we describe the retrieval model.", "labels": [], "entities": [{"text": "FAQ test collection", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.7980658213297526}]}, {"text": "Experimental evaluation is given in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper and outlines future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because our retrieval model is supervised, we evaluate it using five-fold cross-validation on the FAQ test collection.", "labels": [], "entities": [{"text": "FAQ test collection", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.9506707390149435}]}, {"text": "In each fold we train our system on the training data as described in Section 4, and evaluate the retrieval performance on the queries from the test set.", "labels": [], "entities": []}, {"text": "While each (q, F rel ) occurs in the test set exactly once, the same FAQ may occur in both the train and test set.", "labels": [], "entities": [{"text": "F rel )", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.909000555674235}, {"text": "FAQ", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9986873269081116}]}, {"text": "Note that this does not pose a problem because the query part of the pair will differ (due to paraphrasing).", "labels": [], "entities": []}, {"text": "To gain a better understanding of which features contribute the most to retrieval performance, we created several models.", "labels": [], "entities": []}, {"text": "The models use increasingly complex feature sets; an overview is given in.", "labels": [], "entities": []}, {"text": "We leave exhaustive feature analysis and selection for future work.", "labels": [], "entities": []}, {"text": "As a baseline to compare against, we use a standard tf-idf weighted retrieval model.", "labels": [], "entities": []}, {"text": "This model ranks the FAQs by the cosine similarity of tf-idf weighted vectors representing the query and the FAQ.", "labels": [], "entities": [{"text": "FAQ", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.8997974991798401}]}, {"text": "When computing the vector of the FAQ pair, the question, answer, and category name are concatenated into a single text unit.: Classification results", "labels": [], "entities": [{"text": "FAQ", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7377030849456787}, {"text": "Classification", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.9627403616905212}]}], "tableCaptions": [{"text": " Table 2: FAQ test collection statistics", "labels": [], "entities": [{"text": "FAQ test collection", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7064779798189799}]}, {"text": " Table 3: Examples from query expansions dictionary", "labels": [], "entities": []}]}