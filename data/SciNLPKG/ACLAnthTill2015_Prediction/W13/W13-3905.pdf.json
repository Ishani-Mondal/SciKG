{"title": [{"text": "Comparing and combining classifiers for self-taught vocal interfaces", "labels": [], "entities": []}], "abstractContent": [{"text": "An attractive approach to enable the use of vocal interfaces by impaired users with dysarthric speech is the use of a system which learns from the end-user.", "labels": [], "entities": []}, {"text": "To enable such technology, it is imperative that the learning is fast to reduce the time spent training the interface.", "labels": [], "entities": []}, {"text": "In this paper we investigate to what extend various machine learning techniques are able to learn from only a single or a few spoken training samples.", "labels": [], "entities": []}, {"text": "Additionally, we explore whether these techniques can be combined through boosting to improve the performance.", "labels": [], "entities": []}, {"text": "Our evaluations on a small, but highly realistic home automation database reveal that non-negative matrix factorization seems best suited for fast learning and that some of the boosting approaches can indeed improve performance, especially for small amounts of training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vocal user interfaces (VUIs) allow us to control a wide range of appliances and devices such as computers, smart phones, car navigation and other domestic devices and environments.", "labels": [], "entities": []}, {"text": "While for most the use of a VUI is just a luxury, for individuals with a physical disability using a VUI can greatly improve their independence and quality of living, because for them operating and controlling devices would often require exhausting physical effort.", "labels": [], "entities": []}, {"text": "Conventional speech recognition systems employed in VUIs are trained by the developer using vast amounts of speech material.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7638238370418549}]}, {"text": "While offering impressive performances for users whose word choice, grammar and speech conforms to the training material used, performance suffers in the presence of accented, dialectical and disordered speech.", "labels": [], "entities": []}, {"text": "A possible solution, adaptation of existing acoustic models, may not suffice for severe speech pathologies.", "labels": [], "entities": []}, {"text": "The goal of this research is to explore methods which allow training speech commands by the end-user himself.", "labels": [], "entities": []}, {"text": "This way, the acoustic models of the VUI are maximally adapted to the end-user's speech while at the same time bringing development costs down.", "labels": [], "entities": []}, {"text": "The challenge is to employ a learning strategy that can learn from only one or a few examples, in order to minimize the time the end-user spends on training the system.", "labels": [], "entities": []}, {"text": "In this work, we will offer a comparison between multiple popular machine learning strategies to evaluate their effectiveness in developing a fast learning self-taught VUI.", "labels": [], "entities": []}, {"text": "In previous work, we obtained encouraging results on fast vocabulary acquisition using non-negative matrix factorization (NMF).", "labels": [], "entities": [{"text": "vocabulary acquisition", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7553738653659821}]}, {"text": "Although in that work the learning speed of acquiring acoustic models was investigated, it focused on larger amounts of training data than targeted in this work.", "labels": [], "entities": []}, {"text": "Moreover, it considered a multi-label task in which spoken utterances were associated with multiple labels at once, which penalized other machine learning methods less suited for multi-label learning.", "labels": [], "entities": []}, {"text": "In contrast, in this work we will focus on speech classification, labelling a spoken utterance with a single label.", "labels": [], "entities": [{"text": "speech classification", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7251497805118561}]}, {"text": "First, we compare the performance of five machine learning techniques: Dynamic Time Warping (DTW), Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs), Support Vector Machines (SVMs) and.", "labels": [], "entities": []}, {"text": "Each of these techniques have their strengths and weaknesses; for example, while a HMM improves upon a GMM by being able to model temporal structure, it does require more parameters to be trained.", "labels": [], "entities": []}, {"text": "When training with only one or a few training samples, this may lead to overfitting.", "labels": [], "entities": []}, {"text": "Second, we investigate to what extent combining the aforementioned classification techniques, 'boosting', can improve results.", "labels": [], "entities": []}, {"text": "We do this by comparing a number of combination rules operating at the class label posterior level.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we give an overview of the speech classification methods that are investigated.", "labels": [], "entities": [{"text": "speech classification", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7352017462253571}]}, {"text": "In Section 3 we describe the various boosting approaches that will be considered.", "labels": [], "entities": []}, {"text": "In Sections 4 and 5 we describe the experimental setup for evaluation on a small, but highly realistic home automation database collected in the ALADIN project.", "labels": [], "entities": [{"text": "home automation database collected in the ALADIN project", "start_pos": 103, "end_pos": 159, "type": "DATASET", "confidence": 0.6635349728167057}]}, {"text": "The results of these experiments are presented in Section 6 and discussed in Section 7, and we conclude with our summary and thoughts for future work in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are performed on the first home automation dataset (DOMOTICA-1) of the ALADIN project.", "labels": [], "entities": []}, {"text": "The dataset consists of non-pathological speech commands which were recorded in a realistic setting, i.e. a fully automated room using a wizard-of-oz device control.", "labels": [], "entities": []}, {"text": "The commands were prompted using visual cues (a video) on a computer screen.", "labels": [], "entities": []}, {"text": "In order to simulate situations with environmental noise, recordings were also made with a concurrent sound source.", "labels": [], "entities": []}, {"text": "In addition to a close-talk microphone, multichannel audio recordings were made with multiple microphone arrays, placed near the user, on walls and near the optional noise source.", "labels": [], "entities": []}, {"text": "The noisy recordings were created by playing a radio in the background with a sound level of 60dB Sound Pressure Level, which is the sound level of average speech.", "labels": [], "entities": [{"text": "Sound Pressure Level", "start_pos": 98, "end_pos": 118, "type": "METRIC", "confidence": 0.7951500415802002}]}, {"text": "It was ensured that the measured SNR to the nearest microphone remained above 15dB.", "labels": [], "entities": [{"text": "SNR", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.8804241418838501}]}, {"text": "The dataset consists of 27 test subjects of which 20 are of the targeted user group.", "labels": [], "entities": []}, {"text": "Each person was asked to go repeatedly through a list of 33 different actions, until a recording time of 30 minutes was reached, yielding a dataset of 1888 commands for the target group.", "labels": [], "entities": []}, {"text": "In addition to this set, longer recording sessions with 7 non-target users were carried out, yielding 1699 spoken commands.", "labels": [], "entities": []}, {"text": "The experiments are performed on persons 5,7,20,22 and 26, on the noisy dataset recorded with the close-talk microphone.", "labels": [], "entities": []}, {"text": "These speakers were selected because they were the only speakers with at least 3 spoken samples of each command.", "labels": [], "entities": []}, {"text": "We will refer to them by these numbers to keep correspondence with other work on the same dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracies obtained with classification methods in  noisy recording scenario for person 5, 7, 20, 22 and 26.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9888501763343811}]}, {"text": " Table 4: Weights for person 26 with combination rules.", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9740168452262878}]}, {"text": " Table 5: Accuracies for person 26 with combination rules.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9984288811683655}]}]}