{"title": [{"text": "Utilizing State-of-the-art Parsers to Diagnose Problems in Treebank Annotation fora Less Resourced Language", "labels": [], "entities": []}], "abstractContent": [{"text": "The recent success of statistical parsing methods has made treebanks become important resources for building good parsers.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.6742959022521973}]}, {"text": "However, constructing high-quality annotated treebanks is a challenging task.", "labels": [], "entities": []}, {"text": "We utilized two publicly available parsers, Berkeley and MST parsers, for feedback on improving the quality of part-of-speech tagging for the Vietnamese Treebank.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.6724032610654831}, {"text": "Vietnamese Treebank", "start_pos": 142, "end_pos": 161, "type": "DATASET", "confidence": 0.962022215127945}]}, {"text": "Analysis of the treebank and parsing errors revealed how problems with the Vietnamese Treebank influenced the parsing results and real difficulties of Viet-namese parsing that required further improvements to existing parsing technologies .", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9817075729370117}, {"text": "Vietnamese Treebank", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.986475020647049}, {"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.975124716758728}, {"text": "Viet-namese parsing", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.6245535463094711}]}], "introductionContent": [{"text": "Treebanks, corpora annotated with syntactic structures, have become more and more important for language processing.", "labels": [], "entities": [{"text": "language processing", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7238858938217163}]}, {"text": "The Vietnamese Treebank (VTB) has been built as part of the national project \"Vietnamese language and speech processing (VLSP)\" to strengthen automatic processing of the Vietnamese language (.", "labels": [], "entities": [{"text": "Vietnamese Treebank (VTB)", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.9111394047737121}, {"text": "Vietnamese language and speech processing (VLSP)\"", "start_pos": 78, "end_pos": 127, "type": "TASK", "confidence": 0.685527615249157}]}, {"text": "However, when we trained the Berkeley parser () in our preliminary experiment with VTB and evaluated it using the corpus, the parser only achieved an F-score of 72.1%.", "labels": [], "entities": [{"text": "VTB", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.9523207545280457}, {"text": "F-score", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.9993033409118652}]}, {"text": "This percentage was far lower than the state-of-the-art performance reported for the Berkeley parser on the English Penn Treebank of 90.2% ().", "labels": [], "entities": [{"text": "Berkeley parser on the English Penn Treebank", "start_pos": 85, "end_pos": 129, "type": "DATASET", "confidence": 0.8773457152502877}]}, {"text": "There are two possible reasons for this.", "labels": [], "entities": []}, {"text": "First, the quality of VTB is not good enough to construct a good parser that included the quality of the annotation scheme, the annotation guidelines, and the annotation process.", "labels": [], "entities": [{"text": "VTB", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.78467857837677}]}, {"text": "Second, parsing Vietnamese is a difficult problem on its own, and we need to seek new solutions to this.", "labels": [], "entities": [{"text": "parsing Vietnamese", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.8316742479801178}]}, {"text": "proposed methods of improving the annotations of word segmentation (WS) for VTB.", "labels": [], "entities": [{"text": "word segmentation (WS)", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8369006931781768}]}, {"text": "They also evaluated different WS criteria in two applications, i.e., machine translation and text classification.", "labels": [], "entities": [{"text": "WS", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9903159737586975}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8346036970615387}, {"text": "text classification", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8338345587253571}]}, {"text": "This paper focuses on improving the quality of parts-of-speech (POS) annotations by using state-of-the-art parsers to provide feedback for this process.", "labels": [], "entities": []}, {"text": "The difficulties with Vietnamese POS tagging have been recognized by many researchers.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.6681029349565506}]}, {"text": "There is little consensus as to the methodology for classifying words.", "labels": [], "entities": []}, {"text": "Polysemous words, words with the same surface form but having different meanings and grammar functions, are very popular in the Vietnamese language.", "labels": [], "entities": []}, {"text": "For example, the word \"cc\" can be a noun that means neck/she, or an adjective that means ancient depending on the context.", "labels": [], "entities": []}, {"text": "This characteristic makes it difficult to tag POSs for Vietnamese, both manually and automatically.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: a brief introduction to VTB and its annotation schemes are provided in Section 2.", "labels": [], "entities": [{"text": "VTB", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8302818536758423}]}, {"text": "Then, previous work is summarized in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes our methods of detecting and correcting inconsistencies in POSs in the VTB corpus.", "labels": [], "entities": [{"text": "VTB corpus", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.9544232189655304}]}, {"text": "Evaluations of these methods are described in Section 5.", "labels": [], "entities": []}, {"text": "Finally, Section 6 explains our evaluations of the Berkeley parser and Minimum-Spanning Tree (MST) parser on different versions of the VTB corpus, which were created by using detected inconsistencies.", "labels": [], "entities": [{"text": "VTB corpus", "start_pos": 135, "end_pos": 145, "type": "DATASET", "confidence": 0.9222952127456665}]}, {"text": "These results from evaluations are considered to be away of measuring the effect of automatically detected and corrected inconsistencies.", "labels": [], "entities": []}, {"text": "We could observe difficulties with Vietnamese that affected the quality of parsers by analyzing the results from parsing.", "labels": [], "entities": []}, {"text": "Our experiences in using state-of-the-art parsers for treebank annotation, which are presented in this paper, should not only benefit the Vietnamese language, but also other languages with similar: VTB part-of-speech tag set characteristics.", "labels": [], "entities": []}], "datasetContent": [{"text": "We detected and corrected MIs and NcIs based on the two data sets, ORG and VAR_FREQ.", "labels": [], "entities": [{"text": "corrected MIs", "start_pos": 16, "end_pos": 29, "type": "METRIC", "confidence": 0.732003390789032}, {"text": "ORG", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9751169085502625}, {"text": "VAR_FREQ", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8308016260464987}]}, {"text": "The ORG data set was the original VTB corpus and VAR_FREQ was the original corpus with modifications to WS annotation.", "labels": [], "entities": [{"text": "ORG data set", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8563220699628195}, {"text": "VTB corpus", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.9486528933048248}, {"text": "VAR_", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9287407100200653}, {"text": "FREQ", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.5261135697364807}]}, {"text": "This setting was made similar to that used by to enable comparison.", "labels": [], "entities": [{"text": "comparison", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.9645732641220093}]}, {"text": "There area total of 128,871 phrases in the VTB corpus.", "labels": [], "entities": [{"text": "VTB corpus", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9455529749393463}]}, {"text": "The top five types of phrases are noun phrases (NPs) (representing 49.6% of the total number of phrases), verb phrases (VPs), prepositional phrases (PPs), adjectival phrases (ADJPs), and quantity phrases (QPs), representing 99.1% of the total number of phrases in the VTB corpus.", "labels": [], "entities": [{"text": "VTB corpus", "start_pos": 268, "end_pos": 278, "type": "DATASET", "confidence": 0.9724234044551849}]}, {"text": "We analyzed the VTB corpus based on these five types of phrases.", "labels": [], "entities": [{"text": "VTB corpus", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.9262698888778687}]}, {"text": "show the overall statistics for MIs and NcIs for each phrase category.", "labels": [], "entities": []}, {"text": "The second and third columns in these tables indicate the numbers of inconsistencies and their instances that were detected in the ORG data set.", "labels": [], "entities": [{"text": "ORG data set", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.9044419924418131}]}, {"text": "The fourth and fifth columns indicate the numbers of inconsistencies and their instances that were detected in the VAR_FREQ data set.", "labels": [], "entities": [{"text": "VAR_FREQ data set", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.7026801407337189}]}, {"text": "The rows in indicate the number of NcIs and the number of instances detected with the NcI_DM1 and NcI_DM2 methods.", "labels": [], "entities": []}, {"text": "We estimated the accuracy of our methods which detected and corrected inconsistencies in POS tag-   ging by manually inspecting inconsistent annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.999639630317688}, {"text": "POS tag-   ging", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.598016194999218}]}, {"text": "We manually inspected the two data sets of ORG_EVAL and ORG_POS_EVAL.", "labels": [], "entities": [{"text": "ORG", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.6180681586265564}]}, {"text": "To create ORG_EVAL, we randomly selected 100 sentences which contained instances of POS inconsistencies in the ORG data set.", "labels": [], "entities": [{"text": "ORG data set", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9182325601577759}]}, {"text": "ORG_EVAL contained 459 instances of 157 POS inconsistencies.", "labels": [], "entities": [{"text": "ORG_EVAL", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8207579652468363}]}, {"text": "ORG_POS_EVAL was the ORG_EVAL data set with corrections made to multi-POS inconsistencies and Nc inconsistencies with our methods of correction above.", "labels": [], "entities": [{"text": "ORG_EVAL data set", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.6816911935806275}]}, {"text": "Detection: We manually checked POS inconsistencies and found that 153 cases out of 157 POS inconsistencies (97.5%) were actual inconsistencies.", "labels": [], "entities": [{"text": "Detection", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7928247451782227}]}, {"text": "There were four cases that our method detected as multi-POS inconsistencies, but they were actually ambiguities in Vietnamese POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 126, "end_pos": 137, "type": "TASK", "confidence": 0.6410664618015289}]}, {"text": "They were polysemous words whose meanings and POS tags depended on surrounding words, but did not depend on their positions in phrases.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8574719727039337}]}, {"text": "For example, the word \"s\u00e1ng\" in the post-head positions of the verb phrases VP1 and VP2 below, can be a noun that means morning in English, or it can bean adjective that means bright, depending on the preceding verb.", "labels": [], "entities": []}, {"text": "Nine configurations of the VTB corpus were created as follows: \u2022 ORG: The original VTB corpus.", "labels": [], "entities": [{"text": "VTB corpus", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.9021926820278168}, {"text": "ORG", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9909845590591431}, {"text": "VTB corpus", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.9532794058322906}]}, {"text": "\u2022 BASE, STRUCT_AFFIX, STRUCT_NC, VAR_SPLIT, VAR_COMB, and VAR_FREQ correspond to different settings for WS described in.", "labels": [], "entities": [{"text": "BASE", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9988937973976135}, {"text": "AFFIX", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.4821745753288269}, {"text": "STRUCT_NC", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.4573228061199188}, {"text": "VAR_SPLIT", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.916684647401174}, {"text": "VAR_COMB", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.8691891431808472}, {"text": "VAR_FREQ", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8087923924128214}, {"text": "WS", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9734243750572205}]}, {"text": "\u2022 ORG_POS: The ORG data set with corrections for multi-POS inconsistencies and Nc inconsistencies by using the methods in Section 4.1 and 4.2.", "labels": [], "entities": [{"text": "ORG data set", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.8614611228307089}]}, {"text": "\u2022 VAR_FREQ_POS: The VAR_FREQ data set with corrections for multi-POS inconsistencies and Nc inconsistencies by using the methods in Section 4.1 and 4.2.", "labels": [], "entities": [{"text": "VAR_FREQ_POS", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.7844801425933838}, {"text": "VAR_FREQ data set", "start_pos": 20, "end_pos": 37, "type": "DATASET", "confidence": 0.678448760509491}]}, {"text": "Each of the nine data sets was randomly split into two subsets for training and testing our parser models.", "labels": [], "entities": []}, {"text": "The training set contained 9,443 sentences, and the testing set contained 1,000 sentences.", "labels": [], "entities": []}, {"text": "Tools We used the Berkeley parser () to evaluate the syntactic parser on VTB.", "labels": [], "entities": [{"text": "VTB", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9549844264984131}]}, {"text": "This parser has been used in experiments in English, German, and Chinese and achieved an F1 of 90.2% on the English Penn Treebank.", "labels": [], "entities": [{"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.999772846698761}, {"text": "English Penn Treebank", "start_pos": 108, "end_pos": 129, "type": "DATASET", "confidence": 0.8918139139811198}]}, {"text": "We used the conversion tool built by to convert VTB into dependency trees.", "labels": [], "entities": []}, {"text": "We used the MST parser to evaluate the dependency parsing on VTB.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.6315032839775085}, {"text": "dependency parsing", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.626357302069664}, {"text": "VTB", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9319076538085938}]}, {"text": "This parser was evaluated on the English Penn Treebank () and 13 other languages ().", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.9502419630686442}]}, {"text": "Its accuracy achieved 90.7% on the English Penn Treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992683529853821}, {"text": "English Penn Treebank", "start_pos": 35, "end_pos": 56, "type": "DATASET", "confidence": 0.8986844619115194}]}, {"text": "We made use of the bracket scoring program EVALB, which was built by,   to evaluate the performance of the Berkeley parser.", "labels": [], "entities": [{"text": "EVALB", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.6981286406517029}]}, {"text": "As an evaluation tool was included in the MST parser tool, we used it to evaluate the MST parser.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.5676356852054596}]}, {"text": "The bracketing F-measures of the Berkeley parser on nine configurations of the VTB corpus are listed in.", "labels": [], "entities": [{"text": "VTB corpus", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9778580367565155}]}, {"text": "The dependency accuracies of the MST parser on nine configurations of the VTB corpus are shown in.", "labels": [], "entities": [{"text": "VTB corpus", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.932633250951767}]}, {"text": "These results indicate that the quality of the treebank strongly affected the quality of the parsers.", "labels": [], "entities": []}, {"text": "According to, all modifications to WS inconsistencies improved the performance of the Berkeley parser except for STRUCT_NC and VAR_SPLIT.", "labels": [], "entities": [{"text": "STRUCT_NC", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.6008820831775665}, {"text": "VAR_SPLIT", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.8779433369636536}]}, {"text": "More importantly, the ORG_POS model achieved better results than the ORG model, and the VAR_FREQ_POS model achieved better results than the VAR_FREQ model, which indicates that the modifications to POS inconsistencies improved the performance of the Berkeley parser.", "labels": [], "entities": [{"text": "VAR", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.8821424841880798}, {"text": "FREQ", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.5136517882347107}, {"text": "FREQ", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.5190296173095703}]}, {"text": "The VAR_FREQ_POS model scored 1.11 point higher than ORG, which is a significant improvement.", "labels": [], "entities": [{"text": "VAR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9953994154930115}, {"text": "FREQ_POS", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8444810907046}, {"text": "ORG", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9304482936859131}]}, {"text": "Dependency accuracies of the MST parser in indicate that all modifications to POS inconsistencies improved the performance of the MST parser.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 130, "end_pos": 140, "type": "TASK", "confidence": 0.7342405617237091}]}], "tableCaptions": [{"text": " Table 2: Statistics for multi-POS inconsistencies  for each phrase category in VTB. Number of In- consistencies (Inc) and Number of Instances (Ins).", "labels": [], "entities": [{"text": "VTB", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9250180125236511}, {"text": "Number of In- consistencies (Inc)", "start_pos": 85, "end_pos": 118, "type": "METRIC", "confidence": 0.8324331864714622}]}, {"text": " Table 3: Statistics for Nc inconsistencies in head  positions of noun phrases in VTB.", "labels": [], "entities": [{"text": "VTB", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9348670840263367}]}, {"text": " Table 5: Top five pairs of confusing POS tags.", "labels": [], "entities": []}, {"text": " Table 6: Bracketing F-measures of Berkeley  parser on nine configurations of VTB corpus.", "labels": [], "entities": [{"text": "Bracketing F-measures", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.6727190613746643}, {"text": "VTB corpus", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9523999392986298}]}, {"text": " Table 7: Dependency accuracy of MSTParser on  nine configurations of VTB corpus. Unlabeled  Accuracy (UA), Labeled Accuracy (LA).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.856317937374115}, {"text": "VTB corpus", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.8899254500865936}, {"text": "Unlabeled  Accuracy (UA)", "start_pos": 82, "end_pos": 106, "type": "METRIC", "confidence": 0.8861192345619202}, {"text": "Labeled Accuracy (LA)", "start_pos": 108, "end_pos": 129, "type": "METRIC", "confidence": 0.872520899772644}]}]}