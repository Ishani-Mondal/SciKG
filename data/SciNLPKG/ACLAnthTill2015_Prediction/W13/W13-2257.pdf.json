{"title": [{"text": "Efficient solutions for word reordering in German-English phrase-based statistical machine translation", "labels": [], "entities": [{"text": "word reordering", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.764009028673172}, {"text": "phrase-based statistical machine translation", "start_pos": 58, "end_pos": 102, "type": "TASK", "confidence": 0.6259482949972153}]}], "abstractContent": [{"text": "Despite being closely related languages, German and English are characterized by important word order differences.", "labels": [], "entities": []}, {"text": "Long-range reordering of verbs, in particular, represents areal challenge for state-of-the-art SMT systems and is one of the main reasons why translation quality is often so poor in this language pair.", "labels": [], "entities": [{"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9898661971092224}]}, {"text": "In this work, we review several solutions to improve the accuracy of German-English word reordering while preserving the efficiency of phrase-based decoding.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9985390901565552}, {"text": "German-English word reordering", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.5893147885799408}]}, {"text": "Among these, we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena.", "labels": [], "entities": []}, {"text": "Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 179, "end_pos": 182, "type": "TASK", "confidence": 0.780942976474762}]}], "introductionContent": [{"text": "Modeling the German-English language pair is known to be a challenging task for state-of-theart statistical machine translation (SMT) methods.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.7849725186824799}]}, {"text": "A major factor of difficulty is given byword order differences that yield important long-range reordering phenomena.", "labels": [], "entities": []}, {"text": "Thanks to specific reordering modeling components, phrase-based SMT (PSMT) systems) are generally good at handling local reordering phenomena that are not captured inside phrases.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.5286780595779419}]}, {"text": "However, they typically fail to predict long reorderings.", "labels": [], "entities": []}, {"text": "On the other hand, hierarchical SMT (HSMT) systems () can learn reordering patterns by means of discontinuous translation rules, and are therefore considered a better choice for language pairs characterized by massive and hierarchical reordering.", "labels": [], "entities": [{"text": "SMT (HSMT)", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.8036797642707825}]}, {"text": "Looking at the results of the Workshop of Machine Translation's last edition (WMT12), no particular SMT approach appears to be clearly dominating.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7199617773294449}, {"text": "WMT12)", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.668449729681015}, {"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9928370118141174}]}, {"text": "In both language directions (official results excluding the online systems) the rule-based systems outperformed all SMT approaches, and among the best SMT systems we find a variety of approaches: pure phrase-based, phrase-based and hierarchical systems combination, n-gram based, a rich syntaxbased approach, and a phrase-based system coupled with POS-based pre-ordering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9849667549133301}]}, {"text": "This gives an idea of how challenging this language pair is for SMT and raises the question of which SMT approach is best suited to model it.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9933903217315674}, {"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9696597456932068}]}, {"text": "In this work, we aim at answering this question by focussing on the word reordering problem, which is known to bean important factor of SMT performance.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7441497147083282}, {"text": "SMT", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.9972346425056458}]}, {"text": "We hypothesize that PSMT can be as successful for GermanEnglish as the more computationally costly HSMT approach, provided that the reordering-related parameters are carefully chosen and the best available reordering models are used.", "labels": [], "entities": [{"text": "PSMT", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.9501670598983765}, {"text": "GermanEnglish", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9733875393867493}]}, {"text": "More specifically, our study covers the following topics: distortion functions and limits, and dynamic shaping of the reordering search space based on a discriminative reordering model.", "labels": [], "entities": []}, {"text": "We first review these topics, and then evaluate them systematically on the WMT task using both generic and reordering-specific metrics, with the aim of providing a reference for future system developers' choices.", "labels": [], "entities": [{"text": "WMT task", "start_pos": 75, "end_pos": 83, "type": "TASK", "confidence": 0.5703203976154327}]}], "datasetContent": [{"text": "A large number of previous works on word reordering measured their success with generalpurpose metrics such as BLEU () or METEOR ().", "labels": [], "entities": [{"text": "word reordering", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7925701439380646}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9975759387016296}, {"text": "METEOR", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.8585025668144226}]}, {"text": "These metrics, however, are only indirectly sensitive to word order and do not sufficiently penalize long-range reordering errors, as demonstrated for instance by.", "labels": [], "entities": []}, {"text": "While BLEU remains a standard choice for many evaluation campaigns, we believe it is extremely important to complement it with metrics that are specifically designed to capture word order differences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9959895014762878}]}, {"text": "In this work, we adopt two reordering-specific metrics in addition to BLEU and METEOR: Kendall Reordering Score (KRS).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9987140893936157}, {"text": "METEOR", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.988594651222229}, {"text": "Kendall Reordering Score (KRS)", "start_pos": 87, "end_pos": 117, "type": "METRIC", "confidence": 0.9162125190099081}]}, {"text": "As proposed by, the KRS measures the similarity between the input-output reordering and the input-reference reordering.", "labels": [], "entities": []}, {"text": "This is done by converting word alignments to permutations and computing a permutation distance among them.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.697888970375061}]}, {"text": "When interpolated with BLEU, this score is called LRscore.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.998225748538971}, {"text": "LRscore", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9770686030387878}]}, {"text": "Verb-specific KRS (KRS-V).", "labels": [], "entities": []}, {"text": "The ideal way to automatically evaluate our systems would be to use syntax-or semantics-based metrics, as the impact of long reordering errors is particularly important at these levels.", "labels": [], "entities": []}, {"text": "As a light-weight alternative, we instead concentrate the evaluation on those word classes that are typically crucial to guess the general structure of a sentence.", "labels": [], "entities": [{"text": "guess the general structure of a sentence", "start_pos": 121, "end_pos": 162, "type": "TASK", "confidence": 0.7474342329161507}]}, {"text": "To this end, we adopt a word-weighted version of the KRS and set the weights to 1 for verbs and 0 for all other words, so that only verb reordering errors are captured.", "labels": [], "entities": []}, {"text": "We call the resulting metric KRS-V.", "labels": [], "entities": []}, {"text": "The KRS-V rates a translation hypothesis as perfect (100%) when the translations of all source verbs are located in their correct position, regardless of the other words' ordering.", "labels": [], "entities": [{"text": "KRS-V", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6515612602233887}]}, {"text": "In this section we evaluate the impact on translation quality and efficiency of the techniques presented above.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9754391312599182}]}, {"text": "Our main objective is to empirically verify the hypothesis that better reordering modeling and better reordering space definition can significantly improve the accuracy of PSMT in German-English without sacrificing its efficiency.", "labels": [], "entities": [{"text": "reordering space definition", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6975889205932617}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9989282488822937}]}, {"text": "We choose the WMT German-English news translation task as our case study.", "labels": [], "entities": [{"text": "WMT German-English news translation task", "start_pos": 14, "end_pos": 54, "type": "TASK", "confidence": 0.7620347261428833}]}, {"text": "More specifically we use the WMT10 training data: Europarl (v.5) plus News-commentary-2010 fora total of 1.6M parallel sentences, 44M German tokens.", "labels": [], "entities": [{"text": "WMT10 training data", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9064893325169882}, {"text": "Europarl", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9154416918754578}, {"text": "News-commentary-2010", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.9300771951675415}]}, {"text": "The target LM is trained on the monolingual news data provided for the constrained track of WMT10 (1133M English tokens).", "labels": [], "entities": [{"text": "WMT10 (1133M English tokens", "start_pos": 92, "end_pos": 119, "type": "DATASET", "confidence": 0.8263776540756226}]}, {"text": "For development we use the WMT08 news benchmark, while for testing we use the following data sets: tests(09-11): the concatentation of three previous years' benchmarks from 2009 to 2011 (8017 sentences, 21K German tokens).", "labels": [], "entities": [{"text": "WMT08 news benchmark", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.9413903951644897}]}, {"text": "test12: the latest released benchmark (3003 sentences, 8K German tokens).", "labels": [], "entities": []}, {"text": "Each data set includes one reference translation.", "labels": [], "entities": []}, {"text": "Note that our goal is not to reach the performance of the best systems participating at the last WMT edition, but rather to assess the usefulness of our techniques on a larger and therefore more reliable test set, while starting from a reasonable baseline.", "labels": [], "entities": [{"text": "WMT edition", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.6299756616353989}]}, {"text": "For German tokenization and compound splitting we use Tree Tagger ( and the Gertwol morphological analyser (.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.726710855960846}, {"text": "compound splitting", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.818222314119339}]}, {"text": "All our SMT systems are built with the Moses toolkit (, and word alignments are generated by the Berkeley Aligner ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9879409074783325}, {"text": "word alignments", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7388540804386139}]}, {"text": "The target language model is estimated by the IRSTLM toolkit) with modified Kneser-Ney smoothing).", "labels": [], "entities": [{"text": "IRSTLM toolkit", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.9116069376468658}]}, {"text": "The phrase-based baseline decoder includes a phrase translation model (two phrasal and two lexical probability features), a lexicalized reordering model (six features), a 6-gram target language model, distortion cost, word and phrase penalties.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8001149296760559}]}, {"text": "As lexicalized reordering model, we use a hierarchical phrase orientation model () trained on all the parallel data using three orientation classes -monotone, swap or discontinuous -in bidirectional mode.", "labels": [], "entities": []}, {"text": "Statistically Our results on test12 are not directly comparable to the WMT12 submissions due to the different training data: that is, the WMT12 parallel data includes 50M German tokens of Europarl data and 4M of news-commentary, as opposed to the 41M and 2.5M released for WMT10 and used in our experiments.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.8202680945396423}, {"text": "WMT12 parallel data", "start_pos": 138, "end_pos": 157, "type": "DATASET", "confidence": 0.8400850892066956}, {"text": "Europarl data", "start_pos": 188, "end_pos": 201, "type": "DATASET", "confidence": 0.9449054002761841}, {"text": "WMT10", "start_pos": 273, "end_pos": 278, "type": "DATASET", "confidence": 0.9355542659759521}]}, {"text": "6 http://www2.lingsoft.fi/cgi-bin/gertwol improbable phrase pairs are pruned from the translation model as proposed by.", "labels": [], "entities": []}, {"text": "The hierarchical system is trained and tested using the standard Moses configuration which includes: a rule table (two phrasal and two lexical probability features), a 6-gram target language model, word and rule penalties.", "labels": [], "entities": []}, {"text": "We set the span constraint (cf. Section 5) to the default value of 10 words for rule extraction, while for decoding we consider two different settings: the default 10 words and a large value of 20 to enable very longrange reorderings.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.8238031566143036}]}, {"text": "Feature weights for all systems are optimized by minimum BLEU-error training (Och, 2003) on test08.", "labels": [], "entities": [{"text": "BLEU-error", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9972955584526062}, {"text": "Och, 2003)", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.7059556543827057}]}, {"text": "To reduce the effects of the optimizer instability, we tune each configuration four times and use the average of the resulting weight vectors for testing, as suggested by.", "labels": [], "entities": []}, {"text": "The source-to-reference word alignments that are needed to compute the reordering scores are generated by the Berkeley Aligner previously trained on the training data.", "labels": [], "entities": []}, {"text": "Source-to-output alignments are obtained from the decoder's trace.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effects of WaW reordering model and early reordering pruning on PSMT translation quality  and efficiency, compared against a hierarchical SMT baseline. Translation quality is measured with  % BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically  significant differences with respect to the previous row are marked with \ud97b\udf59\ud97b\udf59 at the p \u2264 .05 level and \ud97b\udf59\ud97b\udf59  at the p \u2264 .10 level. Decoding time is measured in milliseconds per input word.", "labels": [], "entities": [{"text": "PSMT translation", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.9107250869274139}, {"text": "BLEU", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9991565942764282}, {"text": "METEOR", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.9939780235290527}, {"text": "Kendall Reordering Score", "start_pos": 220, "end_pos": 244, "type": "METRIC", "confidence": 0.8514989217122396}]}, {"text": " Table 3: Decoding statistics of three PSMT sys- tems exploring different reordering search spaces  for the translation of test12.", "labels": [], "entities": []}]}