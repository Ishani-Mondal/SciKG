{"title": [{"text": "Surface Text based Dialogue Models for Virtual Humans", "labels": [], "entities": []}], "abstractContent": [{"text": "We present virtual human dialogue models which primarily operate on the surface text level and can be extended to incorporate additional information state annotations such as topics or results from simpler models.", "labels": [], "entities": []}, {"text": "We compare these models with previously proposed models as well as two human-level upper baselines.", "labels": [], "entities": []}, {"text": "The models are evaluated by collecting appropri-ateness judgments from human judges for responses generated fora set of fixed dialogue contexts.", "labels": [], "entities": []}, {"text": "Our results show that the best performing models achieve close to human-level performance and require only surface text dialogue transcripts to train.", "labels": [], "entities": []}], "introductionContent": [{"text": "Virtual Humans (VH) are autonomous agents who can play the role of humans in simulations ().", "labels": [], "entities": []}, {"text": "For these simulations to be convincing these agents must have the ability to communicate with humans and other agents using natural language.", "labels": [], "entities": []}, {"text": "Like other dialogue system types, different architectures have been proposed for virtual human dialogue systems.", "labels": [], "entities": []}, {"text": "These architectures can afford different features and require different sets of resources.", "labels": [], "entities": []}, {"text": "E.g., an information state based architecture such as the one used in SASO-ST () can model detailed understanding of the task at hand and progression of dialogue, but at the cost of requiring resources such as information state update rules and an annotated corpus or grammar to be able to map surface text to dialogue acts.", "labels": [], "entities": [{"text": "SASO-ST", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9497618079185486}]}, {"text": "For some virtual human dialogue genres such as simple question-answering or some negotiation domains, a simple model of dialogue progression would suffice.", "labels": [], "entities": []}, {"text": "In such a case we can build dialogue models that primarily operate on a surface text level.", "labels": [], "entities": []}, {"text": "These models only require surface text dialogue transcripts as a resource, and don't require expensive manual update rules, grammars, or even extensive corpus annotation.", "labels": [], "entities": []}, {"text": "In this paper, we describe the construction and evaluation of several models for engaging in dialogue by selecting an utterance that has been seen previously in a corpus.", "labels": [], "entities": []}, {"text": "We include one model that has been used for this task previously (, an adaptation of a model that has been used in a similar manner, though on handauthored data sets, rather than data sets extracted automatically from a corpus (, as well as anew set of models, using perceptrons on surface text features as well as more abstract information state annotations such as topics.", "labels": [], "entities": []}, {"text": "We also tackle the question of evaluating such dialogue models manually as well as automatically, starting with systematically analyzing various decisions involved in the evaluation process.", "labels": [], "entities": []}, {"text": "We situate our work with respect to previous evaluation methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments reported in this paper, we used the human-human spoken dialogue corpus collected for the project SASO-ST ().", "labels": [], "entities": []}, {"text": "In this scenario, the trainee acts as an Army Captain negotiating with a simulated doc- We perform a Static Context evaluation.", "labels": [], "entities": []}, {"text": "In Static Context evaluation, all the dialogue models being evaluated receive the same set of contexts as input.", "labels": [], "entities": [{"text": "Static Context evaluation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.9451262156168619}]}, {"text": "These dialogue contexts are extracted from actual in-domain human-human dialogues and are not affected by the dialogue model being evaluated.", "labels": [], "entities": []}, {"text": "For every turn whose role is to be played by the system, we predict the most appropriate response in place of that turn given the dialogue context.", "labels": [], "entities": []}, {"text": "Since the goal for virtual humans is to be as human-like as possible, a suitable evaluation metric is how appropriate or human-like the responses are fora given dialogue context.", "labels": [], "entities": []}, {"text": "The evaluation reported here employs human judges.", "labels": [], "entities": []}, {"text": "We setup a simple subjective 5-point likert scale for rating appropriateness -1 being a very inappropriate nonsensical response and 5 being a perfectly appropriate response.", "labels": [], "entities": []}, {"text": "We built five dialogue models to play the role of the doctor in SASO-ST domain, viz.: Nearest Context (section 3.1), Cross-lingual Relevance Model (section 3.2) and three perceptron models (section 3.3) with different feature sets.", "labels": [], "entities": [{"text": "SASO-ST domain", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.6218943297863007}]}, {"text": "These dialogue models are evaluated using 5 in-domain human-human dialogues from the training data (2 roleplay and 3 WoZ dialogues, referred to as test dialogues).", "labels": [], "entities": []}, {"text": "A dialogue model is trained in a leaveone-out fashion where the training data consists of all dialogues except the one test dialogue that is being evaluated.", "labels": [], "entities": []}, {"text": "A dialogue model trained in this fashion is then used to predict the most appropriate response for every context that appears in the test dialogue.", "labels": [], "entities": []}, {"text": "This process is repeated for each test dialogue and for each dialogue model being evaluated.", "labels": [], "entities": []}, {"text": "In this evaluation setting, the actual response utterance found in the original human-human dialogue may not belong to the set of utterances being ranked by the dialogue model.", "labels": [], "entities": []}, {"text": "We also compare these five dialogue models with two human-level upper baselines.", "labels": [], "entities": []}, {"text": "in the appendix shows some examples of utterances returned by a couple of the models.", "labels": [], "entities": []}, {"text": "We performed a static context evaluation using four judges for the above-mentioned two humanlevel baselines (Wizard Random and Wizard Max Voted) and five dialogue models (Nearest Context, Cross-lingual Relevance Model and three perceptron models), as described in section 3.3.", "labels": [], "entities": []}, {"text": "We tune the parameters used for the perceptron models based on the automatic evaluation metric, Weak Agreement (DeVault et al., 2011).", "labels": [], "entities": []}, {"text": "According to this evaluation metric a response utterance is judged as perfectly appropriate (a score of 5) if any of the wizards chose this response utterance forgiven context and inappropriate (a score of 0) otherwise.", "labels": [], "entities": []}, {"text": "The Perceptron(surface) model was trained using 30 iterations, the Perceptron(surface+retrieval) using 20 iterations, and the Perceptron(surface+retrieval+topic) was trained using 25 iterations.", "labels": [], "entities": []}, {"text": "For all perceptron models we used threshold x = threshold y = threshold xy = 3.", "labels": [], "entities": []}, {"text": "For a comparative evaluation of dialogue models, we need an evaluation setup where judges could seethe complete dialogue context along with the response utterances generated by the dialogue models to be evaluated.", "labels": [], "entities": []}, {"text": "In this setup, we show all the response utterances next to each other for easy comparison and we do not show the actual response utterance that was encountered in the original human-human dialogue.", "labels": [], "entities": []}, {"text": "We built a web interface for collecting appropriateness ratings that addresses the above requirements.", "labels": [], "entities": []}, {"text": "shows the web interface used by the four judges to evaluate the appropriateness of response utterances forgiven dialogue context.", "labels": [], "entities": []}, {"text": "The appropriateness was rated on the same scale of 1 to 5.", "labels": [], "entities": [{"text": "appropriateness", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9425374865531921}]}, {"text": "The original human-human dialogue (roleplay or WoZ) is shown on the left hand side and the response utterances from different dialogue models are shown on the right hand side.", "labels": [], "entities": []}, {"text": "In cases where different dialogue models produce the same surface text response only one candidate surface text is shown to judge.", "labels": [], "entities": []}, {"text": "Once the judge has rated all the candidate responses they can proceed to the next dialogue context.", "labels": [], "entities": []}, {"text": "This setting allows for comparative evaluation of different dialogue models.", "labels": [], "entities": []}, {"text": "The presentation order of responses from different dialogue models is randomized.", "labels": [], "entities": []}, {"text": "Two of the judges also performed the role of the wizards in our wizard data collection as outlined in section 4.1, but the wizard data collection and the evaluation tasks were separated by a period of over 3 months.", "labels": [], "entities": []}, {"text": "shows the results of our comparative evaluation for each judge and averaged overall judges.", "labels": [], "entities": []}, {"text": "We also computed inter-rater agreement for individual ratings for all response utterances using).", "labels": [], "entities": []}, {"text": "There were a total of n = 397 distinct response utterances that were judged by the evaluators.", "labels": [], "entities": []}, {"text": "The Krippendorff's \u03b1 for all four judges was 0.425 and it ranges from 0.359 to 0.495 for different subsets of judges.", "labels": [], "entities": []}, {"text": "The value of \u03b1 indicates that the inter-rater agreement is substantially above chance (\u03b1 > 0), but indicates a fair amount of disagreement, indicating that judging appropriateness is a hard task even for human judges.", "labels": [], "entities": []}, {"text": "Although there is low inter-rater agreement at the individual response utterance level there is high agreement at the dialogue model level.", "labels": [], "entities": []}, {"text": "Pearson's correlation between the average appropriateness for different dialogue models ranges from 0.928 to 0.995 for different pairs of judges.", "labels": [], "entities": []}, {"text": "We performed a paired Wilcoxon test to check for statistically significant differences in different dialogue models.", "labels": [], "entities": []}, {"text": "Wizard Max Voted is significantly more appropriate than all other models (p < 0.001).", "labels": [], "entities": [{"text": "Wizard Max Voted", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8507136106491089}]}, {"text": "Wizard Random is significantly more appropriate than Cross-lingual Relevance Model (p < 0.05) and significantly more appropriate than the three perceptron models as well as Nearest Context model (p < 0.001).", "labels": [], "entities": []}, {"text": "Cross-lingual Relevance Model is significantly more appropriate than Nearest Context (p < 0.01).", "labels": [], "entities": []}, {"text": "All other differences are not statistically significant at the 5 percent level.", "labels": [], "entities": []}, {"text": "We found that adding topic annotations did not help.", "labels": [], "entities": []}, {"text": "This is in contrast with previous observation (, where topic information helped when evaluation was performed in Dynamic Context setting.", "labels": [], "entities": []}, {"text": "In Dynamic Context setting, the dialogue model is used in an online fashion where the response utterances it generates become part of the dialogue contexts with respect to which the subsequent responses are predicted and evaluated.", "labels": [], "entities": [{"text": "Dynamic Context setting", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.8249091704686483}]}, {"text": "The topic information ensures systematic progression of dialogue.", "labels": [], "entities": []}, {"text": "But for static context evaluation such help is not required as the dialogue contexts are extracted from human human dialogues and are fixed.", "labels": [], "entities": [{"text": "static context evaluation", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.6565848390261332}]}], "tableCaptions": [{"text": " Table 1: Offline comparative evaluation of dialogue models.", "labels": [], "entities": []}]}