{"title": [{"text": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.6325423419475555}]}], "abstractContent": [{"text": "The CoNLL-2013 shared task was devoted to grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.7367118994394938}]}, {"text": "In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task.", "labels": [], "entities": []}, {"text": "We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.847335954507192}, {"text": "Seventeenth Conference on Computational Natural Language Learning in 2013", "start_pos": 55, "end_pos": 128, "type": "TASK", "confidence": 0.6047554446591271}]}, {"text": "In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay.", "labels": [], "entities": []}, {"text": "This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (.", "labels": [], "entities": []}, {"text": "In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.6150784492492676}, {"text": "semantic role labeling", "start_pos": 146, "end_pos": 168, "type": "TASK", "confidence": 0.6485288838545481}, {"text": "dependency parsing", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.7729229629039764}, {"text": "coreference resolution", "start_pos": 193, "end_pos": 215, "type": "TASK", "confidence": 0.9243574142456055}, {"text": "grammatical error correction", "start_pos": 217, "end_pos": 245, "type": "TASK", "confidence": 0.7610117793083191}]}, {"text": "This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.636378655831019}]}, {"text": "Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwide are learning English and they benefit directly from an automated grammar checker.", "labels": [], "entities": []}, {"text": "The CoNLL-2013 shared task provides a forum for participating teams to work on the same grammatical error correction task, with evaluation on the same blind test set using the same evaluation metric and scorer.", "labels": [], "entities": [{"text": "grammatical error correction task", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.7109246179461479}]}, {"text": "This overview paper contains a detailed description of the shared task, and is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides the task definition.", "labels": [], "entities": []}, {"text": "Section 3 describes the annotated training data provided and the blind test data.", "labels": [], "entities": []}, {"text": "Section 4 describes the evaluation metric and the scorer.", "labels": [], "entities": [{"text": "scorer", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9633030891418457}]}, {"text": "Section 5 lists the participating teams and outlines the approaches to grammatical error correction used by the teams.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.712521493434906}]}, {"text": "Section 6 presents the results of the shared task.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "A grammatical error correction system is evaluated by how well its proposed corrections or edits match the gold-standard edits.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.6007264455159506}]}, {"text": "An essay is first sentence-segmented and tokenized before evaluation is carried out on the essay.", "labels": [], "entities": []}, {"text": "To illustrate, consider the following tokenized sentence S written by an English learner: There is no a doubt, tracking system ID Prompt 1 Surveillance technology such as RFID (radio-frequency identification) should not be used to track people (e.g., human implants and RFID tags on people or products).", "labels": [], "entities": []}, {"text": "Support your argument with concrete examples.", "labels": [], "entities": []}, {"text": "2 Population aging is a global phenomenon.", "labels": [], "entities": []}, {"text": "Studies have shown that the current average lifespan is over 65.", "labels": [], "entities": []}, {"text": "Projections of the United Nations indicate that the population aged 60 or over in developed and developing countries is increasing at 2% to 3% annually.", "labels": [], "entities": [{"text": "United Nations", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9207671880722046}]}, {"text": "Explain why rising life expectancies can be considered both a challenge and an achievement. has brought many benefits in this information age . The set of gold-standard edits of a human annotator is g = {a doubt \u2192 doubt, system \u2192 systems, has \u2192 have}.", "labels": [], "entities": []}, {"text": "Suppose the tokenized output sentence H of a grammatical error correction system given the above sentence is: There is no doubt, tracking system has brought many benefits in this information age . That is, the set of system edits is e = {a doubt \u2192 doubt}.", "labels": [], "entities": []}, {"text": "The performance of the grammatical error correction system is measured by how well the two sets g and e match, in the form of recall R, precision P , and F 1 measure: R = 1/3, P = 1/1, F 1 = 2RP/(R + P ) = 1/2.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6126391887664795}, {"text": "recall R", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9547233879566193}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9901211261749268}, {"text": "F 1 measure", "start_pos": 154, "end_pos": 165, "type": "METRIC", "confidence": 0.9442848563194275}]}, {"text": "More generally, given a set of n sentences, where g i is the set of gold-standard edits for sentence i, and e i is the set of system edits for sentence i, recall, precision, and F 1 are defined as follows: where the intersection between g i and e i for sentence i is defined as Evaluation by the HOO scorer () is based on computing recall, precision, and F 1 measure as defined above.", "labels": [], "entities": [{"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9960230588912964}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9927822351455688}, {"text": "F 1", "start_pos": 178, "end_pos": 181, "type": "METRIC", "confidence": 0.9944524168968201}, {"text": "recall", "start_pos": 332, "end_pos": 338, "type": "METRIC", "confidence": 0.9977244734764099}, {"text": "precision", "start_pos": 340, "end_pos": 349, "type": "METRIC", "confidence": 0.9937950372695923}, {"text": "F 1 measure", "start_pos": 355, "end_pos": 366, "type": "METRIC", "confidence": 0.9881332914034525}]}, {"text": "Note that there are multiple ways to specify a set of gold-standard edits that denote the same corrections.", "labels": [], "entities": []}, {"text": "For example, in the above learner-written sentence S, alternative but equivalent sets of goldstandard edits are {a \u2192 , system \u2192 systems, has \u2192 have}, {a \u2192 , system has \u2192 systems have}, etc.", "labels": [], "entities": []}, {"text": "Given the same learner-written sentence Sand the same system output sentence H shown above, one would expect a scorer to give the same R, P, F 1 scores regardless of which of the equivalent sets of gold-standard edits is specified by an annotator.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.951092541217804}]}, {"text": "However, this is not the case with the HOO scorer.", "labels": [], "entities": [{"text": "HOO scorer", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.7539092004299164}]}, {"text": "This is because the HOO scorer uses GNU wdiff 2 to extract the differences between the learner-written sentence Sand the system output sentence H to form a set of system edits.", "labels": [], "entities": []}, {"text": "Since in general there are multiple ways to specify a set of gold-standard edits that denote the same corrections, the set of system edits computed by the HOO scorer may not match the set of gold-standard edits specified, leading to erroneous scores.", "labels": [], "entities": [{"text": "HOO scorer", "start_pos": 155, "end_pos": 165, "type": "DATASET", "confidence": 0.7121243476867676}]}, {"text": "In the above example, the set of system edits computed by the HOO scorer for Sand H is {a \u2192 }.", "labels": [], "entities": [{"text": "HOO scorer", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.708001583814621}]}, {"text": "Given that the set of gold-standard edits g is {a doubt \u2192 doubt, system \u2192 systems, has \u2192 have}, the scores computed by the HOO scorer are R = P = F 1 = 0, which are erroneous.", "labels": [], "entities": [{"text": "HOO scorer", "start_pos": 123, "end_pos": 133, "type": "DATASET", "confidence": 0.5786832422018051}]}, {"text": "The MaxMatch (M 2 ) scorer 3 (Dahlmeier and Ng, 2012b) was designed to overcome this limitation of the HOO scorer.", "labels": [], "entities": [{"text": "MaxMatch (M 2 ) scorer 3", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.6741723077637809}]}, {"text": "The key idea is that the set of system edits automatically computed and used in scoring should be the set that maximally matches the set of gold-standard edits specified by the annotator.", "labels": [], "entities": []}, {"text": "The M 2 scorer uses an efficient algorithm to search for such a set of system edits using an edit lattice.", "labels": [], "entities": []}, {"text": "In the above example, given S, H, and g, the M 2 scorer is able to come up with the best matching set of system edits e = {a doubt \u2192 doubt}, thus giving the correct scores R = 1/3, P = 1/1, F 1 = 1/2.", "labels": [], "entities": []}, {"text": "We use the M 2 scorer in the CoNLL-2013 shared task.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.8249256014823914}, {"text": "CoNLL-2013 shared task", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.8227324485778809}]}, {"text": "The original M 2 scorer implemented in assumes that there is one set of gold-standard edits g i for each sentence i.", "labels": [], "entities": []}, {"text": "However, it is often the case that multiple alternative corrections are acceptable fora sentence.", "labels": [], "entities": []}, {"text": "As we allow participating teams to submit alternative sets of gold-standard edits fora sentence, we also extend the M 2 scorer to deal with multiple alternative sets of gold-standard edits.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.827326238155365}]}, {"text": "Based on Equations 1 and 2, Equation 3 can be re-expressed as: To deal with multiple alternative sets of goldstandard edits g i fora sentence i, the extended M 2 scorer chooses the g i that maximizes the cumulative F 1 score for sentences 1, . .", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 215, "end_pos": 224, "type": "METRIC", "confidence": 0.9259506662686666}]}, {"text": "Ties are broken based on the following criteria: first choose the g i that maximizes the numerator n i=1 |g i \u2229 e i |, then choose the g i that minimizes the denominator n i=1 (|g i | + |e i |), finally choose the g i that appears first in the list of alternatives.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of training and test data.", "labels": [], "entities": []}, {"text": " Table 3: Error type distribution of the training and  test data.", "labels": [], "entities": [{"text": "Error type distribution", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8907162745793661}]}, {"text": " Table 7: Scores (in %) without alternative an- swers.", "labels": [], "entities": []}, {"text": " Table 8: Scores (in %) with alternative answers.", "labels": [], "entities": []}]}