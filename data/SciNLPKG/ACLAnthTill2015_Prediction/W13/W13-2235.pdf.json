{"title": [{"text": "Dramatically Reducing Training Data Size Through Vocabulary Saturation", "labels": [], "entities": [{"text": "Dramatically Reducing Training Data Size", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5699028074741364}, {"text": "Saturation", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.7848367691040039}]}], "abstractContent": [{"text": "Our field has seen significant improvements in the quality of machine translation systems over the past several years.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7605920732021332}]}], "introductionContent": [{"text": "The push to build higher and higher quality Statistical Machine Translation systems has led the efforts to collect more and more data.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.8500657280286154}]}, {"text": "The English-French (nearly) Gigaword Parallel Corpus, which we will refer to henceforth as EnFrGW, is the result of one such effort.", "labels": [], "entities": [{"text": "Gigaword Parallel Corpus", "start_pos": 28, "end_pos": 52, "type": "DATASET", "confidence": 0.5743240813414255}, {"text": "EnFrGW", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.9306263327598572}]}, {"text": "The EnFrGW is a publicly available corpus scraped from Canadian, European and international Web sites, consisting of over 22.5M parallel English-French sentences.", "labels": [], "entities": [{"text": "EnFrGW", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9069383144378662}]}, {"text": "This corpus has been used regularly in the WMT competition since 2009.", "labels": [], "entities": [{"text": "WMT competition", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6575965583324432}]}, {"text": "As the size of data increases, BLEU scores increase, but the increase in BLEU is not linear in relation to data size.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9962620139122009}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9986233711242676}]}, {"text": "The relationship between data size and BLEU flattens fairly quickly, as demonstrated in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.999128520488739}]}, {"text": "Here we see that BLEU scores increase rapidly with small amounts of data, but they taper off and flatten at much larger amounts.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.9733769297599792}]}, {"text": "Clearly, as we add more data, the value of the new data diminishes with each increase, until very little value is achieved through the addition of each new sentence.", "labels": [], "entities": []}, {"text": "However, given that this figure represents samples from EnFrGW, can we be more efficient in the samples we take?", "labels": [], "entities": [{"text": "EnFrGW", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.952587902545929}]}, {"text": "Can we achieve near equivalent BLEU scores on much smaller amounts of data drawn from the same source, most especially better than what we can achieve through random sampling?", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9986985921859741}]}, {"text": "The focus of this work is three-fold.", "labels": [], "entities": []}, {"text": "First, we seek to devise a method to reduce the size of training data, which can be run independently of particular dev and test data, so as to maintain the independence of the data, since we are not interested herein domain adaptation or selective tuning.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 218, "end_pos": 235, "type": "TASK", "confidence": 0.7152452319860458}]}, {"text": "Second, we desire an algorithm that is (mostly) quality preserving, as measured by BLEU, OOV rates, and human eval, ultimately resulting in decreased training times and reduced model sizes.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9996163845062256}, {"text": "OOV rates", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9815063178539276}]}, {"text": "Reduced: BLEU score increase as more data is added (in millions of words), random samples from EnFrGW training times provide for greater iterative capacity, since we can make more rapid algorithmic improvements and do more experimentation on smaller data than we canon much larger data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9774920642375946}]}, {"text": "Since we operate in a production environment, deploying smaller models is also desirable.", "labels": [], "entities": []}, {"text": "Third, we require a method that scales to very large data.", "labels": [], "entities": []}, {"text": "We show in the sections below the application of an algorithm at various settings to the 22.5M sentence EnFrGW corpus.", "labels": [], "entities": [{"text": "EnFrGW corpus", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.9256246089935303}]}, {"text": "Although large, 22.5M sentences does not represent the full total of the English-French data on the Web.", "labels": [], "entities": []}, {"text": "We require an algorithm that can apply to even larger samples of data, on the order of tens to hundreds of millions of sentences.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: English-side Sentence Counts (in mil- lions) for different thresholds for VSF, VSF after  ordering the data based on normalized combined  alignment score and random baselines.", "labels": [], "entities": [{"text": "VSF", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.6665667295455933}, {"text": "VSF", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9142683744430542}]}, {"text": " Table 3: English-side Word Counts for different  thresholds for VSF, VSF after ordering the data  based on normalized combined alignment score  and random baselines.", "labels": [], "entities": [{"text": "VSF", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8019638061523438}, {"text": "VSF", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9792241454124451}]}, {"text": " Table 4: Word alignment times (hh:mm) for dif- ferent thresholds for VSF, VSF after model score  ordering, and a random baseline", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6458019912242889}]}, {"text": " Table 5: BLEU Score results for VSF, S+VSF (Sorted VSF), and Random Baseline at different thresholds  t.", "labels": [], "entities": [{"text": "BLEU Score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9574577510356903}]}, {"text": " Table 7: VSF applied to a 65.2M sentence baseline  system.", "labels": [], "entities": []}]}