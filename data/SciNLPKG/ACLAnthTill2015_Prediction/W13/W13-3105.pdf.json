{"title": [{"text": "CIST System Report for ACL MultiLing 2013 --Track 1: Multilingual Multi-document Summarization", "labels": [], "entities": [{"text": "CIST System Report for ACL MultiLing 2013", "start_pos": 0, "end_pos": 41, "type": "DATASET", "confidence": 0.7415371835231781}, {"text": "Multilingual Multi-document Summarization", "start_pos": 53, "end_pos": 94, "type": "TASK", "confidence": 0.5934825539588928}]}], "abstractContent": [{"text": "This report provides a description of the methods applied in CIST system participating ACL MultiLing 2013.", "labels": [], "entities": [{"text": "CIST system participating ACL MultiLing 2013", "start_pos": 61, "end_pos": 105, "type": "DATASET", "confidence": 0.9319823384284973}]}, {"text": "Summarization is based on sentence extraction.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7336854934692383}]}, {"text": "hLDA topic model is adopted for multilingual multi-document mod-eling.", "labels": [], "entities": []}, {"text": "Various features are combined to evaluate and extract candidate summary sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "CIST system has participated Track 1: Multilingual Multi-document Summarization in ACL MultiLing 2013 workshop.", "labels": [], "entities": [{"text": "CIST system", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9671431481838226}, {"text": "Multilingual Multi-document Summarization in ACL MultiLing 2013 workshop", "start_pos": 38, "end_pos": 110, "type": "TASK", "confidence": 0.7026557587087154}]}, {"text": "It could deal with all ten languages: Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian and Spanish.", "labels": [], "entities": []}, {"text": "It summarizes every topic containing 10 texts and generates a summary in plain text, UTF8 encoding, less than 250 words.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to make sure that a hierarchy is good, we need to evaluate its performance.", "labels": [], "entities": []}, {"text": "The best method is human reading, but it's too laborious to browse all topics and all languages.", "labels": [], "entities": [{"text": "human reading", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.7007519900798798}]}, {"text": "In fact, we could not understand all ten languages at all.", "labels": [], "entities": []}, {"text": "So we build another simpler and faster evaluation method based on numbers.", "labels": [], "entities": []}, {"text": "According to former empirical analysis, if a hierarchy has more than 4 paths and the sentence numbers for all paths appear in balanced order from bigger to smaller, and the sentences in bigger paths could occupy 70-85% in all sentences, then we could possibly infer that this hierarchy is good.", "labels": [], "entities": []}, {"text": "In the hLDA result, sentences are clustered into sub-topics in a hierarchical tree.", "labels": [], "entities": []}, {"text": "A sub-topic is more important if it contains more sentences.", "labels": [], "entities": []}, {"text": "Trivial sub-topics containing only one or two sentences could be neglected.", "labels": [], "entities": []}, {"text": "Final summary should cover those most important sub-topics with their most representative sentences.", "labels": [], "entities": []}, {"text": "We evaluate the sentence importance in a sub-topic considering three features.", "labels": [], "entities": []}, {"text": "1) Sentence coverage, which means that how much a sentence could contain words appearing in more sentences fora sub-topic.", "labels": [], "entities": [{"text": "Sentence coverage", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.4850616455078125}]}, {"text": "We consider sentence coverage of each word in one sentence.", "labels": [], "entities": []}, {"text": "The sentence weight is calculated as eq. is the number of sentences that w i covers, | s | is the number of words in the sentence, and n is the total number of all sentences.", "labels": [], "entities": []}, {"text": "2) Word Abstractive level.", "labels": [], "entities": [{"text": "Abstractive level", "start_pos": 8, "end_pos": 25, "type": "METRIC", "confidence": 0.8284785747528076}]}, {"text": "hLDA constructs a hierarchy by positioning all sentences on a threelevel tree.", "labels": [], "entities": []}, {"text": "Level 0 is the most abstractive one, level 2 is the most specific one, and level 1 is between them.", "labels": [], "entities": []}, {"text": "We evaluate the sentence abstractive feature as eq..", "labels": [], "entities": []}, {"text": "Where num(W 0 ), num(W 1 ), num(W 2 ) are numbers of level 0, 1 and 2 words respectively in the sentence.", "labels": [], "entities": []}, {"text": "There are three parameters: a, band c, which are used to control the weights for words in different levels.", "labels": [], "entities": []}, {"text": "Although we hope the summary to be as abstractive as possible, there is really some specific information we also want.", "labels": [], "entities": []}, {"text": "For instance, earthquake news needs specific information about death toll and money lost.", "labels": [], "entities": []}, {"text": "We consider the number of named entities in one sentence.", "labels": [], "entities": []}, {"text": "This time we only have time to use Stanford's named entity recognition toolkit 4 , which could identify English person, address and institutional names.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.5612684090932211}]}, {"text": "If one sentence contains more entities, then it has a high priority to be chosen as candidate summary sentence.", "labels": [], "entities": []}, {"text": "Let Sn be the number of named entity categories in one sentence.", "labels": [], "entities": []}, {"text": "For example, if one sentence has only person names, then Sn is 1; else if it also has address information, then Sn is 2; else if it contains all three categories, then Sn is 3.", "labels": [], "entities": []}, {"text": "At last, we calculate sentence score S as eq.", "labels": [], "entities": []}, {"text": "We've got only the automatic evaluation result.", "labels": [], "entities": []}, {"text": "CIST could get best performance in some language, such as Hindi in ROUGE, and in some topics, such as Arabic M104, English and Romania M005, Czech M007, Spanish M103 etc.", "labels": [], "entities": [{"text": "CIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9350385665893555}, {"text": "ROUGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.8752473592758179}, {"text": "English and Romania M005", "start_pos": 115, "end_pos": 139, "type": "DATASET", "confidence": 0.6213182806968689}]}, {"text": "in Ngram graph methods: AutoSummENG, MeMoG and NPowER.", "labels": [], "entities": [{"text": "MeMoG", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8642609119415283}]}, {"text": "CIST could also get nearly worst performance in some cases, such as French and Hebrew.", "labels": [], "entities": [{"text": "CIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8847361207008362}]}, {"text": "In other cases it gets middle performance.", "labels": [], "entities": [{"text": "middle performance", "start_pos": 23, "end_pos": 41, "type": "METRIC", "confidence": 0.9734659194946289}]}, {"text": "But Chinese result looks very strange to us; we think that it needs more special discussion.", "labels": [], "entities": [{"text": "Chinese result", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8184838593006134}]}], "tableCaptions": [{"text": " Table 1 shows the details.  Parameter  Setting  ETA  1.2 0.5 0.05  GAM  1.0 1.0  GEM_MEAN  0.5  GEM_SCALE  100  SCALING_SHAPE  1.0  SCALING_SCALING  0.5  SAMPLE_ETA  0  SAMPLE_GAM  0", "labels": [], "entities": [{"text": "Parameter  Setting  ETA  1.2 0.5 0.05  GAM  1.0 1.0  GEM_MEAN", "start_pos": 29, "end_pos": 90, "type": "METRIC", "confidence": 0.7680731564760208}]}, {"text": " Table 2: original bad result", "labels": [], "entities": [{"text": "original bad result", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.6912046869595846}]}, {"text": " Table 3.  Parameter  Setting  ETA  5.2 0.005 0.0005  GAM  1.0 1.0  GEM_MEAN  0.35  GEM_SCALE  100  SCALING_SHAPE  2.0  SCALING_SCALING  1.0  SAMPLE_ETA  0  SAMPLE_GAM  0", "labels": [], "entities": [{"text": "ETA  5.2 0.005 0.0005  GAM  1.0 1.0  GEM_MEAN  0.35  GEM", "start_pos": 31, "end_pos": 87, "type": "DATASET", "confidence": 0.7512217462062836}]}, {"text": " Table 3: Adjusted parameter settings", "labels": [], "entities": []}]}