{"title": [{"text": "Overview of the SPMRL 2013 Shared Task: Cross-Framework Evaluation of Parsing Morphologically Rich Languages *", "labels": [], "entities": [{"text": "SPMRL 2013 Shared Task", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.673717200756073}]}], "abstractContent": [{"text": "This paper reports on the first shared task on statistical parsing of morphologically rich languages (MRLs).", "labels": [], "entities": [{"text": "statistical parsing of morphologically rich languages (MRLs)", "start_pos": 47, "end_pos": 107, "type": "TASK", "confidence": 0.8766128155920241}]}, {"text": "The task features data sets from nine languages, each available both in constituency and dependency annotation.", "labels": [], "entities": []}, {"text": "We report on the preparation of the data sets, on the proposed parsing scenarios, and on the evaluation metrics for parsing MRLs given different representation types.", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9656805396080017}, {"text": "parsing MRLs", "start_pos": 116, "end_pos": 128, "type": "TASK", "confidence": 0.9251886308193207}]}, {"text": "We present and analyze parsing results obtained by the task participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9418692290782928}]}, {"text": "Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades.", "labels": [], "entities": [{"text": "dependency-based parsing", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.6448800712823868}]}, {"text": "These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set of English, and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend.", "labels": [], "entities": [{"text": "Wall Street Journal data set of English", "start_pos": 94, "end_pos": 133, "type": "DATASET", "confidence": 0.9618881940841675}, {"text": "summarization)", "start_pos": 294, "end_pos": 308, "type": "TASK", "confidence": 0.8687501847743988}]}, {"text": "While progress on parsing English -the main language of focus for the ACL community -has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains.", "labels": [], "entities": [{"text": "parsing English", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.890740841627121}]}, {"text": "This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the casein English and other configurational languages.", "labels": [], "entities": []}, {"text": "MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and soon ().", "labels": [], "entities": []}, {"text": "The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs.", "labels": [], "entities": [{"text": "parsing MRLs", "start_pos": 105, "end_pos": 117, "type": "TASK", "confidence": 0.9151116907596588}]}, {"text": "In addition, insufficient language resources were shown to also contribute to parsing difficulty (, and references therein).", "labels": [], "entities": [{"text": "parsing", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9821069836616516}]}, {"text": "These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9956821203231812}]}, {"text": "More recently, advances in PCFG-LA parsing () and language-agnostic data-driven dependency parsing ( have made it possible to reach high accuracy with classical feature engineering techniques in addition to, or instead of, language-specific knowledge.", "labels": [], "entities": [{"text": "PCFG-LA parsing", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7564610838890076}, {"text": "language-agnostic data-driven dependency parsing", "start_pos": 50, "end_pos": 98, "type": "TASK", "confidence": 0.5446026101708412}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9940650463104248}]}, {"text": "With these recent advances, the time has come for establishing the state of the art, and assessing strengths and weaknesses of parsers across different MRLs.", "labels": [], "entities": []}, {"text": "This paper reports on the first shared task on statistical parsing of morphologically rich languages (the SPMRL Shared Task), organized in collaboration with the 4th SPMRL meeting and co-located with the conference on Empirical Methods in Natural Language Processing (EMNLP).", "labels": [], "entities": [{"text": "statistical parsing of morphologically rich languages", "start_pos": 47, "end_pos": 100, "type": "TASK", "confidence": 0.8541753788789114}, {"text": "SPMRL Shared Task)", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7683485299348831}, {"text": "Natural Language Processing (EMNLP)", "start_pos": 239, "end_pos": 274, "type": "TASK", "confidence": 0.7149779200553894}]}, {"text": "In defining and executing this shared task, we pursue several goals.", "labels": [], "entities": []}, {"text": "First, we wish to provide standard training and test sets for MRLs in different representation types and parsing scenarios, so that researchers can exploit them for testing existing parsers across different MRLs.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.9607065320014954}]}, {"text": "Second, we wish to standardize the evaluation protocol and metrics on morphologically ambiguous input, an under-studied challenge, which is also present in English when parsing speech data or web-based nonstandard texts.", "labels": [], "entities": []}, {"text": "Finally, we aim to raise the awareness of the community to the challenges of parsing MRLs and to provide a set of strong baseline results for further improvement.", "labels": [], "entities": [{"text": "parsing MRLs", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.9251390397548676}]}, {"text": "The task features data from nine, typologically diverse, languages.", "labels": [], "entities": []}, {"text": "Unlike previous shared tasks on parsing, we include data in both dependency-based and constituency-based formats, and in addition to the full data setup (complete training data), we provide a small setup (a training subset of 5,000 sentences).", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9661034941673279}]}, {"text": "We provide three parsing scenarios: one in which gold segmentation, POS tags, and morphological features are provided, one in which segmentation, POS tags, and features are automatically predicted by an external resource, and one in which we provide a lattice of multiple possible morphological analyses and allow for joint disambiguation of the morphological analysis and syntactic structure.", "labels": [], "entities": []}, {"text": "These scenarios allow us to obtain the performance upper bound of the systems in lab settings using gold input, as well as the expected level of performance in realistic parsing scenarios -where the parser follows a morphological analyzer and is apart of a full-fledged NLP pipeline.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first survey previous work on parsing MRLs ( \u00a72) and provide a detailed description of the present task, parsing scenarios, and evaluation metrics ( \u00a73).", "labels": [], "entities": [{"text": "parsing MRLs", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9608249068260193}, {"text": "parsing", "start_pos": 108, "end_pos": 115, "type": "TASK", "confidence": 0.9613129496574402}]}, {"text": "We then describe the data sets for the nine languages ( \u00a74), present the different systems ( \u00a75), and empirical results ( \u00a76).", "labels": [], "entities": []}, {"text": "Then, we compare the systems along different axes ( \u00a77) in order to analyze their strengths and weaknesses.", "labels": [], "entities": []}, {"text": "Finally, we summarize and conclude with challenges to address in future shared tasks ( \u00a78).", "labels": [], "entities": []}], "datasetContent": [{"text": "The present task serves as the first attempt to standardize the data sets, parsing scenarios, and evaluation metrics for MRL parsing, for the purpose of gaining insights into parsers' performance across languages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.962816059589386}, {"text": "MRL parsing", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.9711097180843353}]}, {"text": "Ours is not the first cross-linguistic task on statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7781539261341095}]}, {"text": "As mentioned earlier, two previous CoNLL shared tasks focused on cross-linguistic dependency parsing and covered thirteen different languages ().", "labels": [], "entities": [{"text": "cross-linguistic dependency parsing", "start_pos": 65, "end_pos": 100, "type": "TASK", "confidence": 0.6617312729358673}]}, {"text": "However, the settings of these tasks, e.g., in terms of data set sizes or parsing scenarios, made it difficult to draw conclusions about strengths and weaknesses of different systems on parsing MRLs.", "labels": [], "entities": [{"text": "parsing", "start_pos": 74, "end_pos": 81, "type": "TASK", "confidence": 0.9661168456077576}, {"text": "parsing MRLs", "start_pos": 186, "end_pos": 198, "type": "TASK", "confidence": 0.9265251159667969}]}, {"text": "A key aspect to consider is the relation between input tokens and tree terminals.", "labels": [], "entities": []}, {"text": "In the standard statistical parsing setup, every input token is assumed to be a terminal node in the syntactic parse tree (after deterministic tokenization of punctuation).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.5657275319099426}]}, {"text": "In MRLs, morphological processes may have conjoined several words into a single token.", "labels": [], "entities": []}, {"text": "Such tokens need to be segmented and their analyses need to be disambiguated in order to identify the nodes in the parse tree.", "labels": [], "entities": []}, {"text": "In previous shared tasks on statistical parsing, morphological information was assumed to be known in advance in order to make the setup comparable to that of parsing English.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6893027126789093}, {"text": "parsing English", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.8946033120155334}]}, {"text": "In realistic scenarios, however, morphological analyses are initially unknown and are potentially highly ambiguous, so external resources are used to predict them.", "labels": [], "entities": []}, {"text": "Incorrect morphological disambiguation sets a strict ceiling on the expected performance of parsers in real-world scenarios.", "labels": [], "entities": []}, {"text": "Results reported for MRLs using gold morphological information are then, at best, optimistic.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.9864835143089294}]}, {"text": "One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario.", "labels": [], "entities": []}, {"text": "Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree.", "labels": [], "entities": []}, {"text": "When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval ( or Attachment Scores () fail to produce a score.", "labels": [], "entities": [{"text": "ParsEval", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.922458827495575}]}, {"text": "In this task, we use TedEval (), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.6896424293518066}, {"text": "joint morpho-syntactic evaluation", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.6302164296309153}]}, {"text": "Finally, the previous tasks focused on dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.9210364818572998}]}, {"text": "When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types.", "labels": [], "entities": []}, {"text": "We are now faced with an additional question: how can we compare parsing results across different frameworks?", "labels": [], "entities": []}, {"text": "Adopting standard metrics will not suffice as we would be comparing apples and oranges.", "labels": [], "entities": []}, {"text": "In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees).", "labels": [], "entities": [{"text": "TedEval", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9093555212020874}]}, {"text": "Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 6, "end_pos": 13, "type": "DATASET", "confidence": 0.9342972040176392}, {"text": "constituency parsing", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.6624150574207306}]}, {"text": "This task features nine languages, two different representation types and three different evaluation scenarios.", "labels": [], "entities": []}, {"text": "In order to evaluate the quality of the predicted structures in the different tracks, we use a combination of evaluation metrics that allow us to compare the systems along different axes.", "labels": [], "entities": []}, {"text": "In this section, we formally define the different evaluation metrics and discuss how they support system comparison.", "labels": [], "entities": []}, {"text": "Throughout this paper, we will be referring to different evaluation dimensions: \u2022 Cross-Parser Evaluation in Gold/Predicted Scenarios.", "labels": [], "entities": []}, {"text": "Here, we evaluate the results of different parsers on a single data set in the Gold or Predicted setting.", "labels": [], "entities": []}, {"text": "We use standard evaluation metrics for the different types of analyses, that is, on phrase-structure trees, and Labeled Attachment Scores (LAS) () for dependency trees.", "labels": [], "entities": [{"text": "Labeled Attachment Scores (LAS)", "start_pos": 112, "end_pos": 143, "type": "METRIC", "confidence": 0.7506768703460693}]}, {"text": "Since ParsEval is known to be sensitive to the size and depth of trees (), we also provide the Leaf-Ancestor metric, which is less sensitive to the depth of the phrase-structure hierarchy.", "labels": [], "entities": []}, {"text": "In both scenarios we also provide metrics to evaluate the prediction of MultiWord Expressions.", "labels": [], "entities": []}, {"text": "\u2022 Cross-Parser Evaluation in Raw Scenarios.", "labels": [], "entities": [{"text": "Cross-Parser Evaluation", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.7749280035495758}]}, {"text": "Here, we evaluate the results of different parsers on a single data set in scenarios where morphological segmentation is not known in advance.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.7354282736778259}]}, {"text": "When a hypothesized segmentation is not identical to the gold segmentation, standard evaluation metrics such as ParsEval and Attachment Scores breakdown.", "labels": [], "entities": [{"text": "ParsEval", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.897912859916687}]}, {"text": "Therefore, we use TedEval (), which jointly assesses the quality of the morphological and syntactic analysis in morphologically-complex scenarios.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.5243862271308899}]}, {"text": "Here, we compare the results obtained by a dependency parser and a constituency parser on the same set of sentences.", "labels": [], "entities": []}, {"text": "In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees.", "labels": [], "entities": []}, {"text": "Here we use TedEval's crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9587216973304749}]}, {"text": "Here, we compare parsers for the same representation type across different languages.", "labels": [], "entities": []}, {"text": "Conducting a complete and faithful evaluation across languages would require a harmonized universal annotation scheme (possibly along the lines of (de) or task based evaluation.", "labels": [], "entities": []}, {"text": "As an approximation we use unlabeled TedEval.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.6030405163764954}]}, {"text": "Since it is unlabeled, it is not sensitive to label set size.", "labels": [], "entities": []}, {"text": "Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011).", "labels": [], "entities": []}, {"text": "The former two dimensions are evaluated on the full sets.", "labels": [], "entities": []}, {"text": "The latter two are evaluated on smaller, comparable, test sets.", "labels": [], "entities": []}, {"text": "For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used.", "labels": [], "entities": []}, {"text": "ParsEval The ParsEval metrics are evaluation metrics for phrase-structure trees.", "labels": [], "entities": []}, {"text": "Despite various shortcomings, they are the de-facto standard for system comparison on phrase-structure parsing, used in many campaigns and shared tasks (e.g.,).", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.7343779802322388}]}, {"text": "Assume that G and H are phrase-structure gold and hypothesized trees respectively, each of which is represented by a set of tuples (i, A, j) where A is a labeled constituent spanning from i to j.", "labels": [], "entities": []}, {"text": "Assume that g is the same as G except that it discards the root, preterminal, and terminal nodes, likewise for hand H.", "labels": [], "entities": []}, {"text": "The ParsEval scores define the accuracy of the hypothesis in terms of the normalized size of the intersection of the constituent sets.", "labels": [], "entities": [{"text": "ParsEval", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9734920263290405}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.999363124370575}]}, {"text": "We evaluate accuracy on phrase-labels ignoring any further decoration, as it is in standard practices.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992308616638184}]}, {"text": "Evalb, the standard software that implements ParsEval, 4 takes a parameter file and ignores the labels specified therein.", "labels": [], "entities": [{"text": "Evalb", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9455344676971436}]}, {"text": "As usual, we ignore root and POS labels.", "labels": [], "entities": []}, {"text": "Contrary to the standard practice, we do take punctuation into account.", "labels": [], "entities": []}, {"text": "Note that, as opposed to the official version, we used the SANCL'2012 version 5 modified to actually penalize non-parsed trees.", "labels": [], "entities": [{"text": "SANCL'2012 version 5 modified", "start_pos": 59, "end_pos": 88, "type": "DATASET", "confidence": 0.8972783237695694}]}, {"text": "Leaf-Ancestor The Leaf-Ancestor metric) measures the similarity between the path from each terminal node to the root node in the output tree and the corresponding path in the gold tree.", "labels": [], "entities": []}, {"text": "The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance.", "labels": [], "entities": []}, {"text": "This distance is normalized bypath length, and the score of the tree is an aggregated score of the values for all terminals in the tree (x t is the leaf-ancestor path oft in tree x).", "labels": [], "entities": []}, {"text": "This metric was shown to be less sensitive to differences between annotation schemes in (, and was shown by to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions.", "labels": [], "entities": []}, {"text": "We used the implementation of Wagner (2012).", "labels": [], "entities": []}, {"text": "Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks ( and have since assumed the role of standard metrics in multiple shared tasks and independent studies.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7893590331077576}]}, {"text": "Assume that g, hare gold and hypothesized dependency trees respectively, each of which is represented by a set of arcs (i, A, j) where A is a labeled arc from terminal i to terminal j.", "labels": [], "entities": []}, {"text": "Recall that in the gold and predicted settings, |g| = |h| (because the number of terminals determines the number of arcs and hence it is fixed).", "labels": [], "entities": []}, {"text": "So Labeled Attachment Score equals precision and recall, and it is calculated as a normalized size of the intersection between the sets of gold and parsed arcs.", "labels": [], "entities": [{"text": "Labeled Attachment Score", "start_pos": 3, "end_pos": 27, "type": "METRIC", "confidence": 0.6992387175559998}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9996483325958252}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9996732473373413}]}, {"text": "7  The TedEval metrics and protocols have been developed by, and for coping with non-trivial evaluation scenarios, e.g., comparing parsing results across different frameworks, across representation theories, and across different morphological segmentation hypotheses.", "labels": [], "entities": []}, {"text": "8 Contrary to the previous metrics, which view accuracy as a normalized intersection over sets, TedEval computes the accuracy of a parse tree based on the tree-edit distance between complete trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.997525155544281}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9983375072479248}]}, {"text": "Assume a finite set of (possibly parameterized) edit operations A = {a 1 ....a n }, and a cost function c : A \u2192 1.", "labels": [], "entities": []}, {"text": "An edit script is the cost of a sequence of edit operations, and the edit distance of g, h is the minimal cost edit script that turns g into h (and vice versa).", "labels": [], "entities": []}, {"text": "The normalized distance subtracted from 1 provides the level of accuracy on the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9992637038230896}]}, {"text": "Formally, the TedEval score on g, h is defined as follows, where ted is the tree-edit distance, and the |x| (size in nodes) discards terminals and root nodes.", "labels": [], "entities": [{"text": "TedEval score", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.6098433434963226}]}, {"text": "In the gold scenario, we are not allowed to manipulate terminal nodes, only non-terminals.", "labels": [], "entities": []}, {"text": "In the raw scenarios, we can add and delete both terminals and non-terminals so as to match both the morphological and syntactic hypotheses.", "labels": [], "entities": []}, {"text": "Multiword-Expression Identification As pointed out in section 3.1, the French data set is provided with tree structures encoding both syntactic information and groupings of terminals into MWEs.", "labels": [], "entities": [{"text": "Multiword-Expression Identification", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.777507096529007}, {"text": "French data set", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9583480755488077}]}, {"text": "A given MWE is defined as a continuous sequence of terminals, plus a POS tag.", "labels": [], "entities": []}, {"text": "In the constituency trees, the POS tag of the MWE is an internal node of the tree, dominating the sequence of pre-terminals, each dominating a terminal.", "labels": [], "entities": []}, {"text": "In the dependency trees, there is no specific node for the MWE as such (the nodes are the terminals).", "labels": [], "entities": []}, {"text": "So, the first token of a MWE is taken as the head of the other tokens of the same MWE, with the same label (see section 4.4).", "labels": [], "entities": []}, {"text": "To evaluate performance on MWEs, we use the following metrics.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.8893546462059021}]}, {"text": "\u2022 R_MWE, P_MWE, and F_MWE are recall, precision, and F-score overfull MWEs, in which a predicted MWE counts as correct if it has the correct span (same group as in the gold data).", "labels": [], "entities": [{"text": "F", "start_pos": 20, "end_pos": 21, "type": "METRIC", "confidence": 0.9929747581481934}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9961556792259216}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9918414354324341}, {"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9949794411659241}]}, {"text": "\u2022 R_MWE +POS, R_MWE +POS, and F_MWE +POS are defined in the same fashion, except that a predicted MWE counts as correct if it has both correct span and correct POS tag.", "labels": [], "entities": [{"text": "F", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.956017792224884}]}, {"text": "\u2022 R_COMP, R_COMP, and F_COMP are recall, precision and F-score over non-head components of MWEs: a non-head component of MWE counts as correct if it is attached to the head of the MWE, with the specific label that indicates that it is part of an MWE.", "labels": [], "entities": [{"text": "R_COMP", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9377636512120565}, {"text": "R_COMP", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9308691819508871}, {"text": "F_COMP", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9430231253306071}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9979998469352722}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9972679615020752}, {"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9982268214225769}]}, {"text": "In the predicted setting, shown in the second block of table 4 for the full training set and in the third block of the same table for the 5k training set, we see that only two systems, IGM:ALPAGE and IMS:SZEGED:CIS can predict the MWE label when it is not present in the training set.", "labels": [], "entities": [{"text": "ALPAGE", "start_pos": 189, "end_pos": 195, "type": "METRIC", "confidence": 0.577459454536438}]}, {"text": "IGM:ALPAGE's approach of using a separate classifier in combination with external dictionaries is very successful, reaching an F_MWE+POS score of 77.37.", "labels": [], "entities": [{"text": "IGM", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5121444463729858}, {"text": "F_MWE+POS score", "start_pos": 127, "end_pos": 142, "type": "METRIC", "confidence": 0.9204632341861725}]}, {"text": "This is compared to the score of 70.48 by IMS:SZEGED:CIS, which predicts this node label as aside effect of their constituent feature enriched dependency model.", "labels": [], "entities": [{"text": "IMS:SZEGED:CIS", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.7158812522888184}]}, {"text": "AI:KU has a zero score for all predicted settings, which results from an erroneous training on the gold data rather than the predicted data.", "labels": [], "entities": []}, {"text": "Section 6 reported evaluation scores across systems for different scenarios.", "labels": [], "entities": []}, {"text": "However, as noted, these results are not comparable across languages, representation types and parsing scenarios due to differences in the data size, label set size, length of sentences and also differences in evaluation metrics.", "labels": [], "entities": []}, {"text": "Our following discussion in the first part of this section highlights the kind of impact that data set properties have on the standard metrics (label set size on LAS, non-terminal nodes per sentence on F-score).", "labels": [], "entities": []}, {"text": "Then, in the second part of this section we use the TedEval cross-experiment protocols for comparative evaluation that is less sensitive to representation types and annotation idiosyncrasies.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.8956651091575623}]}, {"text": "In this section we analyze the results in crossscenario, cross-annotation, and cross-framework settings using the evaluation protocols discussed in.", "labels": [], "entities": []}, {"text": "As a starting point, we select comparable sections of the parsed data, based on system runs trained on the small train set (train5k).", "labels": [], "entities": []}, {"text": "For those, we selected subsets containing the first 5,000 tree terminals (respecting sentence boundaries) of the test set.", "labels": [], "entities": []}, {"text": "We only used TedEval on sentences up to 70 terminals long, and projectivized non-projective sentences in all sets.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.6266586780548096}]}, {"text": "We use the TedEval metrics to calculate scores on both constituency and dependency structures in all languages and all scenarios.", "labels": [], "entities": [{"text": "TedEval metrics", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.8915117681026459}]}, {"text": "Since the metric defines one scale for all of these different cases, we can compare the performance across annotation schemes, assuming that those subsets are representative of their original source.", "labels": [], "entities": []}, {"text": "Ideally, we would be using labeled TedEval scores, as the labeled parsing task is more difficult, and labeled parses are far more informative than unlabeled ones.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.8798577785491943}]}, {"text": "However, most constituency-based parsers do not provide function labels as part of the output, to be compared with the dependency arcs.", "labels": [], "entities": []}, {"text": "Furthermore, as mentioned earlier, we observed a huge difference between label set sizes for the dependency runs.", "labels": [], "entities": []}, {"text": "Consequently, labeled scores will not be as informative across treebanks and representation types.", "labels": [], "entities": []}, {"text": "We will therefore only use labels across scenarios for the same language and representation type.", "labels": [], "entities": []}, {"text": "We choose this sample scheme for replicability.", "labels": [], "entities": [{"text": "replicability", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9801467061042786}]}, {"text": "We first tried sampling sentences, aiming at the same average sentence length, but that seemed to create artificially difficult test sets for languages as Polish and overly simplistic ones for French or Arabic.", "labels": [], "entities": []}, {"text": "One novel aspect of this shared task is the evaluation on non-gold segmentation in addition to gold morphology.", "labels": [], "entities": []}, {"text": "One drawback is that the scenarios are currently not using the same metrics -the metrics generally applied for gold and predicted scenrios cannot apply for raw.", "labels": [], "entities": []}, {"text": "To assess how well state of the art parsers perform in raw scenarios compared to gold scenarios, we present here TedEval results comparing raw and gold systems using the evaluation protocol of. presents the labeled and unlabeled results for Arabic and Hebrew (in Full and 5k training settings), and presents unlabeled TedEval results (for all languages) in the gold settings.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 318, "end_pos": 325, "type": "METRIC", "confidence": 0.7548131942749023}]}, {"text": "The unlabeled TedEval results for the raw settings are substantially lower then TedEval results on the gold settings for both languages.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.7481179237365723}, {"text": "TedEval", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.7626224160194397}]}, {"text": "When comparing the unlabeled TedEval results for Arabic and Hebrew on the participating systems, we see a loss of 3-4 points between (raw) and Table 10 (gold).", "labels": [], "entities": [{"text": "TedEval", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.7115042209625244}]}, {"text": "In particular we see that for the best per-forming systems on Arabic (IMS:SZEGED:CIS for both constituency and dependency), the gap between gold and realistic scenarios is 3.4 and 4.3 points, for the constituency and the dependency parser respectively.", "labels": [], "entities": []}, {"text": "These results are on a par with results by, who showed for different settings, constituency and dependency based, that raw scenarios are considerably more difficult to parse than gold ones on the standard split of the Modern Hebrew treebank.", "labels": [], "entities": [{"text": "Modern Hebrew treebank", "start_pos": 218, "end_pos": 240, "type": "DATASET", "confidence": 0.6597887476285299}]}, {"text": "For Hebrew, the performance gap between unlabeled TedEval in raw) and gold is even more salient, with around 7 and 8 points of difference between the scenarios.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.873547375202179}]}, {"text": "We can only speculate that such a difference maybe due to the difficulty of resolving Hebrew morpho-syntactic ambiguities without sufficient syntactic information.", "labels": [], "entities": []}, {"text": "Since Hebrew and Arabic now have standardized morphologically and syntactically analyzed data sets available through this task, it will be possible to investigate further how cross-linguistic differences in morphological ambiguity affect full-parsing accuracy in raw scenarios.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.9843140840530396}]}, {"text": "This section compared the raw and gold parsing results only on unlabeled TedEval metrics.", "labels": [], "entities": [{"text": "TedEval metrics", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.8998825550079346}]}, {"text": "According to what we have seen so far is expected that for labeled TedEval metrics using the same protocol, the gap between gold and raw scenario will be even greater.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.7995901107788086}]}, {"text": "In this section, our focus is on comparing parsing results across constituency and dependency parsers based on the protocol of We have only one submission from IMS:SZEGED:CIS in the constituency track, and. from the same group, a submission on the dependency track.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9274904131889343}, {"text": "IMS", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.9127353429794312}]}, {"text": "We only compare the IMS:SZEGED:CIS results on constituency and dependency parsing with the two baselines we provided.", "labels": [], "entities": [{"text": "IMS:SZEGED:CIS", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.4478411257266998}, {"text": "constituency and dependency parsing", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.6171446070075035}]}, {"text": "The results of the cross-framework evaluation protocol are shown in.", "labels": [], "entities": []}, {"text": "The results comparing the two variants of the IMS:SZEGED:CIS systems show that they are very close for all languages, with differences ranging from 0.03 for German to 0.8 for Polish in the gold setting.", "labels": [], "entities": []}, {"text": "It has often been argued that dependency parsers perform better than a constituency parser, but we notice that when using across framework protocol, such as TedEval, and assuming that our test set sample is representative, the difference between the interpretation of both representation's performance is alleviated.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7821032702922821}, {"text": "TedEval", "start_pos": 157, "end_pos": 164, "type": "DATASET", "confidence": 0.9767249226570129}]}, {"text": "Of course, here the metric is unlabeled, so it simply tells us that both kind of parsing models are equally able to provide similar tree structures.", "labels": [], "entities": []}, {"text": "Said differently, the gaps in the quality of predicting the same underlying structure across representations for MRLs is not as large as is sometimes assumed.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 113, "end_pos": 117, "type": "TASK", "confidence": 0.845888078212738}]}, {"text": "For most languages, the baseline constituency parser performs better than the dependency baseline one, with Basque and Korean as an exception, and at the same time, the dependency version of IMS:SZEGED:CIS performs slightly better than their constituent parser for most languages, with the exception of Hebrew and Hungarian.", "labels": [], "entities": []}, {"text": "It goes to show that, as far as these present MRL results go, there is no clear preference fora dependency over a constituency parsing representation, just preferences among particular models.", "labels": [], "entities": [{"text": "MRL", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.942395031452179}]}, {"text": "More generally, we can say that even if the linguistic coverage of one theory is shown to be better than another one, it does not necessarily mean that the statistical version of the formal theory will perform better for structure prediction.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 221, "end_pos": 241, "type": "TASK", "confidence": 0.8654016852378845}]}, {"text": "System performance is more tightly related to the efficacy of the learning and search algorithms, and feature engineering on top of the selected formalism.", "labels": [], "entities": []}, {"text": "We conclude with an overall outlook of the TedEval scores across all languages.", "labels": [], "entities": [{"text": "TedEval scores", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.6967588663101196}]}, {"text": "The results on the gold scenario, for the small training set and the 5k test set are presented in.", "labels": [], "entities": []}, {"text": "We concentrate on gold scenarios (to avoid the variation in coverage of external morphological analyzers) and choose unlabeled metrics as they are not sensitive to label set sizes.", "labels": [], "entities": []}, {"text": "We emphasize in bold, for each parsing system (row in the table), the top two languages that most accurately parsed by it (boldface) and the two languages it performed the worse on (italics).", "labels": [], "entities": []}, {"text": "We see that the European languages German and Hungarian are parsed most accurately in the constituency-based setup, with Polish and Swedish having an advantage in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.709963858127594}]}, {"text": "Across all systems, Korean is the hardest to parse, with Ara-: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and a 5k-terminals test set.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.7519452571868896}]}, {"text": "The upper part refers to constituency parsing and the lower part refers to dependency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7656951248645782}, {"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.820551186800003}]}, {"text": "For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.", "labels": [], "entities": []}, {"text": "bic, Hebrew and to some extent French following.", "labels": [], "entities": []}, {"text": "It appears that on a typological scale, Semitic and Asian languages are still harder to parse than a range of European languages in terms of structural difficulty and complex morpho-syntactic interaction.", "labels": [], "entities": []}, {"text": "That said, note that we cannot tell why certain treebanks appear more challenging to parse then others, and it is still unclear whether the difficulty is inherent on the language, in the currently available models, or because of the annotation scheme and treebank consistency.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Overview of participating languages and treebank properties. 'Sents' = number of sentences, 'Tokens' =  number of raw surface forms. 'Lex. size' and 'Avg. Length' are computed in terms of tagged terminals. 'NT' = non- terminals in constituency treebanks, 'Dep Labels' = dependency labels on the arcs of dependency treebanks. -A more  comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.", "labels": [], "entities": [{"text": "Avg. Length'", "start_pos": 160, "end_pos": 172, "type": "METRIC", "confidence": 0.8712810277938843}]}, {"text": " Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold  show the best results per language and setting.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7840549051761627}, {"text": "LAS", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9736683964729309}]}, {"text": " Table 4: Dependency Parsing: MWE results", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8475841879844666}, {"text": "MWE", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.5150929093360901}]}, {"text": " Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in  bold show the best results per language and setting.", "labels": [], "entities": [{"text": "ParsEval F-scores", "start_pos": 31, "end_pos": 48, "type": "METRIC", "confidence": 0.7967669069766998}]}, {"text": " Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.", "labels": [], "entities": []}, {"text": " Table 3. Across the board, the re- sults are considerably lower than the gold sce-", "labels": [], "entities": [{"text": "re- sults", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9255308707555135}]}, {"text": " Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.  The upper part refers to constituency results, the lower part refers to dependency results", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.7228055596351624}]}, {"text": " Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.  Top upper part refers to constituency results, the lower part refers to dependency results.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.8036567568778992}]}, {"text": " Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.  The upper part refers to constituency parsing and the lower part refers to dependency parsing.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.6614087820053101}, {"text": "constituency parsing", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.7306444048881531}, {"text": "dependency parsing", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.8047834634780884}]}, {"text": " Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and  a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.  For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.8164176344871521}, {"text": "constituency parsing", "start_pos": 169, "end_pos": 189, "type": "TASK", "confidence": 0.7595219612121582}, {"text": "dependency parsing", "start_pos": 219, "end_pos": 237, "type": "TASK", "confidence": 0.8340257406234741}]}, {"text": " Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k  sentences and tested on 5k terminals.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.5579792857170105}]}]}