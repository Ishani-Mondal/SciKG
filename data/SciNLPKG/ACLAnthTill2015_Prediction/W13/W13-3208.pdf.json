{"title": [{"text": "Determining Compositionality of Word Expressions Using Various Word Space Models and Measures", "labels": [], "entities": [{"text": "Determining Compositionality of Word Expressions", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8844854950904846}]}], "abstractContent": [{"text": "This paper presents a comparative study of 5 different types of Word Space Models (WSMs) combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of word expressions.", "labels": [], "entities": []}, {"text": "Many combinations of WSMs and measures have never been applied to the task before.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.8883149027824402}]}, {"text": "The study follows Biemann and Gies-brecht (2011) who attempted to find a list of expressions for which the composition-ality assumption-the meaning of an expression is determined by the meaning of its constituents and their combination-does not hold.", "labels": [], "entities": []}, {"text": "Our results are very promising and can be appreciated by those interested in WSMs, compositionality, and/or relevant evaluation methods.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 77, "end_pos": 81, "type": "TASK", "confidence": 0.9272873997688293}]}], "introductionContent": [{"text": "Our understanding of WSM is in agreement with: \"The word space model is a computational model of word meaning that utilizes the distributional patterns of words collected overlarge text data to represent semantic similarity between words in terms of spatial proximity\".", "labels": [], "entities": [{"text": "WSM", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.89766925573349}]}, {"text": "There are many types of WSMs built by different algorithms.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.9732379913330078}]}, {"text": "WSMs are based on the Harris distributional hypothesis, which assumes that words are similar to the extent to which they share similar linguistic contexts.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.924271821975708}]}, {"text": "WSM can be viewed as a set of words associated with vectors representing contexts in which the words occur.", "labels": [], "entities": []}, {"text": "Then, similar vectors imply (semantic) similarity of the words and vice versa.", "labels": [], "entities": []}, {"text": "Consequently, WSMs provide a means to find words semantically similar to a given word.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.9304439425468445}]}, {"text": "This capability of WSMs is exploited by many Natural Language Processing (NLP) applications as listed e.g. by.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.9390778541564941}]}, {"text": "This study follows, who attempted to find a list of noncompositional expressions whose meaning is not fully determined by the meaning of its constituents and their combination.", "labels": [], "entities": []}, {"text": "The task turned out to be frustratingly hard).", "labels": [], "entities": []}, {"text": "Biemann's idea and motivation is that noncompositional expressions could be treated as single units in many NLP applications such as Information) or Machine Translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8365457057952881}]}, {"text": "We extend this motivation by stating that WSMs could also benefit from a set of non-compositional expressions.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.9402639269828796}]}, {"text": "Specifically, WSMs could treat semantically non-compositional expressions as single units.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.9561287760734558}]}, {"text": "As an example, consider \"kick the bucket\", \"hot dog\", or \"zebra crossing\".", "labels": [], "entities": [{"text": "zebra crossing", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7567024230957031}]}, {"text": "Treating such expressions as single units might improve the quality of WSMs since the neighboring words of these expressions should not be related to their constituents (\"kick\", \"bucket\", \"dog\" or \"zebra\"), but instead to the whole expressions.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 71, "end_pos": 75, "type": "TASK", "confidence": 0.9219387173652649}]}, {"text": "Recent works, including that of Lin (1999),,,,,, and, show the applicability of WSMs in determining the compositionality of word expressions.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 80, "end_pos": 84, "type": "TASK", "confidence": 0.8828849792480469}]}, {"text": "The proposed methods exploit various types of WSMs combined with various measures for determining the compositionality applied to various datasets.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.8533280491828918}]}, {"text": "First, this leads to non-directly comparable results and second, many combinations of WSMs and measures have never before been applied to the task.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 86, "end_pos": 90, "type": "TASK", "confidence": 0.7876587510108948}]}, {"text": "The main contribution and novelty of our study lies in systematic research of several basic and also advanced WSMs combined with all the so far, to the best of our knowledge, proposed WSM-based measures for determining the semantic compositionality.", "labels": [], "entities": []}, {"text": "The explored WSMs, described in more detail in Section 2, include the Vector Space Model, Latent Semantic Analysis, Hyperspace Analogue to Language, Correlated Occurrence Analogue to Lexical Semantics, and Random Indexing.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.92574143409729}, {"text": "Latent Semantic Analysis", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.5521721045176188}]}, {"text": "The measures, including substitutability, endocentricity, compositionality, and neighbors-in-commonbased, are described in detail in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes our experiments performed on the manually annotated datasets -Distributional Semantics and Compositionality dataset (DISCO) and the dataset built by.", "labels": [], "entities": []}, {"text": "Section 5 summarizes the results and Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the ability of various combinations of WSMs and Measures to rank expressions as the human annotators had done ahead of time.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.8050586581230164}]}, {"text": "Datasets We experimented with the DISCO () and Reddy () human annotated datasets, built for the task of automatic determining of semantic compositionality.", "labels": [], "entities": [{"text": "DISCO () and Reddy () human annotated datasets", "start_pos": 34, "end_pos": 80, "type": "DATASET", "confidence": 0.6980556435883045}]}, {"text": "The DISCO and Reddy datasets consist of manually scored expressions of adjective-noun (AN), verb-object (VO), and subject-verb (SV) types and the noun-noun WSM construction Since the DISCO and Reddy data were extracted from the ukWaC corpus (), we also build our WSMs from the same corpus.", "labels": [], "entities": [{"text": "Reddy datasets", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.8153536319732666}, {"text": "Reddy data", "start_pos": 193, "end_pos": 203, "type": "DATASET", "confidence": 0.7235476076602936}, {"text": "ukWaC corpus", "start_pos": 228, "end_pos": 240, "type": "DATASET", "confidence": 0.9885500967502594}]}, {"text": "We use our own modification of the S-Space package (.", "labels": [], "entities": []}, {"text": "The modification lies in treating multiword expressions and handling stopwords.", "labels": [], "entities": []}, {"text": "Specifically, we extended the package with the capability of building WSM vectors for the examined expressions in such away that the WSM vectors previously built for words are preserved.", "labels": [], "entities": []}, {"text": "This differentiates our approach e.g. from, who label the expressions in the corpus ahead of time and treat them as single words.", "labels": [], "entities": []}, {"text": "As for treating stopwords, we map trigrams containing determiners as the middle word into bigrams without the determiners.", "labels": [], "entities": []}, {"text": "The intuition is to extract better co-occurrence statistics for VO expressions often containing an intervening determiner.", "labels": [], "entities": []}, {"text": "As an example, compare the occurrences of \"reinvent (de-terminer) wheel\" and \"reinvent wheel\" in ukWaC being 623 and 27, respectively.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9795140027999878}]}, {"text": "We experimented with lemmas (noT) or with lemmas concatenated with their part of speech (POS) tags (yesT).", "labels": [], "entities": []}, {"text": "We labeled the following strings in ukWaC as stopwords: low-frequency words (lemmas with frequency < 50), strings containing two adjacent non-letter characters (thus omitting sequences of various symbols), and closed-class words.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9761541485786438}]}, {"text": "For our experiments, we built WSMs using various parameters examined in previous works (see Section 2) and parameters which are implied from our own experience with WSMs.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9224358797073364}]}, {"text": "summarizes all the parameters we used for building WSMs.", "labels": [], "entities": [{"text": "WSMs", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.8293155431747437}]}, {"text": "Measure settings We examined various Measure settings (see Section 3), summarized in Table 2.", "labels": [], "entities": []}, {"text": "For all the vector comparisons, we used the cos similarity.", "labels": [], "entities": [{"text": "cos similarity", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.8410211503505707}]}, {"text": "Only for HAL we also examined euc and for COALS cor, since these are the recommended similarity functions for these particular WSMs ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Numbers of expressions of all the differ- ent types from the DISCO and Reddy datasets.", "labels": [], "entities": [{"text": "DISCO and Reddy datasets", "start_pos": 71, "end_pos": 95, "type": "DATASET", "confidence": 0.7717647403478622}]}, {"text": " Table 3: The Spearman correlations \u03c1 of the best performing (wAvg) combinations of particular WSMs  and Measures from all the tested Setups applied to TrValD. The highest correlation values in the particular  columns and the correlation values which are not statistically different from them (p < 0.05) are in bold  (yet we do not know how to calculate the stat. significance for the wAvg(of \u03c1) column). The parameters  of WSMs and Measures corresponding to the indexes are depicted in Tables 5 and 6, respectively.", "labels": [], "entities": [{"text": "Spearman correlations \u03c1", "start_pos": 14, "end_pos": 37, "type": "METRIC", "confidence": 0.6696311036745707}, {"text": "TrValD", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.8917363882064819}]}, {"text": " Table 5: Parameters of WSMs (Section 2) which,  combined with particular Measures, achieved the  highest average correlation in TrValD.", "labels": [], "entities": [{"text": "Measures", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.77162766456604}, {"text": "correlation", "start_pos": 114, "end_pos": 125, "type": "METRIC", "confidence": 0.8516843318939209}, {"text": "TrValD", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9914910197257996}]}]}