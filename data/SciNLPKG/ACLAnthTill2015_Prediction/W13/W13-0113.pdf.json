{"title": [{"text": "Domain Adaptable Semantic Clustering in Statistical NLG", "labels": [], "entities": [{"text": "Statistical NLG", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.5733812600374222}]}], "abstractContent": [{"text": "We present a hybrid natural language generation system that utilizes Discourse Representation Structures (DRSs) for statistically learning syntactic templates from a given domain of discourse in sentence \"micro\" planning.", "labels": [], "entities": []}, {"text": "In particular, given a training corpus of target texts, we extract semantic predicates and domain general tags from each sentence and then organize the sentences using supervised clustering to represent the \"conceptual meaning\" of the corpus.", "labels": [], "entities": []}, {"text": "The sentences, additionally tagged with domain specific information (determined separately), are reduced to templates.", "labels": [], "entities": []}, {"text": "We use a SVM ranking model trained on a subset of the corpus to determine the optimal template during generation.", "labels": [], "entities": []}, {"text": "The combination of the conceptual unit, a set of ranked syntactic templates, and a given set of information, constrains output selection and yields acceptable texts.", "labels": [], "entities": []}, {"text": "Our system is evaluated with automatic, non-expert crowdsourced and expert evaluation metrics and, for generated weather, financial and biography texts, falls within acceptable ranges.", "labels": [], "entities": []}, {"text": "Consequently, we argue that our DRS driven statistical and template-based method is robust and domain adaptable as, while content will be dictated by a target domain of discourse, significant investments in sentence planning can be minimized without sacrificing performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we propose a sentence (or \"micro\") planning system that can quickly adapt to new domains provided a corpus of sentences from the target domain is supplied.", "labels": [], "entities": []}, {"text": "First, all sentences from the corpus are parsed and a semantic representation is generated.", "labels": [], "entities": []}, {"text": "We used predicate and domain general named entities from Discourse Representation Structures (DRSs) derived by Boxer, a robust analysis tool that creates DRSs from text ().", "labels": [], "entities": []}, {"text": "Second, the sentences are automatically clustered by their conceptual meaning with a k-means clustering algorithm and then manually reviewed for consistency and purity.", "labels": [], "entities": [{"text": "consistency", "start_pos": 145, "end_pos": 156, "type": "METRIC", "confidence": 0.9903050661087036}, {"text": "purity", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9700302481651306}]}, {"text": "Third, named entity and domain specific content tagging creates banks of templates (syntactic representations) associated with the respective cluster (a \"conceptual unit\").", "labels": [], "entities": [{"text": "domain specific content tagging", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6658793389797211}]}, {"text": "Finally, a ranking algorithm is used to train a ranker that determines the optimal template at a given point in the generated discourse given various features based on the conceptual units and the text derived so far.", "labels": [], "entities": []}, {"text": "Our system generates sentences from templates given a semantic representation as part of a larger Natural Language Generation (\"NLG\") system for three domains: financial, biography and weather (from the SUMTIME-METEO corpus ()).", "labels": [], "entities": [{"text": "SUMTIME-METEO corpus", "start_pos": 203, "end_pos": 223, "type": "DATASET", "confidence": 0.698911502957344}]}, {"text": "NLG is traditionally seen as a multistage process whereby decisions are made on the type of text to be generated (communicative goal); entities, events and relationships that express the content of that text; and forging grammatical constructions with the content into a \"natural\" sounding text.", "labels": [], "entities": []}, {"text": "These stages are articulated in a variety of architectures -for example, Bateman and Zock summarize NLG as follows: (1) Macro Planning creating a document plan; (2) Micro Planning sentence planning; (3) Surface Realization concatenating the information from (1-2) into coherent and grammatical text; and (4) Physical Presentation document layout considerations (formatting, titles, etc.)", "labels": [], "entities": [{"text": "Micro Planning sentence planning", "start_pos": 165, "end_pos": 197, "type": "TASK", "confidence": 0.6300966814160347}, {"text": "Surface Realization concatenating the information from (1-2) into coherent and grammatical text", "start_pos": 203, "end_pos": 298, "type": "TASK", "confidence": 0.8192383519240788}]}, {"text": "Each one of these stages can have several subtasks and vary considerably in terms of complexity (see generally,;;).", "labels": [], "entities": []}, {"text": "However, in general, some abstract representation is developed in (1-2) and (3-4) deal with translating the abstraction to natural language largely through either rule-based or statistical approaches.", "labels": [], "entities": []}, {"text": "Significant human investments often need to be made to create systems from scratch.", "labels": [], "entities": []}, {"text": "But while these systems may perform very well fora specific domain, extending to alternative domains may require starting over.", "labels": [], "entities": []}, {"text": "Statistical approaches can streamline some human investment, but domain adaptability remains a concern.", "labels": [], "entities": [{"text": "domain adaptability", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7340550422668457}]}, {"text": "Finding the appropriate balance between investing in input and achieving an appropriate level of evaluated acceptance of the output, let alone whether or not the approach is adaptable, can be problematic.", "labels": [], "entities": []}, {"text": "More abstracted representations may require more rules to process and generate acceptable texts while less abstract representations may require less rules but more investment inhuman resources.", "labels": [], "entities": []}, {"text": "When evaluated, we find that our system produces texts that fall within acceptable ranges for automatic metrics (BLEU and METEOR), non-expert crowdsourced evaluations via CrowdFlower and expert evaluations of the biography domain (based on similar evaluation comparisons for other NLG systems).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.993413507938385}, {"text": "METEOR", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9777116775512695}]}, {"text": "Basile and Bos suggest that DRSs provide an appropriate form of abstraction for NLG tasks (Basile and Bos (2011)).", "labels": [], "entities": []}, {"text": "The reason being that DRSs provide deep semantic content in the form of named entities, relationships between entities, identity relations and logical implications (e.g. negation, scope) all of which have a straightforward mapping to syntactic parses (e.g., within Combinatorial Categorial Grammar) and, in sum, provide a useable architecture to perform a myriad of NLG tasks.", "labels": [], "entities": []}, {"text": "We adopt Discourse Representation Theory () as a starting point for our experiments for domain adaptable NLG.", "labels": [], "entities": [{"text": "Discourse Representation Theory", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.7788568536440531}]}, {"text": "And while we only use a few features of the DRS in the current work, we anticipate that the logical representations in DRT can be useful for future work, as in improving the clustering of conceptual units in the training corpus, for example.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: \u2022 A hybrid approach to sentence planning that combines a statistical system with a template-based system where templates are generated semi-automatically with minimal human review \u2022 Domain adaptability is shown in three different domains (financial, biography and weather).", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.8755717873573303}]}, {"text": "\u2022 Non-expert human evaluation is carried out by means of crowdsourcing.", "labels": [], "entities": []}, {"text": "The evaluation provides scores for overall fluency of the generated text as well as sentence-level preferences between generated and original texts.", "labels": [], "entities": []}, {"text": "These evaluations are supplemented by expert evaluations for the biography domain.", "labels": [], "entities": []}, {"text": "This article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes existing rule-based and statistical NLG approaches and domain adaptability.", "labels": [], "entities": []}, {"text": "Section 3 explains our methodology; including DRSs and their use in clustering the three corpora and how the generated clusters are ranked and deployed in the generation of texts.", "labels": [], "entities": []}, {"text": "Section 4 presents sample generated texts and the results of automatic and crowdsourced evaluations.", "labels": [], "entities": []}, {"text": "Section 5 concludes with limitations and avenues of future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our NLG system with automatic and human metrics and the correlations between them.", "labels": [], "entities": []}, {"text": "The human evaluations can (and, in some circumstances, must be) performed by both non-experts and experts.", "labels": [], "entities": []}, {"text": "We provide non-expert crowdsourced evaluations to determine grammatical, informative and semantic appropriateness and the same evaluations by several experts in biography generation.", "labels": [], "entities": [{"text": "biography generation", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.7689807415008545}]}, {"text": "The automatic metrics used here are BLEU-4 () and METEOR (v.1.3) (Denkowski and Lavie (2011)) and originate from machine translation research.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9969769716262817}, {"text": "METEOR", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9815048575401306}, {"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7773612439632416}]}, {"text": "BLEU-4 measures the degree of 4-gram overlap between documents.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9817186594009399}]}, {"text": "METEOR uses a unigram weighted f-score less a penalty based on chunking dissimilarity.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.715571403503418}]}, {"text": "We also calculated an error rate as an exact match between strings of a document.", "labels": [], "entities": [{"text": "error rate", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9698788225650787}]}, {"text": "provides the automatic evaluations of financial, biography and weather domains for both random and system for all of the testing documents in each domain (financial (367); weather (209); biography (350)).", "labels": [], "entities": []}, {"text": "For each domain, the general trend is that random exhibits a higher error rate and lower BLEU-4 and METEOR scores as compared to system.", "labels": [], "entities": [{"text": "error rate", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9725529253482819}, {"text": "BLEU-4", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9995865225791931}, {"text": "METEOR", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9946367144584656}]}, {"text": "This suggests that the system is more informative than the random text.", "labels": [], "entities": []}, {"text": "However, scores for the financial domain exhibit a smaller difference compared to weather and biography.", "labels": [], "entities": []}, {"text": "Further, the BLEU-4 and METEOR scores are very similar.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9992209672927856}, {"text": "METEOR", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.984982430934906}]}, {"text": "This is arguably related to the fact that the average number of templates is significantly lower for the financial disourses than the weather and biography domains.", "labels": [], "entities": []}, {"text": "That is to say, there is a greater chance of the random system selecting the same template as system.", "labels": [], "entities": []}, {"text": "So, from an automatic metric standpoint, applying model weights increases \"performance\" of the generation (based on coarse content overlap).", "labels": [], "entities": []}, {"text": "However, human evaluations of the texts are necessary to confirm and augment what the automatic metrics indicate.", "labels": [], "entities": []}, {"text": "Two sets of crowdsourced human evaluation tasks (run on CrowdFlower) were constructed to compare against automatic metrics: (1) an understandability evaluation of the entire text on a three-point scale: Fluent = no grammatical or informative barriers; Understandable = some grammatical or informative barriers; Disfluent = significant grammatical or informative barriers; and (2) a sentence-level preference between sentence pairs (e.g., \"Do you prefer Sentence A (from original) or the corresponding Sentence B (from random/system)\").", "labels": [], "entities": [{"text": "Fluent", "start_pos": 203, "end_pos": 209, "type": "METRIC", "confidence": 0.9807953238487244}]}, {"text": "100 different texts and sentence pairs for system, random and the original texts from each domain were selected at random.", "labels": [], "entities": []}, {"text": "In all cases, the original texts in each domain demonstrate the highest comparative fluency and the lowest comparative disfluency.", "labels": [], "entities": []}, {"text": "Further, the system texts demonstrate the highest fluency and the lowest disfluency compared to the random texts.", "labels": [], "entities": []}, {"text": "However, the difference between the system and random for the financial and weather domains are fairly close whereas the differences for the biography domain is much greater.", "labels": [], "entities": []}, {"text": "This makes sense as the biography domain is human generated and exhibits a high amount of variability.", "labels": [], "entities": []}, {"text": "Given that the weather domain is also human generated and exhibits more variability compared to the financial domain, but they read more like the financial domain because of their narrow geographic and subject matter vernacular.", "labels": [], "entities": []}, {"text": "Similar trends are demonstrated in the sentence preferences.", "labels": [], "entities": []}, {"text": "In all cases, the original and system sentences are preferred to random.", "labels": [], "entities": []}, {"text": "The original sentences are also preferred to system sentences, but the difference is very close for the financial and weather domains.", "labels": [], "entities": []}, {"text": "This indicates that, at the sentence level, our system is performing similar to the original texts.", "labels": [], "entities": []}, {"text": "As indicated in, Pearson Correlation, based on 300 documents (100 from each domain), between the automatic metrics are high with the appropriate direction (e.g., error rate correlates negatively with BLEU-4 and METEOR scores, which correlate positively with each other).", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 17, "end_pos": 36, "type": "DATASET", "confidence": 0.5565215051174164}, {"text": "error rate", "start_pos": 162, "end_pos": 172, "type": "METRIC", "confidence": 0.9773681163787842}, {"text": "BLEU-4", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.998155415058136}, {"text": "METEOR", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.9501118659973145}]}, {"text": "The human ratings -a consolidated score (Fluent = 1, Understandable = .66, Disfluent = .33) averaged over four raters per document -behave similar to the BLEU-4 and METEOR automatic metrics, but much less stong.", "labels": [], "entities": [{"text": "Fluent", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9857864379882812}, {"text": "Disfluent", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9817642569541931}, {"text": "BLEU-4", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9772393703460693}, {"text": "METEOR", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.8002375960350037}]}, {"text": "There is more variability captured in the human judgments as compared to the automatic metrics which are both stricter and more consistent.", "labels": [], "entities": []}, {"text": "Extreme cases aside, there is no exact formula for translating automatic and human evaluations to a true estimation for how the generated texts are performing.", "labels": [], "entities": []}, {"text": "It is a relative determination at best and, in all actuality, deference is paid to the human evaluations.", "labels": [], "entities": []}, {"text": "Human understandability of the texts is key.", "labels": [], "entities": []}, {"text": "We were able to perform expert evaluation of the biography domain.", "labels": [], "entities": []}, {"text": "Three experts journalists, who write short biographies for news archives, performed the same two non-expert crowdsourced tasks.", "labels": [], "entities": []}, {"text": "For the text evaluation, the experts rated both the original and system texts to be 100% Fluent (with the random texts following a similar distribution of non-expert ratings).", "labels": [], "entities": [{"text": "Fluent", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9976400136947632}]}, {"text": "For the sentence evaluations, the experts still preferred the original to the system sentences, but with an increase in preference for the system as compared to the non-experts -27% preference by non-experts versus a 35% preference by experts.", "labels": [], "entities": []}, {"text": "This trend is a reverse of what is reported for weather texts.", "labels": [], "entities": []}, {"text": "For example, Belz and Reiter report a reduction in acceptability with experts as compared to non-experts ().", "labels": [], "entities": []}, {"text": "This makes sense as the expert should be more discriminant based on experience.", "labels": [], "entities": []}, {"text": "For the present texts, it could be the case that our system is capturing nuances of biography writing that experts are sensitive to.", "labels": [], "entities": []}, {"text": "However, more critical expert feedback is required before saying more.", "labels": [], "entities": []}, {"text": "The performances that we present here are comparable to other rule-based and statistical systems.", "labels": [], "entities": []}, {"text": "However, comparing systems can be problematic given the different goals and architectures.", "labels": [], "entities": []}, {"text": "Nonetheless, the evaluations and generated texts indicate that we have been able to appropriately capture interesting and varied semantic structures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data and Semantic Cluster Distribution.  Financial  Biography  Weather  Texts  1067  1150  1045  Conceptual Units  38  19  9  Templates  1379  2836  2749  Average Template/CU (Range) 36 (6-230) 236 (7-666) 305 (6-800)", "labels": [], "entities": [{"text": "Semantic Cluster Distribution", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.7265850305557251}, {"text": "Financial  Biography  Weather  Texts  1067  1150  1045  Conceptual Units  38", "start_pos": 51, "end_pos": 127, "type": "DATASET", "confidence": 0.9247559905052185}, {"text": "Average Template/CU", "start_pos": 165, "end_pos": 184, "type": "METRIC", "confidence": 0.8114179074764252}]}, {"text": " Table 3: Automatic Metric Evaluations of Biography, Financial and Weather Domains.  Metric  Bio Rand Bio Sys Fin Rand Fin Sys Weather Rand Weather Sys  Error Rate  0.815  0.350  0.571  0.477  0.996  0.698", "labels": [], "entities": [{"text": "Fin Rand Fin Sys Weather Rand Weather Sys  Error Rate  0.815  0.350  0.571  0.477  0.996  0.698", "start_pos": 110, "end_pos": 205, "type": "DATASET", "confidence": 0.7537049651145935}]}, {"text": " Table 4: Human-Automatic Pearson Correlation ( p\u2264.0001)).  Error Rate BLEU-4 METEOR Human  Error Rate  1  -.719  -.715  -.406", "labels": [], "entities": [{"text": "Error Rate BLEU-4 METEOR Human  Error Rate  1  -.719  -.715  -.406", "start_pos": 60, "end_pos": 126, "type": "METRIC", "confidence": 0.8129414107118335}]}]}