{"title": [{"text": "Why Letter Substitution Puzzles are Not Hard to Solve: A Case Study in Entropy and Probabilistic Search-Complexity", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we investigate the theoretical causes of the disparity between the theoretical and practical running times for the A * algorithm proposed in Corlett and Penn (2010) for deciphering letter-substitution ciphers.", "labels": [], "entities": []}, {"text": "We argue that the difference seen is due to the relatively low entropies of the probability distributions of character transitions seen in natural language, and we develop a principled way of incorporating entropy into our complexity analysis.", "labels": [], "entities": []}, {"text": "Specifically, we find that the low entropy of natural languages can allow us, with high probability, to bound the depth of the heuristic values expanded in the search.", "labels": [], "entities": []}, {"text": "This leads to a novel probabilistic bound on search depth in these tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "When working in NLP, we can find ourselves using algorithms whose worst-case running time bounds do not accurately describe their empirically determined running times.", "labels": [], "entities": []}, {"text": "Specifically, we can often find that the algorithms that we are using can be made to run efficiently on real-world instances of their problems despite having theoretically high running times.", "labels": [], "entities": []}, {"text": "Thus, we have an apparent disparity between the theoretical and practical running times of these algorithms, and so we must ask why these algorithms can provide results in a reasonable time frame.", "labels": [], "entities": []}, {"text": "We must also ask to what extent we can expect our algorithms to remain practical as we change the downstream domains from which we draw problem instances.", "labels": [], "entities": []}, {"text": "At a high level, the reason such algorithms can work well in the real world is that the real world applications from which we draw our inputs do not tend to include the high complexity inputs.", "labels": [], "entities": []}, {"text": "In other words, our problem space either does not coverall possible inputs to the algorithm, or it does, but with a probability distribution that gives a vanishingly small likelihood to the \"hard\" inputs.", "labels": [], "entities": []}, {"text": "Thus, it would be beneficial to incorporate into our running time analysis the fact that our possible inputs are restricted, even if only restricted in relative frequency rather than in absolute terms.", "labels": [], "entities": []}, {"text": "This means that any running time that we observe must be considered to be dependent on the distribution of inputs that we expect to sample from.", "labels": [], "entities": []}, {"text": "It probably does not come as a surprise that any empirical analysis of running time carries with it the assumption that the data on which the tests were run are typical of the data which we expect to see in practice.", "labels": [], "entities": []}, {"text": "Yet the received wisdom on the asymptotic complexity of algorithms in computational linguistics (generally what one might see in an advanced undergraduate algorithms curriculum) has been content to consider input only in terms of its size or length, and not the distribution from which it was sampled.", "labels": [], "entities": []}, {"text": "Indeed, many algorithms in NLP actually take entire distributions as input, such as language models.", "labels": [], "entities": []}, {"text": "Without a more mature theoretical understanding of time complexity, it is not clear exactly what any empirical running time results would mean.", "labels": [], "entities": []}, {"text": "A worst-case complexity result gives a guarantee that an algorithm will take no more than a certain number of steps to complete.", "labels": [], "entities": []}, {"text": "An average-case result gives the expected number of steps to complete.", "labels": [], "entities": []}, {"text": "But an empirical running time found by sampling from a distribution that is potentially different from what the algorithm was designed for is only a lesson in how truly different the distribution is.", "labels": [], "entities": []}, {"text": "It is also common for the theoretical study of asymptotic time complexity in NLP to focus on the worst-case complexity of a problem or algorithm rather than an expected complexity, in spite of the existence for now over 20 years of methods for average-case analysis of an algorithm.", "labels": [], "entities": []}, {"text": "Even these, however, often assume a uniform distribu-tion over input, when in fact the true expectation must consider the probability distribution that we will draw the inputs from.", "labels": [], "entities": []}, {"text": "Uniform distributions are only common because we may not know what the distribution is beforehand.", "labels": [], "entities": []}, {"text": "Ideally, we should want to characterize the running time of an algorithm using some known properties of its input distribution, even if the precise distribution is not known.", "labels": [], "entities": []}, {"text": "Previous work that attempts this does exist.", "labels": [], "entities": []}, {"text": "In particular, there is a variant of analysis referred to as smoothed analysis which gives abound on the average-case running time of an algorithm under the assumption that all inputs are sampled with Gaussian measurement error.", "labels": [], "entities": []}, {"text": "As we will argue in Section 2, however, this approach is of limited use to us.", "labels": [], "entities": []}, {"text": "We instead approach the disparity of theoretical and practical running time by making use of statistics such as entropy, which are taken from the input probability distributions, as eligible factors in our analysis of the running time complexity.", "labels": [], "entities": []}, {"text": "This is a reasonable approach to the problem, in view of the numerous entropic studies of word and character distributions dating back to Shannon.", "labels": [], "entities": []}, {"text": "Specifically, we analyze the running time of the A * search algorithm described in.", "labels": [], "entities": []}, {"text": "This algorithm deciphers text that has been enciphered using a consistent letter substitution, and its running time is linear in the length of the text being deciphered, but theoretically exponential in the size of the input and output alphabets.", "labels": [], "entities": []}, {"text": "This na\u00a8\u0131vena\u00a8\u0131ve theoretical analysis assumes that characters are uniformly distributed, however.", "labels": [], "entities": []}, {"text": "A far more informative bound is attainable by making reference to the entropy of the input.", "labels": [], "entities": []}, {"text": "Because the algorithm takes a language model as one of its inputs (the algorithm is guaranteed to find the model-optimal letter substitution over a given text), there are actually two input distributions: the distribution assumed by the input language model, and the distribution from which the text to be deciphered was sampled.", "labels": [], "entities": []}, {"text": "Another way to view this problem is as a search fora permutation of letters as the outcomes of one distribution such that the two distributions are maximally similar.", "labels": [], "entities": []}, {"text": "So our informative bound is attained through reference to the cross-entropy of these two distributions.", "labels": [], "entities": []}, {"text": "We first formalize our innate assumption that these two distributions are similar, and build an upper bound for the algorithm's complexity that incorporates the cross-entropy between the two distributions.", "labels": [], "entities": []}, {"text": "The analysis concludes that, rather than being exponential in the length of the input or in the size of the alphabets, it is merely exponential in the cross-entropy of these two distributions, thus exposing the importance of their similarity.", "labels": [], "entities": []}, {"text": "Essentially, our bound acts as a probability distribution over the necessary search depth.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}