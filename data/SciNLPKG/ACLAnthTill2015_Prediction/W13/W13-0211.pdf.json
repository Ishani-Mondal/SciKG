{"title": [{"text": "Predicate-specific Annotations for Implicit Role Binding: Corpus Annotation, Data Analysis and Evaluation Experiments", "labels": [], "entities": [{"text": "Implicit Role Binding", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.6322305798530579}, {"text": "Data Analysis", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.6618532836437225}]}], "abstractContent": [{"text": "Current research on linking implicit roles in discourse is severely hampered by the lack of sufficient training resources, especially in the verbal domain: learning algorithms require higher-volume annotations for specific predicates in order to derive valid generalizations, and a larger volume of annotations is crucial for insightful evaluation and comparison of alternative models for role linking.", "labels": [], "entities": [{"text": "role linking", "start_pos": 389, "end_pos": 401, "type": "TASK", "confidence": 0.7341989576816559}]}, {"text": "We present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.929959237575531}, {"text": "VerbNet", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.9172988533973694}]}, {"text": "A qualitative data analysis leads to observations regarding implicit role realization that can guide further annotation efforts.", "labels": [], "entities": [{"text": "role realization", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7471555173397064}]}, {"text": "Experiments using role linking annotations for five predicates demonstrate high performance for these target predicates.", "labels": [], "entities": []}, {"text": "Using our additional data in the SemEval task, we obtain overall performance gains of 2-4 points F 1-score.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9050594568252563}]}], "introductionContent": [{"text": "Automatic annotation of semantic predicate-argument structure (PAS) is an important subtask to be solved for high-quality information access and natural language understanding.", "labels": [], "entities": [{"text": "Automatic annotation of semantic predicate-argument structure (PAS)", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.6876088745064206}, {"text": "natural language understanding", "start_pos": 145, "end_pos": 175, "type": "TASK", "confidence": 0.6463693479696909}]}, {"text": "Semantic role labeling (SRL) has made tremendous progress in addressing this task, using supervised and recently also semi-and unsupervised methods . Traditional SRL is restricted to the local syntactic domain.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8171278685331345}, {"text": "SRL", "start_pos": 162, "end_pos": 165, "type": "TASK", "confidence": 0.9733269810676575}]}, {"text": "In discourse interpretation, however, we typically find locally unrealized argument roles that are contextually bound to antecedents beyond their local structure.", "labels": [], "entities": [{"text": "discourse interpretation", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7447787523269653}]}, {"text": "Thus, by using strictly local methods, we are far from capturing the full potential offered by semantic argument structure).", "labels": [], "entities": []}, {"text": "The task of resolving the reference of implicit arguments has been addressed in previous work: Gerber and Chai (2012) address the task in the nominal domain by learning a model from manually annotated data following the NomBank paradigm ().", "labels": [], "entities": []}, {"text": "In contrast, follow the FrameNet paradigm, which is not restricted to nominal predicates.", "labels": [], "entities": []}, {"text": "However, their data set suffers from considerable sparsity with respect to annotation instances per predicate (cf. Section 2).", "labels": [], "entities": []}, {"text": "Our contribution addresses the problem of sparse training resources for implicit role binding by providing a higher volume of predicate-specific annotations for non-local role binding, using OntoNotes (Weischedel et al., 2011) as underlying corpus.", "labels": [], "entities": [{"text": "role binding", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.7009767889976501}, {"text": "OntoNotes (Weischedel et al., 2011)", "start_pos": 191, "end_pos": 226, "type": "DATASET", "confidence": 0.7869107499718666}]}, {"text": "A qualitative analysis of the produced annotations leads to a number of hypotheses on implicit role realization.", "labels": [], "entities": [{"text": "implicit role realization", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.5973672668139139}]}, {"text": "Using the extended set of annotations, we perform experiments to measure their impact, using a state-of-the-art system for implicit role binding.", "labels": [], "entities": [{"text": "role binding", "start_pos": 132, "end_pos": 144, "type": "TASK", "confidence": 0.7107506692409515}]}], "datasetContent": [{"text": "We evaluate the impact of predicate-specific annotations for classification using two scenarios: (CV) we examine the linking performance of models trained and tested on the same predicate by adopting the 10-fold Cross-Validation scenario used by Gerber and Chai (2012) (G&C).", "labels": [], "entities": []}, {"text": "11 (SemEval) Secondly, we examine the direct effect of using our annotations as additional training data for linking NIs in the SemEval 2010 task on implicit role binding.", "labels": [], "entities": [{"text": "SemEval 2010 task", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.7797907193501791}]}, {"text": "We use the state-of-the-art system and best performing feature set described in   ically added by extracting constituents that overtly fill a role according to the semantic annotations in the OntoNotes gold standard.", "labels": [], "entities": [{"text": "OntoNotes gold standard", "start_pos": 192, "end_pos": 215, "type": "DATASET", "confidence": 0.9242692192395529}]}, {"text": "We only consider phrases of type NPB, S, VP, SBAR and SG within the current and the two preceding sentences as potential fillers.", "labels": [], "entities": []}, {"text": "This setting is identical with the linking evaluation in S&F.", "labels": [], "entities": [{"text": "S&F", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8658831715583801}]}, {"text": "Like them, we (optionally) apply an additional step of feature selection (\u00b1FS) on the SemEval training data to select a feature subset that generalizes best across data sets, i.e., the fully annotated novel from the shared task and our predicatespecific annotations based on OntoNotes.", "labels": [], "entities": [{"text": "FS", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9612646698951721}, {"text": "SemEval training data", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.6659128864606222}]}, {"text": "We further compare models trained w/ and w/o non-genuine frame annotations (\u00b1NgFNS).", "labels": [], "entities": []}, {"text": "As in the CV setting, we assume that all resolvable NIs are known and only the correct fillers are unknown.", "labels": [], "entities": []}, {"text": "Thus, our results are not comparable to those of participants of the full SemEval task, who solved two further sub-tasks.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.883912056684494}]}, {"text": "Instead we compare to the NI linking results in S&F, with models trained on the SemEval data and using additional heuristically labelled data.", "labels": [], "entities": [{"text": "NI linking", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.6133758276700974}, {"text": "S&F", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.7489778796831766}, {"text": "SemEval data", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.7887291014194489}]}, {"text": "summarizes our results for both settings.", "labels": [], "entities": []}, {"text": "They are not strictly comparable due to varying properties, i.a., the number of available annotations.", "labels": [], "entities": []}, {"text": "The CV results show that few annotations can be sufficient to achieve a high linking precision and f-score (up to 72.7 P, 51.9 F 1 ).", "labels": [], "entities": [{"text": "CV", "start_pos": 4, "end_pos": 6, "type": "DATASET", "confidence": 0.8519349098205566}, {"text": "linking", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9156813621520996}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.7494899034500122}, {"text": "f-score", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9967756867408752}, {"text": "P", "start_pos": 119, "end_pos": 120, "type": "METRIC", "confidence": 0.6973656415939331}, {"text": "F 1", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.8759751617908478}]}, {"text": "However, this is highly dependent on the target predicate (cf. bring vs. pay).", "labels": [], "entities": [{"text": "bring", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9534897208213806}]}, {"text": "Overall, the results exhibit a similar variance and lie within the same range as those reported by G&C.", "labels": [], "entities": [{"text": "variance", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9584206938743591}, {"text": "G&C", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.8924699823061625}]}, {"text": "Even though the numbers are not directly comparable, they generally indicate a similar difficulty of linking implicit arguments across lexical predicate types.", "labels": [], "entities": []}, {"text": "In the SemEval setting, we obtain improved precision and recall over S&F's results (\u00b1 additional heuristic data, cf. Silberer&Frank, 2012)) when linking NIs using our additional training data and feature selection.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9994706511497498}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9991618394851685}]}, {"text": "Using our full additional data set (+NgFNS) we obtain higher performance compared to S&F's best setting with heuristically labelled data, yielding highest scores of 34.3% precision and 26.3% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.999035120010376}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9988611936569214}]}, {"text": "The resulting F 1 -score of 29.76% lies 2.1 points above the best model of S&F, whose full system also achieved state-of-the-art performance on the full SemEval task.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9904913306236267}, {"text": "S&F", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.7453591426213583}, {"text": "SemEval task", "start_pos": 153, "end_pos": 165, "type": "TASK", "confidence": 0.8769015073776245}]}], "tableCaptions": [{"text": " Table 1: Annotated predicates and data analysis: Implicit role interpretation and linking.", "labels": [], "entities": [{"text": "Annotated predicates", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7234857380390167}, {"text": "data analysis", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.7377157211303711}, {"text": "Implicit role interpretation", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.5923318068186442}]}, {"text": " Table 2: Distribution of resolvable vs. non-resolvable NI roles over predicate roles (in percent).", "labels": [], "entities": []}, {"text": " Table 3: Results for both evaluations (all figures are percentages). FS indicates whether feature selection  was applied. NgFNS indicates the use of frame annotations that do not match the contextual meaning.", "labels": [], "entities": [{"text": "FS", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.999004065990448}, {"text": "NgFNS", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.692754328250885}]}]}