{"title": [{"text": "An Analysis of Memory-based Processing Costs using Incremental Deep Syntactic Dependency Parsing *", "labels": [], "entities": [{"text": "Incremental Deep Syntactic Dependency Parsing", "start_pos": 51, "end_pos": 96, "type": "TASK", "confidence": 0.5348822176456451}]}], "abstractContent": [{"text": "Reading experiments using naturalistic stimuli have shown unanticipated facili-tations for completing center embeddings when frequency effects are factored out.", "labels": [], "entities": []}, {"text": "To eliminate possible confounds due to surface structure, this paper introduces a processing model based on deep syntactic dependencies.", "labels": [], "entities": []}, {"text": "Results on eye-tracking data indicate that completing deep syntactic embeddings yields significantly more facilitation than completing surface em-beddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Self-paced reading and eye-tracking experiments have often been used to support theories about inhibitory effects of working memory operations in sentence processing), but it is possible that many of these effects can be explained by frequency.", "labels": [], "entities": []}, {"text": "Experiments on large naturalistic text corpora have shown significant memory effects at the ends of center embeddings when frequency measures have been included as separate factors, but these memory effects have been facilitatory rather than inhibitory.", "labels": [], "entities": []}, {"text": "Some of the memory-based measures that produce these facilitatory effects ( are defined in terms of initiation and integration of connected components of syntactic structure, 1 with the presumption * *Thanks to Micha Elsner and three anonymous reviewers for their feedback.", "labels": [], "entities": []}, {"text": "This work was funded by an Ohio State University Department of Linguistics Targeted Investment for Excellence (TIE) grant for collaborative interdisciplinary projects conducted during the academic year 2012-13.", "labels": [], "entities": [{"text": "Ohio State University Department of Linguistics Targeted", "start_pos": 27, "end_pos": 83, "type": "TASK", "confidence": 0.5912665000983647}]}, {"text": "Graph theoretically, the set of connected components that referents that belong to the same connected component may cue one another using contentbased features, while those that do not must rely on noisier temporal features that just encode how recently a referent was accessed.", "labels": [], "entities": []}, {"text": "These measures, based on left-corner parsing processes, abstract counts of unsatisfied dependencies from noun or verb referents) to coverall syntactic dependencies, motivated by observations of Demberg and and of the inadequacies of Gibson's narrower measure.", "labels": [], "entities": []}, {"text": "But these experiments use naturalistic stimuli without constrained manipulations and therefore might be susceptible to confounds.", "labels": [], "entities": []}, {"text": "It is possible that the purely phrase-structure-based connected components used previously may ignore some integration costs associated with filler-gap constructions, making them an unsuitable generalization of Gibson-style dependencies.", "labels": [], "entities": []}, {"text": "It is also possible that the facilitatory effect for integration operations in naturally-occurring stimuli maybe driven by syntactic center embeddings that arise from modifiers (e.g. The CEO sold [[the shares] of the company]), which do not require any dependencies to be deferred, but which might be systematically under-predicted by frequency measures, producing a confound with memory measures when frequency measures are residualized out.", "labels": [], "entities": []}, {"text": "In order to eliminate possible confounds due to exclusion of unbounded dependencies in filler-gap constructions, this paper evaluates a processing model that calculates connected components on deep syntactic dependency structures rather than surface phrase structure trees.", "labels": [], "entities": []}, {"text": "This model accounts unattached fillers and gaps as belonging to separate connected components, and therefore performs additional initiation and integration opof a graph V, E is the set of maximal subsets of it {V 1 , E 1 , V 2 , E 2 , ...} such that any pair of vertices in each V i can be connected by edges in the corresponding erations in filler-gap constructions as hypothesized by and others.", "labels": [], "entities": []}, {"text": "Then, in order to control for possible confounds due to modifierinduced center embedding, this refined model is applied to two partitions of an eye-tracking corpus (: one consisting of sentences containing only non-modifier center embeddings, in which dependencies are deferred, and the other consisting of sentences containing no center embeddings or containing center embeddings arising from attachment of final modifiers, in which no dependencies are deferred.", "labels": [], "entities": []}, {"text": "Processing this partitioned corpus with deep syntactic connected components reveals a significant increase in facilitation in the non-modifier partition, which lends credibility to the observation of negative integration cost in processing naturally-occurring sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "The F, L, and N productions defined in the previous section can be made probabilistic by first computing a probabilistic context-free grammar (PCFG) from a tree-annotated corpus, then transforming that PCFG model into a model of probabilities over incremental parsing operations using a grammar transform.", "labels": [], "entities": []}, {"text": "This allows the intermediate PCFG to be optimized using an existing PCFG-based latent variable trainer ().", "labels": [], "entities": []}, {"text": "When applied to the output of this trainer, this transform has been shown to produce comparable accuracy to that of the original).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9993106126785278}]}, {"text": "The transform used in these experiments diverges from that of, in that the probability associated with introducing a gap in a filler-gap construction is reallocated from a -F-L operation to a +F-L operation (to encode the previously most subordinate connected component with the filler as its awaited sign and begin anew disjoint connected component), and the probability associated with resolving such a gap is reallocated from an implicit -N operation to a +N operation (to integrate the connected component containing the gap with that containing the filler).", "labels": [], "entities": []}, {"text": "In order to verify that the modifications to the transform correctly reallocate probability mass for gap operations, the goodness of fit to reading times of a model using this modified transform is compared against the publicly-available baseline model from van, which uses the original Schuler (2009) transform.", "labels": [], "entities": []}, {"text": "To ensure a valid comparison, both parsers are trained on a GCG-reannotated version of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) before being fit to reading times using linear mixed-effects models (.", "labels": [], "entities": [{"text": "GCG-reannotated version of the Wall Street Journal portion of the Penn Treebank", "start_pos": 60, "end_pos": 139, "type": "DATASET", "confidence": 0.863935982187589}]}, {"text": "This evaluation focuses on the processing that can be done up to a given point in a sentence.", "labels": [], "entities": []}, {"text": "In human subjects, this processing includes both immediate lexical access and regressions that The models used here also use random slopes to reduce their variance, which makes them less anticonservative.", "labels": [], "entities": []}, {"text": "The models are built using lmer from the lme4 R package ( aid in the integration of new information, so the reading times of interest in this evaluation are logtransformed go-past durations.", "labels": [], "entities": []}, {"text": "The first and last word of each line in the Dundee corpus, words not observed at least 5 times in the WSJ training corpus, and fixations after long saccades (>4 words) are omitted from the evaluation to filter out wrap-up effects, parser inaccuracies, and inattention and track loss of the eyetracker.", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9890594482421875}, {"text": "WSJ training corpus", "start_pos": 102, "end_pos": 121, "type": "DATASET", "confidence": 0.8686774571736654}, {"text": "track loss", "start_pos": 272, "end_pos": 282, "type": "METRIC", "confidence": 0.9547871947288513}]}, {"text": "The following predictors are centered and used in each baseline model: sentence position, word length, whether or not the previous or next word were fixated upon, and unigram and bigram probabilities.", "labels": [], "entities": []}, {"text": "8 Then each of the following predictors is residualized off each baseline before being centered and added to it to help residualize the next factor: length of the go-past region, cumulative total surprisal, total surprisal, and cumulative entropy reduction.", "labels": [], "entities": [{"text": "length", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.9781167507171631}]}, {"text": "All 2-way interactions between these effects are Go-past durations are calculated by summing all fixations in a region of text, including regressions, until anew region is fixated, which accounts for additional processing that may take place after initial lexical access, but before the next region is processed.", "labels": [], "entities": []}, {"text": "For example, if one region ends at word 5 in a sentence, and the next fixation lands on word 8, then the go-past region consists of words 6-8 while go-past duration sums all fixations until a fixation occurs afterword 8.", "labels": [], "entities": []}, {"text": "Log-transforming eye movements and fixations may make their distributions more normal and does not substantially affect the results of this paper.", "labels": [], "entities": []}, {"text": "8 For the n-gram model, this study uses the Brown corpus (, the WSJ Sections 02-21, the written portion of the British National Corpus, and the Dundee corpus () smoothed with modified) in SRILM.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9800602197647095}, {"text": "WSJ Sections 02-21", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.94053981701533}, {"text": "British National Corpus", "start_pos": 111, "end_pos": 134, "type": "DATASET", "confidence": 0.904421309630076}, {"text": "Dundee corpus", "start_pos": 144, "end_pos": 157, "type": "DATASET", "confidence": 0.9862381219863892}, {"text": "SRILM", "start_pos": 188, "end_pos": 193, "type": "DATASET", "confidence": 0.9501626491546631}]}, {"text": "Non-cumulative metrics are calculated from the final word of the go-past region; cumulative metrics are summed over the go-past region.", "labels": [], "entities": []}, {"text": "included as predictors along with the predictors from the previous go-past region (to account for spillover effects).", "labels": [], "entities": []}, {"text": "Finally, each model has subject and item random intercepts added in addition to by-subject random slopes (cumulative total surprisal, whether the previous word was fixated, and length of the go-past region) and is fit to centered log-transformed go-past durations.", "labels": [], "entities": [{"text": "length", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.964806079864502}]}, {"text": "The Akaike Information Criterion (AIC) indicates that the gap-reallocating model (AIC = 128,605) provides a better fit to reading times than the original model (AIC = 128,619).", "labels": [], "entities": [{"text": "Akaike Information Criterion (AIC)", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.5554075340429941}, {"text": "AIC", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9724898338317871}]}, {"text": "As described in Section 1, previous findings of negative integration cost maybe due to a confound whereby center-embedded constructions caused by modifiers, which do not require deep syntactic dependencies to be deferred, maybe driving the effect.", "labels": [], "entities": []}, {"text": "Under this hypothesis, embeddings that do not arise from final adjunction of modifiers (henceforth canonical embeddings) should yield a positive integration cost as found by.", "labels": [], "entities": []}, {"text": "To investigate this potential confound, the Dundee corpus is partitioned into two parts.", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9895192980766296}]}, {"text": "First, the model described in this paper is used to annotate the Dundee corpus.", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9925743639469147}]}, {"text": "From this annotated corpus, all sentences are collected that contain canonical embeddings and lack modifier-induced embeddings.", "labels": [], "entities": []}, {"text": "This produces two corpora: one consisting entirely of canonical center-embeddings such as those used in self-paced reading experiments with findings of positive integration cost (e.g. Gibson 2000), the other consisting of the remainder of the Dundee corpus, which contains sentences with canonical embeddings but also includes modifier-caused embeddings.", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 243, "end_pos": 256, "type": "DATASET", "confidence": 0.9758950769901276}]}, {"text": "The coefficient estimates for integration operations (-F+L and +N) on each of these corpora are then calculated using the baseline described above.", "labels": [], "entities": [{"text": "F", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.9677364826202393}]}, {"text": "To ensure embeddings are driving any observed effect rather than sentence wrap-up effects, the first and last words of each sentence are excluded from both data sets.", "labels": [], "entities": []}, {"text": "Integration cost is measured by the amount of probability mass the parser allocates to -F+L and +N operations, accu-10 Each fixed effect that has an absolute t-value greater than 10 when included in a random-intercepts only model is added as a random slope by-subject.", "labels": [], "entities": []}, {"text": "The relative likelihood of the original model to the gapsensitive model is 0.0009 (n = 151,331), which suggests the improvement is significant.", "labels": [], "entities": []}, {"text": "Modifier-induced embeddings are found by looking for embeddings that arise from inference rules Ma-h in Section 3.", "labels": [], "entities": []}], "tableCaptions": []}