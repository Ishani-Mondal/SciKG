{"title": [{"text": "Deconstr ucting Human Liter atur e Reviews - A Fr amewor k for Multi-Document Summar ization", "labels": [], "entities": [{"text": "A", "start_pos": 45, "end_pos": 46, "type": "METRIC", "confidence": 0.9295057058334351}]}], "abstractContent": [{"text": "Abstr act This study is conducted in the area of multi-document summarization, and develops a literature review framework based on a deconstruction of human-written literature review sections in information science research papers.", "labels": [], "entities": [{"text": "Abstr act", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9054068624973297}, {"text": "multi-document summarization", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6706967055797577}]}, {"text": "The first part of the study presents the results of a multi-level discourse analysis to investigate their discourse and content characteristics.", "labels": [], "entities": []}, {"text": "These findings were incorporated into a framework for literature reviews, focusing on their macro-level document structure and the sentence-level templates, as well as the information summarization strategies.", "labels": [], "entities": [{"text": "information summarization", "start_pos": 172, "end_pos": 197, "type": "TASK", "confidence": 0.7082766443490982}]}, {"text": "The second part of this study discusses insights from this analysis, and how the framework can be adapted to automatic summaries resembling human written literature reviews.", "labels": [], "entities": []}, {"text": "Summaries generated from a partial implementation are evaluated against human written summaries and assessors' comments are discussed to formulate recommendations for future work.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "To evaluate the framework, the objective was to compare its \"human-ness\" represented by its Comprehensibility, Readability and Usefulness against human-written literature reviews and machine-generated sentence extracts.", "labels": [], "entities": []}, {"text": "For this purpose, the framework was partially adapted in a summarization method focusing on comparing research objective information extracted from Abstracts and Introduction sections, and presenting a topical overview resembling a three-level literature review.", "labels": [], "entities": []}, {"text": "The output generated is similar to the summaries generated by) -sentences are extracted to provide a synopsis of similarities and unique features of studies are highlighted for individual papers; however our prototype does so without rely on external domain knowledge.", "labels": [], "entities": []}, {"text": "The method was implemented in Java on the Eclipse IDE, and it comprised three stages: \uf0b7 Text pre-processing: to extract sentences from the Abstract and Introduction of the input source papers.", "labels": [], "entities": []}, {"text": "Here the text is segmented, tokenized, parsed, stop-words are filtered and n-grams of noun phrases are created to represent concepts in the source papers.", "labels": [], "entities": []}, {"text": "\uf0b7 Information selection and integration: to identify similarities and differences across the research objective sentences of source papers.", "labels": [], "entities": [{"text": "Information selection and integration", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.7384992837905884}]}, {"text": "It selects important concepts based on the document frequency of lexical concept chains (, and applies the research objective sentence selection rules developed in the framework to select important information for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 214, "end_pos": 227, "type": "TASK", "confidence": 0.9807331562042236}]}, {"text": "\uf0b7 Text presentation: to produce text that has the characteristics of the literature review.", "labels": [], "entities": [{"text": "Text presentation", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.6950230002403259}]}, {"text": "It applies the document structure described in the framework, to organize the literature review, and sentence templates particular to research objective information in integrative literature reviews (the ones listed in).", "labels": [], "entities": []}, {"text": "The resultant summaries resemble a human written literature review because they are laid out as a topic tree and present a comparative overview of similarities and unique features.", "labels": [], "entities": []}, {"text": "However, some grammatical errors can be spotted, which would need a post-processing module to remove.", "labels": [], "entities": []}, {"text": "30 sets of information science source papers were prepared by sampling topics from 30 literature reviews from 2000-2008 issues of JASIST, Journal of Documentation and Journal of Information Science and downloading the papers they cited.", "labels": [], "entities": [{"text": "JASIST", "start_pos": 130, "end_pos": 136, "type": "DATASET", "confidence": 0.928767740726471}]}, {"text": "Only 3-10 source papers were downloaded for every sampled topic; this was so that the task could be manageable for the researchers constructing the human summaries.", "labels": [], "entities": []}, {"text": "An excerpt system summary is provided in.", "labels": [], "entities": []}, {"text": "For each input set of related research papers, three types of summaries were generated, each with a different kind of method -framework-based structure (by our method), sentence-extraction structure (by the baseline, MEAD) and a humanwritten summary by a researcher: \uf0b7 MEAD: The MEAD summarization system) was the baseline; it followed a sentence-extraction approach to generate multidocument extracts of information (generally news articles).", "labels": [], "entities": [{"text": "MEAD", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.8251147866249084}]}, {"text": "\uf0b7 System: Our system based on the framework, and focusing on the similarities and differences between research objectives at the lexical and syntactic level.", "labels": [], "entities": []}, {"text": "\uf0b7 Human: Five researchers from the School of Humanities and Social Sciences of our university summarized the research objective sentences from set of source papers in the context of a given (main) topic.", "labels": [], "entities": []}, {"text": "This literature review presents research in relevance published by,,, and.", "labels": [], "entities": []}, {"text": "Studies by and focus on retrieval mechanism.", "labels": [], "entities": []}, {"text": "Researchers in relevance have also considered users.", "labels": [], "entities": []}, {"text": "The study by demonstrates that it is productive to study relevance as a task and process-oriented user construct.", "labels": [], "entities": []}, {"text": "Studies by and focus on dynamic models.", "labels": [], "entities": []}, {"text": "The study by is a step in the empirical exploration of the evolutionary nature of relevance judgments.: Excerpt from a system summary In the human summaries, the coders selected an average of 3 sub-topics and 8 unique sub-topics in their summaries.", "labels": [], "entities": []}, {"text": "Human summaries also had the highest compression rate of 18%, as compared to a compression rate of 25% by MEAD and our System.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 37, "end_pos": 53, "type": "METRIC", "confidence": 0.9875210225582123}, {"text": "compression rate", "start_pos": 79, "end_pos": 95, "type": "METRIC", "confidence": 0.9713490307331085}, {"text": "MEAD", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.7949120998382568}]}, {"text": "An inter-coder agreement was conducted over 10 summaries by taking the summaries done by one of the post-graduate researchers as reference and comparing each pair of summaries, considering each of the \"similarities\" or \"differences\" as a \"common\" or \"unique\" subtopic.", "labels": [], "entities": []}, {"text": "Comparisons revealed that the coders usually had the same idea of what constituted an important \"similarity\" or common sub-topic (percent agreement= 70%) though they often chose different \"differences\" or unique sub-topics in their summaries (percent agreement= 56%).", "labels": [], "entities": []}, {"text": "Content evaluation of the 30 sets of summaries by the ROUGE-1 metric revealed that system summaries had a higher but not significantly different effectiveness or fmeasure of 0.38 as compared to the baseline (0.33).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.784604012966156}, {"text": "fmeasure", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9587092399597168}]}, {"text": "We developed our own version of ROUGE to measure information overlap by comparing the information concepts extracted from summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.6248064041137695}]}, {"text": "It was different from the standard ROUGE-1 in three ways: it filtered out \"research stopwords\" such as \"method\", \"experiment\" and \"study\", which didn't represent research information; it aggregated words which shared the same lemma; and it also conflated co-occurring adjacent words into the same information concepts.", "labels": [], "entities": []}, {"text": "Consequently, we obtained real scores of effectiveness in terms of higher f-measure scores for both the system and the baseline.", "labels": [], "entities": []}, {"text": "The system's f-measure (0.57) was a significant improvement over the baseline (0.50) at the 0.01 level.", "labels": [], "entities": [{"text": "f-measure", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9958695769309998}]}, {"text": "The results are provided in.", "labels": [], "entities": []}, {"text": "For the quality evaluation, 90 questionnaires were prepared from the 30 sets of summaries, using permutations of presentation orders to account for carry-over effects during assessment.", "labels": [], "entities": []}, {"text": "To recruit assessors, a call for participation in the evaluation was broadcast over the internet, through postings in discussion boards, personal emails and library sciences mailing lists.", "labels": [], "entities": []}, {"text": "The invitation was also personally extended to authors of other publications in JASIST, JDoc and JIS.", "labels": [], "entities": [{"text": "JASIST", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.9515382647514343}, {"text": "JDoc", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8915644884109497}, {"text": "JIS", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9148996472358704}]}, {"text": "The invitation for participation was restricted to only Library and Information Science and Computer Science researchers and PhD students who had passed their qualifying exam.", "labels": [], "entities": []}, {"text": "It was anticipated that such assessors would be more familiar with the topics in the summary, and would be able to make meaningful comments about the summaries and their characteristics, such as lack of evident comparisons and generalizations, or incorrect comparisons and generalizations among unlike information.", "labels": [], "entities": []}, {"text": "There were a total number of 35 assessors with a mean research experience of 6 years, who provided 67 responses, by filling out 1 or 2 each, over a period of two months.", "labels": [], "entities": []}, {"text": "The assessors were from reputable international universities in different countries.", "labels": [], "entities": []}, {"text": "The highest degrees held by the assessors varied from Bachelors (for PhD students who had passed their qualifying exam) to PhD.", "labels": [], "entities": [{"text": "Bachelors", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9944013953208923}, {"text": "PhD", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.926628053188324}]}, {"text": "They scored the summaries on their Comprehensibility, Readability and Usefulness and also provided qualitative comments to the following questions: \uf0b7 What did you like about this summary?", "labels": [], "entities": [{"text": "Comprehensibility", "start_pos": 35, "end_pos": 52, "type": "METRIC", "confidence": 0.9513799548149109}]}, {"text": "\uf0b7 What did you find confusing about this summary?", "labels": [], "entities": []}, {"text": "\uf0b7 How is this summary, a good/bad literature review?", "labels": [], "entities": []}, {"text": "The quantitative results in show that the System summary was significantly more readable and more useful than the baseline at the 0.05 level.", "labels": [], "entities": []}, {"text": "The qualitative results (provided in) are equally interesting and show that researchers with different number of years of research liked or disliked different things about the System summary.", "labels": [], "entities": []}, {"text": "Researchers with 0-4 years of experience did not have any specific preference of one type of summary over another.", "labels": [], "entities": []}, {"text": "Researchers with 5-8 years of experience were more conscious of grammatical errors and repetition mistakes in the system summary.", "labels": [], "entities": []}, {"text": "Researchers with 9-12 years of experience ignored the grammatical errors in Human summaries and System and instead criticized their lack of detail.", "labels": [], "entities": [{"text": "detail", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.954499363899231}]}, {"text": "Researchers with 13 years or experience or more were sensitive to the overall \"context\" and \"flow\" of the summary.", "labels": [], "entities": []}, {"text": "Most of the assessors were able to identify the main topic and its related sub-topics; however, they experienced the System as being more disjointed, lacking \"focus\" as compared to the Human summaries.", "labels": [], "entities": []}, {"text": "On the whole, researchers were satisfied with the overview provided as well as the hierarchical organization.", "labels": [], "entities": []}, {"text": "It would be interesting to see whether these findings and differences would be replicated in a larger study..", "labels": [], "entities": []}, {"text": "Results from the quality evaluation (N=67)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4. Results from the content evaluation  (N=30)", "labels": [], "entities": []}, {"text": " Table 5. Results from the quality evaluation  (N=67)", "labels": [], "entities": []}]}