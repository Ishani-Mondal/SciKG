{"title": [{"text": "Can MDL Improve Unsupervised Chinese Word Segmentation?", "labels": [], "entities": [{"text": "MDL Improve Unsupervised Chinese Word Segmentation", "start_pos": 4, "end_pos": 54, "type": "TASK", "confidence": 0.8998242318630219}]}], "abstractContent": [{"text": "It is often assumed that Minimum Description Length (MDL) is a good criterion for unsupervised word segmentation.", "labels": [], "entities": [{"text": "Minimum Description Length (MDL)", "start_pos": 25, "end_pos": 57, "type": "METRIC", "confidence": 0.8003639280796051}, {"text": "word segmentation", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7121832221746445}]}, {"text": "In this paper, we introduce anew approach to unsupervised word segmentation of Mandarin Chinese, that leads to segmentations whose Description Length is lower than what can be obtained using other algorithms previously proposed in the literature.", "labels": [], "entities": [{"text": "word segmentation of Mandarin Chinese", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.8263126492500306}]}, {"text": "Suprisingly, we show that this lower Description Length does not necessarily corresponds to better segmentation results.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 99, "end_pos": 111, "type": "TASK", "confidence": 0.9614980220794678}]}, {"text": "Finally, we show that we can use very basic linguistic knowledge to coerce the MDL towards a linguistically plausible hypothesis and obtain better results than any previously proposed method for unsupervised Chinese word segmentation with minimal human effort.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 208, "end_pos": 233, "type": "TASK", "confidence": 0.675171842177709}]}], "introductionContent": [{"text": "In Chinese script, very few symbols can be considered as word boundary markers.", "labels": [], "entities": []}, {"text": "The only easily identifiable boundaries are sentence beginnings and endings, as well as positions before and after punctuation marks.", "labels": [], "entities": [{"text": "sentence beginnings and endings", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.7574486881494522}]}, {"text": "Although the script doesn't rely on typography to define (orthographic) \"words\", a word-level segmentation is often required for further natural language processing.", "labels": [], "entities": []}, {"text": "This level corresponds to minimal syntactic units that can be POS-tagged or used as input for parsing.", "labels": [], "entities": []}, {"text": "Without word-boundary characters, like whitespace in Latin script, there is no trivial tokenization method that can yield a good enough approximation for further processing.", "labels": [], "entities": []}, {"text": "Therefore, the first step of many NLP systems for written Chinese is the Chinese word segmentation task.", "labels": [], "entities": [{"text": "Chinese word segmentation task", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.6495111957192421}]}, {"text": "A great variety of methods have been proposed in the literature, mostly in supervised machine learning settings.", "labels": [], "entities": []}, {"text": "Our work addresses the question of unsupervised segmentation, i.e., without any manually segmented training data.", "labels": [], "entities": []}, {"text": "Although supervised learning typically performs better than unsupervised learning, we believe that unsupervised systems are worth investigating as they require less human labour and are likely to be more easily adaptable to various genres, domains and time periods.", "labels": [], "entities": []}, {"text": "They can also provide more valuable insight for linguistic studies.", "labels": [], "entities": []}, {"text": "Amongst the unsupervised segmentation systems described in the literature, two paradigms are often used: Branching Entropy (BE) and Minimum Description Length (MDL).", "labels": [], "entities": [{"text": "Branching Entropy (BE)", "start_pos": 105, "end_pos": 127, "type": "METRIC", "confidence": 0.5839538991451263}]}, {"text": "The system we describe in this paper relies on both.", "labels": [], "entities": []}, {"text": "We introduce anew algorithm which searches in a larger hypothesis space using the MDL criterion, thus leading to lower Description Lengths than other previously published systems.", "labels": [], "entities": []}, {"text": "Still, this improvement concerning the Description Length does not come with better results on the Chinese word segmentation task, which raises interesting issues.", "labels": [], "entities": [{"text": "Chinese word segmentation task", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.6347677111625671}]}, {"text": "However, it turns out that it is possible to add very simple constraints to our algorithm in order to adapt it to the specificities of Mandarin Chinese in away that leads to results better than the state-of-the-art on the Chinese word segmentation task.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 230, "end_pos": 252, "type": "TASK", "confidence": 0.7565448780854543}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the role of Branching Entropy in various previous works on Chinese word segmentation, including the algorithm we use as an initialisation step in this paper.", "labels": [], "entities": [{"text": "Branching Entropy", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.9551364183425903}, {"text": "Chinese word segmentation", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.6251870095729828}]}, {"text": "In Section 3 we explain how the MDL paradigm is used amongst different Chinese word segmentation systems in the literature.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.7273600101470947}]}, {"text": "We describe in Section 4 the way we use MDL for trying and improving the results of the initialisation step.", "labels": [], "entities": [{"text": "initialisation step", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.9116635620594025}]}, {"text": "A first evaluation and the error analysis given in Section 5 allow us to refine the algorithm and achieve our best results, as shown in Section 6.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.9685055017471313}]}, {"text": "Finally, we discuss our findings and their implications for our futur work in Section 7.", "labels": [], "entities": [{"text": "Section 7", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.7114257216453552}]}], "datasetContent": [{"text": "The metric used for all following evaluations is a standard f-score on words.", "labels": [], "entities": []}, {"text": "It is the harmonic mean of the word recall R w = #correct words in the results #words in the gold corpus and the word precision P w = #correct words in the result #words in the result , which leads to the following: For each corpus and method, we also present the Description Length of each segmentation.", "labels": [], "entities": [{"text": "word precision P w", "start_pos": 113, "end_pos": 131, "type": "METRIC", "confidence": 0.8389312773942947}]}, {"text": "Note that, as mentioned by several studies, the agreement between the different guidelines and even between untrained native speakers is not high.", "labels": [], "entities": []}, {"text": "Using crosstrained supervized systems or inter-human agreement, these studies suggest that the topline for unsupervised segmentation is beetween 0.76 and 0.85.", "labels": [], "entities": []}, {"text": "As a result, not only the output of an unsupervised system cannot be expected to perfectly mimic a given \"gold\" segmented corpus, but performances around 0.80 against multiple \"gold\" segmented corpora using different guidelines can be regarded as satisfying.", "labels": [], "entities": []}, {"text": "Given this error analysis, there are three main types of common mistakes that we would like to avoid: \u2022 merging MWEs such as named entities; \u2022 merging function words with content words when the co-occurrence is frequent;   \u2022 splitting bigrams that were correct in the initial segmentation.", "labels": [], "entities": [{"text": "merging MWEs such as named entities", "start_pos": 104, "end_pos": 139, "type": "TASK", "confidence": 0.7118189533551534}]}, {"text": "If we give upon having a strictly languageindependent system and focus on Mandarin Chinese segmentation, these three issues are easy to address with a fairly low amount of human work to add some basic linguistic knowledge about Chinese to the system.", "labels": [], "entities": [{"text": "Mandarin Chinese segmentation", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.6446457405885061}]}, {"text": "The first issue can be dealt with by limiting the length of a merge's output.", "labels": [], "entities": []}, {"text": "A MWE will be larger than atypical Chinese word that very rarely exceeds 3 characters.", "labels": [], "entities": []}, {"text": "With the exception of phonetic loans for foreign languages, larger units typically correspond to MWE that are segmented in the various gold corpora.", "labels": [], "entities": []}, {"text": "The question whether it is a good thing to door not will be raised in the discussion section, but fora higher f-score on word segmentation, leaving them segmented does help.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7197616696357727}]}, {"text": "The second issue can be addressed using a closed list of function words such as aspectual markers and pre/post-positions.", "labels": [], "entities": []}, {"text": "As those area closed list of items, listing all of them is an easily manually tractable task.", "labels": [], "entities": []}, {"text": "Here is the list we used in our experiments: \u7684\u3001\u4e86\u3001\u4e0a\u3001\u5728\u3001\u4e0b\u3001\u4e2d\u3001\u662f\u3001\u6709\u3001\u548c\u3001\u4e0e\u3001 \u548c\u3001\u5c31\u3001\u591a\u3001\u4e8e\u3001\u5f88\u3001\u624d\u3001\u8ddf As for the third issue, since Chinese is known to favour bigram words, we simply prevent our system to split those.", "labels": [], "entities": []}, {"text": "We implemented these three constraints to restrict the search space for our minimization of the Description Length an re-run the experiments.", "labels": [], "entities": []}, {"text": "Results are presented in the next section.", "labels": [], "entities": []}, {"text": "The scores obtained by our second system are given in.", "labels": [], "entities": []}, {"text": "They show a large improvement on our initial segmentation and outperform previously reported results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of the different corpora", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9773651957511902}]}, {"text": " Table 2: Scores on different Corpora for Zhikov  et al.'s", "labels": [], "entities": []}]}