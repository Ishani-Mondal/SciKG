{"title": [{"text": "The CMU Machine Translation Systems at WMT 2013: Syntax, Synthetic Translation Options, and Pseudo-References", "labels": [], "entities": [{"text": "CMU Machine Translation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6825841267903646}, {"text": "WMT 2013", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8658157587051392}, {"text": "Synthetic Translation Options", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.849631110827128}]}], "abstractContent": [{"text": "We describe the CMU systems submitted to the 2013 WMT shared task in machine translation.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7395381132761637}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.6980147808790207}]}, {"text": "We participated in three language pairs, French-English, Russian-English, and English-Russian.", "labels": [], "entities": []}, {"text": "Our particular innovations include: a label-coarsening scheme for syntactic tree-to-tree translation and the use of specialized modules to create \"synthetic translation options\" that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context.", "labels": [], "entities": [{"text": "syntactic tree-to-tree translation", "start_pos": 66, "end_pos": 100, "type": "TASK", "confidence": 0.6738811532656351}]}], "introductionContent": [{"text": "The MT research group at Carnegie Mellon University's Language Technologies Institute participated in three language pairs for the 2013 Workshop on Machine Translation shared translation task: French-English, Russian-English, and English-Russian.", "labels": [], "entities": [{"text": "Machine Translation shared translation task", "start_pos": 148, "end_pos": 191, "type": "TASK", "confidence": 0.8381764888763428}]}, {"text": "Our French-English system ( \u00a73) showcased our group's syntactic system with coarsened nonterminal types).", "labels": [], "entities": []}, {"text": "Our Russian-English and English-Russian system demonstrate anew multiphase approach to translation that our group is using, in which synthetic translation options ( \u00a74) to supplement the default translation rule inventory that is extracted from word-aligned training data.", "labels": [], "entities": []}, {"text": "In the Russian-English system ( \u00a75), we used a CRF-based transliterator to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language model probability.", "labels": [], "entities": []}, {"text": "In the English-Russian system ( \u00a76), we used a conditional logit model to predict the most likely inflectional morphology of Russian lemmas, conditioning on rich source syntactic features ( \u00a76.1).", "labels": [], "entities": []}, {"text": "In addition to being able to generate inflected forms that were otherwise unobserved in the parallel training data, the translations options generated in this matter had features reflecting their appropriateness given much broader source language context than usually would have been incorporated in current statistical MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 320, "end_pos": 322, "type": "TASK", "confidence": 0.8788792490959167}]}, {"text": "For our Russian-English system, we additionally used a secondary \"pseudo-reference\" translation when tuning the parameters of our RussianEnglish system.", "labels": [], "entities": []}, {"text": "This was created by automatically translating the Spanish translation of the provided development data into English.", "labels": [], "entities": []}, {"text": "While the output of an MT system is not always perfectly grammatical, previous work has shown that secondary machine-generated references improve translation quality when only a single human reference is available when BLEU is used as an optimization criterion).", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9857903718948364}, {"text": "BLEU", "start_pos": 219, "end_pos": 223, "type": "METRIC", "confidence": 0.9939185380935669}]}], "datasetContent": [{"text": "We tuned our system to the newstest2008 set of 2051 segments.", "labels": [], "entities": [{"text": "newstest2008 set of 2051 segments", "start_pos": 27, "end_pos": 60, "type": "DATASET", "confidence": 0.9425004124641418}]}, {"text": "Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year's newstest2012 set Each system variant is run with two independent MERT steps in order to control for optimizer instability.", "labels": [], "entities": [{"text": "newstest2013 test set", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.9456776777903239}, {"text": "MERT", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.7973406910896301}]}, {"text": "presents the results, with the metric scores averaged over both MERT runs.", "labels": [], "entities": [{"text": "MERT runs", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.6791655570268631}]}, {"text": "Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases.", "labels": [], "entities": []}, {"text": "No system is fully statistically separable (at p < 0.05) from the others according to MultEval's approximate randomization algorithm.", "labels": [], "entities": []}, {"text": "The closest is the variant with cutoff 200, which is generally judged to be slightly worse than the other two.", "labels": [], "entities": [{"text": "cutoff 200", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9685076773166656}]}, {"text": "METEOR claims full distinction on the 2013 test set, ranking the system with the strictest grammar cutoff (500) best.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.72646564245224}, {"text": "2013 test set", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.8959277669588724}, {"text": "strictest grammar cutoff (500)", "start_pos": 81, "end_pos": 111, "type": "METRIC", "confidence": 0.7996111412843069}]}, {"text": "This is the version that we ultimately submitted to the shared translation task.", "labels": [], "entities": [{"text": "shared translation task", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.6622565189997355}]}], "tableCaptions": [{"text": " Table 1: French-English automatic metric scores for three grammar pruning cutoffs, averaged over two  MERT runs each. Scores that are statistically separable (p < 0.05) from both others in the same column  are marked with an asterisk (*).", "labels": [], "entities": [{"text": "MERT", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.754271924495697}]}]}