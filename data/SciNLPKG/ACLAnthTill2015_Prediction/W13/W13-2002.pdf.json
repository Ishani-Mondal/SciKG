{"title": [{"text": "The Genia Event Extraction Shared Task, 2013 Edition -Overview", "labels": [], "entities": [{"text": "Genia Event Extraction Shared Task", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.809025514125824}]}], "abstractContent": [{"text": "The Genia Event Extraction task is organized for the third time, in BioNLP Shared Task 2013.", "labels": [], "entities": [{"text": "Genia Event Extraction task", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8699478209018707}, {"text": "BioNLP Shared Task 2013", "start_pos": 68, "end_pos": 91, "type": "DATASET", "confidence": 0.7453604340553284}]}, {"text": "Toward knowledge based construction , the task is modified in a number of points.", "labels": [], "entities": [{"text": "knowledge based construction", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.7235977252324423}]}, {"text": "As the final results, it received 12 submissions, among which 2 were withdrawn from the final report.", "labels": [], "entities": []}, {"text": "This paper presents the task setting, data sets, and the final results with discussion for possible future directions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Among various resources of life science, literature is regarded as one of the most important types of knowledge base.", "labels": [], "entities": []}, {"text": "Nevertheless, lack of explicit structure in natural language texts prevents computer systems from accessing fine-grained information written in literature.", "labels": [], "entities": []}, {"text": "BioNLP Shared Task (ST) series () is one of the community-wide efforts to address the problem.", "labels": [], "entities": [{"text": "BioNLP Shared Task (ST)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5521209239959717}]}, {"text": "Since its initial organization in 2009, BioNLP-ST series has published a number of finegrained information extraction (IE) tasks motivated for bioinformatics projects.", "labels": [], "entities": [{"text": "finegrained information extraction (IE)", "start_pos": 83, "end_pos": 122, "type": "TASK", "confidence": 0.78312519689401}]}, {"text": "Having solicited wide participation from the community of natural language processing, machine learning, and bioinformatics, it has contributed to the production of rich resources for fine-grained BioIE, e.g., TEES The Genia Event Extraction (GE) task is a seminal task of BioNLP-ST.", "labels": [], "entities": [{"text": "Genia Event Extraction (GE) task", "start_pos": 219, "end_pos": 251, "type": "TASK", "confidence": 0.8034166012491498}]}, {"text": "It was first organized as the sole task of the initial 2009 edition of BioNLP-ST.", "labels": [], "entities": [{"text": "BioNLP-ST", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.7941362857818604}]}, {"text": "The task was originally designed and implemented based on the Genia event corpus) which represented domain knowledge around NF\u03baB proteins.", "labels": [], "entities": [{"text": "Genia event corpus", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.7154541611671448}]}, {"text": "There were also some efforts to explore the possibility of literature mining for pathway construction.", "labels": [], "entities": [{"text": "pathway construction", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.8531155288219452}]}, {"text": "The GE task was designed to make such an effort a community-driven one by sharing available resources, e.g., benchmark data sets, and evaluation tools, with the community.", "labels": [], "entities": [{"text": "GE task", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8402230143547058}]}, {"text": "In its second edition () organized in), the data sets were extended to include full text articles.", "labels": [], "entities": []}, {"text": "The data sets consisted of two collections.", "labels": [], "entities": []}, {"text": "The abstract collection, that had come from the first edition, was used again to measure the progress of the community between 2009 and 2011 editions, and the full text collection, that was newly created, was used to measure the generalization of the technology to full text papers.", "labels": [], "entities": []}, {"text": "In its third edition this year, while succeeding the fundamental characteristics from its previous editions, the GE task tries to evolve with the goal to make it a more \"real\" task toward knowledge base construction.", "labels": [], "entities": [{"text": "GE task", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.7952493727207184}, {"text": "knowledge base construction", "start_pos": 188, "end_pos": 215, "type": "TASK", "confidence": 0.6248241066932678}]}, {"text": "The first design choice to address the goal is to construct the data sets fully with recent full papers, so that the extracted pieces of information can represent up-to-date knowledge of the domain.", "labels": [], "entities": []}, {"text": "The abstract collection, that had been already used twice (in 2009 and 2011), is removed from official evaluation this time . Second, GE task subsumes the coreference task which has long been considered critical for improvement of event extraction performance.", "labels": [], "entities": [{"text": "GE", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.7506263852119446}, {"text": "event extraction", "start_pos": 231, "end_pos": 247, "type": "TASK", "confidence": 0.7594821155071259}]}, {"text": "It is implemented by providing coreference annotation in integration with event annotation in the data sets.", "labels": [], "entities": [{"text": "coreference annotation", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.8727709054946899}]}, {"text": "The paper explains the task setting and data sets, presents the final results of participating systems, and discusses notable observations with conclusions.: Event types and their arguments for Genia Event Extraction task.", "labels": [], "entities": [{"text": "Genia Event Extraction task", "start_pos": 194, "end_pos": 221, "type": "TASK", "confidence": 0.778681293129921}]}, {"text": "The type of each filler entity is specified in parenthesis.", "labels": [], "entities": []}, {"text": "Arguments that maybe filled more than once per event are marked with \"+\", and optional arguments are with \"?\".", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics of annotations in training, de- velopment, and test sets", "labels": [], "entities": []}, {"text": " Table 5: System profiles: SnowBall=SnowBall Stemmer, CNLP=Stanford CoreNLP (tokenization),  McCCJ=McClosky-Charniak-Johnson Parser, Stanford=Stanford Parser, S.=Speculation, N.=Negation", "labels": [], "entities": [{"text": "SnowBall", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.9346360564231873}, {"text": "SnowBall", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9277717471122742}]}, {"text": " Table 6: Evaluation results (recall / precision / f-score) of Task 1. Some notable figures are emphasized  in bold.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9988090991973877}, {"text": "precision / f-score)", "start_pos": 39, "end_pos": 59, "type": "METRIC", "confidence": 0.7656151056289673}]}, {"text": " Table 7: Evaluation results (recall / precision / f-score) of Task 1 in titles and abstracts. Some notable  figures are emphasized in bold.", "labels": [], "entities": [{"text": "recall / precision / f-score)", "start_pos": 30, "end_pos": 59, "type": "METRIC", "confidence": 0.7718669672807058}]}, {"text": " Table 8: Evaluation results (recall / precision / f-score) of Task 1 in Methods section group. Some notable  figures are emphasized in bold.", "labels": [], "entities": [{"text": "recall / precision / f-score)", "start_pos": 30, "end_pos": 59, "type": "METRIC", "confidence": 0.7763607203960419}]}, {"text": " Table 9: Evaluation results (recall / precision / f-score) of Task 1 in Captions section group. Some notable  figures are emphasized in bold.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9988070726394653}, {"text": "precision / f-score)", "start_pos": 39, "end_pos": 59, "type": "METRIC", "confidence": 0.7574311047792435}, {"text": "Captions section", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.8734787106513977}]}, {"text": " Table 10: Evaluation results (recall / precision / f-score) of Task 2", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9993083477020264}, {"text": "precision / f-score)", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.7427936345338821}]}]}