{"title": [], "abstractContent": [{"text": "We apply multi-rate HMMs, a tree struc-tured HMM model, to the word-alignment problem.", "labels": [], "entities": []}, {"text": "Multi-rate HMMs allow us to model reordering at both the morpheme level and the word level in a hierarchical fashion.", "labels": [], "entities": []}, {"text": "This approach leads to better machine translation results than a morpheme-aware model that does not explicitly model morpheme reordering.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7389012575149536}]}], "introductionContent": [{"text": "We present an HMM-based word-alignment model that addresses transitions between morpheme positions and word positions simultaneously.", "labels": [], "entities": []}, {"text": "Our model is an instance of a multi-scale HMM, a widely used method for modeling different levels of a hierarchical stochastic process.", "labels": [], "entities": []}, {"text": "In multi-scale modeling of language, the deepest level of the hierarchy may consist of the phoneme sequence, and going up in the hierarchy, the next level may consist of the syllable sequence, and then the word sequence, the phrase sequence, and soon.", "labels": [], "entities": []}, {"text": "By the same token, in the hierarchical wordalignment model we present here, the lower level consists of the morpheme sequence and the higher level the word sequence.", "labels": [], "entities": []}, {"text": "Multi-scale HMMs have a natural application in language processing due to the hierarchical nature of linguistic structures.", "labels": [], "entities": []}, {"text": "They have been used for modeling text and handwriting, in signal processing), knowledge extraction (, as well as in other fields of AI such as vision () and robotics ().", "labels": [], "entities": [{"text": "knowledge extraction", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8556363880634308}]}, {"text": "The model we propose here is most similar to multi-rate HMMs, which were applied to a classification problem in industrial machine tool wear.", "labels": [], "entities": []}, {"text": "The vast majority of languages exhibit morphology to some extent, leading to various efforts in machine translation research to include morphology in translation models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7461961507797241}]}, {"text": "For the word-alignment problem, and suggested word alignment models that address morphology directly.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7731151580810547}]}, {"text": "introduced two-level alignment models (TAM), which adopt a hierarchical representation of alignment: the first level involves word alignment, the second level involves morpheme alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.7047147452831268}]}, {"text": "TAMs jointly induce word and morpheme alignments using an EM algorithm.", "labels": [], "entities": [{"text": "word and morpheme alignments", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6626764833927155}]}, {"text": "TAMs can align rarely occurring words through their frequently occurring morphemes.", "labels": [], "entities": []}, {"text": "In other words, they use morpheme probabilities to smooth rare word probabilities.", "labels": [], "entities": []}, {"text": "introduced TAM 1, which is analogous to IBM Model 1, in that the first level is a bag of words in a pair of sentences, and the second level is a bag of morphemes.", "labels": [], "entities": [{"text": "TAM", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.730606734752655}]}, {"text": "By introducing distortion probabilities at the word level, defined the HMM extension of TAM 1, the TAM-HMM.", "labels": [], "entities": []}, {"text": "TAM-HMM was shown to be superior to its single-level counterpart, i.e., the HMM-based word alignment model of.", "labels": [], "entities": [{"text": "TAM-HMM", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6513062119483948}, {"text": "HMM-based word alignment", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.7129096190134684}]}, {"text": "The alignment example in shows a Turkish word aligned to an English phrase.", "labels": [], "entities": []}, {"text": "The morphemes of the Turkish word are aligned to the English words.", "labels": [], "entities": []}, {"text": "As the example shows, morphologically rich languages exhibit complex reordering phenomena at the morpheme level, which is left unutilized in TAM-HMMs.", "labels": [], "entities": []}, {"text": "In this paper, we add morpheme sequence modeling to TAMs to capture morpheme level distortions.", "labels": [], "entities": [{"text": "morpheme sequence modeling", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.6498418152332306}]}, {"text": "The example also shows that the Turkish morpheme or-from our people who sell eyeglasses g\u00f6z\u00adl\u00fck\u00ad\u00e7\u00fc\u00adler\u00adimiz\u00adden der is the reverse of the English word order.", "labels": [], "entities": []}, {"text": "Because this pattern spans several English words, it can only be captured by modeling morpheme reordering across word boundaries.", "labels": [], "entities": []}, {"text": "We chose multirate HMMs over other hierarchical HMM models because multi-rate HMMs allow morpheme sequence modeling across words over the entire sentence.", "labels": [], "entities": []}, {"text": "It is possible to model the morpheme sequence by treating morphemes as words: segmenting the words into morphemes, and using word-based word alignment models on the segmented data.", "labels": [], "entities": [{"text": "word-based word alignment", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.6080132722854614}]}, {"text": "showed that TAM-HMM performs better than treating morphemes as words.", "labels": [], "entities": [{"text": "TAM-HMM", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.7932102084159851}]}, {"text": "Since the multi-rate HMM allows both word and morpheme sequence modeling, it is a generalization of TAM-HMM, which allows only word sequence modeling.", "labels": [], "entities": [{"text": "word and morpheme sequence modeling", "start_pos": 37, "end_pos": 72, "type": "TASK", "confidence": 0.6386811554431915}, {"text": "word sequence modeling", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.6560284694035848}]}, {"text": "TAM-HMM in turn is a generalization of the model suggested by and TAM 1.", "labels": [], "entities": [{"text": "TAM-HMM", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.4458925724029541}]}, {"text": "Our results show that multi-rate HMMs are superior to TAMHMMs.", "labels": [], "entities": [{"text": "HMMs", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.794234037399292}]}, {"text": "Therefore, multi-rate HMMs are the best two-level alignment models proposed so far.", "labels": [], "entities": []}], "datasetContent": [{"text": "We initialized our implementation of the single level 'word-only' model, which we call 'baseline' in We used Dirichlet priors in both IBM Model 1 and TAM 1 training.", "labels": [], "entities": []}, {"text": "We experimented with using Dirichlet priors on the HMM extensions of both IBM-HMM and TAM-HMM.", "labels": [], "entities": [{"text": "IBM-HMM", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9117645621299744}, {"text": "TAM-HMM", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.8319585919380188}]}, {"text": "We report the best results obtained for each model and translation direction.", "labels": [], "entities": []}, {"text": "We evaluated the performance of our model in two different ways.", "labels": [], "entities": []}, {"text": "First, we evaluated against gold word alignments for 75 Turkish-English sentences.", "labels": [], "entities": []}, {"text": "shows the AER ( of the word alignments; we report the growdiag-final ( of the Viterbi alignments.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.99941086769104}]}, {"text": "Second, we used the Moses toolkit ( to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.6854453235864639}, {"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.998736560344696}]}, {"text": "In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9994446635246277}]}, {"text": "Since the BLEU scores we obtained are close, we did a significance test on the scores).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988240599632263}]}, {"text": "In, the colors partition the table into equivalence classes: If two scores within the same row have different background colors, then the difference between their scores is statistically significant.", "labels": [], "entities": []}, {"text": "The best scores in the leftmost column were obtained from multi-rate HMMs with Dirichlet priors only during the TAM 1 training.", "labels": [], "entities": [{"text": "TAM 1 training", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.6928473909695944}]}, {"text": "On the contrary, the best scores for TAM-HMM and the baseline-HMM were obtained with Dirichlet priors both during the TAM 1 and the TAM-HMM training.", "labels": [], "entities": [{"text": "TAM-HMM", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.5931811928749084}]}, {"text": "In, as the scores improve gradually towards the left, the background color gets gradually lighter, depicting the statistical significance of the improvements.", "labels": [], "entities": []}, {"text": "The multi-rate HMM performs better than the TAM-HMM, which in turn performs better than the word-only models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: AER and BLEU Scores", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.99796462059021}, {"text": "BLEU Scores", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9583428204059601}]}]}