{"title": [{"text": "Combining Shallow and Linguistically Motivated Features in Native Language Identification", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.686426838239034}]}], "abstractContent": [{"text": "We explore a range of features and ensembles for the task of Native Language Identification as part of the NLI Shared Task (Tetreault et al., 2013).", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.6243407428264618}]}, {"text": "Starting with recurring word-based n-grams (Bykh and Meurers, 2012), we tested different linguistic abstractions such as part-of-speech, dependencies, and syntactic trees as features for NLI.", "labels": [], "entities": []}, {"text": "We also experimented with features encoding morphological properties , the nature of the realizations of particular lemmas, and several measures of complexity developed for proficiency and readabil-ity classification (Vajjala and Meurers, 2012).", "labels": [], "entities": [{"text": "readabil-ity classification", "start_pos": 189, "end_pos": 216, "type": "TASK", "confidence": 0.6751645058393478}]}, {"text": "Employing an ensemble classifier incorporating all of our features we achieved an accuracy of 82.2% (rank 5) in the closed task and 83.5% (rank 1) in the open-2 task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9995890259742737}]}, {"text": "In the open-1 task, the word-based recurring n-grams outperformed the ensemble, yielding 38.5% (rank 2).", "labels": [], "entities": []}, {"text": "Overall, across all three tasks, our best accuracy of 83.5% for the standard TOEFL11 test set came in second place.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9995480179786682}, {"text": "TOEFL11 test set", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9162107507387797}]}], "introductionContent": [{"text": "Native Language Identification (NLI) tackles the problem of determining the native language of an author based on a text the author has written in a second language.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.782477060953776}]}, {"text": "With,, and as first publications on NLI, the research focus in computational linguistics is relatively young.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.7129291892051697}]}, {"text": "But with over a dozen new publications in the last two years, it is gaining significant momentum.", "labels": [], "entities": []}, {"text": "In, we explored a datadriven approach using recurring n-grams with three levels of abstraction using parts-of-speech (POS).", "labels": [], "entities": []}, {"text": "In the present work, we continue exploring the contribution and usefulness of more linguistically motivated features in the context of the NLI Shared Task ( , where our approach is included under the team name \"T\u00fcbingen\".", "labels": [], "entities": []}, {"text": "2 Corpora used T11: TOEFL11 ( ) This is the main corpus of the NLI Shared Task 2013.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.627263605594635}, {"text": "NLI Shared Task 2013", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.7346039265394211}]}, {"text": "It consists of essays written by English learners with 11 native language (L1) backgrounds (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, Turkish), and from three different proficiency levels (low, medium, high).", "labels": [], "entities": []}, {"text": "Each L1 is represented by a set of 1100 essays (train: 900, dev: 100, test: 100).", "labels": [], "entities": []}, {"text": "The labels for the train and dev sets were given from the start, the labels for the test set were provided after the results were submitted.)", "labels": [], "entities": []}, {"text": "The ICLEv2 corpus consists of 6085 essays written by English learners of 16 different L1 backgrounds.", "labels": [], "entities": [{"text": "ICLEv2 corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9015680551528931}]}, {"text": "They are at a similar level of English proficiency, namely higher intermediate to advanced and of about the same age.", "labels": [], "entities": []}, {"text": "For the crosscorpus tasks we used the essays for the seven L1s in the intersection with T11, i.e., Chinese (982 essays), French (311), German (431), Italian (391), Japanese (366), Spanish (248), and Turkish (276).", "labels": [], "entities": [{"text": "T11", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.9131926894187927}]}, {"text": "For the cross-corpus tasks, we used the essays by learners of the eight L1s in the intersection with T11, i.e., Chinese (66 essays), French (145), German (69), Italian (76), Japanese (81),,, and Turkish (73).", "labels": [], "entities": []}], "datasetContent": [{"text": "We developed our approach with a focus on the closed task, training the models on the T11 train set and testing them on the T11 dev set.", "labels": [], "entities": [{"text": "T11 train set", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.9789453744888306}, {"text": "T11 dev set", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.9548709789911906}]}, {"text": "For the closed task, we report the accuracies on the dev set for all models (single feature type models and ensembles as introduced in sections 4.2 and 4.3), before presenting the accuracies on the submitted test set models, which were trained on the T11 train \u222a dev set.", "labels": [], "entities": [{"text": "T11 train \u222a dev set", "start_pos": 251, "end_pos": 270, "type": "DATASET", "confidence": 0.9583259105682373}]}, {"text": "In addition, for the submitted models we report the accuracies obtained via 10-fold crossvalidation on the T11 train \u222a dev set using the folds specification provided by the organizers of the NLI Shared Task 2013.", "labels": [], "entities": [{"text": "T11 train \u222a dev set", "start_pos": 107, "end_pos": 126, "type": "DATASET", "confidence": 0.9283058166503906}, {"text": "NLI Shared Task 2013", "start_pos": 191, "end_pos": 211, "type": "DATASET", "confidence": 0.7730620801448822}]}, {"text": "The results for the open-1 task are obtained by training the models on the NT11 set, and the results for the open-2 task are obtained by training the models on the T11 train \u222a dev set \u222a NT11 set.", "labels": [], "entities": [{"text": "NT11 set", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9221839308738708}]}, {"text": "For the open-1 and open-2 tasks, we report the basic single feature type results on the T11 dev set and two sets of results on the T11 test set: the results for the actual submitted systems and the results for the complete systems, i.e., including the features used in the closed task submissions that for the open tasks were only computed after the submission deadline (given our focus on the closed task and finite computational infrastructure).", "labels": [], "entities": [{"text": "T11 dev set", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9239669243494669}, {"text": "T11 test set", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.9501465757687887}]}, {"text": "We include the figures for the complete systems to allow a proper comparison of the performance of our models across the tasks.", "labels": [], "entities": []}, {"text": "Below we provide a description of the various accuracies (%) we report for the different tasks: \u2022 Acc test : Accuracy on the T11 test set after training the model on: \u2022 Acc dev : Accuracy on the T11 dev set after training the model on: \u2022 Acc 10 train\u222adev : Accuracy on the T11 train \u222a dev set obtained via 10-fold cross-validation using the data split information provided by the organizers, applicable only for the closed task.", "labels": [], "entities": [{"text": "Acc test", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9768048524856567}, {"text": "Accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9787265062332153}, {"text": "T11 test set", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.8934855461120605}, {"text": "Acc", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9781785607337952}, {"text": "Accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9802501201629639}, {"text": "Acc", "start_pos": 238, "end_pos": 241, "type": "METRIC", "confidence": 0.961086630821228}, {"text": "Accuracy", "start_pos": 257, "end_pos": 265, "type": "METRIC", "confidence": 0.9835289716720581}]}, {"text": "In terms of the tools used for classification, we employed LIBLINEAR () using L2-regularized logistic regression, LIBSVM (Chang and Lin, 2011) using C-SVC with the RBF kernel and WEKA SMO) fitting logistic models to SVM outputs (the -M option).", "labels": [], "entities": [{"text": "LIBLINEAR", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9659548401832581}, {"text": "RBF kernel", "start_pos": 164, "end_pos": 174, "type": "DATASET", "confidence": 0.8916915953159332}, {"text": "WEKA SMO", "start_pos": 179, "end_pos": 187, "type": "DATASET", "confidence": 0.7058463990688324}]}, {"text": "Which classifier was used where is discussed below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of essays for the 11 L1s in NT11", "labels": [], "entities": [{"text": "NT11", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8886885046958923}]}, {"text": " Table 2: Single feature type results on T11 dev set", "labels": [], "entities": []}, {"text": " Table 4: Results for the closed task", "labels": [], "entities": []}, {"text": " Table 5. We  report two different Acc test values: the accuracy for  the actual submitted systems (Acc test ) and for the  corresponding complete systems (Acc test with  * ) as  discussed in section 4.1.", "labels": [], "entities": [{"text": "Acc", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9069505929946899}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9994389414787292}, {"text": "Acc test )", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9446897705396017}, {"text": "Acc test with  * )", "start_pos": 156, "end_pos": 174, "type": "METRIC", "confidence": 0.8582059025764466}]}, {"text": " Table 5: Results for the open-1 task", "labels": [], "entities": []}, {"text": " Table 6: Results for the open-2 task", "labels": [], "entities": []}]}