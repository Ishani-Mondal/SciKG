{"title": [{"text": "Extracting Biomedical Events and Modifications Using Subgraph Matching with Noisy Training Data", "labels": [], "entities": [{"text": "Extracting Biomedical Events and Modifications", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8515928268432618}, {"text": "Subgraph Matching", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8412909507751465}]}], "abstractContent": [{"text": "The Genia Event (GE) extraction task of the BioNLP Shared Task addresses the extraction of biomedical events from the natural language text of the published literature.", "labels": [], "entities": [{"text": "Genia Event (GE) extraction", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6448392470677694}]}, {"text": "In our submission, we modified an existing system for learning of event patterns via dependency parse subgraphs to utilise a more accurate parser and significantly more, but noisier, training data.", "labels": [], "entities": []}, {"text": "We explore the impact of these two aspects of the system and conclude that the change in parser limits recall to an extent that cannot be offset by the large quantities of training data.", "labels": [], "entities": [{"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9982860684394836}]}, {"text": "However, our extensions of the system to extract modification events shows promise.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we describe our submission to the Genia Event (GE) information extraction subtask of the BioNLP Shared Task.", "labels": [], "entities": [{"text": "Genia Event (GE) information extraction", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.5895371990544456}]}, {"text": "This task requires the development of systems that are capable of identifying bio-molecular events as those events are expressed in full-text publications.", "labels": [], "entities": []}, {"text": "The task represents an important contribution to the broader problem of converting unstructured information captured in the biomedical literature into structured information that can be used to index and analyse bio-molecular relationships.", "labels": [], "entities": []}, {"text": "This year's task builds on previous instantiations of this task (, with only minor changes in the task definition introduced for 2011.", "labels": [], "entities": []}, {"text": "The task organisers provided full text publications annotated with mentions of biological entities including proteins and genes, and asked participants to provide annotations of simple events including gene expression, binding, localization, and protein modification, as well as higher-order regulation events (e.g., positive regulation of gene expression).", "labels": [], "entities": []}, {"text": "In our submission, we built on a system originally developed for the) and extended in more recent work ().", "labels": [], "entities": []}, {"text": "This system learns to recognise subgraphs of syntactic dependency parse graphs that express a given bio-molecular event, and matches those subgraphs to new text using an algorithm called Approximate Subgraph Matching.", "labels": [], "entities": [{"text": "recognise subgraphs of syntactic dependency parse graphs", "start_pos": 22, "end_pos": 78, "type": "TASK", "confidence": 0.7117470588002887}, {"text": "Approximate Subgraph Matching", "start_pos": 187, "end_pos": 216, "type": "TASK", "confidence": 0.6253499388694763}]}, {"text": "Due to the method's fundamental dependency on the syntactic dependency parse of the text, in this work we set out to explore the impact of substituting the previously employed dependency parsers with a different parser which has been demonstrated to achieve higher performance than other commonly used parsers for full-text biomedical literature ).", "labels": [], "entities": []}, {"text": "The augmentation of training corpora with external unlabelled data that is automatically processed to generate additional labels has been explored for retraining the same system, in an approach known as self-training.", "labels": [], "entities": []}, {"text": "This approach has been shown to be very effective for improving parsing performance (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9838361144065857}]}, {"text": "Self-training of the TEES system has been previously explored, with somewhat mixed results, but with evidence suggesting it could be useful with an appropriate strategy for selecting training examples.", "labels": [], "entities": []}, {"text": "Here, rather than training our system with its own output over external data, we explore a semi-supervised learning approach in which we train our system with the outputs of a different system (TEES) over external data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Impact of adding extra training data to the  ASM method. top5k,20k,30k: using the top 5,000,  20,000, and 30,000 events. pt1k: using the top  1,000 events per event-type. trx4: following the  training bias of events, with a multiplying factor  of four. For TEES we always use the top 10,000  events. Evaluated over GE13dev.", "labels": [], "entities": [{"text": "ASM", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9727180004119873}, {"text": "TEES", "start_pos": 267, "end_pos": 271, "type": "DATASET", "confidence": 0.5673322677612305}, {"text": "GE13dev", "start_pos": 325, "end_pos": 332, "type": "DATASET", "confidence": 0.971938967704773}]}, {"text": " Table 3: Adding GE11 data to the training and op- timisation steps. Evaluated over GE13dev.", "labels": [], "entities": [{"text": "GE11 data", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.8184373378753662}, {"text": "GE13dev", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.956474781036377}]}, {"text": " Table 4: Performance depending on the applied  parsing pipeline (clearnlp for this work against  the CJM pipeline of", "labels": [], "entities": [{"text": "clearnlp", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9111994504928589}]}, {"text": " Table 5: Test set results, always optimised over  gold data only. * denotes the official submission.", "labels": [], "entities": []}, {"text": " Table 6: Results for SPECULATION and NEGATION using automatically-annotated events (showing the  F-score of the configuration), as well as using oracle event annotations from the gold standard, over our  development set and the official test set. Rules are learned from GE13+GE11 gold data (excluding any  test data). Cues for learning rules are either the basic manually-specified set (34 SPEC/21 NEG) or the  augmented set with data-driven additions (47 SPEC/26 NEG). * denotes the official submission.", "labels": [], "entities": [{"text": "F-score", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9854721426963806}, {"text": "GE13+GE11 gold data", "start_pos": 271, "end_pos": 290, "type": "DATASET", "confidence": 0.843930697441101}]}]}