{"title": [{"text": "Answer Extraction by Recursive Parse Tree Descent", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8987087905406952}, {"text": "Parse Tree Descent", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.5471311310927073}]}], "abstractContent": [{"text": "We develop a recursive neural network (RNN) to extract answers to arbitrary natural language questions from supporting sentences, by training on a crowdsourced data set (to be released upon presentation).", "labels": [], "entities": []}, {"text": "The RNN defines feature representations at every node of the parse trees of questions and supporting sentences, when applied recursively, starting with token vectors from a neural probabilistic language model.", "labels": [], "entities": []}, {"text": "In contrast to prior work, we fix neither the types of the questions nor the forms of the answers; the system classifies tokens to match a sub-string chosen by the question's author.", "labels": [], "entities": []}, {"text": "Our classifier decides to follow each parse tree node of a support sentence or not, by classifying its RNN embedding together with those of its siblings and the root node of the question, until reaching the tokens it selects as the answer.", "labels": [], "entities": []}, {"text": "A novel co-training task for the RNN, on subtree recognition, boosts performance, along with a scheme to consistently handle words that are not well-represented in the language model.", "labels": [], "entities": [{"text": "subtree recognition", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.759727954864502}]}, {"text": "On our data set, we surpass an open source system epitomizing a classic \"pat-tern bootstrapping\" approach to question answering .", "labels": [], "entities": [{"text": "question answering", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.9086539447307587}]}], "introductionContent": [{"text": "The goal of this paper is to learn the syntax used to answer arbitrary natural language questions.", "labels": [], "entities": []}, {"text": "If the kinds of questions were fixed but the supporting sentences were open, this would be a kind of relation extraction or slot-filling.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8319316506385803}]}, {"text": "If the questions were open but the supporting information was encoded in a database, this would be a kind of semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.7166907489299774}]}, {"text": "In spite of many evaluation sets, no suitable data set for learning to answer questions has existed before.", "labels": [], "entities": []}, {"text": "Data sets such as TREC () do not identify supporting sentences or even answers unless a competing system submitted an answer and a human verified it.", "labels": [], "entities": [{"text": "TREC", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8681638240814209}]}, {"text": "Exceeding the capabilities of current systems is difficult by training on such labels; any newly discovered answer is penalized as wrong.", "labels": [], "entities": []}, {"text": "The Jeopardy Archive) offers more than 200,000 answer/question pairs, but no pointers to information that supports the solutions.", "labels": [], "entities": [{"text": "Jeopardy Archive)", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8965394099553426}]}, {"text": "Believing that it is impossible to learn to answer questions, QA systems in TREC tended to measure syntactic similarity between question and candidate answer, or to map the question into an enumerated set of possible question types.", "labels": [], "entities": []}, {"text": "For the pre-determined question types, learning could be achieved, not from the QA data itself, but from pattern bootstrapping or distant supervision against an ontology like Freebase ().", "labels": [], "entities": [{"text": "QA data", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.7545402646064758}]}, {"text": "These techniques lose precision; found the distant supervision assumption was violated on 31% of examples aligning Freebase relations to text from The New York Times.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9990630745887756}, {"text": "Freebase relations to text from The New York Times", "start_pos": 115, "end_pos": 165, "type": "DATASET", "confidence": 0.734847860203849}]}, {"text": "We introduce anew, crowdsourced dataset, TurkQA, to enable question answering to be learned.", "labels": [], "entities": [{"text": "TurkQA", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.8819763660430908}, {"text": "question answering", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8511559963226318}]}, {"text": "TurkQA consists of single sentences, each with several crowdsourced questions.", "labels": [], "entities": [{"text": "TurkQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8641861081123352}]}, {"text": "The answer to each question is given as a substring of the supporting sentence.", "labels": [], "entities": []}, {"text": "For example, James Hervey (February 26, 1714 -December 25, 1758), English divine, was born at Hardingstone, near Northampton, and was educated at the grammar school of Northampton, and at Lincoln College, Oxford.", "labels": [], "entities": [{"text": "James Hervey (February 26, 1714 -December 25, 1758)", "start_pos": 13, "end_pos": 64, "type": "TASK", "confidence": 0.8506532082190881}]}, {"text": "could have questions like \"Where did James Hervey attend school as a boy?\" with answers like \"the grammar school of Northampton.\"", "labels": [], "entities": [{"text": "grammar school of Northampton", "start_pos": 98, "end_pos": 127, "type": "DATASET", "confidence": 0.6193773448467255}]}, {"text": "Our approach has yielded almost 40,000 such questions, and easily scales to many more.", "labels": [], "entities": []}, {"text": "Since the sentence containing the answer has already been located, the machine's output can be judged without worrying about missing labels elsewhere in the corpus.", "labels": [], "entities": []}, {"text": "Token-level ground truth forces the classifier to isolate the relevant information.", "labels": [], "entities": []}, {"text": "To meet this challenge, we develop a classifier that recursively classifies nodes of the parse tree of a supporting sentence.", "labels": [], "entities": []}, {"text": "The positively classified nodes are followed down the tree, and any positively classified terminal nodes become the tokens in the answer.", "labels": [], "entities": []}, {"text": "Feature representations are dense vectors in a continuous feature space; for the terminal nodes, they are the word vectors in a neural probabilistic language model (like), and for interior nodes, they are derived from children by recursive application of an autoencoder.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: a data set for learning to answer free-form questions; a top-down supervised method using continuous word features in parse trees to find the answer; and a co-training task for training a recursive neural network that preserves deep structural information.", "labels": [], "entities": []}], "datasetContent": [{"text": "From the TurkQA data, we disregard the yes/no questions, and obtain 12,916 problem sets with 38,083 short answer questions for training, and 508 problem sets with 1,488 short answer questions for testing.", "labels": [], "entities": [{"text": "TurkQA data", "start_pos": 9, "end_pos": 20, "type": "DATASET", "confidence": 0.9264596998691559}]}, {"text": "Because there maybe several ways of stating a short answer, short answer questions in other data sets are typically judged by humans.", "labels": [], "entities": []}, {"text": "In TurkQA, because answers must be extracted as substrings, we can approximate the machine's correctness by considering the token classification error against the substring originally chosen by the Turk worker.", "labels": [], "entities": []}, {"text": "Of course, answer validation strategies could be used to cleanup the short answers-for instance, require that they are contiguous substrings (as is guaranteed by the task)-but we did not employ them here, so as not to obfuscate the performance of the substring extraction system itself.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7896715998649597}]}, {"text": "Inevitably, some token misclassification will occur because question writers choose more or less complete answers (\"in Nigeria\" or just \"Nigeria\").", "labels": [], "entities": []}, {"text": "shows the performance of our main algorithm, evaluated both as short-answer and as multiple choice.", "labels": [], "entities": []}, {"text": "The short answer results describe the main setting, which formulates answer extraction as token classification.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7390324175357819}, {"text": "token classification", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7192213386297226}]}, {"text": "The multiple choice results come from considering all the short answers in the problem sets as alternative choices, and comparing the classifier's outputs averaged over the words in each response to select the best, or skip if no average is positive.", "labels": [], "entities": []}, {"text": "(Thus, all questions in a single problem set have the same set of choices.)", "labels": [], "entities": []}, {"text": "Although the multiple choice setting is less challenging, it helps us see how much of the short answer error maybe due to finding poor answer boundaries as opposed to the classifier being totally misled.", "labels": [], "entities": []}, {"text": "On more than half of the 1,488 test questions, no answer at all is selected, so that multiple choice precision remains high even with low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9753667712211609}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9974965453147888}]}, {"text": "As one baseline method, we took the OpenEphyra question answering system, an open source project led by Carnegie Mellon University, which evolved out of submissions to TREC question answering contests (, bypassing its retrieval module to simply use our support sentence.", "labels": [], "entities": [{"text": "OpenEphyra question answering", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.796461284160614}, {"text": "TREC question answering contests", "start_pos": 168, "end_pos": 200, "type": "TASK", "confidence": 0.7231503799557686}]}, {"text": "In contrast to our system, OpenEphyra's question analysis module is trained to map questions to one of a fixed number of answer types, such as PERCENTAGE, or PROPER_NAME.PERSON.FIRST_NAME, and utilizes a large database of answer patterns for these types.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.770110011100769}]}, {"text": "In spite of OpenEphyra's laborious pattern coding, our system performs 17% better on a multiple choice basis, and 77% better on short answers, the latter likely because OpenEphyra's answer types cover shorter strings than the Turks' answers.", "labels": [], "entities": []}, {"text": "The results show the impact of several of our algorithmic contributions.", "labels": [], "entities": []}, {"text": "If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).", "labels": [], "entities": [{"text": "subtree recognition", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7735810577869415}, {"text": "F1 score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9856268167495728}, {"text": "token classification", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.8779003918170929}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9927207827568054}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9881189465522766}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9639352560043335}, {"text": "recall", "start_pos": 206, "end_pos": 212, "type": "METRIC", "confidence": 0.9894947409629822}]}, {"text": "Without extended embeddings to differentiate unknown words, F1 is only .305 (66.8% precision, 19.7% recall).", "labels": [], "entities": [{"text": "F1", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9994359612464905}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9968361854553223}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9990818500518799}]}, {"text": "We are encouraged that increasing the amount of data contributes 50% to the F1 score (from only F1=.236 training on 1,333 questions), as it suggests that the power of our algorithms is not saturated while picking up the simplest features.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9910363554954529}, {"text": "F1", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9982341527938843}]}, {"text": "gives examples of questions in the test set, together with the classifier's selection from the support sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on TurkQA test set, for short answer and multiple choice (MC) evaluations.", "labels": [], "entities": [{"text": "TurkQA test set", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.928327480951945}]}]}