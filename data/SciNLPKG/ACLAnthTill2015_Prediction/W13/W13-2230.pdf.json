{"title": [{"text": "Munich-Edinburgh-Stuttgart Submissions at WMT13: Morphological and Syntactic Processing for SMT", "labels": [], "entities": [{"text": "WMT13", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.7333489656448364}, {"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.6925917267799377}]}], "abstractContent": [{"text": "We present 5 systems of the Munich-Edinburgh-Stuttgart 1 joint submissions to the 2013 SMT Shared Task: FR-EN, EN-FR, RU-EN, DE-EN and EN-DE.", "labels": [], "entities": [{"text": "SMT Shared Task", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7165156404177347}, {"text": "FR-EN", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.8621727228164673}]}, {"text": "The first three systems employ inflectional generalization , while the latter two employ parser-based reordering, and DE-EN performs compound splitting.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7156826555728912}]}, {"text": "For our experiments , we use standard phrase-based Moses systems and operation sequence models (OSM).", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphologically complex languages often lead to data sparsity problems in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7379618883132935}]}, {"text": "For translation pairs with morphologically rich source languages and English as target language, we focus on simplifying the input language in order to reduce the complexity of the translation model.", "labels": [], "entities": []}, {"text": "The pre-processing of the source-language is language-specific, requiring morphological analysis (FR, RU) as well as sentence reordering and dealing with compounds (DE).", "labels": [], "entities": [{"text": "FR", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9580799341201782}, {"text": "dealing with compounds (DE)", "start_pos": 141, "end_pos": 168, "type": "TASK", "confidence": 0.634199246764183}]}, {"text": "Due to time constraints we did not deal with inflection for DE-EN and EN-DE.", "labels": [], "entities": []}, {"text": "The morphological simplification process consists in lemmatizing inflected word forms and dealing with word formation (splitting portmanteau prepositions or compounds).", "labels": [], "entities": [{"text": "morphological simplification", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.776190459728241}, {"text": "word formation", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.7471996247768402}]}, {"text": "This needs to take into account translation-relevant features (e.g. number) which vary across the different language pairs: while French only has the features number and gender, a wider array of features needs to be considered when modelling Russian (cf. table 6).", "labels": [], "entities": []}, {"text": "In addition to morphological reduction, we also apply transliteration models learned from automatically mined transliterations to handle out-of-vocabulary words (OOVs) when translating from Russian.", "labels": [], "entities": [{"text": "morphological reduction", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7410436272621155}]}, {"text": "Replacing inflected word forms with simpler variants (lemmas or the components of split compounds) aims not only at reducing the general complexity of the translation model, but also at decreasing the amount of out-of-vocabulary words in the input data.", "labels": [], "entities": []}, {"text": "This is particularly the case with German compounds, which are very productive and thus often lack coverage in the parallel training data, whereas the individual components can be translated.", "labels": [], "entities": []}, {"text": "Similarly, inflected word forms (e.g. adjectives) benefit from the reduction to lemmas if the full inflection paradigm does not occur in the parallel training data.", "labels": [], "entities": []}, {"text": "For EN-FR, a translation pair with a morphologically complex target language, we describe a two-step translation system built on non-inflected word stems with a post-processing component for predicting morphological features and the generation of inflected forms.", "labels": [], "entities": []}, {"text": "In addition to the advantage of a more general translation model, this method also allows the generation of inflected word forms which do not occur in the training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "The translation experiments in this paper are carried outwith either a standard phrase-based Moses system (DE-EN, EN-DE, EN-FR and FR-EN) or with an operation sequence model (RU-EN, DE-EN), cf. for more details.", "labels": [], "entities": [{"text": "FR-EN", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.9297087788581848}]}, {"text": "An operation sequence model (OSM) is a stateof-the-art SMT-system that learns translation and reordering patterns by representing a sentence pair and its word alignment as a unique sequence of operations (see e.g., for more details).", "labels": [], "entities": [{"text": "SMT-system", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.9096453785896301}, {"text": "translation and reordering patterns", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.792228639125824}]}, {"text": "For the Moses systems we used the old train-model perl scripts rather than the EMS, so we did not perform Good-Turing smoothing; parameter tuning was carried outwith batch-mira (Cherry and Foster, 2012).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of the French to English system  (WMT-2012). The marked system (*) corresponds  to the system submitted for manual evaluation. (cs:  case-sensitive, ci: case-insensitive)", "labels": [], "entities": [{"text": "French to English system  (WMT-2012)", "start_pos": 25, "end_pos": 61, "type": "DATASET", "confidence": 0.6371212729385921}]}, {"text": " Table 5: Results for French inflection prediction  on the WMT-2012 test set. The marked system (*)  corresponds to the system submitted for manual  evaluation.", "labels": [], "entities": [{"text": "French inflection prediction", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6195659438769022}, {"text": "WMT-2012 test set", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9721431136131287}]}, {"text": " Table 7: Russian to English machine translation  system evaluated on WMT-2012 and WMT-2013.  Human evaluation in WMT13 is performed on the  system trained using the original corpus with TA- GIZA++ for alignment (marked with *).", "labels": [], "entities": [{"text": "Russian to English machine translation", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.5453987538814544}, {"text": "WMT-2012", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9781508445739746}, {"text": "WMT-2013", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.9252251982688904}, {"text": "WMT13", "start_pos": 114, "end_pos": 119, "type": "DATASET", "confidence": 0.9546994566917419}, {"text": "TA- GIZA++", "start_pos": 187, "end_pos": 197, "type": "METRIC", "confidence": 0.9262471497058868}]}, {"text": " Table 8: Results on WMT-2013 (blindtest)", "labels": [], "entities": [{"text": "WMT-2013", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8739346861839294}, {"text": "blindtest", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.957912027835846}]}]}