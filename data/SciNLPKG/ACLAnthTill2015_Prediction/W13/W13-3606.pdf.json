{"title": [{"text": "A Tree Transducer Model for Grammatical Error Correction", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7464014093081156}]}], "abstractContent": [{"text": "We present an approach to grammatical error correction for the CoNLL 2013 shared task based on a weighted tree-to-string transducer.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.5866906940937042}, {"text": "CoNLL 2013 shared task", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.8910159021615982}]}, {"text": "Rules for the transducer are extracted from the NUCLE training data.", "labels": [], "entities": [{"text": "NUCLE training data", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.9604716102282206}]}, {"text": "An n-gram language model is used to rerank k-best sentence lists generated by the transducer.", "labels": [], "entities": []}, {"text": "Our system obtains a precision , recall and F1 score of 0.27, 0.1333 and 0.1785, respectively, on the official test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9998631477355957}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9995847344398499}, {"text": "F1 score", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9867809414863586}]}, {"text": "On the revised annotations, the F1 score increases to 0.2505.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9795226156711578}]}, {"text": "Our system ranked 6th out of the participating teams on both the original and revised test set annotations .", "labels": [], "entities": []}], "introductionContent": [{"text": "There has recently been an increase in research on automated grammatical error detection and correction for writing by English language learners ().", "labels": [], "entities": [{"text": "automated grammatical error detection and correction", "start_pos": 51, "end_pos": 103, "type": "TASK", "confidence": 0.6438674827416738}]}, {"text": "In the most prominent line of research, statistical classifiers are trained to detector correct specific error types.", "labels": [], "entities": []}, {"text": "Features for these classifiers are based on word context and local syntactic information.", "labels": [], "entities": []}, {"text": "The classifiers are combined, and a language model is often used to filter corrections.", "labels": [], "entities": []}, {"text": "Research on this approach focusses especially on preposition and determiner errors.", "labels": [], "entities": []}, {"text": "Most of the systems in the HOO 2011 and 2012 shared tasks () fall under this broad approach.", "labels": [], "entities": [{"text": "HOO 2011 and 2012 shared tasks", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.806664913892746}]}, {"text": "Ina second class of models, a model for generating corrected sentences is formulated in the noisy-channel framework, relying strongly on a language model to distinguish between grammatical and ungrammatical candidate corrections ().", "labels": [], "entities": []}, {"text": "Such models are often inspired by techniques developed for statistical machine translation).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6682377854983012}]}, {"text": "Finally, rule-based methods are often used in commercial language processing systems such as word processors.", "labels": [], "entities": []}, {"text": "Here large hand-crafted linguistically expressive, error-tolerant grammars are used to analyse sentences and identify where constraints have been broken.", "labels": [], "entities": []}, {"text": "In this paper we present our system for the CoNLL 2013 shared task in grammatical error correction ( . Our grammar correction model is based on a tree-to-string transducer that is specified by a set of rules that each rewrite a tree fragment to a string of words and variables.", "labels": [], "entities": [{"text": "CoNLL 2013 shared task", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.8111374825239182}, {"text": "grammatical error correction", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.5870190262794495}, {"text": "grammar correction", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7297126650810242}]}, {"text": "These rules are extracted automatically from a set of training examples.", "labels": [], "entities": []}, {"text": "Each training example consists of an incorrect sentence, a corresponding correct sentence with its parse tree, and a word alignment between the incorrect and correct sentences.", "labels": [], "entities": []}, {"text": "During decoding the model searches for parsed well-formed sentences that could be transformed into a given incorrect sentence with high probability.", "labels": [], "entities": []}, {"text": "Sentences are split into linguistically plausible clauses to decrease the average sentence length, in order to improve decoding runtime.", "labels": [], "entities": []}, {"text": "In order to discriminate more accurately between candidate sentence corrections an n-gram language model trained on a large corpus of well-formed text is used to rerank the k-best hypotheses that the transducer model generates.", "labels": [], "entities": []}, {"text": "The tree transducer and language model scores are weighted to maximize the model F1 score on a validation set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.972822517156601}]}, {"text": "After decoding the clauses are recombined into the original sentence structure.", "labels": [], "entities": []}, {"text": "The next section describes preprocessing and the resources used by our system.", "labels": [], "entities": []}, {"text": "Section 3 defines weighted tree-to-string transducers.", "labels": [], "entities": []}, {"text": "We present the formulation of our error correction model in section 4, and discuss decoding with it in section 5.", "labels": [], "entities": [{"text": "error correction", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7355786859989166}]}, {"text": "Section 6 describes language model reranking.", "labels": [], "entities": [{"text": "language model reranking", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6261983613173167}]}, {"text": "System results are given in section 7.", "labels": [], "entities": []}, {"text": "Finally, section 8 draws some conclusions and discuss future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The standard evaluation metric used for grammatical error correction is precision, recall and F1 score.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.5646234353383383}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9997562766075134}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9992929697036743}, {"text": "F1 score", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9859893321990967}]}, {"text": "Changes made to a given incorrect sentence are represented by edits.", "labels": [], "entities": []}, {"text": "For a sample sentence, the sufficient statistics for this evaluation metric is the 3-tuple (#correct system edits, #system edits, #gold standard edits).", "labels": [], "entities": []}, {"text": "This can be summed overall the examples being evaluated, and the precision, recall and F1 scores can be computed from that.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9997707009315491}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9986457228660583}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9993890523910522}]}, {"text": "The shared task uses the M 2 scorer, as described by.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.8829302986462911}]}, {"text": "Given the original and system sentences, possible system edit sequences are represented with a lattice.", "labels": [], "entities": []}, {"text": "The edit sequence that is the best match with the gold standard edit sequence is chosen to compute the edit scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Recall for each error type, on the devel- opment set and original test set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9848731756210327}]}]}