{"title": [{"text": "Entailment: An Effective Metric for Comparing and Evaluating Hierarchical and Non-hierarchical Annotation Schemes", "labels": [], "entities": []}], "abstractContent": [{"text": "Hierarchical or nested annotation of linguistic data often co-exists with simpler non-hierarchical or flat counterparts, a classic example being that of annotations used for parsing and chunking.", "labels": [], "entities": [{"text": "parsing and chunking", "start_pos": 174, "end_pos": 194, "type": "TASK", "confidence": 0.6427576740582784}]}, {"text": "In this work, we propose a general strategy for comparing across these two schemes of annotation using the concept of entailment that formalizes a correspondence between them.", "labels": [], "entities": []}, {"text": "We use crowdsourcing to obtain query and sentence chunking and show that entailment cannot only be used as an effective evaluation metric to assess the quality of annotations, but it can also be employed to filter out noisy annotations.", "labels": [], "entities": [{"text": "query and sentence chunking", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6532339453697205}]}], "introductionContent": [{"text": "Linguistic annotations at all levels of linguistic organization -phonological, morpho-syntactic, semantic, discourse and pragmatic, are often hierarchical or nested in nature.", "labels": [], "entities": []}, {"text": "For instance, syntactic dependencies are annotated as phrase structure or dependency trees).", "labels": [], "entities": []}, {"text": "Nevertheless, the inherent cognitive load associated with nested segmentation and the sufficiency of simpler annotation schemes for building NLP applications have often lead researchers to define non-hierarchical or flat annotation schemes.", "labels": [], "entities": []}, {"text": "The flat annotation, in essence, is a \"flattened\" version of the tree.", "labels": [], "entities": []}, {"text": "For instance, chunking of Natural Language (NL) text, which is often considered an essential preprocessing step for many NLP applications, is, loosely speaking, a flattened version of the phrase structure tree.", "labels": [], "entities": [{"text": "chunking of Natural Language (NL) text", "start_pos": 14, "end_pos": 52, "type": "TASK", "confidence": 0.8786291182041168}]}, {"text": "The closely related task of Query Segmentation is of special interest to us here, as it is * The work was done during author's internship at Microsoft Research Lab India.", "labels": [], "entities": [{"text": "Query Segmentation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8811008036136627}, {"text": "Microsoft Research Lab India", "start_pos": 141, "end_pos": 169, "type": "DATASET", "confidence": 0.8541887402534485}]}], "datasetContent": [{"text": "We obtained nested and flat segmentation of Web search queries through crowdsourcing as well as from trained experts.", "labels": [], "entities": []}, {"text": "Furthermore, we also conducted similar crowdsourcing experiments for NL sentences, which helped us understand the specific challenges in annotating queries because of their apparent lack of a well-defined syntactic structure.", "labels": [], "entities": []}, {"text": "In this section, we first describe the experimental setup and datasets, and then present the observations and results.", "labels": [], "entities": []}, {"text": "In this study we use the same set of crowdsourced annotations as described in).", "labels": [], "entities": []}, {"text": "For the sake of completeness, we briefly describe the annotation procedure here as well.", "labels": [], "entities": []}, {"text": "We used Amazon Mechanical Turk for the crowdsourcing experiments.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 8, "end_pos": 30, "type": "DATASET", "confidence": 0.9264388879140218}]}, {"text": "Two separate Human Intelligence Tasks were designed for flat and nested segmentation.", "labels": [], "entities": []}, {"text": "The concept of flat and nested segmentation was introduced to the Turkers with the help of two short videos 4 . When in doubt regarding the meaning of a query, the Turkers were advised to issue the query on a search engine of their choice and find out its possible interpretation(s).", "labels": [], "entities": []}, {"text": "Only Turkers who had completed more than 100 tasks at an acceptance rate of \u2265 60% were allowed to participate in the task and were paid $0.02 fora flat and $0.06 fora nested segmentation.", "labels": [], "entities": []}, {"text": "Every query was annotated by 10 different annotators.", "labels": [], "entities": []}, {"text": "The following sets of queries and sentences were used for annotations: Q500, QG500: Saha Roy et al. released a dataset of 500 queries, 5 to 8 words long, for the evaluation of various segmentation algorithms.", "labels": [], "entities": [{"text": "Q500", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.8865475654602051}]}, {"text": "This dataset has flat segmentations from three annotators obtained under controlled experimental settings, and could be considered as Gold annotation.", "labels": [], "entities": []}, {"text": "Hence, we selected this set for our experiments as well.", "labels": [], "entities": []}, {"text": "We procured the corresponding nested segmentation for these queries from two human experts who are regular search engine users.", "labels": [], "entities": []}, {"text": "They annotated the data under supervision and were trained and paid for the task.", "labels": [], "entities": []}, {"text": "We shall refer to the set of flat and nested gold annotations as QG500, whereas Q500 will be reserved for the dataset procured through the AMT experiments.", "labels": [], "entities": [{"text": "QG500", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8096514344215393}, {"text": "AMT experiments", "start_pos": 139, "end_pos": 154, "type": "DATASET", "confidence": 0.7603127658367157}]}, {"text": "Q700: As 500 queries are not enough for making reliable conclusions and also, since the queries may not have been chosen specifically for the purpose of annotation experiments, we expanded the set with another 700 queries sampled from the logs of a popular commercial search engine.", "labels": [], "entities": [{"text": "Q700", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9353196620941162}]}, {"text": "We picked, uniformly at random, queries that were 4 to 8 words long.", "labels": [], "entities": []}, {"text": "S300: We randomly selected 300 English sentences from a collection of full texts of public domain books 5 that were 5 to 15 words long, and manually checked them for well-formedness.", "labels": [], "entities": [{"text": "S300", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6726263761520386}]}, {"text": "reports two statistics -the values of Kripendorff's \u03b1 and the average observed entailment (expressed as %) for flat and nested segmentations along with the corresponding expected values for entailment by chance.", "labels": [], "entities": []}, {"text": "For nested segmentation, the \u03b1 values were computed for two different distance metrics 6 d 1 and d 2 .", "labels": [], "entities": [{"text": "nested segmentation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6094304919242859}]}], "tableCaptions": [{"text": " Table 3: \u03b1 and Average Entailment Statistics", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9979519248008728}, {"text": "Average", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9872352480888367}]}]}