{"title": [{"text": "Ranking the annotators: An agreement study on argumentation structure", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate methods for evaluating agreement among a relatively large group of annotators who have not received extensive training and differ in terms of ability and motivation.", "labels": [], "entities": []}, {"text": "We show that it is possible to isolate a reliable subgroup of anno-tators, so that aspects of the difficulty of the underlying task can be studied.", "labels": [], "entities": []}, {"text": "Our task is to annotate the argumentative structure of short texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Scenarios for evaluating annotation experiments differ in terms of the difficulty of the task, the number of annotators, and the amount of training that annotators receive.", "labels": [], "entities": []}, {"text": "For simple tasks, crowdsourcing involving very many annotators has recently attracted attention.", "labels": [], "entities": []}, {"text": "For more difficult tasks, the standard setting still is to work with two or a few more annotators, train them well, and compute agreement, usually in terms of the kappa measure.", "labels": [], "entities": []}, {"text": "In this paper, we study a different scenario, which maybe called 'classroom annotation': The group of annotators is bigger (in our example, 26), and there are no extensive training sessions: Students receive detailed written guidelines, there is a brief QA period, and annotation starts.", "labels": [], "entities": []}, {"text": "In such a setting, one has to expect some agreement problems that are due to different abilities and different motivation of the students.", "labels": [], "entities": []}, {"text": "Our goal is to develop methods for systematically studying the annotation results in such groups, to identify more or less competent subgroups, yet at the same time also learn about the difficulty of various aspects of the underlying annotation task.", "labels": [], "entities": []}, {"text": "To this end, we investigate ways of ranking and clustering annotators.", "labels": [], "entities": []}, {"text": "Our task is the annotation of argumentation in short texts, which is somewhat similar to marking the rhetorical structure, e.g. in terms of RST ().", "labels": [], "entities": [{"text": "annotation of argumentation in short texts", "start_pos": 16, "end_pos": 58, "type": "TASK", "confidence": 0.659207671880722}]}, {"text": "Thus we are dealing with a relatively difficult task involving text interpretation.", "labels": [], "entities": [{"text": "text interpretation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8029919564723969}]}, {"text": "We devised an annotation scheme (which is more fully described elsewhere), and in order to study the feasibility, first ran experiments with short hand-crafted texts that collectively coverall the relevant phenomena.", "labels": [], "entities": []}, {"text": "This is the setting we report in this paper.", "labels": [], "entities": []}, {"text": "A separate step for future work is guideline revision on the basis of the results, and then applying the scheme to authentic argumentative text (e.g., user generated content on various websites).", "labels": [], "entities": [{"text": "guideline revision", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7094819247722626}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Agreement for all 26 annotators on 115 items for the different levels. The number of categories  on each level (without '?') is shown in the second column (possible target categories depend on text  length). We report Fleiss's \u03ba with the associated observed (A O ) and expected agreement (A E ). Weighted  scores were calculated using Krippendorff's \u03b1, with observed (D O ) and expected disagreement (D E ).", "labels": [], "entities": [{"text": "expected agreement (A E )", "start_pos": 279, "end_pos": 304, "type": "METRIC", "confidence": 0.8329964280128479}, {"text": "observed (D O ) and expected disagreement (D E )", "start_pos": 368, "end_pos": 416, "type": "METRIC", "confidence": 0.7640915686885515}]}, {"text": " Table 3: Krippendorff's category definition diag- nostic for the level 'role+type', base \u03ba=0.45.", "labels": [], "entities": []}, {"text": " Table 2: Confusion probability matrix over all 26 annotators for the level 'role+type'.", "labels": [], "entities": []}, {"text": " Table 4: Krippendorff's category distinction diag- nostic for the level 'role+type', base \u03ba=0.45.", "labels": [], "entities": []}]}