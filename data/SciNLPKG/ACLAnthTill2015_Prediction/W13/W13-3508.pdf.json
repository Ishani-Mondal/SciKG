{"title": [{"text": "Sentence Compression with Joint Structural Inference", "labels": [], "entities": []}], "abstractContent": [{"text": "Sentence compression techniques often assemble output sentences using fragments of lexical sequences such as n-grams or units of syntactic structure such as edges from a dependency tree representation.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9191909730434418}]}, {"text": "We present a novel approach for discriminative sentence compression that unifies these notions and jointly produces sequential and syntactic representations for output text, leveraging a compact integer linear programming formulation to maintain structural integrity.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7175350934267044}]}, {"text": "Our supervised models permit rich features over heterogeneous linguistic structures and generalize over previous state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "Experiments on corpora featuring human-generated compressions demonstrate a 13-15% relative gain in 4-gram accuracy over a well-studied language model-based compression system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9660924673080444}]}], "introductionContent": [{"text": "Recent years have seen increasing interest in textto-text generation tasks such as paraphrasing and text simplification, due in large part to their direct utility in high-level natural language tasks such as abstractive summarization.", "labels": [], "entities": [{"text": "textto-text generation tasks", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.817173977692922}, {"text": "text simplification", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7493345439434052}, {"text": "abstractive summarization", "start_pos": 208, "end_pos": 233, "type": "TASK", "confidence": 0.599943220615387}]}, {"text": "The task of sentence compression in particular has benefited from the availability of a number of useful resources such as the the Ziff-Davis compression corpus) and the Edinburgh compression corpus) which make compression problems highly relevant for datadriven approaches involving language generation.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8192151784896851}, {"text": "Edinburgh compression corpus", "start_pos": 170, "end_pos": 198, "type": "DATASET", "confidence": 0.9451214273770651}, {"text": "language generation", "start_pos": 284, "end_pos": 303, "type": "TASK", "confidence": 0.7389197647571564}]}, {"text": "The sentence compression task addresses the problem of minimizing the lexical footprint of a sentence, i.e., the number of words or characters in it, while preserving its most salient information.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7627716362476349}]}, {"text": "This is illustrated in the following example from the compression corpus of Clarke and Lapata (2006b): Original: In 1967 Chapman, who had cultivated a conventional image with his ubiquitous tweed jacket and pipe, by his own later admission stunned a party attended by his friends and future Python colleagues by coming out as a homosexual.", "labels": [], "entities": []}, {"text": "Compressed: In 1967 Chapman, who had cultivated a conventional image, stunned a party by coming out as a homosexual.", "labels": [], "entities": []}, {"text": "Compression can therefore be viewed as analogous to text summarization 1 defined at the sentence level.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7270245850086212}]}, {"text": "Unsurprisingly, independent selection of tokens for an output sentence does not lead to fluent or meaningful compressions; thus, compression systems often assemble output text from units that are larger than single tokens such as n-grams) or edges in a dependency structure).", "labels": [], "entities": []}, {"text": "These systems implicitly rely on a structural representation of text-as a sequence of tokens or as a dependency tree respectively-to to underpin the generation of an output sentence.", "labels": [], "entities": []}, {"text": "In this work, we present structured transduction: a novel supervised framework for sentence compression which employs a joint inference strategy to simultaneously recover sentence compressions under both these structural representations of text-a token sequence as well as a tree of syntactic dependencies.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7391916662454605}]}, {"text": "Sentence generation is treated as a discriminative structured prediction task in which rich linguistically-motivated features can be used to predict the informativeness of specific tokens within the input text as well as the fluency of n-grams and dependency relationships in the output text.", "labels": [], "entities": [{"text": "Sentence generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.942076176404953}]}, {"text": "We present a novel constrained integer linear program that optimally solves the joint inference problem, using the notion of commodity flow to ensure the production of valid acyclic sequences and trees for an output sentence.", "labels": [], "entities": []}, {"text": "The primary contributions of this work are: \u2022 A supervised sequence-based compression model which outperforms state-of-the-art sequence-based compression system without relying on any hard syntactic constraints.", "labels": [], "entities": []}, {"text": "\u2022 A formulation to jointly infer tree structures alongside sequentially-ordered n-grams, thereby permitting features that factor over both phrases and dependency relations.", "labels": [], "entities": []}, {"text": "The structured transduction models offer additional flexibility when compared to existing models that compress via n-gram or dependency factorizations.", "labels": [], "entities": []}, {"text": "For instance, the use of commodity flow constraints to ensure well-formed structure permits arbitrary reorderings of words in the input and is not restricted to producing text in the same order as the input like much previous work inter alia.", "labels": [], "entities": []}, {"text": "We ran compression experiments with the proposed approaches on well-studied corpora from the domains of written news) and broadcast news.", "labels": [], "entities": []}, {"text": "Our supervised approaches show significant gains over the language model-based compression system of under a variety of performance measures, yielding 13-15% relative F 1 improvements for 4-gram retrieval over under identical compression rate conditions.", "labels": [], "entities": [{"text": "F 1", "start_pos": 167, "end_pos": 170, "type": "METRIC", "confidence": 0.947957456111908}]}], "datasetContent": [{"text": "In order to evaluate the performance of the structured transduction framework, we ran compression experiments over the newswire (NW) and broadcast news transcription (BN) corpora collected by.", "labels": [], "entities": []}, {"text": "Sentences in these datasets are accompanied by gold compressions-one per sentence for NW and three for BN-produced by trained human annotators who were restricted to using word deletion, so paraphrasing and word reordering do not play a role.", "labels": [], "entities": [{"text": "BN-produced", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.6695815920829773}, {"text": "word reordering", "start_pos": 207, "end_pos": 222, "type": "TASK", "confidence": 0.7517678439617157}]}, {"text": "For this reason, we chose to evaluate the systems using n-gram precision and recall (among other metrics), following and standard MT evaluations.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9422534108161926}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9988355040550232}, {"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.8844658732414246}]}, {"text": "We filtered the corpora to eliminate instances with less than 2 and more than 110 tokens and used the same training/development/test splits from, yielding 953/63/603 sentences respectively for the NW corpus and 880/78/404 for the BN corpus.", "labels": [], "entities": [{"text": "NW corpus", "start_pos": 197, "end_pos": 206, "type": "DATASET", "confidence": 0.8987016677856445}, {"text": "BN corpus", "start_pos": 230, "end_pos": 239, "type": "DATASET", "confidence": 0.9750634729862213}]}, {"text": "Dependency parses were retrieved using the Stanford parser and ILPs were solved using Gurobi.", "labels": [], "entities": [{"text": "Dependency parses", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7189448177814484}]}, {"text": "As a state-of-the-art baseline for these experiments, we used a reimplementation of the LM-based system of, which we henceforth refer to as CL08.", "labels": [], "entities": []}, {"text": "This is equivalent to a variant of our proposed model that excludes variables for syntactic structure, uses LM log-likelihood as a feature for trigram variables and a tf*idf -based significance score for token variables, and incorporates several targeted syntactic constraints based on grammatical relations derived from RASP () designed to encourage fluent output.", "labels": [], "entities": [{"text": "syntactic structure", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7501580715179443}]}, {"text": "Due to the absence of word reordering in the gold compressions, trigram variables y that were considered in the structured transduction approach were restricted to only those for which tokens appear in the same order as the input as is the case with CL08.", "labels": [], "entities": []}, {"text": "Furthermore, in order to reduce computational overhead for potentially-expensive ILPs, we also excluded dependency arc variables which inverted an existing governor-dependent relationship from the input sentence parse.", "labels": [], "entities": []}, {"text": "A recent analysis of approaches to evaluating compression () has shown a strong correlation between the compression rate and human judgments of compression quality, thereby concluding that comparisons of systems which compress at different rates are unreliable.", "labels": [], "entities": []}, {"text": "Consequently, all comparisons that we carryout here involve a restriction to a particular compression rate to ensure that observed differences can be interpreted meaningfully.", "labels": [], "entities": []}, {"text": "summarizes the results from compression experiments in which the target compression rate is set to the average gold compression rate for each instance.", "labels": [], "entities": []}, {"text": "We observe a significant gain for the joint structured transduction system over the Clarke and Lapata (2008) approach for n-gram F 1 . Since n-gram metrics do not distinguish between content words and function words, we also include an evaluation metric that observes the precision, recall and F-measure of nouns and verbs as a proxy for the content in compressed output.", "labels": [], "entities": [{"text": "precision", "start_pos": 272, "end_pos": 281, "type": "METRIC", "confidence": 0.9990403056144714}, {"text": "recall", "start_pos": 283, "end_pos": 289, "type": "METRIC", "confidence": 0.9942100048065186}, {"text": "F-measure", "start_pos": 294, "end_pos": 303, "type": "METRIC", "confidence": 0.9879383444786072}]}, {"text": "From these, we see that the primary contribution of the supervised joint approach is in enhancing the recall of meaning-bearing words.", "labels": [], "entities": [{"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.8719939589500427}]}], "tableCaptions": [{"text": " Table 1: Experimental results under various quality metrics (see text for descriptions). Systems were  restricted to produce compressions that matched their average gold compression rate. Boldfaced entries  indicate significant differences (p < 0.0005) under the paired t-test and Wilcoxon's signed rank test.", "labels": [], "entities": [{"text": "Wilcoxon's signed rank test", "start_pos": 282, "end_pos": 309, "type": "METRIC", "confidence": 0.5977036356925964}]}]}