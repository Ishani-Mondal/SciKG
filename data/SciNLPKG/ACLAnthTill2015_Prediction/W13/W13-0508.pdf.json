{"title": [{"text": "Annotation for annotation -Toward eliciting implicit linguistic knowledge through annotation - (Project Note)", "labels": [], "entities": []}], "abstractContent": [{"text": "The last two decades witnessed a great success of revived empiricism in NLP research.", "labels": [], "entities": []}, {"text": "However, there are still several NLP tasks that are not successful enough.", "labels": [], "entities": []}, {"text": "As one of many directions for going beyond the revived empiri-cism, this paper introduces a project for annotating annotations with annotators' rationales behind them.", "labels": [], "entities": []}, {"text": "As a first step of this enterprise, the paper particularly focuses on data collection during the annotation and discusses their potential uses.", "labels": [], "entities": []}, {"text": "Finally a preliminary experiment for data collection is described with the data analysis.", "labels": [], "entities": [{"text": "data collection", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7494120597839355}]}], "introductionContent": [{"text": "The last two decades witnessed a great success of revived empiricism in NLP research.", "labels": [], "entities": []}, {"text": "Namely, the corpus construction and machine learning (CC-ML) approach has been the mainstream of NLP research, where corpora are annotated fora specific task and then they are used as training data for machine learning (ML) techniques to build a system for the task.", "labels": [], "entities": [{"text": "corpus construction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7844419777393341}]}, {"text": "The CC-ML approach has been expected to remedy the notorious knowledge construction bottleneck in traditional rule-based approaches.", "labels": [], "entities": [{"text": "knowledge construction", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7149548381567001}]}, {"text": "In the rule-based approach, given a specific task (e.g. POS tagging), human experts (e.g. computational linguists) create rules covering various linguistic phenomena based on their insight.", "labels": [], "entities": [{"text": "POS tagging)", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.7636865675449371}]}, {"text": "In contrast, in the CC-ML approach, human experts mainly focus on creating annotation guidelines.", "labels": [], "entities": []}, {"text": "According to the guidelines, annotation is usually performed by a number of annotators who do not necessarily have deep linguistic knowledge, aiming at increasing the corpus size.", "labels": [], "entities": []}, {"text": "Resultant large annotated corpora are expected to cover broader linguistic phenomena in terms of a collection of annotation instances than the expert-constructed rules in a rule-based approach.", "labels": [], "entities": []}, {"text": "Regularities corresponding to the rules are extracted from the annotated corpora by the ML techniques.", "labels": [], "entities": []}, {"text": "The primacy of the CC-ML approach over the rule-based approach has been shown in fundamental NLP tasks (e.g. POS tagging, syntactic parsing and word sense disambiguation) as well as in various applications (e.g. information extraction, machine translation and summarisation) through prevalent competition-type conferences.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 109, "end_pos": 120, "type": "TASK", "confidence": 0.7659711837768555}, {"text": "syntactic parsing", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7078167498111725}, {"text": "word sense disambiguation)", "start_pos": 144, "end_pos": 170, "type": "TASK", "confidence": 0.6930582970380783}, {"text": "information extraction", "start_pos": 212, "end_pos": 234, "type": "TASK", "confidence": 0.8348221480846405}, {"text": "machine translation", "start_pos": 236, "end_pos": 255, "type": "TASK", "confidence": 0.8507058322429657}, {"text": "summarisation)", "start_pos": 260, "end_pos": 274, "type": "TASK", "confidence": 0.9279940724372864}]}, {"text": "However, too much dominance of the revived empiricism has recently worried a number of researchers.", "labels": [], "entities": []}, {"text": "For instance, Church (2011), who is one of the initiators of the revived empiricism, warned us that we should follow the CC-ML approach with an awareness of the limitations of the underlying ML techniques.", "labels": [], "entities": []}, {"text": "One of the problems of the CC-ML approach is that the annotated information in the corpora is often limited to the output fora given specific task.", "labels": [], "entities": []}, {"text": "Together with other clues (e.g. POS of surrounding words of a target), a system for the task is built by using ML techniques.", "labels": [], "entities": []}, {"text": "However, the validity of these clues has been rarely examined deeply.", "labels": [], "entities": [{"text": "validity", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9065961241722107}]}, {"text": "This would be because the annotator's decision process has attracted less attention than the resultant annotations themselves.", "labels": [], "entities": []}, {"text": "Therefore, there have been few attempts to systematically collect the annotator's rationales behind the annotation process.", "labels": [], "entities": []}, {"text": "From an engineering viewpoint, a machine does not need to perform a task in the same manner as a human does.", "labels": [], "entities": []}, {"text": "The currently used clues might be sufficient for doing the job even though a human uses different information.", "labels": [], "entities": []}, {"text": "For instance, POS tagging and parsing are successful instances of the CC-ML approach.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.8326335847377777}, {"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9529367685317993}]}, {"text": "However, this approach does not always work well on other tasks such as semantic and discourse processing.", "labels": [], "entities": [{"text": "semantic and discourse processing", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.6330910176038742}]}, {"text": "For instance, the performance of the state-of-the-art coreference resolution model still stays around 0.7 in F-score.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.936857283115387}, {"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.99021977186203}]}, {"text": "Furthermore, the performance of zero anaphora resolution in Japanese is much worse, around 0.4 in F-score ().", "labels": [], "entities": [{"text": "F-score", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9977145195007324}]}, {"text": "Such relatively low performance suggests that some crucial information should have been utilised for ML techniques.", "labels": [], "entities": [{"text": "ML", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9928159713745117}]}, {"text": "Against this background, we propose annotating each annotation with the annotator's rationale behind her/his decision.", "labels": [], "entities": []}, {"text": "Since the rationale explains the validity of the annotation instance, it can be considered as a kind of meta-level annotation against the object-level annotation rather than a mere attribute of the annotation.", "labels": [], "entities": []}, {"text": "We expect that analysing these rationales behind human decisions reveals more effective information fora given task that has never been used by existing ML techniques.", "labels": [], "entities": [{"text": "ML", "start_pos": 153, "end_pos": 155, "type": "TASK", "confidence": 0.9585472941398621}]}, {"text": "We believe this is one of the ways to integrate the revived empiricism and rationalism.", "labels": [], "entities": []}, {"text": "As a first step of this enterprise, this paper particularly discusses what kinds of information can be collected during the annotation process for estimating a rationale behind each annotation decision.", "labels": [], "entities": []}, {"text": "We also explain potential uses of the collected information.", "labels": [], "entities": []}, {"text": "Finally, we describe our preliminary experiment for collecting the annotator's actions and eye-gaze during her/his annotation process.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}