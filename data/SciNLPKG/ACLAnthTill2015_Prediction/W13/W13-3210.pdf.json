{"title": [{"text": "Towards Dynamic Word Sense Discrimination with Random Indexing", "labels": [], "entities": [{"text": "Dynamic Word Sense Discrimination", "start_pos": 8, "end_pos": 41, "type": "TASK", "confidence": 0.7235049158334732}, {"text": "Random Indexing", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.6316419094800949}]}], "abstractContent": [{"text": "Most distributional models of word similarity represent a word type by a single vector of contextual features, even though, words commonly have more than one sense.", "labels": [], "entities": []}, {"text": "The multiple senses can be captured by employing several vectors per word in a multi-prototype distributional model, prototypes that can be obtained by first constructing all the context vectors for the word and then clustering similar vectors to create sense vectors.", "labels": [], "entities": []}, {"text": "Storing and clustering context vectors can be expensive though.", "labels": [], "entities": []}, {"text": "As an alternative, we introduce Multi-Sense Random Indexing, which performs on-the-fly (incremental) clustering.", "labels": [], "entities": []}, {"text": "To evaluate the method, a number of measures for word similarity are proposed, both contextual and non-contextual, including new measures based on optimal alignment of word senses.", "labels": [], "entities": []}, {"text": "Experimental results on the task of predicting semantic textual similarity do, however, not show a systematic difference between single-prototype and multi-prototype models.", "labels": [], "entities": [{"text": "predicting semantic textual similarity", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.8649835884571075}]}], "introductionContent": [{"text": "Many terms have more than one meaning, or sense.", "labels": [], "entities": []}, {"text": "Some of these senses are static and can be listed in dictionaries and thesauri, while other senses are dynamic and determined by the contexts the terms occur in.", "labels": [], "entities": []}, {"text": "Work in Word Sense Disambiguation often concentrate on the static word senses, making the task of distinguishing between them one of classification into a predefined set of classes (i.e., the given word senses); see, e.g., for overviews of current work in the area.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.700230727593104}]}, {"text": "The idea of fixed generic word senses has received a fair amount of criticism in the literature).", "labels": [], "entities": []}, {"text": "This paper instead primarily investigates dynamically appearing word senses, word senses that depend on the actual usage of a term in a corpus or a domain.", "labels": [], "entities": []}, {"text": "This task is often referred to as Word Sense Induction or Word Sense Discrimination.", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.5857063333193461}, {"text": "Word Sense Discrimination", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6263015866279602}]}, {"text": "This is, in contrast, essentially a categorisation problem, distinguished by different senses being more or less similar to each other at a given time, given some input data.", "labels": [], "entities": []}, {"text": "The dividing line between Word Sense Disambiguation and Discrimination is not necessarily razor sharp though: also different senses of a term listed in a dictionary tend to have some level of overlap.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6744691928227743}]}, {"text": "In recent years, distributional models have been widely used to infer word similarity.", "labels": [], "entities": []}, {"text": "Most such models represent a word type by a single vector of contextual features obtained from co-occurrence counts in large textual corpora.", "labels": [], "entities": []}, {"text": "By assigning a single vector to each term in the corpus, the resulting model assumes that each term has a fixed semantic meaning (relative to all the other terms).", "labels": [], "entities": []}, {"text": "However, due to homonomy and polysemy, word semantics cannot be adequately represented by a single-prototype vector.", "labels": [], "entities": []}, {"text": "Multi-prototype distributional models in contrast employ different vectors to represent different senses of a word.", "labels": [], "entities": []}, {"text": "Multiple prototypes can be obtained by first constructing context vectors for all words and then clustering similar context vectors to create a sense vector.", "labels": [], "entities": []}, {"text": "This maybe expensive, as vectors need to stored and clustered.", "labels": [], "entities": []}, {"text": "As an alternative, we propose anew method called Multi-Sense Random Indexing (MSRI), which is based on Random Indexing () and performs an on-the-fly (incremental) clustering.", "labels": [], "entities": [{"text": "Multi-Sense Random Indexing (MSRI)", "start_pos": 49, "end_pos": 83, "type": "TASK", "confidence": 0.6488553086916605}]}, {"text": "MSRI is a method for building a multiprototype / multi-sense vector space model, which attempts to capture one or more senses per unique term in an unsupervised manner, where each sense is represented as a separate vector in the model.", "labels": [], "entities": []}, {"text": "This differs from the classical Random Indexing (RI) method which assumes a static sense inventory by restricting each term to have only one vector (sense) per term, as described in Section 2.", "labels": [], "entities": [{"text": "Random Indexing (RI)", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.6151896059513092}]}, {"text": "The MSRI method is introduced in Section 3.", "labels": [], "entities": [{"text": "MSRI", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.45179757475852966}]}, {"text": "Since the induced dynamic senses do not necessarily correspond to the traditional senses distinguished by humans, we perform an extrinsic evaluation by applying the resulting models to data from the Semantic Textual Similarity shared task (, in order to compare MSRI to the classical RI method.", "labels": [], "entities": [{"text": "Semantic Textual Similarity shared task", "start_pos": 199, "end_pos": 238, "type": "TASK", "confidence": 0.7194936990737915}]}, {"text": "The experimental setup is the topic of Section 4, while the results of the experiments are given in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 then sums up the discussion and points to ways in which the present work could be continued.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to explore the potential of the MSRI model and the textual similarity measures proposed here, experiments were carried out on data from the Semantic Textual Similarity (STS) shared task ().", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) shared task", "start_pos": 149, "end_pos": 194, "type": "TASK", "confidence": 0.7539361715316772}]}, {"text": "Given a pair of sentences, systems participating in this task shall compute how semantically similar the two sentences are, returning a similarity score between zero (completely unrelated) and five (completely semantically equivalent).", "labels": [], "entities": []}, {"text": "Gold standard scores are obtained by averaging multiple scores obtained from human annotators.", "labels": [], "entities": []}, {"text": "System performance is then evaluated using the Pearson product-moment correlation coefficient (\u03c1) between the system scores and the human scores.", "labels": [], "entities": [{"text": "Pearson product-moment correlation coefficient (\u03c1)", "start_pos": 47, "end_pos": 97, "type": "METRIC", "confidence": 0.945789098739624}]}, {"text": "The goal of the experiments reported here was not to build a competitive STS system, but rather to investigate whether MSRI can outperform classical Random Indexing on a concrete task such as computing textual similarity, as well as to identify which similarity measures and meaning representations appear to be most suitable for such a task.", "labels": [], "entities": []}, {"text": "The system is therefore quite rudimentary: a simple linear regression model is fitted on the training data, using a single sentence similarity measure as input and the similarity score as the dependent variable.", "labels": [], "entities": []}, {"text": "The implementations of RI and MSRI are based on JavaSDM).", "labels": [], "entities": []}, {"text": "As data for training random indexing models, we used the CLEF 2004-2008 English corpus, consisting of approximately 130M words of newspaper articles ().", "labels": [], "entities": [{"text": "CLEF 2004-2008 English corpus", "start_pos": 57, "end_pos": 86, "type": "DATASET", "confidence": 0.970892146229744}]}, {"text": "All text was tokenized and lemmatized using the TreeTagger for English.", "labels": [], "entities": []}, {"text": "Stopwords were removed using a customized version of the stoplist provided by the Lucene project ().", "labels": [], "entities": [{"text": "Lucene project", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.9524850249290466}]}, {"text": "Data for fitting and evaluating the linear regression models came from the STS development and test data, consisting of sentence pairs with a gold standard similarity score.", "labels": [], "entities": [{"text": "STS development and test data", "start_pos": 75, "end_pos": 104, "type": "DATASET", "confidence": 0.7970414400100708}]}, {"text": "The STS 2012 development data stems from the Microsoft Research Paraphrase corpus (MSRpar, 750 pairs), the Microsoft Research Video Description corpus (MSvid, 750 pairs), and statistical machine translation output based on the Europarl corpus (SMTeuroparl, 734 pairs).", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase corpus (MSRpar", "start_pos": 45, "end_pos": 89, "type": "DATASET", "confidence": 0.7858074903488159}, {"text": "Microsoft Research Video Description corpus (MSvid", "start_pos": 107, "end_pos": 157, "type": "DATASET", "confidence": 0.7011389902659825}, {"text": "statistical machine translation", "start_pos": 175, "end_pos": 206, "type": "TASK", "confidence": 0.6270932952562968}, {"text": "Europarl corpus", "start_pos": 227, "end_pos": 242, "type": "DATASET", "confidence": 0.9897149503231049}]}, {"text": "Test data for STS 2012 consists of more data from the same sources: MSRpar (750 pairs), MSRvid (750 pairs) and SMTeuroparl (459 pairs).", "labels": [], "entities": [{"text": "STS 2012", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.7910368740558624}, {"text": "MSRpar", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.8057212233543396}, {"text": "MSRvid", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.8162526488304138}]}, {"text": "In addition, different test data comes from translation data in the news domain (SMTnews, 399 pairs) and ontology mappings between OntoNotes and WordNet (OnWN, 750 pairs).", "labels": [], "entities": [{"text": "SMTnews", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.7585852742195129}, {"text": "WordNet", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.8540465235710144}, {"text": "OnWN", "start_pos": 154, "end_pos": 158, "type": "DATASET", "confidence": 0.8502582311630249}]}, {"text": "When testing on the STS 2012 data, we used the corresponding development data from the same domain for training, except for OnWN where we used all development data combined.", "labels": [], "entities": [{"text": "STS 2012 data", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.8714825709660848}, {"text": "OnWN", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.9747332334518433}]}, {"text": "The development data for STS 2013 consisted of all development and test data from STS 2012 combined, whereas test data comprised machine translation output (SMT, 750 pairs), ontology mappings both between WordNet and OntoNotes (OnWN, 561 pairs) and between WordNet and FrameNet, as well as news article headlines (HeadLine, 750 pairs).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.6790127605199814}, {"text": "WordNet", "start_pos": 205, "end_pos": 212, "type": "DATASET", "confidence": 0.9376624822616577}, {"text": "HeadLine", "start_pos": 314, "end_pos": 322, "type": "DATASET", "confidence": 0.9373384118080139}]}, {"text": "For simplicity, all development data combined were used for fitting the linear regression model, even though careful matching of development and test data sets may improve performance.", "labels": [], "entities": []}, {"text": "shows Pearson correlation scores per feature on the STS 2012 test data using simple linear regression.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 6, "end_pos": 25, "type": "METRIC", "confidence": 0.8996455073356628}, {"text": "STS 2012 test data", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.9378403425216675}]}, {"text": "The most useful features for each data set are marked in bold.", "labels": [], "entities": []}, {"text": "For reference, the scores of the best performing STS systems for each data set are also shown, as well as baseline scores obtained with a simple normalized token overlap measure.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pearson correlation scores per feature on STS 2012 test data using simple linear regression", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8967415988445282}, {"text": "STS 2012 test data", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.9017157107591629}]}, {"text": " Table 3: Pearson correlation scores per feature on STS 2013 test data using simple linear regression", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8940166532993317}, {"text": "STS 2013 test data", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.8827143162488937}]}]}