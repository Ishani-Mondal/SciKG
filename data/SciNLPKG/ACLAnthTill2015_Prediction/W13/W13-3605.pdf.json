{"title": [{"text": "UM-Checker: A Hybrid System for English Grammatical Error Cor- rection", "labels": [], "entities": [{"text": "English Grammatical Error Cor- rection", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.61987833182017}]}], "abstractContent": [{"text": "This paper describes the NLP 2 CT Grammatical Error Detection and Correction system for the CoNLL 2013 shared task, with a focus on the errors of article or determiner (ArtOrDet), noun number (Nn), preposition (Prep), verb form (Vform) and subject-verb agreement (SVA).", "labels": [], "entities": [{"text": "NLP 2 CT Grammatical Error Detection", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.5753664473692576}, {"text": "CoNLL 2013 shared task", "start_pos": 92, "end_pos": 114, "type": "DATASET", "confidence": 0.8508890867233276}]}, {"text": "A hybrid model is adopted for this special task.", "labels": [], "entities": []}, {"text": "The process starts with spell-checking as a preprocessing step to correct any possible erroneous word.", "labels": [], "entities": []}, {"text": "We used a Maximum Entropy classifier together with manually rule-based filters to detect the grammatical errors in English.", "labels": [], "entities": []}, {"text": "A language model based on the Google N-gram corpus was employed to select the best correction candidate from a confusion matrix.", "labels": [], "entities": [{"text": "Google N-gram corpus", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.6387540598710378}]}, {"text": "We also explored a graph-based label propagation approach to overcome the sparsity problem in training the model.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7369564771652222}]}, {"text": "Finally , a number of deterministic rules were used to increase the precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.999734103679657}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.999107301235199}]}, {"text": "The proposed model was evaluated on the test set consisting of 50 essays and with about 500 words in each essay.", "labels": [], "entities": []}, {"text": "Our system achieves the 5 th and 3 rd F 1 scores on official test set among all 17 participating teams based on gold-standard edits before and after revision, respectively .", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.8727424144744873}]}], "introductionContent": [{"text": "With the increasing number of people allover the world who study English as their second language 1 , grammatical errors in writing often occurs due to cultural diversity, language habits, education background, etc.", "labels": [], "entities": []}, {"text": "Thus, there is a substantial and increasing need of using computer A well-known fact is that the most popular language chosen as a first foreign language is techniques to improve the writing ability for second language learners.", "labels": [], "entities": []}, {"text": "Grammatical error correction is the task of automatically detecting and correction erroneous word usage and ill-formed grammatical constructions in text . In recent decades, this special task has gained more attention by some organizations such as the Helping Our Own (HOO) challenge.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8086368640263876}, {"text": "automatically detecting and correction erroneous word usage and ill-formed grammatical constructions in text", "start_pos": 44, "end_pos": 152, "type": "TASK", "confidence": 0.689109953550192}, {"text": "Helping Our Own (HOO) challenge", "start_pos": 252, "end_pos": 283, "type": "TASK", "confidence": 0.6536445702825274}]}, {"text": "Although the performance of grammatical error correction systems has been improved, it is still mostly limited to dealing with the determiner and preposition error types with a very low recall and precision.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6204128464063009}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9990895986557007}, {"text": "precision", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9935763478279114}]}, {"text": "This year, the CoNLL-2013 shared task extends to include a more comprehensive list of error types, as shown in.", "labels": [], "entities": []}, {"text": "To take on this challenge, this paper proposes pipe-line architecture in combination with several error detection and correction models based on a hybrid approach.", "labels": [], "entities": [{"text": "error detection and correction", "start_pos": 98, "end_pos": 128, "type": "TASK", "confidence": 0.7915224507451057}]}, {"text": "As a preprocessing step we firstly employ a spelling correction to correct the misspelled words.", "labels": [], "entities": []}, {"text": "To correct the grammatical errors, a hybrid system is designed that integrated with Maximum Entropy (ME) classifier, deterministic filter and N-gram language model scorer, each of which is constructed as an individual model.", "labels": [], "entities": []}, {"text": "According to the phenomena of the problems, we use different combinations of the models trained on specific data to tackle the corresponding types of errors.", "labels": [], "entities": []}, {"text": "For instance, Prep and Nn have a strong inter-relation with the words (surface) that are preceding and following the active word.", "labels": [], "entities": []}, {"text": "This can be detected and recovered by using a language model.", "labels": [], "entities": []}, {"text": "On the other hand, SVA is more complicated and it is more effective to determine the mistakes by using the linguistic and grammatical rules.", "labels": [], "entities": [{"text": "SVA", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9276122450828552}]}], "datasetContent": [{"text": "The evaluation is provided by the organizer and generated by M 2 scorer ).", "labels": [], "entities": []}, {"text": "The result consists of precision, recall and Fscore.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9997603297233582}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9995236396789551}, {"text": "Fscore", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9969407916069031}]}, {"text": "Our grammatical error correction system has proposed 1,011 edits.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.5878660579522451}]}, {"text": "The evaluation result of our system output for the CoNLL-2013 test data is shown in.", "labels": [], "entities": [{"text": "CoNLL-2013 test data", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.9686300158500671}]}, {"text": "The data in table 5 and 6 are the detailed information for each error type which was calculated by us, the table 5 is the data before revision, and the table 6 is that after revision.", "labels": [], "entities": []}, {"text": "Second column is the amount of the gold edits, and the third column is the amount of our correct edits, and the last column is the percentage of correct edits.", "labels": [], "entities": []}, {"text": "We analyzed the results in detail, and found several critical reasons of causing low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9982446432113647}]}, {"text": "Firstly, the five error types are associated relatively, if one is modified, it may cause a chain reaction, such as the article will affect the noun number, and the noun number will cause the SVA errors.", "labels": [], "entities": []}, {"text": "Some Nn errors still cannot be detected or given a wrong correction by our system, which decreases the precision and recall of SVA.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9996652603149414}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9994434714317322}, {"text": "SVA", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.5811324119567871}]}, {"text": "Another reason is our system does not perform well in Vform and Prep error correction.", "labels": [], "entities": [{"text": "Prep error correction", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7029946247736613}]}, {"text": "In our output, just a few errors have been revised.", "labels": [], "entities": []}, {"text": "This means the quantity of correction rules is not enough that cannot coverall the linguistic phenomena.", "labels": [], "entities": []}, {"text": "For instance, the situation of missing verb or unnecessary verb cannot be detected.", "labels": [], "entities": []}, {"text": "On the other hand, the hybrid method of our system has filtered some wrong suggestion candidates that improve the precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9986425042152405}]}], "tableCaptions": [{"text": " Table 4: Evaluation result of Precision, Recall and F- score.", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9751988649368286}, {"text": "Precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9991636276245117}, {"text": "Recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9962376356124878}, {"text": "F- score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9928344488143921}]}, {"text": " Table 5: Detail information of evaluation result (Be- fore Revision).", "labels": [], "entities": [{"text": "Be- fore Revision", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.8185800462961197}]}, {"text": " Table 6: Detail information of evaluation result (After  Revision).", "labels": [], "entities": [{"text": "Detail", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.7289475798606873}]}]}