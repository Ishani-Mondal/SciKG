{"title": [{"text": "DCU Participation in WMT2013 Metrics Task", "labels": [], "entities": [{"text": "WMT2013 Metrics", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.563176691532135}]}], "abstractContent": [{"text": "In this paper, we propose a novel syntactic based MT evaluation metric which only employs the dependency information in the source side.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9493987858295441}]}, {"text": "Experimental results show that our method achieves higher correlation with human judgments than BLEU, TER, HWCM and METEOR at both sentence and system level for all of the four language pairs in WMT 2010.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9988868832588196}, {"text": "TER", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.99244225025177}, {"text": "HWCM", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.8275842070579529}, {"text": "METEOR", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9676474332809448}, {"text": "WMT 2010", "start_pos": 195, "end_pos": 203, "type": "DATASET", "confidence": 0.8540975451469421}]}], "introductionContent": [{"text": "Automatic evaluation plays a more important role in the evolution of machine translation.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7152466177940369}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.80124831199646}]}, {"text": "At the earliest stage, the automatic evaluation metrics only use the lexical information, in which, BLEU) is the most popular one.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9990425705909729}]}, {"text": "BLEU is simple and effective.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9796039462089539}]}, {"text": "Most of the researchers regard BLEU as their primary evaluation metric to develop and compare MT systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9970466494560242}, {"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9882323145866394}]}, {"text": "However, BLEU only employs the lexical information and cannot adequately reflect the structural level similarity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.996541440486908}]}, {"text": "Translation Error Rate (TER)) measures the number of edits required to change the hypothesis into one of the references.", "labels": [], "entities": [{"text": "Translation Error Rate (TER))", "start_pos": 0, "end_pos": 29, "type": "METRIC", "confidence": 0.8124222308397293}]}, {"text": "METEOR, which defines loose unigram matching between the hypothesis and the references with the help of stemming and Wordnet-looking-up, is also a lexical based method and achieves the first-class humanevaluation-correlation score.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6877480149269104}, {"text": "Wordnet-looking-up", "start_pos": 117, "end_pos": 135, "type": "DATASET", "confidence": 0.9580472707748413}]}, {"text": "AMBER () incorporates recall, extra penalties and some text processing variants on the basis of BLEU.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7585716247558594}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9994207620620728}, {"text": "extra penalties", "start_pos": 30, "end_pos": 45, "type": "METRIC", "confidence": 0.9266789555549622}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9964156150817871}]}, {"text": "The main weakness of all the above lexical based methods is that they cannot adequately reflect the structural level similarity.", "labels": [], "entities": []}, {"text": "To overcome the weakness of the lexical based methods, many syntactic based metrics were proposed.", "labels": [], "entities": []}, {"text": "proposed STM, a constituent tree based approach, and HWCM, a dependency tree based approach.", "labels": [], "entities": [{"text": "HWCM", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8157206773757935}]}, {"text": "Both of the two methods compute the similarity between the sub-trees of the hypothesis and the reference.", "labels": [], "entities": [{"text": "similarity", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9710675477981567}]}, {"text": "presented a method using the Lexical-Functional Grammar (LFG) dependency tree.", "labels": [], "entities": []}, {"text": "MAXSIM) and the method proposed by also employed the syntactic information in association with lexical information.With the syntactic information which can reflect structural information, the correlation with the human judgments can be improved to a certain extent.", "labels": [], "entities": [{"text": "MAXSIM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8237321972846985}]}, {"text": "As we know that the hypothesis is potentially noisy, and these errors expand through the parsing process.", "labels": [], "entities": [{"text": "parsing", "start_pos": 89, "end_pos": 96, "type": "TASK", "confidence": 0.9654670357704163}]}, {"text": "Thus the power of syntactic information could be considerably weakened.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to overcome the shortcoming of the syntactic based methods and propose a novel dependency based MT evaluation metric.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.9189149141311646}]}, {"text": "The proposed metric only employs the reference dependency tree which contains both the lexical and syntactic information, leaving the hypothesis side unparsed to avoid the error propagation.", "labels": [], "entities": []}, {"text": "In our metric, F-score is calculated using the string of hypothesis and the dependency based ngrams which are extracted from the reference dependency tree.", "labels": [], "entities": [{"text": "F-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9958701729774475}]}, {"text": "Experimental results show that our method achieves higher correlation with human judgments than BLEU, HWCM, TER and METEOR at both sentence level and system level for all of the four language pairs in WMT 2010.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9985927939414978}, {"text": "TER", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9925490617752075}, {"text": "METEOR", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9696126580238342}, {"text": "WMT 2010", "start_pos": 201, "end_pos": 209, "type": "DATASET", "confidence": 0.8404388725757599}]}], "datasetContent": [{"text": "In this new method, we calculate F-score using the string of hypothesis and the dep-n-grams which are extracted from the reference dependency tree.", "labels": [], "entities": [{"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9916456341743469}]}, {"text": "The new method is named DEPREF since it is a DEPendency based method only using dependency tree of REference to calculate the F-score.", "labels": [], "entities": []}, {"text": "In DEPREF, after the parsing of the reference sentences, there are three steps below being carried out.", "labels": [], "entities": [{"text": "DEPREF", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.803485095500946}]}, {"text": "1) Extracting the dependency based n-gram (dep-n-gram) in the dependency tree of the reference.", "labels": [], "entities": []}, {"text": "2) Matching the dep-n-gram with the string of hypothesis.", "labels": [], "entities": []}, {"text": "3) Obtaining the final score of a hypothesis.", "labels": [], "entities": [{"text": "Obtaining the final score", "start_pos": 3, "end_pos": 28, "type": "METRIC", "confidence": 0.8153759241104126}]}, {"text": "The detail description of our method will be found in paper () . We only give the experiment results in this paper.", "labels": [], "entities": []}, {"text": "Both the sentence level evaluation and the system level evaluation are conducted to assess the performance of our automatic metric.", "labels": [], "entities": []}, {"text": "At the sentence level evaluation, Kendall's rank correlation coefficient \u03c4 is used.", "labels": [], "entities": [{"text": "Kendall's rank correlation coefficient \u03c4", "start_pos": 34, "end_pos": 74, "type": "METRIC", "confidence": 0.7303315997123718}]}, {"text": "At the system level evaluation, the Spearman's rank correlation coefficient \u03c1 is used.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient \u03c1", "start_pos": 36, "end_pos": 77, "type": "METRIC", "confidence": 0.7492324759562811}]}, {"text": "Kendall's rank correlation coefficient \u03c4 is employed to evaluate the correlation of all the MT evaluation metrics and human judgements at the sentence level.", "labels": [], "entities": [{"text": "rank correlation coefficient \u03c4", "start_pos": 10, "end_pos": 40, "type": "METRIC", "confidence": 0.7965972051024437}, {"text": "MT evaluation", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.898564338684082}]}, {"text": "A higher value of \u03c4 means a better ranking similarity with the human judges.", "labels": [], "entities": []}, {"text": "The correlation scores of the four language pairs and the average scores are shown in.A (without external resources) and.B (with stemming and synonym), Our method performs best when maximum length of dep-n-gram is set to 3, so we present only the results when the maximum length of dep-n-gram equals 3.", "labels": [], "entities": []}, {"text": "From.A, we can see that all our methods are far more better than BLEU, TER and HWCM when there is no external resources applied on all of the four language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9982514977455139}, {"text": "TER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9930833578109741}, {"text": "HWCM", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.6244879364967346}]}, {"text": "In.B, external resources is considered.", "labels": [], "entities": []}, {"text": "DEPREF is also better than METEOR on the four language pairs.", "labels": [], "entities": [{"text": "DEPREF", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7295551300048828}, {"text": "METEOR", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.8431583642959595}]}, {"text": "From the comparison between.B, we can conclude that external resources is helpful for DEPREF on most of the language pairs.", "labels": [], "entities": [{"text": "DEPREF", "start_pos": 86, "end_pos": 92, "type": "TASK", "confidence": 0.46074604988098145}]}, {"text": "When comparing DEPREF without external resources with METEOR, we find that DEPREF obtains better results on Czech-English and GermanEnglish.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.5794346332550049}, {"text": "DEPREF", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.6039353609085083}]}, {"text": "We also evaluated the metrics with the human rankings at the system level to further investigate the effectiveness of our metrics.", "labels": [], "entities": []}, {"text": "The matching of the words in DEPREF is correlated with the position of the words, so the traditional way of computing system level score, like what BLEU does, is not feasible for DEPREF.", "labels": [], "entities": [{"text": "DEPREF", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.7509118318557739}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9945762157440186}]}, {"text": "Therefore, we resort to the way of adding the sentence level scores together to obtain the system level score.", "labels": [], "entities": []}, {"text": "At system level evaluation, we employ Spearman's rank correlation coefficient \u03c1.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient \u03c1", "start_pos": 38, "end_pos": 79, "type": "METRIC", "confidence": 0.6537339289983114}]}, {"text": "The correlations of the four language pairs and the average scores are shown in.A (without external resources) and Table 2.B (with stem and synonym).", "labels": [], "entities": []}, {"text": "From.A, we can see that the correlation of DEPREF is better than BLEU, TER and HWCM on German-English, Spanish-English and French-English.", "labels": [], "entities": [{"text": "DEPREF", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9284770488739014}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9992769360542297}, {"text": "TER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9975091218948364}, {"text": "HWCM", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9494357705116272}]}, {"text": "On Czech-English, our metric DEPREF is better than BLEU and TER.", "labels": [], "entities": [{"text": "DEPREF", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.8976106643676758}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.998958945274353}, {"text": "TER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9945781230926514}]}, {"text": "In.B (with stem and synonym), DEPREF obtains better results than METEOR on all of the language pairs except one case that DEPREF gets the same result as METEOR on Czech-English.", "labels": [], "entities": [{"text": "DEPREF", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.7348098754882812}, {"text": "METEOR", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.8343312740325928}]}, {"text": "When comparing DEPREF without external resources with METEOR, we can find that DEPREF gets better result than METEOR on Spanish-English and French-English.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.7614272832870483}, {"text": "DEPREF", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.5697633028030396}]}, {"text": "From, we can conclude that, DEPREF without external resources can obtain comparable result with METEOR, and DE-PREF with external resources can obtain better results than METEOR.", "labels": [], "entities": [{"text": "DEPREF", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.5796687602996826}, {"text": "METEOR", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.6241264939308167}, {"text": "METEOR", "start_pos": 171, "end_pos": 177, "type": "DATASET", "confidence": 0.8754780292510986}]}, {"text": "The only exception is that at the system level evaluation, Czech-English's best score is abtained by HWCM.", "labels": [], "entities": [{"text": "abtained", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9705556035041809}, {"text": "HWCM", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.9466851949691772}]}, {"text": "Notice that there are only 12 systems in Czech-English, which means there are only 12 numbers to be sorted, we believe    The system level correlations with the human judgments for Czech-to-English, German-toEnglish, Spanish-to-English and French-to-English.", "labels": [], "entities": []}, {"text": "The number in bold is the maximum value in each column.", "labels": [], "entities": []}, {"text": "N stands for the max length of the headword chains in HWCM in.A. the spareness issure is more serious in this case.", "labels": [], "entities": [{"text": "max length", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9396471381187439}, {"text": "HWCM", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9178633093833923}, {"text": "spareness issure", "start_pos": 69, "end_pos": 85, "type": "METRIC", "confidence": 0.9364199936389923}]}], "tableCaptions": [{"text": " Table 1.A Sentence level correlations of the metrics without external resources.", "labels": [], "entities": []}, {"text": " Table 1.B Sentence level correlations of the metrics with stemming and synonym.", "labels": [], "entities": []}, {"text": " Table 2.A System level correlations of the metrics without external resources.", "labels": [], "entities": []}, {"text": " Table 2.B System level correlations of the metrics with stemming and synonym.", "labels": [], "entities": []}]}