{"title": [{"text": "Detecting Missing Annotation Disagreement using Eye Gaze Information", "labels": [], "entities": [{"text": "Detecting Missing Annotation Disagreement", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8713123649358749}]}], "abstractContent": [{"text": "This paper discusses the detection of missing annotation disagreements (MADs), in which an annotator misses annotating an annotation instance while her counterpart correctly annotates it.", "labels": [], "entities": [{"text": "detection of missing annotation disagreements (MADs)", "start_pos": 25, "end_pos": 77, "type": "TASK", "confidence": 0.8619423285126686}]}, {"text": "We employ anno-tator eye gaze as a clue for detecting this type of disagreement together with linguistic information.", "labels": [], "entities": []}, {"text": "More precisely, we extract highly frequent gaze patterns from the pre-extracted gaze sequences related to the annotation target, and then use the gaze patterns as features for detecting the MADs.", "labels": [], "entities": []}, {"text": "Through the empirical evaluation using the data set collected in our previous study, we investigated the effectiveness of each type of information.", "labels": [], "entities": []}, {"text": "The results showed that both eye gaze and linguistic information contributed to improving performance of our MAD detection model compared with the baseline model.", "labels": [], "entities": [{"text": "MAD detection", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.9799171984195709}]}, {"text": "Furthermore, our additional investigation revealed that some specific gaze patterns could be a good indicator for detecting the MADs.", "labels": [], "entities": [{"text": "detecting the MADs", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.6742620468139648}]}], "introductionContent": [{"text": "Over the last two decades, with the development of supervised machine learning techniques, annotating texts has become an essential task in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 140, "end_pos": 173, "type": "TASK", "confidence": 0.7923221190770467}]}, {"text": "Since the annotation quality directly impacts on performance of ML-based NLP systems, many researchers have been concerned with building high-quality annotated corpora at a lower cost.", "labels": [], "entities": [{"text": "ML-based NLP", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.8583792746067047}]}, {"text": "Several different approaches have been taken for this purpose, such as semi-automating annotation by combining human annotation and existing NLP tools (, implementing better annotation tools (.", "labels": [], "entities": []}, {"text": "The assessment of annotation quality is also an important issue in corpus building.", "labels": [], "entities": [{"text": "corpus building", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7490715980529785}]}, {"text": "The annotation quality is often evaluated with the agreement ratio among annotation results by multiple independent annotators.", "labels": [], "entities": [{"text": "agreement", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9615375995635986}]}, {"text": "Various metrics for measuring reliability of annotation have been proposed, which are based on inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes.", "labels": [], "entities": []}, {"text": "This is inline with Behaviour mining) instead of data mining.", "labels": [], "entities": [{"text": "Behaviour mining", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7574381530284882}, {"text": "data mining", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.6985359787940979}]}, {"text": "There is few work looking at the annotation process for assessing annotation quality with a few exceptions like, which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation process.", "labels": [], "entities": []}, {"text": "They concluded that the annotation difficulty depended on the semantic and syntactic complexity of the annotation targets, and the estimated difficulty would be useful for selecting training data for active learning techniques.", "labels": [], "entities": []}, {"text": "We also reported an analysis of relations between a necessary time for annotating a single predicate-argument relation in Japanese text and the agreement ratio of the annotation among three annotators ().", "labels": [], "entities": []}, {"text": "The annotation time was defined based on annotator actions and eye gaze.", "labels": [], "entities": []}, {"text": "The analysis revealed that a longer annotation time suggested difficult annotation.", "labels": [], "entities": []}, {"text": "Thus, we could estimate annotation quality based on the eye gaze and actions of a single annotator instead of the annotation results of multiple annotators.", "labels": [], "entities": []}, {"text": "Following up our previous work, this paper particularly focuses on a certain type of disagreement in which an annotator misses annotating a predicate-argument relation while her counterpart correctly annotates it.", "labels": [], "entities": []}, {"text": "We call this type of disagreement missing annotation disagreement (MAD).", "labels": [], "entities": [{"text": "disagreement missing annotation disagreement (MAD)", "start_pos": 21, "end_pos": 71, "type": "TASK", "confidence": 0.6721310700689044}]}, {"text": "MADs were excluded from our previous analysis.", "labels": [], "entities": [{"text": "MADs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.5429689288139343}]}, {"text": "Estimating MADs from the behaviour of a single annotator would be useful in a situation where only a single annotator is available.", "labels": [], "entities": [{"text": "MADs", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.9065545201301575}]}, {"text": "Against this background, we tackle a problem of detecting MADs based on both linguistic information of annotation targets and annotator eye gaze.", "labels": [], "entities": [{"text": "detecting MADs", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.6636654883623123}]}, {"text": "In our approach, the eye gaze data is transformed into a sequence of fixations, and then fixation patterns suggesting MADs are discovered by using a text mining technique.", "labels": [], "entities": [{"text": "MADs", "start_pos": 118, "end_pos": 122, "type": "TASK", "confidence": 0.9060885906219482}]}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents details of the experiment for collecting annotator behavioural data during annotation, as well as details on the collected data.", "labels": [], "entities": []}, {"text": "Section 3 overviews our problem setting, and then Section 4 explains a model of MAD detection based on eyetracking data.", "labels": [], "entities": [{"text": "MAD detection", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.9949145019054413}]}, {"text": "Section 5 reports the empirical results of MAD detection.", "labels": [], "entities": [{"text": "MAD detection", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9930092394351959}]}, {"text": "Section 6 reviews the related work and Section 7 concludes and discusses future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To investigate the effectiveness of gaze patterns introduced in Section 4, we evaluate performance of detecting MADs in our data.", "labels": [], "entities": []}, {"text": "In actual annotation review situations for detecting MADs, it is reasonable to assume that an annotator concentrates her/his attention on only non-annotated predicateargument relations.", "labels": [], "entities": [{"text": "detecting MADs", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.6419273018836975}]}, {"text": "We therefore conducted a 10-fold cross validation with the data shown in except for the instances annotated by both annotators.", "labels": [], "entities": []}, {"text": "The evaluation is two-fold, one evaluates the performance of detecting missing annotations of A 0 , assuming that A 2 annotation is the gold standard, i.e. distinguishing 281 positive instances from 561 negative instances, and the other way around.", "labels": [], "entities": []}, {"text": "We used a Support Vector Machine (Vapnik, 1998) with a linear kernel, altering parameters for the cost and slack variables, i.e. -j and -c options of svm light . The parameters of the prefixspan algorithm were set so that the maximum size of patterns was 5 and the minimum size of patterns was 3 due to the computing efficiency.", "labels": [], "entities": []}, {"text": "We used the top-50 frequent gaze patterns for both positive and negative cases as gaze features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Results of detecting MADs", "labels": [], "entities": [{"text": "MADs", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.6567301750183105}]}, {"text": " Table 6: Top-20 frequent gaze patterns  (gold:A 2 , eval:A 0 )", "labels": [], "entities": []}]}