{"title": [{"text": "A Data-driven Model for Timing Feedback in a Map Task Dialogue System", "labels": [], "entities": [{"text": "Timing Feedback in a Map Task Dialogue", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.6667943724564144}]}], "abstractContent": [{"text": "We present a data-driven model for detecting suitable response locations in the user's speech.", "labels": [], "entities": []}, {"text": "The model has been trained on human-machine dialogue data and implemented and tested in a spoken dialogue system that can perform the Map Task with users.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first example of a dialogue system that uses automatically extracted syntactic, prosodic and contextual features for online detection of response locations.", "labels": [], "entities": [{"text": "online detection of response locations", "start_pos": 147, "end_pos": 185, "type": "TASK", "confidence": 0.7579811453819275}]}, {"text": "A subjective evaluation of the dialogue system suggests that interactions with a system using our trained model were perceived significantly better than those with a system using a model that made decisions at random.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, dialogue systems have rested on a very simple model for turn-taking, where the system uses a fixed silence threshold to detect the end of the user's utterance, after which the system responds.", "labels": [], "entities": []}, {"text": "However, this model does not capture human-human dialogue very accurately; sometimes a speaker just hesitates and no turnchange is intended, sometimes the turn changes after barely any silence (.", "labels": [], "entities": []}, {"text": "Therefore, such models can result in systems that interrupt the user or are perceived as unresponsive.", "labels": [], "entities": []}, {"text": "Related to the problem of turn-taking is that of backchannels.", "labels": [], "entities": []}, {"text": "Backchannel feedback -short acknowledgements such as uhhuh or mm-hm -are used by human interlocutors to signal continued attention to the speaker, without claiming the floor.", "labels": [], "entities": [{"text": "mm-hm", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.8432320952415466}]}, {"text": "If a dialogue system should be able to manage smooth turn-taking and back-channelling, it must be able to first identify suitable locations in the user's speech to do so. found that human interlocutors continuously monitor several cues, such as content, syntax, intonation, paralanguage, and body motion, in parallel to manage turn-taking.", "labels": [], "entities": []}, {"text": "Similar observations have been made in various other studies investigating the turn-taking and backchannelling phenomena inhuman conversations. has suggested that a low pitch region is a good cue that backchannel feedback is appropriate.", "labels": [], "entities": []}, {"text": "On the other hand, have argued that both syntactic and prosodic features make significant contributions in identifying turn-taking and back-channelling relevant places.", "labels": [], "entities": []}, {"text": "have shown that syntax in combination with pause duration is a strong predictor for backchannel continuers.", "labels": [], "entities": []}, {"text": "observed that the likelihood of occurrence of a backchannel increases with the number of syntactic and prosodic cues conjointly displayed by the speaker.", "labels": [], "entities": []}, {"text": "However, there is a general lack of studies on how such models could be used online in dialogue systems and to what extent that would improve the interaction.", "labels": [], "entities": []}, {"text": "There are two main problems in doing so.", "labels": [], "entities": []}, {"text": "First, the data used in the studies mentioned above are from human-human dialogue and it is not obvious to what extent the models derived from such data transfers to human-machine dialogue.", "labels": [], "entities": []}, {"text": "Second, many of the features used were manually extracted.", "labels": [], "entities": []}, {"text": "This is especially true for the transcription of utterances, but several studies also rely on manually annotated prosodic features.", "labels": [], "entities": [{"text": "transcription of utterances", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.8772978782653809}]}, {"text": "In this paper, we present a data-driven model of what we call Response Location Detection (RLD), which is fully online.", "labels": [], "entities": [{"text": "Response Location Detection (RLD)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.7897588411966959}]}, {"text": "Thus, it only relies on automatically extractable features-covering syntax, prosody and context.", "labels": [], "entities": []}, {"text": "The model has been trained on human-machine dialogue data and has been implemented in a dialogue system that is in turn evaluated with users.", "labels": [], "entities": []}, {"text": "The setting is that of a Map Task, where the user describes the route and the system may respond with for example acknowledgements and clarification requests.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the usefulness of the combined model, we have performed a user evaluation where we test the trained model in the Map Task dialogue system that was used to collect the corpus (cf. section 3).", "labels": [], "entities": []}, {"text": "A version of the dialogue system was created that uses a Random model, which makes a random choice between Respond and Hold.", "labels": [], "entities": []}, {"text": "The Random model thus approximates our majority class baseline (50.79% for Respond).", "labels": [], "entities": []}, {"text": "Another version of the system used the Trained model -our data-driven model -to make the decision.", "labels": [], "entities": []}, {"text": "For both models, if the decision was a Hold, the system waited 1.5 seconds and then responded anyway if no more speech was detected from the user.", "labels": [], "entities": []}, {"text": "We hypothesize that since the Random model makes random choices, it is likely to produce false-positive responses (resulting in overlap in interaction) as well as false-negative responses (resulting in gap/delayed response) in equal proportion.", "labels": [], "entities": []}, {"text": "The Trained model on the other hand would produce fewer overlaps and gaps.", "labels": [], "entities": []}, {"text": "In order to evaluate the models, 8 subjects (2 female, 6 male) were asked to perform the Map Task with the two systems.", "labels": [], "entities": []}, {"text": "Each subject performed five dialogues (which included 1 trial and 2 tests) with each version of the system.", "labels": [], "entities": []}, {"text": "This resulted in 16 test dialogues each for the two systems.", "labels": [], "entities": []}, {"text": "The trial session was used to allow the users to familiarize themselves with the dialogue system.", "labels": [], "entities": []}, {"text": "Also, the audio recording of the users' speech from this session was used to normalize the user pitch and intensity for the online prosodic extraction.", "labels": [], "entities": [{"text": "prosodic extraction", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.6980246007442474}]}, {"text": "The order in which the systems and maps were presented to the subjects was varied over the subjects to avoid any ordering effect in the analysis.", "labels": [], "entities": []}, {"text": "The 32 dialogues from the user evaluation were, on average, 1.7 min long (SD = 0.5 min).", "labels": [], "entities": [{"text": "SD", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9518770575523376}]}, {"text": "The duration of the interactions with the Random and the Trained model were not significantly different.", "labels": [], "entities": []}, {"text": "A total of 557 IPUs were classified by the Random model whereas the Trained model classified 544 IPUs.", "labels": [], "entities": []}, {"text": "While the Trained model classified 57.7% of the IPUs as Respond type the Random model classified only 48.29% of the total IPUs as Respond type, suggesting that the Random model was somewhat quieter.", "labels": [], "entities": []}, {"text": "It turned out that it was very hard for the subjects to perform the Map Task and at the same time make a valid subjective comparison between the two versions of the system, as we had initially intended.", "labels": [], "entities": []}, {"text": "Therefore, we instead conducted another subjective evaluation to compare the two systems.", "labels": [], "entities": []}, {"text": "We asked subjects to listen to the interactions and press a key whenever a system response was either lacking or inappropriate.", "labels": [], "entities": []}, {"text": "The subjects were asked not to consider how the system actually responded, only evaluate the timing of the response.", "labels": [], "entities": [{"text": "timing", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9538617134094238}]}, {"text": "Eight users participated in this subjective judgment task.", "labels": [], "entities": []}, {"text": "Although five of these were from the same set of users who had performed the Map Task, none of them got to judge their own interactions.", "labels": [], "entities": []}, {"text": "The judges listened to the Map Task interactions in the same order as the users had interacted, including the trial session.", "labels": [], "entities": []}, {"text": "Whereas it had been hard for the subjects who participated in the dialogues to characterize the two versions of the system, almost all of the judges could clearly tell the two versions apart.", "labels": [], "entities": []}, {"text": "They stated that the Trained system provided fora smooth flow of dialogue.", "labels": [], "entities": []}, {"text": "The timing of the IPUs was aligned with the timing of the judges' keypresses in order to measure the numbers of IPUs that had been given inappropriate response decisions.", "labels": [], "entities": [{"text": "timing", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9555580615997314}, {"text": "IPUs", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.7410972714424133}, {"text": "timing", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9807643294334412}]}, {"text": "The results show that for the Random model, 26.75% of the RLD decisions were perceived as inappropriate, whereas only 11.39% of the RLD decisions for the Trained model were perceived inappropriate.", "labels": [], "entities": []}, {"text": "A two-tailed twosample t-test for difference in mean of the fraction of inappropriate instances (key-press count divided by IPU count) for Random and Trained model show a clear significant difference (t = 4.66, dF = 30, p < 0.001).", "labels": [], "entities": []}, {"text": "We have not yet analysed whether judges penalized false-positives or false-negatives to a larger extent, this is left to future work.", "labels": [], "entities": []}, {"text": "However, some judges informed us that they did not penalize delayed response (false-negative), as the system eventually responded after a delay.", "labels": [], "entities": []}, {"text": "In the context of a system trying to follow a route description, such delays could sometimes be expected and wouldn't be unnatural.", "labels": [], "entities": []}, {"text": "For other types of interactions (such as story-telling), such delays may on the other hand be perceived as unresponsive.", "labels": [], "entities": []}, {"text": "Thus, the balance between falsepositives and false-negatives might need to be tuned depending on the topic of the conversation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentage accuracy of prosodic features  in detecting response locations", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9229751229286194}]}, {"text": " Table 3: Percentage accuracy of syntactic features  in detecting response locations", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9386777281761169}]}, {"text": " Table 4: Percentage accuracy of contextual features  in detecting response locations", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9089085459709167}]}, {"text": " Table 5: Percentage accuracy of combined models", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9512754082679749}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9097177386283875}]}, {"text": " Table 6: Precision and Recall scores of the NB and  the SVM learners trained on combined prosodic, con- textual and syntactic features.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985736608505249}, {"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9968383312225342}, {"text": "NB", "start_pos": 45, "end_pos": 47, "type": "DATASET", "confidence": 0.9073159694671631}]}]}