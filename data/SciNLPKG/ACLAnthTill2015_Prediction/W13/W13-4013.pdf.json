{"title": [{"text": "Will my Spoken Dialogue System be a Slow Learner ?", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a practical methodology for the integration of reinforcement learning during the design of a Spoken Dialogue System (SDS).", "labels": [], "entities": [{"text": "Spoken Dialogue System (SDS)", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.6308440814415613}]}, {"text": "It proposes a method that enables SDS designers to know, in advance, the number of dialogues that their system will need in order to learn the value of each state-action couple.", "labels": [], "entities": []}, {"text": "We ask the designer to provide a user model in a simple way.", "labels": [], "entities": []}, {"text": "Then, we run simulations with this model and we compute confidence intervals for the mean of the expected return of the state-action couples.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Dialogue Manager (DM) of a Spoken Dialogue System (SDS) selects actions according to its current beliefs concerning the state of the dialogue.", "labels": [], "entities": []}, {"text": "Reinforcement Learning (RL) has been more and more used for the optimisation of dialogue management, freeing designers from having to fully implement the strategy of the DM.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6021746575832367}, {"text": "dialogue management", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7830002009868622}]}, {"text": "A framework known as Module-Variable Decision Process (MVDP) was proposed by who integrated RL into an automaton-based DM.", "labels": [], "entities": []}, {"text": "This led to the deployment of the first commercial SDS implementing RL (.", "labels": [], "entities": []}, {"text": "Our work intends to continue this effort in bridging the gap between research advances on RL-based SDS and industrial release.", "labels": [], "entities": [{"text": "RL-based SDS", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.7869009971618652}]}, {"text": "One important issue concerning the design of an RL-based SDS is that it is difficult to evaluate the number of training dialogues that will be necessary for the system to learn an optimal behaviour.", "labels": [], "entities": [{"text": "RL-based SDS", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.904349684715271}]}, {"text": "The underlying mathematical problem is the estimation of the training sample size needed by the RL algorithm for convergence.", "labels": [], "entities": []}, {"text": "Yet, designers are often not experts in RL.", "labels": [], "entities": [{"text": "RL", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.959111750125885}]}, {"text": "Therefore, this paper presents a simple methodology for evaluating the necessary sample size for an RL algorithm embedded into an SDS.", "labels": [], "entities": []}, {"text": "This methodology does not require any RL expertise from designers.", "labels": [], "entities": [{"text": "RL", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9457826614379883}]}, {"text": "The latter are asked to provide a model of user behaviour in a simple way.", "labels": [], "entities": []}, {"text": "According to this model, numerous simulations are run and the sample size for each module-state-action triple of the DM is estimated.", "labels": [], "entities": []}, {"text": "This methodology was tested on an SDS designed during the CLASSiC European project) and we show that these computations are robust to varying models of user behaviour.", "labels": [], "entities": [{"text": "CLASSiC European project", "start_pos": 58, "end_pos": 82, "type": "DATASET", "confidence": 0.8881205320358276}]}], "datasetContent": [], "tableCaptions": []}