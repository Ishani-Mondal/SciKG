{"title": [{"text": "Collapsed Variational Bayesian Inference for PCFGs", "labels": [], "entities": [{"text": "Collapsed Variational Bayesian Inference", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6385899111628532}, {"text": "PCFGs", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.5368303060531616}]}], "abstractContent": [{"text": "This paper presents a collapsed variational Bayesian inference algorithm for PCFGs that has the advantages of two dominant Bayesian training algorithms for PCFGs, namely variational Bayesian inference and Markov chain Monte Carlo.", "labels": [], "entities": []}, {"text": "In three kinds of experiments, we illustrate that our algorithm achieves close performance to the Hastings sampling algorithm while using an order of magnitude less training time; and outperforms the standard variational Bayesian inference and the EM algorithms with similar training time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic context-free grammars (PCFGs) are commonly used in parsing and grammar induction systems).", "labels": [], "entities": []}, {"text": "The traditional method for estimating the parameters of PCFGs from terminal strings is the inside-outside (IO) algorithm.", "labels": [], "entities": []}, {"text": "As a special instance of the Expectation-Maximization (EM) algorithm, based on the principle of maximum-likelihood estimation (MLE), the standard IO algorithm learns relatively uniform probability distributions for grammars, while the true distributions can be highly skewed.", "labels": [], "entities": []}, {"text": "In order to encourage sparse grammars and avoid overfitting, recent research for training PCFGs has drifted away from MLE in favor of Bayesian inference algorithms that make either deterministic or stochastic approximations (.", "labels": [], "entities": []}, {"text": "Variational Bayesian inference (VB) () for PCFGs extends EM and places no constraints when updating parameters in the M step.", "labels": [], "entities": [{"text": "EM", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.6145538091659546}]}, {"text": "By minimising the divergence between the true posterior and an approximate one in which the strong dependencies between the parameters and latent variables are broken, this deterministic algorithm efficiently converges to an inaccurate and only locally optimal solution like EM.", "labels": [], "entities": []}, {"text": "Alternatively, proposed two Markov Chain Monte Carlo algorithms for PCFGs that can reach the true posterior after convergence.", "labels": [], "entities": []}, {"text": "However, it is often difficult to diagnose a sampler's convergence, and mixing is notoriously slow for distributions with tightly coupled hidden variables such as PCFGs, especially when the data sets are large.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 163, "end_pos": 168, "type": "DATASET", "confidence": 0.913955569267273}]}, {"text": "Therefore, there remains a challenge for more efficient, but also accurate and deterministic inference algorithms for PCFGs.", "labels": [], "entities": []}, {"text": "In this paper, we present a collapsed variational Bayesian inference (CVB) algorithm for PCFGs.", "labels": [], "entities": []}, {"text": "It has the same computational complexity as the standard variational Bayesian inference, but offers almost the same performance as the stochastic algorithms due to its weak assumptions.", "labels": [], "entities": []}, {"text": "The idea of operating VB in the collapsed space was proposed by and, and it was successfully applied to \"bag-of-words\" models such as latent Dirichlet allocation (LDA)) and mixture of, where the latent variables are conditionally independent given the parameters.", "labels": [], "entities": []}, {"text": "By combining the CVB idea and the dynamic programming techniques used in structurally dependent models, we deliver a both efficient and accurate algorithm for training PCFGs and other structured natural language models.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "We begin with the Bayesian models of PCFGs, and relate the existing training algorithms.", "labels": [], "entities": []}, {"text": "Section 3 introduces collapsed variational Bayesian inference for \"bag-of-words\" models (defined in Section 3.1).", "labels": [], "entities": []}, {"text": "We discuss the difficulty in applying such inference to structured models, followed by an approximate CVB algorithm for PCFGs.", "labels": [], "entities": []}, {"text": "An alternative approach is also included in brief.", "labels": [], "entities": []}, {"text": "In Section 4, we validate our CVB algorithm in three simple experiments.", "labels": [], "entities": []}, {"text": "They are inferring a sparse grammar that describes the morphology of the Sotho language (, unsupervised dependency parsing () and supervised parsing with latent annotations ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.6872424483299255}]}, {"text": "Section 5 concludes with future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct three simple experiments to validate our CVB algorithm for PCFGs.", "labels": [], "entities": []}, {"text": "In Section 4.1, we illustrate the significantly reduced training time of our CVB algorithm compared to the related Hastings algorithm; whereas in later two sections, we demonstrate the increased performance of our CVB algorithm compared to the corresponding VB and EM algorithms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: PCFG-LA (2 subtypes) trained by EM,  VB and CVB. Precision, Recall, F1 scores, Exact  match scores on section 23, WSJ.", "labels": [], "entities": [{"text": "PCFG-LA", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.8817870616912842}, {"text": "EM", "start_pos": 42, "end_pos": 44, "type": "DATASET", "confidence": 0.6473947167396545}, {"text": "VB", "start_pos": 47, "end_pos": 49, "type": "DATASET", "confidence": 0.7520678043365479}, {"text": "Precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9964938759803772}, {"text": "Recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9868406057357788}, {"text": "F1 scores", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9763956367969513}, {"text": "Exact  match scores", "start_pos": 89, "end_pos": 108, "type": "METRIC", "confidence": 0.8919663627942404}, {"text": "WSJ", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.8459382653236389}]}]}