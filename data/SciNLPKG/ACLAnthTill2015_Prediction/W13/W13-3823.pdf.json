{"title": [{"text": "Alternative measures of word relatedness in distributional semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an alternative method to measuring word-word semantic relat-edness in distributional semantics framework.", "labels": [], "entities": []}, {"text": "The main idea is to represent target words as rankings of all co-occurring words in a text corpus, ordered by their tf-idf weight and use a metric between rankings (such as Jaro distance or Rank distance) to compute semantic relatedness.", "labels": [], "entities": []}, {"text": "This method has several advantages over the standard approach that uses cosine measure in a vector space, mainly in that it is computationally less expensive (i.e. does not require working in a high dimensional space, employing only rankings and a distance which is linear in the rank's length) and presumably more robust.", "labels": [], "entities": []}, {"text": "We tested this method on the standard WS-353 Test, obtaining the co-occurrence frequency from the Wacky corpus.", "labels": [], "entities": [{"text": "WS-353 Test", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.8825080394744873}, {"text": "Wacky corpus", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9707393944263458}]}, {"text": "The results are comparable to the methods which use vector space models; and, most importantly , the method can be extended to the very challenging task of measuring phrase semantic relatedness.", "labels": [], "entities": [{"text": "measuring phrase semantic relatedness", "start_pos": 156, "end_pos": 193, "type": "TASK", "confidence": 0.6404127553105354}]}], "introductionContent": [{"text": "This paper presents a method of measuring wordword semantic relatedness in the distributional semantics (DS) framework.", "labels": [], "entities": [{"text": "wordword semantic relatedness", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7720833222071329}]}, {"text": "DS relies on a usage-based perspective on meaning, assuming that the statistical distribution of words in context plays a key role in characterizing their semantic behavior.", "labels": [], "entities": []}, {"text": "The idea that word cooccurrence statistics extracted from text corpora can provide a basis for semantic representations can be traced back at least to: \"You shall know a word by the company it keeps\" and Harris: \"words that occur in similar contexts tend to have similar meanings\".", "labels": [], "entities": []}, {"text": "This view is complementary to the formal semantics perspective, focusing on the meaning of content words, (such as nouns, adjectives, verbs or adverbs) and not on grammatical words (prepositions, auxiliary verbs, pronouns, quantifiers, coordination, negation), which are the focus of formal semantics.", "labels": [], "entities": []}, {"text": "Since many semantic issues come from the lexicon of content words and not from grammatical terms, DS offers semantical insight into problems that cannot be addressed by formal semantics.", "labels": [], "entities": []}, {"text": "Moreover, DS Models can be induced fully automatically on a large scale, from corpus data.", "labels": [], "entities": []}, {"text": "Thus, a word maybe represented by a vector in which the elements are derived from the occurrences of the word in various contexts, such as windows of words, grammatical dependencies, and richer contexts consisting of dependency links and selectional preferences on the argument positions.", "labels": [], "entities": []}, {"text": "The task of measuring word-word relatedness was previously performed in DS by using vector space models (see for an excellent survey of vector-space models), that is employing high dimensional matrices to store cooccurrence frequency of target words and some set of dimension words, usually highly frequent (but not grammatical) words.", "labels": [], "entities": []}, {"text": "The relatedness of two target words was typically given by the cosine of the angle between their vectors.", "labels": [], "entities": []}, {"text": "Instead of using vector space models, we propose to represent the target words only by rankings (vectors) of words in their decreasing order of co-occurrence frequency or their tf -idf weight.", "labels": [], "entities": []}, {"text": "The tf -idf weight increases with the number of co-occurrences and with the \"selectiveness\" of the term -the fewer distinct words it occurs with, the higher the weight.", "labels": [], "entities": []}, {"text": "This proposal has some advantages, as discussed in Approach section.", "labels": [], "entities": [{"text": "Approach", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.7799414992332458}]}, {"text": "We can measure the semantic relatedness between two target words by computing the distance between the two cor-responding rankings, using distances defined on rankings.", "labels": [], "entities": []}, {"text": "In the remaining of the paper we will present our approach, describe the data we have used, compare the results and draw the conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison with vector space experi- ments for WS-", "labels": [], "entities": []}]}