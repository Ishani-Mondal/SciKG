{"title": [{"text": "Sentence paraphrase detection: When determiners and word order make the difference", "labels": [], "entities": [{"text": "Sentence paraphrase detection", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9590776562690735}]}], "abstractContent": [{"text": "Researchers working on distributional semantics have recently taken up the challenge of going beyond lexical meaning and tackle the issue of compositionality.", "labels": [], "entities": []}, {"text": "Several Compositional Distribu-tional Semantics Models (CDSMs) have been developed and promising results have been obtained in evaluations carried out against data sets of small phrases and as well as data sets of sentences.", "labels": [], "entities": []}, {"text": "However , we believe there is the need to further develop good evaluation tasks that show whether CDSM truly capture compositionality.", "labels": [], "entities": []}, {"text": "To this end, we present an evaluation task that highlights some differences among the CDSMs currently available by challenging them in detecting semantic differences caused byword order switch and by determiner replacements.", "labels": [], "entities": [{"text": "determiner replacements", "start_pos": 200, "end_pos": 223, "type": "TASK", "confidence": 0.732292890548706}]}, {"text": "We take as starting point simple intransitive and transitive sentences describing similar events, that we consider to be paraphrases of each other but not of the foil paraphrases we generate from them.", "labels": [], "entities": []}, {"text": "Only the models sensitive to word order and determiner phrase meaning and their role in the sentence composition will not be captured into the foils' trap.", "labels": [], "entities": [{"text": "determiner phrase meaning", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6330429216225942}]}], "introductionContent": [{"text": "Distributional semantics models (DSMs) have recently taken the challenge to move up from lexical to compositional semantics.", "labels": [], "entities": []}, {"text": "Through many years of almost exclusive focus on lexical semantics, many data sets have been developed to properly evaluate which aspects of lexical meaning and lexical relations are captured by DSMs.", "labels": [], "entities": []}, {"text": "For instance, DSMs have been shown to obtain high performance in simulating semantic priming, predicting semantic similarity) and association ( and have been shown to achieve human level performance on synonymy tests such as those included in the Test of English as a Foreign Language (TOEFL).", "labels": [], "entities": [{"text": "predicting semantic similarity", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.8333198030789694}]}, {"text": "Compositional DSMs (CDSMs) are of more recent birth, and thus their proponents have focused effort on the study of the compositional operations that are mathematically available and empirically justifiable in vector-space models.", "labels": [], "entities": [{"text": "Compositional DSMs (CDSMs)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7550449728965759}]}, {"text": "Important progress has been made and several models have now been implemented ranging from the additive and multiplicative models of, to functional models based on tensor contraction, to the one based on recursive neural networks of.", "labels": [], "entities": []}, {"text": "We believe it is now necessary to shift focus somewhat to the semantic tasks against which to evaluate these models, and to develop appropriate data sets to better understand which aspects of natural language compositionality we are already capturing, what could still be achieved and what might be beyond the scope of this framework.", "labels": [], "entities": []}, {"text": "This paper tries to contribute to this new effort.", "labels": [], "entities": []}, {"text": "To this end, we start by looking at data sets of phrases and sentences used so far to evaluate CDSMs.", "labels": [], "entities": []}, {"text": "Word order in phrase similarity Starting from a data set of pairs of noun-noun, verb-noun and adjective-noun phrases (e.g., certain circumstance and particular case) rated by humans with respect to similarity, obtains an extended version including word order variations of the original phrases, which are automatically judged to have a very low similarity (e.g., certain circumstance and case particular).", "labels": [], "entities": [{"text": "Word order in phrase similarity", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5897020041942597}]}, {"text": "Sentence similarity: Intransitive Sentences One of the first proposals to look at verb-argument composition traces back to who was interested in capturing the different verb senses activated by different arguments, e.g., \"The color ran\" vs. \"The horse ran\", but the model was tested only on a few sentences.", "labels": [], "entities": [{"text": "Sentence similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8366808295249939}]}, {"text": "Starting from this work, made an important step forward by developing a larger data set of subject+intransitive-verb sentences.", "labels": [], "entities": []}, {"text": "They began with frequent noun-verb tuples (e.g., horse ran) extracted from the British National Corpus (BNC) and paired them with sentences with two synonyms of the verb, representing distinct verb senses, one compatible and the other incompatible with the argument (e.g., horse galloped and horse dissolved).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 79, "end_pos": 108, "type": "DATASET", "confidence": 0.9673080444335938}]}, {"text": "The tuples were converted into simple sentences (in past tense form) and articles were added to nouns when appropriate.", "labels": [], "entities": []}, {"text": "The final data set consists of 120 sentences with 15 original verbs each composed with two subject nouns and paired with two synonyms.", "labels": [], "entities": []}, {"text": "Sentence pair similarity was rated by 49 volunteers on the web.", "labels": [], "entities": [{"text": "similarity", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.4976486265659332}]}, {"text": "Sentence similarity: Transitive Sentences Following the method proposed in, developed an analogous data set of transitive sentences.", "labels": [], "entities": [{"text": "Sentence similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9001388251781464}]}, {"text": "Again the focus is on how arguments (subjects and objects) influence the selection of the meaning of an ambiguous verb.", "labels": [], "entities": []}, {"text": "For instance, meet is synonymous both of satisfy and visit.", "labels": [], "entities": []}, {"text": "For each verb (in total 10 verbs), 10 subject+transitive-verb+object tuples with the given verb were extracted from the BNC and sentences in simple past form (with articles if necessary) were generated.", "labels": [], "entities": [{"text": "BNC", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.8764615058898926}]}, {"text": "For example, starting from met, the two sentences \"The system met the criterion\" and \"The child met the house\" were generated.", "labels": [], "entities": []}, {"text": "For each sentence, two new versions were created by replacing the verb with two synonyms representing two verb senses (e.g., \"The system visited the criterion\" and \"The system satisfied the criterion\").", "labels": [], "entities": []}, {"text": "The data set consists of 200 pairs of sentences annotated with human similarity judgments.", "labels": [], "entities": []}, {"text": "Large-scale full sentence paraphrasing data and tackle the challenging task of evaluating CDSMs against large-scale full sentence data.", "labels": [], "entities": []}, {"text": "They use the Microsoft Research Paraphrase Corpus () as data set.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus", "start_pos": 13, "end_pos": 49, "type": "DATASET", "confidence": 0.9321148693561554}]}, {"text": "The corpus consists of 5800 pairs of sentences extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship.", "labels": [], "entities": []}, {"text": "The evaluation experiments conducted against these data sets seem to support the following conclusions: \u2022 \"the model should be sensitive to the order of the words in a phrase (for composition) or a word pair (for relations), when the order affects the meaning.\"", "labels": [], "entities": []}, {"text": "\u2022 \"experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments [about sentence similarity].\"", "labels": [], "entities": []}, {"text": "\u2022 \"shallow approaches are as good as more computationally intensive alternatives [in sentence paraphrase detection].", "labels": [], "entities": [{"text": "sentence paraphrase detection", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.7810697356859843}]}, {"text": "They achieve considerable semantic expressivity without any learning, sophisticated linguistic processing, or access to very large corpora.\"", "labels": [], "entities": []}, {"text": "With this paper, we want to put these conclusions on stand-by by asking the question of whether the appropriate tasks have really been tackled.", "labels": [], "entities": []}, {"text": "The first conclusion above regarding word order is largely shared, but still no evaluation of CDSMs against sentence similarity considers word order seriously.", "labels": [], "entities": []}, {"text": "We do not exclude that in real-world tasks systems which ignore word order may still attain satisfactory results (as the results of Blacoe and Lapata 2012 suggest), but this will not be evidence of having truly captured compositionality.", "labels": [], "entities": []}, {"text": "Moreover, a hidden conclusion (or, better, assumption!) of the evaluations conducted so far on CDSMs seems to be that grammatical words, in particular determiners, play no role in sentence meaning and hence sentence similarity and paraphrase detection.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.6955847889184952}, {"text": "paraphrase detection", "start_pos": 231, "end_pos": 251, "type": "TASK", "confidence": 0.8730846047401428}]}, {"text": "A first study on this class of words has been presented in where it is shown that DSMs can indeed capture determiner meaning and their role in the entailment between quantifier phrases.", "labels": [], "entities": []}, {"text": "The data sets used in and Grefenstette and Sadrzadeh (2011b) focus on verb meaning and its sense disambiguation within context, and consider sentences where determiners are just place-holders to simply guarantee grammaticality, but do not play any role neither in the human judgments nor in the model evaluation -in which they are simply ignored.", "labels": [], "entities": []}, {"text": "Similarly, evaluate the compositional models on full sentences but again ignore the role of grammatical words that are treated as \"stop-words\".", "labels": [], "entities": []}, {"text": "Should we conclude that from a distributional semantic view \"The system met the criterion\", \"No system met the criterion\" and \"Neither system met the criterion\" boil down to the same meaning?", "labels": [], "entities": []}, {"text": "This is a conclusion we cannot exclude but neither accept a-priori.", "labels": [], "entities": []}, {"text": "To start considering these questions more seriously, we built a data set of intransitive and transitive sentences in which word order and determiners have the chance to prove their worth in sentence similarity and paraphrase detection.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 214, "end_pos": 234, "type": "TASK", "confidence": 0.8613375723361969}]}], "datasetContent": [{"text": "We have carried out two experiments.", "labels": [], "entities": []}, {"text": "The first is a classic paraphrase detection task, in which CDSMs have to automatically cluster the sentences from the paraphrase set into the ground-truth groups.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.9020373225212097}]}, {"text": "The second one aims to highlight when the possible good performance in the paraphrase detection task does correspond to true modelling of compositionality, that should be sensitive to word order and disruptive changes in the determiner.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.909177819887797}]}, {"text": "Clustering We have carried out this experiment against the paraphrase set.", "labels": [], "entities": []}, {"text": "We used the standard globally-optimized repeated bisecting method as implemented in the widely used CLUTO toolkit, using cosines as distance functions, and accepting all of CLUTO's default values.", "labels": [], "entities": []}, {"text": "Performance is measured by purity, one of the standard clustering quality measures returned by CLUTO (.", "labels": [], "entities": [{"text": "purity", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9867215156555176}, {"text": "CLUTO", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.9135701656341553}]}, {"text": "If n i r is the number of items from the i-th true (gold standard) group that were assigned to the r-th cluster, n is the total number of items and k the number of clusters, then: In the case of perfect clusters, purity will be of 100%; as cluster quality deteriorates, purity approaches 0.", "labels": [], "entities": [{"text": "purity", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.9950645565986633}, {"text": "purity", "start_pos": 270, "end_pos": 276, "type": "METRIC", "confidence": 0.9941768646240234}]}, {"text": "Sentence similarity to paraphrases vs. foils The idea of the second experiment is to measure the similarity of each sentence in the paraphrase set to all other sentences in the same group (i.e., valid paraphrases, P), as well as to sentences in the corresponding foil set (FP).", "labels": [], "entities": []}, {"text": "For each sentence in a P group, we computed the mean of the cosine of the sentence with all the sentences in the same ground-truth group (with all the P sentences) (cos.para) and the mean of the cosine with all the foil paraphrases (with all the FP sentences, viz.", "labels": [], "entities": []}, {"text": "those marked by S, D, SD) of the same group (cos.foil).", "labels": [], "entities": []}, {"text": "Then, we computed the difference between cos.para and cos.foil (diff.para.foil= cos.paracos.foil).", "labels": [], "entities": []}, {"text": "Finally, we computed the mean of diff.para.foil for all the sentences in the data set.", "labels": [], "entities": [{"text": "mean", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9559277296066284}]}, {"text": "Models which achieve higher values are those that are not captured by the foils' trap, since they are able to distinguish paraphrases from their foils: Only a model that realizes that A man plays an instrument is a better paraphrase of A man plays guitar than either A guitar plays a manor The man plays no guitar can be said to truly catch compositional meaning, beyond simple word meaning overlap.", "labels": [], "entities": []}, {"text": "To focus more specifically on word order, we will report the same analysis also when considering only the scrambled sentences as foils: diff.para.scrambled=cos.para -cos.scrambled, where the latter are means of the cosine of the paraphrase with all the scrambled sentences (with all the sentences marked by S) of the same group (with no manipulation of the determiners).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experiment results (mean diff.para.foil and diff.para.scrambled values fol- lowed by standard deviations)", "labels": [], "entities": []}]}