{"title": [{"text": "Online Polylingual Topic Models for Fast Document Translation Detection", "labels": [], "entities": [{"text": "Fast Document Translation Detection", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.7347036823630333}]}], "abstractContent": [{"text": "Many tasks in NLP and IR require efficient document similarity computations.", "labels": [], "entities": [{"text": "IR", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9510799646377563}, {"text": "document similarity computations", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.7246675491333008}]}, {"text": "Beyond their common application to exploratory data analysis, latent variable topic models have been used to represent text in a low-dimensional space, independent of vocabulary, where documents maybe compared.", "labels": [], "entities": [{"text": "exploratory data analysis", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6477239231268564}]}, {"text": "This paper focuses on the task of searching a large multilingual collection for pairs of documents that are translations of each other.", "labels": [], "entities": []}, {"text": "We present (1) efficient, online inference for representing documents in several languages in a common topic space and (2) fast approximations for finding near neighbors in the probability simplex.", "labels": [], "entities": []}, {"text": "Empirical evaluations show that these methods are as accurate as-and significantly faster than-Gibbs sampling and brute-force all-pairs search.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical topic models, such as latent Dirichlet allocation (LDA) ( , have proven to be highly effective at discovering hidden structure in document collections (, e.g.).", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.638420045375824}]}, {"text": "Often, these models facilitate exploratory data analysis, by revealing which collocations of terms are favored in different kinds of documents or which terms and topics rise and fall overtime (;).", "labels": [], "entities": []}, {"text": "One of the greatest advantages in using topic models to analyze and process large document collections is their ability to represent documents as probability distributions over a small number of topics, thereby mapping documents into a low-dimensional latent space-the T -dimensional probability simplex, where T is the number of topics.", "labels": [], "entities": []}, {"text": "A document, represented by some point in this simplex, is said to have a particular \"topic distribution\".", "labels": [], "entities": []}, {"text": "Representing documents as points in a lowdimensional shared latent space abstracts away from the specific words used in each document, thereby facilitating the analysis of relationships between documents written using different vocabularies.", "labels": [], "entities": []}, {"text": "For instance, topic models have been used to identify scientific communities working on related problems in different disciplines, e.g., work on cancer funded by multiple Institutes within the NIH).", "labels": [], "entities": []}, {"text": "While vocabulary mismatch occurs within the realm of one language, naturally this mismatch occurs across different languages.", "labels": [], "entities": []}, {"text": "Therefore, mapping documents in different languages into a common latent topic space can be of great benefit when detecting document translation pairs (.", "labels": [], "entities": [{"text": "detecting document translation pairs", "start_pos": 114, "end_pos": 150, "type": "TASK", "confidence": 0.7779318541288376}]}, {"text": "Aside from the benefits that it offers in the task of detecting document translation pairs, topic models offer potential benefits to the task of creating translation lexica, aligning passages, etc.", "labels": [], "entities": [{"text": "detecting document translation pairs", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.8605591803789139}]}, {"text": "The process of discovering relationship between documents using topic models involves: (1) representing documents in the latent space by inferring their topic distributions and (2) comparing pairs of topic distributions to find close matches.", "labels": [], "entities": []}, {"text": "Many widely used techniques do not scale efficiently, however, as the size of the document collection grows.", "labels": [], "entities": []}, {"text": "Posterior inference by Gibbs sampling, for instance, may make thousands of passes through the data.", "labels": [], "entities": []}, {"text": "For the task of comparing topic distributions, recent work has also resorted to comparing all pairs of documents.", "labels": [], "entities": []}, {"text": "This paper presents efficient methods for both of these steps and performs empirical evaluations on the task of detected translated document pairs embedded in a large multilingual corpus.", "labels": [], "entities": []}, {"text": "Unlike some more exploratory applications of topic models, translation detection is easy to evaluate.", "labels": [], "entities": [{"text": "translation detection", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.9931715726852417}]}, {"text": "The need for bilingual training data in many language pairs and domains also makes it attractive to mitigate the quadratic runtime of brute force translation detection.", "labels": [], "entities": [{"text": "brute force translation detection", "start_pos": 134, "end_pos": 167, "type": "TASK", "confidence": 0.7720533758401871}]}, {"text": "We begin in \u00a72 by extending the online variational Bayes approach of to polylingual topic models ().", "labels": [], "entities": []}, {"text": "Then, in \u00a73, we build on prior work on efficient approximations to the nearest neighbor problem by presenting theoretical and empirical evidence for applicability to topic distributions in the probability simplex and in \u00a74, we evaluate the combination of online variational Bayes and approximate nearest neighbor methods on the translation detection task.", "labels": [], "entities": [{"text": "translation detection task", "start_pos": 328, "end_pos": 354, "type": "TASK", "confidence": 0.9643362959225973}]}], "datasetContent": [{"text": "We use Mallet's) implementation of the PLTM to train and infer topics on the same data set used in.", "labels": [], "entities": [{"text": "PLTM", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7312777042388916}]}, {"text": "That paper used the Europarl () multilingual collection of English and Spanish sessions.", "labels": [], "entities": [{"text": "Europarl () multilingual collection of English and Spanish sessions", "start_pos": 20, "end_pos": 87, "type": "DATASET", "confidence": 0.9202054474088881}]}, {"text": "Their training collection consists of speeches extracted from all Europarl sessions from the years 1996 through 1999 and the year 2002 and a development set which consists of speeches from sessions in 2001.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9476173520088196}]}, {"text": "The test collection consists of Europarl speeches from the year 2000 and the first nine months of 2003.", "labels": [], "entities": [{"text": "Europarl speeches", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.9514569342136383}]}, {"text": "While do offer absolute performance comparison between their JPLSA approach and previous results published by), these performance comparisons are not done on the same training and test sets-a gap that we fill below.", "labels": [], "entities": [{"text": "JPLSA", "start_pos": 61, "end_pos": 66, "type": "TASK", "confidence": 0.8179630637168884}]}, {"text": "We train PLTM models with number of topics T set to 50, 100, 200, and 500.", "labels": [], "entities": []}, {"text": "In order to compare exactly the same topic distributions when computing speed vs. accuracy of various approximate and exhaustive all-pairs comparisons we focus only on one inference approach -the Gibbs sampling and ignore the online VB approach as it yields similar performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9984764456748962}]}, {"text": "For all four topic models, we use the same settings for PLTM (hyperparameter values and number of Gibbs sampling iterations) as in ( 2 . Topic distributions were then inferred on the test collection using the trained topics.", "labels": [], "entities": [{"text": "PLTM", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9800316095352173}]}, {"text": "We then performed all-pairs comparison using JS divergence, Hellinger distance, and approximate, LSH and kd-trees based, Hellinger distance.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.43603184819221497}, {"text": "LSH", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9384229779243469}]}, {"text": "We measured the total time that it takes to perform exhaustive all-pairs comparison using JS divergence, the LSH and kdtrees version on a single machine consisting of a core 2 duo quad processors with a clock speed of 2.66GHz on each core and a total of 8GB of memory.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.40038639307022095}, {"text": "LSH", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.7666720151901245}]}, {"text": "Since the time performance of the E2LSH depends on the radius R of data set points considered for each query point), we performed measurements with different values of R.", "labels": [], "entities": []}, {"text": "For this task, the all-pairs JS code implementation first reads both source and target sets of documents and stores them in hash tables.", "labels": [], "entities": []}, {"text": "We then go over each entry in the source table and compute divergence against all target table entries.We refer to this code implementation as hash map implementation.", "labels": [], "entities": []}, {"text": "Performance of the four PLTM models and the performance across the four different similarity measurements was evaluated based on the percentage of document translation pairs (out of the whole test set) that were discovered at rank one.", "labels": [], "entities": []}, {"text": "This same approach was used by to show the absolute performance comparison.", "labels": [], "entities": []}, {"text": "As in the case of the previous two tasks, in order to evaluate the approximate, LSH based, Hellinger distance we used values of R=0.4, R=0.6 and R=0.8.", "labels": [], "entities": [{"text": "R", "start_pos": 145, "end_pos": 146, "type": "METRIC", "confidence": 0.931990385055542}]}, {"text": "Since in) numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection.", "labels": [], "entities": []}, {"text": "Shown in are results across the four different measurements for all four PLTM models.", "labels": [], "entities": []}, {"text": "When using regular JS divergence, our PLTM model with 200 topics performs the best with 99.42% of the top one ranked candidate translation documents being true translations.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9546867907047272}]}, {"text": "When using approximate, kd-trees based, Hellinger distance, we outperform regular JS and Hellinger divergence across all topics and for T=500 we achieve the best overall accuracy of 99.61%.", "labels": [], "entities": [{"text": "T", "start_pos": 136, "end_pos": 137, "type": "METRIC", "confidence": 0.9569918513298035}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9993246793746948}]}, {"text": "We believe that this is due to the small amount of error  in the search introduced by ANN, due to its approximate nature, which for this task yields positive results.", "labels": [], "entities": [{"text": "ANN", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.6180206537246704}]}, {"text": "On the same data set, () report accuracy of 98.9% using 50 topics, a slightly different prior distribution, and MAP instead of posterior inference.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9996542930603027}, {"text": "MAP", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9895825386047363}]}, {"text": "Shown in are the relative differences in time between all pairs JS divergence, approximate kd-trees and LSH based Hellinger distance with different value of R.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.5428872257471085}, {"text": "LSH based Hellinger distance", "start_pos": 104, "end_pos": 132, "type": "METRIC", "confidence": 0.8235794007778168}]}, {"text": "Rather than showing absolute speed numbers, which are often influenced by the processor configuration and available memory, we show relative speed improvements where we take the slowest running configuration as a referent value.", "labels": [], "entities": []}, {"text": "In our case we assign the referent speed value of 1 to the configuration with T=500 and allpairs JS computation.", "labels": [], "entities": [{"text": "T", "start_pos": 78, "end_pos": 79, "type": "METRIC", "confidence": 0.9700710773468018}, {"text": "JS", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.6839683651924133}]}, {"text": "Results shown are based on comparing running time of E2LSH and ANN against the all-pairs similarity comparison implementation that uses hash tables to store all documents in the bilingual collection which is significantly faster than the other code implementation.", "labels": [], "entities": [{"text": "E2LSH", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9021807312965393}]}, {"text": "For the approximate, LSH based, Hellinger distance with T=100 we obtain a speed improvement of 24.2 times compared to regular all-pairs JS divergence while maintaining the same performance compared to Hellinger distance metric and insignificant loss over all-pairs JS divergence.", "labels": [], "entities": [{"text": "T", "start_pos": 56, "end_pos": 57, "type": "METRIC", "confidence": 0.9946118593215942}, {"text": "JS divergence", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.8672345876693726}, {"text": "JS divergence", "start_pos": 265, "end_pos": 278, "type": "TASK", "confidence": 0.7892103791236877}]}, {"text": "From it is evident that as we increase the radius R we reduce the relative speed of performance since the range of points that LSH considers fora given query point increases.", "labels": [], "entities": []}, {"text": "Also, as the number of topics increases, the speed benefit is reduced for both the LSH and k-d tree techniques.", "labels": [], "entities": [{"text": "speed", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9926983714103699}]}], "tableCaptions": [{"text": " Table 1: Percentage of document pairs with the  correct translation discovered at rank 1: compari- son of different divergence measurements and dif- ferent numbers T of PLTM topics.", "labels": [], "entities": []}, {"text": " Table 2: Relative speed improvement between all- pairs JS divergence and approximate He diver- gence via kd-trees and LSH across different values  of radius R. The baseline is brute-force all-pairs  comparison with Jensen-Shannon and 500 topics.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7464257478713989}, {"text": "He diver- gence", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.9528938829898834}]}]}