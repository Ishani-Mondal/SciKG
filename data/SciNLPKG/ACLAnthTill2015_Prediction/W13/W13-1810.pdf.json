{"title": [{"text": "Modeling Graph Languages with Grammars Extracted via Tree Decompositions", "labels": [], "entities": [{"text": "Modeling Graph Languages with Grammars Extracted via Tree Decompositions", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.797485179371304}]}], "abstractContent": [{"text": "Work on probabilistic models of natural language tends to focus on strings and trees, but there is increasing interest in more general graph-shaped structures since they seem to be better suited for representing natural language semantics, ontologies, or other varieties of knowledge structures.", "labels": [], "entities": []}, {"text": "However , while there are relatively simple approaches to defining generative models over strings and trees, it has proven more challenging for more general graphs.", "labels": [], "entities": []}, {"text": "This paper describes a natural generalization of the n-gram to graphs, making use of Hyperedge Replacement Grammars to define genera-tive models of graph languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "While most work in natural language processing (NLP), and especially within statistical NLP, has historically focused on strings and trees, there is increasing interest in deeper graph-based analyses which could facilitate natural language understanding and generation applications.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.8114151159922282}, {"text": "natural language understanding", "start_pos": 223, "end_pos": 253, "type": "TASK", "confidence": 0.6699968576431274}]}, {"text": "Graphs have along tradition within knowledge representation, natural language semantics (, and in models of deep syntax.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7222401201725006}]}, {"text": "Graphs seem particularly appropriate for representing semantic structures, since a single concept could play multiple roles within a sentence.", "labels": [], "entities": []}, {"text": "For instance, in the semantic representation at the bottom right of lake is an argument of both rich-in and own in the sentence, \"The lake is said to be rich in fish but is privately owned.\"", "labels": [], "entities": []}, {"text": "However, work on graphs has been hampered, due, in part, to the absence of a general agreed upon formalism for processing and modeling such data structures.", "labels": [], "entities": []}, {"text": "Where string and tree modeling benefits from the wildly popular Probabilistic Context Free Grammar (PCFG) and related formalisms such as Tree Substitution Grammar, Regular Tree Grammar, Hidden Markov Models, and n-grams, there is nothing of similar popularity for graphs.", "labels": [], "entities": []}, {"text": "We need a slightly different formalism, and Hyperedge Replacement Grammar (HRG) (), a variety of context-free grammar for graphs, suggests itself as a reasonable choice given its close analogy with CFG.", "labels": [], "entities": [{"text": "Hyperedge Replacement Grammar (HRG)", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.7208898613850275}, {"text": "CFG", "start_pos": 198, "end_pos": 201, "type": "DATASET", "confidence": 0.9297281503677368}]}, {"text": "Of course, in order to make use of the formalism we need actual grammars, and this paper fills that gap by introducing a procedure for automatically extracting grammars from a corpus of graphs.", "labels": [], "entities": []}, {"text": "Grammars are appealing for the intuitive and systematic way they capture the compositionality of language.", "labels": [], "entities": []}, {"text": "For instance, just as a PCFG could be used to parse \"the lake\" as a syntactic subject, so could a graph grammar represent lake as a constituent in a parse of the corresponding semantic graph.", "labels": [], "entities": []}, {"text": "In fact, picking a formalism that is so similar to the PCFG makes it easy to adapt proven, familiar techniques for training and inference such as the inside-outside algorithm, and because HRG is context-free, parses can be represented by trees, facilitating the use of many more tools from tree automata).", "labels": [], "entities": [{"text": "PCFG", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9448379874229431}]}, {"text": "Furthermore, the operational parallelism with PCFG makes it easy to integrate graph-based systems with syntactic models in synchronous grammars.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9203066229820251}]}, {"text": "Probabilistic versions of deep syntactic models such as Lexical Functional Grammar and HPSG) are one grammar-based approach to modeling graphs represented in the form of feature structures.", "labels": [], "entities": []}, {"text": "However, these models are tied to a particular linguistic paradigm, and they are complex, requiring a great deal of effort to engineer and annotate the necessary grammars and corpora.", "labels": [], "entities": []}, {"text": "It is also not obvious how to define generative probabilistic models with such grammars, limiting their utility in certain applications.", "labels": [], "entities": []}, {"text": "In contrast, this paper describes a method of automatically extracting graph grammars from a corpus of graphs, allowing us to easily estimate rule probabilities and define generative models.", "labels": [], "entities": []}, {"text": "The class of grammars we extract generalize the types of regular string and tree grammars one might use to define a bigram or similar Markov model for trees.", "labels": [], "entities": []}, {"text": "In fact, the procedure produces regular string and tree grammars as special cases when the input graphs themselves are strings or trees.", "labels": [], "entities": []}, {"text": "There is always overhead in learning anew formalism, so we will endeavor to provide the necessary background as simply as possible, according to the following structure.", "labels": [], "entities": []}, {"text": "Section 2 introduces Hyperedge Replacement Grammars, which generate graphs, and their probabilistic extension, weighted HRGs.", "labels": [], "entities": [{"text": "Hyperedge Replacement Grammars", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.86424587170283}]}, {"text": "Section 3 explains how each HRG derivation of a graph induces a tree decomposition of that graph.", "labels": [], "entities": []}, {"text": "Given a tree decomposition of a graph, we use that mapping \"in reverse\" to induce an HRG that generates that graph (section 4).", "labels": [], "entities": []}, {"text": "Section 4 also introduces four different strategies for finding tree decompositions of (and hence inducing HRGs from) a set of graphs.", "labels": [], "entities": []}, {"text": "Section 5 applies these strategies to the LOGON corpus () and evaluates the induced weighted HRGs in terms of held-out perplexity.", "labels": [], "entities": [{"text": "LOGON corpus", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.7980968058109283}]}, {"text": "Section 6 concludes the paper and discusses possible applications and extensions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment using 5564 elementary semantic dependency graphs taken from the LOGON portion of the Redwoods corpus ().", "labels": [], "entities": [{"text": "LOGON portion of the Redwoods corpus", "start_pos": 78, "end_pos": 114, "type": "DATASET", "confidence": 0.6983881592750549}]}, {"text": "From, we can see that, while there area few tree-shaped graphs, the majority are more general DAGs.", "labels": [], "entities": []}, {"text": "Nevertheless, edge density is low; the average graph contains about 15.4 binary edges and 14.9 vertices.", "labels": [], "entities": []}, {"text": "We set aside every 10 th graph for the test set, and estimate the models from the remaining 5,008, replacing terminals occurring \u2264 1 times in the training set with special symbol UNK.", "labels": [], "entities": []}, {"text": "Model parameters are calculated from the frequency of extracted rules using a meanfield Variational Bayesian approximation of asymmetric Dirichlet prior with parameter \u03b2).", "labels": [], "entities": []}, {"text": "This amounts to counting the number of times each ruler with left-hand side symbol A is extracted and then computing its weight \u03b8 r according to where n r is the frequency of rand \u03a8 is the standard digamma function.", "labels": [], "entities": []}, {"text": "This approximation of a Dirichlet prior offers a simple yet principled way of simultaneously smoothing rule weights and incorporating a soft assumption of sparsity (i.e., only a few rules should receive very high probability).", "labels": [], "entities": []}, {"text": "Specifically, we somewhat arbitrarily selected a value of 0.2 for \u03b2, which should result in a moderately sparse distribution.", "labels": [], "entities": []}, {"text": "We evaluate each model by computing per- , where N is the number of graphs in the test set, g i is the i th graph, and p(g i ) is its probability according to the model, computed as the product of the weights of the rules in the extracted derivation.", "labels": [], "entities": []}, {"text": "Better models should assign higher probability tog i , thereby achieving lower perplexity.", "labels": [], "entities": []}, {"text": "lists the perplexities of the language models defined according to our four different tree decomposition strategies.", "labels": [], "entities": []}, {"text": "Linear is relatively poor since it makes little distinction between local and more distant relations between edges.", "labels": [], "entities": []}, {"text": "For instance, the tree in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: LOGON corpus. (a) Graph types (r  stands for rooted). (b) Edge types and tokens.", "labels": [], "entities": []}, {"text": " Table 2: Model perplexity and grammar size.", "labels": [], "entities": []}]}