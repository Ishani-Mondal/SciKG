{"title": [{"text": "Overview of the First Content Selection Challenge from Open Semantic Web Data", "labels": [], "entities": [{"text": "First Content Selection", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.720975657304128}]}], "abstractContent": [{"text": "In this overview paper we present the outcome of the first content selection challenge from open semantic web data, fo-cusing mainly on the preparatory stages for defining the task and annotating the data.", "labels": [], "entities": []}, {"text": "The task to perform was described in the challenge's call as follows: given a set of RDF triples containing facts about a celebrity, select those triples that are reflected in the target text (i.e., a short biography about that celebrity).", "labels": [], "entities": []}, {"text": "From the initial nine expressions of interest, finally two participants submitted their systems for evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "In (Bouayad-Agha et al., 2012), we presented the NLG challenge of content selection from semantic web data.", "labels": [], "entities": [{"text": "content selection from semantic web", "start_pos": 66, "end_pos": 101, "type": "TASK", "confidence": 0.7892361462116242}]}, {"text": "The task to perform was described as follows: given a set of RDF triples containing facts about a celebrity, select those triples that are reflected in the target text (i.e., a short biography about that celebrity).", "labels": [], "entities": []}, {"text": "The task first required a data preparation stage that involved the following two subtasks: 1) data gathering and preparation, that is, deciding which data and texts to use, then downloading and pairing them, and 2) working dataset selection and annotation, that is, defining the criteria/guidelines for determining when a triple is marked as selected in the target text, and producing a corpus of triples annotated for selection.", "labels": [], "entities": [{"text": "data gathering and preparation", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.758646547794342}]}, {"text": "There were initially nine interested participants (including the two organizing parties).", "labels": [], "entities": []}, {"text": "Five of which participated in the (voluntary) triple annotation rounds.", "labels": [], "entities": []}, {"text": "In the end, only two participants submitted their systems: We would like to thank Angelos Georgaras and Stasinos Konstantopoulos from NCSR (Greece) for their participation in the annotation rounds.", "labels": [], "entities": []}], "datasetContent": [{"text": "The manual annotation task consisted in emulating the content selection task of a Natural Language Generation system, by marking in the triple dataset associated with a person the triples predicated in the summary biography of that person according to a set of guidelines.", "labels": [], "entities": []}, {"text": "We performed two rounds of annotations.", "labels": [], "entities": []}, {"text": "In the first round, participants were asked to select content for the same three celebrities.", "labels": [], "entities": []}, {"text": "The objectives of this annotation, in which five individuals belonging to four distinct institutions participated, were 1) for participants to get acquainted with the content selection task envisaged, the domain and guidelines, 2) to validate the guidelines, and 3) to formally evaluate the complexity of the task by calculating inter-annotator agreement.", "labels": [], "entities": []}, {"text": "For the latter we used free-marginal multi-rater Kappa, as it seemed suited for the annotation task (i.e. independent ratings, discrete categories, multiple raters, annotators are not restricted in how they distribute categories across cases)).", "labels": [], "entities": []}, {"text": "We obtained an average Kappa of 0.92 across the three pairs for the 5 annotators and 2 categories (selected, not selected), which indicates a high level of agreement and therefore validates our annotation guidelines.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.992384672164917}]}, {"text": "Our objective for the second round of annotations was to obtain a dataset for participants to work with.", "labels": [], "entities": []}, {"text": "In the end, we gathered 344 pairs from 5 individuals of 5 distinct institutions.", "labels": [], "entities": []}, {"text": "It should be noted that although both rounds of annotations follow the anchor restriction presented in Section 2, the idea to set a minimum number of predicates for the larger corpus of 60000+ pairs came forth after analysing the results of the second round and noting the data sparsity in some pairs.", "labels": [], "entities": []}, {"text": "In what follows, we detail how the triples were presented to human annotators and what were the annotation criteria set forth in the guidelines.", "labels": [], "entities": []}, {"text": "Briefly speaking, the UA system uses a general heuristic based on the cognitive notion of communal common ground regarding each celebrity, which is approximated by scoring each lexicalized triple (or property) associated with a celebrity according to the number of hits of the Google search API.", "labels": [], "entities": []}, {"text": "Only the top-ranked triples are selected).", "labels": [], "entities": []}, {"text": "The UIC system uses a small set of rules for the conditional inclusion of predicates that was derived offline from the statistical analysis of the co-occurrence between predicates that are about the same topic or that share some shared arguments; only the best performing rules tested against a subset of the development set are included.", "labels": [], "entities": [{"text": "UIC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9090045690536499}]}, {"text": "For the baseline evaluation, we used the development set obtained in the second round annotation (see Section 3).", "labels": [], "entities": []}, {"text": "However, we only consider pairs obtained during the second round annotation that 1) follow both restrictions presented in SecBaseline UIC: Baseline evaluation results (%) tion 2, and 2) have no coreferring triples.", "labels": [], "entities": [{"text": "SecBaseline UIC", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.7795623540878296}]}, {"text": "This last restriction was added to minimize errors because we observed that annotators were not always consistent in their annotation of triple coreference.", "labels": [], "entities": []}, {"text": "We therefore considered 188 annotations from the 344 annotations of the development set.", "labels": [], "entities": []}, {"text": "Of these, we used 40 randomly selected annotations for evaluating the systems and 144 for estimating a baseline that only considers the top 5 predicates (i.e., the predicates most often selected) and the type-predicate.", "labels": [], "entities": []}, {"text": "The evaluation results of the three systems (baseline, UIC and UA) are presented in.", "labels": [], "entities": [{"text": "UIC", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.6835405826568604}]}, {"text": "The figures in the table were obtained by comparing the triples selected and rejected by each system against the manual annotation.", "labels": [], "entities": []}, {"text": "The performance of the baseline is quite high.", "labels": [], "entities": []}, {"text": "The UA system based on a general heuristic scores lower than the baseline, whilst the UIC system has a better precision than the baseline, albeit a lower recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9989169836044312}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9988741278648376}]}, {"text": "This might be due, as the UA authors observe in their summary (, to \"the large number of predicates that are present only in a few files . .", "labels": [], "entities": []}, {"text": "makes it harder to decide whether we have to include these predicates or not.\"", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline evaluation results (%)", "labels": [], "entities": []}]}