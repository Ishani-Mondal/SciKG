{"title": [{"text": "Statistical Representation of Grammaticality Judgements: the Limits of N-Gram Models", "labels": [], "entities": [{"text": "Statistical Representation of Grammaticality Judgements", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8413386821746827}]}], "abstractContent": [{"text": "We use a set of enriched n-gram models to track grammaticality judgements for different sorts of passive sentences in English.", "labels": [], "entities": []}, {"text": "We construct these models by specifying scoring functions to map the log probabilities (logprobs) of an n-gram model fora test set of sentences onto scores which depend on properties of the string related to the parameters of the model.", "labels": [], "entities": []}, {"text": "We test our models on classification tasks for different kinds of passive sentences.", "labels": [], "entities": []}, {"text": "Our experiments indicate that our n-gram models achieve high accuracy in identifying ill-formed pas-sives in which ill-formedness depends on local relations within the n-gram frame, but they are far less successful in detecting non-local relations that produce unacceptability in other types of passive construction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9724603295326233}]}, {"text": "We take these results to indicate some of the strengths and the limitations of word and lexical class n-gram models as candidate representations of speakers' grammatical knowledge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most advocates and critics) of a probabilistic view of grammatical knowledge have assumed that this view identifies the grammatical status of a sentence directly with the probability of its occurrence.", "labels": [], "entities": []}, {"text": "By contrast, we seek to characterize grammatical knowledge statistically, but without reducing grammaticality directly to probability.", "labels": [], "entities": []}, {"text": "Instead we specify a set of scoring procedures for mapping the logprob value of a sentence into a relative grammaticality score, on the basis of the properties of the sentence and of the logprobs that an n-gram word model generates for the corpus containing the sentence.", "labels": [], "entities": []}, {"text": "A scoring procedure in this set generates scores in terms of which we construct a grammaticality classifier, using a parameterized standard deviation from the mean value.", "labels": [], "entities": []}, {"text": "The classifier provides a procedure for testing the accuracy of different scoring criteria in separating grammatical from ungrammatical passive sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9988073110580444}, {"text": "separating grammatical from ungrammatical passive sentences", "start_pos": 94, "end_pos": 153, "type": "TASK", "confidence": 0.771103044350942}]}, {"text": "We evaluate this approach by applying it to the task of distinguishing well and ill-formed sentences with passive constructions headed by four different sorts of verbs: intransitives (appear, last), pseudo-transitives, which take a restricted set of notional objects (laugh a hearty laugh, weigh 10 kg), ambiguous transitives, which allow both agentive and thematic subjects (the jeans / the tailor fitted John), and robust transitives that passivize freely (write, move).", "labels": [], "entities": []}, {"text": "Intransitives and pseudo-transitives generally yield ill-formed passives.", "labels": [], "entities": []}, {"text": "Passives formed from ambiguous transitives tend to be well-formed only on the agentive reading.", "labels": [], "entities": []}, {"text": "Robust transitives, for the most part, yield acceptable passives, even if they are semantically (or pragmatically) odd.", "labels": [], "entities": []}, {"text": "Experimenting with several scoring procedures and alternative values for our standard deviation parameter, we found that our classifier can distinguish pairwise between elements of the first two classes of passives and those of the latter two with a high degree of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 265, "end_pos": 273, "type": "METRIC", "confidence": 0.9976763129234314}]}, {"text": "However, its performance is far less reliable in identifying the difference between ambiguous and robust transitive passives.", "labels": [], "entities": []}, {"text": "The first classification task relies on local lexical patterns that can be picked up by n-gram models, while the second requires identification of anomalous relations between passivized verbs and by-phrases, which are not generally accessible to measurement within the range of an n-gram.", "labels": [], "entities": []}, {"text": "We also observed that as we increased the size of the training corpus, the performance of our enriched models on the classification task also increased.", "labels": [], "entities": [{"text": "classification task", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.9191349446773529}]}, {"text": "This result suggests that better n-gram language models are more sensitive to the sorts of patterns that our scoring procedures rely onto generate accurate grammaticality classifications.", "labels": [], "entities": []}, {"text": "We note the important difference between grammaticality and acceptability.", "labels": [], "entities": []}, {"text": "Following standard assumptions, we take grammaticality to be a theoretical notion, and acceptability to bean empirically testable property.", "labels": [], "entities": []}, {"text": "Acceptability is, in part, determined by grammaticality, but also by factors such as sentence length, processing limitations, semantic acceptability and many other elements.", "labels": [], "entities": []}, {"text": "Teasing apart these two concepts, and explicating their precise relationship raises a host of subtle methodological issues that we will not address here.", "labels": [], "entities": []}, {"text": "Oversimplifying somewhat, we are trying to reconstruct a gradient notion of grammaticality which is derived from probabilistic models, that can serve as a core component of a full model of acceptability.", "labels": [], "entities": []}, {"text": "We distinguish our task from the standard task of error detection in NLP (e.g. Post (2011)), that can be used in various language processing systems, such as machine translation, language modeling and soon.", "labels": [], "entities": [{"text": "error detection", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7106574922800064}, {"text": "machine translation", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.81768399477005}, {"text": "language modeling", "start_pos": 179, "end_pos": 196, "type": "TASK", "confidence": 0.7498332262039185}]}, {"text": "In error detection, the problem is a supervised learning task.", "labels": [], "entities": [{"text": "error detection", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.8040516674518585}]}, {"text": "Given a corpus of examples labeled as grammatical or ungrammatical, the problem is to learn a classifier to distinguish them.", "labels": [], "entities": []}, {"text": "We use supervised learning as well, but only to measure the upper bound of an unsupervised learning method.", "labels": [], "entities": []}, {"text": "We assume that native speakers do not, in general, have access to systematic sets of ungrammatical sentences that they can use to calibrate their judgement of acceptability.", "labels": [], "entities": []}, {"text": "Rather ungrammatical sentences are unusual or unlikely.", "labels": [], "entities": []}, {"text": "However, we use some ungrammatical sentences to set an optimal threshold for our scoring procedures.", "labels": [], "entities": []}], "datasetContent": [{"text": "Rather than trying to test the performance of these models overall types of ungrammaticality, we limit ourselves to a case study of the passive.", "labels": [], "entities": []}, {"text": "By tightly controlling the verb types and grammatical construction to which we apply our models we are better able to study the power and the limits of these models as candidate representations of grammatical knowledge.", "labels": [], "entities": []}], "tableCaptions": []}