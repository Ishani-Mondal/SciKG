{"title": [{"text": "Candidate Scoring Using Web-Based Measure for Chinese Spelling Error Correction", "labels": [], "entities": [{"text": "Candidate Scoring", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.5814702957868576}, {"text": "Chinese Spelling Error Correction", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.6404670104384422}]}], "abstractContent": [{"text": "Chinese character correction involves two major steps: 1) Providing candidate corrections for all or partially identified characters in a sentence, and 2) Scoring all altered sentences and identifying which is the best corrected sentence.", "labels": [], "entities": [{"text": "Chinese character correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6093535323937734}, {"text": "Providing candidate corrections", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.7704641024271647}, {"text": "Scoring all altered sentences", "start_pos": 155, "end_pos": 184, "type": "TASK", "confidence": 0.830799475312233}]}, {"text": "In this paper a web-based measure is used to score candidate sentences, in which there exists one continuous error character in a sentence in almost all sentences in the Bakeoff corpora.", "labels": [], "entities": []}, {"text": "The approach of using a web-based measure can be applied directly to sentences with multiple error characters, either consecutive or not, and is not optimized for one-character error correction of Chinese sentences.", "labels": [], "entities": []}, {"text": "The results show that the approach achieved a fair precision score whereas the recall is low compared to results reported in this Bakeoff.", "labels": [], "entities": [{"text": "precision score", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.9809658527374268}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9995180368423462}]}], "introductionContent": [{"text": "Errors existing in Chinese sentences can be classified into five categories: 1) Deletion, 2) Insertion, 3) Substitution, 4) Word-Order and 5) NonWord errors (C.-H. Liu,.", "labels": [], "entities": []}, {"text": "Deletion errors occur when there are missing Chinese characters/words in a sentence; Insertion errors occur when there are grammatically redundant characters/words; Substitution errors occur when characters/words are mis-typed by similar, either visually or phonologically, ones; Word-Order errors occur when the word order of a sentence does not conform to the language, which is a common error type exists in writings of second-language learners; Non-Word errors occur when a Chinese character is written incorrectly by hand, e.g., miss of a stroke.", "labels": [], "entities": []}, {"text": "Of the five error types, the Substitution errors is addressed in this SIGHAN-7 Chinese Spelling Check bakeoff and might be referred to as \"Chinese spelling error\" to emphasize its resemblance to counterparts in spelling-based languages such as English.", "labels": [], "entities": [{"text": "SIGHAN-7 Chinese Spelling Check bakeoff", "start_pos": 70, "end_pos": 109, "type": "DATASET", "confidence": 0.7303343057632447}]}, {"text": "It should be noted that Non-Word errors is also a kind of Chinese spelling errors.", "labels": [], "entities": []}, {"text": "It is also a common error type in hand-writings of second-language learners.", "labels": [], "entities": []}, {"text": "However, since it only exists in hand-writings of humans and because all characters used in computers are legal ones, it is not necessary to address this kind of spelling errors when given erroneous texts are of electronic forms.", "labels": [], "entities": []}, {"text": "The task addressed in SIGHAN-7 is a restricted type of Substitution errors, where there exists at most one continuous error (mis-spelled) character in its context within a sentence, with only one exception in which there is a two-character error; C.-L..", "labels": [], "entities": [{"text": "Substitution errors", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8529918193817139}]}, {"text": "This allows the system to assume that when a character is to be corrected, its adjacent characters are correct.", "labels": [], "entities": []}, {"text": "The correction procedure is comprised of two consecutive steps: 1) Providing candidate corrections for each character in the sentence, and 2) Scoring the altered correction sentences and identifying which is the best corrected sentence (C.-H.; C.-H.", "labels": [], "entities": [{"text": "Scoring the altered correction sentences", "start_pos": 142, "end_pos": 182, "type": "TASK", "confidence": 0.8392230629920959}]}, {"text": "In this paper, a web-based measure is employed in the second step to score and identify the best correction sentence.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the system architecture for spelling error correction.", "labels": [], "entities": [{"text": "spelling error correction", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.8836872378985087}]}, {"text": "Section 3 provides the details of the model using web-based measure to score candidate corrections.", "labels": [], "entities": []}, {"text": "In Section 4 the experimental setup and results are detailed.", "labels": [], "entities": []}, {"text": "The last section summarized the conclusions and future work of this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the proposed system, Academia Sinica's CKIP Chinese Segmenter is used to derive segmentation results) and the language model (trigrams using Chen and Goodman's modified Kneser-Ney discounting) is trained using SRILM with Chinese Gigaword (LDC Catalog No.: LDC2003T09).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.8521713018417358}]}, {"text": "Ina brief summary of the results, our system did not perform well in the final test of SIGHAN-7 bakeoff.", "labels": [], "entities": [{"text": "SIGHAN-7 bakeoff", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.42347073554992676}]}, {"text": "The authors would like to defend the proposed method with a major problem in the runtime of the final test.", "labels": [], "entities": []}, {"text": "In theory, the \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 , , \ud97b\udf59 \ud97b\udf59 , \u2026 , , \ud97b\udf59 \ud97b\udf59 as derived in Equation 1 should all be estimated using the web-based measure using Equation 2.", "labels": [], "entities": []}, {"text": "However, since the number of sentences in the final testis huge (Sub-Tasks 1 and 2 each has 1,000 paragraphs and each paragraph contains about five Chinese sentences), the enormous number of queries sent to the search engine (Yahoo!) has caused our experiments being banned for several times.", "labels": [], "entities": []}, {"text": "To solve this problem, two strategies were used to complete the final test, 1) only three of the candidates \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 , , \ud97b\udf59 \ud97b\udf59 , \u2026 , , \ud97b\udf59 \ud97b\udf59 (ranked the highest three using ngram) are considered in the final test using webbased measure, and 2) three computers with different physical IP addresses were setup for the experiment.", "labels": [], "entities": []}, {"text": "Therefore, the potential of the proposed method is far from fully exploited.", "labels": [], "entities": []}, {"text": "A post-workshop experiment will be administered for further analysis of the method.", "labels": [], "entities": []}, {"text": "The comparisons of the proposed system and highly ranked systems in SIGHAN-7 are excerpted in this section.", "labels": [], "entities": [{"text": "SIGHAN-7", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.7619544267654419}]}, {"text": "The first result that attracts our attention is error location accuracy as shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.8117690086364746}]}, {"text": "This is a common measure in both Sub-Tasks and is defined as \"number of sentences error locations are correctly detected\" over \"number of all test sentences\".", "labels": [], "entities": []}, {"text": "The report of our system (NCKU&YZU-1) on error location accuracy in Sub-Task 1 (Detection) is 0.705, whereas it is only 0.117 in Sub-Task 2 (Correction).", "labels": [], "entities": [{"text": "NCKU", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9434932470321655}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.5751209855079651}]}, {"text": "This result puzzled the authors because in our system, there is no error detection module.", "labels": [], "entities": [{"text": "error detection", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7059405446052551}]}, {"text": "Similar results on both Sub-Tasks are expected since the same error correction method is used.", "labels": [], "entities": []}, {"text": "A possible explanation is that the final test corpora of the two Sub-Tasks exhibited substantial differences in the composition of correct and erroneous sentences or in sentential characteristics.", "labels": [], "entities": []}, {"text": "The results of other systems reported in both Sub-Tasks seem to support this point of view.", "labels": [], "entities": []}, {"text": "However, further analysis on the test corpora is still needed to clarify this problem.", "labels": [], "entities": []}, {"text": "show the results on error location detection and error detection in Sub-Task 1 (Detection).", "labels": [], "entities": [{"text": "error location detection", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6979069411754608}, {"text": "error detection", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7692352831363678}]}, {"text": "The difference between these two is that \"error location detection\" requires the detected location is correct while \"error detection\" will report correctly detected even the locations in sentences is not correct.", "labels": [], "entities": [{"text": "error location detection", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.6043075422445933}, {"text": "error detection", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.6549228131771088}]}, {"text": "Therefore it is expected that scores of Error Location Detection area little bit higher than those of Error Detection.", "labels": [], "entities": [{"text": "Error Location Detection", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7970604101816813}]}, {"text": "Our system exhibits a relative smaller difference between these two scores, 2.4%, compared to other systems.", "labels": [], "entities": []}, {"text": "The major weakness of our system is its low recall rate, which might be the result of not applying an error detection module.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9879018068313599}]}, {"text": "Therefore an error detection method using web-based measure will be examined in our future work.", "labels": [], "entities": [{"text": "error detection", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.6688055843114853}]}, {"text": "shows the error detection accuracy of our system is significantly lower although FalseAlarm Rate is relatively small.", "labels": [], "entities": [{"text": "error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6876978278160095}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.7754740715026855}, {"text": "FalseAlarm Rate", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.9847758412361145}]}, {"text": "The correction accuracy and precision are also much lower than high-ranked systems in the Bakeoff, as shown in.", "labels": [], "entities": [{"text": "correction", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.949751079082489}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9389469027519226}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9997040629386902}, {"text": "Bakeoff", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.8371272087097168}]}, {"text": "Further investigation is required to examine if more thoroughly exploiting web-based measures will provide useful additional information for the purpose of Chinese spelling error detection and correction.", "labels": [], "entities": [{"text": "Chinese spelling error detection and correction", "start_pos": 156, "end_pos": 203, "type": "TASK", "confidence": 0.7329102506240209}]}], "tableCaptions": [{"text": " Table 1. Comparisons on Error Location Accura- cy in SIGHAN-7 Sub-Tasks 1 and 2.", "labels": [], "entities": [{"text": "Error Location Accura- cy", "start_pos": 25, "end_pos": 50, "type": "METRIC", "confidence": 0.8380472779273986}]}, {"text": " Table 2. Comparisons on Error Location  measures in SIGHAN-7 Sub-Task 1.", "labels": [], "entities": [{"text": "Error Location", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.8847725689411163}]}, {"text": " Table 3. Comparisons on Error Detection  measures in SIGHAN-7 Sub-Task 1.", "labels": [], "entities": [{"text": "Error Detection", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.6477040648460388}]}, {"text": " Table 4. Comparisons on False-Alarm Rate and  Detection Accuracy in SIGHAN-7 Sub-Task 1.", "labels": [], "entities": [{"text": "False-Alarm Rate", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9708001017570496}, {"text": "Accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.5445561408996582}]}, {"text": " Table 5. Comparisons on Correction Accuracy  and Precision in SIGHAN-7 Sub-Task 2.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.7078454494476318}]}]}