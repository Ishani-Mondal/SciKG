{"title": [{"text": "Constrained grammatical error correction using Statistical Machine Translation", "labels": [], "entities": [{"text": "Constrained grammatical error correction", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6126303598284721}, {"text": "Statistical Machine Translation", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.6603315969308218}]}], "abstractContent": [{"text": "This paper describes our use of phrase-based statistical machine translation (PB-SMT) for the automatic correction of errors in learner text in our submission to the CoNLL 2013 Shared Task on Grammatical Error Correction.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.6199186071753502}, {"text": "automatic correction of errors in learner text", "start_pos": 94, "end_pos": 140, "type": "TASK", "confidence": 0.7907367476395198}, {"text": "CoNLL 2013 Shared Task on Grammatical Error Correction", "start_pos": 166, "end_pos": 220, "type": "TASK", "confidence": 0.7399899289011955}]}, {"text": "Since the limited training data provided for the task was insufficient for training an effective SMT system, we also explored alternative ways of generating pairs of incorrect and correct sentences automatically from other existing learner corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9944024682044983}]}, {"text": "Our approach does not yield particularly high performance but reveals many problems that require careful attention when building SMT systems for error correction.", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.990251898765564}, {"text": "error correction", "start_pos": 145, "end_pos": 161, "type": "TASK", "confidence": 0.7013134509325027}]}], "introductionContent": [{"text": "Most approaches to error correction for non-native text are based on machine learning classifiers for specific error types ().", "labels": [], "entities": [{"text": "error correction", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6921742111444473}]}, {"text": "Thus, for correcting determiner or preposition errors, for example, a multiclass model is built that uses a set of features from the local context around the target and predicts the expected article or preposition.", "labels": [], "entities": [{"text": "correcting determiner or preposition", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.8284824043512344}]}, {"text": "If the output of the classifier is the same as the original sentence, the sentence is not corrected.", "labels": [], "entities": []}, {"text": "Otherwise, a correction is made based on the predicted class.", "labels": [], "entities": []}, {"text": "This is the de facto approach to error correction and is widely adopted in previous work.", "labels": [], "entities": [{"text": "error correction", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8063830733299255}]}, {"text": "Building effective classifiers requires identification of features types from the text that discriminate well correcting each specific error type, such as part-of-speech tags of neighbouring words, ngram statistics, etc., which in turn require additional linguistic resources.", "labels": [], "entities": []}, {"text": "Classifiers designed to correct only one type of error do not perform well on nested or sequential errors.", "labels": [], "entities": []}, {"text": "Correcting more than one type of error requires building and combining multiple classifiers.", "labels": [], "entities": []}, {"text": "These factors make the solution highly dependent on engineering decisions (e.g. as regards features and algorithms) as well as complex and laborious to extend to new types.", "labels": [], "entities": []}, {"text": "An attractive and simpler alternative is to think of error correction as a translation task.", "labels": [], "entities": [{"text": "error correction", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.6562589406967163}, {"text": "translation task", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.908474862575531}]}, {"text": "The underlying idea is that a statistical machine translation (SMT) system should be able to translate text written in 'bad' (incorrect) English into 'good' (correct) English.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.7685422251621882}]}, {"text": "An advantage of using this approach is that there is no need for an explicit encoding of the contexts that surround each error (i.e. features) since SMT systems learn contextuallyappropriate source-target mappings from the training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9899146556854248}]}, {"text": "Likewise, they do not require any special modification for correcting multiple error types sequentially, since they generate an overall corrected version of the sentence fixing as much as possible from what they have learnt.", "labels": [], "entities": []}, {"text": "Provided the system is trained using a sufficiently large parallel corpus of incorrect-to-correct sentences, the model should handle all the observed errors without any further explicit information like previously detected error types, context or error boundaries, and so forth.", "labels": [], "entities": []}, {"text": "The increasing performance of state-of-the-art SMT systems also suggests they could prove successful for other applications, such as error correction.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.993057370185852}, {"text": "error correction", "start_pos": 133, "end_pos": 149, "type": "TASK", "confidence": 0.7328502982854843}]}, {"text": "In fact, SMT systems have been successfully used in a few such experiments, as we report below.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9944378137588501}]}, {"text": "The work presented here builds upon these initial experiments and explores the factors that may affect the performance of such systems.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows: Section 2 gives a summary of previous research using SMT for error correction, Section 3 describes our approach and resources, and Section 4 reports our experiments and results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9833806753158569}, {"text": "error correction", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7388830482959747}]}, {"text": "Section 5 discusses a number of issues related to the performance of our system and reports some at-tempts at improving it while Section 6 includes our official performance in the shared task.", "labels": [], "entities": []}, {"text": "Finally, Section 7 provides conclusions and ideas for future work.", "labels": [], "entities": []}, {"text": "describe the use of an SMT system for correcting a set of 14 countable/uncountable nouns which are often confusing for learners of English as a second language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9889679551124573}]}, {"text": "Their training data consists of a large corpus of sentences extracted from news articles which were deliberately modified to include typical countability errors involving the target words as observed in a Chinese learner corpus.", "labels": [], "entities": []}, {"text": "Artificial errors are introduced in a deterministic manner using handcoded rules including operations such as changing quantifiers (much \u2192 many), generating plurals (advice \u2192 advices) or inserting unnecessary determiners.", "labels": [], "entities": []}, {"text": "Experiments show their SMT system was generally able to beat the standard Microsoft Word 2003 grammar checker, although it produced a relatively higher rate of erroneous corrections.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9939334988594055}]}], "datasetContent": [{"text": "We first built a baseline SMT system using only the NUCLE v2.3 corpus and compared it to other systems trained on incremental additions of the remaining corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9896209239959717}, {"text": "NUCLE v2.3 corpus", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.92429651816686}]}, {"text": "All our systems were trained using 4-fold cross-validation where the training set for each run always included the full FCE, IELTS and EVP corpora but only 3/4 of the NUCLE data, leaving the remaining fourth chunk for testing.", "labels": [], "entities": [{"text": "FCE", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.724694013595581}, {"text": "IELTS", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.9715354442596436}, {"text": "NUCLE data", "start_pos": 167, "end_pos": 177, "type": "DATASET", "confidence": 0.9365623593330383}]}, {"text": "This training method allowed us to concentrate on how the system performed on NUCLE data.", "labels": [], "entities": [{"text": "NUCLE data", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9183651804924011}]}, {"text": "Performance was evaluated in terms of precision, recall and F 1 as computed by the M 2 Scorer (, with the maximum number of unchanged words per edit set to 3 (an initial suggestion by the shared task organisers which was eventually changed for the official evaluation).", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9995824694633484}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9997406601905823}, {"text": "F 1", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9835614860057831}, {"text": "M 2 Scorer", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.504841277996699}]}, {"text": "The average performance of each system is reported in.", "labels": [], "entities": []}, {"text": "In general, results show that precision tends to drop as we add more training data whereas recall and F 1 slightly increase.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9996249675750732}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9997344613075256}, {"text": "F 1", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9910719096660614}]}, {"text": "This suggests that our additional corpora do not resemble NUCLE very much, although they allow the system to correct some further errors.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.8834697604179382}]}, {"text": "Contrary to our expectations, the biggest difference between precision and recall is observed when we add the EVP-derived data, which was deliberately engineered to replicate NUCLE errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9992152452468872}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9989114999771118}, {"text": "EVP-derived data", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.794550210237503}]}, {"text": "Although it has been reported that artificial errors often cause drops in performance, in our case this may also be due to differences inform (e.g. sentence length, grammatical structures covered, error coding) and content (i.e. topics) between our source (EVP) and target (NUCLE) corpora as well as poor control over the artificial error generation process.", "labels": [], "entities": []}, {"text": "In fact, our method does not explicitly consider error contexts, error type distribution or other factors that  certainly have an impact on the quality of the generated sentences and may introduce noise if not controlled.", "labels": [], "entities": []}, {"text": "Nevertheless, the system trained on all four corpora yields the best F 1 performance.", "labels": [], "entities": [{"text": "F 1", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9185711145401001}]}, {"text": "We also tested factored models which include PoS information.", "labels": [], "entities": [{"text": "PoS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.7039129137992859}]}, {"text": "The same behaviour is observed for the metrics, although values for precision are now generally higher while values for recall are lower.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9994730353355408}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9992048144340515}]}, {"text": "Again, the best system in terms of F 1 is the one trained on all our corpora, slightly outperforming our previous best system.", "labels": [], "entities": [{"text": "F 1", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9894145429134369}]}, {"text": "Systems were evaluated using a set of 50 essays containing about 500 words each (\u223c25,000 words in total) which were written in response to two different prompts.", "labels": [], "entities": []}, {"text": "One of these prompts had been used fora subset of the training data while the other was new.", "labels": [], "entities": []}, {"text": "No error annotations were initially available for this set.", "labels": [], "entities": []}, {"text": "As we mentioned above, the M 2 scorer was set to be sensitive to capitalisation and white space as well as limit the maximum number of unchanged tokens per edit to 2.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.43616283933321637}]}, {"text": "Initially, each participating team received their official system results individually.", "labels": [], "entities": []}, {"text": "After the goldstandard annotations of the test set were released,: Official results of our system before and after revision of the test set annotations.", "labels": [], "entities": []}, {"text": "The number of correct, proposed and gold edits are also included for comparison.", "labels": [], "entities": []}, {"text": "many participants raised concerns about their accuracy so they were given the opportunity to submit alternative annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9955021739006042}]}, {"text": "These suggestions were manually revised by a human annotator and merged into anew test set which was used to rescore all the submitted systems in a second official evaluation round.", "labels": [], "entities": []}, {"text": "Evaluation results of our system in both rounds (before and after revision of the test set annotations) are included in.", "labels": [], "entities": []}, {"text": "Although this measure helped overcome some of the problems described in Section 5.6, other problems such as whitespace and case sensitivity were not addressed.", "labels": [], "entities": [{"text": "Section 5.6", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.8746341168880463}, {"text": "case sensitivity", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.6680733561515808}]}, {"text": "In both evaluation rounds, our system scores third in terms of precision, which is particularly encouraging for error correction environments where precision is preferred over recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9995912909507751}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9986612796783447}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9958621263504028}]}, {"text": "However, these values should be considerably higher in order to prove useful in applications like selfassessment and tutoring systems.", "labels": [], "entities": []}, {"text": "Results also reveal precision on the test set is considerably higher than in our cross-validation experiments.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9996594190597534}]}, {"text": "This maybe partly a result of the larger amount of training data in our final system and/or greater grammatical or thematic similarity between the test and training sets.", "labels": [], "entities": []}, {"text": "shows the distribution of system edits by error type.", "labels": [], "entities": []}, {"text": "The results suggest that lexical heterogeneity in the contexts surrounding errors is a factor in performance, which might be improved through larger training sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of our lexical SMT models.  The best results are marked in bold. Standard devi- ation (\u03c3) indicates how stable/homogeneous each  dataset is (lower values are better).", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9481562972068787}, {"text": "devi- ation (\u03c3)", "start_pos": 96, "end_pos": 111, "type": "METRIC", "confidence": 0.7767338107029597}]}, {"text": " Table 4: Performance of our PoS factored  SMT models. The best results are marked in  bold. Standard deviation (\u03c3) indicates how sta- ble/homogeneous each dataset is (lower values are  better).", "labels": [], "entities": [{"text": "PoS factored  SMT", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.545066754023234}, {"text": "Standard deviation (\u03c3)", "start_pos": 93, "end_pos": 115, "type": "METRIC", "confidence": 0.9702872514724732}]}, {"text": " Table 5: Performance of the baseline system plus  different individual settings. Bold values indicate  an improvement over the original baseline system.", "labels": [], "entities": []}, {"text": " Table 6: Official results of our system before and  after revision of the test set annotations. The num- ber of correct, proposed and gold edits are also in- cluded for comparison.", "labels": [], "entities": []}, {"text": " Table 7: Distribution of system edits by error  type for the two official evaluation rounds (before  and after revision of the test annotations). 'Corr.'  stands for correct edits, 'Missed' for missed ed- its and 'Unnec.' for unnecessary edits. The cate- gory 'Other' includes changes made by our system  which do not belong to any of the other categories.", "labels": [], "entities": [{"text": "Corr", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9779158234596252}]}]}