{"title": [{"text": "Learning Hierarchical Linguistic Descriptions of Visual Datasets", "labels": [], "entities": [{"text": "Learning Hierarchical Linguistic Descriptions of Visual", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.6795933196942011}]}], "abstractContent": [{"text": "We propose a method to learn succinct hierarchical linguistic descriptions of visual datasets, which allow for improved navigation efficiency in image collections.", "labels": [], "entities": []}, {"text": "Classic exploratory data analysis methods, such as ag-glomerative hierarchical clustering, only provide a means of obtaining a tree-structured partitioning of the data.", "labels": [], "entities": []}, {"text": "This requires the user to go through the images first, in order to reveal the semantic relationship between the different nodes.", "labels": [], "entities": []}, {"text": "On the other hand, in this work we propose to learn a hierarchy of linguistic descriptions, referred to as attributes, which allows fora textual description of the semantic content that is captured by the hierarchy.", "labels": [], "entities": []}, {"text": "Our approach is based on a generative model, which relates the attribute descriptions associated with each node, and the node assignments of the data instances, in a probabilistic fashion.", "labels": [], "entities": []}, {"text": "We furthermore use a nonparametric Bayesian prior, known as the tree-structured stick breaking process, which allows for the structure of the tree to be learned in an unsu-pervised fashion.", "labels": [], "entities": []}, {"text": "We also propose appropriate performance measures, and demonstrate superior performance compared to other hierarchical clustering algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the abundance of images available both for personal use and in large internet based datasets, such as Flickr and Google Image Search, hierarchies of images are an important tool that allows for convenient browsing and efficient search and retrieval.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 107, "end_pos": 113, "type": "DATASET", "confidence": 0.936825692653656}]}, {"text": "Intuitively, desirable hierarchies should capture similarity in a semantic space, i.e. nearby nodes should include categories which are semantically more similar, as compared to nodes which are more distant.", "labels": [], "entities": []}, {"text": "Recent works that are concerned with learning image hierarchies, have relied on a bag of visual-words feature space, and therefore have been shown to provide unsatisfactory results with respect to the latter requirement (.", "labels": [], "entities": []}, {"text": "A recent trend in visual recognition systems, has been to shift from using a low-level feature based representation to an attribute based feature space, which can capture higher level semantics.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.71993687748909}]}, {"text": "Attributes are detectors that are trained using annotation data, to identify particular properties in an instance image.", "labels": [], "entities": []}, {"text": "By evaluating these detectors on a query image, one can obtain a linguistic description of the image.", "labels": [], "entities": []}, {"text": "Therefore, learning a visual hierarchy based on an attribute representation can allow for an improved semantic grouping, as compared to the previous use of low-level image features.", "labels": [], "entities": []}, {"text": "In this work we wish to utilize an attribute based representation to learn a hierarchical linguistic description of a visual dataset, in which few attributes are associated with each node of the tree.", "labels": [], "entities": []}, {"text": "As is illustrated in, such an attribute hierarchy is tightly related to a category hierarchy, in which the instances associated with every node are described using all the attributes associated with all the nodes along the path leading up to the root node (the instances in are described by the corresponding photographs).", "labels": [], "entities": []}, {"text": "This \"duality\" between the at-!\"#$%&'()*+#$!'##$,'-.#//$01-2+34$ tribute and category hierarchies, offers an important advantage when characterizing the dataset to the end-user, since it eliminates the need to visually inspect the images assigned to each node, in order to reveal the semantic relationship between the categories that are associated with the different nodes.", "labels": [], "entities": []}, {"text": "Exploratory data analysis methods for learning hierarchies, such as agglomerative hierarchical clustering (AHC) p.", "labels": [], "entities": []}, {"text": "59), only assign instances to different nodes in the tree, whereas our approach learns an attribute hierarchy which is used to assign the instances to the nodes of the tree.", "labels": [], "entities": []}, {"text": "The attribute hierarchy provides a linguistic description of a category hierarchy.", "labels": [], "entities": []}, {"text": "We develop a generative model, which we refer to as the attribute tree process (ATP), and which ties together the attribute and category hierarchies in a probabilistic fashion.", "labels": [], "entities": []}, {"text": "The tree structure is learned by incorporating a nonparametric Bayesian prior, known as the tree-structured stick breaking process (TSSBP) (, in the probabilistic formulation.", "labels": [], "entities": []}, {"text": "An important observation which we make about the attribute hierarchies which are learned using the ATP, is that attributes which are related to more image instances tend to be associated with nodes which are closer to the root, and vice versa, attributes which are associated with fewer instances tend to be associated with leaf nodes.", "labels": [], "entities": []}, {"text": "A hierarchical clustering algorithm that is based on the TSSBP for binary feature vectors was developed in (, and is known as the factored Bernoulli likelihood model (FBLM).", "labels": [], "entities": [{"text": "factored Bernoulli likelihood model (FBLM)", "start_pos": 130, "end_pos": 172, "type": "METRIC", "confidence": 0.653034154857908}]}, {"text": "However, similarly to AHC, it does not produce the attribute hierarchy in which we are interested.", "labels": [], "entities": []}, {"text": "In order to evaluate the ATP quantitatively, we compare its performance to other hierarchical clustering algorithms.", "labels": [], "entities": []}, {"text": "If the ground truth of the category hierarchy is available, we propose to use the semantic distance between the categories, that is given by the ground truth hierarchy, to evaluate the degree to which the semantic distance between the categories is preserved by the hierarchical clustering algorithm.", "labels": [], "entities": []}, {"text": "If the ground truth is not available, we use two criteria, which as we argue, capture the properties that should be demonstrated by desirable semantic hierarchies.", "labels": [], "entities": []}, {"text": "The first is the \"purity criterion\") which measures the degree to which each node is occupied by instances from a single class, and the second is the \"locality criterion\" which we propose, and which measures the degree to which instances from the same class are assigned to nearby nodes in the hierarchy.", "labels": [], "entities": []}, {"text": "Our experimental results show that when compared to AHC and FBLM, our approach captures the ground truth semantic distance between the categories more accurately, and without significant dependence on hyperparameters.", "labels": [], "entities": [{"text": "FBLM", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8743054270744324}]}, {"text": "The remaining of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "2 we provide background on agglomerative hierarchical clustering, and on the TSSBP, and in Sec.", "labels": [], "entities": [{"text": "TSSBP", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.7687975168228149}]}, {"text": "3 we develop the generative model for the ATP.", "labels": [], "entities": [{"text": "generative", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.9719551801681519}]}, {"text": "4 we propose evaluation metrics for the attribute hierarchy, and in Sec.", "labels": [], "entities": []}, {"text": "5 we present the experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we learn the attribute hierarchy using our proposed ATP algorithm.", "labels": [], "entities": []}, {"text": "In order to evaluate the performance we evaluate the ATP as a hierarchical clustering algorithm, and compare it to the FBLM and AHC.", "labels": [], "entities": [{"text": "FBLM", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.587411642074585}, {"text": "AHC", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.7821619510650635}]}, {"text": "We use subsets of the PASCAL VOC2008, and SUN09 datasets, for which attribute annotations are available.", "labels": [], "entities": [{"text": "PASCAL VOC2008", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.7251589000225067}, {"text": "SUN09 datasets", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8981642127037048}]}, {"text": "We learn hierarchies using the ground truth attribute annotation of the training set, and using the attribute scores obtained for the image instances in the testing set, where the attribute detectors are trained using the training set.", "labels": [], "entities": []}, {"text": "We used the FBLM implementation which is available online.", "labels": [], "entities": [{"text": "FBLM", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8449358940124512}]}, {"text": "Our implementation of the ATP is based the TSSBP implementation which is available online, where we extended it to implement our ATP generative model.", "labels": [], "entities": []}, {"text": "We used the AHC implementation available at (Mullner, ), where we used the average distance metric, which is also known as the Unweighted Pair Group Method with Arithmetic Mean (UMPGA).", "labels": [], "entities": [{"text": "Arithmetic Mean (UMPGA)", "start_pos": 161, "end_pos": 184, "type": "METRIC", "confidence": 0.8083827495574951}]}], "tableCaptions": [{"text": " Table 1: Average edge error using the attribute annotation  of the training set, for different hyper-parameters.", "labels": [], "entities": [{"text": "Average edge error", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7918297449747721}]}]}