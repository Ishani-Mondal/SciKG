{"title": [{"text": "Model-free POMDP optimisation of tutoring systems with echo-state networks", "labels": [], "entities": []}], "abstractContent": [{"text": "Intelligent Tutoring Systems (ITSs) are now recognised as an interesting alternative for providing learning opportunities in various domains.", "labels": [], "entities": []}, {"text": "The Reinforcement Learning (RL) approach has been shown reliable for finding efficient teaching strategies.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.666931688785553}]}, {"text": "However, similarly to other human-machine interaction systems such as spoken dialogue systems, ITSs suffer from a partial knowledge of the interlocu-tor's intentions.", "labels": [], "entities": []}, {"text": "In the dialogue case, engineering work can infer a precise state of the user by taking into account the uncertainty provided by the spoken understanding language module.", "labels": [], "entities": []}, {"text": "A model-free approach based on RL and Echo State New-torks (ESNs), which retrieves similar information , is proposed here for tutoring.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the last decades, Intelligent Tutoring Systems (ITSs) have become powerful tools in various domains such as mathematics (), physics (), computer sciences (, reading), or foreign languages).", "labels": [], "entities": []}, {"text": "Their appeal relies on the fact that each student does not have to follow an average teaching strategy, especially as the one-to-one tutoring has been proven the most efficent.", "labels": [], "entities": []}, {"text": "The expertise of a teacher relies on his capacity to advice at the right time the student to acquire new skills.", "labels": [], "entities": []}, {"text": "To do so, the teacher is able to choose iteratively pedagogical activities.", "labels": [], "entities": []}, {"text": "From this perspective, teaching is a sequential decision-making problem.", "labels": [], "entities": []}, {"text": "To solve it, the reinforcement learning) approach and the Markov Decision Process (MDP) paradigm have been successfully used (.", "labels": [], "entities": []}, {"text": "Given a situation, each teacher's decision is locally quantified by a reward.", "labels": [], "entities": []}, {"text": "However, the consequences of the teacher's actions on the student's cognition cannot be exactly determined, which introduce uncertainty.", "labels": [], "entities": []}, {"text": "To find a solution, one can notice that spoken dialogue management and tutoring are closely related.", "labels": [], "entities": [{"text": "spoken dialogue management", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7100830674171448}]}, {"text": "Both are humain-computer interactions in which the human user's intentions are not perfectly known.", "labels": [], "entities": []}, {"text": "In the spoken dialogue case, the partial observability is due to the recognition errors introduced by the speech understanding module.", "labels": [], "entities": []}, {"text": "They are taken into account by using some hypotheses about how the language is constructed.", "labels": [], "entities": []}, {"text": "Thus, accurate models to link observations from the user's recognised utterances to the underlying intentions can beset up.", "labels": [], "entities": []}, {"text": "For example, the Hidden Information State paradigm () builds a state which is a summary of the dialogue history).", "labels": [], "entities": []}, {"text": "However, in the ITS case, such a state is harder to develop since the cognition cannot be determined by analysing a physical signal.", "labels": [], "entities": []}, {"text": "Thus, a model-free approach is preferred here.", "labels": [], "entities": []}, {"text": "To do so, a memory of the past observations and actions is built by means of a Recurrent Neural Network (RNN) and more precisely an Echo State Network (ESN)).", "labels": [], "entities": []}, {"text": "The internal state of the network can be shown (under some resonable conditions) to meet the Markov property ().", "labels": [], "entities": []}, {"text": "This internal state is then used with a standard RL algorithm to estimate the optimal solution.", "labels": [], "entities": []}, {"text": "It has already been applied to RL in () in limited toy applications and it is, to our knowledge, the first attempt to use it in an interaction framework.", "labels": [], "entities": [{"text": "RL", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9222673177719116}]}, {"text": "The proof of concept presented in Szita's article uses the common SARSA algorithm which is an on-line and onpolicy algorithm.", "labels": [], "entities": []}, {"text": "Each improvement of the strat-egy is directly tested.", "labels": [], "entities": []}, {"text": "In the case of teaching, testing poor decisions can be problematic.", "labels": [], "entities": []}, {"text": "Here, we thus propose the combination of an ESN with an off-line and off-policy algorithm, namely the Least Square Policy Algorithm (LSPI), which is another original contribution of this paper.", "labels": [], "entities": []}, {"text": "Indeed, learning the solution with Partially Observable MDPs in a batch and off-policy manner is not common in the literature.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments, we assume that the teaching can be done by means of three actions.", "labels": [], "entities": []}, {"text": "First, a lesson can be presented to make the knowledge of the student increase.", "labels": [], "entities": []}, {"text": "The second and third actions are evaluations.", "labels": [], "entities": []}, {"text": "They can either be a simple question or a final exam.", "labels": [], "entities": []}, {"text": "The final exam consists in asking a hundred yes/no questions of equal complexity and on the same topic.", "labels": [], "entities": []}, {"text": "The student does not have a feedback.", "labels": [], "entities": []}, {"text": "Once it is proposed, anew teaching episode starts.", "labels": [], "entities": []}, {"text": "Three observations are returned to the ITS.", "labels": [], "entities": [{"text": "ITS", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9809026718139648}]}, {"text": "If a lesson is proposed to the user, the observation is neutral: no feedback comes from the student since the direct influence of the lesson remains unknown.", "labels": [], "entities": []}, {"text": "The two other obervations appear when a question is asked (yes or no).", "labels": [], "entities": []}, {"text": "Consequently, one observation is not enough to choose the next action since no clue is given about how many lessons have led to this result.", "labels": [], "entities": []}, {"text": "A non-null reward is only given when a final exam is proposed.", "labels": [], "entities": []}, {"text": "In this case, it is proportional to the rate of correct answers among all the answers given during the exam.", "labels": [], "entities": []}, {"text": "Thus, each improvement is taken into account.", "labels": [], "entities": []}, {"text": "The \u03b3 factor is set to 0.97.", "labels": [], "entities": [{"text": "\u03b3 factor", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9587195217609406}]}, {"text": "In this proof of concept, the results have been obtained with simulated students from () to ensure the reproducibility of the experiments.", "labels": [], "entities": []}, {"text": "The simulation implements two abilities: answering a question and learning with a lesson.", "labels": [], "entities": []}, {"text": "Three groups of students have been setup.", "labels": [], "entities": []}, {"text": "The first one, T 1, is supposed to be able to learn very efficiently, the second, T 2, needs a few more lessons to provide good answers, and the third, T 3, needs a lot of lessons to answer correctly.", "labels": [], "entities": []}], "tableCaptions": []}