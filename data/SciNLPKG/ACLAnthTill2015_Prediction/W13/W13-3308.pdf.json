{"title": [{"text": "Feature Weight Optimization for Discourse-Level SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.6506612300872803}]}], "abstractContent": [{"text": "We present an approach to feature weight optimization for document-level decoding.", "labels": [], "entities": [{"text": "feature weight optimization", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7008964320023855}]}, {"text": "This is an essential task for enabling future development of discourse-level statistical machine translation, as it allows easy integration of discourse features in the decoding process.", "labels": [], "entities": [{"text": "discourse-level statistical machine translation", "start_pos": 61, "end_pos": 108, "type": "TASK", "confidence": 0.574303649365902}]}, {"text": "We extend the framework of sentence-level feature weight optimization to the document-level.", "labels": [], "entities": [{"text": "sentence-level feature weight optimization", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.6846427395939827}]}, {"text": "We show experimentally that we can get competitive and relatively stable results when using a standard set of features, and that this framework also allows us to optimize document-level features, which can be used to model discourse phenomena.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse has largely been ignored in traditional machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.8398300290107727}]}, {"text": "Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences.", "labels": [], "entities": []}, {"text": "It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents.", "labels": [], "entities": []}, {"text": "Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion) or two-step translation to address pronoun anaphora.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.8109398782253265}]}, {"text": "Recently, however, we presented Docent ( ), a decoder based on local search that translates full documents.", "labels": [], "entities": []}, {"text": "So far this decoder has not included a feature weight optimization framework.", "labels": [], "entities": [{"text": "feature weight optimization", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.6287910640239716}]}, {"text": "However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9361701607704163}]}, {"text": "In previous research with Docent, we used grid search to find weights for document-level features while base features were optimized using standard sentence-level techniques.", "labels": [], "entities": []}, {"text": "This approach is impractical since many values for the extra features have to be tried, and, more importantly, it might not give the same level of performance as jointly optimizing all parameters.", "labels": [], "entities": []}, {"text": "Principled feature weight optimization is thus essential for researchers that want to use document-level features to model discourse phenomena such as anaphora, discourse connectives, and lexical consistency.", "labels": [], "entities": [{"text": "Principled feature weight optimization", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.739319771528244}]}, {"text": "In this paper, we therefore propose an approach that supports discourse-wide features in documentlevel decoding by adapting existing frameworks for sentence-level optimization.", "labels": [], "entities": [{"text": "documentlevel decoding", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.6335975974798203}, {"text": "sentence-level optimization", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.7093237340450287}]}, {"text": "Furthermore, we include a thorough empirical investigation of this approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report experimental results where we investigate several issues in connection with document-level feature weight optimization for SMT.", "labels": [], "entities": [{"text": "document-level feature weight optimization", "start_pos": 102, "end_pos": 144, "type": "TASK", "confidence": 0.6139752045273781}, {"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9853354692459106}]}, {"text": "We first describe the experimental setup, followed by baseline results using sentencelevel optimization.", "labels": [], "entities": []}, {"text": "We then present validation experiments with standard sentence-level features, which can be compared to standard optimization.", "labels": [], "entities": []}, {"text": "Finally, we report results with a set of documentlevel features that have been proposed for joint translation and text simplification ( ).", "labels": [], "entities": [{"text": "text simplification", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7314010560512543}]}, {"text": "Most of our experiments are for German-toEnglish news translation using data from the WMT13 workshop.", "labels": [], "entities": [{"text": "German-toEnglish news translation", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.6671220262845358}, {"text": "WMT13 workshop", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.9397872686386108}]}, {"text": "We also show results with document-level features for English-to-Swedish Europarl ().", "labels": [], "entities": [{"text": "Europarl", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.7950232028961182}]}, {"text": "The size of the training, tuning, and test sets are shown in.", "labels": [], "entities": []}, {"text": "First of all, we need to extract documents for tuning and testing with Docent.", "labels": [], "entities": [{"text": "tuning", "start_pos": 47, "end_pos": 53, "type": "TASK", "confidence": 0.9534655213356018}, {"text": "Docent", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.9358274936676025}]}, {"text": "Fortunately, the news data already contain document markup, corresponding to individual news articles.", "labels": [], "entities": []}, {"text": "For Europarl we define a document as a consecutive sequence of utterances from a single speaker.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9657592177391052}]}, {"text": "To investigate the effect of the size of the tuning set, we used different subsets of the available tuning data.", "labels": [], "entities": []}, {"text": "All our document-level experiments are carried outwith Docent but we also contrast with the Moses decoder (.", "labels": [], "entities": [{"text": "Docent", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.871506929397583}]}, {"text": "For the purpose of comparison, we use a standard set of sentence-level features used in Moses inmost of our experiments: five translation model features, one language model feature, a distance-based reordering penalty, and a word count feature.", "labels": [], "entities": []}, {"text": "For feature weight optimization we also apply the standard settings in the Moses toolkit.", "labels": [], "entities": [{"text": "feature weight optimization", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8031132618586222}]}, {"text": "We optimize towards the Bleu metric, and optimization ends either when no weights are changed by more than 0.00001, or after 25 iterations.", "labels": [], "entities": [{"text": "Bleu metric", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.7920510768890381}]}, {"text": "MERT is used unless otherwise noted.", "labels": [], "entities": [{"text": "MERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7012396454811096}]}, {"text": "Except for one of our baselines, we always run Docent with random initialization.", "labels": [], "entities": []}, {"text": "For test we run the document decoder fora maximum of 2 27 iterations with a rejection limit of 100,000.", "labels": [], "entities": []}, {"text": "In our experiments, the decoder always stopped when reaching the rejection limit, usually between 1-5 million iterations.", "labels": [], "entities": [{"text": "rejection", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.935413122177124}]}, {"text": "We show results on the Bleu () and NIST) metrics.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.8239162564277649}, {"text": "NIST) metrics", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9143470525741577}]}, {"text": "For German-English we show the average result and standard deviation of three optimization runs, to control for optimizer instability as proposed by.", "labels": [], "entities": [{"text": "standard", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.966336727142334}]}, {"text": "For English-Swedish we report results on single optimization runs, due to time constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Domain and number of sentences and documents for the corpora", "labels": [], "entities": []}, {"text": " Table 2: Baseline results, where Docent-M is ini- tialized with Moses and Docent-R randomly", "labels": [], "entities": []}, {"text": " Table 3: Results for German-English with varying  sizes of tuning set, where the number of sentences  and documents are varied, as well as the minimum  and maximum number of sentences per document", "labels": [], "entities": []}, {"text": " Table 4: Results for German-English with a vary- ing number of iterations and k-list size (UTK is  the average number of unique translations per doc- ument in the k-lists)", "labels": [], "entities": [{"text": "UTK", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9001705050468445}]}, {"text": " Table 5: Results with different k-list-sample inter- vals for k-lists size 101 (UTK is the average num- ber of unique translations per document in the k- lists)", "labels": [], "entities": [{"text": "UTK", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8111650943756104}]}, {"text": " Table 6: Results with different optimization algo- rithms for German-English", "labels": [], "entities": []}, {"text": " Table 7: Results when using document-level features", "labels": [], "entities": []}]}