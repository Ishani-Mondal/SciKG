{"title": [{"text": "Using Latent Dirichlet Allocation for Child Narrative Analysis", "labels": [], "entities": [{"text": "Child Narrative Analysis", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.7586539288361868}]}], "abstractContent": [{"text": "Child language narratives are used for language analysis, measurement of language development, and the detection of language impairment.", "labels": [], "entities": [{"text": "language analysis", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7749511897563934}]}, {"text": "In this paper, we explore the use of Latent Dirichlet Allocation (LDA) for detecting topics from narratives , and use the topics derived from LDA in two classification tasks: automatic prediction of coherence and language impairment.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 37, "end_pos": 70, "type": "METRIC", "confidence": 0.843377411365509}, {"text": "automatic prediction of coherence", "start_pos": 175, "end_pos": 208, "type": "TASK", "confidence": 0.6677403897047043}]}, {"text": "Our experiments show LDA is useful for detecting the topics that correspond to the narrative structure.", "labels": [], "entities": []}, {"text": "We also observed improved performance for the automatic prediction of coherence and language impairment when we use features derived from the topic words provided by LDA.", "labels": [], "entities": [{"text": "LDA", "start_pos": 166, "end_pos": 169, "type": "DATASET", "confidence": 0.9553579092025757}]}], "introductionContent": [{"text": "Language sample analysis is a common technique used by speech language researchers to measure various aspects of language development.", "labels": [], "entities": [{"text": "Language sample analysis", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5817582607269287}]}, {"text": "These include speech fluency, syntax, semantics, and coherence.", "labels": [], "entities": []}, {"text": "For such analysis, spontaneous narratives have been widely used.", "labels": [], "entities": []}, {"text": "Narrating a story or a personal experience requires the narrator to build a mental model of the story and use the knowledge of semantics and syntax to produce a coherent narrative.", "labels": [], "entities": [{"text": "Narrating a story or a personal experience", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8552846142223903}]}, {"text": "Children learn from a very early age to narrate stories.", "labels": [], "entities": []}, {"text": "The different processes involved in generating a narrative have been shown to provide insights into the language status of children.", "labels": [], "entities": []}, {"text": "There has been some prior work on child language sample analysis using NLP techniques.", "labels": [], "entities": [{"text": "child language sample analysis", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.6620364859700203}]}, {"text": "used a set of linguistic features computed on child speech samples to create language metrics that included age prediction.", "labels": [], "entities": []}, {"text": "combined commonly used measurements in communication disorders with several NLP based features for the prediction of Language Impairment (LI) vs. Typically Developing (TD) children.", "labels": [], "entities": []}, {"text": "The features they used included measures of language productivity, morphosyntactic skills, vocabulary knowledge, sentence complexity, probabilities from language models, standard scores, and error patterns.", "labels": [], "entities": []}, {"text": "In their work, they explored the use of language models and machine learning methods for the prediction of LI on two types of child language data: spontaneous and narrative data.", "labels": [], "entities": [{"text": "prediction of LI", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.821674644947052}]}, {"text": "analyzed the use of coherence in child language and performed automatic detection of coherence from child language transcripts using features derived from narrative structure such as the presence of critical narrative components and the use of narrative elements such as cognitive inferences and social engagement devices.", "labels": [], "entities": []}, {"text": "In another study, used several coherence related features to automatically detect language impairment.", "labels": [], "entities": []}, {"text": "LDA has been used in the field of narrative analysis.", "labels": [], "entities": [{"text": "narrative analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7986023724079132}]}, {"text": "adapted LDA to the task of multiple narrative disentanglement, in which the aim was to tease apart narratives by assigning passages from a text to the subnarratives that they belong to.", "labels": [], "entities": [{"text": "LDA", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.9123984575271606}, {"text": "multiple narrative disentanglement", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.6440365711847941}]}, {"text": "They achieved strong empirical results.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of LDA for child narrative analysis.", "labels": [], "entities": [{"text": "child narrative analysis", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7408033013343811}]}, {"text": "We aim to answer two questions: Can we apply LDA to children narratives to identify meaningful topics?", "labels": [], "entities": []}, {"text": "Can we represent these topics automatically and use them for other tasks, such as coherence detection and language impairment prediction?", "labels": [], "entities": [{"text": "coherence detection", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7921931743621826}, {"text": "language impairment prediction", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.6982684632142385}]}, {"text": "We found that using LDA topic modeling can infer useful topics, and incorporating features derived from such automatic topics improves the performance of coherence classification and language impairment detection over the previously reported results.", "labels": [], "entities": [{"text": "coherence classification", "start_pos": 154, "end_pos": 178, "type": "TASK", "confidence": 0.7021554857492447}, {"text": "language impairment detection", "start_pos": 183, "end_pos": 212, "type": "TASK", "confidence": 0.5948885381221771}]}], "datasetContent": [{"text": "We treat the automatic evaluation of coherence as a classification task.", "labels": [], "entities": []}, {"text": "A transcript could either be classified as coherent or incoherent.", "labels": [], "entities": []}, {"text": "We use the results of as a baseline.", "labels": [], "entities": []}, {"text": "They used the presence of narrative episodes, and the counts of narrative quality elements such as cognitive inferences and social engagement devices as features in the automatic prediction of coherence.", "labels": [], "entities": []}, {"text": "We add the features that we automatically extracted using LDA.", "labels": [], "entities": []}, {"text": "We checked for the presence of at least six of the ten topic words or their synonyms per topic in a window of 3 utterances.", "labels": [], "entities": []}, {"text": "If the topic words were present, we took this as a presence of a topic; otherwise we denoted it as an absence of a topic.", "labels": [], "entities": []}, {"text": "In total, there were 20 topics that we extracted using LDA, which is higher compared to the 8 narrative structure topics that were annotated for by. gives the results for the automatic classification of coherence.", "labels": [], "entities": []}, {"text": "As we observe in, there is an improvement in performance over the baseline.", "labels": [], "entities": []}, {"text": "We attribute this to the inclusion of subtopics that were extracted using LDA.", "labels": [], "entities": []}, {"text": "We extended the use of LDA to create a summary of the narratives.", "labels": [], "entities": []}, {"text": "For the purpose of generating the summary, we considered only the narratives generated by TD children in the training set.", "labels": [], "entities": []}, {"text": "We generated a summary, by choosing 5 utterances corresponding to each topic that was generated using LDA, thereby yielding a summary that consisted of 100 utterances.", "labels": [], "entities": []}, {"text": "We observed that different words were used to represent the same concept.", "labels": [], "entities": []}, {"text": "For example, \"look\" and \"search\" were used to represent the concept of searching for the frog.", "labels": [], "entities": []}, {"text": "Since the narration was based on a picture storybook, many of the children used different terms to refer to the same animal.", "labels": [], "entities": []}, {"text": "For example, \"the deer\" in the story has been interpreted to be \"deer\", \"reindeer\", \"moose\", \"stag\", \"antelope\" by different children.", "labels": [], "entities": []}, {"text": "We created an extended topic vocabulary using Wordnet to include words that were semantically similiar to the topic keywords.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9831288456916809}]}, {"text": "In addition, for an utterance to be: Automatic classification of coherence on a 2-scale coherence level in the summary, we put in the additional constraint that neighbouring utterances within a window of 3 utterances also talk about the same topic.", "labels": [], "entities": []}, {"text": "We used this summary for constructing unigram and bigram word features for the automatic prediction of LI.", "labels": [], "entities": []}, {"text": "The features we constructed for the prediction of LI were as follows: 1.", "labels": [], "entities": [{"text": "prediction of LI", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.6513190170129141}]}, {"text": "Bigrams of the words in the summary 2.", "labels": [], "entities": []}, {"text": "Presence or absence of the words in the summary regardless of the position 3.", "labels": [], "entities": [{"text": "Presence", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9698550701141357}]}, {"text": "Presence or absence of the topics detected by LDA in the narratives 4.", "labels": [], "entities": []}, {"text": "Presence or absence of the topic words that were detected using LDA We used both the topics detected and the presence/absence of topic words as features since the same topic word could be used across several topics.", "labels": [], "entities": [{"text": "Presence", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9362356662750244}]}, {"text": "For example, the words \"frog\", \"dog\", \"boy\", and \"search\" are common across several topics.", "labels": [], "entities": []}, {"text": "We refer to the above features as \"new features\".", "labels": [], "entities": []}, {"text": "gives the results for the automatic prediction of LI using different features.", "labels": [], "entities": []}, {"text": "As we can observe, the performance improves to 0.872 when we add the new features to Gabani's and the narrative structure features.", "labels": [], "entities": []}, {"text": "When we use the new features by themselves to predict language impairment, the performance is the worst.", "labels": [], "entities": []}, {"text": "We attribute this to the fact that other feature sets are richer since these features take into account aspects such as syntax and narrative structure.", "labels": [], "entities": []}, {"text": "We performed feature analysis on the new features to see what features contributed the most.", "labels": [], "entities": []}, {"text": "The top scoring features were the presence or absence of the topics detected by LDA that corresponded to the introduction of the narrative, the resolution of the narrative, the search for the frog in the room, and the search for the frog behind the log.", "labels": [], "entities": [{"text": "resolution of the narrative", "start_pos": 144, "end_pos": 171, "type": "TASK", "confidence": 0.7888609021902084}]}, {"text": "The following bigram features generated from the summary contributed the most: \"deer: Automatic classification of language impairment rock\", \"lost frog\", and \"boy hole\".", "labels": [], "entities": []}, {"text": "Using a subset of these best features did not improve the performance when we added them to the narrative features and Gabani's features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of TD and LI children on a 2- scale coherence level", "labels": [], "entities": []}, {"text": " Table 3: Automatic classification of coherence on a 2-scale coherence level", "labels": [], "entities": []}, {"text": " Table 4: Automatic classification of language im- pairment", "labels": [], "entities": [{"text": "Automatic classification of language im- pairment", "start_pos": 10, "end_pos": 59, "type": "TASK", "confidence": 0.5591078272887638}]}]}