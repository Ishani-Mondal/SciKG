{"title": [], "abstractContent": [{"text": "The present article provides a comprehensive review of the work carried out on developing PRESEMT, a hybrid language independent machine translation (MT) methodology.", "labels": [], "entities": [{"text": "hybrid language independent machine translation (MT)", "start_pos": 101, "end_pos": 153, "type": "TASK", "confidence": 0.7734354510903358}]}, {"text": "This methodology has been designed to facilitate rapid creation of MT systems for uncon-strained language pairs, setting the lowest possible requirements on specialised resources and tools.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9922835230827332}]}, {"text": "Given the limited availability of resources for many languages , only a very small bilingual corpus is required, while language modelling is performed by sampling a large target language (TL) monolingual corpus.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.6942580491304398}]}, {"text": "The article summarises implementation decisions, using the Greek-English language pair as a test case.", "labels": [], "entities": []}, {"text": "Evaluation results are reported, for both objective and subjective metrics.", "labels": [], "entities": []}, {"text": "Finally, main error sources are identified and directions are described to improve this hybrid MT methodology.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.994816243648529}]}], "introductionContent": [], "datasetContent": [{"text": "The evaluation results reported in this article focus on the Greek -English language pair.", "labels": [], "entities": []}, {"text": "Two datasets have been used (a development set and a test set), each of which comprises 200 sentences, with a length of between 7 and 40 words.", "labels": [], "entities": []}, {"text": "For every sentence, exactly one reference translation has been created, by SL-language native speakers and then the translation correctness was cross-checked by TL-language native speakers.", "labels": [], "entities": []}, {"text": "To objectively evaluate the translation accuracy, four automatic evaluation metrics have been chosen, namely BLEU (), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER ().", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9502111077308655}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.702418327331543}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.998802900314331}, {"text": "NIST (NIST 2002)", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.8852462887763977}, {"text": "TER", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9940926432609558}]}, {"text": "When developing the MT methodology, extensive evaluation was carried out at regular intervals ().", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9914500117301941}]}, {"text": "The evolution of translation accuracy is depicted within.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9597762823104858}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9210456013679504}]}, {"text": "The falling trend for TER, signifies a continuously improving translation performance.", "labels": [], "entities": [{"text": "TER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9840478897094727}, {"text": "translation", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9543656706809998}]}, {"text": "The current results fora number of MT systems for the development set are reported in.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9690857529640198}]}, {"text": "These results show that at the current stage of development the proposed approach has a quality exceeding that of WorldLingo and Systran, but is still inferior to Google and Bing.", "labels": [], "entities": []}, {"text": "The results are particularly promising, taking into account that the proposed methodology has been developed fora substantially shorter period than the other systems, and has no language-specific information injected into it.", "labels": [], "entities": []}, {"text": "According to an er-ror analysis carried out, most of the errors are due to the lack of syntactic information (e.g. the inability to distinguish between object/subject).", "labels": [], "entities": []}, {"text": "Also a point which can be improved concerns the mapping of sentence structures from SL to TL.", "labels": [], "entities": []}, {"text": "To address this, additional experiments are currently underway involving larger monolingual corpora.", "labels": [], "entities": []}, {"text": "To fully evaluate translation quality, both objective and subjective evaluation have been implemented.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9798556566238403}]}, {"text": "The latter type is carried out by humans who assess translation quality.", "labels": [], "entities": []}, {"text": "Human evaluation is considered to be more representative of the actual MT quality, though on the other hand it is time-consuming and laborious.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9909968376159668}]}, {"text": "Furthermore, it lacks objectivity (single evaluators may not be consistent in assessing a given translation through time while two evaluators may yield completely different judgements on the same text) and must be repeated for every new test result.", "labels": [], "entities": []}, {"text": "For the human evaluation, for each language pair, a total of 15 language professionals were recruited, who were either language professionals, closely associated with MT tasks, or postgraduate university students in the area of linguistics.", "labels": [], "entities": [{"text": "MT tasks", "start_pos": 167, "end_pos": 175, "type": "TASK", "confidence": 0.9270282685756683}]}, {"text": "Two types of subjective evaluation were carried out.", "labels": [], "entities": []}, {"text": "The first one involves the experts grading translations generated by the PRE-SEMT system regarding their adequacy and fluency.", "labels": [], "entities": [{"text": "PRE-SEMT", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.7071837782859802}]}, {"text": "Adequacy refers to the amount of information from the SL text that is retained in the translation, based on a 1-5 scale of scores (with a score of 1 corresponding to the worst translation).", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9482429027557373}]}, {"text": "Fluency measures whether the translation is well-formed, also on a 1-5 scale, with emphasis being placed on grammaticality.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9646771550178528}]}, {"text": "The second type of subjective evaluation involves direct comparison between the translations generated by PRESEMT and by other established MT systems over the same dataset.", "labels": [], "entities": [{"text": "MT", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.9523935317993164}]}, {"text": "In this case, each evaluator ranks the translations of the different systems, these systems being presented in randomised order to ensure the dependability of the feedback received.", "labels": [], "entities": []}, {"text": "Subjective evaluation activities were carried out during two distinct periods (namely October and December 2012), separated by two months.", "labels": [], "entities": [{"text": "Subjective evaluation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8517143428325653}]}, {"text": "The purpose of implementing two sessions has been to judge the improvement in the system within the intervening period.", "labels": [], "entities": []}, {"text": "Thus, two distinct versions of the EL-EN MT system corresponding to these two time points were used.", "labels": [], "entities": [{"text": "EL-EN MT", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.5016532689332962}]}, {"text": "For ref-erence, the objective evaluation results obtained for the test sentences are listed in.", "labels": [], "entities": []}, {"text": "In both cases, the CRF-based PMG was used since it was more mature at the time of evaluation.", "labels": [], "entities": [{"text": "CRF-based PMG", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.528729647397995}]}, {"text": "A specifically-designed platform has been developed to support subjective evaluation activities . This platform has been used to (a) collect the human evaluators' feedback for the different language pairs and (b) support the subsequent assessment of the results via statistical methods.", "labels": [], "entities": []}, {"text": "In addition, in phase 2 of subjective evaluation, adequacy and fluency measurements were collected for the three operational systems used as reference systems (namely Google Translate, Bing and WorldLingo).", "labels": [], "entities": [{"text": "WorldLingo", "start_pos": 194, "end_pos": 204, "type": "DATASET", "confidence": 0.9108182787895203}]}, {"text": "These operational systems have higher adequacy and fluency values than PRESEMT, as indicated in.", "labels": [], "entities": []}, {"text": "Furthermore, paired t-tests have confirmed that at a 0.99 level of significance, these three systems have statistically superior subjective measurements to the proposed methodology.", "labels": [], "entities": []}, {"text": "To provide a reference, for the same set of 200 sentences, objective metrics are shown in for each system.", "labels": [], "entities": []}, {"text": "As can be seen the relative order of the systems in the subjective evaluations (in terms of adequacy and fluency) is confirmed by the objective measurements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Objective metrics results for PRESEMT  & other MT systems (development set)", "labels": [], "entities": []}, {"text": " Table 2. Effect on PRESEMT translation accu- racy of using the two distinct PMG variants", "labels": [], "entities": [{"text": "PRESEMT translation accu- racy", "start_pos": 20, "end_pos": 50, "type": "METRIC", "confidence": 0.728197431564331}]}, {"text": " Table 3. Objective metrics results for PRESEMT  & other MT systems (test set)", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9299620389938354}]}, {"text": " Table 4. Summary of measurements (in terms of  average and standard deviation) for fluency and  adequacy for various MT systems (test set)", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.95662921667099}, {"text": "MT", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.9712696075439453}]}]}