{"title": [], "abstractContent": [{"text": "Lexical and syntactic simplification aim to make texts more accessible to certain audiences.", "labels": [], "entities": []}, {"text": "Syntactic simplification uses either hand-crafted linguistic rules for deep syntactic transformations, or machine learning techniques to model simpler transformations.", "labels": [], "entities": [{"text": "Syntactic simplification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.950393557548523}]}, {"text": "Lexical simplification performs a lookup for synonyms followed by context and/or frequency-based models.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9080376625061035}]}, {"text": "In this paper we investigate modelling both syntactic and lexical simplification through the learning of general tree transduction rules.", "labels": [], "entities": [{"text": "general tree transduction rules", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.665606215596199}]}, {"text": "Experiments with the Simple En-glish Wikipedia corpus show promising results but highlight the need for clever filtering strategies to remove noisy transformations.", "labels": [], "entities": [{"text": "Simple En-glish Wikipedia corpus", "start_pos": 21, "end_pos": 53, "type": "DATASET", "confidence": 0.6481917500495911}]}, {"text": "A simplifica\u00e7simplifica\u00e7\u02dcsimplifica\u00e7\u00e3o em n\u00edvel lexical e sint\u00e1tico objetiva tornar textos mais acess\u00edveis a certos p\u00fablicos-alvo.", "labels": [], "entities": []}, {"text": "Simplifica\u00e7Simplifica\u00e7\u02dcSimplifica\u00e7\u00e3o em n\u00edvel sint\u00e1tico usa regras confeccionadas manualmente para empregar transforma\u00e7transforma\u00e7\u02dctransforma\u00e7\u00f5es sint\u00e1ticas, ou t\u00e9cnicas de aprendizado de m\u00e1quina para modelar transforma\u00e7transforma\u00e7\u02dctransforma\u00e7\u00f5es mais simples.", "labels": [], "entities": []}, {"text": "Simplifica\u00e7Simplifica\u00e7\u02dcSimplifica\u00e7\u00e3o em n\u00edvel lexical emprega busca por sin\u00f4nimos para palavras complexas seguida por an\u00e1lise de contexto e/ou busca em modelos de frequ\u00eancia de palavras.", "labels": [], "entities": []}, {"text": "Neste trabalho investiga-se a modelagem de ambas estrat\u00e9gias de simplifica\u00e7simplifica\u00e7\u02dcsimplifica\u00e7\u00e3o em n\u00edvel sint\u00e1tico e lexical pelo aprendizado de regras atrav\u00e9s da transdu\u00e7transdu\u00e7\u02dctransdu\u00e7\u00e3o d\u00e9 arvores.", "labels": [], "entities": []}, {"text": "Experimentos com dados da Simple English Wikipedia mostram resultados promissores, por\u00e9m destacam a necessidade de estrat\u00e9gias inteligentes de filtragem para remover transforma\u00e7transforma\u00e7\u02dctransforma\u00e7\u00f5es ruidosas.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic text simplification approaches modify a sentence's structure in order to make it easier to comprehend, whereas lexical text simplification approaches mainly apply localised modifications to words based on their local lexical context (often a sentence).", "labels": [], "entities": [{"text": "Syntactic text simplification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6903279225031534}]}, {"text": "Most work on syntactic simplification is based on hand-crafted rules.", "labels": [], "entities": [{"text": "syntactic simplification", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.880562961101532}]}, {"text": "Manually crafting syntactic rules is costly and time consuming, which often results in sets of rules that are virtually purely syntactic and thus only cover very general phenomena.", "labels": [], "entities": []}, {"text": "One such example is the splitting of sentences with multiple clauses, where very little lexical information, such as a small set of clause markers (e.g. relative pronouns or conjunctions), is represented.", "labels": [], "entities": [{"text": "splitting of sentences with multiple clauses", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.8247493207454681}]}, {"text": "Adding lexical information to such rules would make the rule creation process even more costly.", "labels": [], "entities": [{"text": "rule creation", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.8806235194206238}]}, {"text": "It is often the case that these rules only apply to a small subset of the complex sentences.", "labels": [], "entities": []}, {"text": "For example, in an experiment with the rules available in]'s system on a random subset of sentences deemed complex in the Simple English Wikipedia corpus, we found that only 30% of these sentences are covered by the rules available.", "labels": [], "entities": [{"text": "Simple English Wikipedia corpus", "start_pos": 122, "end_pos": 153, "type": "DATASET", "confidence": 0.8740407228469849}]}, {"text": "Efforts on using statistical and machine learning techniques to address syntactic simplification include several approaches inspired by phrase-or tree-based statistical models for machine translation, which learn limited transformations such as short-distance reordering.", "labels": [], "entities": [{"text": "syntactic simplification", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.7150217443704605}, {"text": "machine translation", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7360828816890717}]}, {"text": "[] design templates for subject-verb-object simplification, which limit the application of rules to cases matching the templates.", "labels": [], "entities": []}, {"text": "A more general approach is proposed in using a quasi-synchronous grammar and integer programming to generate syntactic simplification rules.", "labels": [], "entities": []}, {"text": "These are generally precise, but at the cost of low coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9855066537857056}]}, {"text": "Most work on lexical simplification is based on synonym substitution using frequency-related or readability statistics, and also context models based mostly on bagof-words to identify whether a candidate substitution fits the context].", "labels": [], "entities": [{"text": "lexical simplification", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.716732993721962}, {"text": "synonym substitution", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8974372446537018}]}, {"text": "For a comprehensive overview, we refer the reader to the SemEval-2012 shared task on the topic.", "labels": [], "entities": []}, {"text": "Overall, lexical simplification work disregards explicit syntactic cues.", "labels": [], "entities": []}, {"text": "To overcome these limitations we investigate the learning of tree transduction rules to produce larger volumes of both lexical and lexicalized syntactic simplification rules.", "labels": [], "entities": []}, {"text": "This approach has the potential to produce rule sets with high coverage and variability, and at the same time make them more specific by lexicalising some of the rule components of syntactic simplification rules.", "labels": [], "entities": []}], "datasetContent": [{"text": "Training Corpus For the extraction of the tree transformations, we use 133K pairs of original-simple sentences from the Simple Wikipedia corpus parsed by the Stanford constituency parser.", "labels": [], "entities": [{"text": "Simple Wikipedia corpus parsed", "start_pos": 120, "end_pos": 150, "type": "DATASET", "confidence": 0.7910231426358223}]}, {"text": "Language Model For this experiment a 3-gram language model is used to rank candidate simplifications.", "labels": [], "entities": []}, {"text": "The model is trained on a superset of the Simple Wikipedia corpus containing 710K simple sentences.", "labels": [], "entities": [{"text": "Simple Wikipedia corpus", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.8687479297320048}]}, {"text": "We use the SRILM toolkit] to build the 3-gram language model.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8368581235408783}]}, {"text": "The motivation to use a 3-gram rather than a 4 or 5-gram language model is based on the studies presented at, which suggests that language models of higher order tend to lead to data sparsity when the corpus is either noisy or not large enough.", "labels": [], "entities": []}, {"text": "Test Corpus The corpus to be simplified in our tests contains 130 original sentences randomly selected from a held-out portion of the parallel Simple Wikipedia corpus.", "labels": [], "entities": [{"text": "Simple Wikipedia corpus", "start_pos": 143, "end_pos": 166, "type": "DATASET", "confidence": 0.8280910849571228}]}, {"text": "All sentences have a simpler version in the corpus which is different from the original version, and vary between 50 and 240 characters in length.", "labels": [], "entities": []}, {"text": "Evaluation Metrics We evaluate the simplification both automatically by the string matching metric BLEU, and manually using five human subjects.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9690420031547546}]}, {"text": "Our system produces two simplified versions for each sentence, resulting in 260 simplified sentences to be evaluated.", "labels": [], "entities": []}, {"text": "As part of the manual evaluation, each evaluator is assigned 100 sentences: the syntactically and lexically simplified versions of 50 originally complex sentences.", "labels": [], "entities": []}, {"text": "30 of these sentences are common to all evaluators so that inter-annotator agreement can be computed.", "labels": [], "entities": []}, {"text": "Evaluators are asked to answer three questions for each syntactically or lexically simplified sentence: \u2022 Is it simpler than the original sentence?", "labels": [], "entities": []}, {"text": "\u2022 Does it preserve the meaning of the original sentence?", "labels": [], "entities": []}, {"text": "All answers are binary (yes/no), but not applicable is also an option, e.g. for cases that are too ungrammatical for one to judge simplicity.", "labels": [], "entities": []}, {"text": "Short guidelines specify that a simpler sentence must have a structure which eases its understanding by the reader and/or have fewer terms which may challenge non-native readers.", "labels": [], "entities": []}, {"text": "To make the evaluation less subjective, all evaluators are non-native speakers of English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Manual evaluation results", "labels": [], "entities": []}]}