{"title": [{"text": "PhraseFix: Statistical Post-Editing of TectoMT", "labels": [], "entities": []}], "abstractContent": [{"text": "We present two English-to-Czech systems that took part in the WMT 2013 shared task: TECTOMT and PHRASEFIX.", "labels": [], "entities": [{"text": "WMT 2013 shared task", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.6556454300880432}, {"text": "TECTOMT", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.8849778771400452}, {"text": "PHRASEFIX", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.7800682187080383}]}, {"text": "The former is a deep-syntactic transfer-based system, the latter is a more-or-less standard statistical post-editing (SPE) applied on top of TECTOMT.", "labels": [], "entities": []}, {"text": "Ina brief survey, we put SPE in context with other system combination techniques and evaluate SPE vs. another simple system combination technique: using synthetic parallel data from TECTOMT to train a statistical MT system (SMT).", "labels": [], "entities": [{"text": "SPE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9689128398895264}, {"text": "MT system (SMT)", "start_pos": 213, "end_pos": 228, "type": "TASK", "confidence": 0.7548085808753967}]}, {"text": "We confirm that PHRASEFIX (SPE) improves the output of TECTOMT, and we use this to analyze errors in TEC-TOMT.", "labels": [], "entities": [{"text": "PHRASEFIX (SPE", "start_pos": 16, "end_pos": 30, "type": "METRIC", "confidence": 0.8059156735738119}]}, {"text": "However, we also show that extending data for SMT is more effective.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9957677125930786}]}], "introductionContent": [{"text": "This paper describes two submissions to the WMT 2013 shared task: 1 TECTOMT -a deepsyntactic tree-to-tree system and PHRASEFIXstatistical post-editing of TECTOMT using Moses ( ).", "labels": [], "entities": [{"text": "WMT 2013 shared task", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.5635543689131737}, {"text": "PHRASEFIXstatistical", "start_pos": 117, "end_pos": 137, "type": "METRIC", "confidence": 0.9565184116363525}]}, {"text": "We also report on experiments with another hybrid method where TEC-TOMT is used to produce additional (so-called synthetic) parallel training data for Moses.", "labels": [], "entities": [{"text": "TEC-TOMT", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.8168824315071106}]}, {"text": "This method was used in CU-BOJAR and CU-DEPFIX submissions, see.", "labels": [], "entities": [{"text": "CU-BOJAR", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.8978308439254761}]}], "datasetContent": [{"text": "All our systems (including TECTOMT) were trained on the CzEng (Bojar et al., 2012) parallel corpus (development and evaluation subsets were omitted), see for statistics.", "labels": [], "entities": [{"text": "CzEng (Bojar et al., 2012) parallel corpus", "start_pos": 56, "end_pos": 98, "type": "DATASET", "confidence": 0.7303674668073654}]}, {"text": "We translated the English side of CzEng with TECTOMT to obtain \"synthetic Czech\".", "labels": [], "entities": [{"text": "TECTOMT", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9810540080070496}]}, {"text": "This way we obtained anew parallel corpus, denoted tmt (CzEng), with English \u2194 synthetic Czech sentences.", "labels": [], "entities": []}, {"text": "Analogically, we translated the WMT 2013 test set (newstest2013) with TECTOMT and obtained tmt (newstest2013).", "labels": [], "entities": [{"text": "WMT 2013 test set", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.9535404145717621}, {"text": "TECTOMT", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9876185655593872}]}, {"text": "Our baseline SMT system (Moses) trained on CzEng corpus only was then also used for WMT 2013 test set translation, and we obtained smt (newstest2013).", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9538853764533997}, {"text": "CzEng corpus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8230454921722412}, {"text": "WMT 2013 test set translation", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.7748683035373688}, {"text": "newstest2013", "start_pos": 136, "end_pos": 148, "type": "DATASET", "confidence": 0.9209365844726562}]}, {"text": "For all MERT tuning, newstest2011 was used.", "labels": [], "entities": [{"text": "MERT", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.47526997327804565}, {"text": "newstest2011", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.898955225944519}]}, {"text": "We trained abase SPE system as described in Section 2.1 and dubbed it PHRASEFIX.", "labels": [], "entities": [{"text": "SPE", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8979121446609497}, {"text": "PHRASEFIX", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.7041164636611938}]}, {"text": "First two rows of show that the firststage TECTOMT system (serving here as the baseline) was significantly improved in terms of BLEU () by PHRASEFIX (p < 0.001 according to the paired bootstrap test), but the difference in TER () is not significant.", "labels": [], "entities": [{"text": "TECTOMT", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.8190752267837524}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9993318915367126}, {"text": "PHRASEFIX", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9893396496772766}, {"text": "TER", "start_pos": 223, "end_pos": 226, "type": "METRIC", "confidence": 0.99880051612854}]}, {"text": "The preliminary results of WMT 2013 manual evaluation show only a minor improvement: TECTOMT=0.476 vs. PHRASEFIX=0.484 (higher means better, for details on the ranking see Callison-Burch et al. (2012)).", "labels": [], "entities": [{"text": "WMT 2013 manual evaluation", "start_pos": 27, "end_pos": 53, "type": "DATASET", "confidence": 0.7728260308504105}, {"text": "TECTOMT", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.999686598777771}, {"text": "PHRASEFIX", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9970654845237732}]}, {"text": "The BLEU and TER results reported here slightly differ from the results shown at http://matrix.statmt.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9979913234710693}, {"text": "TER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.994088888168335}]}, {"text": "org/matrix/systems_list/1720 because of different tokenization and normalization.", "labels": [], "entities": []}, {"text": "It seems that statmt.org disables the --international-tokenization switch, so e.g. the correct Czech quotes (\"word\") are not tokenized, hence the neighboring tokens are never counted as matching the reference (which is tokenized as \" word \").", "labels": [], "entities": []}, {"text": "Despite of the improvement, PHRASEFIX's phrase table (synthetic Czech \u2194 genuine Czech) still contains many wrong phrase pairs that worsen the TECTOMT output instead of improving it.", "labels": [], "entities": [{"text": "TECTOMT", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.8715985417366028}]}, {"text": "They naturally arise in cases where the genuine Czech is a too loose translation (or when the English-Czech sentence pair is simply misaligned in CzEng), and the word alignment between genuine and synthetic Czech struggles.", "labels": [], "entities": []}, {"text": "Apart from removing such garbage phrase pairs, it would also be beneficial to have some control over the SPE.", "labels": [], "entities": []}, {"text": "For instance, we would like to generally prefer the original output of TECTOMT except for clear errors, so only reliable phrase pairs should be used.", "labels": [], "entities": []}, {"text": "We examine several strategies: Phrase table filtering.", "labels": [], "entities": [{"text": "Phrase table filtering", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.885576327641805}]}, {"text": "We filter out all phrase pairs with forward probability \u2264 0.7 and all singleton phrase pairs.", "labels": [], "entities": [{"text": "forward probability", "start_pos": 36, "end_pos": 55, "type": "METRIC", "confidence": 0.9346361756324768}]}, {"text": "These thresholds were set based on our early experiments.", "labels": [], "entities": []}, {"text": "Similar filtering was used by.", "labels": [], "entities": []}, {"text": "This strategy is similar to the previous one, but the low-frequency phrase pairs are not filtered-out.", "labels": [], "entities": []}, {"text": "Instead, a special feature marking these pairs is added.", "labels": [], "entities": []}, {"text": "The subsequent MERT of the SPE system selects the best weight for this indicator feature.", "labels": [], "entities": [{"text": "MERT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9593024849891663}, {"text": "SPE", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.7523047924041748}]}, {"text": "The frequency and probability thresholds for marking a phrase pair are the same as in the previous case.", "labels": [], "entities": []}, {"text": "Marking of identities A special feature indicating the equality of the source and target phrase in a phrase pair is added.", "labels": [], "entities": [{"text": "Marking of identities", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8684941728909811}]}, {"text": "In general, if the output of TECTOMT matched the reference, then such output was probably good and does not need any post-editing.", "labels": [], "entities": []}, {"text": "These phrase pairs should be perhaps slightly preferred by the SPE.", "labels": [], "entities": [{"text": "SPE", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.5554917454719543}]}, {"text": "As apparent from Table 2, marking either reliable phrases or identities is useful in our SPE setting in terms of BLEU score.", "labels": [], "entities": [{"text": "SPE", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8509740829467773}, {"text": "BLEU score", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9812667071819305}]}, {"text": "In terms of TER measure, marking the identities slightly improves PHRASEFIX.", "labels": [], "entities": [{"text": "TER measure", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9850890040397644}, {"text": "PHRASEFIX", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.997215747833252}]}, {"text": "However, none of the improvements is statistically significant.", "labels": [], "entities": []}, {"text": "We now describe experiments with phrase table and corpus combination.", "labels": [], "entities": []}, {"text": "There is a trade-off in the choice: the source side of the test set is obviously most useful for the given input, but it restricts the applicability (all systems must be installed or available online in the testing time) and speed (we must wait for the slowest system and the combination).", "labels": [], "entities": []}, {"text": "So far, in PTComb we tried adding the full synthetic CzEng (\"CzEng + tmt (CzEng)\"), adding the test set (\"CzEng + tmt (newstest2013)\" and \"CzEng + smt (newstest2013)\"), and adding both (\"CzEng + tmt (CzEng) + tmt (newstest2013)\").", "labels": [], "entities": [{"text": "PTComb", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.9674394130706787}]}, {"text": "In CComb, we concatenated CzEng and full synthetic CzEng (\"CzEng + tmt (CzEng)\").", "labels": [], "entities": []}, {"text": "There are two flavors of PTComb: either the two phrase tables are used both at once as alternative decoding paths (\"Alternative\"), where each source span is equipped with translation options from any of the tables, or the synthetic Czech phrase table is used only as a back-off method if a source phrase is not available in the primary table (\"Back-off\").", "labels": [], "entities": []}, {"text": "The back-off model was applied to source phrases of up to 5 tokens.", "labels": [], "entities": []}, {"text": "summarizes our results with phrase table and corpus combination.", "labels": [], "entities": []}, {"text": "We see that adding synthetic data unrelated to the test set does bring only a small benefit in terms of BLEU in the case of CComb, and we see a small improvement in TER in two cases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9993495345115662}, {"text": "TER", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.998441755771637}]}, {"text": "Adding the (synthetic) translation of the test set helps.", "labels": [], "entities": []}, {"text": "However, adding translated source side of the test set is helpful only if it is translated by the TECTOMT system.", "labels": [], "entities": []}, {"text": "If our baseline system is used for this translation, the results even slightly drop.", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9760581851005554}]}, {"text": "Somewhat related experiments for pivot languages by  showed a significant gain when the outputs of a rule-based system were added to the training data of Moses.", "labels": [], "entities": []}, {"text": "In their case however, the genuine parallel corpus was much smaller than the synthetic data.", "labels": [], "entities": []}, {"text": "The benefit of unrelated synthetic data seems to vanish with larger parallel data available.: Automatic (BLEU) and manual (number of sentences judged better than the other system) evaluation of SPE vs. PTComb.", "labels": [], "entities": [{"text": "Automatic (BLEU)", "start_pos": 94, "end_pos": 110, "type": "METRIC", "confidence": 0.8423287272453308}, {"text": "SPE", "start_pos": 194, "end_pos": 197, "type": "TASK", "confidence": 0.710438072681427}, {"text": "PTComb", "start_pos": 202, "end_pos": 208, "type": "DATASET", "confidence": 0.7864916324615479}]}], "tableCaptions": [{"text": " Table 2: Comparison of several strategies of SPE.  Best results are in bold.", "labels": [], "entities": [{"text": "SPE", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9726881980895996}]}, {"text": " Table 3: Comparison of several strategies used for Synthetic Data Combination (PTComb -phrase table  combination and CComb -corpus combination).", "labels": [], "entities": [{"text": "Synthetic Data Combination", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.8475772738456726}]}, {"text": " Table 4: Automatic (BLEU) and manual (number  of sentences judged better than the other system)  evaluation of SPE vs. PTComb.", "labels": [], "entities": [{"text": "Automatic (BLEU)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8246016502380371}, {"text": "SPE", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.7318146228790283}, {"text": "PTComb", "start_pos": 120, "end_pos": 126, "type": "DATASET", "confidence": 0.7853326797485352}]}]}