{"title": [{"text": "Semantic Similarity Computation for Abstract and Concrete Nouns Using Network-based Distributional Semantic Models", "labels": [], "entities": [{"text": "Semantic Similarity Computation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8395740787188212}]}], "abstractContent": [{"text": "Motivated by cognitive lexical models, network-based distributional semantic models (DSMs) were proposed in [Iosif and Potamianos (2013)] and were shown to achieve state-of-the-art performance on semantic similarity tasks.", "labels": [], "entities": []}, {"text": "Based on evidence for cognitive organization of concepts based on degree of concreteness, we investigate the performance and organization of network DSMs for abstract vs. concrete nouns.", "labels": [], "entities": []}, {"text": "Results show a \"concrete-ness effect\" for semantic similarity estimation.", "labels": [], "entities": [{"text": "semantic similarity estimation", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.8271997968355814}]}, {"text": "Network DSMs that implement the maximum sense similarity assumption perform best for concrete nouns, while attributional network DSMs perform best for abstract nouns.", "labels": [], "entities": []}, {"text": "The performance of metrics is evaluated against human similarity ratings on an English and a Greek corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic similarity is the building block for numerous applications of natural language processing (NLP), such as grammar induction] and affective text categorization.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.7588188846906027}, {"text": "grammar induction", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.7206154316663742}]}, {"text": "Distributional semantic models (DSMs)] are based on the distributional hypothesis of meaning] assuming that semantic similarity between words is a function of the overlap of their linguistic contexts.", "labels": [], "entities": []}, {"text": "DSMs are typically constructed from co-occurrence statistics of word tuples that are extracted from a text corpus or from data harvested from the web.", "labels": [], "entities": []}, {"text": "A wide range of contextual features are also used by DSM exploiting lexical, syntactic, semantic, and pragmatic information.", "labels": [], "entities": []}, {"text": "DSMs have been successfully applied to the problem of semantic similarity computation.", "labels": [], "entities": [{"text": "semantic similarity computation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.850824256738027}]}, {"text": "Recently proposed network-based DSMs motivated by the organization of words, attributes and concepts inhuman cognition.", "labels": [], "entities": []}, {"text": "The proposed semantic networks can operate under either the attributional similarity or the maximum sense similarity assumptions of lexical semantics.", "labels": [], "entities": []}, {"text": "According to attributional similarity], semantic similarity between words is based on the commonality of their sense attributes.", "labels": [], "entities": []}, {"text": "Following the maximum sense similarity hypothesis, the semantic similarity of two words can be estimated as the similarity of their two closest senses].", "labels": [], "entities": []}, {"text": "Network-based DSMs have been shown to achieve state-of-the-art performance for semantic similarity tasks.", "labels": [], "entities": [{"text": "semantic similarity tasks", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.8229530254999796}]}, {"text": "Typically, the degree of semantic concreteness of a word is not taken into account in distributional models.", "labels": [], "entities": []}, {"text": "However, evidence from neuro-and psycho-linguistics demonstrates significant differences in the cognitive organization of abstract and concrete nouns.", "labels": [], "entities": []}, {"text": "For example, and show that concrete concepts are processed more efficiently than abstract ones (aka \"the concreteness effect\"), i.e., participants in lexical decision tasks recall concrete stimuli faster than abstract.", "labels": [], "entities": []}, {"text": "According to dual code theory], the stored semantic information for concrete concepts is both verbal and visual, while for abstract concepts stored information is only verbal.", "labels": [], "entities": []}, {"text": "Neuropsychological studies show that people with acquired dyslexia (deep dyslexia) face problems in reading abstract nouns aloud, verifying that concrete and abstract concepts are stored in different regions of the human brain anatomy].", "labels": [], "entities": []}, {"text": "The reversal concreteness effect is also reported for people with semantic dementia with a striking impairment in semantic memory].", "labels": [], "entities": [{"text": "reversal concreteness", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8128469288349152}]}, {"text": "Motivated by this evidence, we study the semantic network organization and performance of DSMs for estimating the semantic similarity of abstract vs. concrete nouns.", "labels": [], "entities": []}, {"text": "Specifically, we investigate the validity of the maximum sense and attributional similarity assumptions in network-based DSMs for abstract and concrete nouns (for both English and Greek).", "labels": [], "entities": []}, {"text": "Semantic similarity metrics can be divided into two broad categories: (i) metrics that rely on knowledge resources, and (ii) corpus-based metrics.", "labels": [], "entities": [{"text": "Semantic similarity metrics", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8090548316637675}]}, {"text": "A representative example of the first category are metrics that exploit the WordNet ontology].", "labels": [], "entities": []}, {"text": "Corpus-based metrics are formalized as DSM] and are based on the distributional hypothesis of meaning].", "labels": [], "entities": [{"text": "DSM", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9483498930931091}]}, {"text": "DSM can be categorized into unstructured (unsupervised) that employ a bag-of-words model] and structured that rely on syntactic relationships between words [Pado and].", "labels": [], "entities": []}, {"text": "Recently, motivated by the graph theory, several aspects of the human languages have been modeled using network-based methods.", "labels": [], "entities": []}, {"text": "In, an overview of network-based approaches is presented fora number of NLP problems.", "labels": [], "entities": []}, {"text": "Different types of language units can be regarded as vertices of such networks, spanning from single words to sentences.", "labels": [], "entities": []}, {"text": "Typically, network edges represent the relations of such units capturing phenomena such as co-occurrence, syntactic dependencies, and lexical similarity.", "labels": [], "entities": []}, {"text": "An example of a large co-occurrence network is presented in] for the automatic creation of semantic classes.", "labels": [], "entities": []}, {"text": "In], anew paradigm for implementing DSMs is proposed: a two tier system in which corpus statistics are parsimoniously encoded in a network, while the task of similarity computation is shifted (from corpus-based techniques) to operations over network neighborhoods.", "labels": [], "entities": []}], "datasetContent": [{"text": "Lexica and corpora creation: For English we used a lexicon consisting of 8, 752 English nouns taken from the SemCor3 1 corpus.", "labels": [], "entities": [{"text": "corpora creation", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.747326135635376}, {"text": "SemCor3 1 corpus", "start_pos": 109, "end_pos": 125, "type": "DATASET", "confidence": 0.83835901816686}]}, {"text": "In addition, this lexicon was translated into Greek using Google Translate 2 , while it was further augmented resulting into a set of 9, 324 entries.", "labels": [], "entities": []}, {"text": "For each noun an individual query was formulated and the 1, 000 top ranked results (document snippets) were retrieved using the Yahoo!", "labels": [], "entities": []}, {"text": "search engine 3 . A corpus was created for each language by aggregating the snippets for all nouns of the lexicon.", "labels": [], "entities": []}, {"text": "Network creation: For each language the semantic neighborhoods of lexicon noun pairs were computed following the procedure described in Section 4 using either co-occurrence D or context-based Q H=1 metrics 4 . Network-based similarity computation: For each language, the semantic similarity between noun pairs was computed applying either the max-sense Mn or the attributional Rn network-based metric.", "labels": [], "entities": [{"text": "Network creation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7509579658508301}]}, {"text": "The Pearson's correlation coefficient was used as evaluation metric to compare estimated similarities against the ground truth (human ratings).", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.7984780967235565}]}, {"text": "The following datasets were used: English (WS353): Subset of WS353 dataset] consisting of 272 noun pairs (that are also included in the SemCor3 corpus).", "labels": [], "entities": [{"text": "English (WS353)", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.6541701704263687}, {"text": "WS353 dataset", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9853537678718567}, {"text": "SemCor3 corpus", "start_pos": 136, "end_pos": 150, "type": "DATASET", "confidence": 0.8150089681148529}]}, {"text": "Greek (GIP): In total, 82 native speakers of modern Greek were asked to score the similarity of the noun pairs in a range from 0 (dissimilar) to 4 (similar).", "labels": [], "entities": [{"text": "similarity", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9429868459701538}]}, {"text": "The resulting dataset consists of 99 nouns pairs (a subset of pairs translated from WS353) and is freely available . Abstract vs. Concrete: From each of the above datasets two subsets of pairs were selected, where both nouns in the pair are either abstract or concrete, i.e., pairs consisting of one abstract and one concrete nouns were ruled out.", "labels": [], "entities": [{"text": "WS353", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9730623364448547}]}, {"text": "More specifically, 74 abstract and 74 concrete noun pairs were selected from WS353, fora total of 148 pairs.", "labels": [], "entities": [{"text": "WS353", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.9660145044326782}]}, {"text": "Regarding GIP, 18 abstract and 18 concrete noun pairs were selected, fora total of 36 pairs.", "labels": [], "entities": [{"text": "GIP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.4524535834789276}]}], "tableCaptions": [{"text": " Table 1: Pearson correlation with human ratings for neighborhood-based metrics for English and Greek datasets.  Four combinations of the co-occurrence-based metric D and the context-based metric Q H were used for the defi- nition of semantic neighborhoods and the computation of similarity scores. Baseline performance is also shown.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8048423826694489}]}, {"text": " Table 2: Distribution of abstract vs. concrete nouns in (abstract/concrete noun) neighbourhoods.", "labels": [], "entities": []}]}