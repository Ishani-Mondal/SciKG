{"title": [{"text": "Description of HLJU Chinese Spelling Checker for SIGHAN Bakeoff 2013", "labels": [], "entities": [{"text": "HLJU Chinese Spelling Checker", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6705175489187241}, {"text": "SIGHAN Bakeoff 2013", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.7412658433119456}]}], "abstractContent": [{"text": "In this paper, we describe in brief our system for Chinese Spelling Check Backof-f sponsored by ACL-SIGHAN.", "labels": [], "entities": [{"text": "Chinese Spelling Check Backof-f", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.5838581025600433}, {"text": "ACL-SIGHAN", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.8998264670372009}]}, {"text": "It consists of three main components, namely potential incorrect character detection with a multiple-level analysis, correction candidate generation with similar character sets and correction scoring with n-grams.", "labels": [], "entities": [{"text": "character detection", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7450400441884995}, {"text": "correction candidate generation", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.6144306659698486}, {"text": "correction scoring", "start_pos": 181, "end_pos": 199, "type": "METRIC", "confidence": 0.8181260228157043}]}, {"text": "We participated in all the two sub-tasks at the Bakeoff.", "labels": [], "entities": [{"text": "the Bakeoff", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.6659362614154816}]}, {"text": "We also make a summary of this work and give some analysis on the results .", "labels": [], "entities": []}], "introductionContent": [{"text": "As one typical task in written language processing, spelling check is aiming at detecting incorrect characters within a sentence and correcting them.", "labels": [], "entities": [{"text": "spelling check", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.8366080522537231}]}, {"text": "While a number of successful spelling checker have been available for English and many other other alphabetical languages, it is still a challenge to develop a practical spelling checker for Chinese due to its language-specific issues, in particular the writing system of Chinese without explicit delimiters for word boundaries.", "labels": [], "entities": [{"text": "spelling checker", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.8448022902011871}]}, {"text": "Furthermore, no data set are commonly available for spelling check in Chinese.", "labels": [], "entities": [{"text": "spelling check", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.898677259683609}]}, {"text": "As such, ACL-SIGHAN sponsor a Backoff on Chinese spelling check, which consists of two subtasks, namely spelling error detection and spelling error correction.", "labels": [], "entities": [{"text": "spelling error detection", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.6502379973729452}, {"text": "spelling error correction", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.6836314996083578}]}, {"text": "Based on the task specification the data sets for SIGHAN, we develop a spelling checker for Chinese.", "labels": [], "entities": [{"text": "SIGHAN", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.6437051296234131}]}, {"text": "It consists of three main components, namely potential incorrect character detection with a multiple-level analysis, correction candidate generation with similar character sets and correction scoring with n-grams.", "labels": [], "entities": [{"text": "character detection", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7450400441884995}, {"text": "correction candidate generation", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.6144306659698486}, {"text": "correction scoring", "start_pos": 181, "end_pos": 199, "type": "METRIC", "confidence": 0.8181260228157043}]}, {"text": "We have participated in all the two sub-tasks at the Bakeoff.", "labels": [], "entities": [{"text": "the Bakeoff", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.5773174166679382}]}, {"text": "We also make a summary of this work and give some analysis on the results.", "labels": [], "entities": []}, {"text": "The rest of this paper is organize as follows.", "labels": [], "entities": []}, {"text": "First, we describe in brief our system for Chinese spelling check in Section 2.", "labels": [], "entities": [{"text": "Chinese spelling check", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6830679575602213}]}, {"text": "Then in Section 3, we present the settings or configuration of our system for different subtasks, and report the relevant results at this Bakeoff.", "labels": [], "entities": []}, {"text": "Finally, we give our conclusions on this work in Section 4.", "labels": [], "entities": []}, {"text": "shows the architecture of our system.", "labels": [], "entities": []}, {"text": "It works in three main steps.", "labels": [], "entities": []}, {"text": "Given a plain Chinese sentence with/without spelling errors, our system first segments it to words.", "labels": [], "entities": []}, {"text": "Then, a multilevel analysis module is used to detect potential incorrect characters within the input and thus a 5401\u00d75401 similarity matrix generated from the similar character set (viz.", "labels": [], "entities": []}, {"text": "the Bakeoff 2013 CSC Datasets) () is further employed to generate set of corrections for the input.", "labels": [], "entities": [{"text": "Bakeoff 2013 CSC Datasets)", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.8796373963356018}]}, {"text": "Finally, ngrams are used to score and decode a sentence as the best correction for the input.", "labels": [], "entities": []}, {"text": "For convenience, we refer to this sentence as output sentence.", "labels": [], "entities": []}, {"text": "If the output sentence is same as the original input sentence, then the input sentence does not contain any spelling errors; Or else, it has incorrect characters, and the output sentence would be its correction.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our system participated in both subTask at the Chinese Spelling Check Bakeoff.", "labels": [], "entities": [{"text": "Chinese Spelling Check Bakeoff", "start_pos": 47, "end_pos": 77, "type": "DATASET", "confidence": 0.8776633739471436}]}, {"text": "This section reports the results and discussions on its evaluation.", "labels": [], "entities": []}, {"text": "As mentioned above, SIGHAN Bakeoff 2013 consists of two sub-tasks: namely error detection (viz. Subtask 1) and error correction (viz. Subtask2).", "labels": [], "entities": [{"text": "SIGHAN Bakeoff 2013", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.7078450322151184}, {"text": "error detection", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7640467584133148}, {"text": "error correction", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.7712432742118835}]}, {"text": "For the error detection task, the system should return the locations of the incorrect characters fora given Chinese sentence that may have or do not have spelling errors, while in Subtask2, the system should return the locations of the incorrect characters within the input and correct them.", "labels": [], "entities": [{"text": "error detection", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.6853035539388657}]}, {"text": "Obviously, Subtask2 is a follow-up problem of error detection for sentences with errors.", "labels": [], "entities": [{"text": "error detection", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7198545634746552}]}, {"text": "In SIGHAN Bakeoff 2013, ninth measures for subTask1 and three measures for subTask2 are employed to score the performance of a spelling correction system.", "labels": [], "entities": [{"text": "SIGHAN Bakeoff 2013", "start_pos": 3, "end_pos": 22, "type": "DATASET", "confidence": 0.8198785980542501}, {"text": "spelling correction", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.9146977663040161}]}, {"text": "They are False-Alarm Rate(FAR), Detection Accuracy(DA), Detection Precision(DP), Detection Recall(DR), Detection F-score(DF), Error Location Accuracy(ELA), Error Location Recall(ELR), Error Location Fscore(ELF), Location Accuracy(LA), Correction Accuracy(CA) and Correction Precision(CP).", "labels": [], "entities": [{"text": "False-Alarm Rate(FAR)", "start_pos": 9, "end_pos": 30, "type": "METRIC", "confidence": 0.9483364820480347}, {"text": "Detection Recall(DR)", "start_pos": 81, "end_pos": 101, "type": "METRIC", "confidence": 0.8977827548980712}, {"text": "Detection F-score(DF)", "start_pos": 103, "end_pos": 124, "type": "METRIC", "confidence": 0.7894789814949036}, {"text": "Error Location Accuracy(ELA)", "start_pos": 126, "end_pos": 154, "type": "METRIC", "confidence": 0.8079803983370463}, {"text": "Error Location Fscore(ELF)", "start_pos": 184, "end_pos": 210, "type": "METRIC", "confidence": 0.7531529515981674}, {"text": "Location Accuracy(LA)", "start_pos": 212, "end_pos": 233, "type": "METRIC", "confidence": 0.8720619440078735}, {"text": "Correction Accuracy(CA)", "start_pos": 235, "end_pos": 258, "type": "METRIC", "confidence": 0.8232669353485107}, {"text": "Correction Precision(CP)", "start_pos": 263, "end_pos": 287, "type": "METRIC", "confidence": 0.8038141846656799}]}, {"text": "In our system, we employ the SRILM Toolkit() to build n-gram models for spelling correction selection from the Academia Sinica Segmentation Corpus(3.0)).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.5013615489006042}, {"text": "spelling correction selection", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.9321719209353129}, {"text": "Academia Sinica Segmentation Corpus", "start_pos": 111, "end_pos": 146, "type": "DATASET", "confidence": 0.8667517006397247}]}, {"text": "Furthermore, we use the similar character sets (CSC datasets)() to build the similarity matrix for correct sentence candidate generation.", "labels": [], "entities": [{"text": "correct sentence candidate generation", "start_pos": 99, "end_pos": 136, "type": "TASK", "confidence": 0.6140637546777725}]}, {"text": "In addition, we also uses Academia Sinica Segmentation System (CKIP) to perform word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7724620997905731}]}, {"text": "We use three different sets of parameters presented three sets of results, namely HLJU Run1, HLJU Run2 and HLJU Run3.", "labels": [], "entities": [{"text": "HLJU Run1", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.7345948219299316}, {"text": "HLJU Run2", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.7520884871482849}, {"text": "HLJU Run3", "start_pos": 107, "end_pos": 116, "type": "DATASET", "confidence": 0.8829696178436279}]}, {"text": "See the table 1 below for details: 0.6 0.8 5 \u2264: Parameter Selection.", "labels": [], "entities": []}, {"text": "\u03b1 and \u03b2 have been introduced in section 2.4.", "labels": [], "entities": []}, {"text": "The Similarity less than or equal a value x , it represents only consider the similarity less than x characters in similarity matrix.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9716598987579346}]}, {"text": "For example, the Similarity of Run2 is less than 2, so we consider only two cases, the \"same sound and same tone (SS)\" and \"similar shape\".", "labels": [], "entities": [{"text": "Similarity", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9652961492538452}, {"text": "Run2", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.7813705205917358}]}, {"text": "shows the result of sub-Task1 and shows the result of sub-Task2.", "labels": [], "entities": []}, {"text": "The \"Best\" indicates the high score achieved in Chinese Spelling Check task.", "labels": [], "entities": [{"text": "Chinese Spelling Check task", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.7039734944701195}]}, {"text": "The \"Average\" represents the average level.", "labels": [], "entities": [{"text": "Average\"", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.975681483745575}]}, {"text": "The numbers in boldindicate the highest values of each metric.", "labels": [], "entities": []}, {"text": "From the above table, we can see that results are not satisfactory, and many metrics from the best score is still a certain gap.", "labels": [], "entities": []}, {"text": "The value of FAR is too high, and the precision is low.", "labels": [], "entities": [{"text": "FAR", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9947803020477295}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9998137354850769}]}, {"text": "It means our method causes a lot of false positive errors and shows our system is not strictly for candidate list.", "labels": [], "entities": []}, {"text": "And the parameter setting remains to be determined.", "labels": [], "entities": []}, {"text": "In ad-   1) There are some errors in the training and CSC data set, and we do not deal with it; 2) Our methods are still based on ngram models for correcting spelling errors, and we failed to the breakthrough.", "labels": [], "entities": [{"text": "CSC data set", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.8351134459177653}]}, {"text": "However, the performance of Run2 is much better than other schemes.", "labels": [], "entities": []}, {"text": "We can conclude that low character similarity has no any help for the correction task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation Results of Sub-Task1.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation Results of Sub-Task2.", "labels": [], "entities": []}]}