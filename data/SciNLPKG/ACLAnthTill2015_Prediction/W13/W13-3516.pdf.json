{"title": [{"text": "Towards Robust Linguistic Analysis Using OntoNotes", "labels": [], "entities": [{"text": "Robust Linguistic Analysis", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.6954356630643209}]}], "abstractContent": [{"text": "Large-scale linguistically annotated corpora have played a crucial role in advancing the state of the art of key natural language technologies such as syntactic, semantic and discourse analyzers, and they serve as training data as well as evaluation benchmarks.", "labels": [], "entities": []}, {"text": "Up till now, however, most of the evaluation has been done on mono-lithic corpora such as the Penn Treebank, the Proposition Bank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9936783611774445}, {"text": "Proposition Bank", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.9468620121479034}]}, {"text": "As a result, it is still unclear how the state-of-the-art analyzers perform in general on data from a variety of genres or domains.", "labels": [], "entities": []}, {"text": "The completion of the OntoNotes corpus, a large-scale, multi-genre, multilingual corpus manually annotated with syntactic, semantic and discourse information, makes it possible to perform such an evaluation.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.8329089879989624}]}, {"text": "This paper presents an analysis of the performance of publicly available, state-of-the-art tools on all layers and languages in the OntoNotes v5.0 corpus.", "labels": [], "entities": [{"text": "OntoNotes v5.0 corpus", "start_pos": 132, "end_pos": 153, "type": "DATASET", "confidence": 0.7916867733001709}]}, {"text": "This should set the benchmark for future development of various NLP components in syntax and semantics, and possibly encourage research towards an integrated system that makes use of the various layers jointly to improve overall performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Roughly a million words of text from the Wall Street Journal newswire (WSJ), circa 1989, has had a significant impact on research in the language processing community -especially those in the area of syntax and (shallow) semantics, the reason for this being the seminal impact of the Penn Treebank project which first selected this text for annotation.", "labels": [], "entities": [{"text": "Wall Street Journal newswire (WSJ)", "start_pos": 41, "end_pos": 75, "type": "DATASET", "confidence": 0.9399457659040179}, {"text": "Penn Treebank project", "start_pos": 284, "end_pos": 305, "type": "DATASET", "confidence": 0.9724318385124207}]}, {"text": "Taking advantage of a solid syntactic foundation, later researchers who wanted to annotate semantic phenomena on a relatively large scale, also used it as the basis of their annotation.", "labels": [], "entities": []}, {"text": "For example the Proposition Bank ( ), BBN Name Entity and Pronoun coreference corpus), the Penn Discourse Treebank (, and many other annotation projects, all annotate the same underlying body of text.", "labels": [], "entities": [{"text": "BBN Name Entity and Pronoun coreference corpus", "start_pos": 38, "end_pos": 84, "type": "DATASET", "confidence": 0.7614389700548989}, {"text": "Penn Discourse Treebank", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.9768016537030538}]}, {"text": "It was also converted to dependency structures and other syntactic formalisms such as CCG) and LTAG (, thereby creating an even bigger impact through these additional syntactic resources.", "labels": [], "entities": [{"text": "LTAG", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.621237576007843}]}, {"text": "The most recent one of these efforts is the OntoNotes corpus ).", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.846293717622757}]}, {"text": "However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages -Chinese and Arabic: portions of the Chinese Treebank () and the Arabic Treebank () have been used to sample the genre of text that they represent.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 232, "end_pos": 248, "type": "DATASET", "confidence": 0.8445905148983002}, {"text": "Arabic Treebank", "start_pos": 260, "end_pos": 275, "type": "DATASET", "confidence": 0.7050623148679733}]}, {"text": "One of the current hurdles in language processing is the problem of domain, or genre adaptation.", "labels": [], "entities": [{"text": "language processing", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7081585824489594}, {"text": "genre adaptation", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.7336539775133133}]}, {"text": "Although genre or domain are popular terms, their definitions are still vague.", "labels": [], "entities": []}, {"text": "In OntoNotes, \"genre\" means a type of source -newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), telephone conversation (TC), web data (WB) or pivot text (PT).", "labels": [], "entities": []}, {"text": "Changes in the entity and event profiles across source types, and even in the same source over a time duration, as explicitly expressed by surface lexical forms, usually account fora lot of the decrease in performance of models trained on one source and tested on another, usually because these are the salient cues that are relied upon by statistical models.", "labels": [], "entities": []}, {"text": "Large-scale corpora annotated with multiple layers of linguistic information exist in various languages, but they typically consist of a single source or collection.", "labels": [], "entities": []}, {"text": "The Brown corpus, which consists of multiple genres, have been usually used to investigate issues of genres of sensitivity, but it is relatively small and does not include any infor-: Coverage for each layer in the OntoNotes v5.0 corpus, by number of documents, words, and some other attributes.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.864547997713089}, {"text": "OntoNotes v5.0 corpus", "start_pos": 215, "end_pos": 236, "type": "DATASET", "confidence": 0.7689368923505148}]}, {"text": "The numbers in parenthesis are the total number of parts in the documents.", "labels": [], "entities": []}, {"text": "mal genres such as web data.", "labels": [], "entities": []}, {"text": "Very seldom has it been the case that the exact same phenomena have been annotated on abroad cross-section of the same language before OntoNotes.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 135, "end_pos": 144, "type": "DATASET", "confidence": 0.9141107797622681}]}, {"text": "The OntoNotes corpus thus provides an opportunity for studying the genre effect on different syntactic, semantic and discourse analyzers.", "labels": [], "entities": []}, {"text": "Parts of the OntoNotes Corpus have been used for various shared tasks organized by the language processing community.", "labels": [], "entities": [{"text": "OntoNotes Corpus", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.7715911865234375}]}, {"text": "The word sense layer was the subject of prediction in two SemEval-2007 tasks, and the coreference layer was the subject of prediction in the SemEval-2010 2,).", "labels": [], "entities": []}, {"text": "The CoNLL-2012 shared task provided predicted information to the participants, however, that did not include a few layers such as the named entities for Chinese and Arabic, propositions for Arabic, and for better comparison of the English data with the CoNLL-2011 task, a smaller OntoNotes v4.0 portion of the English parse and propositions was used for training.", "labels": [], "entities": []}, {"text": "This paper is a first attempt at presenting a coherent high-level picture of the performance of various publicly available state-of-the-art tools on all the layers of OntoNotes in all three languages, so as to pave the way for further explorations in the area of syntax and semantics processing.", "labels": [], "entities": []}, {"text": "The possible avenues for exploratory studies on various fronts are enormous.", "labels": [], "entities": []}, {"text": "However, given space considerations, in this paper, we will restrict our presentation of the performance on all layers of annotation in the data by using a stratified cross-section of the corpus for training, development, and testing.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 gives an overview of the OntoNotes corpus.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.819945752620697}]}, {"text": "Section 3 explains the parameters of the evaluation and the various underlying assumptions.", "labels": [], "entities": []}, {"text": "Section 4 presents the experimental results and discussion, and Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset.", "labels": [], "entities": []}, {"text": "There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses), semantic role labeling, word sense (, and named entities.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.6544300317764282}]}, {"text": "The phenomenon of coreference is somewhat of an outlier.", "labels": [], "entities": [{"text": "coreference", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9689984321594238}]}, {"text": "The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.9046137928962708}]}, {"text": "Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations.", "labels": [], "entities": []}, {"text": "Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so as to facilitate future studies.", "labels": [], "entities": []}, {"text": "In this section, we will report on the experiments carried out using all available data in the training set for training models fora particular layer, and using the CoNLL-2012 test set as the test set.", "labels": [], "entities": [{"text": "CoNLL-2012 test set", "start_pos": 165, "end_pos": 184, "type": "DATASET", "confidence": 0.9815946022669474}]}, {"text": "8 There is another phrase type -EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases by different speakers, so we decided not to remove that from the data.", "labels": [], "entities": []}, {"text": "A study by shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 precision and 67 recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.999692440032959}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9870535731315613}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9969587326049805}]}, {"text": "The predicted part of speech for Arabic area mapped down version of the richer gold version present in the Treebank", "labels": [], "entities": [{"text": "Treebank", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.9024978876113892}]}], "tableCaptions": [{"text": " Table 1: Coverage for each layer in the OntoNotes v5.0 corpus, by number of documents, words, and  some other attributes. The numbers in parenthesis are the total number of parts in the documents.", "labels": [], "entities": [{"text": "OntoNotes v5.0 corpus", "start_pos": 41, "end_pos": 62, "type": "DATASET", "confidence": 0.8320983648300171}]}, {"text": " Table 3: Parser performance on the CoNLL-2012  test set.", "labels": [], "entities": [{"text": "Parser", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9207496047019958}, {"text": "CoNLL-2012  test set", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.9839775959650675}]}, {"text": " Table 4: Word sense performance on the CoNLL- 2012 test set.", "labels": [], "entities": [{"text": "CoNLL- 2012 test set", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.948331606388092}]}, {"text": " Table 5: Proposition and frameset disambiguation  performance 14 in the CoNLL-2012 test set.", "labels": [], "entities": [{"text": "frameset disambiguation", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.6489022076129913}, {"text": "CoNLL-2012 test set", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.9750912586847941}]}, {"text": " Table 6: Performance of the named entity recog- nizer on the CoNLL-2012 test set.", "labels": [], "entities": [{"text": "CoNLL-2012 test set", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.9769553343454996}]}]}