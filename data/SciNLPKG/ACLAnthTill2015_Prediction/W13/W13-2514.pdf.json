{"title": [{"text": "Mining for Domain-specific Parallel Text from Wikipedia", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous attempts in extracting parallel data from Wikipedia were restricted by the monotonicity constraint of the alignment algorithm used for matching possible candidates.", "labels": [], "entities": []}, {"text": "This paper proposes a method for exploiting Wikipedia articles without worrying about the position of the sentences in the text.", "labels": [], "entities": []}, {"text": "The algorithm ranks the candidate sentence pairs by means of a customized metric, which combines different similarity criteria.", "labels": [], "entities": []}, {"text": "Moreover, we limit the search space to a specific topical domain, since our final goal is to use the extracted data in a domain-specific Statistical Machine Translation (SMT) setting.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 137, "end_pos": 174, "type": "TASK", "confidence": 0.7694037109613419}]}, {"text": "The precision estimates show that the extracted sentence pairs are clearly semantically equivalent.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9993906021118164}]}, {"text": "The SMT experiments, however, show that the extracted data is not refined enough to improve a strong in-domain SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9870581030845642}, {"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.98443603515625}]}, {"text": "Nevertheless, it is good enough to boost the performance of an out-of-domain system trained on sizable amounts of data.", "labels": [], "entities": []}], "introductionContent": [{"text": "A high-quality Statistical Machine Translation (SMT) system can only be built with large quantities of parallel texts.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.8334009448687235}]}, {"text": "Moreover, systems specialized in specific domains require in-domain training data.", "labels": [], "entities": []}, {"text": "A well-known problem of SMT systems is that existing parallel corpora cover a small percentage of the possible language pairs and very few domains.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.99674391746521}]}, {"text": "We therefore need a languageindependent approach for discovering parallel sentences in the available multilingual resources.", "labels": [], "entities": []}, {"text": "This idea was explored intensively in the last decade with different text sources, generically called comparable corpora, such as news feeds, encyclopedias or even the entire Web.", "labels": [], "entities": []}, {"text": "The first approaches focused merely on news corpora and were either based on IBM alignment models () or employing machine learning techniques (.", "labels": [], "entities": []}, {"text": "The multilingual Wikipedia is another source of comparable texts, not yet thoroughly explored.", "labels": [], "entities": []}, {"text": "Adafre and de Rijke (2006) describe two methods for identifying parallel sentences across it based on monolingual sentence similarity (MT and respectively, lexicon based).", "labels": [], "entities": []}, {"text": "approach the problem by combining recalland precision-oriented methods for sentence alignment, such as the DK-vec algorithm or algorithms based on cosine similarities.", "labels": [], "entities": [{"text": "recalland precision-oriented", "start_pos": 34, "end_pos": 62, "type": "METRIC", "confidence": 0.715466320514679}, {"text": "sentence alignment", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7447457313537598}]}, {"text": "Both approaches have achieved good results in terms of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9996106028556824}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9986598491668701}]}, {"text": "However, we are interested in real application scenarios, such as SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9946537017822266}]}, {"text": "The following approaches report significant performance improvements when using the extracted data as training material for SMT: use a maximum entropy-based classifier with various feature functions (e.g. alignment coverage, word fertility, translation probability, distortion).", "labels": [], "entities": [{"text": "SMT", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.9955748319625854}]}, {"text": "S \u00b8 tef\u02d8 anescu et al. propose an algorithm based on cross-lingual information retrieval, which also considers similarity features equivalent to the ones mentioned in the previous paper.", "labels": [], "entities": [{"text": "cross-lingual information retrieval", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.6230493982632955}]}, {"text": "The presented approaches extract general purpose sentences, but we are interested in a specific topical domain.", "labels": [], "entities": []}, {"text": "We have previously tackled the problem) and encountered two major bottlenecks: the alignment algorithm for matching possible candidates and the similarity metric used to compare them.", "labels": [], "entities": []}, {"text": "To our knowledge, existing sentence alignment algorithms (including the one we have employed in the first place) have a monotonic order constraint, meaning that crossing alignments are not allowed.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7336138337850571}]}, {"text": "But this phenomenon occurs often in Wikipedia, because its articles in different languages are edited independently, without respecting any guidelines.", "labels": [], "entities": []}, {"text": "Moreover, the string-based comparison metric proved to be unreliable for identifying parallel sentences.", "labels": [], "entities": []}, {"text": "In this paper we propose an improved approach for selecting parallel sentences in Wikipedia articles which considers all possible sentence pairs, regardless of their position in the text.", "labels": [], "entities": []}, {"text": "The selection will be made by means of a more informed similarity metric, which rates different aspects concerning the correspondences between two sentences.", "labels": [], "entities": []}, {"text": "Although the approach is language and domain-independent, the present paper reports results obtained through querying the German and French Wikipedia for Alpine texts (i.e. mountaineering reports, hiking recommendations, articles on the biology and the geology of mountainous regions).", "labels": [], "entities": []}, {"text": "Moreover, we report preliminary results regarding the use of the extracted corpus for SMT training.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.925883412361145}]}], "datasetContent": [{"text": "The conducted experiments have focused only on the extraction of parallel clauses and their use in a SMT scenario.", "labels": [], "entities": [{"text": "extraction of parallel clauses", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.8242094218730927}, {"text": "SMT scenario", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.9367446005344391}]}, {"text": "For this purpose, we have used as input the articles selected and preprocessed in the previous development phase (.", "labels": [], "entities": []}, {"text": "Specifically, the data set consists of 39 000 parallel articles with approximately 6 million German clauses and 2.7 million French ones.", "labels": [], "entities": []}, {"text": "We were able to extract 225 000 parallel clause pairs out of them, by setting the final filter threshold to 0.2.", "labels": [], "entities": []}, {"text": "This means that roughly 4% of the German clauses have an French equivalent (and 8% when reporting to the French clauses), figures comparable to our previous results on a different sized data set.", "labels": [], "entities": []}, {"text": "However, the quality of the extracted data is higher than in our previous approaches.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the parallel data extracted, we manually checked a set of 200 automatically aligned clauses with similarity scores above 0.25.", "labels": [], "entities": []}, {"text": "For this test set, 39% of the extracted data represent perfect translations, 26% are translations with an extra segment (e.g. a noun phrase) on one side and 35% represent misalignments.", "labels": [], "entities": []}, {"text": "However, given the high degree of parallelism between the clauses from the middle class, we consider them as true positives, achieving a precision of 65%.", "labels": [], "entities": [{"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9994866847991943}]}, {"text": "Furthermore, 40% of the false positives have been introduced by matching proper names, 32% contain matching subsentential segments (word sequences longer than 3 words) and 27% represent failures in the alignment process.", "labels": [], "entities": []}, {"text": "In addition to the manual evaluation discussed in the previous subsection, we have run preliminary investigations with regard to the usefulness of the extracted corpus for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 172, "end_pos": 175, "type": "TASK", "confidence": 0.9944880604743958}]}, {"text": "In this evaluation scenario, we use only pairs with a similarity score above 0.35.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 54, "end_pos": 70, "type": "METRIC", "confidence": 0.9583562016487122}]}, {"text": "The results discussed in this section refer only to the translation direction GermanFrench.", "labels": [], "entities": [{"text": "GermanFrench", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.957488477230072}]}, {"text": "The SMT systems are trained with the Moses toolkit (, according to the WMT 2011 guidelines . The translation performance was measured using the BLEU evaluation metric on a single reference translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9921005368232727}, {"text": "WMT 2011 guidelines", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.9228201707204183}, {"text": "translation", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.9576126337051392}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9917884469032288}]}, {"text": "We also report statistical significance scores, in order to indicate the validity of the comparisons between the MT systems ().", "labels": [], "entities": [{"text": "statistical significance scores", "start_pos": 15, "end_pos": 46, "type": "METRIC", "confidence": 0.7207509676615397}, {"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.8610458374023438}]}, {"text": "We consider the BLEU score difference significant if the computed p-value is below 0.05.", "labels": [], "entities": [{"text": "BLEU score difference", "start_pos": 16, "end_pos": 37, "type": "METRIC", "confidence": 0.9817913373311361}]}, {"text": "We compare two baseline MT systems and several systems with different model mixtures (trans-lation models, language models or both).", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9760135412216187}]}, {"text": "The first baseline system is an in-domain one, trained on the Text+Berg corpus and is the same used for the automatic translations required in the extraction step (see section 3).", "labels": [], "entities": [{"text": "Text+Berg corpus", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9186446070671082}]}, {"text": "The second system is purely outof-domain and it is trained on Europarl, a collection of parliamentary proceedings).", "labels": [], "entities": [{"text": "Europarl, a collection of parliamentary proceedings", "start_pos": 62, "end_pos": 113, "type": "DATASET", "confidence": 0.9433331659861973}]}, {"text": "The development set and the test set contain indomain data, held out from the Text+Berg corpus.", "labels": [], "entities": [{"text": "Text+Berg corpus", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9342839568853378}]}, {"text": "Our first intuition was to add the extracted sentences to the existing in-domain training corpus and to evaluate the performance of the system.", "labels": [], "entities": []}, {"text": "In the second scenario, we added the extracted data to an SMT system for which no in-domain parallel data was available.", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9879855513572693}]}, {"text": "For this purpose, we experimented with different combinations of the models involved in the translation process, namely the German-French translation model (responsible for the translation variants) and the French language model (ensures the fluency of the output).", "labels": [], "entities": [{"text": "translation process", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8958091735839844}]}, {"text": "Besides of the models trained on the parallel data available in each of the data sets, we also built combined models with optimized weights for each of the involved data sets.", "labels": [], "entities": []}, {"text": "The optimization was performed with the tools provided by Sennrich (2012) as part of the Moses toolkit.", "labels": [], "entities": []}, {"text": "We also want to compare several language models, some trained on the individual data sets, others obtained by linearly interpolating different data sets, all optimized for minimal perplexity on the in-domain development set.", "labels": [], "entities": []}, {"text": "The results are summarized in table 4.", "labels": [], "entities": []}, {"text": "A first remark is that an out-of-domain language model (LM) adapted with in-domain data (extracted from Wikipedia and/or SAC data) significantly improves on top of a baseline system trained with out-of-domain texts (Europarl, EP) with up to 1.7 BLEU points.", "labels": [], "entities": [{"text": "Europarl, EP)", "start_pos": 216, "end_pos": 229, "type": "DATASET", "confidence": 0.9227441102266312}, {"text": "BLEU", "start_pos": 245, "end_pos": 249, "type": "METRIC", "confidence": 0.9988952875137329}]}, {"text": "And this improvement can be achieved with only a small quantity of additional data compared to the size of the original training data (120k or 220k versus 1680k sentence pairs).", "labels": [], "entities": []}, {"text": "When replacing the out-of-domain: SMT results for German-French translation model with a combined one (including the Wikipedia data set) and keeping only the adapted language models, we can observe two tendencies.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9130661487579346}, {"text": "Wikipedia data set", "start_pos": 117, "end_pos": 135, "type": "DATASET", "confidence": 0.9465279579162598}]}, {"text": "In the first case (using a combination of out-of-domain and Wikipedia-data for the language model), the BLEU score remains approximately at the same level, the difference not being statistically significant (p-value = 0.387).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9813830852508545}]}, {"text": "The addition of quality in-domain data for the LM from the previous configuration brings an improvement of 0.5 BLEU points on top of the best Europarl system (11.22 BLEU points).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9994595646858215}, {"text": "Europarl", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.946469783782959}, {"text": "BLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9970992803573608}]}, {"text": "Given that all other factors are kept constant, this improvement can be attributed to the additional translation model (TM) trained on Wikipedia data.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 135, "end_pos": 149, "type": "DATASET", "confidence": 0.9146954417228699}]}, {"text": "Moreover, the statistical significance tests confirm that the improved system performs better than the previous one (p-value = 0.005).", "labels": [], "entities": []}, {"text": "To demonstrate that these results are not accidental, we replaced the Wikipedia extracted sentences with a random combination thereof (referred to as WMix) and retrained the system.", "labels": [], "entities": []}, {"text": "Under these circumstances, the performance of the system dropped to 10.40 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9988672733306885}]}, {"text": "These findings demonstrate the effect of a small in-domain data set on the performance of an out-of-domain system trained on big amounts of data.", "labels": [], "entities": []}, {"text": "If the data is of good quality, it can improve the performance of the system, otherwise it significantly deteriorates it.", "labels": [], "entities": []}, {"text": "We notice that the performance of a strong indomain baseline system (SAC) cannot be heavily influenced (either positively or negatively) by translation and language model mixtures combining existing in-domain data with Wikipedia data.", "labels": [], "entities": []}, {"text": "In terms of BLEU points, the mixture models trained with \"good\" Wikipedia data cause a perfor-mance drop of 0.2, but the significance test shows that the difference is not statistically significant (pvalue = 0.08).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9993748068809509}, {"text": "Wikipedia data", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.889920711517334}]}, {"text": "On the other hand, the TM including shuffled Wikipedia sentences causes a performance drop of 0.34 BLEU points, which is statistically significant (p-value = 0.013).", "labels": [], "entities": [{"text": "TM including shuffled Wikipedia sentences", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.7910097599029541}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9993100166320801}]}, {"text": "We can conclude that the quantity of the data is not the decisive factor for the performance change, but rather the quality of the data.", "labels": [], "entities": []}, {"text": "The Wikipedia extracted data set maintains the good performance, whereas a random mixture of the Wikipedia data set causes a performance decrease.", "labels": [], "entities": [{"text": "Wikipedia extracted data set", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.9364699423313141}, {"text": "Wikipedia data set", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.91646808385849}]}, {"text": "Therefore the focus of future work should be on obtaining high quality data, regardless of its amount.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of extracted clause pairs", "labels": [], "entities": []}, {"text": " Table 2: The average sentence length for different  score ranges", "labels": [], "entities": []}, {"text": " Table 3: The size of the German-French data sets", "labels": [], "entities": [{"text": "German-French data sets", "start_pos": 26, "end_pos": 49, "type": "DATASET", "confidence": 0.8068135480086008}]}, {"text": " Table 4: SMT results for German-French", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8108507990837097}]}]}