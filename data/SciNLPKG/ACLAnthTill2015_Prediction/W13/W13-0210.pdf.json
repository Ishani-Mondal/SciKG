{"title": [{"text": "Learning Semantic Representations in a Bigram Language Model", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper investigates the extraction of semantic representations from bigrams.", "labels": [], "entities": [{"text": "extraction of semantic representations from bigrams", "start_pos": 28, "end_pos": 79, "type": "TASK", "confidence": 0.8295202553272247}]}, {"text": "The major obstacle to this objective is that while these word to word dependencies do contain a semantic component, other factors, e.g. syntax, play a much stronger role.", "labels": [], "entities": []}, {"text": "An effective solution will therefore require some means of isolating semantic structure from the remainder.", "labels": [], "entities": []}, {"text": "Here, the possibility of modelling semantic dependencies within the bigram in terms of the similarity of the two words is explored.", "labels": [], "entities": []}, {"text": "A model based on this assumption of semantic coherence is contrasted and combined with a relaxed model lacking this assumption.", "labels": [], "entities": []}, {"text": "The induced representations are evaluated in terms of the correlation of predicted similarities to a dataset of noun-verb similarity ratings gathered in an online experiment.", "labels": [], "entities": []}, {"text": "The results show that the coherence assumption can be used to induce semantic representations, and that the combined model, which breaks the dependencies down into a semantic and a non-semantic component, achieves the best performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantics derives semantic representations from the way that words are distributed across contexts.", "labels": [], "entities": []}, {"text": "The assumption behind this approach is that words that occur in similar contexts will tend to have similar meanings.", "labels": [], "entities": []}, {"text": "expressed this in a well known slogan -you shall know a word by the company it keeps.", "labels": [], "entities": []}, {"text": "In application, these representations have proven successful in automatic thesaurus generation, enhancing language models) and modelling of reading times ( and the effects of priming).", "labels": [], "entities": [{"text": "automatic thesaurus generation", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.6434956789016724}]}, {"text": "However, the high level identification of meaning with distributional properties leaves the question of exactly which distributional properties are relevant to semantics a little vague.", "labels": [], "entities": []}, {"text": "In practice, researchers evaluate various approaches and select those that produce the best performance.", "labels": [], "entities": []}, {"text": "Moreover, other linguistic characteristics, such as syntax, are also analysed in terms of distributional properties.", "labels": [], "entities": []}, {"text": "Bigram distributions, for example, are commonly used to induce POS classes (e.g), but they have also been investigated as a basis for semantic representations.", "labels": [], "entities": []}, {"text": "Here we examine the question of what statistical properties can be used to distinguish semantic factors from other dependencies in the distribution of words across bigram contexts.", "labels": [], "entities": []}, {"text": "We carry this out in terms of class based bigram language models, and explore the possibility that semantic dependencies can be characterised in terms of coherence or similarity across the bigram.", "labels": [], "entities": []}, {"text": "We then evaluate the induced representations in terms of their ability to predict human similarity ratings for noun-verb pairs.", "labels": [], "entities": []}, {"text": "By evaluating the similarity predictions of our models across POS classes in this way, we assess the ability of the model to focus purely on the semantic content while ignoring other information, such as syntax.", "labels": [], "entities": []}], "datasetContent": [{"text": "The induced representations were evaluated in terms of their ability to predict semantic similarity ratings fora set of word pairs.", "labels": [], "entities": []}, {"text": "We measured the cosine similarity of our word representations and correlated that with the human ratings to produce a measure of agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9456945061683655}]}, {"text": "Because the strongest dependencies within the bigrams are likely to be syntactic effects based on the POS classes of the two words, measuring semantic similarity across POS classes is particularly relevant.", "labels": [], "entities": []}, {"text": "That is, the semantic representations should contain as much information about the meaning of the words as possible, while containing as little part-of-speech information as possible, which should instead be shifted into the other part of the model.", "labels": [], "entities": []}, {"text": "Predicting the similarity between nouns and verbs should therefore bean effective evaluation, as these two word classes contain the core of a sentence's semantic content while having substantially divergent distributional properties in regards of syntax.", "labels": [], "entities": []}, {"text": "In this way, we can test whether the POS differences are genuinely being ignored to allow just the semantic similarity to be focussed on.", "labels": [], "entities": []}, {"text": "Thus, an experiment was run to collect similarity ratings for noun-verb pairs.", "labels": [], "entities": []}, {"text": "Each participant rated one of three groups of 36 noun-verb pairs, giving a total of 108 items.", "labels": [], "entities": []}, {"text": "Each group consisted of 12 high similarity pairs, 12 medium similarity pairs and 12 low similarity pairs.", "labels": [], "entities": []}, {"text": "contains a small sample of these items, with rows corresponding to the three experimental groups of participants and columns corresponding to the high, medium and low similarity sets of items seen by each group.", "labels": [], "entities": []}, {"text": "The items in the high similarity set (e.g. anticipation-predict) are related, via an intermediary word, by a combination of morphology (e.g. anticipation-anticipate) and synonymy (e.g. anticipate-predict), drawing on Catvar ( and WordNet to identify these relationships.", "labels": [], "entities": [{"text": "Catvar", "start_pos": 217, "end_pos": 223, "type": "DATASET", "confidence": 0.9852461218833923}, {"text": "WordNet", "start_pos": 230, "end_pos": 237, "type": "DATASET", "confidence": 0.9645954370498657}]}, {"text": "The medium and low sets are then recombinations of nouns and verbs from the high set, with the medium items being the most similar such pairings, as rated by WordNetSimilarity (), and low being the least similar.", "labels": [], "entities": [{"text": "WordNetSimilarity", "start_pos": 158, "end_pos": 175, "type": "DATASET", "confidence": 0.9410635232925415}]}, {"text": "60 participants were paid $2 each to rate all 36 items from a single group, with equal numbers seeing each group.", "labels": [], "entities": []}, {"text": "The experiments were conducted online, with participants instructed to rate the similarity in meaning of each pair of words on a scale of 1 to 5.", "labels": [], "entities": []}, {"text": "They were initially presented with five practice items before the experimental materials were presented with randomisation of both within and between item orders.", "labels": [], "entities": []}, {"text": "Individual Spearman correlations were calculated for each participant's ratings against the predicted similarities, and the average of these values was used as the evaluation measure for the semantic representations induced by a model.", "labels": [], "entities": []}, {"text": "A t-test on the z-transformed participant correlations was used to assess the significance of differences between these averages.", "labels": [], "entities": []}, {"text": "The performance of these models simply as language models was also evaluated, in terms of their perplexity over the test set, T , calculated in terms of the probability assigned to the test set, p(T ), and the number of words it contains, |T |.", "labels": [], "entities": []}, {"text": "In contrast, the combined models, which allow a separation of the dependencies into distinct components, are able to achieve higher correlations, as plotted in.", "labels": [], "entities": []}, {"text": "Among these models, the highest correlation of 0.31, which is significantly greater than the best aggregate model, t(59) = 9.35, p < 0.001, is achieved by a model having |Z| = 50 and |S| = 10.", "labels": [], "entities": [{"text": "correlation", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9726163744926453}]}, {"text": "In fact, all the correlations over 0.2 in are significantly greater at the p < 0.001 level, except |Z| = 100, |S| = 10 and |Z| = 20, |S| = 20, which are only significant at the p < 0.05 and p < 0.01 levels respectively.", "labels": [], "entities": []}, {"text": "This leaves only the four lowest performing combined models as not significantly outperforming the best aggregate model.", "labels": [], "entities": []}, {"text": "Nonetheless, these values are substantially lower than the inter-subject correlations (mean = 0.74, min = 0.64), suggesting that the model could be improved further.", "labels": [], "entities": []}, {"text": "In particular, extending the span of the model to longer ngrams ought to allow the induction of stronger and more detailed semantic representations.", "labels": [], "entities": []}, {"text": "The fact that the best performing model only contains 10 semantic classes underscores the limitations of extracting such representations from bigrams.", "labels": [], "entities": []}, {"text": "In addition to the ability of these models to induce semantic representations, their performance simply as language models was also evaluated.", "labels": [], "entities": []}, {"text": "plots perplexity on the test set against number of parameters per word (|S| + 2|Z|) for the aggregate and combined models.", "labels": [], "entities": []}, {"text": "In general lower perplexities are achieved by larger models for both approaches, as is to be expected.", "labels": [], "entities": []}, {"text": "Within this trend, the combined model tends to have a lower perplexity than the aggregate model by about 5%.", "labels": [], "entities": []}, {"text": "The single casein which the combined model is above the trend line of the aggregate model occurs fora model within which a very small aggregate component, |Z| = 10, is dominated by a large similarity component, |S| = 100.", "labels": [], "entities": []}, {"text": "The performance of these models does not, however, rival that of a standard bigram model with back-off and smoothing, which achieves a perplexity of 185.", "labels": [], "entities": []}, {"text": "On the other hand, neither the aggregate nor combined models are explicitly designed to address the issue of small or zero counts.", "labels": [], "entities": []}], "tableCaptions": []}