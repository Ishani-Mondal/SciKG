{"title": [{"text": "Factored Machine Translation Systems for Russian-English", "labels": [], "entities": [{"text": "Factored Machine Translation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6668874621391296}]}], "abstractContent": [{"text": "We describe the LIA machine translation systems for the Russian-English and English-Russian translation tasks.", "labels": [], "entities": [{"text": "LIA machine translation", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.6276153922080994}, {"text": "English-Russian translation tasks", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.7348880569140116}]}, {"text": "Various factored translation systems were built using MOSES to take into account the morphological complexity of Russian and we experimented with the romanization of un-translated Russian words.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.7812160849571228}]}], "introductionContent": [{"text": "This paper presents the factored phrase-based Machine Translation (MT) systems ) developed at LIA, for the RussianEnglish and English-Russian translation tasks at WMT'13.", "labels": [], "entities": [{"text": "phrase-based Machine Translation (MT)", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.7863287975390753}, {"text": "English-Russian translation", "start_pos": 126, "end_pos": 153, "type": "TASK", "confidence": 0.6181032806634903}, {"text": "WMT'13", "start_pos": 163, "end_pos": 169, "type": "DATASET", "confidence": 0.8693163394927979}]}, {"text": "These systems use only data provided for the evaluation campaign along with the LDC English Gigaword corpus.", "labels": [], "entities": [{"text": "LDC English Gigaword corpus", "start_pos": 80, "end_pos": 107, "type": "DATASET", "confidence": 0.9537039548158646}]}, {"text": "We summarize in Section 2 the resources used and the main characteristics of the systems based on the MOSES toolkit ( ).", "labels": [], "entities": [{"text": "MOSES toolkit", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.8809047341346741}]}, {"text": "Section 3 reports experiments on the use of factored translation models.", "labels": [], "entities": []}, {"text": "Section 4 describes the transliteration process used to improve the Russian to English task.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation was performed using caseinsensitive BLEU and was computed with the mteval-v13a.pl script provided by NIST.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9790526032447815}, {"text": "NIST", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.9582767486572266}]}, {"text": "The BLEU scores shown in the tables below are all averaged on the test parts obtained from the 3-fold cross validation process.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9991169571876526}]}, {"text": "In the remainder of the paper, we employ the notation proposed by to refer to factored translation models.", "labels": [], "entities": []}, {"text": "For example, tW-W:tL-L+tP-P+gLaP-W, where \"t\" and \"g\" stand for \"translation\" and \"generation\", denotes a translation system with two decoding paths: \u2022 a first one directly translates words to words (tW-W), \u2022 a second one is divided into three steps: 1.", "labels": [], "entities": []}, {"text": "translation from lemmas to lemmas (tL-L), 2.", "labels": [], "entities": []}, {"text": "translation from PoS to PoS (tP-P) and 3.", "labels": [], "entities": []}, {"text": "generation of target words from target lemmas and PoS (gLaP-W). is populated with the results of PBMs which use words as their sole factor.", "labels": [], "entities": []}, {"text": "When LMs are built on mono-news-c and news-s, an improvement of BLEU is observed each time a training parallel corpus is used, both for both translation directions (columns 1 and 3).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9992893934249878}]}, {"text": "We can also notice an absolute increase of 0.4 BLEU score when the English LM is additionally trained on ldc (column 2).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9803071320056915}, {"text": "English LM", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.8462814390659332}]}, {"text": "The many inflections for Russian induce a hight out-of-vocabulary rate for the PBMs, which generates many untranslated Russian words for Russian to English.", "labels": [], "entities": []}, {"text": "We experimented with the training of a PMB on lemmatized Russian corpora (, line 1) but observed a decrease in BLEU score w.r.t. a PBM trained on words (line 2).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9853728413581848}]}, {"text": "With two decoding paths -one from words, one from lemmas (line 4) -using the MOSES ability to manage multiple decoding paths for factored translation models, an absolute improvement of 0.2 BLEU score was observed.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 189, "end_pos": 199, "type": "METRIC", "confidence": 0.9839175641536713}]}, {"text": "Another interest of factored models is disambiguating translated words according to their PoS.", "labels": [], "entities": []}, {"text": "Translating a (word, PoS) pair results in an absolute increase of 0.3 BLEU (line 5), and of 0.4 BLEU when considering two decoding paths (last line).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9980946183204651}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.997702419757843}]}, {"text": "Disambiguating source words with PoS did not seem to help the translation process (line 3).", "labels": [], "entities": [{"text": "translation", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9776551723480225}]}, {"text": "The Russian inflections are far more problematic in the other translation direction since morphological information, including case, gender and number, has to be induced from the English words and PoS, which are restrained for that language to the grammatical category and knowledge about number (singular/plural for nouns, 3rd person singular or not for verbs).", "labels": [], "entities": []}, {"text": "Disambiguating translated Russian words with their PoS resulted in a dramatic increase of BLEU by 1.6 points (Table 6, last line vs line 3).", "labels": [], "entities": [{"text": "PoS", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.7850310802459717}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9992383718490601}]}, {"text": "The model that translates independently PoS and lemmas, before generating words, albeit appealing for its potential to deal with data sparsity, turned out to be very disappointing (first line).", "labels": [], "entities": []}, {"text": "We additionally led experiments training generation models gLaP-W on monolingual corpora instead of the less voluminous parallel corpora, but we did not observed again in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9954524636268616}]}], "tableCaptions": [{"text": " Table 2: BLEU scores measured with standard  PBMs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9971930384635925}]}, {"text": " Table 3: Statistics on Russian tagsets.", "labels": [], "entities": [{"text": "Russian tagsets", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.8046569526195526}]}, {"text": " Table 4: BLEU scores for EN\u2192RU using news-c  as training parallel corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998920202255249}]}, {"text": " Table 5: BLEU scores for RU\u2192EN using the three  available parallel corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991018772125244}]}, {"text": " Table 6: BLEU scores for EN\u2192RU using the three  available parallel corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992201328277588}]}, {"text": " Table 7: BLEU scores for RU \u2192 EN before and  after transliteration.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994654059410095}, {"text": "RU \u2192 EN", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.7271131674448649}]}]}