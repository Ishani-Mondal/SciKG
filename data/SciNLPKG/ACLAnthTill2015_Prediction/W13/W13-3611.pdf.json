{"title": [{"text": "IITB System for CoNLL 2013 Shared Task: A Hybrid Approach to Grammatical Error Correction", "labels": [], "entities": [{"text": "IITB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7895448207855225}, {"text": "CoNLL 2013 Shared Task", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.5683846697211266}, {"text": "Grammatical Error Correction", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.7285348176956177}]}], "abstractContent": [{"text": "We describe our grammar correction system for the CoNLL-2013 shared task.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.713786169886589}, {"text": "CoNLL-2013 shared task", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.7608362038930258}]}, {"text": "Our system corrects three of the five error types specified for the shared task-noun-number, determiner and subject-verb agreement errors.", "labels": [], "entities": []}, {"text": "For noun-number and determiner correction, we apply a classification approach using rich lexical and syntactic features.", "labels": [], "entities": [{"text": "determiner correction", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7948949933052063}]}, {"text": "For subject-verb agreement correction, we propose anew rule-based system which utilizes dependency parse information and a set of conditional rules to ensure agreement of the verb group with its subject.", "labels": [], "entities": [{"text": "subject-verb agreement correction", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.6902429759502411}, {"text": "dependency parse", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.724353164434433}]}, {"text": "Our system obtained an F-score of 11.03 on the official test set using the M 2 evaluation method (the official evaluation method).", "labels": [], "entities": [{"text": "F-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9995297193527222}, {"text": "official test set", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8304909467697144}]}], "introductionContent": [{"text": "Grammatical Error Correction (GEC) is an interesting and challenging problem and the existing methods that attempt to solve this problem take recourse to deep linguistic and statistical analysis.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8722774038712183}]}, {"text": "In general, GEC may partly assist in solving natural language processing (NLP) tasks like Machine Translation, Natural Language Generation etc.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8291392922401428}, {"text": "Natural Language Generation", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.629577616850535}]}, {"text": "However, a more evident application of GEC is in building automated grammar checkers thereby benefiting non-native speakers of a language.", "labels": [], "entities": [{"text": "GEC", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8836159706115723}]}, {"text": "The CoNLL-2013 shared task ( ) looks at improving the current approaches for GEC and for inviting novel perspectives towards solving the same.", "labels": [], "entities": [{"text": "CoNLL-2013 shared task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8116430838902792}, {"text": "GEC", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.8241431713104248}]}, {"text": "The shared task makes the NUCLE corpus () available in the public domain and participants have been asked to correct grammatical errors belonging to the following categories: noun-number, determiner, subject-verb agreement (SVA), verb form and preposition.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.9289661645889282}]}, {"text": "The key challenges are handling interaction between different error groups and handling potential mistakes made by off-theshelf NLP components run on erroneous text.", "labels": [], "entities": []}, {"text": "For the shared task, we have addressed the following problems: noun-number, determiner and subject-verb agreement correction.", "labels": [], "entities": [{"text": "determiner and subject-verb agreement correction", "start_pos": 76, "end_pos": 124, "type": "TASK", "confidence": 0.6484586358070373}]}, {"text": "For nounnumber and determiner correction, we use a classification based approach to predict corrections -which is a widely used approach.", "labels": [], "entities": [{"text": "determiner correction", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.8617725074291229}, {"text": "predict corrections", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.6555191576480865}]}, {"text": "For subject-verb agreement correction, we propose anew rule-based approach which applies a set of conditional rules to correct the verb group to ensure its agreement with its subject.", "labels": [], "entities": [{"text": "subject-verb agreement correction", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.6872711082299551}]}, {"text": "Our system obtained a score of 11.03 on the official test set using the M 2 method.", "labels": [], "entities": []}, {"text": "Our SVA correction system performs very well with a F-score of 28.45 on the official test set.", "labels": [], "entities": [{"text": "SVA correction", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.5373406708240509}, {"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9997013211250305}]}, {"text": "Section 2 outlines our approach to solving the grammar correction problem.", "labels": [], "entities": [{"text": "grammar correction problem", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.8603397409121195}]}, {"text": "Sections 3, 4 and 5 describe the details of the noun-number, determiner and SVA correction components of our system.", "labels": [], "entities": [{"text": "SVA correction", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.5598566681146622}]}, {"text": "Section 6 explains our experimental setup.", "labels": [], "entities": []}, {"text": "Section 7 discusses the results of the experiments and Section 8 concludes the report.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our training data came from the NUCLE corpus provided for the shared task.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9664365649223328}]}, {"text": "The corpus was split into three parts: training set (55151 sentences), threshold tuning set (1000 sentences) and development test set (1000 sentences).", "labels": [], "entities": []}, {"text": "In addition, evaluation was done on the official test set (1381 sentences).", "labels": [], "entities": [{"text": "official test set", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.7875162959098816}]}, {"text": "Maximum Entropy classifiers were trained for noun-number and determiner correction systems.", "labels": [], "entities": [{"text": "noun-number and determiner correction", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.6419126018881798}]}, {"text": "In the training set, the number of instances with no corrections far exceeds the number of instances with corrections.", "labels": [], "entities": []}, {"text": "Therefore, a balanced training set was created by including all the instances with corrections and sampling \u03b1 instances with no corrections from the training set.", "labels": [], "entities": []}, {"text": "By trial and error, \u03b1 was determined to be 10000 for the noun-number and determiner correction systems.", "labels": [], "entities": []}, {"text": "The confidence score threshold which maximizes the F-score was calibrated on the tuning set.", "labels": [], "entities": [{"text": "confidence score threshold", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.9760505358378092}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9988929629325867}]}, {"text": "We determined threshold = 0  for the noun-number and the determiner correction systems.", "labels": [], "entities": [{"text": "determiner correction", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.6491764783859253}]}, {"text": "The following tools were used in the development of the system for the shared task: (i) NLTK (MaxEntClassifier, Wordnet lemmatizer), (ii) Stanford tools -POS Tagger, Parser and NER and Python interface to the Stanford NER, (iii) Lingua::EN::Inflect module for noun and verb pluralization, and (iv) Wiktionary list of mass nouns, pluralia tantum.", "labels": [], "entities": [{"text": "noun and verb pluralization", "start_pos": 260, "end_pos": 287, "type": "TASK", "confidence": 0.6269066706299782}]}, {"text": "shows the results on the test set (development and official) for each component of the correction system and the integrated system.", "labels": [], "entities": []}, {"text": "The evaluation was done using the M 2 method.", "labels": [], "entities": []}, {"text": "This involves computing F1 measure between a set of proposed system edits and a set of human-annotated gold-standard edits.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9827181696891785}]}, {"text": "However, evaluation is complicated by the fact that there maybe multiple edits which generate the same correction.", "labels": [], "entities": [{"text": "evaluation", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.9606372117996216}]}, {"text": "The following example illustrates this behaviour:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: M 2 scores for IIT Bombay correction system: component-wise and integrated", "labels": [], "entities": [{"text": "M 2 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9481447339057922}, {"text": "IIT Bombay correction", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.6202484369277954}]}, {"text": " Table 5: M 2 scores (original and modified) for SVA correction", "labels": [], "entities": [{"text": "M 2 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9184196392695109}]}]}