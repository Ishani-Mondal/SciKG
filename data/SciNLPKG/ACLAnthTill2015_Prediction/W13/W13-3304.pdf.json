{"title": [{"text": "Associative Texture is Lost in Translation", "labels": [], "entities": [{"text": "Associative Texture is Lost in Translation", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.705486903587977}]}], "abstractContent": [{"text": "We present a suggestive finding regarding the loss of associative texture in the process of machine translation, using comparisons between (a) original and back-translated texts, (b) reference and system translations, and (c) better and worse MT systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.740709662437439}, {"text": "MT", "start_pos": 243, "end_pos": 245, "type": "TASK", "confidence": 0.9681168794631958}]}, {"text": "We represent the amount of association in a text using word association profile-a distribution of pointwise mutual information between all pairs of content word types in a text.", "labels": [], "entities": []}, {"text": "We use the average of the distribution, which we term lexical tightness, as a single measure of the amount of association in a text.", "labels": [], "entities": []}, {"text": "We show that the lexical tightness of human-composed texts is higher than that of the machine translated materials; human references are tighter than machine translations , and better MT systems produce lexically tighter translations.", "labels": [], "entities": []}, {"text": "While the phenomenon of the loss of associative texture has been theoretically predicted by translation scholars, we present a measure capable of quantifying the extent of this phenomenon.", "labels": [], "entities": []}], "introductionContent": [{"text": "While most current approaches to machine translation concentrate on single sentences, there is emerging interest in phenomena that go beyond a single sentence and pertain to the whole text being translated.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7401629686355591}]}, {"text": "For example, demonstrated that repetition of content words is a predictor of translation quality, with poorer translations failing to repeat words appropriately. and present caching of translations from earlier sections of a document to facilitate the translation of its later sections.", "labels": [], "entities": []}, {"text": "In scholarship that deals with properties of human translation of literary texts, translation is often rendered as a process that tends to deform the original, and a number of particular aspects of deformation have been identified.", "labels": [], "entities": []}, {"text": "Specifically, discusses the problem of quantitative impoverishment thus: This refers to a lexical loss.", "labels": [], "entities": []}, {"text": "Every work in prose presents a certain proliferation of signifiers and signifying chains.", "labels": [], "entities": []}, {"text": "Great novelist prose is \"abundant.\"", "labels": [], "entities": []}, {"text": "These signifiers can be described as unfixed, especially as a signified may have a multiplicity of signifiers.", "labels": [], "entities": []}, {"text": "For the signified visage (face) Arlt employs semblante, rosto and cara without justifying a particular choice in a particular sentence.", "labels": [], "entities": [{"text": "rosto", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9211374521255493}]}, {"text": "The essential thing is that visage is marked as an important reality in his work by the use of three signifiers.", "labels": [], "entities": []}, {"text": "The translation that does not respect this multiplicity renders the \"visage\" of an unrecognizable work.", "labels": [], "entities": []}, {"text": "There is a loss, then, since the translation contains fewer signifiers than the original.\"", "labels": [], "entities": []}, {"text": "1 While Berman's remarks refer to literary translation, recent work demonstrates its relevance for machine translation, showing that MT systems tend to under-use linguistic devices that are commonly used for repeated reference, such as superordinates or meronyms, although the pattern with synonyms and near-synonyms was not clear cut.", "labels": [], "entities": [{"text": "literary translation", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7229566127061844}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7585118412971497}, {"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.9854464530944824}]}, {"text": "Studying a complementary phenomenon of translation of same-lemma lexical items in the source document into a target language, found that when MT systems produce different target language translations, they are stylistically, syntactically, or semantically inadequate inmost cases (see upper panel of therein), that is, diversifying the signifiers appropriately is a challenging task.", "labels": [], "entities": [{"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9763163924217224}]}, {"text": "For recent work on biasing SMT systems towards consistent translations of repeated words, see and.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9241139888763428}]}, {"text": "Moving beyond single signifieds, or concepts, Berman faults translations for \"the destruction of underlying networks of signification\", whereby groups of related words are translated without preserving the relatedness in the target language.", "labels": [], "entities": []}, {"text": "While these might be unavoidable in any translation, we show below that machine translation specifically indeed suffers from such a loss (section 3) and that machine translation suffers from it more than the human translations (section 4).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7614091038703918}]}], "datasetContent": [{"text": "We use apart of the dataset used in the NIST Open MT 2008 Evaluation.", "labels": [], "entities": [{"text": "NIST Open MT 2008 Evaluation", "start_pos": 40, "end_pos": 68, "type": "DATASET", "confidence": 0.8741910219192505}]}, {"text": "Our set contains translations of 120 news and web articles from Arabic to English.", "labels": [], "entities": []}, {"text": "For each document, there are 4 human reference translations and 17 machine translations by various systems that participated in the benchmark.", "labels": [], "entities": []}, {"text": "shows the average and standard deviation of lexical tightness values across the 120 texts for each of the four reference translations, each of the 17 MT systems, as well as an average across the four reference translations, and an average across the 17 MT systems.", "labels": [], "entities": [{"text": "standard", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9817765355110168}]}, {"text": "Each of the 17 MT systems is statistically significantly less tight than the average reference human translation (17 applications of the t-test for correlated samples, n=120, p<0.05); 12 of the 17 MT systems are statistically significantly less tight than the least tight human reference (reference translation #3) at p<0.05; the average system translation is statistically significantly less tight that the average human translation at p<0.05.", "labels": [], "entities": []}, {"text": "To exemplify a large gap in associative texture between reference and machine translations, consider the following extracts.", "labels": [], "entities": []}, {"text": "8 As the raw MT version (MT-raw) is barely readable, we provide aversion where words are re-arranged for readability (MT-read), preserving most of the vocabulary.", "labels": [], "entities": []}, {"text": "Since lexical tightness operates on content word types, adding or removing repetitions and function words does not impact the calculation, so we removed or inserted those for the sake of readability   in the MT-read version.", "labels": [], "entities": []}, {"text": "MT-raw vision came tome on dream in view of her dream: Arab state to travel to and group of friends on my mission and travel quickly I was with one of the girls seem close to the remaining more than I was happy and you're raised ended === known now MT-read A vision came tome in a dream.", "labels": [], "entities": []}, {"text": "I was to travel quickly to an Arab state with a group of friends on a mission.", "labels": [], "entities": []}, {"text": "I was with one of the girls who seemed close to the remaining ones.", "labels": [], "entities": []}, {"text": "I was happy and you are raised.", "labels": [], "entities": []}, {"text": "My sister came to tell me about a dream she had while she slept.", "labels": [], "entities": []}, {"text": "She was saying: I saw you preparing to travel to an Arab country, myself and a group of girlfriends.", "labels": [], "entities": []}, {"text": "You were sent on a scholarship abroad, and you were preparing to travel quickly.", "labels": [], "entities": []}, {"text": "You were with one of the girls, who appeared to be closer to you than the others, and I was happy and excited because you were traveling.", "labels": [], "entities": []}, {"text": "I now know ! The use of vision instead of dream, state instead of country, friends instead of girlfriends, mission instead of scholarship, raised instead of excited, along with the complete disapperance of slept, sister, preparing, abroad, all contribute to a dramatic loss of associative texture in the MT version.", "labels": [], "entities": [{"text": "MT", "start_pos": 304, "end_pos": 306, "type": "TASK", "confidence": 0.916651725769043}]}, {"text": "Highly associated pairs like dreamslept, tell-saying, girlfriends-girls, travel-abroad, sister-girls, happy-excited, travel-traveling are all missed in the machine translation, while the newly introduced word raised is quite unrelated to the rest of the vocabulary in the extract.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: PMI associations of words introduced in  back-translations with baseball terms rookie, base- man, and hitting.", "labels": [], "entities": [{"text": "PMI", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6890078783035278}]}, {"text": " Table 2: Average lexical tightness (Av. LT) for the  original vs back translated versions, on 20 base- ball texts from the New York Times. Standard de- viation, minimum, and maximum values are also  shown.", "labels": [], "entities": [{"text": "Average lexical tightness (Av. LT)", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.8802408661161151}, {"text": "base- ball texts from the New York Times", "start_pos": 98, "end_pos": 138, "type": "DATASET", "confidence": 0.836795535352495}]}, {"text": " Table 3: Average lexical tightness (Av. LT) for  the reference vs machine translations, on the NIST  Open MT 2008 Evaluation Arabic to English cor- pus. Standard deviation, minimum, and maximum  values across the 120 texts are also shown.", "labels": [], "entities": [{"text": "Average lexical tightness (Av. LT)", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.8462048768997192}, {"text": "NIST  Open MT 2008 Evaluation Arabic to English cor- pus", "start_pos": 96, "end_pos": 152, "type": "DATASET", "confidence": 0.9326281981034712}]}]}