{"title": [{"text": "LIMSI Submission for the WMT'13 Quality Estimation Task: an Experiment with n-gram Posteriors", "labels": [], "entities": [{"text": "WMT'13 Quality Estimation Task", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.8213438838720322}]}], "abstractContent": [{"text": "This paper describes the machine learning algorithm and the features used by LIMSI for the Quality Estimation Shared Task.", "labels": [], "entities": [{"text": "Quality Estimation Shared Task", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.7609019726514816}]}, {"text": "Our submission mainly aims at evaluating the usefulness for quality estimation of n-gram posterior probabilities that quantify the probability fora given n-gram to be part of the system output.", "labels": [], "entities": []}], "introductionContent": [{"text": "The dissemination of statistical machine translation (SMT) systems in the professional translation industry is still limited by the lack of reliability of SMT outputs, the quality of which varies to a great extent.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.8064927558104197}, {"text": "SMT outputs", "start_pos": 155, "end_pos": 166, "type": "TASK", "confidence": 0.9002376198768616}]}, {"text": "In this context, a critical piece of information would be for MT systems to assess their output translations with automatically derived quality measures.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.989928662776947}]}, {"text": "This problem is the focus of a shared task, the aim of which is to predict the quality of a translation without knowing any human reference(s).", "labels": [], "entities": [{"text": "predict the quality of a translation", "start_pos": 67, "end_pos": 103, "type": "TASK", "confidence": 0.5729237347841263}]}, {"text": "To the best of our knowledge, all approaches so far have tackled quality estimation as a supervised learning problem (.", "labels": [], "entities": []}, {"text": "A wide variety of features have been proposed, most of which can be described as loosely 'linguistic' features that describe the source sentence, the target sentence and the association between them).", "labels": [], "entities": []}, {"text": "Surprisingly enough, information used by the decoder to choose the best translation in the search space, such as its internal scores, have hardly been considered and never proved to be useful.", "labels": [], "entities": []}, {"text": "Indeed, it is well-known that these scores are hard to interpret and to compare across hypotheses.", "labels": [], "entities": []}, {"text": "Furthermore, mapping scores of a linear classifier (such as the scores estimated by MERT) into consistent probabilities is a difficult task.", "labels": [], "entities": []}, {"text": "This work aims at assessing whether information extracted from the decoder search space can help to predict the quality of a translation.", "labels": [], "entities": []}, {"text": "Rather than using directly the decoder score, we propose to consider a finer level of information, the n-gram posterior probabilities that quantifies the probability fora given n-gram to be part of the system output.", "labels": [], "entities": []}, {"text": "These probabilities can be directly interpreted as the confidence the system has fora given n-gram to be part of the translation.", "labels": [], "entities": []}, {"text": "As they are directly derived from the number of hypotheses in the search space that contains this n-gram, these probabilities might be more reliable than the ones estimated from the decoder scores.", "labels": [], "entities": []}, {"text": "We first quickly review, in Section 2, the n-gram posteriors introduced by) and explain how they can be used in the QE task; we then describe, in Section 3 the different systems that have developed for our participation in the WMT'13 shared task on Quality Estimation and assess their performance in Section 4.", "labels": [], "entities": [{"text": "QE task", "start_pos": 116, "end_pos": 123, "type": "TASK", "confidence": 0.7750928401947021}, {"text": "WMT'13 shared task on Quality Estimation", "start_pos": 227, "end_pos": 267, "type": "TASK", "confidence": 0.6353731453418732}]}], "datasetContent": [{"text": "We have tested different combinations of features and learning methods using a standard metric for regression: Mean Absolute Error (MAE) defined by: The two standard loss functions used to train and evaluate a regressor where n is the number of examples, y i and\u02c6yand\u02c6 and\u02c6y i the true label and predicted label of the i th example.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 111, "end_pos": 136, "type": "METRIC", "confidence": 0.9673627316951752}]}, {"text": "MAE can be understood as the averaged error made in predicting the quality of a translation.", "labels": [], "entities": [{"text": "MAE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9065011739730835}]}, {"text": "Performance of both task 1-1 and task 1-3 4 was also evaluated by the Spearman rank correlation coefficient \u03c1 that assesses how well the relationship between two variables can be described using a monotonic function.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient \u03c1", "start_pos": 70, "end_pos": 109, "type": "METRIC", "confidence": 0.8089039146900177}]}, {"text": "While the value of the correlation coefficient is harder to interpret as it not directly related to the value to predict, it can be used to compare the performance achieved when predicting different measures of the post-editing effort.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 23, "end_pos": 46, "type": "METRIC", "confidence": 0.9692644476890564}]}, {"text": "Indeed, several sentence-level (or document level) annotation types can be used to reflect translation quality (Specia, 2011), such as the time needed to post-edit a translation hypothesis, the hTER, or qualitative judgments as it was the case for the shared task of WMT 2012.", "labels": [], "entities": [{"text": "WMT 2012", "start_pos": 267, "end_pos": 275, "type": "DATASET", "confidence": 0.8356213569641113}]}, {"text": "Comparing directly these different settings is complicated, since each of them requires to optimize a different loss, and even if the losses are the same, their actual values will depend on the actual annotation to be predicted (refer again to the discussion in).", "labels": [], "entities": []}, {"text": "Using a metric that relies on the predicted rank of the example rather than the actual value predicted allows us to directly compare the performance achieved on the two tasks.", "labels": [], "entities": []}, {"text": "As the labels for the different tasks were not released before the evaluation, all the reported results are obtained on an 'internal' test set, made of 20% of the data released by the shared task organizers as 'training' data.", "labels": [], "entities": []}, {"text": "The remaining data were used to train the regressor in a 10 folds crossvalidation setting.", "labels": [], "entities": []}, {"text": "In order to get reliable estimate of our methods performances, we used bootstrap resampling to compute confidence intervals of the different scores: 10 random splits of the data into a training and sets were generated; a regressor was then trained and tested for each of these splits and the resulting confidence intervals at 95% computed.", "labels": [], "entities": []}, {"text": "contain the results achieved by our different conditions.", "labels": [], "entities": []}, {"text": "We used, as a baseline, the set of 17 features released by the shared task organizers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the task 1-1", "labels": [], "entities": []}]}