{"title": [{"text": "Aggregating Continuous Word Embeddings for Information Retrieval", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7044002711772919}]}], "abstractContent": [{"text": "While words in documents are generally treated as discrete entities, they can be embedded in a Euclidean space which reflects an a priori notion of similarity between them.", "labels": [], "entities": []}, {"text": "In such a case, a text document can be viewed as a bag-of-embedded-words (BoEW): a set of real-valued vectors.", "labels": [], "entities": []}, {"text": "We propose a novel document representation based on such continuous word embeddings.", "labels": [], "entities": []}, {"text": "It consists in non-linearly mapping the word-embeddings in a higher-dimensional space and in aggregating them into a document-level representation.", "labels": [], "entities": []}, {"text": "We report retrieval and clustering experiments in the case where the word-embeddings are computed from standard topic models showing significant improvements with respect to the original topic models.", "labels": [], "entities": []}], "introductionContent": [{"text": "For many tasks such as information retrieval (IR) or clustering, a text document is represented by a vector, where each dimension corresponds to a given word and where each value encodes the word importance in the document.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.8659457445144654}]}, {"text": "This Vector Space Model (VSM) or bag-of-words (BoW) representation is at the root of topic models such as Latent Semantic Indexing (LSI), Probablistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) (.", "labels": [], "entities": []}, {"text": "All these topic models consist in \"projecting\" documents on a set of topics generally learned in an unsupervised manner.", "labels": [], "entities": []}, {"text": "During the learning stage, as a by-product of the projection of the training documents, one also obtains an embedding of the words in a typically smalldimensional continuous space.", "labels": [], "entities": []}, {"text": "The distance between two words in this space translates the measure of similarity between words which is captured by the topic models.", "labels": [], "entities": []}, {"text": "For LSI, PLSA or LDA, the implicit measure is the number of co-occurrences in the training corpus.", "labels": [], "entities": []}, {"text": "In this paper, we raise the following question: if we were provided with an embedding of words in a continuous space, how could we best use it in IR/clustering tasks?", "labels": [], "entities": [{"text": "IR/clustering tasks", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.9153066724538803}]}, {"text": "Especially, could we develop probabilistic models which would be able to benefit from this a priori information on the similarity between words?", "labels": [], "entities": []}, {"text": "When the words are embedded in a continuous space, one can view a document as a Bag-of-Embedded-Words (BoEW).", "labels": [], "entities": []}, {"text": "We therefore draw inspiration from the computer vision community where it is common practice to represent an image as a bag-of-features (BoF) where each real-valued feature describes local properties of the image (such as its color, texture or shape).", "labels": [], "entities": []}, {"text": "We model the generative process of embedded words using a mixture model where each mixture component can be loosely thought of as a \"topic\".", "labels": [], "entities": [{"text": "generative process of embedded words", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.8517925381660462}]}, {"text": "To transform the variable-cardinality BoEW into a fixed-length representation which is more amenable to comparison, we make use of the Fisher kernel framework of).", "labels": [], "entities": [{"text": "BoEW", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.71129310131073}]}, {"text": "We will show that this induces a non-linear mapping of the embedded words in a higher-dimensional space where their contributions are aggregated.", "labels": [], "entities": []}, {"text": "We underline that our contribution is not the application of the FK to text analysis (see) for such an attempt).", "labels": [], "entities": [{"text": "text analysis", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.8531723022460938}]}, {"text": "Knowing that words can be embedded in a continuous space, our main contribution is to show that we can consequently represent a document as a bag-ofembedded-words.", "labels": [], "entities": []}, {"text": "The FK is just one possible way to subsequently transform this bag representation into a fixed-length vector which is more amenable to large-scale processing.", "labels": [], "entities": [{"text": "FK", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9746131896972656}]}, {"text": "The remainder of the article is organized as fol-lows.", "labels": [], "entities": []}, {"text": "In the next section, we review related works.", "labels": [], "entities": []}, {"text": "In section 3, we describe the proposed framework based on embedded words, GMM topic models and the Fisher kernel.", "labels": [], "entities": []}, {"text": "In section 4, we report and discuss experimental results on clustering and retrieval tasks before concluding in section 5.", "labels": [], "entities": [{"text": "clustering and retrieval tasks", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.6663323193788528}]}], "datasetContent": [{"text": "The experiments aim at demonstrating that the proposed continuous model is competitive with existing topic models on discrete words.", "labels": [], "entities": []}, {"text": "We focus our experiments on the case where the embedding of the continuous words is obtained by LSI as it enables us to compare the quality of the document representation obtained originally by LSI and the one derived by our framework on top of LSI.", "labels": [], "entities": []}, {"text": "In what follows, we will refer to the FV on the LSI embedding simply as the FV.", "labels": [], "entities": [{"text": "FV", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.6958949565887451}]}, {"text": "We assessed the performance of the FV on clustering and ad-hoc IR tasks.", "labels": [], "entities": [{"text": "FV", "start_pos": 35, "end_pos": 37, "type": "DATASET", "confidence": 0.6836130619049072}, {"text": "IR tasks", "start_pos": 63, "end_pos": 71, "type": "TASK", "confidence": 0.901951253414154}]}, {"text": "We used two datasets for clustering and three for IR.", "labels": [], "entities": [{"text": "clustering", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9697171449661255}, {"text": "IR", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9558900594711304}]}, {"text": "Using the Lemur toolkit), we applied a standard processing pipeline on all these datasets including stopword removal, stemming or lemmatization and the filtering of rare words to speedup computations.", "labels": [], "entities": [{"text": "stopword removal", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.7384940981864929}]}, {"text": "The GMMs were trained on 1,000,000 word occurences, which represents roughly 5,000 documents for the collections we have used.", "labels": [], "entities": []}, {"text": "In what follows, the cosine similarity was used to compare FVs and LSI document vectors.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 21, "end_pos": 38, "type": "METRIC", "confidence": 0.7604095637798309}]}], "tableCaptions": [{"text": " Table 1: Characteristics of the clustering and IR  collections", "labels": [], "entities": [{"text": "IR", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.921906590461731}]}, {"text": " Table 2: Clustering experiments on 20NG and the  WebKB TDT Corpus: Mean performance over 20  runs (in %).", "labels": [], "entities": [{"text": "WebKB TDT Corpus", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9705589811007181}]}, {"text": " Table 4: Mean Average Precision(%) for the PL2  and TFIDF model on the three IR Collections  compared to Fisher Vector and LSI", "labels": [], "entities": [{"text": "Mean Average Precision", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8128585418065389}]}]}