{"title": [{"text": "The LIGM-Alpage Architecture for the SPMRL 2013 Shared Task: Multiword Expression Analysis and Dependency Parsing", "labels": [], "entities": [{"text": "SPMRL 2013 Shared Task", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6742558628320694}, {"text": "Multiword Expression Analysis", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7943901817003886}]}], "abstractContent": [{"text": "This paper describes the LIGM-Alpage system for the SPMRL 2013 Shared Task.", "labels": [], "entities": [{"text": "SPMRL 2013 Shared Task", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.5670029148459435}]}, {"text": "We only participated to the French part of the dependency parsing track, focusing on the realistic setting where the system is informed neither with gold tagging and morphology nor (more importantly) with gold grouping of tokens into multi-word expressions (MWEs).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.9179585874080658}, {"text": "gold grouping of tokens into multi-word expressions (MWEs)", "start_pos": 205, "end_pos": 263, "type": "TASK", "confidence": 0.6916940867900848}]}, {"text": "While the realistic scenario of predicting both MWEs and syntax has already been investigated for constituency parsing, the SPMRL 2013 shared task datasets offer the possibility to investigate it in the dependency framework.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.8634922802448273}, {"text": "SPMRL 2013 shared task datasets", "start_pos": 124, "end_pos": 155, "type": "DATASET", "confidence": 0.8352172613143921}]}, {"text": "We obtain the best results for French, both for overall parsing and for MWE recognition , using a reparsing architecture that combines several parsers, with both pipeline architecture (MWE recognition followed by parsing), and joint architecture (MWE recognition performed by the parser).", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.9677853286266327}]}], "introductionContent": [{"text": "As shown by the remarkable permanence over the years of specialized workshops, multiword expressions (MWEs) identification is still receiving considerable attention.", "labels": [], "entities": [{"text": "multiword expressions (MWEs) identification", "start_pos": 79, "end_pos": 122, "type": "TASK", "confidence": 0.7319782177607218}]}, {"text": "For some languages, such as Arabic, French, English, or German, a large quantity of MWE resources have been generated).", "labels": [], "entities": []}, {"text": "Yet, while special treatment of complex lexical units, such as MWEs, has been shown to boost performance in tasks such as machine translation (), there has been relatively little work exploiting MWE recognition to improve parsing performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7990653812885284}, {"text": "MWE recognition", "start_pos": 195, "end_pos": 210, "type": "TASK", "confidence": 0.925660640001297}]}, {"text": "Indeed, a classical parsing scenario is to pregroup MWEs using gold MWE annotation).", "labels": [], "entities": []}, {"text": "This non-realistic scenario has been shown to help parsing (), but the situation is quite different when switching to automatic MWE prediction.", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9900336861610413}, {"text": "MWE prediction", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.9482908248901367}]}, {"text": "In that case, errors in MWE recognition alleviate their positive effect on parsing performance . While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing, the French dataset of the) offers one of the first opportunities to evaluate this scenario within the framework of dependency syntax.", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.9835644662380219}, {"text": "parsing", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.9705696105957031}, {"text": "syntactic parsing", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.6529616713523865}, {"text": "MWE recognition", "start_pos": 162, "end_pos": 177, "type": "TASK", "confidence": 0.9044919908046722}, {"text": "constituency parsing", "start_pos": 250, "end_pos": 270, "type": "TASK", "confidence": 0.7987387180328369}, {"text": "French dataset", "start_pos": 276, "end_pos": 290, "type": "DATASET", "confidence": 0.9079383909702301}]}, {"text": "In this paper, we discuss the systems we submitted to the SPMRL 2013 shared task.", "labels": [], "entities": [{"text": "SPMRL 2013 shared task", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6089836657047272}]}, {"text": "We focused our participation on the French dependency parsing track using the predicted morphology scenario, because it is the only data set that massively contains MWEs.", "labels": [], "entities": [{"text": "French dependency parsing", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.8513676126797994}]}, {"text": "Our best system ranked first on that track (for all training set sizes).", "labels": [], "entities": []}, {"text": "It is a reparsing system that makes use of predicted parses obtained both with pipeline and joint architectures.", "labels": [], "entities": []}, {"text": "We applied it to the French data set only, as we focused on MWE analysis for dependency parsing.", "labels": [], "entities": [{"text": "French data set", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.981806735197703}, {"text": "MWE analysis", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.8040222823619843}, {"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8674599230289459}]}, {"text": "Section 2 gives its general description, section 3 describes the handling of MWEs.", "labels": [], "entities": []}, {"text": "We detail the underlying parsers in section 4 and their combination in section 5.", "labels": [], "entities": []}, {"text": "Experiments are described and discussed in sections 6 and 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Parsing results on development corpus (38820 tokens)", "labels": [], "entities": []}, {"text": " Table 2: MWE Results on the development corpus (2119 MWEs) with full training.", "labels": [], "entities": [{"text": "MWE", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8843913078308105}]}, {"text": " Table 3: Official parsing results on the evaluation corpus (75216 tokens)", "labels": [], "entities": []}, {"text": " Table 4: Official MWE results on the evaluation corpus (4043 MWEs). The scores correspond to the F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9916259050369263}]}, {"text": " Table 5: MWE+POS results on the development corpus, broken down by POS (recall, precision, F-measure, number  of gold MWEs, predicted MWEs, correct MWEs with such POS.", "labels": [], "entities": [{"text": "POS", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.8912854790687561}, {"text": "POS", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9886179566383362}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9985911250114441}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9961808919906616}, {"text": "F-measure", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9937887787818909}]}]}