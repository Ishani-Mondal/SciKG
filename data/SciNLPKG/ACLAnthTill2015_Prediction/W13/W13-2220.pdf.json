{"title": [{"text": "Pre-reordering for machine translation using transition-based walks on dependency parse trees", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7862258553504944}]}], "abstractContent": [{"text": "We propose a pre-reordering scheme to improve the quality of machine translation by permuting the words of a source sentence to a target-like order.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7074536383152008}]}, {"text": "This is accomplished as a transition-based system that walks on the dependency parse tree of the sentence and emits words in target-like order , driven by a classifier trained on a parallel corpus.", "labels": [], "entities": []}, {"text": "Our system is capable of generating arbitrary permutations up to flexible constraints determined by the choice of the classifier algorithm and input features.", "labels": [], "entities": []}], "introductionContent": [{"text": "The dominant paradigm in statistical machine translation consists mainly of phrase-based system such as Moses.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.670184443394343}]}, {"text": "Different languages, however, often express the same concepts in different idiomatic word orders, and while phrase-based system can deal to some extent with short-distance word swaps that are captured by short segments, they typically perform poorly on long-distance (more than four or five words apart) reordering.", "labels": [], "entities": []}, {"text": "In fact, according to, the amount of reordering between two languages is the most predictive feature of phrase-based translation accuracy.", "labels": [], "entities": [{"text": "phrase-based translation accuracy", "start_pos": 104, "end_pos": 137, "type": "TASK", "confidence": 0.5946292082468668}]}, {"text": "A number of approaches to deal with longdistance reordering have been proposed.", "labels": [], "entities": [{"text": "longdistance reordering", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7919281721115112}]}, {"text": "Since an extuasive search of the permutation space is unfeasible, these approaches typically constrain the search space by leveraging syntactical structure of natural languages.", "labels": [], "entities": []}, {"text": "In this work we consider approaches which involve reordering the words of a source sentence in a target-like order as a preprocessing step, before feeding it to a phrase-based decoder which has itself been trained with a reordered training set.", "labels": [], "entities": []}, {"text": "These methods also try to leverage syntax, typically by applying hand-coded or automatically induced reordering rules to a constituency or dependency parse of the source sentence.", "labels": [], "entities": []}, {"text": "( or by treating reordering as a global optimization problem).", "labels": [], "entities": []}, {"text": "In order to keep the training and execution processes tractable, these methods impose hard constrains on the class of permutations they can generate.", "labels": [], "entities": []}, {"text": "We propose a pre-reordering method based on a walk on the dependency parse tree of the source sentence driven by a classifier trained on a parallel corpus.", "labels": [], "entities": []}, {"text": "In principle, our system is capable of generating arbitrary permutations of the source sentence.", "labels": [], "entities": []}, {"text": "Practical implementations will necessarily limit the available permutations, but these constraints are not intrinsic to the model, rather they depend on the specific choice of the classifier algorithm, its hyper-parameters and input features.", "labels": [], "entities": []}, {"text": "2 Reordering as a walk on a dependency tree", "labels": [], "entities": []}], "datasetContent": [{"text": "Following), we generate a source-side reference reordering of a parallel training corpus.", "labels": [], "entities": []}, {"text": "For each sentence pair, we generate a bidirectional word alignment using GIZA++ and the \"grow-diag-final-and\" heuristic implemented in Moses, then we assign to each source-side word a integer index corresponding to the position of the leftmost targetside word it is aligned to (attaching unaligned words to the following aligned word) and finally we perform a stable sort of source-side words according to this index.", "labels": [], "entities": []}, {"text": "On language pairs where GIZA++ produces substantially accurate alignments (generally all European languages) this scheme generates a target-like reference reordering of the corpus.", "labels": [], "entities": []}, {"text": "In order to tune the parameters of the downstream phrase-based translation system and to test the overall translation accuracy, we need two additional small parallel corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9594331383705139}]}, {"text": "We don't need a reference reordering for the tuning corpus since it is not used for training the reordering system, however we generate a reference reordering for the test corpus in order to evaluate the accuracy of the reordering system in isolation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.998665452003479}]}, {"text": "We obtain an alignment of this corpus by appending it to the training corpus, and processing it with GIZA++ and the heuristic described above.", "labels": [], "entities": []}, {"text": "We performed German-to-English and Italian-toEnglish reordering and translation experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Monolingual reordering scores", "labels": [], "entities": [{"text": "Monolingual reordering", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8338676691055298}]}]}