{"title": [{"text": "ACL 2013 51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality", "labels": [], "entities": [{"text": "Compositionality", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.5880447626113892}]}], "abstractContent": [], "introductionContent": [{"text": "In recent years, there has been a growing interest in algorithms that learn a continuous representation for words, phrases, or documents.", "labels": [], "entities": []}, {"text": "For instance, one can see latent semantic analysis) and latent Dirichlet allocation () as a mapping of documents or words into a continuous lower dimensional topic-space.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.6218555569648743}, {"text": "latent Dirichlet allocation", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.5950780908266703}]}, {"text": "Another example, continuous word vector-space models) represent word meanings with vectors that capture semantic and syntactic information.", "labels": [], "entities": []}, {"text": "These representations can be used to induce similarity measures by computing distances between the vectors, leading to many useful applications, such as information retrieval, search query expansions (), document classification) and question answering ().", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.8269520103931427}, {"text": "search query expansions", "start_pos": 176, "end_pos": 199, "type": "TASK", "confidence": 0.6382058461507162}, {"text": "document classification", "start_pos": 204, "end_pos": 227, "type": "TASK", "confidence": 0.8054473400115967}, {"text": "question answering", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.9323100447654724}]}, {"text": "On the fundamental task of language modeling, many hard clustering approaches have been proposed such as Brown clustering) or exchange clustering).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.749197244644165}, {"text": "Brown clustering", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.5914094299077988}, {"text": "exchange clustering", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7845669984817505}]}, {"text": "These algorithms can provide desparsification and can be seen as examples of unsupervised pre-training.", "labels": [], "entities": []}, {"text": "However, they have not been shown to consistently outperform models based on Kneser-Ney smoothed language models which have at their core discrete n-gram representations.", "labels": [], "entities": []}, {"text": "On the contrary, one influential proposal that uses the idea of continuous vector spaces for language modeling is that of neural language models (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7257037162780762}]}, {"text": "In these approaches, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations, using a neural network that performs both the projection and the probability estimate.", "labels": [], "entities": []}, {"text": "They report state of the art performance on several well studied language modeling datasets.", "labels": [], "entities": []}, {"text": "Other neural network based models that use continuous vector representations achieve state of the art performance in speech recognition applications), multitask learning, NER and POS tagging) or sentiment analysis ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 179, "end_pos": 190, "type": "TASK", "confidence": 0.5999125987291336}, {"text": "sentiment analysis", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.9575753808021545}]}, {"text": "Moreover, in (), a continuous space translation model was introduced and its use in a large scale machine translation system yielded promising results in the last WMT evaluation.", "labels": [], "entities": [{"text": "continuous space translation", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.719067394733429}, {"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7244544923305511}, {"text": "WMT", "start_pos": 163, "end_pos": 166, "type": "TASK", "confidence": 0.7948821783065796}]}, {"text": "Despite the success of single word vector space models, they are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them.", "labels": [], "entities": []}, {"text": "This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences.", "labels": [], "entities": []}, {"text": "Recently, there has been much progress in capturing compositionality in vector spaces, e.g.,.", "labels": [], "entities": []}, {"text": "The work of compares several of these approaches on supervised tasks and for phrases of arbitrary type and length.", "labels": [], "entities": []}, {"text": "Another different trend of research belongs to the family of spectral methods.", "labels": [], "entities": []}, {"text": "The motivation in that context is that working in a continuous space allows for the design of algorithms that are not plagued with the local minima issues that discrete latent space models (e.g. HMM trained with EM) tend to suffer from ().", "labels": [], "entities": []}, {"text": "In fact, this motivation strikes with the conventional justification behind vector space models from the neural network literature, which are usually motivated as away of tackling data sparsity issues.", "labels": [], "entities": []}, {"text": "This apparent dichotomy is interesting and has not been investigated yet.", "labels": [], "entities": []}, {"text": "Finally, spectral methods have recently been developed for word representation learning (), dependency parsing () and probabilistic context-free grammars).", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.8404955466588339}, {"text": "dependency parsing", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.838717132806778}]}, {"text": "In this workshop, we bring together researchers who are interested in how to learn continuous vector space models, their compositionality and how to use this new kind of representation in NLP applications.", "labels": [], "entities": []}, {"text": "The goal is to review the recent progress and propositions, to discuss the challenges, to identify promising future research directions and the next challenges for the NLP community.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}