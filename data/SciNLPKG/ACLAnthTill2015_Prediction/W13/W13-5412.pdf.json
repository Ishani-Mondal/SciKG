{"title": [{"text": "Mixing in Some Knowledge: Enriched Context Patterns for Bayesian Word Sense Induction", "labels": [], "entities": [{"text": "Bayesian Word Sense Induction", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.5841342359781265}]}], "abstractContent": [{"text": "Bayesian topic models have recently been shown to perform well in word sense induction (WSI) tasks.", "labels": [], "entities": [{"text": "word sense induction (WSI) tasks", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.8417218582970756}]}, {"text": "Such models have almost exclusively used bag-of-words features, and failed to attain improvement by including other feature types.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the impact of integrating syntactic and knowledge-based features and show that both parametric and non-parametric models consistently benefit from additional feature types.", "labels": [], "entities": []}, {"text": "We perform evaluation on the Se-mEval2010 WSI verb data and show statistically significant improvement inaccuracy (p < 0.001) both over the bag-of-words baselines and over the best system that competed in the SemEval2010 WSI task.", "labels": [], "entities": [{"text": "Se-mEval2010 WSI verb data", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.5786658748984337}, {"text": "SemEval2010 WSI task", "start_pos": 209, "end_pos": 229, "type": "TASK", "confidence": 0.7045235832532247}]}], "introductionContent": [{"text": "The resolution of lexical ambiguity in language is essential to true language understanding.", "labels": [], "entities": [{"text": "resolution of lexical ambiguity in language", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.8711235721906027}]}, {"text": "It has been shown to improve the performance of such applications as statistical machine translation, and crosslanguage information retrieval and question answering).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.7855390310287476}, {"text": "crosslanguage information retrieval", "start_pos": 106, "end_pos": 141, "type": "TASK", "confidence": 0.7105766733487447}, {"text": "question answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.8198224604129791}]}, {"text": "Word sense induction (WSI) is the task of automatically grouping the target word's contexts of occurrence into clusters corresponding to different senses.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8121075481176376}]}, {"text": "Unlike word sense disambiguation (WSD), it does not rely on a pre-existing set of senses.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.8060649385054907}]}, {"text": "Much of the classic bottom-up WSI and thesaurus construction work -as well as many successful systems from the recent SemEval competitionshave explicitly avoided the use of existing knowledge sources, instead representing the disambiguating context using bag-of-words (BOW) or syntactic features.", "labels": [], "entities": [{"text": "WSI and thesaurus construction", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.679053820669651}]}, {"text": "This particularly concerns the attempts to integrate the information about semantic classes of words present in the sense-selecting contexts.", "labels": [], "entities": []}, {"text": "Semantic roles (such as those found in PropBank () or FrameNet ()) tend to generalize poorly across the vocabulary.", "labels": [], "entities": []}, {"text": "Lexical ontologies) in particular) are not always empirically grounded in language use and often do not represent the relevant semantic distinctions.", "labels": [], "entities": []}, {"text": "Very often, some parts of the ontology are better suited fora particular disambiguation task than others.", "labels": [], "entities": []}, {"text": "In this work, we assume that features based on such ontology segments would correlate well with other context features.", "labels": [], "entities": []}, {"text": "Consider, for example, the expression \"to deny the visa\".", "labels": [], "entities": []}, {"text": "When choosing between two senses of 'deny' ('refuse to grant' vs. 'declare untrue'), we would like our lexical ontology to place 'visa' in the same subtree as approval, request, recognition, commendation, endorsement, etc.", "labels": [], "entities": []}, {"text": "And indeed, WordNet places all of these, including 'visa', under the same node.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9723027348518372}]}, {"text": "However, their least common subsumer is 'message, content, subject matter, substance', which also subsumes 'statement', 'significance', etc., which would activate the other sense of 'deny'.", "labels": [], "entities": []}, {"text": "In other words, the distinctions made at this level in the nominal hierarchy in WordNet would not be useful in disambiguating the verb 'deny', unless our model can select the appropriate nodes of the subtree rooted at the synset 'message, content, subject matter, substance'.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9545785188674927}]}, {"text": "Our model should also infer the associations between such nodes and other context relevant features that select the sense 'refuse to grant' (such as the presence of ditransitive constructions, etc.)", "labels": [], "entities": []}, {"text": "In this paper, we use the topic modeling approach to identify ontology-derived features that can prove useful for sense induction.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.8479610681533813}]}, {"text": "Bayesian approaches to sense induction have recently been shown to perform well in the WSI task.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.8441748917102814}, {"text": "WSI task", "start_pos": 87, "end_pos": 95, "type": "TASK", "confidence": 0.9259239733219147}]}, {"text": "In particular, have adapted the Latent Dirichlet Allocation (LDA) generative topic model to WSI by treating each occurrence context of an ambiguous word as a document, and the derived topics as sense-selecting context patterns represented as collections of features.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA) generative topic", "start_pos": 32, "end_pos": 82, "type": "TASK", "confidence": 0.5401578545570374}, {"text": "WSI", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.8741441369056702}]}, {"text": "They applied their model to the SemEval2007 set of ambiguous nouns, beating the best-performing system in its WSI task.", "labels": [], "entities": [{"text": "SemEval2007 set of ambiguous nouns", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.735851788520813}, {"text": "WSI task", "start_pos": 110, "end_pos": 118, "type": "TASK", "confidence": 0.7201264500617981}]}, {"text": "Yao and Van Durme (2011) used a non-parametric Bayesian model, the Hierarchical Dirichlet Process (HDP), for the same task and showed that following the same basic assumptions, it performs comparably, with the advantage of avoiding the extra tuning for the number of senses.", "labels": [], "entities": []}, {"text": "We investigate the question of how well such models would perform when some knowledge of syntactic structure and semantics is added into the system, in particular, when bag-of-words features are supplemented by the knowledge-enriched syntactic features.", "labels": [], "entities": []}, {"text": "We use the SemEval2010 WSI task data for the verbs for evaluation.", "labels": [], "entities": [{"text": "SemEval2010 WSI task data", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.6429244577884674}]}, {"text": "This data set choice is motivated by the fact that (1) for verbs, sense-selecting context patterns often most directly depend on the nouns that occur in syntactic dependencies with them, and (2) the nominal parts of WordNet tend to have much cleaner ontological distinctions and property inheritance than, say, the verb synsets, where the subsumption hierarchy is organized according how specific the verb's manner of action is.", "labels": [], "entities": []}, {"text": "The choice of the SemEval2010 verb data set was motivated by the fact that SemEval2007 verb data is dominated by the most frequent sense for many target verbs, with 11 out of 65 verbs only having one sense in the combined test and training data.", "labels": [], "entities": [{"text": "SemEval2010 verb data set", "start_pos": 18, "end_pos": 43, "type": "DATASET", "confidence": 0.7457351088523865}]}, {"text": "All verbs in the SemEval2010 verb data set have at least two senses in the data provided.", "labels": [], "entities": [{"text": "SemEval2010 verb data set", "start_pos": 17, "end_pos": 42, "type": "DATASET", "confidence": 0.7975736707448959}]}, {"text": "The implications of this work are two-fold: (1) we confirm independently on a different data set that parametric and non-parametric models perform comparably, and outperform the current state-of-the-art methods using the baseline bag-of-words feature set (2) we show that integrating populated syntactic and ontology-based features directly into the generative model consistently leads to statistically significant improvement inaccuracy.", "labels": [], "entities": []}, {"text": "Our system outperforms both the bag-of-words baselines and the best-performing system in the SemEval2010 competition.", "labels": [], "entities": [{"text": "SemEval2010 competition", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.7439934611320496}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the relevant related work.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 give the details on how the models are defined and trained, and describe the incorporated feature classes.", "labels": [], "entities": []}, {"text": "Section 5 describes the data used to conduct the experiments.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we describe the evaluation methods and present and discuss the experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the established practice in SemEval competitions and subsequent work), we conduct supervised evaluation.", "labels": [], "entities": [{"text": "SemEval competitions", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.8992337584495544}]}, {"text": "A small amount of labeled data is used to map the induced topics to real-world senses; fora description of the method see.", "labels": [], "entities": []}, {"text": "The resulting mapping is probabilistic; for topics 1, . .", "labels": [], "entities": []}, {"text": ", K and senses 1, . .", "labels": [], "entities": []}, {"text": ", S, we compute the KS values Then given \u03b8 j * , we can make a better prediction for instance j * than just assigning the most likely sense to its most likely topic.", "labels": [], "entities": []}, {"text": "Instead, we compute the sense with the highest probability of being correct for this instance, given the topic probabilities and the KS mapping probabilities.", "labels": [], "entities": []}, {"text": "The supervised metrics traditionally reported include precision, recall, and F-score, but since our WSI system makes a prediction for every instance, we report accuracy throughout this section.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9995661377906799}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.999204695224762}, {"text": "F-score", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.999099612236023}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9991617202758789}]}, {"text": "For the five SemEval2010 test sets, senses are assigned slightly differently than in cross-validation.", "labels": [], "entities": [{"text": "SemEval2010 test sets", "start_pos": 13, "end_pos": 34, "type": "DATASET", "confidence": 0.8000181913375854}]}, {"text": "Instead of averaging over five models trained per target, for each instance, we predict the sense assigned by the majority of these models.", "labels": [], "entities": []}, {"text": "shows the comparison of the configuration with the best cross-validation accuracy (HDP, 20w +WN1h) against the following: (1) MSF baseline, (2) the baseline bag-of-words model (3) the results obtained on this data set by the best-performing SemEval2010 system using supervised evaluation, Duluth-Mix-Narrow-Gap from the University of Minnesota Duluth (  The improvements obtained by the best configuration are statistically significant by paired two-tailed t-test, treating each of the 3354 distinct test instances as separate samples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.49705392122268677}]}, {"text": "We consider a system's prediction on one such instance to be the sense it predicted in the majority of the test sets in which the instance appears.", "labels": [], "entities": []}, {"text": "Significance levels are as follows: \u2022 The best HDP configuration (20w +WN1h) vs. Duluth-Mix-Narrow-Gap: p < 0.0001 \u2022 The best HDP configuration (20w +WN1h) vs. HDP 20w: p < 0.001 \u2022 12-sense LDA configuration 20w +WN1h vs. Duluth-Mix-Narrow-Gap: p < 0.0001 \u2022 12-sense LDA configuration 20w +WN1h vs. 12-sense LDA 20w: p < 0.05.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-validation accuracies using the Se- mEval2010 mapping sets.", "labels": [], "entities": [{"text": "Se- mEval2010 mapping sets", "start_pos": 48, "end_pos": 74, "type": "DATASET", "confidence": 0.6796241402626038}]}, {"text": " Table 2: Test set accuracies, SemEval2010 verbs", "labels": [], "entities": []}]}