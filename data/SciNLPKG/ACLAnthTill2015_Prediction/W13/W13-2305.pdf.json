{"title": [{"text": "Continuous Measurement Scales in Human Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Human Evaluation of Machine Translation", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.5369206726551056}]}], "abstractContent": [{"text": "We explore the use of continuous rating scales for human evaluation in the context of machine translation evaluation, comparing two assessor-intrinsic quality-control techniques that do not rely on agreement with expert judgments.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.8838818271954855}]}, {"text": "Experiments employing Amazon's Mechanical Turk service show that quality-control techniques made possible by the use of the continuous scale show dramatic improvements to intra-annotator agreement of up to +0.101 in the kappa coefficient, with inter-annotator agreement increasing by up to +0.144 when additional standardization of scores is applied.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk service", "start_pos": 22, "end_pos": 54, "type": "DATASET", "confidence": 0.8608075857162476}, {"text": "kappa coefficient", "start_pos": 220, "end_pos": 237, "type": "METRIC", "confidence": 0.8650030195713043}]}], "introductionContent": [{"text": "Human annotations of language are often required in natural language processing (NLP) tasks for evaluation purposes, in order to estimate how well a given system mimics activities traditionally performed by humans.", "labels": [], "entities": []}, {"text": "In tasks such as machine translation (MT) and natural language generation, the system output is a fully-formed string in a target language.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8509541928768158}, {"text": "natural language generation", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7104454040527344}]}, {"text": "Annotations can take the form of direct estimates of the quality of those outputs or be structured as the simpler task of ranking competing outputs from best-to-worst.", "labels": [], "entities": [{"text": "Annotations", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9167549014091492}]}, {"text": "A direct estimation method of assessment, as opposed to ranking outputs from best-to-worst, has the advantage that it includes in annotations not only that one output is better than another, but also the degree to which that output was better than the other.", "labels": [], "entities": []}, {"text": "In addition, direct estimation of quality within the context of machine translation extends the usefulness of the annotated data to other tasks such as quality-estimation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7591888904571533}]}, {"text": "For an evaluation to be credible, the annotations must be credible.", "labels": [], "entities": []}, {"text": "The simplest way of establishing this is to have the same data point annotated by multiple annotators, and measure the agreement between them.", "labels": [], "entities": []}, {"text": "There has been a worrying trend in recent MT shared tasks -whether the evaluation was structured as ranking translations from best-to-worst, or by direct estimation of fluency and adequacy -of agreement between annotators decreasing.", "labels": [], "entities": [{"text": "MT shared tasks", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.9252131779988607}, {"text": "agreement", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9651183485984802}]}, {"text": "Inconsistency inhuman evaluation of machine translation calls into question conclusions drawn from those assessments, and is the target of this paper: by revising the annotation process, can we improve annotator agreement, and hence the quality of human annotations?", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.6916051954030991}]}, {"text": "Direct estimates of quality are intrinsically continuous in nature, but are often collected using an interval-level scale with a relatively low number of categories, perhaps to make the task cognitively easier for human assessors.", "labels": [], "entities": []}, {"text": "In MT evaluation, five and seven-point interval-level scales are common.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9669232368469238}]}, {"text": "However, the interval-level scale commonly used for direct estimation of translation quality (and other NLP annotation tasks) forces human judges to discretize their assessments into a fixed number of categories, and this process could be a cause of inconsistency inhuman judgments.", "labels": [], "entities": []}, {"text": "In particular, an assessor maybe repeatedly forced to choose between two categories, neither of which really fits their judgment.", "labels": [], "entities": []}, {"text": "The continuous nature of translation quality assessment, as well as the fact that many statistical methods exist that can be applied to continuous data but not interval-level data, motivates our trial of a continuous rating scale.", "labels": [], "entities": [{"text": "translation quality assessment", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.9011116623878479}]}, {"text": "We use human judgments of translation fluency as a test case and compare consistency levels when the conventional 5-point interval-level scale and a continuous visual analog scale (VAS) are used for human evaluation.", "labels": [], "entities": [{"text": "translation fluency", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.9547375440597534}, {"text": "consistency", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9751824140548706}, {"text": "continuous visual analog scale (VAS)", "start_pos": 149, "end_pos": 185, "type": "METRIC", "confidence": 0.7364320542131152}]}, {"text": "We collected data via Amazon's Mechanical Turk, where the quality of annotations is known to vary considerably.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 22, "end_pos": 46, "type": "DATASET", "confidence": 0.9098707139492035}]}, {"text": "As such, we test two qualitycontrol techniques based on statistical significance -made possible by the use of the continuous rating scale -to intrinsically assess the quality of individual human judges.", "labels": [], "entities": []}, {"text": "The quality-control techniques are not restricted to fluency judgments and are relevant to more general MT evaluation, as well as other NLP annotation tasks.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.9803690612316132}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Mean and standard deviation of score differences for continuous scale with ask again items  within a given judge and across two distinct judges, for no quality control (unfiltered), Welch's t-test and  Mann-Whitney U-test with a quality-control threshold of p < 0.05.", "labels": [], "entities": []}, {"text": " Table 2: Intra-annotator (same judge) agreement levels for 5-point interval and continuous scales for  unfiltered judgments and judgments of workers with p < 0.05 for Welch's t-test.", "labels": [], "entities": []}, {"text": " Table 3: Inter-annotator (distinct judge) agreement levels for 5-point interval and continuous scales for  unfiltered judgments and judgments of workers with p < 0.05 for Welch's t-test.", "labels": [], "entities": []}, {"text": " Table 5: WMT system rankings based on approximately 80 randomly-selected fluency judgments per  system, with and without quality control for radio button and continuous input types, based on German- English. The quality control method applied is annotators who score worsened system output and gen- uine system outputs with statistically significant lower scores according to paired Student's t-test.", "labels": [], "entities": []}]}