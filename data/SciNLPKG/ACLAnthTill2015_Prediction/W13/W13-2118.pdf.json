{"title": [{"text": "Automatic Voice Selection in Japanese based on Various Linguistic Information", "labels": [], "entities": [{"text": "Automatic Voice Selection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6543119450410207}]}], "abstractContent": [{"text": "This paper focuses on a subtask of natural language generation (NLG), voice selection , which decides whether a clause is realised in the active or passive voice according to its contextual information.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.8312390148639679}]}, {"text": "Automatic voice selection is essential for re-alising more sophisticated MT and sum-marisation systems, because it impacts the readability of generated texts.", "labels": [], "entities": [{"text": "Automatic voice selection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7743851939837137}, {"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9622835516929626}]}, {"text": "However , to the best of our knowledge, the NLG community has been less concerned with explicit voice selection.", "labels": [], "entities": [{"text": "explicit voice selection", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.715128481388092}]}, {"text": "In this paper , we propose an automatic voice selection model based on various linguistic information, ranging from lexical to discourse information.", "labels": [], "entities": [{"text": "automatic voice selection", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7031434377034506}]}, {"text": "Our empirical evaluation using a manually annotated corpus in Japanese demonstrates that the proposed model achieved 0.758 in F-score, outper-forming the two baseline models.", "labels": [], "entities": [{"text": "F-score", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9994069337844849}]}], "introductionContent": [{"text": "Generating a readable text is the primary goal in natural language generation (NLG).", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.8038486043612162}]}, {"text": "To realise such text, we need to arrange discourse entities (e.g. NPs) inappropriate positions in a sentence according to their discourse salience.", "labels": [], "entities": []}, {"text": "Consider the two following Japanese texts, each of which consists of two sentences.", "labels": [], "entities": []}, {"text": "(1) Tomi-wa kouenj-ni it-ta . Tomi-TOP parkj-IOBJ go-PAST (Tomi went to a parkj.)", "labels": [], "entities": []}, {"text": "Karei-wa sokoj-de ookina inu-ni oikake-rareta . hei-TOP therej-LOC big dog-IOBJ chase-PASSIVE/PAST (Hei was chased by a big dog therej.)", "labels": [], "entities": [{"text": "PAST", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9109342694282532}]}, {"text": "Tomi-wa kouenj-ni it-ta . Tomi-TOP parkj-IOBJ go-PAST (Tomi went to a parkj.)", "labels": [], "entities": []}, {"text": "Ookina inu-ga sokoj-de karei-o oikake-ta . big dog-SUBJ therej-LOC hei-OBJ chase-PAST (A big dog chased himi therej.)", "labels": [], "entities": []}, {"text": "In (1), 'Tom i ' is topicalised in the first sentence, and then it appears at the subject position in the second sentence.", "labels": [], "entities": []}, {"text": "In contrast, the same argument, i.e. 'he i ' is realised at the object position in the second sentence of text.", "labels": [], "entities": []}, {"text": "Intuitively, text (1) is relatively more natural than text (2).", "labels": [], "entities": []}, {"text": "Thus, given the two predicate argument relations, go(SUBJ:Tom i , IOBJ:park j ) and chase(SUBJ:big dog, OBJ:Tom i , IOBJ:park j ), a generation system should choose text (1).", "labels": [], "entities": []}, {"text": "The realisation from a semantic representation (e.g. predicate argument structures) to an actual text has been mainly developed in the area of natural language generation, and has been applied to various NLP applications such as multi-document summarisation) and tutoring systems).", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 143, "end_pos": 170, "type": "TASK", "confidence": 0.7124646306037903}, {"text": "multi-document summarisation", "start_pos": 229, "end_pos": 257, "type": "TASK", "confidence": 0.6045222878456116}]}, {"text": "During the course of a text generation process, various kinds of decisions should be made, including decisions on textual content, clustering the content of each clause, discourse structure of the clauses, lexical choices, types of referring expressions and syntactic structures.", "labels": [], "entities": [{"text": "text generation", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.719540536403656}]}, {"text": "Since these different kinds of decisions are interrelated to each other, it is not a trivial problem to find an optimal order among these decisions.", "labels": [], "entities": []}, {"text": "This issue has been much discussed in terms of architecture of generation systems.", "labels": [], "entities": []}, {"text": "Although a variety of architectures has been proposed in the past, e.g. an integrated architecture and a revision-based architecture, a pipeline architecture is considered as a consensus architecture in which decisions are made in a predetermined order.", "labels": [], "entities": []}, {"text": "Voice selection is a syntactic decision that tends to be made in a later stage of the pipeline architecture, even though it influences various decisions, such as discourse structure and lexical choice.", "labels": [], "entities": [{"text": "Voice selection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6991466581821442}]}, {"text": "Unlike referring expression generation, voice selection has received less attention and been less discussed in the past.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.7873396476109823}, {"text": "voice selection", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7059510946273804}]}, {"text": "Against this background, this research tackles the problem of voice selection considering a wide range of linguistic information that is assumed to be already decided in the preceding stages of a generation process.", "labels": [], "entities": [{"text": "voice selection", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7472609877586365}]}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "We first overview the related work in Section 2, and then propose a voice selection model based on the four kinds of information that impact voice selection in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 then demonstrates the results of empirical evaluation using the NAIST Text Corpus () as training and evaluation data sets.", "labels": [], "entities": [{"text": "NAIST Text Corpus", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.9892630378405253}]}, {"text": "Finally, Section 5 concludes and discusses our future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted an empirical evaluation using manually annotated newspaper articles in Japanese.", "labels": [], "entities": []}, {"text": "To estimate the feature weights of each classifier, we used MEGAM 6 , an implementation of the Maximum Entropy model, with default parameter settings.", "labels": [], "entities": [{"text": "MEGAM 6", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.776350200176239}]}, {"text": "We also used SVM 7 with a polynomial kernel for explicitly handling the dependency of the proposed features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data set division for evaluation", "labels": [], "entities": [{"text": "evaluation", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.6360750198364258}]}, {"text": " Table 3: Effect of threshold \u03b8 for score pas", "labels": [], "entities": []}, {"text": " Table 4: Results of automatic voice selection", "labels": [], "entities": [{"text": "automatic voice selection", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6070957581202189}]}]}