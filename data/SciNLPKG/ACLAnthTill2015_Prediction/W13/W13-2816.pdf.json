{"title": [{"text": "Two Approaches to Correcting Homophone Confusions in a Hybrid Machine Translation System", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8003668487071991}]}], "abstractContent": [{"text": "In the context of a hybrid French-to-English SMT system for translating on-line forum posts, we present two methods for addressing the common problem of homophone confusions in colloquial written language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9133232235908508}, {"text": "translating on-line forum posts", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.8726701438426971}, {"text": "homophone confusions in colloquial written language", "start_pos": 153, "end_pos": 204, "type": "TASK", "confidence": 0.7551194181044897}]}, {"text": "The first is based on hand-coded rules; the second on weighted graphs derived from a large-scale pronunciation resource, with weights trained from a small bicorpus of domain language.", "labels": [], "entities": []}, {"text": "With automatic evaluation, the weighted graph method yields an improvement of about +0.63 BLEU points, while the rule-based method scores about the same as the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.999202311038971}]}, {"text": "On contrastive manual evaluation , both methods give highly significant improvements (p < 0.0001) and score about equally when compared against each other.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We compared the rule-based and weighted graph approaches, evaluating each of them on the 511 sentence devtest b corpus.", "labels": [], "entities": []}, {"text": "The baseline SMT system, with no pre-editing, achieves an average BLEU score of 42.47 on this set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9827321171760559}, {"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9842630326747894}]}, {"text": "Due to the relatively small size of the evaluation set and instability inherent in minimum error rate training), results of individual tuning and evaluation runs can be unreliable.", "labels": [], "entities": []}, {"text": "We therefore preformed multiple tuning and evaluation runs for each system (baseline, rule-based and weighted graph).", "labels": [], "entities": []}, {"text": "To illustrate the precision of the BLEU score on our data sets, we plot in for each individual tuning run the BLEU score achieved on the tuning set (x-axis) against the performance on the evaluation set (y-axis).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9991851449012756}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9965937733650208}, {"text": "BLEU score", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9738762974739075}]}, {"text": "The variance along the x-axis for each system is due to search errors in parameter optimization.", "labels": [], "entities": []}, {"text": "Since the search space is not convex, the tuning process can get stuck in local maxima.", "labels": [], "entities": []}, {"text": "The apparent poor local correlation between performance on the tuning set and performance on the evaluation set for each system shows the effect of the sampling error.", "labels": [], "entities": []}, {"text": "With larger tuning and evaluation sets, we would expect the correlation between the two to improve.", "labels": [], "entities": []}, {"text": "The scatter plot suggests that the weighted-graph system does on average produce significantly better translations (with respect to BLEU) than both the baseline and the rule-based system, whereas the difference between the baseline and the rule-based system is within the range  of statistical error.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9991492033004761}]}, {"text": "To study the effect of tuning condition (tuning on raw vs. input pre-processed by rules), we also translated both the raw and the pre-processed evaluation corpus with all parameter setting that we had obtained during the various experiments.", "labels": [], "entities": []}, {"text": "plots (with solid markers) performance on raw input (x-axis) against translation of preprocessed input (y-axis).", "labels": [], "entities": []}, {"text": "We observe that while preprocessing harms performance for certain parameter settings, most of the time proprocessing does lead to improvements in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.9828905761241913}]}, {"text": "The slight deterioration we observed when comparing system tuned on exactly the type of input that they were to translate later (i.e., raw or preprocessed) seems to be a imprecision in the measurement caused by training instability and sampling error rather than the result of systematic input deterioration due to preprocessing.", "labels": [], "entities": []}, {"text": "Overall, the improvements are small and not statistically significant, but there appears to be a positive trend.", "labels": [], "entities": []}, {"text": "To gauge the benefits of more extensive preprocessing and input error correction we produced and translated 'oracle' input by also applying rules from the Acrolinx engine that currently require a human in the loop who decides whether or not the rule in question should be applied.", "labels": [], "entities": []}, {"text": "The boost in performance is shown by the hollow markers in.", "labels": [], "entities": []}, {"text": "Here, translation of pre-processed input consistently fares better than translation of the raw input.", "labels": [], "entities": [{"text": "translation of pre-processed input", "start_pos": 6, "end_pos": 40, "type": "TASK", "confidence": 0.8284129351377487}]}, {"text": "Although BLEU suggests that the weighted-graph method significantly outscores both the baseline and the rule-based method (p < 0.05 over 25 tuning runs), the absolute differences are small, and we decided that it would be prudent to carryout a human evaluation as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9915448427200317}]}, {"text": "Following the methodology of, we performed contrastive judging on the Amazon Mechanical Turk (AMT) to compare different versions of the system.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 70, "end_pos": 98, "type": "DATASET", "confidence": 0.935445656379064}]}, {"text": "Subjects were recruited from Canada, a bilingual French/English country, requesting English native speakers with good written French; we also limited the call to AMT workers who had already completed at least 50 assignments, at least 80% of which had been accepted.", "labels": [], "entities": [{"text": "AMT", "start_pos": 162, "end_pos": 165, "type": "TASK", "confidence": 0.9395907521247864}]}, {"text": "Judging assignments were split into groups of 20 triplets, where each triplet consisted of a source sentence and two different target sentences; the judge was asked to say which translation was better, using a five-point scale {better, slightly-better, about-equal, slightlyworse, worse}.", "labels": [], "entities": []}, {"text": "The order of the two targets was BLEU score on raw baseline input baseline vs. oracle input; system tuned on baseline input baseline vs. oracle input; system tuned on preprocessed input baseline vs. rule-processed input; system tuned on baseline input baseline vs. rule-processed input; system tuned on preprocessed input threshold for improvement (above this line) vs. deterioration (below): BLEU scores (in points) the two input conditions \"baseline\" and \"rule-based\" (solid markers).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9993038177490234}, {"text": "BLEU", "start_pos": 393, "end_pos": 397, "type": "METRIC", "confidence": 0.9988425374031067}]}, {"text": "The hollow markers show the BLEU score on human-corrected 'oracle' input using a more extensive set of rules / suggestions from the Acrolinx engine that require a human in the loop. randomised.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9813302755355835}]}, {"text": "Judges were paid $1 for each group of 20 triplets.", "labels": [], "entities": []}, {"text": "Each triplet was judged three times.", "labels": [], "entities": []}, {"text": "Using the above method, we posted AMT tasks to compare a) the baseline system against the rule-based system, b) the baseline system against the best weighted-graph system (interpolatedbigram) from Section 3.2.2 and c) the rulebased system and the weighted-graph system against each other.", "labels": [], "entities": [{"text": "AMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8831561207771301}]}, {"text": "The results are shown in; in the second and third columns, disagreements are resolved by majority voting, and in the fourth and fifth we only count cases where the judges are unanimous, the others being scored as unclear.", "labels": [], "entities": []}, {"text": "In both cases, we reduce the original five-point scale to a three-point scale {better, equal/unclear, worse} 3 . Irrespective of the method used to resolve disagreements, the differences \"rule-based system/baseline\" and \"weighted-graph system/baseline\" are highly significant (p < 0.0001) according to the McNemar sign test, while the difference \"rule-based system/weighted-graph system\" is not significant.", "labels": [], "entities": [{"text": "McNemar sign test", "start_pos": 306, "end_pos": 323, "type": "DATASET", "confidence": 0.7184632619222006}]}, {"text": "We were somewhat puzzled that BLEU makes the weighted-graph system clearly better than the rule-based one, while manual evaluation rates them as approximately equal.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9984637498855591}]}, {"text": "The explanation seems to be to do with the fact that manual evaluation operates at the sentence level, giving equal importance to all sentences, while BLEU oper-ates at the word level and consequently counts longer sentences as more important.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9981353282928467}]}, {"text": "If we calculate BLEU on a per-sentence basis and then average the scores, we find that the results for the two systems are nearly the same; per-sentence BLEU differences also correlate reasonably well with majority judgements (Pearson correlation coefficient of 0.39).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9979999661445618}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9571161270141602}, {"text": "Pearson correlation coefficient", "start_pos": 227, "end_pos": 258, "type": "METRIC", "confidence": 0.9752740661303202}]}, {"text": "It is unclear to us, however, whether the difference between per-sentence and per-word BLEU evaluation points to anything particularly interesting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9819093942642212}]}], "tableCaptions": [{"text": " Table 5: Comparison between baseline, rule-based  and weighted-graph versions, evaluated on the  511-utterance devtest b corpus and judged by  three AMT-recruited judges. Figures are presented  both for majority voting and for unanimous deci- sions only.", "labels": [], "entities": [{"text": "511-utterance devtest b corpus", "start_pos": 98, "end_pos": 128, "type": "DATASET", "confidence": 0.6587377190589905}]}]}