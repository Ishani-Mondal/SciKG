{"title": [{"text": "MT Quality Estimation: The CMU System for WMT'13", "labels": [], "entities": [{"text": "MT Quality Estimation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.754735936721166}, {"text": "WMT'13", "start_pos": 42, "end_pos": 48, "type": "TASK", "confidence": 0.7855226993560791}]}], "abstractContent": [{"text": "In this paper we present our entry to the WMT'13 shared task: Quality Estimation (QE) for machine translation (MT).", "labels": [], "entities": [{"text": "WMT'13 shared task", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.694029172261556}, {"text": "Quality Estimation (QE)", "start_pos": 62, "end_pos": 85, "type": "METRIC", "confidence": 0.8778346061706543}, {"text": "machine translation (MT)", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8315021395683289}]}, {"text": "We participated in the 1.1, 1.2 and 1.3 sub-tasks with our QE system trained on features from diverse information sources like MT decoder features, n-best lists, mono-and bilingual corpora and giza training models.", "labels": [], "entities": []}, {"text": "Our system shows competitive results in the workshop shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "As MT becomes more and more reliable, more people are inclined to use automatically translated texts.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9764769673347473}]}, {"text": "If coming across a passage that is obviously a mistranslation, any reader would probably start to doubt the reliability of the information in the whole article, even though the rest might be adequately translated.", "labels": [], "entities": [{"text": "reliability", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9772449135780334}]}, {"text": "If the MT system had a QE component to mark translations as reliable or possibly erroneous, the reader would know to use information from passages marked as bad translations with caution, while still being able to trust other passages.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.8519249558448792}, {"text": "QE", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9477155804634094}]}, {"text": "In post editing a human translator could use translation quality annotation as an indication to whether editing the MT output or translating from scratch might be faster.", "labels": [], "entities": [{"text": "MT output", "start_pos": 116, "end_pos": 125, "type": "TASK", "confidence": 0.7869777679443359}]}, {"text": "Or he could use this information to decide whereto start in order to improve the worst passages first or skip acceptable passages altogether in order to save time.", "labels": [], "entities": []}, {"text": "Confidence scores can also be useful for applications such as cross lingual information retrieval or question answering.", "labels": [], "entities": [{"text": "cross lingual information retrieval", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.6804320216178894}, {"text": "question answering", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.9001051783561707}]}, {"text": "Translation quality could be a valuable ranking feature there.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9485844969749451}]}, {"text": "Most previous work in the field estimates confidence on the sentence level (e.g.), some operate on the word level (e.g.,, and), whereas Soricut and Echihabi (2010) use the document level.", "labels": [], "entities": []}, {"text": "Various classifiers and regression models have been used in QE in the past.", "labels": [], "entities": [{"text": "QE", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9301197528839111}]}, {"text": "compare single layer to Multi Layer Perceptron (MLP), report that Linear Regression (LR) produced the best results in a comparison of LR, MLP and SVM, use SVM, find the M5P tree works best among a number of regression models, while define the problem as a word sequence labeling task and use MIRA.", "labels": [], "entities": [{"text": "word sequence labeling task", "start_pos": 256, "end_pos": 283, "type": "TASK", "confidence": 0.6722403392195702}, {"text": "MIRA", "start_pos": 292, "end_pos": 296, "type": "METRIC", "confidence": 0.9124804735183716}]}, {"text": "The QE shared task was added to the WMT evaluation campaign in 2012, providing standard training and test data for system development.", "labels": [], "entities": [{"text": "WMT evaluation", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7199000716209412}]}], "datasetContent": [{"text": "For Tasks 1.1 and 1.3 we used the 1000-best output provided.", "labels": [], "entities": []}, {"text": "As first step we removed duplicate entries in these n-best list.", "labels": [], "entities": []}, {"text": "This brought the size down to an average of 152.9 hypotheses per source sentence for the Task 1.1 training data, 172.7 on the WMT12 tests set and 204.3 hypotheses per source sentence on the WMT13 blind test data.", "labels": [], "entities": [{"text": "WMT12 tests set", "start_pos": 126, "end_pos": 141, "type": "DATASET", "confidence": 0.9832357366879781}, {"text": "WMT13 blind test data", "start_pos": 190, "end_pos": 211, "type": "DATASET", "confidence": 0.9464351981878281}]}, {"text": "The training data for task 1.3 has on average 129.0 hypothesis per source sentence, the WMT13 blind test data 129.8.", "labels": [], "entities": [{"text": "WMT13 blind test data", "start_pos": 88, "end_pos": 109, "type": "DATASET", "confidence": 0.8335096687078476}]}, {"text": "In addition to our own features described above we extracted the 17 features used in the WMT12 baseline for all sub-tasks via the software provided for the WMT12-QE shared task.", "labels": [], "entities": [{"text": "WMT12 baseline", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.9443316161632538}, {"text": "WMT12-QE shared task", "start_pos": 156, "end_pos": 176, "type": "DATASET", "confidence": 0.8221440315246582}]}], "tableCaptions": [{"text": " Table 1: Sentence Length Statistics for the  English-Spanish Parallel Corpus", "labels": [], "entities": [{"text": "Sentence Length", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9395469129085541}, {"text": "English-Spanish Parallel Corpus", "start_pos": 46, "end_pos": 77, "type": "DATASET", "confidence": 0.6864447593688965}]}, {"text": " Table 2: Task 1.1: Results in MAE and RMSE on the WMT12 test set for WMT12 manual labels as well  as WMT13 HTER as target class and the WMT13 test set for HTER", "labels": [], "entities": [{"text": "MAE", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9586372375488281}, {"text": "RMSE", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.5793194770812988}, {"text": "WMT12 test set", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9489296277364095}, {"text": "WMT13 HTER", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.8653169870376587}, {"text": "WMT13 test set", "start_pos": 137, "end_pos": 151, "type": "DATASET", "confidence": 0.9608268936475118}, {"text": "HTER", "start_pos": 156, "end_pos": 160, "type": "TASK", "confidence": 0.6239650249481201}]}]}