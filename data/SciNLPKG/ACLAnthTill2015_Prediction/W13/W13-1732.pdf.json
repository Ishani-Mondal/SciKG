{"title": [{"text": "Using N-gram and Word Network Features for Native Language Identification", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6562158862749735}]}], "abstractContent": [{"text": "We report on the performance of two different feature sets in the Native Language Identification Shared Task (Tetreault et al., 2013).", "labels": [], "entities": [{"text": "Native Language Identification Shared Task", "start_pos": 66, "end_pos": 108, "type": "TASK", "confidence": 0.746699982881546}]}, {"text": "Our feature sets were inspired by existing literature on native language identification and word networks.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.682414690653483}]}, {"text": "Experiments show that word networks have competitive performance against the baseline feature set, which is a promising result.", "labels": [], "entities": []}, {"text": "We also present a discussion of feature analysis based on information gain, and an overview on the performance of different word network features in the Native Language Identification task.", "labels": [], "entities": [{"text": "Native Language Identification task", "start_pos": 153, "end_pos": 188, "type": "TASK", "confidence": 0.7546976804733276}]}], "introductionContent": [{"text": "Native Language Identification (NLI) is a wellestablished problem in NLP, where the goal is to identify a writer's native language (L1) from his/her writing in a second language (L2), usually English.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7724223434925079}]}, {"text": "NLI is generally framed as a multi-class classification problem (, where native languages (L1) are considered class labels, and writing samples in L2 are used as training and test data.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7016988098621368}]}, {"text": "The NLI problem has recently seen a big surge in interest, sparked in part by three influential early papers on this problem).", "labels": [], "entities": []}, {"text": "Apart from shedding light on the way nonnative learners (also called \"L2 learners\") learn anew language, the NLI task allows constrastive analysis (, study of different types of errors that people make while learning anew language, and identification of language transfer patterns, thereby helping L2-students improve their writing styles and expediting the learning process.", "labels": [], "entities": [{"text": "constrastive analysis", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.6731204837560654}, {"text": "identification of language transfer patterns", "start_pos": 236, "end_pos": 280, "type": "TASK", "confidence": 0.8430958032608032}]}, {"text": "It also helps L2 educators to concentrate their efforts on particular areas of a language that cause the most learning difficulty for different L1s.", "labels": [], "entities": []}, {"text": "The NLI task is closely related to traditional NLP problems of authorship attribution) and author profiling (, and shares many of the same features.", "labels": [], "entities": [{"text": "author profiling", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.7386514842510223}]}, {"text": "Like authorship attribution, NLI is greatly benefitted by having function words and character n-grams as features (.", "labels": [], "entities": []}, {"text": "Native languages form apart of an author's socio-cultural and psychological profiles, thereby being related to author profiling.", "labels": [], "entities": []}, {"text": "Researchers have used different types of features for the NLI problem, including but not limited to function words; character, word and POS n-grams; spelling and syntactic errors (); CFG productions (Brooke and Hirst, 2012b); Tree Substitution Grammar productions; dependencies (Brooke and Hirst, 2012b); Adaptor Grammar features (); L1-influence; stylometric features (; recurrent n-grams on words and POS; and features derived from topic models ).", "labels": [], "entities": []}, {"text": "State-of-the-art results are typically in the 80%-90% range, with results above 90% reported in some cases.", "labels": [], "entities": []}, {"text": "Note, however, that results vary greatly across different datasets, depending on the number of languages being considered, size and difficulty of data, etc.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance summary and description of the systems we submitted.", "labels": [], "entities": []}]}