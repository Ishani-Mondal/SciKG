{"title": [{"text": "An Empirical Study of Smoothing Techniques for Language Modeling", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7102619260549545}]}], "abstractContent": [{"text": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mer-cer (1980), Katz (1987), and Church and Gale (1991).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7344683706760406}]}, {"text": "We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data.", "labels": [], "entities": [{"text": "Brown versus Wall Street Journal", "start_pos": 88, "end_pos": 120, "type": "DATASET", "confidence": 0.6348754405975342}]}, {"text": "In addition, we introduce two novel smoothing techniques , one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which out-perform existing methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Smoothing is a technique essential in the construction of n-gram language models, a staple in speech recognition as well as many other domains.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7460076808929443}]}, {"text": "A language model is a probability distribution over strings P(s) that attempts to reflect the frequency with which each string s occurs as a sentence in natural text.", "labels": [], "entities": []}, {"text": "Language models are used in speech recognition to resolve acoustically ambiguous utterances.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7351124882698059}, {"text": "resolve acoustically ambiguous utterances", "start_pos": 50, "end_pos": 91, "type": "TASK", "confidence": 0.7541516870260239}]}, {"text": "For example, if we have that P(it takes two) >> P(it takes too), then we know ceteris paribus to prefer the former transcription over the latter.", "labels": [], "entities": []}, {"text": "While smoothing is a central issue in language modeling, the literature lacks a definitive comparison between the many existing techniques.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7776732444763184}]}, {"text": "Previous studies only compare a small number of methods (typically two) on a single corpus and using a single training data size.", "labels": [], "entities": []}, {"text": "As a result, it is currently difficult fora researcher to intelligently choose between smoothing schemes.", "labels": [], "entities": []}, {"text": "In this work, we carryout an extensive empirical comparison of the most widely used smoothing techniques, including those described by,, and.", "labels": [], "entities": []}, {"text": "We carryout experiments over many training data sizes on varied corpora using both bigram and trigram models.", "labels": [], "entities": []}, {"text": "We demonstrate that the relative performance of techniques depends greatly on training data size and n-gram order.", "labels": [], "entities": []}, {"text": "For example, for bigram models produced from large training sets Church-Gale smoothing has superior performance, while Katz smoothing performs best on bigram models produced from smaller data.", "labels": [], "entities": []}, {"text": "For the methods with parameters that can be tuned to improve performance, we perform an automated search for optimal values and show that sub-optimal parameter selection can significantly decrease performance.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first smoothing work that systematically investigates any of these issues.", "labels": [], "entities": []}, {"text": "In addition, we introduce two novel smoothing techniques: the first belonging to the class of smoothing models described by 3elinek and Mercer, the second a very simple linear interpolation method.", "labels": [], "entities": []}, {"text": "While being relatively simple to implement, we show that these methods yield good performance in bigram models and superior performance in trigram models.", "labels": [], "entities": []}, {"text": "We take the performance of a method m to be its cross-entropy on test data", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Implementation difficulty of various meth- ods in terms of lines of C++ code", "labels": [], "entities": [{"text": "Implementation", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.7218079566955566}]}]}