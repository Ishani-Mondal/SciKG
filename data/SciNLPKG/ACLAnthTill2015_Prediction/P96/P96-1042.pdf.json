{"title": [{"text": "Minimizing Manual Annotation Cost In Supervised Training From Corpora", "labels": [], "entities": [{"text": "Minimizing Manual Annotation Cost", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8564332872629166}]}], "abstractContent": [{"text": "Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6651246349016825}]}, {"text": "This paper investigates methods for reducing annotation cost by sample selection.", "labels": [], "entities": []}, {"text": "In this approach , during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage.", "labels": [], "entities": []}, {"text": "This avoids redundantly annotating examples that contribute little new information.", "labels": [], "entities": []}, {"text": "This paper extends our previous work on committee-based sample selection for probabilistic classifiers.", "labels": [], "entities": []}, {"text": "We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-of-speech tagging.", "labels": [], "entities": [{"text": "committee-based sample selection", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.5999553104241689}, {"text": "stochastic part-of-speech tagging", "start_pos": 118, "end_pos": 151, "type": "TASK", "confidence": 0.600843628247579}]}, {"text": "We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs.", "labels": [], "entities": []}, {"text": "In particular, the simplest method, which has no parameters to tune, gives excellent results.", "labels": [], "entities": []}, {"text": "We also show that sample selection yields a significant reduction in the size of the model used by the tagger.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many corpus-based methods for natural language processing (NLP) are based on supervised training--acquiring information from a manually annotated corpus.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.8264904717604319}]}, {"text": "Therefore, reducing annotation cost is an important research goal for statistical NLP.", "labels": [], "entities": [{"text": "statistical NLP", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7951204478740692}]}, {"text": "The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all.", "labels": [], "entities": []}, {"text": "It has been shown, however, that some supervised training prior to the unsupervised phase is often beneficial.", "labels": [], "entities": []}, {"text": "Indeed, fully unsupervised training may not be feasible for certain tasks.", "labels": [], "entities": []}, {"text": "This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9941976070404053}]}, {"text": "In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost.", "labels": [], "entities": []}, {"text": "When using sample selection, a learning program examines many unlabeled (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training.", "labels": [], "entities": []}, {"text": "This avoids redundantly annotating many examples that contribute roughly the same information to the learner.", "labels": [], "entities": []}, {"text": "Our work focuses on sample selection for training probabilistic classifiers.", "labels": [], "entities": []}, {"text": "In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (, word categories, or word senses).", "labels": [], "entities": []}, {"text": "As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.6658224165439606}]}, {"text": "We first review the basic approach of committeebased sample selection and its application to partof-speech tagging.", "labels": [], "entities": [{"text": "committeebased sample selection", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.5890764693419138}, {"text": "partof-speech tagging", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.8330375850200653}]}, {"text": "This basic approach gives rise to a family of algorithms (including the original algorithm described in) which we then describe.", "labels": [], "entities": []}, {"text": "First, we describe the 'simplest' committee-based selection algorithm, which has no parameters to tune.", "labels": [], "entities": []}, {"text": "We then generalize the selection scheme, allowing more options to adapt and tune the approach for specific tasks.", "labels": [], "entities": []}, {"text": "The paper compares the performance of several instantiations of the general scheme, including a batch selection method similar to that of.", "labels": [], "entities": []}, {"text": "In particular, we found that the simplest version of the method achieves a significant reduction in annotation cost, comparable to that of other versions.", "labels": [], "entities": []}, {"text": "We also evaluate the computational efficiency of the different variants, and the number of unlabeled examples they consume.", "labels": [], "entities": []}, {"text": "Finally, we study the effect of sample selection on the size of the model acquired by the learner.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents results of applying committeebased sample selection to bigram part-of-speech tagging, as compared with complete training on all examples in the corpus.", "labels": [], "entities": [{"text": "bigram part-of-speech tagging", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.6024150947729746}]}, {"text": "Evaluation was performed using the University of Pennsylvania tagged corpus from the ACL/DCI CD-ROM I.", "labels": [], "entities": [{"text": "ACL/DCI CD-ROM I", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.850928521156311}]}, {"text": "For ease of implementation, we used a complete (closed) lexicon which contains all the words in the corpus.", "labels": [], "entities": []}, {"text": "The committee-based sampling algorithm was initialized using the first 1,000 words from the corpus, and then sequentially examined the following examples in the corpus for possible labeling.", "labels": [], "entities": []}, {"text": "The training set consisted of the first million words in the corpus, with sentence ordering randomized to compensate for inhomogeneity in corpus composition.", "labels": [], "entities": []}, {"text": "The test set was a separate portion of the corpus, consisting of 20,000 words.", "labels": [], "entities": []}, {"text": "We compare the amount of training required by different selection methods to achieve a given tagging accuracy on the test set, where both the amount of training and tagging accuracy are measured over ambiguous words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.8583006858825684}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.926289439201355}]}, {"text": "The effectiveness of randomized committee-based 5Note that most other work on tagging has measured accuracy overall words, not just ambiguous ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9932020306587219}]}, {"text": "Complete training of our system on 1,000,000 words gave us an accuracy of 93.5% over ambiguous words, which corresponds to an accuracy of 95.9% overall words in the 0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9995001554489136}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9989550113677979}]}, {"text": "selection for part-of-speech tagging, with 5 and 10 committee members, was demonstrated in.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.8052338659763336}]}, {"text": "Here we present and compare results for batch, randomized, thresholded, and two member committee-based selection.", "labels": [], "entities": []}, {"text": "presents the results of comparing the several selection methods against each other.", "labels": [], "entities": []}, {"text": "The plots shown are for the best parameter settings that we found through manual tuning for each method.", "labels": [], "entities": []}, {"text": "shows the advantage that sample selection gives with regard to annotation cost.", "labels": [], "entities": []}, {"text": "For example, complete training requires annotated examples containing 98,000 ambiguous words to achieve a 92.6% accuracy (beyond the scale of the graph), while the selective methods require only 18,000-25,000 ambiguous words to achieve this accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9953483939170837}, {"text": "accuracy", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9902563095092773}]}, {"text": "We also find test set, comparable to other published results on bigram tagging. that, to a first approximation, all selection methods considered give similar results.", "labels": [], "entities": [{"text": "bigram tagging.", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.6793871819972992}]}, {"text": "Thus, it seems that a refined choice of the selection method is not crucial for achieving large reductions in annotation cost.", "labels": [], "entities": []}, {"text": "This equivalence of the different methods also largely holds with respect to computational efficiency.", "labels": [], "entities": []}, {"text": "plots classification accuracy versus number of words examined, instead of those selected.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9490326642990112}]}, {"text": "We see that while all selective methods are less efficient in terms of examples examined than complete training, they are comparable to each other.", "labels": [], "entities": []}, {"text": "Two member selection seems to have a clear, though small, advantage.", "labels": [], "entities": []}, {"text": "In we investigate further the properties of batch selection.", "labels": [], "entities": []}, {"text": "shows that accuracy increases with batch size only up to a point, and then starts to decrease.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9995386600494385}]}, {"text": "This result is inline with theoretical difficulties with batch selection) in that batch selection does not account for the distribution of input examples.", "labels": [], "entities": []}, {"text": "Hence, once batch size increases pasta point, the input distribution has too little influence on which examples are selected, and hence classification accuracy decreases.", "labels": [], "entities": [{"text": "classification", "start_pos": 136, "end_pos": 150, "type": "TASK", "confidence": 0.9618114829063416}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9420275092124939}]}, {"text": "Furthermore, as batch size increases, computational efficiency, in terms of the number of examples examined to attain a given accuracy, decreases tremendously).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9931219220161438}]}, {"text": "The ability of committee-based selection to focus on the more informative parts of the training corpus is analyzed in.", "labels": [], "entities": []}, {"text": "Here we examined the number of lexical and bigram counts that were stored (i.e, were non-zero) during training, using the two member selection algorithm and complete training.", "labels": [], "entities": []}, {"text": "As the graphs show, the sample selection method achieves the same accuracy as complete training with fewer lexical and bigram counts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9993707537651062}]}, {"text": "This means that many counts in the data are less useful for correct tagging, as replacing them with smoothed estimates works just as well.", "labels": [], "entities": [{"text": "correct tagging", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.677515298128128}]}, {"text": "6 Committee-based selection ignores such counts, focusing on parameters which improve the model.", "labels": [], "entities": []}, {"text": "This behavior has the practical advantage of reducing the size of the model significantly (by a factor of three here).", "labels": [], "entities": []}, {"text": "Also, the average count is lower in a model constructed by selective training than in a fully trained model, suggesting that the selection method avoids using examples which increase the counts for already known parameters.", "labels": [], "entities": []}], "tableCaptions": []}