{"title": [{"text": "The Rhythm of Lexical Stress in Prose", "labels": [], "entities": []}], "abstractContent": [{"text": "\"Prose rhythm\" is a widely observed but scarcely quantified phenomenon.", "labels": [], "entities": [{"text": "Prose rhythm\"", "start_pos": 1, "end_pos": 14, "type": "TASK", "confidence": 0.8591601848602295}]}, {"text": "We describe an information-theoretic model for measuring the regularity of lexical stress in English texts, and use it in combination with trigram language models to demonstrate a relationship between the probability of word sequences in English and the amount of rhythm present in them.", "labels": [], "entities": []}, {"text": "We find that the stream of lexical stress in text from the Wall Street Journal has an en-tropy rate of less than 0.75 bits per syllable for common sentences.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.9458877642949423}]}, {"text": "We observe that the average number of syllables per word is greater for rarer word sequences, and to normalize for this effect we run control experiments to show that the choice of word order contributes significantly to stress regularity , and increasingly with lexical probability .", "labels": [], "entities": []}], "introductionContent": [{"text": "Rhythm inheres in creative output, asserting itself as the meter in music, the iambs and trochees of poetry, and the uniformity in distances between objects in art and architecture.", "labels": [], "entities": []}, {"text": "More subtly there is widely believed to be rhythm in English prose, reflecting the arrangement of words, whether deliberate or subconscious, to enhance the perceived acoustic signal or reduce the burden of remembrance for the reader or author.", "labels": [], "entities": []}, {"text": "In this paper we describe an information-theoretic model based on lexical stress that substantiates this common perception and relates stress regularity in written speech (which we shall equate with the intuitive notion of \"rhythm\") to the probability of the text itself.", "labels": [], "entities": []}, {"text": "By computing the stress entropy rate for both a set of Wall Street Journal sentences and aversion of the corpus with randomized intra-sentential word order, we also find that word order contributes significantly to rhythm, particularly within highly probable sentences.", "labels": [], "entities": [{"text": "stress entropy rate", "start_pos": 17, "end_pos": 36, "type": "METRIC", "confidence": 0.7429347038269043}, {"text": "Wall Street Journal sentences", "start_pos": 55, "end_pos": 84, "type": "DATASET", "confidence": 0.9502987861633301}, {"text": "rhythm", "start_pos": 215, "end_pos": 221, "type": "METRIC", "confidence": 0.960055410861969}]}, {"text": "We regard this as a first step in quantifying the extent to which metrical properties influence syntactic choice in writing.", "labels": [], "entities": []}], "datasetContent": [{"text": "The efficiency of the n-gram training procedure allowed us to exploit a wealth of data--over 60 million syllables--from 38 million words of Wall Street Journal text.", "labels": [], "entities": [{"text": "Wall Street Journal text", "start_pos": 140, "end_pos": 164, "type": "DATASET", "confidence": 0.9404416531324387}]}, {"text": "We discarded sentences not completely covered by the pronunciation dictionary, leaving 36.1 million words and 60.7 million syllables for experimentation.", "labels": [], "entities": []}, {"text": "Our first experiments used the binary ~1 alphabet.", "labels": [], "entities": []}, {"text": "The maximum entropy rate possible for this process is one bit per syllable, and given the unigram distribution of stress values in the data (55.2% are primary), an upper bound of slightly over 0.99 bits can be computed.", "labels": [], "entities": []}, {"text": "Examining the 4-gram frequencies for the entire corpus sharpens this substantially, yielding an entropy rate estimate of 0.846 bits per syllable.", "labels": [], "entities": []}, {"text": "Most frequent among the 4-grams are the patterns WSWS and SWSW, consistent with the principle of binary alternation mentioned in section 1.", "labels": [], "entities": [{"text": "WSWS", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9220556616783142}, {"text": "SWSW", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8339233994483948}]}, {"text": "The 4-gram estimate matches quite closely with the estimate of 0.852 bits that can be derived from the distribution of word stress patterns excerpted in.", "labels": [], "entities": []}, {"text": "But both measures overestimate the entropy rate by ignoring longer-range dependencies that become evident when we use larger values of n.", "labels": [], "entities": []}, {"text": "For n = 6 we obtain a rate of 0.795 bits per syllable over the entire corpus.", "labels": [], "entities": []}, {"text": "Since we had several thousand times more data than is needed to make reliable estimates of stress entropy rate for values of n less than 7, it was practical to subdivide the corpus according to some criterion, and calculate the stress entropy rate for each subset as well as for the whole.", "labels": [], "entities": []}, {"text": "We chose to divide at the sentence level and to partition the 1.59 million sentences in the data based on a likelihood measure suitable for testing the hypothesis from section 1.", "labels": [], "entities": []}, {"text": "A lexical trigram backoff-smoothed language model was trained on separate data to estimate the language perplexity of each sentence in the corpus.", "labels": [], "entities": []}, {"text": "Sentence perplexity PP(S) is the inverse of sentence 1 probability normalized for length, 1/P(S)r~7, where P(S) is the probability of the sentence according to the language model and ISI is its word count.", "labels": [], "entities": [{"text": "ISI", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.7766852974891663}]}, {"text": "This measure gauges the average \"surprise\" after revealing each word in the sentence as judged by the trigram model.", "labels": [], "entities": []}, {"text": "The question of whether more probable word sequences are also more rhythmic can be approximated by asking whether sentences with lower perplexity have lower stress entropy rate.", "labels": [], "entities": []}, {"text": "Each sentence in the corpus was assigned to one of one hundred bins according to its perplexity--sentences with perplexity between 0 and 10 were assigned to the first bin; between 10 and 20, the sec-  L=~gu~e peq~zay Figure 4: The amount of training data, in syllables, in each perplexity bin.", "labels": [], "entities": []}, {"text": "The bin at perplexity level pp contains all sentences in the corpus with perplexity no less than pp and no greater than pp + 10.", "labels": [], "entities": []}, {"text": "The smallest count (at bin 990) is 50662.", "labels": [], "entities": []}, {"text": "Sentences with perplexity greater than 1000, which numbered roughly 106 thousand out of 1.59 million, were discarded from all experiments, as 10-unit bins at that level captured too little data for statistical significance.", "labels": [], "entities": []}, {"text": "A histogram showing the amount of training data (in syllables) per perplexity bin is given in.", "labels": [], "entities": []}, {"text": "It is crucial to detect and understand potential sources of bias in the methodology so far.", "labels": [], "entities": []}, {"text": "It is clear that the perplexity bins are well trained, but not yet that they are comparable with each other.", "labels": [], "entities": []}, {"text": "shows the average number of syllables per word in sentences that appear in each bin.", "labels": [], "entities": []}, {"text": "That this function is roughly increasing agrees with our intuition that sequences with longer words are rarer.", "labels": [], "entities": []}, {"text": "But it biases our perplexity bins at the extremes.", "labels": [], "entities": []}, {"text": "Early bins, with sequences that have a small syllable rate per word (1.57 in the 0 bin, for example), are predisposed to a lower stress entropy rate since primary stresses, which occur roughly once per word, are more frequent.", "labels": [], "entities": []}, {"text": "Later bins are also likely to be prejudiced in that direction, for the inverse reason: The increasing frequency of multisyllabic words makes it more and more fashionable to transit to a weakstressed syllable following a primary stress, sharpening the probability distribution and decreasing entropy.", "labels": [], "entities": []}, {"text": "This is verified when we run the stress entropy rate computation for each bin.", "labels": [], "entities": []}, {"text": "The results for ngram models of orders 3 through 7, for the casein which secondary lexical stress is mapped to the \"weak\" level, are shown in.", "labels": [], "entities": []}, {"text": "All of the rates calculated are substantially less than a bit, but this only reflects the stress regularity inherent in the vocabulary and in word selection, and says nothing about word arrangement.", "labels": [], "entities": [{"text": "word selection", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.7712787687778473}, {"text": "word arrangement", "start_pos": 181, "end_pos": 197, "type": "TASK", "confidence": 0.7236358821392059}]}, {"text": "The atomic elements in the text stream, the words, contribute regularity independently.", "labels": [], "entities": []}, {"text": "To determine how much is contributed by the way they are glued together, we need to remove the bias of word choice.", "labels": [], "entities": []}, {"text": "For this reason we settled on a model size, n = 6, and performed a variety of experiments with both the original corpus and with a control set that contained exactly the same bins with exactly the same sentences, but mixed up.", "labels": [], "entities": []}, {"text": "Each sentence in the control set was permuted with a pseudorandom sequence of swaps based on an insensitive function of the original; that is to say, identical sentences in the  Language ~Olexity: n-gram stress entropy rates for ~z, weak secondary stress corpus were shuffled the same way and sentences differing by only one word were shuffled similarly.", "labels": [], "entities": []}, {"text": "This allowed us to keep steady the effects of multiple copies of the same sentence in the same perplexity bin.", "labels": [], "entities": []}, {"text": "More importantly, these tests hold everything constant--diction, syllable count, syllable rate per word--except for syntax, the arrangement of the chosen words within the sentence.", "labels": [], "entities": []}, {"text": "Comparing the unrandomized results with this control experiment allows us, therefore, to factor out everything but word order.", "labels": [], "entities": []}, {"text": "In particular, subtracting the stress entropy rates of the original sentences from the rates of the randomized sentences gives us a figure, relative entropy, that estimates how many bits we save by knowing the proper word order given the word choice.", "labels": [], "entities": []}, {"text": "The results for these tests for weak and strong secondary stress are shown in Figures 7 and 8, including the difference curves between the randomized-word and original entropy rates.", "labels": [], "entities": []}, {"text": "The consistently positive difference function demonstrates that there is some extra stress regularity to be had with proper word order, about a hundredth of a bit on average.", "labels": [], "entities": []}, {"text": "The difference is small indeed, but its consistency over hundreds of well-trained data points puts the observation on statistically solid ground.", "labels": [], "entities": [{"text": "consistency", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9928143620491028}]}, {"text": "The negative slopes of the difference curves suggests a more interesting conclusion: As sentence perplexity increases, the gap in stress entropy rate between syntactic sentences and randomly permuted sentences narrows.", "labels": [], "entities": []}, {"text": "Restated inversely, using entropy rates for randomly permuted sentences as a baseline, sentences with higher sequence probability are relatively more rhythmical in the sense of our definition from section 1.", "labels": [], "entities": []}, {"text": "To supplement the ~z binary vocabulary tests we ran the same experiments with ~2 = {0, 1, P}, introducing a pause symbol to examine how stress behaves near phrase boundaries.", "labels": [], "entities": []}, {"text": "Commas, dashes, semicolons, colons, ellipses, and all sentenceterminating punctuation in the text, which were removed in the E1 tests, were mapped to a single pause symbol for E~.", "labels": [], "entities": []}, {"text": "Pauses in the text arise not only from semantic constraints but also from physiological limitations.", "labels": [], "entities": []}, {"text": "These include the \"breath groups\" of syllables that influence both vocalized and written production..", "labels": [], "entities": []}, {"text": "The results for these experiments are shown in.", "labels": [], "entities": []}, {"text": "Expectedly, adding the symbol increases the confusion and hence the entropy, but the rates remain less than a bit.", "labels": [], "entities": [{"text": "confusion", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9945995807647705}, {"text": "entropy", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9686819314956665}]}, {"text": "The maximum possible rate fora ternary sequence is log 2 3 ~ 1.58.", "labels": [], "entities": []}, {"text": "The experiments in this section were repeated with a larger perplexity interval that partitioned the corpus into 20 bins, each covering 50 units of perplexity.", "labels": [], "entities": []}, {"text": "The resulting curves mirrored the finergrain curves presented here.", "labels": [], "entities": []}], "tableCaptions": []}