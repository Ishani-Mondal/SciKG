{"title": [{"text": "FAST PARSING USING PRUNING AND GRAMMAR SPECIALIZATION", "labels": [], "entities": [{"text": "FAST PARSING USING PRUNING", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6741896569728851}, {"text": "GRAMMAR SPECIALIZATION", "start_pos": 31, "end_pos": 53, "type": "METRIC", "confidence": 0.8819238841533661}]}], "abstractContent": [{"text": "We show how a general grammar maybe automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning.", "labels": [], "entities": []}, {"text": "These methods together give an order of magnitude increase in speed, and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work.", "labels": [], "entities": [{"text": "speed", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9938957095146179}, {"text": "coverage loss", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.9465939104557037}]}, {"text": "Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of areal application.", "labels": [], "entities": []}], "introductionContent": [{"text": "Suppose that we have a general grammar for English, or some other natural language; by this, we mean a grammar which encodes most of the important constructions in the language, and which is intended to be applicable to a large range of different domains and applications.", "labels": [], "entities": []}, {"text": "The basic question attacked in this paper is the following one: can such a grammar be concretely useful if we want to process input from a specific domain?", "labels": [], "entities": []}, {"text": "In particular, how can a parser that uses a general grammar achieve a level of efficiency that is practically acceptable?", "labels": [], "entities": []}, {"text": "The central problem is simple to state.", "labels": [], "entities": []}, {"text": "By the very nature of its construction, a general grammar allows a great many theoretically valid analyses of almost any non-trivial sentence.", "labels": [], "entities": []}, {"text": "However, in the context of a specific domain, most of these will be extremely implausible, and can in practice be ignored.", "labels": [], "entities": []}, {"text": "If we want efficient parsing, we want to be able to focus our search on only a small portion of the space of theoretically valid grammatical analyses.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9662997722625732}]}, {"text": "One possible solution is of course to dispense with the idea of using a general grammar, and simply code anew grammar for each domain.", "labels": [], "entities": []}, {"text": "Many people do this, but one cannot help feeling that something is being missed; intuitively, there are many domain-independent grammatical constraints, which one would prefer only to need to code once.", "labels": [], "entities": []}, {"text": "In the last ten years, there have been a number of attempts to find ways to automatically adapt a general grammar and/or parser to the sub-language defined by a suitable training corpus.", "labels": [], "entities": []}, {"text": "For example, train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and ( describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus.", "labels": [], "entities": []}, {"text": "The work reported here is a logical continuation of two specific strands of research aimed in this general direction.", "labels": [], "entities": []}, {"text": "The first is the popular idea of statistical tagging e.g..", "labels": [], "entities": [{"text": "statistical tagging", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.869747519493103}]}, {"text": "Here, the basic idea is that a given small segment S of the input string may have several possible analyses; in particular, if S is a single word, it may potentially be anyone of several parts of speech.", "labels": [], "entities": []}, {"text": "However, if a substantial training corpus is available to provide reasonable estimates of the relevant parameters, the immediate context surrounding S will usually make most of the locally possible analyses of S extremely implausible.", "labels": [], "entities": []}, {"text": "In the specific case of part-of-speech tagging, it is well-known) that a large proportion of the incorrect tags can be eliminated \"safely\"~ i.e. with very low risk of eliminating correct tags.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7461870312690735}]}, {"text": "In the present paper, the statistical tagging idea is generalized to a method called \"constituent pruning\"; this acts on local analyses of phrases normally larger than single-word units.", "labels": [], "entities": [{"text": "statistical tagging", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7385667562484741}]}, {"text": "Constituent pruning is a bottom-up approach, and is complemented by a second, top-down, method based on Explanation-Based Learning).", "labels": [], "entities": []}, {"text": "This part of the paper is essentially an extension and generalization of the line of work described in.", "labels": [], "entities": []}, {"text": "Here, the basic idea is that grammar rules tend in any specific domain to combine much more frequently in some ways than in others.", "labels": [], "entities": []}, {"text": "Given a sufficiently large corpus parsed by the original, general, grammar, it is possible to identify the common combinations of grammar rules and \"chunk\" them into \"macro-rules\".", "labels": [], "entities": []}, {"text": "The result is a \"specialized\" grammar; this has a larger number of rules, but a simpler structure, allowing it in practice to be parsed very much more quickly using an LRbased method.", "labels": [], "entities": []}, {"text": "The coverage of the specialized grammar is a strict subset of that of the original grammar; thus any analysis produced by the specialized grammar is guaranteed to be valid in the original one as well.", "labels": [], "entities": []}, {"text": "The practical utility of the specialized grammar is largely determined by the loss of coverage incurred by the specialization process.", "labels": [], "entities": []}, {"text": "The two methods, constituent pruning and grammar specialization, are combined as follows.", "labels": [], "entities": [{"text": "grammar specialization", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7391237914562225}]}, {"text": "The rules in the original, general, grammar are divided into two sets, called phrasal and non-phrasal respectively.", "labels": [], "entities": []}, {"text": "Phrasal rules, the majority of which define non-recursive noun phrase constructions, are used as they are; non-phrasal rules are combined using EBL into chunks, forming a specialized grammar which is then compiled further into a set of LRtables.", "labels": [], "entities": []}, {"text": "Parsing proceeds by interleaving constituent creation and deletion.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9694169163703918}, {"text": "interleaving constituent creation", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.6334874232610067}]}, {"text": "First, the lexicon and morphology rules are used to hypothesize word analyses.", "labels": [], "entities": [{"text": "hypothesize word analyses", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6380533377329508}]}, {"text": "Constituent pruning then removes all sufficiently unlikely edges.", "labels": [], "entities": []}, {"text": "Next, the phrasal rules are applied bottom-up, to find all possible phrasal edges, after which unlikely edges are again pruned.", "labels": [], "entities": []}, {"text": "Finally, the specialized grammar is used to search for full parses.", "labels": [], "entities": []}, {"text": "The scheme is fully implemented within aversion of the Spoken Language Translator system, and is normally applied to input in the form of small lattices of hypotheses produced by a speech recognizer.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the constituent pruning method.", "labels": [], "entities": []}, {"text": "Section 3 describes the grammar specialization method, focusing on how the current work extends and improves on previous results.", "labels": [], "entities": []}, {"text": "Section 4 describes experiments where the constituent pruning/grammar specialization method was used on sets of previously unseen speech data.", "labels": [], "entities": []}, {"text": "Section 5 concludes and sketches further directions for research, which we are presently in the process of investigating.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes a number of experiments carried out to test the utility of the theoretical ideas presented above.", "labels": [], "entities": []}, {"text": "The basic corpus used was a set of 16,000 utterances from the Air Travel Planning (ATIS; () domain.", "labels": [], "entities": [{"text": "Air Travel Planning (ATIS; () domain", "start_pos": 62, "end_pos": 98, "type": "DATASET", "confidence": 0.7374962344765663}]}, {"text": "All of these utterances were available in text form; 15,000 of them were used for training, with 1,000 held out for test purposes.", "labels": [], "entities": []}, {"text": "Care was taken to ensure not just that the utterances themselves, but also the speakers of the utterances were disjoint between test and training data; as pointed out in), failure to observe these precautions can result in substantial spurious improvements in test data results.", "labels": [], "entities": []}, {"text": "The 16,000 sentence corpus was analysed by the SRI Core Language Engine (Alshawi (ed), 1992), using a lexicon extended to cover the ATIS domain . All possible grammatical analyses of each utterance were recorded, and an interactive tool was used to allow a human judge to identify the correct and incorrect readings of each utterance.", "labels": [], "entities": [{"text": "SRI Core Language Engine (Alshawi (ed), 1992)", "start_pos": 47, "end_pos": 92, "type": "DATASET", "confidence": 0.9518546313047409}, {"text": "ATIS domain", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9408825039863586}]}, {"text": "The judge was a first-year undergraduate student with a good knowledge of linguistics but no prior experience with the system; the process of judging the corpus took about two and a half person-months.", "labels": [], "entities": []}, {"text": "The input to the EBL-based grammar-specialization process was limited to readings of corpus utterances that had been judged correct.", "labels": [], "entities": []}, {"text": "When utterances had more than one correct reading, a preference heuristic was used to select the most plausible one.", "labels": [], "entities": []}, {"text": "Two sets of experiments were performed.", "labels": [], "entities": []}, {"text": "In the first, increasingly large portions of the training set were used to train specialized grammars.", "labels": [], "entities": []}, {"text": "The coverage loss due to grammar specialization was then measured on the 1,000 utterance test set.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9784886837005615}, {"text": "1,000 utterance test set", "start_pos": 73, "end_pos": 97, "type": "DATASET", "confidence": 0.6755362749099731}]}, {"text": "The experiment was carried out using both the chunking criteria from  (the \"Old\" scheme), and the chunking criteria described in Section 3 above (the \"New\" scheme).", "labels": [], "entities": [{"text": "chunking", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.9501858949661255}, {"text": "Old\" scheme", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.8536425828933716}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "The second set of experiments tested more directly the effect of constituent pruning and grammar specialization on the Spoken Language Translator's speed and coverage; in particular, coverage was measured on the real task of translating English into Swedish, rather than the artificial one of producing a correct QLF analysis.", "labels": [], "entities": [{"text": "speed", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.9773399829864502}, {"text": "coverage", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9828718900680542}, {"text": "coverage", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9989238381385803}]}, {"text": "To this end, the first 500 testset utterances were presented in the form of speech hypothesis lattices derived by aligning and conflating the top five sentence strings produced by aversion of the DECIPHER (TM) recognizer.", "labels": [], "entities": []}, {"text": "The lattices were analysed by four different versions of the parser, exploring the different combinations of turning constituent pruning on or off, and specialized versus unspecialized grammars.", "labels": [], "entities": []}, {"text": "The specialized grammar used the \"New\" scheme, and had been trained on the full training set.", "labels": [], "entities": []}, {"text": "Utterances which took more than 90 CPU seconds to process were timed out and counted as failures.", "labels": [], "entities": []}, {"text": "The four sets of outputs from the parser were then translated into Swedish by the SLT transfer and generation mechanism . Finally, the four sets of candidate translations were pairwise compared in the cases where differing translations had been produced.", "labels": [], "entities": [{"text": "SLT transfer and generation", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.866676077246666}]}, {"text": "We have found this to bean effective way of evaluating system performance.", "labels": [], "entities": []}, {"text": "Although people differ widely in their judgements of whether a given translation can be regarded as \"acceptable\", it is inmost cases surprisingly easy to say which of two possible translations is preferable.", "labels": [], "entities": []}, {"text": "The last two tables summarize the results.", "labels": [], "entities": []}, {"text": "gives the average processing times per input lattice for each type of processing (times measured running SICStus Prolog 3#3 on a SUN Sparc 20/HS21), showing how the time is divided between the various processing phases.", "labels": [], "entities": [{"text": "SUN Sparc 20/HS21", "start_pos": 129, "end_pos": 146, "type": "DATASET", "confidence": 0.7996429562568664}]}, {"text": "shows the relative scores of the four parsing variants, measured according to the \"preferable translation\" criterion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: EBL rules and EBL coverage  number of training examples", "labels": [], "entities": [{"text": "EBL", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.5519870519638062}, {"text": "coverage", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.7636608481407166}]}, {"text": " Table 2: Breakdown of average time spent on each  processing phase for each type of processing (seconds  per utterance)", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.940585732460022}]}, {"text": " Table 3: Comparison between translation results on  the four different analysis alternatives, measured on  the 500-utterance test set. The entry for a given  row and column holds two figures, showing respec- tively the number of examples where the \"row\" vari- ant produced a better translation than the \"col- umn\" variant and the number where it produced a  worse one. Thus for example \"EBL+/pruning+\"  was better than \"EBL-/pruning-\" on 65 examples,  and worse on 24.", "labels": [], "entities": [{"text": "500-utterance test set", "start_pos": 112, "end_pos": 134, "type": "DATASET", "confidence": 0.7108945647875468}]}]}