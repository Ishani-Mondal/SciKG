{"title": [], "abstractContent": [{"text": "In transformation-based parsing, a finite sequence of tree rewriting rules are checked for application to an input structure.", "labels": [], "entities": [{"text": "transformation-based parsing", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.5721213221549988}]}, {"text": "Since in practice only a small percentage of rules are applied to any particular structure, the naive parsing algorithm is rather inefficient.", "labels": [], "entities": []}, {"text": "We exploit this sparseness in rule applications to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm.", "labels": [], "entities": []}, {"text": "1 Introduction The idea of using transformational rules in natural language analysis dates back at least to Chore-sky, who attempted to define a set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)).", "labels": [], "entities": [{"text": "natural language analysis", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6868832111358643}]}, {"text": "Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968).", "labels": [], "entities": [{"text": "generative phonology", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.9160219132900238}]}, {"text": "More recently , transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, preposi-tional phrase attachment disambiguation, and parsing , under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)).", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.6988818496465683}, {"text": "pronunciation network creation", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.7885239322980245}, {"text": "phrase attachment disambiguation", "start_pos": 161, "end_pos": 193, "type": "TASK", "confidence": 0.7583593328793844}, {"text": "parsing", "start_pos": 199, "end_pos": 206, "type": "TASK", "confidence": 0.971900463104248}]}, {"text": "In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand.", "labels": [], "entities": []}, {"text": "Transformation-based systems are typically deter-ministic.", "labels": [], "entities": []}, {"text": "Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed.", "labels": [], "entities": []}, {"text": "Since for each rule the application algorithm must check fora matching at all possible sites to see whether the rule can apply, these systems run in O(rrpn) time, where 7r is the number of rules, p is the cost of a single rule matching , and n is the size of the input structure.", "labels": [], "entities": [{"text": "O(rrpn) time", "start_pos": 149, "end_pos": 161, "type": "METRIC", "confidence": 0.9117895603179932}]}, {"text": "While this results in fast processing, it is possible to create much faster systems.", "labels": [], "entities": []}, {"text": "In (Roche and Schabes, 1995), a method is described for converting a list of transformations that operates on strings into a determin-istic finite state transducer, resulting in an optimal tagger in the sense that tagging requires only one state transition per word, giving a linear time tag-ger whose run-time is independent of the number and size of rules.", "labels": [], "entities": []}, {"text": "In this paper we consider transformation-based parsing, introduced in (Brill, 1993), and we improve upon the O(Trpn) time upper bound..", "labels": [], "entities": [{"text": "transformation-based parsing", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6075437068939209}, {"text": "O(Trpn) time upper", "start_pos": 109, "end_pos": 127, "type": "METRIC", "confidence": 0.9435333609580994}]}, {"text": "In transformation-based parsing, an ordered sequence of tree-rewriting rules (tree transformations) are applied to an initial parse structure for an input sentence , to derive the final parse structure.", "labels": [], "entities": [{"text": "transformation-based parsing", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.613492876291275}]}, {"text": "We observe that inmost transformation-based parsers, only a small percentage of rules are actually applied, for any particular input sentence.", "labels": [], "entities": []}, {"text": "For example, in an application of the transformation-based parser described in (Brill, 1993), 7r = 300 rules were learned, to be applied at each node of the initial parse structure , but the average number of rules that are successfully applied at each node is only about one.", "labels": [], "entities": []}, {"text": "So a lot of time is spent testing whether the conditions are met for applying a transformation and finding out that they are not met.", "labels": [], "entities": []}, {"text": "This paper presents an original algorithm for transformation-based parsing working in O(ptlog(t)) time, where t is the total number of rules applied for an input sentence.", "labels": [], "entities": [{"text": "transformation-based parsing", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.5669650435447693}, {"text": "O", "start_pos": 86, "end_pos": 87, "type": "METRIC", "confidence": 0.9469525814056396}]}, {"text": "Since in practical cases t is smaller than n and we can neglect the log(n) factor, we have achieved a time improvement of a factor of r.", "labels": [], "entities": []}, {"text": "We emphasize that rr can be several hundreds large in actual systems where transformations are lexicalized.", "labels": [], "entities": []}, {"text": "Our result is achieved by preprocessing the transformation list, deriving a finite state, determiflistic tree automaton.", "labels": [], "entities": []}, {"text": "The algorithm then exploits the automaton in away that obviates the need for checking the conditions of a rule when that rule will not apply, thereby greatly improving parsing run-time over the straightforward parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 168, "end_pos": 175, "type": "TASK", "confidence": 0.9744260907173157}]}, {"text": "Ina sense, our algorithm spends time only with rules that can be applied, as if it knew in advance which rules cannot be applied during the parsing process.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.8943249583244324}]}, {"text": "The remainder of this paper is organized as fol-255", "labels": [], "entities": [{"text": "fol-255", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.8856353759765625}]}], "introductionContent": [{"text": "The idea of using transformational rules in natural language analysis dates back at least to Choresky, who attempted to define a set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see).", "labels": [], "entities": [{"text": "natural language analysis", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6853247086207072}]}, {"text": "Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with.", "labels": [], "entities": [{"text": "generative phonology", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.9172443151473999}]}, {"text": "More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see and).", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7146967351436615}, {"text": "pronunciation network creation", "start_pos": 113, "end_pos": 143, "type": "TASK", "confidence": 0.7886826395988464}, {"text": "prepositional phrase attachment disambiguation", "start_pos": 145, "end_pos": 191, "type": "TASK", "confidence": 0.707838699221611}, {"text": "parsing", "start_pos": 197, "end_pos": 204, "type": "TASK", "confidence": 0.9649752378463745}]}, {"text": "In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand.", "labels": [], "entities": []}, {"text": "Transformation-based systems are typically deterministic.", "labels": [], "entities": []}, {"text": "Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed.", "labels": [], "entities": []}, {"text": "Since for each rule the application algorithm must check fora matching at all possible sites to see whether the rule can apply, these systems run in O(rrpn) time, where 7r is the number of rules, p is the cost of a single rule matching, and n is the size of the input structure.", "labels": [], "entities": [{"text": "O(rrpn) time", "start_pos": 149, "end_pos": 161, "type": "METRIC", "confidence": 0.9117895603179932}]}, {"text": "While this results in fast processing, it is possible to create much faster systems.", "labels": [], "entities": []}, {"text": "In), a method is described for converting a list of transformations that operates on strings into a deterministic finite state transducer, resulting in an optimal tagger in the sense that tagging requires only one state transition per word, giving a linear time tagger whose run-time is independent of the number and size of rules.", "labels": [], "entities": []}, {"text": "In this paper we consider transformation-based parsing, introduced in, and we improve upon the O(Trpn) time upper bound..", "labels": [], "entities": [{"text": "transformation-based parsing", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.5873459875583649}, {"text": "O(Trpn) time upper", "start_pos": 95, "end_pos": 113, "type": "METRIC", "confidence": 0.9439766009648641}]}, {"text": "In transformation-based parsing, an ordered sequence of tree-rewriting rules (tree transformations) are applied to an initial parse structure for an input sentence, to derive the final parse structure.", "labels": [], "entities": [{"text": "transformation-based parsing", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.613492876291275}]}, {"text": "We observe that inmost transformation-based parsers, only a small percentage of rules are actually applied, for any particular input sentence.", "labels": [], "entities": []}, {"text": "For example, in an application of the transformation-based parser described in, 7r = 300 rules were learned, to be applied at each node of the initial parse structure, but the average number of rules that are successfully applied at each node is only about one.", "labels": [], "entities": []}, {"text": "So a lot of time is spent testing whether the conditions are met for applying a transformation and finding out that they are not met.", "labels": [], "entities": []}, {"text": "This paper presents an original algorithm for transformation-based parsing working in O(ptlog(t)) time, where t is the total number of rules applied for an input sentence.", "labels": [], "entities": [{"text": "transformation-based parsing", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.5669650435447693}, {"text": "O", "start_pos": 86, "end_pos": 87, "type": "METRIC", "confidence": 0.9469525814056396}]}, {"text": "Since in practical cases t is smaller than n and we can neglect the log(n) factor, we have achieved a time improvement of a factor of r.", "labels": [], "entities": []}, {"text": "We emphasize that rr can be several hundreds large in actual systems where transformations are lexicalized.", "labels": [], "entities": []}, {"text": "Our result is achieved by preprocessing the transformation list, deriving a finite state, determiflistic tree automaton.", "labels": [], "entities": []}, {"text": "The algorithm then exploits the automaton in away that obviates the need for checking the conditions of a rule when that rule will not apply, thereby greatly improving parsing run-time over the straightforward parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 168, "end_pos": 175, "type": "TASK", "confidence": 0.9744260907173157}]}, {"text": "Ina sense, our algorithm spends time only with rules that can be applied, as if it knew in advance which rules cannot be applied during the parsing process.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.8943249583244324}]}, {"text": "The remainder of this paper is organized as fol-lows.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce some preliminaries, and in Section 3 we provide a representation of transformations that uses finite state, deterministic tree automata.", "labels": [], "entities": []}, {"text": "Our algorithm is then specified in Section 4.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we discuss related work in the existing literature.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}