{"title": [{"text": "A New Statistical Parser Based on Bigram Lexical Dependencies", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes anew statistical parser which is based on probabilities of dependencies between head-words in the parse tree.", "labels": [], "entities": []}, {"text": "Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words.", "labels": [], "entities": [{"text": "bigram probability estimation", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.6007011334101359}]}, {"text": "Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Mager-man 95; Jelinek et al.", "labels": [], "entities": [{"text": "Wall Street Journal data", "start_pos": 12, "end_pos": 36, "type": "DATASET", "confidence": 0.9607685208320618}, {"text": "SPATTER", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9754961133003235}]}, {"text": "94), which has the best published results fora statistical parser on this task.", "labels": [], "entities": []}, {"text": "The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes.", "labels": [], "entities": []}, {"text": "With abeam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss inaccuracy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)).", "labels": [], "entities": [{"text": "prepositional-phrase attachment", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.7316759675741196}]}, {"text": "However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabilities on non-terminal labels and part of speech tags alone.", "labels": [], "entities": [{"text": "probabilistic parsing", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.6906988024711609}]}, {"text": "The SPATTER parser (Magerman 95; 3elinek et ah 94) does use lexical information, and recovers labeled constituents in Wall Street Journal text with above 84% accuracy -as far as we know the best published results on this task.", "labels": [], "entities": [{"text": "SPATTER parser", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.6058571636676788}, {"text": "Wall Street Journal text", "start_pos": 118, "end_pos": 142, "type": "DATASET", "confidence": 0.9367953985929489}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9946993589401245}]}, {"text": "This paper describes anew parser which is much simpler than SPATTER, yet performs at least as well when trained and tested on the same Wall Street Journal data.", "labels": [], "entities": [{"text": "Wall Street Journal data", "start_pos": 135, "end_pos": 159, "type": "DATASET", "confidence": 0.9676294326782227}]}, {"text": "The method uses lexical information directly by modeling head-modifier 1 relations between pairs of words.", "labels": [], "entities": []}, {"text": "In this way it is similar to *This research was supported by ARPA Grant N6600194-C6043.", "labels": [], "entities": [{"text": "ARPA Grant N6600194-C6043", "start_pos": 61, "end_pos": 86, "type": "DATASET", "confidence": 0.8666134476661682}]}, {"text": "1By 'modifier' we mean the linguistic notion of either an argument or adjunct.", "labels": [], "entities": []}, {"text": "Link grammars (, and dependency grammars in general.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Percentage of dependencies vs. distance be- tween the head words involved. These figures count  baseNPs as a single word, and are taken from WSJ  training data.", "labels": [], "entities": [{"text": "WSJ  training data", "start_pos": 151, "end_pos": 169, "type": "DATASET", "confidence": 0.9269652962684631}]}, {"text": " Table 2: Percentage of dependencies vs. number of  verbs between the head words involved.", "labels": [], "entities": []}, {"text": " Table 3: Results on Section 23 of the WSJ Treebank. (1) is the basic model; (2) is the basic model  with the punctuation rule described in section 2.7; (3) is model (2) with POS tags ignored when lexical  information is present; (4) is model (3) with probability distributions from the POS tagger. LI:t/LP =  labeled recall/precision. CBs is the average number of crossing brackets per sentence. 0 CBs, ~ 2 CBs  are the percentage of sentences with 0 or < 2 crossing brackets respectively.", "labels": [], "entities": [{"text": "WSJ Treebank", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9243772327899933}, {"text": "recall", "start_pos": 318, "end_pos": 324, "type": "METRIC", "confidence": 0.8324704170227051}, {"text": "precision", "start_pos": 325, "end_pos": 334, "type": "METRIC", "confidence": 0.7895418405532837}]}, {"text": " Table 5: The trade-off between speed and accuracy  as the beam-size is varied. Model (3) was used for  this test on all sentences < 100 words in section 23.", "labels": [], "entities": [{"text": "speed", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9630498290061951}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9987032413482666}]}]}