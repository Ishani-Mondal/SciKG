{"title": [{"text": "An Iterative Algorithm to Build Chinese Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "\u00b0 \u2022 We present an iterative procedure to build a Chinese language model (LM).", "labels": [], "entities": []}, {"text": "We segment Chinese text into words based on a word-based Chinese language model.", "labels": [], "entities": []}, {"text": "However , the construction of a Chinese LM itself requires word boundaries.", "labels": [], "entities": []}, {"text": "To get out of the chicken-and-egg problem, we propose an iterative procedure that alternates two operations: segmenting text into words and building an LM.", "labels": [], "entities": []}, {"text": "Starting with an initial segmented corpus and an LM based upon it, we use a Viterbi-liek algorithm to segment another set of data.", "labels": [], "entities": []}, {"text": "Then, we build an LM based on the second set and use the resulting LM to segment again the first corpus.", "labels": [], "entities": []}, {"text": "The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors.", "labels": [], "entities": []}, {"text": "Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation, but discovers unseen words surprisingly well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9992703795433044}]}, {"text": "The resulting word-based LM has a perplexity of 188 fora general Chinese corpus.", "labels": [], "entities": []}, {"text": "1 Introduction In statistical speech recognition(Bahl et al., 1983), it is necessary to build a language model(LM) for assigning probabilities to hypothesized sentences.", "labels": [], "entities": [{"text": "statistical speech recognition(Bahl et al., 1983)", "start_pos": 18, "end_pos": 67, "type": "TASK", "confidence": 0.7568951606750488}]}, {"text": "The LM is usually built by collecting statistics of words over a large set of text data.", "labels": [], "entities": []}, {"text": "While doing so is straightforward for English, it is not trivial to collect statistics for Chinese words since word boundaries are not marked in written Chinese text.", "labels": [], "entities": []}, {"text": "Chinese is a morphosyllabic language (DeFrancis, 1984) in that almost all Chinese characters represent a single syllable and most Chinese characters are also morphemes.", "labels": [], "entities": []}, {"text": "Since a word can be multi-syllabic, it is generally non-trivial to segment a Chinese sentence into words(Wu and Tseng, 1993).", "labels": [], "entities": []}, {"text": "Since segmentation is a fundamental problem in Chinese information processing , there is a large literature to deal with the problem.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9686357975006104}, {"text": "Chinese information processing", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.645553708076477}]}, {"text": "Recent work includes (Sproat et al., 1994) and (Wang et al., 1992).", "labels": [], "entities": []}, {"text": "In this paper, we adopt a statistical approach to segment Chinese text based on an LM because of its autonomous nature and its capability to handle unseen words.", "labels": [], "entities": []}, {"text": "As far as speech recognition is concerned, what is needed is a model to assign a probability to a string of characters.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7931133508682251}]}, {"text": "One may argue that we could bypass the segmentation problem by building a character-based LM.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.976352870464325}]}, {"text": "However, we have a strong belief that a word-based LM would be better than a character-based 1 one.", "labels": [], "entities": []}, {"text": "In addition to speech recognition, the use of word based models would have value in information retrieval and other language processing applications.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7692208290100098}, {"text": "information retrieval", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7567021250724792}]}, {"text": "If word boundaries are given, all established techniques can be exploited to construct an LM (Jelinek et al., 1992) just as is done for English.", "labels": [], "entities": []}, {"text": "Therefore, segmentation is a key issue in building the Chinese LM.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.9784149527549744}, {"text": "Chinese LM", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.878730446100235}]}, {"text": "In this paper, we propose a segmentation algorithm based on an LM.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9756008386611938}]}, {"text": "Since building an LM itself needs word boundaries, this is a chicken-and-egg problem.", "labels": [], "entities": []}, {"text": "To get out of this, we propose an iterative procedure that alternates between the segmentation of Chinese text and the construction of the LM.", "labels": [], "entities": []}, {"text": "Our preliminary experiments show that the iterative procedure is able to improve the segmentation accuracy and more importantly, it can detect unseen words automatically.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.9411831498146057}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9381850361824036}]}, {"text": "In section 2, the Viterbi-like segmentation algorithm based on a LM is described.", "labels": [], "entities": []}, {"text": "Then in section section:iter-proc we discuss the alternating procedure of segmentation and building Chinese LMs.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.9757176041603088}]}, {"text": "We test the segmentation algorithm and the alternating procedure and the results are reported in sec-I A character-based trigram model has a perplexity of 46 per character or 462 per word (a Chinese word has an average length of 2 characters), while a word-based trigram model has a perplexity 188 on the same set of data.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.9612377882003784}]}, {"text": "While the comparison would be fairer using a 5-gram character model, that the word model would have a lower perplexity as long as the coverage is high.", "labels": [], "entities": []}], "introductionContent": [{"text": "In statistical speech recognition(, it is necessary to build a language model(LM) for assigning probabilities to hypothesized sentences.", "labels": [], "entities": [{"text": "statistical speech recognition", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.682533343633016}]}, {"text": "The LM is usually built by collecting statistics of words over a large set of text data.", "labels": [], "entities": []}, {"text": "While doing so is straightforward for English, it is not trivial to collect statistics for Chinese words since word boundaries are not marked in written Chinese text.", "labels": [], "entities": []}, {"text": "Chinese is a morphosyllabic language in that almost all Chinese characters represent a single syllable and most Chinese characters are also morphemes.", "labels": [], "entities": []}, {"text": "Since a word can be multi-syllabic, it is generally non-trivial to segment a Chinese sentence into words(.", "labels": [], "entities": []}, {"text": "Since segmentation is a fundamental problem in Chinese information processing, there is a large literature to deal with the problem.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9686357975006104}, {"text": "Chinese information processing", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.645553708076477}]}, {"text": "Recent work includes and (.", "labels": [], "entities": []}, {"text": "In this paper, we adopt a statistical approach to segment Chinese text based on an LM because of its autonomous nature and its capability to handle unseen words.", "labels": [], "entities": []}, {"text": "As far as speech recognition is concerned, what is needed is a model to assign a probability to a string of characters.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7931133508682251}]}, {"text": "One may argue that we could bypass the segmentation problem by building a characterbased LM.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9775096774101257}]}, {"text": "However, we have a strong belief that a word-based LM would be better than a characterbased 1 one.", "labels": [], "entities": []}, {"text": "In addition to speech recognition, the use of word based models would have value in information retrieval and other language processing applications.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7692208290100098}, {"text": "information retrieval", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7567021250724792}]}, {"text": "If word boundaries are given, all established techniques can be exploited to construct an LM) just as is done for English.", "labels": [], "entities": []}, {"text": "Therefore, segmentation is a key issue in building the Chinese LM.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.9784149527549744}, {"text": "Chinese LM", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.878730446100235}]}, {"text": "In this paper, we propose a segmentation algorithm based on an LM.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9756008386611938}]}, {"text": "Since building an LM itself needs word boundaries, this is a chicken-and-egg problem.", "labels": [], "entities": []}, {"text": "To get out of this, we propose an iterative procedure that alternates between the segmentation of Chinese text and the construction of the LM.", "labels": [], "entities": []}, {"text": "Our preliminary experiments show that the iterative procedure is able to improve the segmentation accuracy and more importantly, it can detect unseen words automatically.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.9411831498146057}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9381850361824036}]}, {"text": "In section 2, the Viterbi-like segmentation algorithm based on a LM is described.", "labels": [], "entities": []}, {"text": "Then in section section:iter-proc we discuss the alternating procedure of segmentation and building Chinese LMs.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.9757176041603088}]}, {"text": "We test the segmentation algorithm and the alternating procedure and the results are reported in sec-I A character-based trigram model has a perplexity of 46 per character or 462 per word (a Chinese word has an average length of 2 characters), while a word-based trigram model has a perplexity 188 on the same set of data.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.9612377882003784}]}, {"text": "While the comparison would be fairer using a 5-gram character model, that the word model would have a lower perplexity as long as the coverage is high.", "labels": [], "entities": []}, {"text": "Finally, the work is summarized in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In addition to the 5 million characters of segmented text, we had unsegmented data from various sources reaching about 13 million characters.", "labels": [], "entities": []}, {"text": "We applied our iterative algorithm to that corpus.", "labels": [], "entities": []}, {"text": "shows the figure of merit of the resulting segmentation of the 100 sentence test set described earlier.", "labels": [], "entities": []}, {"text": "After one iteration, the agreement with the original segmentation decreased by 3 percentage points, while the agreement with the human segmentation increased by less than one percentage point.", "labels": [], "entities": []}, {"text": "We ran our computation intensive procedure for one iteration only.", "labels": [], "entities": []}, {"text": "The results indicate that the impact on segmentation accuracy would be small.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9820368885993958}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9741901159286499}]}, {"text": "However, the new unsegmented corpus is a good source of automatically discovered words.", "labels": [], "entities": []}, {"text": "A 20 examples picked randomly from about 1500 unseen words are shown in.", "labels": [], "entities": []}, {"text": "16 of them are reasonably good words and are listed with their translated meanings.", "labels": [], "entities": []}, {"text": "The problematic words are marked with \"?\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A segmentation example  chars I C: C2 C3 C4 C5 C6  k I 1  2  3  4  5  6  p(k)  0  1  1  3  3  4", "labels": [], "entities": []}, {"text": " Table 2: Segmentation Accuracy  ORG  P1  P2  ORG  P1  85.9  P2  79.1  90.9  P3  87.4  85.7 82.2", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9421896934509277}, {"text": "Accuracy  ORG  P1  P2  ORG  P1  85.9  P2  79.1  90.9  P3  87.4  85.7 82.2", "start_pos": 23, "end_pos": 96, "type": "DATASET", "confidence": 0.8106416932174138}]}, {"text": " Table 4: Segmentation of accuracy after one itera- tion", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9582301378250122}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9959492683410645}]}]}