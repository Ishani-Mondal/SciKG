{"title": [{"text": "Unsupervised Learning of Word-Category Guessing Rules", "labels": [], "entities": [{"text": "Unsupervised Learning of Word-Category Guessing", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.5529674708843231}]}], "abstractContent": [{"text": "Words unknown to the lexicon present a substantial problem to part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.6915621161460876}]}, {"text": "In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words.", "labels": [], "entities": []}, {"text": "Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus: prefix morphological rules, suffix morphological rules and ending-guessing rules.", "labels": [], "entities": []}, {"text": "The learning was performed on the Brown Corpus data and rule-sets, with a highly competitive performance, were produced and compared with the state-of-the-art.", "labels": [], "entities": [{"text": "Brown Corpus data", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.993110716342926}]}], "introductionContent": [{"text": "Words unknown to the lexicon present a substantial problem to part-of-speech (POS) tagging of realworld texts.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging of realworld texts", "start_pos": 62, "end_pos": 109, "type": "TASK", "confidence": 0.743882741779089}]}, {"text": "Taggers assign a single POS-tag to a word-token, provided that it is known what partsof-speech this word can take on in principle.", "labels": [], "entities": []}, {"text": "So, first words are looked up in the lexicon.", "labels": [], "entities": []}, {"text": "However, 3 to 5% of word tokens are usually missing in the lexicon when tagging real-world texts.", "labels": [], "entities": []}, {"text": "This is where word-Pos guessers take their place --they employ the analysis of word features, e.g. word leading and trailing characters, to figure out its possible POS categories.", "labels": [], "entities": []}, {"text": "A set of rules which on the basis of ending characters of unknown words, assign them with sets of possible POS-tags is supplied with the Xerox tagger.", "labels": [], "entities": [{"text": "Xerox tagger", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.9574337899684906}]}, {"text": "A similar approach was taken in () where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending.", "labels": [], "entities": []}, {"text": "Ina system of rules which uses both ending-guessing and more morphologically motivated rules is described.", "labels": [], "entities": []}, {"text": "The best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9658981561660767}]}, {"text": "The major topic in the development of word-Pos guessers is the strategy which is to be used for the acquisition of the guessing rules.", "labels": [], "entities": [{"text": "word-Pos guessers", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6706251353025436}]}, {"text": "A rule-based tagger described in is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology and intuition.", "labels": [], "entities": []}, {"text": "A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources.", "labels": [], "entities": []}, {"text": "In (Zhang&Kim, 1990) a system for the automated learning of morphological word-formation rules is described.", "labels": [], "entities": []}, {"text": "This system divides a string into three regions and from training examples infers their correspondence to underlying morphological features.", "labels": [], "entities": []}, {"text": "Brill outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus.", "labels": [], "entities": []}, {"text": "A statistical-based suffix learner is presented in.", "labels": [], "entities": []}, {"text": "From a pre-tagged training corpus it constructs the suffix tree where every suffix is associated with its information measure.", "labels": [], "entities": []}, {"text": "Although the learning process in these and some other systems is fully unsupervised and the accuracy of obtained rules reaches current state-of-the-art, they require specially prepared training data --a pretagged training corpus, training examples, etc.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9985507130622864}]}, {"text": "In this paper we describe anew fully automatic technique for learning part-of-speech guessing rules.", "labels": [], "entities": [{"text": "part-of-speech guessing rules", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.7398333648840586}]}, {"text": "This technique does not require specially prepared training data and employs fully unsupervised statistical learning using the lexicon supplied with the tagger and word-frequencies obtained from a raw corpus.", "labels": [], "entities": []}, {"text": "The learning is implemented as a two-staged process with feedback.", "labels": [], "entities": []}, {"text": "First, setting certain parameters a set of guessing rules is acquired, then it is evaluated and the results of evaluation are used for re-acquisition of a better tuned rule-set.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are two important questions which arise at the rule acquisition stage -how to choose the scoring threshold 0, and what is the performance of the rulesets produced with different thresholds.", "labels": [], "entities": [{"text": "rule acquisition", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.8964338898658752}]}, {"text": "The task of assigning a set of Pos-tags to a word is actually quite similar to the task of document categorisation where a document should be assigned with a set of descriptors which represent its contents.", "labels": [], "entities": []}, {"text": "The performance of such assignment can be measured in: recall-the percentage of BOSs which were assigned correctly by the guesser to a word; precision-the percentage of BOSs the guesser assigned correctly over the total number of BOSs it assigned to the word; coverage -the proportion of words which the guesser was able to classify, but not necessarily correctly; In our experiments we measured word precision and word recall (micro-average).", "labels": [], "entities": [{"text": "recall-the percentage of BOSs", "start_pos": 55, "end_pos": 84, "type": "METRIC", "confidence": 0.9543973654508591}, {"text": "precision-the percentage of BOSs", "start_pos": 141, "end_pos": 173, "type": "METRIC", "confidence": 0.9552260041236877}, {"text": "coverage", "start_pos": 260, "end_pos": 268, "type": "METRIC", "confidence": 0.9543710947036743}, {"text": "precision", "start_pos": 401, "end_pos": 410, "type": "METRIC", "confidence": 0.68387371301651}, {"text": "recall", "start_pos": 420, "end_pos": 426, "type": "METRIC", "confidence": 0.8449212312698364}]}, {"text": "There were two types of data in use at this stage.", "labels": [], "entities": []}, {"text": "First, we evaluated the guessing rules against the actual lexicon: every word from the lexicon, except for closed-class words and words shorter than five characters 4, was guessed by the different guessing strategies and the results were compared with the information the word had in the lexicon.", "labels": [], "entities": []}, {"text": "In the other evaluation experiment we measured the performance of the guessing rules against the training corpus.", "labels": [], "entities": []}, {"text": "For every word we computed its metrics exactly as in the previous experiment.", "labels": [], "entities": []}, {"text": "Then we multiplied these results by the corpus frequency of this particular word and averaged them.", "labels": [], "entities": []}, {"text": "Thus the most frequent words had the greatest influence on the aggreagte measures.", "labels": [], "entities": []}, {"text": "First, we concentrated on finding the best thresholds 08 for the rule-sets.", "labels": [], "entities": []}, {"text": "To do that for each rule-set produced using different thresholds we recorded the three metrics and chose the set with the best aggregate.", "labels": [], "entities": []}, {"text": "In some results of that experiment are shown.", "labels": [], "entities": []}, {"text": "The best thresholds were detected: for ending rules -75 points, for suffix rules -60, and for 3For ending-guessing rules this is always true, so only the ending itself counts.", "labels": [], "entities": []}, {"text": "4the actual size of the filtered lexicon was 47,659 entries out of 53,015 entries of the original lexicon.", "labels": [], "entities": []}, {"text": "One can notice a slight difference in the results obtained over the lexicon and the corpus.", "labels": [], "entities": []}, {"text": "The corpus results are better because the training technique explicitly targeted the rule-sets to the most frequent cases of the corpus rather than the lexicon.", "labels": [], "entities": []}, {"text": "In average ending-guessing rules were detected to cover over 96% of the unknown words.", "labels": [], "entities": []}, {"text": "The precision of 74% roughly can be interpreted as that for words which take on three different BOSs in their BOs-class, the ending-guessing rules will assign four, but in 95% of the times (recall) the three required BOSs will be among the four assigned by the guess.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994220733642578}, {"text": "BOs-class", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9128391146659851}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9840664863586426}]}, {"text": "In comparison with the Xerox word-ending guesser taken as the base-line model we detect a substantial increase in the precision by about 22% and a cheerful increase in coverage by about 6%.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9990390539169312}, {"text": "coverage", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9980898499488831}]}, {"text": "This means that the Xerox guesser creates more ambiguity for the disambiguator, assigning five instead of three BOSs in the example above.", "labels": [], "entities": [{"text": "BOSs", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9918631315231323}]}, {"text": "It can also handle 6% less unknown words which, in fact, might decrease its performance even lower.", "labels": [], "entities": []}, {"text": "In comparison with the ending-guessing rules, the morphological rules have much better precision and hence better accuracy of guessing.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9994550347328186}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9995759129524231}]}, {"text": "Virtually almost every word which can be guessed by the morphological rules is guessed exactly correct (97% recall and 97% precision).", "labels": [], "entities": [{"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9977691173553467}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9966105818748474}]}, {"text": "Not surprisingly, the coverage of morphological rules is much lower than that of the ending-guessing onesfor the suffix rules it is less than 40% and for the prefix rules about 5-6%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9935067892074585}]}, {"text": "After obtaining the optimal rule-sets we performed the same experiment on a word-sample which was not included into the training lexicon and corpus.", "labels": [], "entities": []}, {"text": "We gathered about three thousand words from the lexicon developed for the Wall Street Journal corpus 5 and collected frequencies of these words in this corpus.", "labels": [], "entities": [{"text": "Wall Street Journal corpus 5", "start_pos": 74, "end_pos": 102, "type": "DATASET", "confidence": 0.9776836633682251}]}, {"text": "At this experiment we obtained similar metrics apart from the coverage which dropped about 0.5% for Ending 75 and Xerox rule-sets and 7% for the Suffix 60 rule-set.", "labels": [], "entities": [{"text": "coverage", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9979186654090881}, {"text": "Ending 75 and Xerox rule-sets", "start_pos": 100, "end_pos": 129, "type": "DATASET", "confidence": 0.8051523685455322}, {"text": "Suffix 60 rule-set", "start_pos": 145, "end_pos": 163, "type": "DATASET", "confidence": 0.8927647868792216}]}, {"text": "This, actually, did not come as a surprise, since many main forms required by the suffix rules were missing in the lexicon.", "labels": [], "entities": []}, {"text": "In the next experiment we evaluated whether the morphological rules add any improvement if they are used in conjunction with the ending-guessing rules.", "labels": [], "entities": []}, {"text": "We also evaluated in detail whether a conjunctive application with the Xerox guesser would boost the performance.", "labels": [], "entities": [{"text": "Xerox guesser", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.8699340224266052}]}, {"text": "As in the previous experiment we measured the precision, recall and coverage both on the lexicon and on the corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9996627569198608}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9994743466377258}, {"text": "coverage", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.998225748538971}]}, {"text": "demonstrates some results of this experiment.", "labels": [], "entities": []}, {"text": "The first part of the table shows that when the Xerox guesser is applied before the E75 guesser we measure a drop in the performance.", "labels": [], "entities": [{"text": "E75 guesser", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.9243715703487396}]}, {"text": "When the Xerox guesser is applied after the E75 guesser no sufficient changes to the performance are noticed.", "labels": [], "entities": [{"text": "Xerox guesser", "start_pos": 9, "end_pos": 22, "type": "DATASET", "confidence": 0.81095752120018}, {"text": "E75 guesser", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9522940218448639}]}, {"text": "This actually proves that the E75 rule-set fully supercedes the Xerox rule-set.", "labels": [], "entities": [{"text": "E75 rule-set", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.970687597990036}, {"text": "Xerox rule-set", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9655718207359314}]}, {"text": "The second part of the table shows that the cascading application of the morphological rule-sets together with the ending-guessing rules increases the over5these words were not listed in the training lexicon all precision of the guessing by a further 5%.", "labels": [], "entities": [{"text": "precision", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9995318651199341}]}, {"text": "This makes the improvements against the base-line Xerox guesser 28% in precision and 7% in coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9996635913848877}, {"text": "coverage", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9985520243644714}]}], "tableCaptions": [{"text": " Table 3. When us- ing the Xerox tagger with its original guesser, 63 un- known words were incorrectly tagged and the accu- racy on the unknown words was measured at 81.8%.  When the Xerox tagger was equipped with our cas- cading guesser its accuracy on unknown words in- creased by almost 9% upto 90.5%. The same situa- tion was detected with Brill's tagger which in general  was slightly more accurate than the Xerox one 9. The  cascading guesser performed better than Brill's orig- inal guesser by about 8% boosting the performance  on the unknown words from 84.5% 1\u00b0 to 92.2%. The  accuracy of the taggers on the set of 347 unknown  words when they were made known to the lexicon  was detected at 98.5% for both taggers.", "labels": [], "entities": [{"text": "accu- racy", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.964137613773346}, {"text": "accuracy", "start_pos": 242, "end_pos": 250, "type": "METRIC", "confidence": 0.9985425472259521}, {"text": "accuracy", "start_pos": 586, "end_pos": 594, "type": "METRIC", "confidence": 0.9992087483406067}]}, {"text": " Table 4. The accuracy of the taggers  on the 2,215 unknown words when they were made  known to the lexicon was much lower than in the  previous experiment --90.3% for the Xerox tagger  and 91.5% for Brill's tagger. Naturally, the perfor- mance of the guessers was also lower than in the  previous experiment plus the fact that many \"semi- closed\" class adverbs like \"however\", \"instead\", etc.,  were missing in the small lexicon. The accuracy of  the tagging on unknown words dropped by about  5% in general. The best results on unknown words  were again obtained on the cascading guesser (86%- 87.45%) and Brill's tagger again did better then the  Xerox one by 1.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.999346911907196}, {"text": "perfor- mance", "start_pos": 231, "end_pos": 244, "type": "METRIC", "confidence": 0.9428009986877441}, {"text": "accuracy", "start_pos": 435, "end_pos": 443, "type": "METRIC", "confidence": 0.9995753169059753}]}, {"text": " Table 1: Results obtained at the evaluation of the acquired rule-sets over the training lexicon and the  training corpus. Guessing rule-sets produced using different confidence thresholds were compared. Best- scored rule-sets detected: Prefix 80 -prefix morphological rules which scored over 80 points, Suffix 60 -suffix  morphological rules which scored over 60 points and Ending 75 -ending-guessing rules which scored over 75  points. As the base-line model was taken the ending guesser developed by Xerox (X).", "labels": [], "entities": [{"text": "Ending", "start_pos": 375, "end_pos": 381, "type": "METRIC", "confidence": 0.9516952037811279}]}, {"text": " Table 2: Results of the cascading application of the rule-sets over the training lexicon and training corpus.  Ps0 -prefix rule-set scored over 80 points, $60 -suffix rule-set scored over 60 points, E75 -ending-guessing  rule-set scored over 75 points. As the base-line model was taken the ending guesser developed by Xerox (X).  The first part of the table shows that the E75 rule-set outperforms and fully supercedes the Xerox rule-set.  The second part of the table shows that the cascading application of the morphological rule-sets together  with the ending-guessing rules increases the performance by about 5% in precision.", "labels": [], "entities": [{"text": "Ps0 -prefix", "start_pos": 112, "end_pos": 123, "type": "METRIC", "confidence": 0.9301414887110392}, {"text": "precision", "start_pos": 620, "end_pos": 629, "type": "METRIC", "confidence": 0.9993495345115662}]}, {"text": " Table 3: This table shows the results of tagging a text with 347 unknown words by four different combinations  of two taggers and three word-guessing modules using the Brown Corpus model. The accuracy of tagging  the unknown words when they were made known to the lexicon was detected at 98.5% for both taggers.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 169, "end_pos": 181, "type": "DATASET", "confidence": 0.9643032848834991}, {"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9993438124656677}]}, {"text": " Table 4: This table shows the results of tagging the same as in", "labels": [], "entities": [{"text": "tagging", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9810367822647095}]}]}