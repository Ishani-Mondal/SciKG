{"title": [{"text": "Noun-Phrase Analysis in Unrestricted Text for Information Retrieval", "labels": [], "entities": [{"text": "Noun-Phrase Analysis", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6684145629405975}, {"text": "Information Retrieval", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7192919254302979}]}], "abstractContent": [{"text": "Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8426762819290161}]}, {"text": "This paper reports on the application of a few simple, yet robust and efficient noun-phrase analysis techniques to create better indexing phrases for information retrieval.", "labels": [], "entities": [{"text": "noun-phrase analysis", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7665082216262817}, {"text": "information retrieval", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.7929005920886993}]}, {"text": "In particular, we describe a hybrid approach to the extraction of meaningful (continuous or discontinuous) sub-compounds from complex noun phrases using both corpus statistics and linguistic heuristics.", "labels": [], "entities": [{"text": "extraction of meaningful (continuous or discontinuous) sub-compounds from complex noun phrases", "start_pos": 52, "end_pos": 146, "type": "TASK", "confidence": 0.7663961557241586}]}, {"text": "Results of experiments show that indexing based on such extracted sub-compounds improves both recall and precision in an information retrieval system.", "labels": [], "entities": [{"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9991174340248108}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.998809814453125}]}, {"text": "The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction.", "labels": [], "entities": [{"text": "noun-phrase analysis", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7663137316703796}, {"text": "book indexing", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.6673284769058228}, {"text": "automatic thesaurus extraction", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.6119305789470673}]}], "introductionContent": [], "datasetContent": [{"text": "We tested the phrase extraction system (PES) by using it to index documents in an actual retrieval task.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7793637812137604}]}, {"text": "In particular, we substituted the PES for the default NLP module in the CLARIT system and then indexed a large corpus using the terms nominated by the PES, essentially the extracted small compounds and single words (but not words within a lexical atom).", "labels": [], "entities": []}, {"text": "All other normal CLARIT processing--weighting of terms, division of documents into subdocuments (passages), vector-space modeling, etc.--was used in its default mode.", "labels": [], "entities": [{"text": "vector-space modeling", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.7105200588703156}]}, {"text": "As a baseline \u00b0When the phrase data becomes sparse, e.g., after six or seven iterations of processing, it is desirable to reduce the threshold.", "labels": [], "entities": []}, {"text": "for comparison, we used standard CLARIT processing of the same corpus, with the NLP module set to return full NPs and their contained words (and no further subphrase analysis).l 0 The corpus used is a 240-megabyte collection of Associated Press newswire stories from 1989 (AP89), taken from the set of TREC corpora.", "labels": [], "entities": [{"text": "Associated Press newswire stories from 1989 (AP89)", "start_pos": 228, "end_pos": 278, "type": "DATASET", "confidence": 0.8967653181817796}]}, {"text": "There are about 3-million simplex NPs in the corpus and about 1.5-million complex NPs.", "labels": [], "entities": []}, {"text": "For evaluation, we used TREC queries 51-100, ll each of which is a relatively long description of an information need.", "labels": [], "entities": []}, {"text": "Queries were processed by the PES and normal CLARIT NLP modules, respectively, to generate query terms, which were then used for CLARIT retrieval.", "labels": [], "entities": []}, {"text": "To quantify the effects of PES processing, we used the standard IR evaluation measures of recall and precision.", "labels": [], "entities": [{"text": "PES processing", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9463761746883392}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9995225667953491}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9945036172866821}]}, {"text": "Recall measures how many of the relevant documents have been actually retrieved.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.913203239440918}]}, {"text": "Precision measures how many of the retrieved documents are indeed relevant.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.97636878490448}]}, {"text": "For example, if the total number of relevant documents is N and the system returns M documents of which K are relevant, then, We used the judged-relevant documents from the TREC evaluations as the gold standard in scoring the performance of the two processes.", "labels": [], "entities": []}, {"text": "suggests that the PES could be used to support other IR enhancements, such as automatic feedback of the top-returned documents to expand the initial query fora second retrieval step) 2", "labels": [], "entities": [{"text": "PES", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9098889827728271}]}], "tableCaptions": [{"text": " Table 1. While the actual improvement is not sig- nificant for the run of fifty queries, the increase in  absolute numbers of relevant documents returned  indicates that the small compounds supported bet- ter matches in some cases.", "labels": [], "entities": []}, {"text": " Table 2. The general improvement in  precision indicates that small compounds provide  more accurate (and effective) indexing terms than  full NPs.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9994526505470276}]}, {"text": " Table 3. Initial  precision, in particular, improves significantly. This", "labels": [], "entities": [{"text": "Initial", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9432623982429504}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9587061405181885}]}, {"text": " Table 3: Precision at Various Document Levels", "labels": [], "entities": []}]}