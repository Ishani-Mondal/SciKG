{"title": [{"text": "A Polynomial-Time Algorithm for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.8534890214602152}]}], "abstractContent": [{"text": "We introduce a polynomial-time algorithm for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.7319954335689545}]}, {"text": "This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation ar-chitectures.", "labels": [], "entities": []}, {"text": "The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model.", "labels": [], "entities": [{"text": "stochastic bracketing transduction grammar (SBTG)", "start_pos": 25, "end_pos": 74, "type": "TASK", "confidence": 0.7797790340014866}, {"text": "word alignment channel", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.7803422411282858}]}, {"text": "The new algorithm in our experience yields major speed improvement with no significant loss of accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9833568334579468}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9980548620223999}]}, {"text": "1 Motivation The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.6763138175010681}, {"text": "translation", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.9607126712799072}]}, {"text": "Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English.", "labels": [], "entities": [{"text": "translate", "start_pos": 94, "end_pos": 103, "type": "TASK", "confidence": 0.959234893321991}]}, {"text": "The underlying generative model, shown in Figure 1, contains a stochastic English sentence generator whose output is \"corrupted\" by the translation channel to produce Chinese sentences.", "labels": [], "entities": []}, {"text": "In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below.", "labels": [], "entities": []}, {"text": "Estimation of the parameters has been described elsewhere (Brown et al., 1993).", "labels": [], "entities": []}, {"text": "Translation is performed in the reverse direction from generation, as usual for recognition under gen-erative models.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9627999067306519}]}, {"text": "For each Chinese sentence c that is to be translated, the system must attempt to find the English sentence e* such that: (1) e* = argmaxPr(elc) e (2) = argmaxPr(cle) Pr(e) e In the IBM model, the search for the optimal e* is performed using a best-first heuristic \"stack search\" similar to A* methods.", "labels": [], "entities": []}, {"text": "One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* fashion.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.7760407626628876}]}, {"text": "This price is paid for the robustness that is obtained by using very flexible language and translation models.", "labels": [], "entities": []}, {"text": "The language model allows sentences of arbitrary order and the translation model allows arbitrary word-order permutation.", "labels": [], "entities": []}, {"text": "The models employ no structural constraints , relying instead on probability parameters to assign low probabilities to implausible sentences.", "labels": [], "entities": []}, {"text": "This exhaustive space, together with massive number of parameters, permits greater modeling accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9654130339622498}]}, {"text": "But while accuracy is enhanced, translation efficiency suffers due to the lack of structure in the hypothesis space.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99882572889328}, {"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9416940808296204}]}, {"text": "The translation channel is characterized by two sets of parameters: translation and alignment probabilities3 The translation probabilities describe lexical substitution, while alignment probabilities describe word-order permutation.", "labels": [], "entities": []}, {"text": "The key problem is that the formulation of alignment probabilities a(ilj, V, T) permits the Chinese word in position j of a length-T sentence to map to any position i of a length-V English sentence.", "labels": [], "entities": []}, {"text": "So VT alignments are possible, yielding an exponential space with correspondingly slow search times.", "labels": [], "entities": [{"text": "VT alignments", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.8712178766727448}]}, {"text": "Note there are no explicit linguistic grammars in the IBM channel model.", "labels": [], "entities": []}, {"text": "Useful methods do exist for incorporating constraints fed in from other pre-processing modules, and some of these modules do employ linguistic grammars.", "labels": [], "entities": []}, {"text": "For instance, we previously reported a method for improving search times in channel translation models that exploits bracketing information (Wu and Ng, 1995).", "labels": [], "entities": [{"text": "channel translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7134431451559067}, {"text": "bracketing information", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.8801423013210297}]}, {"text": "If any brackets for the Chinese sentence can be supplied as additional input information, produced for example by a preprocessing stage, a modified version of the A*-based algorithm can follow the brackets to guide the search heuristically.", "labels": [], "entities": []}, {"text": "This strategy appears to produces moderate improvements in search speed and slightly better translations.", "labels": [], "entities": []}, {"text": "Such linguistic-preprocessing techniques could 1Various models have been constructed by the IBM team (Brown et al., 1993).", "labels": [], "entities": []}, {"text": "This description corresponds to one of the simplest ones, \"Model 2\"; search costs for the more complex models are correspondingly higher.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}