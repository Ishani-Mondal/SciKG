{"title": [{"text": "Efficiency, Robustness and Accuracy in Picky Chart Parsing*", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9918332099914551}, {"text": "Accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9986490607261658}]}], "abstractContent": [{"text": "This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called p~'ob-abilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input.", "labels": [], "entities": [{"text": "agenda-based chart parsing", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7236347794532776}]}, {"text": "Using a subopti-mal search method, \"Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers.", "labels": [], "entities": [{"text": "CKY-like chart parsing", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.6138759950796763}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9989845156669617}]}, {"text": "Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9988458156585693}]}], "introductionContent": [{"text": "This paper addresses the question: Why should we use probabilistic models in natural language understanding?", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.660571962594986}]}, {"text": "There are many answers to this question, only a few of which are regularly addressed in the literature.", "labels": [], "entities": []}, {"text": "The first and most common answer concerns ambigu~ ity resolution.", "labels": [], "entities": [{"text": "ambigu~ ity resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6634032130241394}]}, {"text": "A probabilistic model provides a clearly defined preference nile for selecting among grammatical alternatives (i.e. the highest probability interpretation is selected).", "labels": [], "entities": []}, {"text": "However, this use of probabilistic models assumes that we already have efficient methods for generating the alternatives in the first place.", "labels": [], "entities": []}, {"text": "While we have O(n 3) algorithms for determining the grammaticality of a sentence, parsing, as a component of a natural language understanding tool, involves more than simply determining all of the grammatical interpretations of an input.", "labels": [], "entities": [{"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.9655144810676575}]}, {"text": "Ill order fora natural language system to process input efficiently and robustly, it must process all intelligible sentences, grammatical or not, while not significantly reducing the system's efficiency.", "labels": [], "entities": []}, {"text": "This observ~ttiou suggests two other answers to the central question of this paper.", "labels": [], "entities": []}, {"text": "Probabilistic models offer a convenient scoring method for partial interpretations in a well-formed substring table.", "labels": [], "entities": []}, {"text": "High probability constituents in the parser's chart call be used to interpret ungrammat.ical sentences.", "labels": [], "entities": []}, {"text": "Probabilistic models can also *Special I.hanks to Jerry Hobbs and F3ob Moo*re at S[II for providing access to their colllptllel's, and to Salim ]-/oukos, Pel:er Brown, and Vincent and Steven Della Piel.ra ,-xt IF3M for their inst.ructive lessons on probabi|isti,: modelling of natural I:mguage.", "labels": [], "entities": []}, {"text": "be used for efficiency by providing a best-first search heuristic to order the parsing agenda.", "labels": [], "entities": [{"text": "parsing agenda", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.8863864541053772}]}, {"text": "This paper proposes an agenda-based probabilistic chart parsing algorithm which is both robust and efficient.", "labels": [], "entities": [{"text": "agenda-based probabilistic chart parsing", "start_pos": 23, "end_pos": 63, "type": "TASK", "confidence": 0.5694159045815468}]}, {"text": "The algorithm, 7)icky 1, is considered robust because it will potentially generate all constituents produced by a pure bottom-up parser and rank these constituents by likelihood.", "labels": [], "entities": []}, {"text": "The efficiency of the algorithm is achieved through a technique called probabilistic prediction, which helps the algorithm avoid worst-case behavior.", "labels": [], "entities": [{"text": "probabilistic prediction", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.7453822195529938}]}, {"text": "Probabilistic prediction is a trainable technique for modelling where edges are likely to occur in the chart-parsing process.", "labels": [], "entities": [{"text": "Probabilistic prediction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8014629781246185}]}, {"text": "2 Once the predicted edges are added to the chart using probabilistic prediction, they are processed in a style similar to agenda-based chart parsing algorithms.", "labels": [], "entities": [{"text": "agenda-based chart parsing", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.7065683205922445}]}, {"text": "By limiting the edges in the chart to those which are predicted by this model, the parser can process a sentence while generating only the most likely constituents given the input.", "labels": [], "entities": []}, {"text": "In this paper, we will present the \"Picky parsing algorithm, describing both the original features of the parser and those adapted from previous work.", "labels": [], "entities": []}, {"text": "Then, we will compare the implementation of`pickyof`picky with existing probabilistic and non-probabilistic parsers.", "labels": [], "entities": []}, {"text": "Finally, we will report the results of experiments exploring how`picky how`picky's algorithm copes with the tradeoffs of efficiency, robustness, and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9970791339874268}]}], "datasetContent": [{"text": "The Picky parser was tested on 3 sets of 100 sentences which were held out from the rest of the corpus during training.", "labels": [], "entities": []}, {"text": "The training corpus consisted of 982 sentences which were parsed using the same grammar that Picky used.", "labels": [], "entities": []}, {"text": "The training and test corpora are samples from the MIT's Voyager direction-finding system.", "labels": [], "entities": []}, {"text": "7 Using Picky's grammar, these test sentences generate, on average, over 100 parses per sentence, with some sentences generated over 1,000 parses.", "labels": [], "entities": []}, {"text": "The purpose of these experiments is to explore the impact of varying of Picky's parsing algorithm on parsing accuracy, efficiency, and robustness.", "labels": [], "entities": [{"text": "parsing", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9010897874832153}, {"text": "parsing", "start_pos": 101, "end_pos": 108, "type": "TASK", "confidence": 0.9670741558074951}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9466063976287842}]}, {"text": "For these experiments, we varied three attributes of the parser: the phases used by parser, the maximum number of edges the parser can produce before failure, and the minimum probability parse acceptable.", "labels": [], "entities": []}, {"text": "In the following analysis, the accuracy rate represents the percentage of the test sentences for which the highest probability parse generated by the parser is identical to the \"correct\" pa.rse tree indicated in the parsed test corpus, s Efficiency is measured by two ratios, the prediction ratio and the completion ratio.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9915819764137268}, {"text": "prediction ratio", "start_pos": 280, "end_pos": 296, "type": "METRIC", "confidence": 0.9512622654438019}, {"text": "completion ratio", "start_pos": 305, "end_pos": 321, "type": "METRIC", "confidence": 0.9796918630599976}]}, {"text": "The prediction ratio is defined as the ratio of number of predictions made by the parser 7Special thanks to Victor Zue at MIT for the use of the speech data from MIT's Voyager system.", "labels": [], "entities": []}, {"text": "8There are two exceptions to this accuracy measure.", "labels": [], "entities": [{"text": "accuracy measure", "start_pos": 34, "end_pos": 50, "type": "METRIC", "confidence": 0.9801802039146423}]}, {"text": "If tile parser generates a plausible parse fora sentences which has multipie plausible int.erpretations, the parse is considered cc~rrcct.", "labels": [], "entities": []}, {"text": "if the parser generates a correct; pal'se~ I)ll~ the parsecl test corpus contains an incorrect parse (i.e. if there is an error in the answer key), the parse is considered col-rect.", "labels": [], "entities": []}, {"text": "during the parse of a sentence to the number of constituents necessary fora correct parse.", "labels": [], "entities": []}, {"text": "The completion ratio is the ratio of the number of completed edges to the number of predictions during the parse of sentence.", "labels": [], "entities": [{"text": "completion ratio", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9719372987747192}]}, {"text": "Robustness cannot be measured directly by these experiments, since there are few ungrammatical sentences and there is no implemented method for interpreting the well-formed substring table when a parse fails.", "labels": [], "entities": []}, {"text": "However, for each configuration of the parser, we will explore the expected behavior of the parser in the face of ungrammatical input.", "labels": [], "entities": []}, {"text": "Since Picky has the power of a pure bottom-up parser, it would be useful to compare its performance and efficiency to that of a probabilistic bottom-up parser.", "labels": [], "entities": []}, {"text": "However, an implementation of a probabilistic bottom-up parser using the same grammar produces on average over 1000 constituents for each sentence, generating over 15,000 edges without generating a parse at all!", "labels": [], "entities": []}, {"text": "This supports our claim that exhaustive CKY-like parsing algorithms are not feasible when probabilistic models are applied to them.", "labels": [], "entities": [{"text": "CKY-like parsing", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.5774245858192444}]}], "tableCaptions": [{"text": " Table 1: Prediction and Completion Ratios and accuracy  statistics for Picky configured with different subsets of  Picky's three phases.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.862459659576416}, {"text": "Completion Ratios", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.8729042112827301}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9986826777458191}]}, {"text": " Table 2: 7~icky's parsing accuracy, categorized by the  phase which the parser reached in processing the test  sentences.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.8583251237869263}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9356560111045837}]}]}