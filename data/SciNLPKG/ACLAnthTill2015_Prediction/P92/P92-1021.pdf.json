{"title": [], "abstractContent": [{"text": "I argue that because of spelling and typing errors and other properties of typed text, the identification of words and word boundaries in general requires syntactic and semantic knowledge.", "labels": [], "entities": [{"text": "identification of words and word boundaries", "start_pos": 91, "end_pos": 134, "type": "TASK", "confidence": 0.840859572092692}]}, {"text": "A lattice representation is therefore appropriate for lexical analysis.", "labels": [], "entities": [{"text": "lexical analysis", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7188067436218262}]}, {"text": "I show how the use of such a representation in the CLARE system allows different kinds of hypothesis about word identity to be integrated in a uniform framework.", "labels": [], "entities": []}, {"text": "I then describe a quantitative evaluation of CLARE's performance on a set of sentences into which typographic errors have been introduced.", "labels": [], "entities": []}, {"text": "The results show that syntax and semantics can be applied as powerful sources of constraint on the possible corrections for misspelled words.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many language processing systems, uncertainty in the boundaries of linguistic units, at various levels, means that data are represented not as a well-defined sequence of units but as a lattice of possibilities.", "labels": [], "entities": []}, {"text": "It is common for speech recognizers to maintain a lattice of overlapping word hypotheses from which one or more plausible complete paths are subsequently selected.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.6968804746866226}]}, {"text": "Syntactic parsing, of either spoken or written language, frequently makes use of a chart or well-formed substring table because the correct bracketing of a sentence cannot (easily) be calculated deterministically.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.870631068944931}]}, {"text": "And lattices are also often used in the task of converting Japanese text typed in kana (syllabic symbols) to kanji; the lack of interword spacing in written Japanese and the complex morphology of the language mean that lexical items and their boundaries cannot be reliably identified without applying syntactic and semantic knowledge).", "labels": [], "entities": [{"text": "converting Japanese text typed in kana (syllabic symbols)", "start_pos": 48, "end_pos": 105, "type": "TASK", "confidence": 0.7542369604110718}]}, {"text": "In contrast, however, it is often assumed that, for languages written with interword spaces, it is sufficient to group an input character stream deterministically into a sequence of words, punctuation symbols and perhaps other items, and to hand this sequence to the parser, possibly after word-by-word morphological analysis.", "labels": [], "entities": []}, {"text": "Such an approach is sometimes adopted even when typographically complex inputs are handled; see, for example, In this paper I observe that, for typed input, spaces do not necessarily correspond to boundaries between lexical items, both for linguistic reasons and because of the possibility of typographic errors.", "labels": [], "entities": []}, {"text": "This means that a lattice representation, not a simple sequence, should be used throughout front end (preparsing) analysis.", "labels": [], "entities": []}, {"text": "The CLARE system underdevelopment at SRI Cambridge uses such a representation, allowing it to deal straightforwardly with combinations or multiple occurrences of phenomena that would be difficult or impossible to process correctly under a sequence representation.", "labels": [], "entities": [{"text": "SRI Cambridge", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.7864183187484741}]}, {"text": "As evidence for the performance of the approach taken, I describe an evaluation of CLARE's ability to deal with typing and spelling errors.", "labels": [], "entities": []}, {"text": "Such errors are especially common in interactive use, for which CLARE is designed, and the correction of as many of them as possible can make an appreciable difference to the usability of a system.", "labels": [], "entities": []}, {"text": "The word identity and word boundary ambiguities encountered in the interpretation of errorful input often require the application of syntactic and semantic knowledge on a phrasal or even sentential scale.", "labels": [], "entities": []}, {"text": "Such knowledge maybe applied as soon as the problem is encountered; however, this brings major problems with it, such as the need for adequate lookahead, and the difficulties of engineering large systems where the processing levels are tightly coupled.", "labels": [], "entities": []}, {"text": "To avoid these difficulties, CLARE adopts a staged architecture, in which indeterminacy is preserved until the knowledge needed to resolve it is ready to be applied.", "labels": [], "entities": [{"text": "CLARE", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.8370344042778015}]}, {"text": "An appropriate representation is of course the key to doing this efficiently.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the usefulness of syntacticosemantic constraints in CLARE's spelling correction, the following experiment, intended to simulate performance (typographic) errors, was carried out.", "labels": [], "entities": [{"text": "CLARE's spelling correction", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.6626274734735489}]}, {"text": "Five hundred sentences, of up to ten words in length, falling within CLARE's current core lexical (1600 root forms) and grammatical coverage were taken at random from the LOB corpus.", "labels": [], "entities": [{"text": "LOB corpus", "start_pos": 171, "end_pos": 181, "type": "DATASET", "confidence": 0.9449769854545593}]}, {"text": "These sentences were passed, character by character, through a channel which transmitted a character without alteration with probability 0.99, and with probability 0.01 introduced a simple error.", "labels": [], "entities": []}, {"text": "The relative probabilities of the four different kinds of error were deduced from of; where anew character had to be inserted or substituted, it was selected at random from the original sentence set.", "labels": [], "entities": []}, {"text": "This process produced a total of 102 sentences that differed from their originals.", "labels": [], "entities": []}, {"text": "The average length was 6.46 words, and there were 123 corrupted tokens in all, some containing more than one simple error.", "labels": [], "entities": []}, {"text": "Because longer sentences were more likely to be changed, the average length of a changed sentence was some 15% more than that of an original one.", "labels": [], "entities": []}, {"text": "The corrupted sentence set was then processed by CLARE with only the spelling correction recovery method in force and with no user intervention.", "labels": [], "entities": [{"text": "CLARE", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.899402379989624}, {"text": "spelling correction recovery", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.792380024989446}]}, {"text": "Up to two simple errors were considered per token.", "labels": [], "entities": []}, {"text": "No domainspecific or context-dependent knowledge was used.", "labels": [], "entities": []}, {"text": "Of the 123 corrupted tokens, ten were corrupted into other known words, and so no correction was attempted.", "labels": [], "entities": []}, {"text": "Parsing failed in nine of these cases; in the tenth, the corrupted word made as much sense as the original out of discourse context.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9566353559494019}]}, {"text": "In three further cases, the original token was not suggested as a correction; one was a special form, and for the other two, alternative corrections involved fewer simple errors.", "labels": [], "entities": []}, {"text": "The corrections for two other tokens were not used because a corruption into a known word elsewhere in the same sentence caused parsing to fail.", "labels": [], "entities": []}, {"text": "Only one correction (the right one) was suggested for 59 of the remaining 108 tokens.", "labels": [], "entities": []}, {"text": "Multiple-token correction, involving the manipulation of space characters, took place in 24 of these cases.", "labels": [], "entities": [{"text": "Multiple-token correction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7619116902351379}]}, {"text": "This left 49 tokens for which more than one correction was suggested, requiring syntactic and semantic processing for further disambiguation.", "labels": [], "entities": []}, {"text": "The average number of corrections suggested for these 49 was 4.57.", "labels": [], "entities": [{"text": "corrections", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.887339174747467}]}, {"text": "However, only an average of 1.69 candidates (including, because of the way the corpus was selected, all the right ones) appeared in QLFs satisfying selectional restrictions; thus only 19% of the wrong candidates found their way into any QLF.", "labels": [], "entities": []}, {"text": "If, in the absence of frequency information, we take all candidates as equally likely, then syntactic and semantic processing reduced the average entropy from 1.92 to 0.54, removing 72% of the uncertainty (see, fora discussion of why entropy is the best measure to use in contexts like this).", "labels": [], "entities": []}, {"text": "When many QLFs are produced fora sentence, CLARE orders them according to a set of scoring functions encoding syntactic and semantic preferences.", "labels": [], "entities": []}, {"text": "For the 49 multiplecandidate tokens, removing all but the bestscoring QLF(s) eliminated 7 (21%) of the 34 wrong candidates surviving to the QLF stage; however, it also eliminated 5 (10~) of the right candidates.", "labels": [], "entities": [{"text": "QLF", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.7826226949691772}]}, {"text": "It is expected that future development of the scoring functions will further improve these figures, which are summarized in backtracking, left-corner algorithm and stores well-formed constituents so as to avoid repeating work where possible.", "labels": [], "entities": []}, {"text": "In general, when a problem token appears late in the sentence and/or when several candidate corrections axe syntactically plausible, the lattice approach is several times faster than processing the alternative strings separately (which tends to be very time-consuming).", "labels": [], "entities": []}, {"text": "When the problem token occurs early and has only one plausible correction, the two methods are about the same speed.", "labels": [], "entities": []}, {"text": "For example, in one case, a corrupted token with 13 candidate corrections occurred in sixth position in an eight-word sentence.", "labels": [], "entities": []}, {"text": "Parsing the resulting lattice was three times faster than parsing each alternative full string separately.", "labels": [], "entities": []}, {"text": "The lattice representation avoided repetition of work on the first six words, tIowever, in another case, where the corrupted token occurred second in an eight-word sentence, and had six candidates, only one of which was syntactically plausible, the lattice representation was no faster, as the incorrect candidates in five of the strings led to the parse being abandoned early.", "labels": [], "entities": []}, {"text": "An analogous experiment was carried outwith 500 sentences from the same corpus which CLARE could not parse.", "labels": [], "entities": [{"text": "CLARE", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.8706421852111816}]}, {"text": "131 of the sentences, with average length 7.39 words, suffered the introduction of errors.", "labels": [], "entities": []}, {"text": "Of these, only seven (5%) received a parse.", "labels": [], "entities": [{"text": "parse", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.901130735874176}]}, {"text": "Four of the seven received no sortally valid QLFs, leaving only three (2%) \"false positives\".", "labels": [], "entities": [{"text": "QLFs", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.7333559393882751}]}, {"text": "This low figure is consistent with the results from the originaJly parseable sentence set; nine out of the ten corruptions into known words in that experiment led to parse failure, and only 19% of wrong suggested candidates led to a sortallyvalid QLF.", "labels": [], "entities": [{"text": "originaJly parseable sentence set", "start_pos": 56, "end_pos": 89, "type": "DATASET", "confidence": 0.5461326986551285}, {"text": "QLF", "start_pos": 247, "end_pos": 250, "type": "DATASET", "confidence": 0.7400420904159546}]}, {"text": "If, as those figures suggest, the replacement of one word by another only rarely maps one sentence inside coverage to another, then a corresponding replacement on a sentence outside coverage should yield something within coverage even more rarely, and this does appear to be the case.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correction candidates for the 49  multiple-candidate tokens", "labels": [], "entities": []}]}