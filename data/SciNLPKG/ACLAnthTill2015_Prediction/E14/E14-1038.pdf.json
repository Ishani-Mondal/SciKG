{"title": [], "abstractContent": [{"text": "Verb errors are some of the most common mistakes made by non-native writers of English but some of the least studied.", "labels": [], "entities": [{"text": "Verb errors", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8008047938346863}]}, {"text": "The reason is that dealing with verb errors requires anew paradigm; essentially all research done on correcting grammatical errors assumes a closed set of triggers -e.g., correcting the use of prepositions or articles-but identifying mistakes in verbs necessitates identifying potentially ambiguous triggers first, and then determining the type of mistake made and correcting it.", "labels": [], "entities": [{"text": "correcting the use of prepositions or articles-but identifying mistakes in verbs", "start_pos": 171, "end_pos": 251, "type": "TASK", "confidence": 0.8225690776651556}]}, {"text": "Moreover, once the verb is identified, modeling verb errors is challenging because verbs fulfill many grammatical functions, resulting in a variety of mistakes.", "labels": [], "entities": []}, {"text": "Consequently, the little earlier work done on verb errors assumed that the error type is known in advance.", "labels": [], "entities": []}, {"text": "We propose a linguistically-motivated approach to verb error correction that makes use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes.", "labels": [], "entities": [{"text": "verb error correction", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.6429847975571951}]}, {"text": "We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9995167255401611}, {"text": "verb correction", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.738421618938446}]}], "introductionContent": [{"text": "We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners.", "labels": [], "entities": [{"text": "correcting grammatical verb mistakes made by English as a Second Language (ESL) learners", "start_pos": 26, "end_pos": 114, "type": "TASK", "confidence": 0.7996047019958497}]}, {"text": "Recent work in ESL error correction has focused on errors in article and preposition usage.", "labels": [], "entities": [{"text": "ESL error correction", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9009069800376892}]}, {"text": "While verb errors occur as often as article and preposition mistakes, with a few exceptions (, there has been little work on verbs.", "labels": [], "entities": []}, {"text": "There are two reasons for why it is difficult to deal with verb mistakes.", "labels": [], "entities": []}, {"text": "First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data).", "labels": [], "entities": []}, {"text": "Second, verbs are more complex linguistically: they fulfill several grammatical functions, and these different roles imply different types of errors.", "labels": [], "entities": []}, {"text": "These difficulties have led all previous work on verb mistakes to assume prior knowledge of the mistake type; however, identifying the specific category of a verb error is nontrivial, since the surface form of the verb maybe ambiguous, especially when that verb is used incorrectly.", "labels": [], "entities": []}, {"text": "Consider the following examples of verb mistakes: These examples illustrate three grammatical verb properties: Agreement, Tense, and non-finite Form choice that encompass the most common grammatical verb problems for ESL learners.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9038146138191223}, {"text": "ESL learners", "start_pos": 217, "end_pos": 229, "type": "TASK", "confidence": 0.8139805197715759}]}, {"text": "The first two examples show mistakes on verbs that function as main verbs in a clause: sentence shows an example of subject-verb Agreement error; is an example of a Tense mistake where the ambiguity is between {will find} (Future tense) and find (Present tense).", "labels": [], "entities": [{"text": "Tense mistake", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.8938161432743073}]}, {"text": "Examples (3) and (4) display Form mistakes: confusing the infinitive and gerund forms in (3) and including an inflection on an infinitive verb in.", "labels": [], "entities": []}, {"text": "This paper addresses the specific challenges of verb error correction that have not been addressed previously -identifying candidates for mistakes and determining which class of errors is present, before proceeding to correct the error.", "labels": [], "entities": [{"text": "verb error correction", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7016138633092245}]}, {"text": "The experimental results show that our linguisticallymotivated approach benefits verb error correction.", "labels": [], "entities": [{"text": "verb error correction", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.6925382415453593}]}, {"text": "In particular, in order to determine the error type, we build on the notion of verb finiteness to distinguish between finite and non-finite verbs, that correspond to Agreement and Tense mistakes (examples (1) and (2) above) and Form mistakes (examples (3) and (4) above), respectively (see Sec. 3).", "labels": [], "entities": []}, {"text": "The approach presented in this work was evaluated empirically and competitively in the context of the CoNLL shared task on error correction ( where it was implemented as part of the highest-scoring University of Illinois system ( ) and demonstrated superior performance on the verb error correction sub-task.", "labels": [], "entities": [{"text": "CoNLL shared task on error correction", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.5652547776699066}, {"text": "verb error correction", "start_pos": 277, "end_pos": 298, "type": "TASK", "confidence": 0.6491690377394358}]}, {"text": "This paper makes the following contributions: \u2022 We present a holistic, linguistically-motivated framework for correcting grammatical verb mistakes; our approach \"starts from scratch\" without any knowledge of which mistakes should be corrected or of the mistake type; in doing that we show that the specific challenges of verb error correction are better addressed by first identifying the finiteness of the verb in the error identification stage.", "labels": [], "entities": [{"text": "correcting grammatical verb mistakes", "start_pos": 110, "end_pos": 146, "type": "TASK", "confidence": 0.8386992663145065}, {"text": "verb error correction", "start_pos": 321, "end_pos": 342, "type": "TASK", "confidence": 0.6431819895903269}, {"text": "error identification stage", "start_pos": 419, "end_pos": 445, "type": "TASK", "confidence": 0.8017913500467936}]}, {"text": "\u2022 Within the proposed model, we describe and evaluate several methods of selecting verb candidates, an algorithm for determining the verb type, and a type-driven verb error correction system.", "labels": [], "entities": []}, {"text": "\u2022 We annotate a subset of the FCE data set with gold verb candidates and gold verb type.", "labels": [], "entities": [{"text": "FCE data set", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9621997872988383}]}], "datasetContent": [{"text": "The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of the problem.", "labels": [], "entities": [{"text": "correcting verb mistakes", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.871445099512736}]}, {"text": "We thus do not focus on features or on the specific learning algorithm.", "labels": [], "entities": []}, {"text": "Our experimental study addresses the following research questions: I.", "labels": [], "entities": []}, {"text": "Linguistic questions: (i) candidate selection methods; (ii) verb finiteness contribution to error identification II.", "labels": [], "entities": [{"text": "candidate selection", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8329911828041077}, {"text": "error identification", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.6884398460388184}]}, {"text": "Computational Framework: error identification vs. correction III.", "labels": [], "entities": [{"text": "error identification", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.6524623930454254}]}, {"text": "Gold annotation: (i) using gold candidates and verb type vs. automatic; (ii) performance comparison by error type Learning Framework There is a lot of understanding for which algorithmic methods work best for ESL correction tasks, how they compare among themselves, and how they compare to ngram based methods.", "labels": [], "entities": [{"text": "ESL correction tasks", "start_pos": 209, "end_pos": 229, "type": "TASK", "confidence": 0.9218864639600118}]}, {"text": "Specifically, despite their intuitive appeal, language models were shown to notwork well on these tasks, while the discriminative learning framework has been shown to be superior to other approaches and thus is commonly used for error correction tasks (see Sec. 2).", "labels": [], "entities": [{"text": "error correction tasks", "start_pos": 229, "end_pos": 251, "type": "TASK", "confidence": 0.7793125510215759}]}, {"text": "Since we do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) fora discussion of these issues.", "labels": [], "entities": []}, {"text": "We train all our models with the SVM learning algorithm implemented in JLIS (.", "labels": [], "entities": [{"text": "JLIS", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.9096150398254395}]}, {"text": "Evaluation We report both Precision/Recall curves and AAUC (as a summary).", "labels": [], "entities": [{"text": "Precision/Recall curves", "start_pos": 26, "end_pos": 49, "type": "METRIC", "confidence": 0.8270652592182159}, {"text": "AAUC", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9975994229316711}]}, {"text": "Error correction is generally evaluated using F1 (); Precision and Recall; or Average Area Under Curve (AAUC) ().", "labels": [], "entities": [{"text": "Error correction", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8023384213447571}, {"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9997175335884094}, {"text": "Precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9983695149421692}, {"text": "Recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9577268958091736}, {"text": "Average Area Under Curve (AAUC)", "start_pos": 78, "end_pos": 109, "type": "METRIC", "confidence": 0.9522353070122855}]}, {"text": "For a discussion on these metrics with respect to error correction tasks, we refer the reader to.", "labels": [], "entities": [{"text": "error correction tasks", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7585360904534658}]}, {"text": "AAUC () is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points.", "labels": [], "entities": [{"text": "AAUC", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9104540944099426}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9659367203712463}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9883577227592468}]}, {"text": "In this paper, AAUC is computed over the first 15 recall points: P recision(i).", "labels": [], "entities": [{"text": "AAUC", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9985411167144775}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9981046915054321}, {"text": "P recision", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.8834023475646973}]}], "tableCaptions": [{"text": " Table 1: Grammatical verb errors in FCE.", "labels": [], "entities": []}, {"text": " Table 3: Inter-annotator agreement based on 250 verb", "labels": [], "entities": []}, {"text": " Table 5: Candidate selection methods performance.", "labels": [], "entities": []}, {"text": " Table 6: Algorithm for determining verb type. numTokens denotes the number of tokens in the verb instance, e.g., for the", "labels": [], "entities": []}, {"text": " Table 8: Impact of candidate selection methods on error", "labels": [], "entities": []}, {"text": " Table 9: Training and test data statistics. Candidates are", "labels": [], "entities": []}, {"text": " Table 10. Precision/Recall curves are  generated by varying the threshold on the confi- dence of the classifier. This graph reveals the be- havior of the systems at multiple recall points: we  observe that at every recall point the type-based  classifier has higher precision.", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9881369471549988}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.6290989518165588}, {"text": "precision", "start_pos": 267, "end_pos": 276, "type": "METRIC", "confidence": 0.9975161552429199}]}, {"text": " Table 10: Verb finiteness contribution to error identifi-", "labels": [], "entities": []}, {"text": " Table 10. The combined  model uses no verb type information. In the hard-decision  type-based model, each verb uses the features according to  its finiteness. The differences are statistically significant (Mc- Nemar's test, p < 0.0001).", "labels": [], "entities": []}, {"text": " Table 11: Verb finiteness contribution to error identifi-", "labels": [], "entities": []}, {"text": " Table 12: Performance of the complete model after the", "labels": [], "entities": []}, {"text": " Table 13: Gold subset: error identification with gold vs.", "labels": [], "entities": [{"text": "error identification", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7883394658565521}]}, {"text": " Table 14: Gold subset: gold vs. automatic finiteness con-", "labels": [], "entities": []}]}