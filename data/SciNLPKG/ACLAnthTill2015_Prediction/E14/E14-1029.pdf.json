{"title": [{"text": "Iterative Constrained Clustering for Subjectivity Word Sense Disambiguation", "labels": [], "entities": [{"text": "Subjectivity Word Sense Disambiguation", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.7329519167542458}]}], "abstractContent": [{"text": "Subjectivity word sense disambiguation (SWSD) is a supervised and application-specific word sense disambiguation task disambiguating between subjective and objective senses of a word.", "labels": [], "entities": [{"text": "Subjectivity word sense disambiguation (SWSD)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8374717491013663}, {"text": "application-specific word sense disambiguation task disambiguating between subjective and objective senses of a word", "start_pos": 66, "end_pos": 182, "type": "TASK", "confidence": 0.6995370175157275}]}, {"text": "Not surprisingly , SWSD suffers from the knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.8400780558586121}, {"text": "knowledge acquisition", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.791749894618988}]}, {"text": "In this work, we use a \"cluster and label\" strategy to generate labeled data for SWSD semi-automatically.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 81, "end_pos": 85, "type": "TASK", "confidence": 0.9393937587738037}]}, {"text": "We define anew algorithm called Iterative Constrained Clustering (ICC) to improve the clustering purity and, as a result, the quality of the generated data.", "labels": [], "entities": []}, {"text": "Our experiments show that the SWSD classifiers trained on the ICC generated data by requiring only 59% of the labels can achieve the same performance as the classifiers trained on the full dataset.", "labels": [], "entities": [{"text": "SWSD classifiers", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.6434886753559113}, {"text": "ICC generated data", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.6867326498031616}]}], "introductionContent": [{"text": "Subjectivity lexicons (e.g.,;) play an important role in opinion, sentiment, and subjectivity analysis.", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.6866576820611954}]}, {"text": "These systems typically look for the presence of clues in text.", "labels": [], "entities": []}, {"text": "Recently, in, we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not -words in such lexicons may have both subjective and objective usages.", "labels": [], "entities": []}, {"text": "We call this problem subjectivity sense ambiguity.", "labels": [], "entities": []}, {"text": "Consider the following sentence containing the clue \"attack\": (1) He was attacked by Milosevic for attempting to carve out anew party from the Socialists.", "labels": [], "entities": []}, {"text": "Knowing that \"attack\" is a subjectivity clue with negative polarity will help a system recognize the negative sentiment in the sentence.", "labels": [], "entities": []}, {"text": "But for (2), the same information is simply misleading, because the clue is used with an objective meaning.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate context representations for the context clustering task on the subjectivity sense tagged data, senSWSD.", "labels": [], "entities": [{"text": "context clustering task", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7725344399611155}]}, {"text": "The evaluation is done separately for each word.", "labels": [], "entities": []}, {"text": "We use the same clustering algorithm for all context representations: agglomerative hierarchical clustering with average linkage criteria.", "labels": [], "entities": []}, {"text": "In all our experiments throughout the paper, we fix the cluster size to 7 as it is done in ().", "labels": [], "entities": []}, {"text": "We think that is reasonable number since SENSEVAL III reports that the average number of senses per word is 6.47.", "labels": [], "entities": []}, {"text": "We choose cluster purity as our evaluation metric.", "labels": [], "entities": [{"text": "cluster purity", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.6364222764968872}]}, {"text": "To compute cluster purity, we assign each cluster to a sense label, which is the most frequent one in the cluster.", "labels": [], "entities": []}, {"text": "The number of the correctly assigned instances divided by the number of all the clustered instances gives us cluster purity.", "labels": [], "entities": [{"text": "cluster purity", "start_pos": 109, "end_pos": 123, "type": "METRIC", "confidence": 0.8330181837081909}]}, {"text": "Row 1 of holds the cumulative results overall the words in senSWSD (micro averages).", "labels": [], "entities": []}, {"text": "The table also reports detailed results for 4 sample selected words from senSWSD.", "labels": [], "entities": [{"text": "senSWSD", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.929347038269043}]}, {"text": "skew stands for the percentage of the most frequent label.", "labels": [], "entities": []}, {"text": "dsm add is the representation based on, dsm mul stands for the representation as described in () and local features is the local feature representation based on ().", "labels": [], "entities": []}, {"text": "The results show that among dsm mul, dsm add, and local features; dsm mul performs the best.", "labels": [], "entities": []}, {"text": "When we look at the context clustering results for single words separately, we observe that the performance of different representations vary.", "labels": [], "entities": []}, {"text": "There is not a single winner among all words.", "labels": [], "entities": []}, {"text": "Thus, perhaps choosing one single representation for all the words is not optimal.", "labels": [], "entities": []}, {"text": "Having that in mind, we try merging the dsm mul and local features representations.", "labels": [], "entities": []}, {"text": "We leave out dsm add representation, since both dsm mul and dsm add rely on the same type of semantic information (i.e., a DSM).", "labels": [], "entities": []}, {"text": "We hypothesize that the two representations, one relying on a semantic space and the other relying on local WSD features, may complement each other.", "labels": [], "entities": []}, {"text": "To merge the representations, we concatenate the two feature vectors into one.", "labels": [], "entities": []}, {"text": "First, however, we normalize each vector to unit length, since the individual vectors have different scales and would have unequal contribution, otherwise.", "labels": [], "entities": []}, {"text": "We call this mixed representation mix rep.", "labels": [], "entities": []}, {"text": "In, we see that, overall, mix rep performs better than all the other representations.", "labels": [], "entities": []}, {"text": "The improvement is statistically significant at the p < .05 level on a paired t-test.", "labels": [], "entities": []}, {"text": "We observe that, even when mix rep does not perform the best, it is never bad.", "labels": [], "entities": []}, {"text": "mix rep is the winner or ties for the winner for 25 out of 39 words.", "labels": [], "entities": []}, {"text": "This number is 13, 13, and 15 for dsm add, dsm mul and local features, respectively.", "labels": [], "entities": []}, {"text": "For the words for which mix rep is not the winner, it is, on average, 1.47 points lower than the winner.", "labels": [], "entities": []}, {"text": "This number is 4.22, 6.83, and 7.07 for the others.", "labels": [], "entities": []}, {"text": "The results provide evidence that mix rep is consistently good and reliable.", "labels": [], "entities": [{"text": "mix rep", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.6962123513221741}]}, {"text": "Thus, in our experiments, mix rep will be our choice as the context representation.", "labels": [], "entities": []}, {"text": "This section gives details on experiments to evaluate the purity of the semi-automatically generated subjectivity sense tagged data by our \"cluster and label\" strategy.", "labels": [], "entities": []}, {"text": "We carryout detailed analysis to quantify the effect of the proposed active selection strategy and of metric learning on the purity of the generated data.", "labels": [], "entities": []}, {"text": "We compare our active selection strategy to random selection and also to ().", "labels": [], "entities": []}, {"text": "The comparison is done on the senSWSD dataset.", "labels": [], "entities": [{"text": "senSWSD dataset", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.977281928062439}]}, {"text": "SenSWSD consists of three subsets, SENSEVAL I,II and III.", "labels": [], "entities": [{"text": "SENSEVAL I,II", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.5398714393377304}]}, {"text": "Since we devel- oped our active selection algorithm on the SEN-SEVAL I subset, we use only SENSEVAL II and III subsets for comparison.", "labels": [], "entities": []}, {"text": "We apply ICC to each word in the comparison set separately, and report cumulative results for the purity of the generated data.", "labels": [], "entities": [{"text": "ICC", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9977028965950012}, {"text": "purity", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9874617457389832}]}, {"text": "We report results for different percentages of the queried data amount (e.g. 10% means that the algorithm queried 10% of the data to create constraints).", "labels": [], "entities": []}, {"text": "This way, we obtain a learning curve.", "labels": [], "entities": []}, {"text": "We fix the cluster number to 7 as in the context representation experiments.", "labels": [], "entities": []}, {"text": "holds the comparison of ICC with silh const selection to a random selection baseline.", "labels": [], "entities": []}, {"text": "\"majority\" stands for majority label frequency in the dateset.", "labels": [], "entities": []}, {"text": "We see that silh const performs better than the random selection.", "labels": [], "entities": []}, {"text": "By providing labels to only 25% of the data, we can achieve 87.67% pure fully labeled data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of Various Context Representations", "labels": [], "entities": [{"text": "Evaluation of Various Context Representations", "start_pos": 10, "end_pos": 55, "type": "TASK", "confidence": 0.6571260452270508}]}]}