{"title": [{"text": "Encoding Semantic Resources in Syntactic Structures for Passage Reranking", "labels": [], "entities": [{"text": "Passage Reranking", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.956398218870163}]}], "abstractContent": [{"text": "In this paper, we propose to use semantic knowledge from Wikipedia and large-scale structured knowledge datasets available as Linked Open Data (LOD) for the answer passage reranking task.", "labels": [], "entities": [{"text": "answer passage reranking", "start_pos": 157, "end_pos": 181, "type": "TASK", "confidence": 0.8768309950828552}]}, {"text": "We represent question and candidate answer passages with pairs of shallow syntac-tic/semantic trees, whose constituents are connected using LOD.", "labels": [], "entities": []}, {"text": "The trees are processed by SVMs and tree kernels, which can automatically exploit tree fragments.", "labels": [], "entities": []}, {"text": "The experiments with our SVM rank algorithm on the TREC Question Answering (QA) corpus show that the added relational information highly improves over the state of the art, e.g., about 15.4% of relative improvement in P@1.", "labels": [], "entities": [{"text": "TREC Question Answering (QA) corpus", "start_pos": 51, "end_pos": 86, "type": "DATASET", "confidence": 0.7993935942649841}]}], "introductionContent": [{"text": "Past work in TREC QA, e.g., and more recent work) in QA has shown that, to achieve human performance, semantic resources, e.g., Wikipedia 1 , must be utilized by QA systems.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.6677469909191132}]}, {"text": "This requires the design of rules or machine learning features that exploit such knowledge by also satisfying syntactic constraints, e.g., the semantic type of the answer must match the question focus words. and carryout logic unification and resolution.", "labels": [], "entities": [{"text": "logic unification", "start_pos": 221, "end_pos": 238, "type": "TASK", "confidence": 0.7253230512142181}]}, {"text": "Therefore, approaches that can automatically generate patterns (i.e., features) from syntactic and semantic representations of the Q/AP are needed.", "labels": [], "entities": []}, {"text": "In this respect, our previous work, e.g.,, has shown that tree kernels for NLP, e.g.,), can exploit syntactic patterns for answer passage reranking significantly improving search engine baselines.", "labels": [], "entities": [{"text": "answer passage reranking", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.8704010645548502}]}, {"text": "Our more recent work,), has shown that using automatically produced semantic labels in shallow syntactic trees, such as question category and question focus, can further improve passage reranking and answer extraction).", "labels": [], "entities": [{"text": "passage reranking", "start_pos": 178, "end_pos": 195, "type": "TASK", "confidence": 0.8001428246498108}, {"text": "answer extraction", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.779706746339798}]}, {"text": "However, such methods cannot solve the class of examples above as they do not use background knowledge, which is essential to answer complex questions.", "labels": [], "entities": []}, {"text": "On the other hand, and showed that semantic match features extracted from largescale background knowledge sources, including the LOD ones, are beneficial for answer reranking.", "labels": [], "entities": [{"text": "answer reranking", "start_pos": 158, "end_pos": 174, "type": "TASK", "confidence": 0.8841579854488373}]}, {"text": "In this paper, we tackle the candidate answer passage reranking task.", "labels": [], "entities": [{"text": "candidate answer passage reranking", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.6907819285988808}]}, {"text": "We define kernel functions that can automatically learn structural patterns enriched by semantic knowledge, e.g., from LOD.", "labels": [], "entities": []}, {"text": "For this purpose, we carryout the following steps: first, we design a representation for the Q/AP pair by engineering a pair of shallow syntactic trees connected with relational nodes (i.e., to match constituents from Q/AP pairs and use their generalizations in our syntactic/semantic structures.", "labels": [], "entities": []}, {"text": "We employ word sense disambiguation to match the right entities in YAGO and DBpedia, and consider all senses of an ambiguous word from WordNet.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6050771474838257}, {"text": "DBpedia", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.8352299928665161}, {"text": "WordNet", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.9703858494758606}]}, {"text": "Finally, we experiment with TREC QA and several models combining traditional feature vectors with automatic semantic labels derived by statistical classifiers and relational structures enriched with LOD relations.", "labels": [], "entities": []}, {"text": "The results show that our methods greatly improve over strong IR baseline, e.g., BM25, by 96%, and on our previous stateof-the-art reranking models, up to 15.4% (relative improvement) in P@1.", "labels": [], "entities": [{"text": "BM25", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.6490603089332581}, {"text": "P@1", "start_pos": 187, "end_pos": 190, "type": "METRIC", "confidence": 0.9538595080375671}]}], "datasetContent": [{"text": "We evaluated our different rerankers encoding several semantic structures on passage retrieval task, using a factoid open-domain TREC QA corpus.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.8341942429542542}, {"text": "TREC QA corpus", "start_pos": 129, "end_pos": 143, "type": "DATASET", "confidence": 0.5865818758805593}]}, {"text": "TREC QA In our experiments, we opted for questions from years 2002 and 2003, which totals to 824 factoid questions.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6465257108211517}]}, {"text": "The AQUAINT corpus is used for searching the supporting passages.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9297114312648773}]}, {"text": "Following (Severyn and Moschitti, 2012) we prune the shallow trees by removing the nodes beyond distance of 2 from the REL, REL-FOCUS or TM nodes.", "labels": [], "entities": [{"text": "REL", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.5992424488067627}]}, {"text": "We used the core RDF distribution of YAGO2 12 , WordNet 3.0 in RDF 13 , and the datasets from the 3.9 DBpedia distribution 14 . Feature Vectors.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.930927574634552}]}, {"text": "We used a subset of the similarity functions between Q and AP described in ().", "labels": [], "entities": []}, {"text": "These are used along with the structural models.", "labels": [], "entities": []}, {"text": "More explicitly: Termoverlap features: i.e., a cosine similarity over question/answer, sim COS (Q, AP ), where the input vectors are composed of lemma or POS-tag  n-grams with n = 1, .., 4.", "labels": [], "entities": [{"text": "COS", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.858477771282196}]}, {"text": "PTK score: i.e., output of the Partial Tree Kernel (PTK), defined in), when applied to the structural representations of Q and AP, sim PT K (Q, AP ) = PT K(Q, AP ) (note that, this is computed within a pair).", "labels": [], "entities": [{"text": "PTK score", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.5811483561992645}]}, {"text": "PTK defines similarity in terms of the number of substructures shared by two trees.", "labels": [], "entities": [{"text": "PTK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9129812717437744}]}, {"text": "Search engine ranking score: the ranking score of our search engine assigned to AP divided by a normalizing factor.", "labels": [], "entities": []}, {"text": "To train our models, we use SVM-light-TK 15 , which enables the use of structural kernels) in SVM-light).", "labels": [], "entities": []}, {"text": "We use default parameters and the preference reranking model described in Metrics.", "labels": [], "entities": []}, {"text": "We used common QA metrics: Precision at rank 1 (P@1), i.e., the percentage of questions with a correct answer ranked at the first position, and Mean Reciprocal Rank (MRR).", "labels": [], "entities": [{"text": "Precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9979583024978638}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 144, "end_pos": 170, "type": "METRIC", "confidence": 0.9676688611507416}]}, {"text": "We also report the Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 19, "end_pos": 47, "type": "METRIC", "confidence": 0.9358005623022715}]}, {"text": "We perform 5-fold cross-validation and report the metrics averaged across all the folds together with the std.dev.", "labels": [], "entities": []}], "tableCaptions": []}