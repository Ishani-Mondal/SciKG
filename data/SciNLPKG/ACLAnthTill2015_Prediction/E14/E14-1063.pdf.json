{"title": [{"text": "Applying the semantics of negation to SMT through n-best list re-ranking", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9858314990997314}]}], "abstractContent": [{"text": "Although the performance of SMT systems has improved over a range of different linguistic phenomena, negation has not yet received adequate treatment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9944620728492737}, {"text": "negation", "start_pos": 101, "end_pos": 109, "type": "TASK", "confidence": 0.9779011011123657}]}, {"text": "Previous works have considered the problem of translating negative data as one of data sparsity (Wetzel and Bond (2012)) or of structural differences between source and target language with respect to the placement of negation (Collins et al. (2005)).", "labels": [], "entities": []}, {"text": "This work starts instead from the questions of what is meant by negation and what makes a good translation of negation.", "labels": [], "entities": [{"text": "translation of negation", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.7853379646937052}]}, {"text": "These questions have led us to explore the use of semantics of negation in SMT-specifically, identifying core semantic elements of negation (cue, event and scope) in a source-side dependency parse and re-ranking hypotheses on the n-best list produced after decoding according to the extent to which an hypothesis realises these elements.", "labels": [], "entities": [{"text": "SMT-specifically", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.9930851459503174}]}, {"text": "The method shows considerable improvement over the baseline as measured by BLEU scores and Stanford's entailment-based MT evaluation metric (Pad\u00f3 et al. (2009)).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9976909160614014}, {"text": "MT evaluation", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.8623543679714203}]}], "introductionContent": [{"text": "Translating negation is a task that involves more than the correct rendering of a negation marker in the target sentence.", "labels": [], "entities": [{"text": "Translating negation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.925640732049942}]}, {"text": "For instance, translating Italy did not defeat France in 1909 differs from translating Italy defeated France in 1909, or France did not defeat Italy in 1909, or Italy did not conquer France in 1909.", "labels": [], "entities": [{"text": "translating Italy", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.8992898762226105}]}, {"text": "These examples show that translating negation also involves placing in the right position the semantic arguments as well as the event directly negated.", "labels": [], "entities": [{"text": "translating negation", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.893137127161026}]}, {"text": "Moreover, if the source sentence was uttered in response to the statement I think Italy defeated France in 1911, where the focus is the temporal argument in 1911, one can see that the system should not lose track of the focus of negation when producing the hypothesis translation.", "labels": [], "entities": []}, {"text": "Although negation must be appropriately rendered to ensure correct representation of the semantics of the source sentence in the machine output, only some of the efforts to improve the translation of negation-bearing sentences in SMT address the problem.", "labels": [], "entities": [{"text": "SMT", "start_pos": 230, "end_pos": 233, "type": "TASK", "confidence": 0.9594287872314453}]}, {"text": "Wetzel and Bond (2012) considered negation as a problem of data sparsity and so attempted to enrich the training data with negative paraphrases of positive sentences. and both addressed differences in the placement of negation in source and target texts, by re-ordering negative elements in the source sentence to better resemble their position in the corresponding target text.", "labels": [], "entities": []}, {"text": "Although these approaches show improvement over the baseline, neither considers negation as a linguistic phenomenon with specific characteristics.", "labels": [], "entities": []}, {"text": "This we do in the work presented here: We identify the elements of negation that an MT system has to reproduce and then devise a strategy to ensure that they are output correctly.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9633597135543823}]}, {"text": "These elements we take to be the cue, event and scope of negation . Unlike previous works, we first validate the hypothesis that if the top-ranked translation in the n-best list does not replicate elements of negation from the source, there maybe a more accurate translation after decoding, somewhere else on the n-best list.", "labels": [], "entities": []}, {"text": "If the hypothesis is false, then problems in the translation of negation lie elsewhere.", "labels": [], "entities": [{"text": "translation of negation", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7899465163548788}]}, {"text": "We use dependency parsing as a basis for Nbest list re-ranking.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8360373675823212}]}, {"text": "Dependencies between lexical elements appear to encode all elements of negation, offering a robust and easily-applicable way to extract negation-related information from a sentence.", "labels": [], "entities": []}, {"text": "We carryout our exploration of N-best list re-ranking in two steps: \u2022 First, an oracle translation is computed both to assess the validity of the approach and to understand the maximal extent to which it could possibly enhance performance.", "labels": [], "entities": []}, {"text": "An oracle translation is obtained by performing nbest list re-ranking using reference translations as a gold-standard.", "labels": [], "entities": []}, {"text": "To avoid the problem in Chinese-English Hierarchical Phrase-Based (HPB) translation of loss and/or misplacement of negation-related elements when hierarchical phrases are built, Chinese source sentences are first broken into sub-clauses, then translated and finally \"stitched\" back together for evaluation.", "labels": [], "entities": [{"text": "Chinese-English Hierarchical Phrase-Based (HPB) translation", "start_pos": 24, "end_pos": 83, "type": "TASK", "confidence": 0.6494273628507342}]}, {"text": "\u2022 Standard n-best list re-ranking is then performed using only source-side information.", "labels": [], "entities": []}, {"text": "Hypotheses are re-ranked according to the degree of similarity between the negationrelated elements in the hypotheses and those in the source sentence.", "labels": [], "entities": []}, {"text": "Here the correspondence between source and target text is established through lexical translation probabilities output after training.", "labels": [], "entities": []}, {"text": "Results of this method show that n-best list reranking does lead to a significant improvement in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9829106628894806}]}, {"text": "However, BLEU says nothing about semantics, so we also evaluate the method using Stanford's entailment based MT metrics, and also show improvement here.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9964648485183716}]}, {"text": "In the final section of the paper, we note the value of developing a custom metric that actually assesses the components of negation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU scores for the baseline system. The difference in BLEU scores between the positive, the  original and the negative conditions is also reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985369443893433}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9991982579231262}]}, {"text": " Table 2: BLEU, RTE and RTE+MT scores for the baseline system as tested on the sub-set only containing  negative sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994231462478638}, {"text": "RTE", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9663386344909668}, {"text": "RTE+MT", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.8818790117899576}]}, {"text": " Table 3: BLEU, RTE and RTE+MT scores for the oracle translation. The test sets evaluated are marked  from 1 to 4. Improvement over the baseline is reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992607235908508}, {"text": "RTE", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9557402729988098}, {"text": "RTE+MT", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.8705009420712789}, {"text": "oracle translation", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.733734667301178}]}, {"text": " Table 4: BLEU, RTE and RTE+MT scores for the sentences re-ranked using source side information  only. Improvement over the baseline is reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995068311691284}, {"text": "RTE", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9756320118904114}, {"text": "RTE+MT scores", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.9125311076641083}]}]}