{"title": [], "abstractContent": [{"text": "Word embeddings resulting from neural language models have been shown to be a great asset fora large variety of NLP tasks.", "labels": [], "entities": []}, {"text": "However, such architecture might be difficult and time-consuming to train.", "labels": [], "entities": []}, {"text": "Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix.", "labels": [], "entities": []}, {"text": "We compare those new word embeddings with some well-known embeddings on named entity recognition and movie review tasks and show that we can reach similar or even better performance.", "labels": [], "entities": [{"text": "named entity recognition and movie review tasks", "start_pos": 72, "end_pos": 119, "type": "TASK", "confidence": 0.7258035327707019}]}, {"text": "Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building word embeddings has always generated much interest for linguists.", "labels": [], "entities": []}, {"text": "Popular approaches such as Brown clustering algorithm) have been used with success in a wide variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Those word embeddings are often seen as a low dimensional-vector space where the dimensions are features potentially describing syntactic or semantic properties.", "labels": [], "entities": []}, {"text": "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings).", "labels": [], "entities": []}, {"text": "However, a neural network architecture can be hard to train.", "labels": [], "entities": []}, {"text": "Finding the right parameters to tune the model is often a challenging task and the training phase is in general computationally expensive.", "labels": [], "entities": []}, {"text": "This paper aims to show that such good word embeddings can be obtained using simple (mostly linear) operations.", "labels": [], "entities": []}, {"text": "We show that similar word embeddings can be computed using the word cooccurrence statistics and a well-known dimensionality reduction operation such as Principal Component Analysis (PCA).", "labels": [], "entities": [{"text": "Principal Component Analysis (PCA)", "start_pos": 152, "end_pos": 186, "type": "TASK", "confidence": 0.7140418837467829}]}, {"text": "We then compare our embeddings with the CW,, HLBL embeddings, which come from deep architectures and the LR-MVL) embeddings, which also come from a spectral method on several NLP tasks.", "labels": [], "entities": []}, {"text": "We claim that, assuming an appropriate metric, a simple spectral method as PCA can generate word embeddings as good as with deep-learning architectures.", "labels": [], "entities": []}, {"text": "On the other hand, deep-learning architectures have shown their potential in several supervised NLP tasks, by using these word embeddings.", "labels": [], "entities": []}, {"text": "As they are usually generated overlarge corpora of unlabeled data, words are represented in a generic manner.", "labels": [], "entities": []}, {"text": "Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER ().", "labels": [], "entities": []}, {"text": "For supervised tasks relying more on the semantic aspect as sentiment classification, it is usually helpful to adapt the existing embeddings to improve performance ().", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.933245986700058}]}, {"text": "We show in this paper that such embedding specialization can be easily done via neural network architectures and that helps to increase general performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks).", "labels": [], "entities": []}, {"text": "Using our word embeddings, we thus trained the sentence-level architecture described in section 4.1 on a NER task.", "labels": [], "entities": []}, {"text": "Named Entity Recognition (NER) It labels atomic elements in the sentence into categories such as \"PERSON\" or \"LOCATION\".", "labels": [], "entities": [{"text": "Named Entity Recognition (NER", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7671434760093689}, {"text": "PERSON", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.990220308303833}]}, {"text": "The CoNLL 2003 setup 6 is a NER benchmark data set based on Reuters data.", "labels": [], "entities": [{"text": "CoNLL 2003 setup 6", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9690733253955841}, {"text": "NER benchmark data set", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.78299830108881}, {"text": "Reuters data", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9430249035358429}]}, {"text": "The contest provides training, validation and testing sets.", "labels": [], "entities": []}, {"text": "The networks are fed with two raw features: word embeddings and a capital letter feature.", "labels": [], "entities": []}, {"text": "The \"caps\" feature tells if each word was in lowercase, was all uppercase, had first letter capital, or had at least one non-initial capital letter.", "labels": [], "entities": []}, {"text": "No other feature has been used to tune the models.", "labels": [], "entities": []}, {"text": "This is a main difference with other systems which usually use more features as POS tags, prefixes and suffixes or gazetteers.", "labels": [], "entities": []}, {"text": "Hyper-parameters were tuned on the validation set.", "labels": [], "entities": []}, {"text": "We selected n = 2 context words leading to a window of 5 words.", "labels": [], "entities": []}, {"text": "We used a special \"PADDING\" word for context at the beginning and the end of each sentence.", "labels": [], "entities": [{"text": "PADDING", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9819737672805786}]}, {"text": "For the non-linear model, the number of hidden units was 300.", "labels": [], "entities": []}, {"text": "As benchmark system, we report the system of, which reached 89.31% F1 with a semi-supervised approach and less specialized features than CoNLL 2003 challengers.", "labels": [], "entities": [{"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.999424934387207}, {"text": "CoNLL 2003 challengers", "start_pos": 137, "end_pos": 159, "type": "DATASET", "confidence": 0.9265560507774353}]}, {"text": "The NER evaluation task is mainly syntactic.", "labels": [], "entities": [{"text": "NER evaluation task", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.940458337465922}]}, {"text": "As we wish to evaluate whether our word embeddings can also capture semantic, we trained the document-level architecture described in section 4.2 over a movie review task.", "labels": [], "entities": []}, {"text": "We used a collection of 50,000 reviews from IMDB 7 . It allows no more than 30 reviews per movie.", "labels": [], "entities": [{"text": "IMDB 7", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.9632097482681274}]}, {"text": "It contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9989203214645386}]}, {"text": "Only highly polarized reviews have been considered.", "labels": [], "entities": []}, {"text": "A nega-6 http://www.cnts.ua.ac.be/conll2003/ner/ 7 Available at http://www.andrew-maas.net/data/sentiment tive review has a score \u2264 4 out of 10, and a positive review has a score \u2265 7 out of 10.", "labels": [], "entities": []}, {"text": "It has been evenly divided into training and test sets (25,000 reviews each).", "labels": [], "entities": []}, {"text": "For this task, we only used the word embeddings as features.", "labels": [], "entities": []}, {"text": "We perform a simple cross-validation on the training set to choose the optimal hyper-parameters.", "labels": [], "entities": []}, {"text": "The network had a window of 5 words and n filter = 1000 filters.", "labels": [], "entities": []}, {"text": "As benchmark system, we report the system of, which reached 88.90% accuracy with a mix of unsupervised and supervised techniques to learn word vectors capturing semantic termdocument information, as well as rich sentiment content.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9964935183525085}]}], "tableCaptions": [{"text": " Table 1: Benchmark of the experiment. Times are  reported in seconds.", "labels": [], "entities": [{"text": "Benchmark", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9785481691360474}, {"text": "Times", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.964443027973175}]}, {"text": " Table 2: Performance comparison on NER task  with different embeddings. The first column is  results with the original embeddings. The sec- ond column is results with embeddings after fine- tuning for this task. Results are reported in F1  score (mean \u00b1 standard deviation of ten training  runs with different initialization).", "labels": [], "entities": [{"text": "NER task", "start_pos": 36, "end_pos": 44, "type": "TASK", "confidence": 0.815611332654953}, {"text": "F1  score", "start_pos": 237, "end_pos": 246, "type": "METRIC", "confidence": 0.9882120490074158}]}, {"text": " Table 3: Performance comparison on movie re- view task with different embeddings. The first  column is results with the original embeddings.  The second column is results with embeddings af- ter fine-tuning for this task. Results are reported  in classification accuracy (mean \u00b1 standard devi- ation of ten training runs with different initializa- tion).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 263, "end_pos": 271, "type": "METRIC", "confidence": 0.8021417260169983}]}]}