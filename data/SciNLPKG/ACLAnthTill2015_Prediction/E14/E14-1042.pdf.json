{"title": [{"text": "Learning from Post-Editing: Online Model Adaptation for Statistical Machine Translation", "labels": [], "entities": [{"text": "Online Model Adaptation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.6155159970124563}, {"text": "Statistical Machine Translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.8389965891838074}]}], "abstractContent": [{"text": "Using machine translation output as a starting point for human translation has become an increasingly common application of MT.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 6, "end_pos": 32, "type": "TASK", "confidence": 0.7755663593610128}, {"text": "human translation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7647543549537659}, {"text": "MT", "start_pos": 124, "end_pos": 126, "type": "TASK", "confidence": 0.9943127632141113}]}, {"text": "We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system's discriminative parameters with a MIRA step.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.8152909874916077}, {"text": "MT", "start_pos": 351, "end_pos": 353, "type": "TASK", "confidence": 0.9242751002311707}, {"text": "MIRA", "start_pos": 428, "end_pos": 432, "type": "METRIC", "confidence": 0.9817553758621216}]}, {"text": "Individually, these techniques can substantially improve MT quality , even over strong baselines.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9944109320640564}]}, {"text": "Moreover, we see super-additive improvements when all three techniques are used in tandem.", "labels": [], "entities": []}], "introductionContent": [{"text": "Using machine translation outputs as a starting point for human translators is becoming increasingly common and is now arguably one of the most commercially important applications of MT.", "labels": [], "entities": [{"text": "machine translation outputs", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.786295473575592}, {"text": "human translators", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.6923764944076538}, {"text": "MT", "start_pos": 183, "end_pos": 185, "type": "TASK", "confidence": 0.9940078258514404}]}, {"text": "Considerable evidence has accumulated showing that human translators are more productive and accurate when post-editing MT output than when translating from scratch.", "labels": [], "entities": [{"text": "MT output", "start_pos": 120, "end_pos": 129, "type": "TASK", "confidence": 0.904735803604126}]}, {"text": "An important (if unsurprising) insight from prior research in this area is that translators become more productive as MT quality improves).", "labels": [], "entities": [{"text": "translators", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9605644345283508}, {"text": "MT", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.9679493308067322}]}, {"text": "While general improvements to MT continue to lead to further productivity gains, we explore how MT quality can be improved specifically in an online post-editing scenario in which sentence-level MT outputs are constantly being presented to human experts, edited, and then returned to the system for immediate learning.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9740707874298096}, {"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9878012537956238}]}, {"text": "This task is challenging in two regards.", "labels": [], "entities": []}, {"text": "First, from a technical perspective, post-edited outputs must be processed rapidly: a productive post-editor cannot wait fora standard batch MT training pipeline to be rerun after each sentence is corrected!", "labels": [], "entities": [{"text": "MT training", "start_pos": 141, "end_pos": 152, "type": "TASK", "confidence": 0.8767786026000977}]}, {"text": "Second, from a methodological perspective, it is expensive to run many human subject experiments, in particular when the human subjects must have translation expertise.", "labels": [], "entities": []}, {"text": "We therefore use a simulated post-editing paradigm in which either non-post-edited reference translations or manually post-edited translations from a similar MT system are used in lieu of human post-editors ( \u00a72).", "labels": [], "entities": []}, {"text": "This paradigm allows us to efficiently develop and evaluate systems that can goon to function in real-time post-editing scenarios without modification.", "labels": [], "entities": []}, {"text": "We present and evaluate three online methods for improving translation models using feedback from editors: adding new translations rules to the translation grammar ( \u00a73), updating a Bayesian language model with observations of the postedited output ( \u00a74), and using an online discriminative parameter update to minimize model error ( \u00a75).", "labels": [], "entities": []}, {"text": "These techniques are computationally efficient and make minimal use of approximation or heuristics, handling initial and incremental data in a uniform way.", "labels": [], "entities": []}, {"text": "We evaluate these techniques in a variety of language and data scenarios that mimic the demands of real-world translation tasks.", "labels": [], "entities": []}, {"text": "Compared to a competitive baseline, we show substantial improvement from updating the translation grammar or language model independently and super-additive gains from combining these techniques with a MIRA update ( \u00a76).", "labels": [], "entities": [{"text": "translation grammar", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8873085677623749}, {"text": "MIRA", "start_pos": 202, "end_pos": 206, "type": "DATASET", "confidence": 0.55400151014328}]}, {"text": "We then discuss how our techniques relate to prior work ( \u00a77) and conclude ( \u00a78).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our online extensions to standard machine translation systems in a series of sim-2 Resetting translation and language models prevents contamination.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7205432057380676}, {"text": "Resetting translation", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.9659607112407684}]}, {"text": "If models retained state from previous passes over the development set, they would include data for input sentences before they were translated, rather than after as in post-editing.", "labels": [], "entities": []}, {"text": "ulated post-editing experiments that cover hightraffic languages and challenging domains.", "labels": [], "entities": []}, {"text": "We show incremental improvement from our adaptive models and significantly larger gains when pairing our models with an online parameter update.", "labels": [], "entities": []}, {"text": "We finally validate our adaptive system on actual post-edited data.", "labels": [], "entities": []}, {"text": "The 2012 ACL Workshop on Machine Translation) makes available a set of 1832 English-Spanish parallel news source sentences, independent references, initial MT outputs, and post-edited MT outputs.", "labels": [], "entities": [{"text": "ACL Workshop on Machine Translation", "start_pos": 9, "end_pos": 44, "type": "TASK", "confidence": 0.5666514217853547}, {"text": "MT outputs", "start_pos": 156, "end_pos": 166, "type": "TASK", "confidence": 0.8316149711608887}]}, {"text": "The employed MT system is trained on largely the same resources as our own English-Spanish system, granting the opportunity fora much closer approximation to an actual post-editing task; our system configurations score between 54 and 56 BLEU against the sample MT, indicating that humans post-edited translations similar but not identical to our own.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9818347096443176}, {"text": "BLEU", "start_pos": 237, "end_pos": 241, "type": "METRIC", "confidence": 0.9985906481742859}]}, {"text": "We split the data into development and test sets, each 916 sentences, and run 3 iterations of optimizing on the development set and evaluating on the test set with both the MERT baseline and our G+L+M system on both types of references.", "labels": [], "entities": [{"text": "MERT baseline", "start_pos": 173, "end_pos": 186, "type": "DATASET", "confidence": 0.7001746594905853}]}, {"text": "Using independent references for tuning and evaluation (as before), our system yields an improvement of 0.6 BLEU (23.3 to 23.9).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9995865225791931}]}, {"text": "With post-edited references, our system yields an improvement of 1.3 BLEU (43.0 to 44.3).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9986298084259033}]}, {"text": "This provides strong evidence that our adaptive systems would provide better translations (both in terms of absolute quality and improvement over a standard baseline) for real-world post-editing scenarios.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU scores for systems with trigram  HPYPLM (no large language model), with and  without incremental updates from simulated post- editing data. Scores are averages over 3 optimizer  runs. Bold scores indicate statistically significant  improvement. Tuning set scores are italicized.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988710284233093}]}, {"text": " Table 3: BLEU scores for baseline and adap- tive systems. Scores are averages over three opti- mizer runs. Highest scores are bold and tuning set  scores are italicized. All fully adaptive systems  (G+L+M) show statistically significant improve- ment over both MERT and MIRA baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988712668418884}, {"text": "MERT", "start_pos": 262, "end_pos": 266, "type": "METRIC", "confidence": 0.7934821844100952}, {"text": "MIRA", "start_pos": 271, "end_pos": 275, "type": "METRIC", "confidence": 0.6162546873092651}]}]}