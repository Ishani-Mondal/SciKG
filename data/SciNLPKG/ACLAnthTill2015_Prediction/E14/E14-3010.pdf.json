{"title": [{"text": "A Graph-Based Approach to String Regeneration", "labels": [], "entities": [{"text": "String Regeneration", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6774290353059769}]}], "abstractContent": [{"text": "The string regeneration problem is the problem of generating a fluent sentence from a bag of words.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7328564077615738}]}, {"text": "We explore the N-gram language model approach to string regeneration.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7535973489284515}]}, {"text": "The approach computes the highest probability permutation of the input bag of words under an N-gram language model.", "labels": [], "entities": []}, {"text": "We describe a graph-based approach for finding the optimal permutation.", "labels": [], "entities": []}, {"text": "The evaluation of the approach on a number of datasets yielded promising results , which were confirmed by conducting a manual evaluation study.", "labels": [], "entities": []}], "introductionContent": [{"text": "The string regeneration problem can be stated as: given a bag of words taken from a fluent grammatical sentence, recover the original sentence.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7548570930957794}]}, {"text": "As it is often difficult to recover the exact original sentence based solely on a bag of words, the problem is relaxed to generating a fluent version of the original sentence ().", "labels": [], "entities": []}, {"text": "The string regeneration problem can generally be considered a difficult problem even for humans.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9284211099147797}]}, {"text": "Consider the following bag of words: { Iraq, list, in, a, third, joins, the, ., of, Bush's, of, critics, policy, senator, republican } and try to recover the original sentence or at least a fluent grammatical sentence.", "labels": [], "entities": [{"text": "Iraq", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8963281512260437}]}, {"text": "The original sentence was: a third republican senator joins the list of critics of Bush's policy in Iraq.", "labels": [], "entities": []}, {"text": "The purpose of investigating and developing approaches to solving the string regeneration problem is grammaticality and fluency improvement of machine generated text.", "labels": [], "entities": [{"text": "string regeneration problem", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.7938710848490397}]}, {"text": "The output of systems generating text, including SMT, abstract-like text summarisation, question answering, and dialogue systems, often lacks grammaticality and fluency).", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.986640989780426}, {"text": "text summarisation", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.6781112849712372}, {"text": "question answering", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.8232057094573975}]}, {"text": "The string regeneration problem is used as an application-independent method of evaluating approaches for improving grammaticality and fluency of such systems.", "labels": [], "entities": [{"text": "string regeneration problem", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7819025814533234}]}, {"text": "The string regeneration can also be viewed as a natural language realization problem.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7755542397499084}, {"text": "natural language realization", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.7252516547838846}]}, {"text": "The basic task of all realization approaches is to take a meaning representation as input and generate humanreadable output.", "labels": [], "entities": []}, {"text": "The approaches differ on how much information is required from the meaning representation, ranging from semantically annotated dependency graphs to shallow syntactic dependency trees.", "labels": [], "entities": []}, {"text": "A simple bag of words can then be considered as the least constrained input provided to a natural language realization system.", "labels": [], "entities": []}, {"text": "The bag of words can be combined with partial constraints to form a more realistic meaning representation.", "labels": [], "entities": []}, {"text": "proposed an algorithm for grammaticality improvement based on dependency spanning trees and evaluated it on the string regeneration task.", "labels": [], "entities": [{"text": "grammaticality improvement", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8723554909229279}, {"text": "string regeneration task", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.7827388445536295}]}, {"text": "They compared its performance against a baseline N-gram language model generator.", "labels": [], "entities": []}, {"text": "They found that their approach performs better with regards to BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.978858470916748}]}, {"text": "The latter approach does well at a local level but nonetheless often produces ungrammatical sentences.", "labels": [], "entities": []}, {"text": "We argue that the authors have not fully explored the N-gram language model approach to string regeneration.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7836742103099823}]}, {"text": "They used a Viterbi-like generator with a 4-gram language model and beam pruning to find approximate solutions.", "labels": [], "entities": []}, {"text": "Additionally, the 4-gram language model was trained on a relatively small dataset of around 20 million words.", "labels": [], "entities": []}, {"text": "The N-gram language model approach finds the highest probability permutation of the input bag of words under an N-gram language model as the solution to the string regeneration problem.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7451141774654388}]}, {"text": "In this paper we describe a graph-based approach to computing the highest probability permutation of a bag of words.", "labels": [], "entities": []}, {"text": "The graph-based approach models the problem as a set of vertices containing words and a set of edges between the vertices, whose cost equals language model probabilities.", "labels": [], "entities": []}, {"text": "Finding the permutation with the highest probability in the graph formulation is equal to finding the shortest tour in the graph or, equally, solving the Travelling Salesman Problem (TSP).", "labels": [], "entities": [{"text": "Travelling Salesman Problem (TSP)", "start_pos": 154, "end_pos": 187, "type": "TASK", "confidence": 0.5753057797749838}]}, {"text": "Despite the TSP being an NP-hard problem, state-of-the-art approaches exist to solving large problem instances.", "labels": [], "entities": [{"text": "TSP", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.930528998374939}]}, {"text": "An introduction to TSP and its variants discussed in this paper can be found in.", "labels": [], "entities": [{"text": "TSP", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.943721354007721}]}, {"text": "In contrast to the baseline N-gram approach by, our approach finds optimal solutions.", "labels": [], "entities": []}, {"text": "We built several models based on 2-gram, 3-gram, and 4-gram language models.", "labels": [], "entities": []}, {"text": "We experimentally evaluated the graph-based approach on several datasets.", "labels": [], "entities": []}, {"text": "The BLEU scores and example output indicated that our approach is successful in constructing a fairly fluent version of the original sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9982715845108032}]}, {"text": "We confirmed the results of automatic evaluation by conducting a manual evaluation.", "labels": [], "entities": []}, {"text": "The human judges were asked to compare the outputs of two systems and decide which is more fluent.", "labels": [], "entities": []}, {"text": "The results are statistically significant and confirm the ranking of the systems obtained using the BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.991327702999115}]}, {"text": "Additionally, we explored computing approximate solutions with time constraints.", "labels": [], "entities": []}, {"text": "We found that approximate solutions significantly decrease the quality of the output compared to optimal ones.", "labels": [], "entities": []}, {"text": "This paper describes work conducted in the MPhil thesis by Horvat (2013).", "labels": [], "entities": [{"text": "MPhil thesis", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.8757986724376678}]}], "datasetContent": [{"text": "We evaluated three different versions of the graphbased approach based on 2-gram, 3-gram, and 4-gram language models.", "labels": [], "entities": []}, {"text": "We evaluated each version of the system on three datasets of news sentences by computing the dataset-wide BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9980549812316895}]}, {"text": "The BLEU evaluation metric was developed by as an inexpensive and fast method of measuring incremental progress of SMT systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9939354062080383}, {"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9938592314720154}]}, {"text": "BLEU measures closeness of a candidate translation to a reference translation using N-gram precision.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9844427704811096}]}, {"text": "Similarly, in the string regeneration problem we measure the closeness of the regenerated sentence to the original sentence.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.743060827255249}]}, {"text": "We used the case insensitive NIST BLEU script v13 against tokenized references to compute the BLEU scores.", "labels": [], "entities": [{"text": "NIST", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.646459698677063}, {"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8763238191604614}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9975529313087463}]}, {"text": "have investigated the use of various automatic evaluation metrics to measure the quality of NLG output.", "labels": [], "entities": []}, {"text": "They found that BLEU correlates moderately well with human judgements of fluency and that it is useful for evaluation of NLG output, but should be used with caution, especially when comparing different systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9993141889572144}]}, {"text": "As the string regeneration problem is a basic form of NLG, BLEU is an appropriate measure of the system's performance with regards to fluency of the output.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7190292477607727}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9993500113487244}]}, {"text": "We provide examples of output and conduct a manual evaluation to confirm that the BLEU scores of individual systems reflect actual changes in output quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.998212456703186}]}, {"text": "We evaluated the graph-based approach on three datasets: MT08 The target side of the Ar-Eng newswire part of the NIST OpenMT08.", "labels": [], "entities": [{"text": "MT08", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.8652023077011108}, {"text": "Ar-Eng newswire", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.7580034732818604}, {"text": "NIST OpenMT08", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9032607972621918}]}, {"text": "We manually evaluated three versions of the graph-based approach: 2-gram, 3-gram, and 4-gram system using 3-gram as back-off.", "labels": [], "entities": []}, {"text": "We conducted a pairwise comparison of the three systems: for each evaluation sentence, we compared the output of a pair of systems and asked which output is more fluent.", "labels": [], "entities": []}, {"text": "We used the crowdsourcing website CrowdFlower 1 to gather fluency judgments.", "labels": [], "entities": []}, {"text": "Judges were asked 'Please read both sentences and compare the fluency of sentence 1 and sentence 2.'", "labels": [], "entities": []}, {"text": "They were given three options: 'Sentence 1 is more fluent', 'Sentence 2 is more fluent', 'Sentence 1 and Sentence 2 are indistinguishable in fluency'.", "labels": [], "entities": []}, {"text": "The order of presentation of the two systems was randomized for each sentence.", "labels": [], "entities": []}, {"text": "100 sentences of length between 5 and 18 words were chosen randomly from the combined MT08 and MT09 dataset.", "labels": [], "entities": [{"text": "MT08", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9248533844947815}, {"text": "MT09 dataset", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.8865250945091248}]}, {"text": "We gathered 5 judgements for each sentence of a single pairwise comparison of two systems.", "labels": [], "entities": []}, {"text": "Each pairwise comparison of two systems is therefore based on 500 human judgements.", "labels": [], "entities": []}, {"text": "The platform measures the reliability of judges by randomly posing gold standard questions in between regular questions.", "labels": [], "entities": [{"text": "reliability", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9683879017829895}]}, {"text": "If any judge incorrectly answered a number of gold standard questions, their judgements were deemed unreliable and were not used in the final result set.", "labels": [], "entities": []}, {"text": "A thorough discussion of suitability and reliability of crowdsourcing for NLP and SMT tasks and related ethical concerns can be found in:,.", "labels": [], "entities": [{"text": "SMT tasks", "start_pos": 82, "end_pos": 91, "type": "TASK", "confidence": 0.8012452721595764}]}, {"text": "The pairwise comparison results are shown in.", "labels": [], "entities": []}, {"text": "Each number represents the proportion of the human judgements that rated the output of the row system as better than the column system.", "labels": [], "entities": []}, {"text": "The raw numbers of pairwise comparison judgements in favor of each system are shown in.", "labels": [], "entities": []}, {"text": "A one-sided sign test indicated that we can reject the null hypothesis of the two systems being equal in favor of the alternative hypothesis of the first system being better than the second for all three system pairings: 3g and 2g, 4g and 2g, and 4g and 3g, p < 0.001 for all three comparisons.", "labels": [], "entities": []}, {"text": "The manual evaluation results therefore confirm the BLEU score differences between the three graph-based systems.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9734935462474823}]}, {"text": "Interestingly, in automatic evaluation the difference in BLEU scores between 2g and 3g systems was much bigger (around 10 BLEU points) than he stressed that the world is taking place in this direction of all major cities . he stressed that all major cities of the world is taking place in this direction . 58.0 (c) he stressed that this direction is taking place in all major cities of the world . 100.0 65.4 --4g 72.9 69.2 -: Manual evaluation results of pairwise comparison between three versions of the system: 2-gram, 3-gram, and the 4-gram system with 3-gram back-off.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9986839890480042}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9985798597335815}]}, {"text": "The numbers represent the percentage of judgements in favor of the row system when paired with the column system.", "labels": [], "entities": []}, {"text": "the difference between 3g and 4g systems (around 1 BLEU point).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9980937838554382}]}, {"text": "However, in manual evaluation the difference between 3g and 4g systems is noticeably bigger (69.2%) than the difference between 2g and 3g systems (65.4%).", "labels": [], "entities": []}, {"text": "sys1 sys2 sys1 equal sys2: The raw numbers of pairwise comparison judgements between the three systems.", "labels": [], "entities": []}, {"text": "The columns give the number of judgements in favor of each of the three options.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The vertex set size after applying  the transformations for several N-gram language  models at increasing sentence length.", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores for four versions of the  graph-based approach, based on 2-gram, 3-gram,  and 4-gram language models. We used the 4-gram  approach on sentences of up to length 18. The re- maining sentences were computed using either a  heuristic TSP solver (opt +heur) or by backing-off  to a 3-gram approach (4g +3g).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982050657272339}]}, {"text": " Table 5: The raw numbers of pairwise compari- son judgements between the three systems. The  columns give the number of judgements in favor  of each of the three options.", "labels": [], "entities": []}]}