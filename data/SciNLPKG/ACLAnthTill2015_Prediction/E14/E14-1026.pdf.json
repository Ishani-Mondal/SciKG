{"title": [{"text": "Source-side Preordering for Translation using Logistic Regression and Depth-first Branch-and-Bound Search *", "labels": [], "entities": [{"text": "Translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9681784510612488}]}], "abstractContent": [{"text": "We present a simple preordering approach for machine translation based on a feature-rich logistic regression model to predict whether two children of the same node in the source-side parse tree should be swapped or not.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8132169544696808}]}, {"text": "Given the pair-wise children regression scores we conduct an efficient depth-first branch-and-bound search through the space of possible children permutations , avoiding using a cascade of classifiers or limiting the list of possible ordering outcomes.", "labels": [], "entities": []}, {"text": "We report experiments in translating English to Japanese and Korean, demonstrating superior performance as (a) the number of crossing links drops by more than 10% absolute with respect to other state-of-the-art pre-ordering approaches, (b) BLEU scores improve on 2.2 points over the baseline with lexicalised reordering model, and (c) decoding can be carried out 80 times faster.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 240, "end_pos": 244, "type": "METRIC", "confidence": 0.999269425868988}]}], "introductionContent": [{"text": "Source-side preordering for translation is the task of rearranging the order of a given source sentence so that it best resembles the order of the target sentence.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.8927304744720459}]}, {"text": "It is a divide-and-conquer strategy aiming to decouple long-range word movement from the core translation task.", "labels": [], "entities": []}, {"text": "The main advantage is that translation becomes computationally cheaper as less word movement needs to be considered, which results in faster and better translations, if preordering is done well and efficiently.", "labels": [], "entities": [{"text": "translation", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9754576683044434}]}, {"text": "Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and * This work was done during an internship of the first author at SDL Research, translation gains can be obtained for various system architectures, e.g. phrase-based, hierarchical phrase-based, etc.", "labels": [], "entities": [{"text": "Preordering", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9602510333061218}]}, {"text": "For these reasons, preordering has a clear research and commercial interest, as reflected by the extensive previous work on the subject (see Section 2).", "labels": [], "entities": [{"text": "preordering", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.984370768070221}]}, {"text": "From these approaches, we are particularly interested in those that (i) involve little or no human intervention, (ii) require limited computational resources at runtime, and (iii) make use of available linguistic analysis tools.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel preordering approach based on a logistic regression model trained to predict whether to swap nodes in the source-side dependency tree.", "labels": [], "entities": []}, {"text": "For each pair of sibling nodes in the tree, the model uses a feature-rich representation that includes lexical cues to make relative reordering predictions between them.", "labels": [], "entities": []}, {"text": "Given these predictions, we conduct a depth-first branch-and-bound search through the space of possible permutations of all sibling nodes, using the regression scores to guide the search.", "labels": [], "entities": []}, {"text": "This approach has multiple advantages.", "labels": [], "entities": []}, {"text": "First, the search for permutations is efficient and does not require specific heuristics or hard limits for nodes with many children.", "labels": [], "entities": []}, {"text": "Second, the inclusion of the regression prediction directly into the search allows for finer-grained global decisions as the predictions that the model is more confident about are preferred.", "labels": [], "entities": []}, {"text": "Finally, the use of a single regression model to handle any number of child nodes avoids incurring sparsity issues, while allowing the integration of avast number of features into the preordering model.", "labels": [], "entities": []}, {"text": "We empirically contrast our proposed method against another preordering approach based on automatically-extracted rules when translating English into Japanese and Korean.", "labels": [], "entities": []}, {"text": "We demonstrate a significant reduction in number of crossing links of more than 10% absolute, as well as translation gains of over 2.2 BLEU points over the baseline.", "labels": [], "entities": [{"text": "translation", "start_pos": 105, "end_pos": 116, "type": "METRIC", "confidence": 0.9838325381278992}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9988589286804199}]}, {"text": "We also show it outperforms a multi-class classification approach and analyse why this is the case.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6699656993150711}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Ablation tests showing crossing scores  and classification accuracy as features are re- moved. All models were trained on 8M samples.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9937268495559692}, {"text": "classification", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.9364264011383057}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9438815712928772}]}, {"text": " Table 3: English-Japanese BLEU scores with var- ious preordering approaches (and improvement  over baseline) under two distortion limits d. Re- sults reported both excluding and including lexi- calised reordering model features (LRM).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9885518550872803}]}]}