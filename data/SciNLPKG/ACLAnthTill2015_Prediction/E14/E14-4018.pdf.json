{"title": [{"text": "Inference of Phrase-Based Translation Models via Minimum Description Length", "labels": [], "entities": [{"text": "Phrase-Based Translation", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7488370537757874}]}], "abstractContent": [{"text": "We present an unsupervised inference procedure for phrase-based translation models based on the minimum description length principle.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6926186382770538}]}, {"text": "In comparison to current inference techniques that rely on long pipelines of training heuristics, this procedure represents a theoretically well-founded approach to directly infer phrase lexicons.", "labels": [], "entities": []}, {"text": "Empirical results show that the proposed inference procedure has the potential to overcome many of the problems inherent to the current inference approaches for phrase-based models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since their introduction at the beginning of the twenty-first century, phrase-based (PB) translation models ( have become the state-of-the-art for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "phrase-based (PB) translation", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.60323646068573}, {"text": "statistical machine translation (SMT)", "start_pos": 147, "end_pos": 184, "type": "TASK", "confidence": 0.7953218966722488}]}, {"text": "PB model provide a big leap in translation quality with respect to the previous word-based translation models (.", "labels": [], "entities": []}, {"text": "However, despite their empirical success, inference procedures for PB models rely on along pipeline of heuristics and mismatched learning models, such as the long outperformed word-based models.", "labels": [], "entities": []}, {"text": "Latter stages of the pipeline cannot recover mistakes or omissions made in earlier stages which forces the individual stages to massively overgenerate hypotheses.", "labels": [], "entities": []}, {"text": "This manifests as a huge redundancy in the inferred phrase lexicons, which in turn largely penalizes the efficiency of PB systems at run-time.", "labels": [], "entities": []}, {"text": "The fact that PB models usually cannot generate the sentence pairs in which they have been trained in, or that it is even possible to improve the performance of a PB system by discarding most of the learned phrases are clear indicators of these deficiencies).", "labels": [], "entities": []}, {"text": "We introduce an unsupervised procedure to infer PB models based on the minimum description length (MDL) principle.", "labels": [], "entities": [{"text": "minimum description length (MDL)", "start_pos": 71, "end_pos": 103, "type": "METRIC", "confidence": 0.7258823464314142}]}, {"text": "MDL, formally described in Section 2, is a general inference procedure that \"learns\" by \"finding data regularities\".", "labels": [], "entities": []}, {"text": "MDL takes its name from the fact that regularities allow to compress the data, i.e. to describe it using fewer symbols than those required to describe the data literally.", "labels": [], "entities": []}, {"text": "As such, MDL embodies a form of Occam's Razor in which the best model fora given data is the one that provides a better trade-off between goodness-of-fit on the data and \"complexity\" or \"richness\" of the model.", "labels": [], "entities": [{"text": "Occam's Razor", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.7157402038574219}]}, {"text": "MDL has been previously used to infer monolingual grammars and inversion transduction grammars ().", "labels": [], "entities": []}, {"text": "Here, we adapt the basic principles described in the latter article to the inference of PB models.", "labels": [], "entities": []}, {"text": "The MDL inference procedure, described in Section 3, learns PB models by iteratively generalizing an initial model that perfectly overfits training data.", "labels": [], "entities": []}, {"text": "An MDL objective is used to guide this process.", "labels": [], "entities": []}, {"text": "MDL inference has the following desirable properties: \u2022 Training and testing are optimized upon the same model; a basic principle of machine learning largely ignored in PB models.", "labels": [], "entities": [{"text": "MDL inference", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7860837876796722}]}, {"text": "\u2022 It provides a joint estimation of the structure (set of bilingual phrases) and the parameters (phrase probabilities) of PB models.", "labels": [], "entities": []}, {"text": "\u2022 It automatically protects against overfitting by implementing a trade-off between the expressiveness of the model and training data fitting.", "labels": [], "entities": []}, {"text": "The empirical evaluation described in Section 4 focuses on understanding the behavior of MDLbased PB models and their specific traits.", "labels": [], "entities": []}, {"text": "That is, in contrast to atypical PB system building paper, we are not exclusively focused on a short term boost in translation quality.", "labels": [], "entities": []}, {"text": "Instead, we aim at studying the adequacy and future potential of MDL as inference procedure for PB models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Main figures of the experimental corpora.  M and k stand for millions and thousands of ele- ments respectively. Perplexity was calculated us- ing 5-gram language models.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 122, "end_pos": 132, "type": "METRIC", "confidence": 0.9636724591255188}]}, {"text": " Table 2: Size (number of phrase pairs) of the  MDL-based PB models, and quality of the gener- ated translations. We compare against a state-of- the-art PB inference pipeline (SotA).", "labels": [], "entities": []}]}