{"title": [{"text": "Regularized Structured Perceptron: A Case Study on Chinese Word Segmentation, POS Tagging and Parsing", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.5849534769852957}, {"text": "POS Tagging", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.7450659275054932}, {"text": "Parsing", "start_pos": 94, "end_pos": 101, "type": "TASK", "confidence": 0.4443243741989136}]}], "abstractContent": [{"text": "Structured perceptron becomes popular for various NLP tasks such as tagging and parsing.", "labels": [], "entities": [{"text": "tagging", "start_pos": 68, "end_pos": 75, "type": "TASK", "confidence": 0.9581974744796753}]}, {"text": "Practical studies on NLP did not pay much attention to its regularization.", "labels": [], "entities": [{"text": "regularization", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.9002125263214111}]}, {"text": "In this paper, we study three simple but effective task-independent regularization methods: (1) one is to average weights of different trained models to reduce the bias caused by the specific order of the training examples; (2) one is to add penalty term to the loss function; (3) and one is to randomly corrupt the data flow during training which is called dropout in the neu-ral network.", "labels": [], "entities": []}, {"text": "Experiments are conducted on three NLP tasks, namely Chinese word segmentation, part-of-speech tagging and dependency parsing.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.6344393491744995}, {"text": "part-of-speech tagging", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7593016922473907}, {"text": "dependency parsing", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8707890510559082}]}, {"text": "Applying proper reg-ularization methods or their combinations, the error reductions with respect to the averaged perceptron for some of these tasks can be up to 10%.", "labels": [], "entities": [{"text": "error reductions", "start_pos": 67, "end_pos": 83, "type": "METRIC", "confidence": 0.9615146517753601}]}], "introductionContent": [{"text": "Structured perceptron is a linear classification algorithm.", "labels": [], "entities": [{"text": "linear classification", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.678907036781311}]}, {"text": "It is used for word segmentation (), POS (part-of-speech) tagging), syntactical parsing (), semantical parsing () and other NLP tasks.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7895604372024536}, {"text": "POS (part-of-speech) tagging", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6660001516342163}, {"text": "syntactical parsing", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.715217798948288}, {"text": "semantical parsing", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7312322854995728}]}, {"text": "The averaged perceptron or the voted perceptron) is proposed for better generalization.", "labels": [], "entities": []}, {"text": "Early update () is used for inexact decoding algorithms such as the beam search.", "labels": [], "entities": [{"text": "Early update", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9422586262226105}]}, {"text": "Distributed training) and the minibatch and parallelization method are recently proposed.", "labels": [], "entities": []}, {"text": "Some other related work focuses on the task-specified feature engineering.", "labels": [], "entities": []}, {"text": "Regularization is to improve the ability of generalization and avoid over-fitting for machine learning algorithms including online learning algorithms.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8113645315170288}, {"text": "generalization", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9636746048927307}]}, {"text": "But practical studies on NLP did not pay much attention to the regularization of the structured perceptron.", "labels": [], "entities": []}, {"text": "As a result, for some tasks the model learned using perceptron algorithm is not as good as the model learned using regularized condition random field.", "labels": [], "entities": []}, {"text": "In this paper, we treat the perceptron algorithm as a special case of the stochastic gradient descent (SGD) algorithm and study three kinds of simple but effective task-independent regularization methods that can be applied.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.7238855957984924}]}, {"text": "The averaging method is to average the weight vectors of different models.", "labels": [], "entities": []}, {"text": "We propose a \"shuffle-and-average\" method to reduce the bias caused by the specific order of the training examples.", "labels": [], "entities": []}, {"text": "The traditional penalty method is to add penalty term to the loss function.", "labels": [], "entities": []}, {"text": "The dropout method is to randomly corrupt the data flow during training.", "labels": [], "entities": []}, {"text": "We show that this dropout method originally used in neural network also helps the structured perceptron.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the perceptron algorithm as a special case of the stochastic gradient descent algorithm.", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.699427048365275}]}, {"text": "Then we discuss three kinds of regularization methods for structured perceptron in Section 3, 4 and 5, respectively.", "labels": [], "entities": []}, {"text": "Experiments conducted in Section 6 shows that these regularization methods and their combinations improve performances of NLP tasks such as Chinese word segmentation, POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 140, "end_pos": 165, "type": "TASK", "confidence": 0.6110098958015442}, {"text": "POS tagging", "start_pos": 167, "end_pos": 178, "type": "TASK", "confidence": 0.8553263545036316}, {"text": "dependency parsing", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.857230544090271}]}, {"text": "Applying proper regularization methods, the error reductions of these NLP tasks can be up to 10%.", "labels": [], "entities": [{"text": "error reductions", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9651806056499481}]}, {"text": "We finally conclude this work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first introduce three NLP tasks using structured perceptron namely Chinese word segmentation, POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.6044191320737203}, {"text": "POS tagging", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.8480715751647949}, {"text": "dependency parsing", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.8408513367176056}]}, {"text": "Then we investigate the effects of regularization methods for structured perceptron mainly on the development set of character-based Chinese word segmentation.", "labels": [], "entities": [{"text": "character-based Chinese word segmentation", "start_pos": 117, "end_pos": 158, "type": "TASK", "confidence": 0.6018852069973946}]}, {"text": "Finally, we compare the final performances on the test sets of these three tasks using regularization methods with related work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Final results of the word-based Chinese  word segmentation task on CTB5.", "labels": [], "entities": [{"text": "word-based Chinese  word segmentation task", "start_pos": 31, "end_pos": 73, "type": "TASK", "confidence": 0.6325656831264496}, {"text": "CTB5", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9529011249542236}]}, {"text": " Table 4: Final results of the POS tagging task on  CTB5.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.8701829115549723}, {"text": "CTB5", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9720199704170227}]}, {"text": " Table 5: Final results of the dependency parsing  task on CTB5.", "labels": [], "entities": [{"text": "dependency parsing  task", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8863683938980103}, {"text": "CTB5", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9616040587425232}]}]}