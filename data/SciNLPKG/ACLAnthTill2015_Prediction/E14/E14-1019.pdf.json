{"title": [{"text": "About Inferences in a Crowdsourced Lexical-Semantic Network", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatically inferring new relations from already existing ones is away to improve the quality of a lexical network by relation densification and error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.7045843452215195}]}, {"text": "In this paper, we devise such an approach for the JeuxDeMots lexical network, which is a freely avalaible lexical network for French.", "labels": [], "entities": [{"text": "JeuxDeMots lexical network", "start_pos": 50, "end_pos": 76, "type": "DATASET", "confidence": 0.9125095804532369}]}, {"text": "We first present deduction (generic to specific) and induction (specific to generic) which are two inference schemes ontologically founded.", "labels": [], "entities": []}, {"text": "We then propose abduction as a third form of inference scheme, which exploits examples similar to a target term.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building resources for Computational Linguistics (CL) is of crucial interest.", "labels": [], "entities": [{"text": "Computational Linguistics (CL)", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.6640705287456512}]}, {"text": "Most of existing lexical-semantic networks have been built by hand (like for instance) and, despite that tools are generally designed for consistency checking, the task remains time consuming and costly.", "labels": [], "entities": [{"text": "consistency checking", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.6874161213636398}]}, {"text": "Fully automated approaches are generally limited to term co-occurrences as extracting precise semantic relations between terms from corpora remains really difficult.", "labels": [], "entities": [{"text": "extracting precise semantic relations between terms from corpora", "start_pos": 75, "end_pos": 139, "type": "TASK", "confidence": 0.8321997076272964}]}, {"text": "Meanwhile, crowdsourcing approaches are flowering in CL especially with the advent of Amazon Mechanical Turk or in a broader scope Wikipedia and Wiktionary, to cite the most well-known examples.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.8865684866905212}]}, {"text": "WordNet is such a lexical network, constructed by hand at great cost, based on synsets which can be roughly considered as concepts.) a multilingual version of) a French version of WordNet, were built by automated crossing of WordNet and other lexical resources along with some manual checking.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9719727635383606}, {"text": "WordNet", "start_pos": 180, "end_pos": 187, "type": "DATASET", "confidence": 0.9384018778800964}, {"text": "WordNet", "start_pos": 225, "end_pos": 232, "type": "DATASET", "confidence": 0.9635275602340698}]}, {"text": "constructed automatically BabelNet a large multilingual lexical network from term cooccurrences in Wikipedia.", "labels": [], "entities": []}, {"text": "A lexical-semantic network can contain lemmas, word forms and multi-word expressions as entry points (nodes) along with word meanings and concepts.", "labels": [], "entities": []}, {"text": "The idea itself of word senses in the lexicographic tradition maybe debatable in the context of resources for semantic analysis, and we generally prefer to consider word usages.", "labels": [], "entities": []}, {"text": "A given polysemous word, as identified by locutors, has several usages that might differ substantially from word senses as classically defined.", "labels": [], "entities": []}, {"text": "A given usage can also in turn have several deeper refinements and the whole set of usages can take the form of a decision tree.", "labels": [], "entities": []}, {"text": "For example, frigate can be a bird or a ship.", "labels": [], "entities": []}, {"text": "A frigate>boat can be distinguished as a modern ship with missiles and radar or an ancient vessel with sails.", "labels": [], "entities": []}, {"text": "In the context of a collaborative construction, such a lexical resource should be considered as being constantly evolving and a general rule of thumb is to have no definite certitude about the state of an entry.", "labels": [], "entities": []}, {"text": "For a polysemic term, some refinements might be just missing at a given time notwithstanding evolution of language which might be very fast, especially in technical domains.", "labels": [], "entities": []}, {"text": "There is noway (unless by inspection) to know if a given entry refinements are fully completed, and even if this question is really relevant.", "labels": [], "entities": []}, {"text": "The building of a collaborative lexical network (or, in all generality, any similar resource) can be devised according to two broad strategies.", "labels": [], "entities": []}, {"text": "First, it can be designed as a contributive system like Wikipedia where people willingly add and complete entries (like for Wiktionary).", "labels": [], "entities": []}, {"text": "Second, contributions can be made indirectly thanks to games (better known as GWAP) and in this case players do not need to be aware that while playing they are helping building a lexical resource.", "labels": [], "entities": []}, {"text": "In any case, the built lexical network is not free of errors which are corrected along their discovery.", "labels": [], "entities": []}, {"text": "Thus, a large number of obvious relations are not contained in the lexical network but are indeed necessary fora high quality resources usable in various NLP applications and notably semantic analysis.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 183, "end_pos": 200, "type": "TASK", "confidence": 0.8736468255519867}]}, {"text": "For example, contributors seldom indicate that a particular bird type can fly, as it is considered as an obvious generality.", "labels": [], "entities": []}, {"text": "Only notable facts which are not easily deductible are naturally contributed.", "labels": [], "entities": []}, {"text": "Well known exceptions are also generally contributed and take the form of a negative weight and annotated as such (for example, fly agent :\u2212100 In order to consolidate the lexical network, we adopt a strategy based on a simple inference mechanism to propose new relations from those already existing.", "labels": [], "entities": []}, {"text": "The approach is strictly endogenous (i.e. self-contained) as it doesn't rely on any other external resources.", "labels": [], "entities": []}, {"text": "Inferred relations are submitted either to contributors for voting or to experts for direct validation/invalidation.", "labels": [], "entities": []}, {"text": "A large percentage of the inferred relations has been found to be correct however, a non-negligible part of them are found to be wrong and understanding why is both interesting and useful.", "labels": [], "entities": []}, {"text": "The explanation process can be viewed as a reconciliation between the inference engine and contributors who are guided through a dialog to explain why they found the considered relation incorrect.", "labels": [], "entities": []}, {"text": "The possible causes fora wrong inferred relation may come from three possible origins: false premises that were used by the inference engine, exception or confusion due to some polysemy.", "labels": [], "entities": [{"text": "exception", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9850155115127563}]}, {"text": "In () an endogenous enrichment of Wiktionary is done thanks to a crowdsourcing tool.", "labels": [], "entities": []}, {"text": "A quite similar approach of using crowdsourcing has been considered by) for evaluating inference rules that are discovered from texts.", "labels": [], "entities": []}, {"text": "In), some specific inference methods are conducted on text with the help of an ontology.", "labels": [], "entities": []}, {"text": "Similarly, capture explanation with ontology-based inference.", "labels": [], "entities": [{"text": "capture explanation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8709741234779358}]}, {"text": "OntoLearn) is a system that automatically build ontologies of specific domains from texts and also makes use of inferences.", "labels": [], "entities": [{"text": "OntoLearn", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8654604554176331}]}, {"text": "There have been also researchs on taxonomy induction based on WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.969113290309906}]}, {"text": "Although extensive work on inference from texts or handcrafted resources has been done, almost none endogenously on lexical network built by the crowds.", "labels": [], "entities": []}, {"text": "Most probably the main reason of that situation is the lack of such specific resources.", "labels": [], "entities": []}, {"text": "In this article, we first present the principles behind the lexical network construction with crowdsourcing and games with a purpose (also know as human-based computation games) and illustrated them with the JeuxDeMots (JDM) project.", "labels": [], "entities": [{"text": "lexical network construction", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.7572592298189799}]}, {"text": "Then, we present the outline of an elicitation engine based on an inference engine using deduction, induction and especially abduction schemes.", "labels": [], "entities": []}, {"text": "An experimentation is then presented.", "labels": [], "entities": []}], "datasetContent": [{"text": "We made an experiment with a unique run of the deduction, induction and abduction engines over the lexical network.", "labels": [], "entities": []}, {"text": "Contributors have either accepted or rejected a subset of those candidates during the normal course of their activity.", "labels": [], "entities": []}, {"text": "This experiment is for an evaluation purpose only, as actually the system is running iteratively along with contributors and games.", "labels": [], "entities": []}, {"text": "The experiment has been done with the parameters given previously, which are determined emprically as those maximizing recall and precision (over a very small subset of the JDM lexical network, around 1\u2030).", "labels": [], "entities": [{"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9983684420585632}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9935091137886047}, {"text": "JDM lexical network", "start_pos": 173, "end_pos": 192, "type": "DATASET", "confidence": 0.7652077078819275}]}], "tableCaptions": [{"text": " Table 1: Global percentages of relations pro- posed per type for deduction and induction.", "labels": [], "entities": []}, {"text": " Table 2: Number of propositions produced by  deduction and ratio of relations found as true or  false.", "labels": [], "entities": []}, {"text": " Table 3: Number of propositions produced by in- duction and ratio of relations found as true or  false.", "labels": [], "entities": []}]}