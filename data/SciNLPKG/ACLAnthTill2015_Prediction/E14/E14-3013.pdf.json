{"title": [{"text": "Generating artificial errors for grammatical error correction", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.6569427251815796}]}], "abstractContent": [{"text": "This paper explores the generation of artificial errors for correcting grammatical mistakes made by learners of English as a second language.", "labels": [], "entities": [{"text": "correcting grammatical mistakes made by learners of English as a second language", "start_pos": 60, "end_pos": 140, "type": "TASK", "confidence": 0.750973622004191}]}, {"text": "Artificial errors are injected into a set of error-free sentences in a probabilistic manner using statistics from a corpus.", "labels": [], "entities": []}, {"text": "Unlike previous approaches, we use linguistic information to derive error generation probabilities and build corpora to correct several error types, including open-class errors.", "labels": [], "entities": [{"text": "error generation probabilities", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.763224850098292}]}, {"text": "In addition, we also analyse the variables involved in the selection of candidate sentences.", "labels": [], "entities": []}, {"text": "Experiments using the NUCLE corpus from the CoNLL 2013 shared task reveal that: 1) training on artificially created errors improves precision at the expense of recall and 2) different types of linguistic information are better suited for correcting different error types.", "labels": [], "entities": [{"text": "NUCLE corpus from the CoNLL 2013 shared task", "start_pos": 22, "end_pos": 66, "type": "DATASET", "confidence": 0.9135326817631721}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9985437393188477}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9981223940849304}]}], "introductionContent": [{"text": "Building error correction systems using machine learning techniques can require a considerable amount of annotated data which is difficult to obtain.", "labels": [], "entities": [{"text": "error correction", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7109799385070801}]}, {"text": "Available error-annotated corpora are often focused on particular groups of people (e.g. nonnative students), error types (e.g. spelling, syntax), genres (e.g. university essays, letters) or topics so it is not clear how representative they are or how well systems based on them will generalise.", "labels": [], "entities": []}, {"text": "On the other hand, building new corpora is not always a viable solution since error annotation is expensive.", "labels": [], "entities": []}, {"text": "As a result, researchers have tried to overcome these limitations either by compiling corpora automatically from the web or using artificial corpora which are cheaper to produce and can be tailored to their needs.", "labels": [], "entities": []}, {"text": "Artificial error generation allows researchers to create very large error-annotated corpora with little effort and control variables such as topic and error types.", "labels": [], "entities": [{"text": "error generation", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7858815491199493}]}, {"text": "Errors can be injected into candidate texts using a deterministic approach (e.g. fixed rules) or probabilities derived from manually annotated samples in order to mimic real data.", "labels": [], "entities": []}, {"text": "Although artificial errors have been used in previous work, we present anew approach based on linguistic information and evaluate it using the test data provided for the CoNLL 2013 shared task on grammatical error correction ( . Our work makes the following contributions.", "labels": [], "entities": [{"text": "CoNLL 2013 shared task on grammatical error correction", "start_pos": 170, "end_pos": 224, "type": "TASK", "confidence": 0.7533057406544685}]}, {"text": "First, we are the first to use linguistic information (such as part-of-speech (PoS) information or semantic classes) to characterise contexts of naturally occurring errors and replicate them in errorfree text.", "labels": [], "entities": []}, {"text": "Second, we apply our technique to a larger number of error types than any other previous approach, including open-class errors.", "labels": [], "entities": []}, {"text": "The resulting datasets are used to train error correction systems aimed at learners of English as a second language (ESL).", "labels": [], "entities": [{"text": "error correction", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.63776695728302}]}, {"text": "Finally, we provide a detailed description of the variables that affect artificial error generation.", "labels": [], "entities": [{"text": "artificial error generation", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.6656144658724467}]}], "datasetContent": [{"text": "We use the NUCLE v2.3 corpus () released for the CoNLL 2013 shared task on error correction, which comprises errorannotated essays written in English by students at the National University of Singapore.", "labels": [], "entities": [{"text": "NUCLE v2.3 corpus", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.9124226967493693}, {"text": "CoNLL 2013 shared task on error correction", "start_pos": 49, "end_pos": 91, "type": "TASK", "confidence": 0.805442580154964}]}, {"text": "These essays cover topics such as environmental pollution, healthcare, welfare, technology, etc.", "labels": [], "entities": []}, {"text": "All the sentences were manually annotated by human experts using a set of 27 error types, but we used the filtered version containing only the five types selected for the shared task: ArtOrDet (article or determiner), Nn (noun number), Prep (preposition), SVA (subject-verb agreements) and Vform (verb form) errors.", "labels": [], "entities": []}, {"text": "The training set of the NUCLE corpus contains 57,151 sentences and 1,161,567 tokens while the test set comprises 1,381 sentences and 29,207 tokens.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9663545191287994}]}, {"text": "The training portion of the corpus was used to estimate the required conditional probabilities and train a few variations of our systems while the test set was reserved to evaluate performance.", "labels": [], "entities": []}, {"text": "Candidate native texts for error injection were extracted from the English Wikipedia, controlling the variables described Section 3.1 as follows: Topic: We chose an initial set of 50 Wikipedia articles based on keywords in the NUCLE training data and proceeded to collect related articles by following hyperlinks in their 'See also' section.", "labels": [], "entities": [{"text": "error injection", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.6436249613761902}, {"text": "English Wikipedia", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9085675477981567}, {"text": "NUCLE training data", "start_pos": 227, "end_pos": 246, "type": "DATASET", "confidence": 0.9180286725362142}]}, {"text": "We retrieved a total of 494 articles which were later preprocessed to remove Information Probability Generated error types Error type distribution P(error type) ArtOrDet, Nn, Prep, SVA, Vform Morphology P(source=determiner|target=determiner, head noun tag) ArtOrDet, SVA P(source=verb tag|target=verb tag, subj head noun tag) PoS disambiguation P(source=word|target=word, PoS) Nn, Vform Semantic classes P(source=determiner|target=determiner, head noun class) ArtOrDet, Prep P(source=preposition|target=preposition, head noun class) Word senses P(source=preposition|verb sense + obj head noun sense) ArtOrDet, Prep, SVA P(source=preposition|target=preposition, head noun sense) P(source=preposition|target=preposition, dep adj sense) P(source=determiner|target=determiner, head noun sense) P(source=verb tag|target=verb tag, subj head noun sense): Probabilities computed for each type of linguistic information.", "labels": [], "entities": []}, {"text": "Error codes correspond to the five error types in the CoNLL 2013 shared task: ArtOrDet (article or determiner), Nn (noun number), Prep (prepositions), SVA (subject-verb agreement) and Vform (verb form).", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9563140273094177}, {"text": "CoNLL 2013 shared task", "start_pos": 54, "end_pos": 76, "type": "DATASET", "confidence": 0.884941965341568}, {"text": "Prep", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9507463574409485}]}, {"text": "wikicode tags, yielding 54,945 sentences and approximately 1,123,739 tokens.", "labels": [], "entities": []}, {"text": "Genre: Both NUCLE and Wikipedia contain expository texts, although they are not necessarily similar.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9151709675788879}]}, {"text": "Style/register: Written, academic and formal.", "labels": [], "entities": []}, {"text": "Text complexity/language proficiency: Essays in the NUCLE corpus are written by advanced university students and are therefore comparable to standard English Wikipedia articles.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9478364586830139}]}, {"text": "For less sophisticated language, the Simple English Wikipedia could bean alternative.", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 37, "end_pos": 61, "type": "DATASET", "confidence": 0.823063313961029}]}, {"text": "Native language: English Wikipedia articles are mostly written by native speakers whereas NUCLE essays are not.", "labels": [], "entities": [{"text": "NUCLE essays", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.8696686625480652}]}, {"text": "This is the only discordant variable.", "labels": [], "entities": []}, {"text": "PoS tagging was performed using RASP ().", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7100004404783249}]}, {"text": "Word sense disambiguation was carried out using the WordNet::SenseRelate:AllWords Perl module) which assigns a sense from WordNet to each content word in a text.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7149384617805481}, {"text": "WordNet::SenseRelate:AllWords Perl module", "start_pos": 52, "end_pos": 93, "type": "DATASET", "confidence": 0.8779712542891502}]}, {"text": "As for semantic information, we use WordNet classes which are readily available in NLTK (.", "labels": [], "entities": []}, {"text": "WordNet classes respond to a classification in lexicographers' files and are defined for content words as shown in: Generated artificial corpora based on different types of linguistic information.", "labels": [], "entities": []}, {"text": "As shown in, we are unable to produce the same set of errors for each different type of information.", "labels": [], "entities": []}, {"text": "This is a limitation of our conditional probabilities which encode different information in each case.", "labels": [], "entities": []}, {"text": "In consequence, comparing overall results between datasets seems unfair as they do not target the same error types.", "labels": [], "entities": []}, {"text": "In order to overcome this problem, we will define new probabilities so that we can generate the same types of error in all cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Evaluation of our correction systems over the original and revised NUCLE test set using the M 2  Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections suggested  by each system. Results in bold show improvements over the baseline.", "labels": [], "entities": [{"text": "NUCLE test set", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9770009517669678}, {"text": "M 2  Scorer", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.6769973039627075}]}, {"text": " Table 5: Error type analysis of our correction systems over the original NUCLE test set using the M 2  Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections outside  the main categories suggested by each system. Results in bold show improvements over the baseline.", "labels": [], "entities": [{"text": "Error type analysis", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8538175423940023}, {"text": "NUCLE test set", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.9736596743265787}, {"text": "M 2  Scorer", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.6062852342923483}]}, {"text": " Table 6: Error type analysis of our correction systems over the revised NUCLE test set using the M 2  Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections outside  the main categories suggested by each system. Results in bold show improvements over the baseline.", "labels": [], "entities": [{"text": "Error type analysis", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8840357859929403}, {"text": "NUCLE test set", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9813059171040853}, {"text": "M 2  Scorer", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.6366640230019888}]}]}