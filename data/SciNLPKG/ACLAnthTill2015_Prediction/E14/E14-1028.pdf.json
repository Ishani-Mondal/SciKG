{"title": [], "abstractContent": [{"text": "We describe an approach to word ordering using modelling techniques from statistical machine translation.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.7902403175830841}, {"text": "statistical machine translation", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.6337577998638153}]}, {"text": "The system incorporates a phrase-based model of string generation that aims to take unordered bags of words and produce fluent, grammatical sentences.", "labels": [], "entities": [{"text": "string generation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7796003818511963}]}, {"text": "We describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases.", "labels": [], "entities": []}, {"text": "Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9816116690635681}]}, {"text": "Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering) and in SMT with arbitrary word reordering.", "labels": [], "entities": [{"text": "Word ordering", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7300156652927399}, {"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9898059964179993}]}, {"text": "Typical solutions involve constraints on the space of permutations, as in multi-document summarisation () and preordering in SMT (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9506441950798035}]}, {"text": "Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search.", "labels": [], "entities": [{"text": "word ordering task", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7958494027455648}]}, {"text": "use a dependency grammar to address word ordering, while use CCG and large-scale n-gram language models.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.7268397510051727}]}, {"text": "These techniques are applied to the unconstrained problem of generating a sentence from a multi-set of input words.", "labels": [], "entities": []}, {"text": "We describe GYRO (Get Your Order Right), a phrase-based approach to word ordering.", "labels": [], "entities": [{"text": "GYRO (Get Your Order Right)", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.5393147936889103}, {"text": "word ordering", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.7331971377134323}]}, {"text": "Given a bag of words, the system first scans a large, trusted text collection and extracts phrases consisting of words from the bag.", "labels": [], "entities": []}, {"text": "Strings are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reordering of the words in the bag, and the results are scored under an n-gram language model (LM).", "labels": [], "entities": []}, {"text": "The motivation is that it is easier to make fluent sentences from phrases (snippets of fluent text) than from words in isolation.", "labels": [], "entities": []}, {"text": "GYRO builds on approaches developed for syntactic SMT).", "labels": [], "entities": [{"text": "GYRO", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8961853981018066}, {"text": "syntactic SMT", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.5848307311534882}]}, {"text": "The system generates strings in the form of weighted automata which can be rescored using higher-order n-gram LMs, dependency LMs, and Minimum Bayes Risk decoding, either using posterior probabilities obtained from GYRO or SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 223, "end_pos": 226, "type": "TASK", "confidence": 0.7499684691429138}]}, {"text": "We report extensive experiments using BLEU and conclude with human assessments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9966020584106445}]}, {"text": "We show that despite its relatively simple formulation, GYRO gives BLEU scores over 20 points higher than the best previously reported results, generated by a syntax-based ordering system.", "labels": [], "entities": [{"text": "GYRO", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.7208101749420166}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9986655712127686}]}, {"text": "Human fluency assessments confirm these substantial improvements. are extracted from large text collections, and contain only words from \u2126.", "labels": [], "entities": []}, {"text": "We refer to phrases as u, i.e. u \u2208 L(\u2126).", "labels": [], "entities": []}, {"text": "The goal is to generate all permutations of \u2126 that can be formed by concatenation of phrases from L(\u2126).", "labels": [], "entities": []}], "datasetContent": [{"text": "We now report various experiments evaluating the performance of the generation approach described above.", "labels": [], "entities": []}, {"text": "The system is evaluated using the MT08-nw, and MT09-nw testsets.", "labels": [], "entities": [{"text": "MT08-nw", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9184705018997192}, {"text": "MT09-nw testsets", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9228033721446991}]}, {"text": "These correspond to the first English reference of the newswire portion of the Arabic-to-English NIST MT evaluation sets 2 . They contain 813 and 586 sentences respectively (53,325 tokens in total; average sentence length = 38.1 tokens after tokenization).", "labels": [], "entities": [{"text": "NIST MT evaluation sets", "start_pos": 97, "end_pos": 120, "type": "DATASET", "confidence": 0.7298416197299957}]}, {"text": "In order to reduce the computational complexity, all sentences with more than 20 tokens were divided into sub-sentences, with 20 tokens being the upper limit.", "labels": [], "entities": []}, {"text": "Between 70-80% of the sentences in the testsets were divided in this way.", "labels": [], "entities": []}, {"text": "For each of these sentences we create a bag.", "labels": [], "entities": []}, {"text": "The GYRO system uses a n-gram LM estimated over 1.3 billion words of English text, including the AFP and Xinhua portions of the GigaWord corpus version 4 (1.1 billion words) and the English side of various Arabic-English parallel corpora typically used in MT evaluations (0.2 billion words).", "labels": [], "entities": [{"text": "AFP", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9561834335327148}, {"text": "GigaWord corpus version 4", "start_pos": 128, "end_pos": 153, "type": "DATASET", "confidence": 0.926246240735054}, {"text": "MT evaluations", "start_pos": 256, "end_pos": 270, "type": "TASK", "confidence": 0.9084289371967316}]}, {"text": "Phrases of up to length 5 are extracted for each bag from a text collection containing 10.6 billion words of English news text.", "labels": [], "entities": []}, {"text": "We use efficient Hadoop-based look-up techniques to carryout this extraction step and to retrieve rules for generation ().", "labels": [], "entities": []}, {"text": "The average number of phrases extracted as a function of the size of the bag is shown in.", "labels": [], "entities": []}, {"text": "These are the phrasebased rules of our generation grammar.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CCG and GYRO BLEU scores.", "labels": [], "entities": [{"text": "CCG", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6315081119537354}, {"text": "GYRO", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9752554297447205}, {"text": "BLEU scores", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.8829768300056458}]}, {"text": " Table 2: Results in BLEU when rescoring the lat- tices generated by GYRO using various strategies.  Tuning conditions are marked by .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9957119226455688}, {"text": "GYRO", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9265257716178894}]}]}