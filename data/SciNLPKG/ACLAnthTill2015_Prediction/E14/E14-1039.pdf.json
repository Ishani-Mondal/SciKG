{"title": [{"text": "Fast Statistical Parsing with Parallel Multiple Context-Free Grammars", "labels": [], "entities": [{"text": "Statistical Parsing", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.771231085062027}]}], "abstractContent": [{"text": "We present an algorithm for incremental statistical parsing with Parallel Multiple Context-Free Grammars (PMCFG).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7113690376281738}]}, {"text": "This is an extension of the algorithm by An-gelov (2009) to which we added statistical ranking.", "labels": [], "entities": []}, {"text": "We show that the new algorithm is several times faster than other statistical PMCFG parsing algorithms on real-sized grammars.", "labels": [], "entities": [{"text": "PMCFG parsing", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.8074962794780731}]}, {"text": "At the same time the algorithm is more general since it supports non-binarized and non-linear grammars.", "labels": [], "entities": []}, {"text": "We also show that if we make the search heuristics non-admissible, the parsing speed improves even further, at the risk of returning sub-optimal solutions.", "labels": [], "entities": [{"text": "parsing", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.9622154831886292}]}], "introductionContent": [{"text": "In this paper we present an algorithm for incremental parsing using Parallel Multiple ContextFree Grammars (PMCFG) (.", "labels": [], "entities": []}, {"text": "This is anon context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity.", "labels": [], "entities": []}, {"text": "The algorithm is an extension of the algorithm by which adds statistical ranking.", "labels": [], "entities": []}, {"text": "This is a top-down algorithm, shown by to be similar to other top-down algorithms.", "labels": [], "entities": []}, {"text": "None of the other top-down algorithms are statistical.", "labels": [], "entities": []}, {"text": "The only statistical PMCFG parsing algorithms () all use bottom-up parsing strategies.", "labels": [], "entities": [{"text": "PMCFG parsing", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.8376925885677338}]}, {"text": "Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS).", "labels": [], "entities": []}, {"text": "In contrast, our algorithm naturally supports the full power of PMCFG.", "labels": [], "entities": [{"text": "PMCFG", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8801676630973816}]}, {"text": "By lifting these restrictions, we make it possible to experiment with novel grammar induction methods and to use statistical disambiguation for hand-crafted grammars.", "labels": [], "entities": []}, {"text": "By extending the algorithm with a statistical model, we allow the parser to explore only parts of the search space, when only the most probable parse tree is needed.", "labels": [], "entities": []}, {"text": "Our cost estimation is similar to the estimation for the Viterbi probability as in, except that we have to take into account that our grammar is not contextfree.", "labels": [], "entities": []}, {"text": "The estimation is both admissible and monotonic ( which guarantees that we always find a tree whose probability is the global maximum.", "labels": [], "entities": []}, {"text": "We also describe a variant with a nonadmissible estimation, which further improves the efficiency of the parser at the risk of returning a suboptimal parse tree.", "labels": [], "entities": []}, {"text": "We start with a formal definition of a weighted PMCFG in Section 2, and we continue with a presentation of our algorithm by means of a weighted deduction system in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we prove that our estimations are admissible and monotonic.", "labels": [], "entities": []}, {"text": "In Section 5 we calculate an estimate for the minimal inside probability for every category, and in Section 6 we discuss the nonadmissible heuristics.", "labels": [], "entities": []}, {"text": "Sections 7 and 8 describe the implementation and our evaluation, and the final Section 9 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We did an initial evaluation on the GF English resource grammar augmented with a large-coverage lexicon of 40 000 lemmas taken from the Oxford Advanced Learner's Dictionary.", "labels": [], "entities": [{"text": "GF English resource grammar", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.9504963308572769}, {"text": "Oxford Advanced Learner's Dictionary", "start_pos": 136, "end_pos": 172, "type": "DATASET", "confidence": 0.9537479043006897}]}, {"text": "In total the grammar has 44 000 productions.", "labels": [], "entities": []}, {"text": "The rule weights were trained from aversion of the Penn Treebank ( which was converted to trees compatible with the grammar.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9964464604854584}]}, {"text": "The trained grammar was tested on Penn Treebank sentences of length up to 35 tokens, and the parsing times were at most 7 seconds per sentence.", "labels": [], "entities": [{"text": "Penn Treebank sentences", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.9799029231071472}]}, {"text": "This initial test was run on a computer with a 2.4 GHz Intel Core i5 processor with 8 GB RAM.", "labels": [], "entities": []}, {"text": "This result was very encouraging, given the complexity of the grammar, so we decided to do a larger test and compare with an existing state-of-the-art statistical PMCFG parser.", "labels": [], "entities": []}, {"text": "Rparse) is a another state-of-the-art training and parsing system for PMCFG.", "labels": [], "entities": [{"text": "Rparse", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8508211374282837}, {"text": "PMCFG", "start_pos": 70, "end_pos": 75, "type": "TASK", "confidence": 0.5499868392944336}]}, {"text": "It is written in Java and developed at the Universities of T\u00fcbingen and D\u00fcsseldorf, Germany.", "labels": [], "entities": []}, {"text": "Rparse can be used for training probabilistic PMCFGs from discontinuous treebanks.", "labels": [], "entities": []}, {"text": "It can also be used for parsing new sentences with the trained grammars.", "labels": [], "entities": [{"text": "parsing new sentences", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.8827003836631775}]}, {"text": "In our evaluation we used Rparse to extract PM-CFG grammars from the discontinuous German Tiger Treebank ().", "labels": [], "entities": [{"text": "German Tiger Treebank", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.902750829855601}]}, {"text": "The reason for using this treebank is that the extracted grammars are non-context-free, and our parser is specifically made for such grammars.", "labels": [], "entities": []}, {"text": "In our evaluations we got the same general results regardless of the size of the grammar, so we only report the results from one of these runs.", "labels": [], "entities": []}, {"text": "In this particular example, we trained the grammar on 40 000 sentences from the Tiger Treebank with lengths up to 160 tokens.", "labels": [], "entities": [{"text": "Tiger Treebank", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.9748256504535675}]}, {"text": "We evaluated on: Training and testing data.", "labels": [], "entities": []}, {"text": "4 600 Tiger sentences, with a length of 5-60 tokens.", "labels": [], "entities": []}, {"text": "The exact numbers are shown in.", "labels": [], "entities": []}, {"text": "All tests were run on a computer with a 2.3 GHz Intel Core i7 processor with 16GB RAM.", "labels": [], "entities": []}, {"text": "As a comparison, Maier et al train on approximately 15 000 sentences from the Negra Treebank, and only evaluate on sentences of at most 40 tokens.", "labels": [], "entities": [{"text": "Negra Treebank", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9727067053318024}]}], "tableCaptions": [{"text": " Table 1: Training and testing data.", "labels": [], "entities": []}, {"text": " Table 1.  All tests were run on a computer with a 2.3 GHz  Intel Core i7 processor with 16GB RAM.  As a comparison, Maier et al", "labels": [], "entities": []}, {"text": " Table 2: Parsing quality for different values of h.", "labels": [], "entities": []}]}