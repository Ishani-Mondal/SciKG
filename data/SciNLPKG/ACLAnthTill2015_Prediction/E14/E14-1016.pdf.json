{"title": [{"text": "Leveraging Verb-Argument Structures to Infer Semantic Relations", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a methodology to infer implicit semantic relations from verb-argument structures.", "labels": [], "entities": []}, {"text": "An annotation effort shows implicit relations boost the amount of meaning explicitly encoded for verbs.", "labels": [], "entities": []}, {"text": "Experimental results with automatically obtained parse trees and verb-argument structures demonstrate that inferring implicit relations is a doable task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic extraction of semantic relations is an important step towards capturing the meaning of text.", "labels": [], "entities": [{"text": "Automatic extraction of semantic relations", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8152001380920411}]}, {"text": "Semantic relations explicitly encode links between concepts.", "labels": [], "entities": []}, {"text": "For example, in The accident left him a changed man, the 'accident' is the CAUSE of the man undergoing some 'change'.", "labels": [], "entities": [{"text": "CAUSE", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9957060217857361}]}, {"text": "A question answering system would benefit from detecting this relation when answering Why did he change?", "labels": [], "entities": [{"text": "question answering", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.7902941107749939}, {"text": "Why did he change?", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.6718591809272766}]}, {"text": "Extracting all semantic relations from text is a monumental task and is at the core of language understanding.", "labels": [], "entities": [{"text": "Extracting all semantic relations from text", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8734009861946106}, {"text": "language understanding", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.7269729375839233}]}, {"text": "In recent years, approaches that aim at extracting a subset of all relations have achieved great success.", "labels": [], "entities": []}, {"text": "In particular, previous research) focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments.", "labels": [], "entities": []}, {"text": "PropBank () is the corpus of reference for verb-argument relations.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9226638674736023}]}, {"text": "However, relations between a verb and its syntactic arguments are only a fraction of the relations present in texts.", "labels": [], "entities": []}, {"text": "Consider the statement [Mr. Brown] NP 1 succeeds [Joseph W. Hibben, who retired last August] NP 2 and its parse tree ().", "labels": [], "entities": []}, {"text": "Verbargument relations encode that NP 1 is the AGENT and NP 2 is the THEME of verb 'succeeds' (PropBank uses labels ARG 0 and ARG 1 ).", "labels": [], "entities": [{"text": "AGENT", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9908701777458191}, {"text": "THEME", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9977174997329712}, {"text": "PropBank", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.9319663643836975}]}, {"text": "Any semantic relation between 'succeeds' and concepts dominated in the parse tree by one of its syntactic arguments NP 1 or NP 2 , e.g., 'succeeds' oc- curred after 'last August', are missing.", "labels": [], "entities": []}, {"text": "Note that in this example, verb-argument structures encode that 'retired ' has TIME 'last August', and this knowledge could be exploited to infer the missing relation.", "labels": [], "entities": [{"text": "TIME", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9879654049873352}]}, {"text": "The work presented here stems from two observations: (1) verbs are semantically connected with concepts that are not direct syntactic arguments (henceforth, implicit relations); and (2) verb-argument structures can be leveraged to infer implicit relations.", "labels": [], "entities": []}, {"text": "This paper goes beyond verb-argument structures and targets implicit relations like the one depicted above.", "labels": [], "entities": []}, {"text": "TIME, LOCATION, MANNER, PURPOSE and CAUSE are inferred without imposing syntactic restrictions between their arguments: systems trained over PropBank do not attempt to extract these relations.", "labels": [], "entities": [{"text": "TIME", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9080753922462463}, {"text": "LOCATION", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9784446358680725}, {"text": "MANNER", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.8789765238761902}, {"text": "PURPOSE", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.990084171295166}, {"text": "CAUSE", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.939849853515625}, {"text": "PropBank", "start_pos": 141, "end_pos": 149, "type": "DATASET", "confidence": 0.9346891641616821}]}, {"text": "An annotation effort demonstrates implicit relations reveal as much as 30% of meaning on top of verb-argument structures.", "labels": [], "entities": []}, {"text": "The main contributions are: (1) empirical study of verb-argument structures and implicit relations in PropBank; (2) annotations of implicit relations on top of PropBank; (3) novel features extracted from verb-argument structures; and (4) experimental results with features derived from gold and automatically obtained linguistic information, showing implicit relations can be extracted in a realistic environment.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.9458122253417969}, {"text": "PropBank", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9579708576202393}]}], "datasetContent": [{"text": "Experiments were carried out using Support Vector Machines with RBF kernel as implemented in  LIBSVM (Chang and Lin, 2011).", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.8498331010341644}, {"text": "LIBSVM", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.8668193817138672}]}, {"text": "Parameters \u03b1 and \u03b3 were tuned by grid search using 10-fold cross validation over training instances.", "labels": [], "entities": []}, {"text": "Results are reported using features extracted from gold and automatic annotations.", "labels": [], "entities": []}, {"text": "Gold annotations are taken directly from the Penn TreeBank and PropBank.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9933095276355743}, {"text": "PropBank", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9173933267593384}]}, {"text": "Automatic annotations are obtained with Polaris (), a semantic parser that among others is trained with PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.9459269642829895}]}, {"text": "Results using gold (automatic) annotations are obtained with a model trained with gold (automatic) annotations.", "labels": [], "entities": []}, {"text": "presents per-relation and overall results.", "labels": [], "entities": []}, {"text": "In general terms, there is a decrease in performance when using automatic annotations.", "labels": [], "entities": []}, {"text": "The difference is most noticeable in recall and it is due to missing semantic roles, which in turn are often due to syntactic parsing errors.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.987097442150116}]}, {"text": "This is not surprising as in order for an implicit relation R(x, y) to be generated as potential and fed to the learning algorithm for classification, a semantic role R \u2032 (x \u2032 , y) must be extracted first (Algorithm 1).", "labels": [], "entities": []}, {"text": "However, using automatic annotations brings very little decrease in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.998878538608551}]}, {"text": "This leads to the conclusion that as long as 'y' is identified as a semantic role of some verb, even if it is mislabeled, one can still infer the right implicit relations.", "labels": [], "entities": []}, {"text": "Since results obtained with automatic parse trees and semantic roles area realistic estimation of performance, the remainder of the discussion focuses on those.", "labels": [], "entities": []}, {"text": "Results with gold annotations are provided for informational purposes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Argument modifiers in PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9478693604469299}]}, {"text": " Table 3: Counts of selected PropBank semantic  roles. Total number of predicates is 112,917.", "labels": [], "entities": []}, {"text": " Table 6: Complete feature set to determine whether a potential implicit semantic relation R(x, y) should  be inferred. Second column indicates the source: first or second argument (x, y), or their respective  predicate structures (x ps, y ps). Features in bold are novel and specially designed for our task.", "labels": [], "entities": []}, {"text": " Table 5: Number of potential implicit relations (in- stances) annotated and counts for each label. Total  number of instances is 30,084.", "labels": [], "entities": []}, {"text": " Table 7: Feature values when deciding if  R(succeeds, last summer) can be inferred from the  verb-argument structures in", "labels": [], "entities": []}, {"text": " Table 9: Results obtained with the test split using features extracted from gold and automatic annotations,  and using basic and predicate structures (ps) features. Statistical significance between F-measures using  basic and basic + predicate structures features is indicated with  *  (confidence 95%).", "labels": [], "entities": []}]}