{"title": [{"text": "Learning part-of-speech taggers with inter-annotator agreement loss", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.7334310710430145}]}], "abstractContent": [{"text": "In natural language processing (NLP) annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure consistent annotations.", "labels": [], "entities": []}, {"text": "However, annotation guidelines often make linguistically debatable and even somewhat arbitrary decisions, and inter-annotator agreement is often less than perfect.", "labels": [], "entities": []}, {"text": "While annotation projects usually specify how to deal with linguistically debatable phenomena, annotator disagreements typically still stem from these \"hard\" cases.", "labels": [], "entities": []}, {"text": "This indicates that some errors are more debatable than others.", "labels": [], "entities": []}, {"text": "In this paper, we use small samples of doubly-annotated part-of-speech (POS) data for Twitter to estimate annotation reliability and show how those metrics of likely inter-annotator agreement can be implemented in the loss functions of POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 236, "end_pos": 247, "type": "TASK", "confidence": 0.6840900778770447}]}, {"text": "We find that these cost-sensitive algorithms perform better across annotation projects and, more surprisingly, even on data annotated according to the same guidelines.", "labels": [], "entities": []}, {"text": "Finally, we show that POS tagging models sensitive to inter-annotator agreement perform better on the downstream task of chunking.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.8256605267524719}]}], "introductionContent": [{"text": "POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory.", "labels": [], "entities": []}, {"text": "The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features.", "labels": [], "entities": []}, {"text": "Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (, as well as for languages where available resources are mutually inconsistent.", "labels": [], "entities": []}, {"text": "Unfortunately, there is no grand unifying linguistic theory of how to analyze the structure of sentences.", "labels": [], "entities": []}, {"text": "While linguists agree on certain things, there is still a wide range of unresolved questions.", "labels": [], "entities": []}, {"text": "Consider the following sentence: (1) @GaryMurphyDCU of @DemMattersIRL will take part in a panel discussion on October 10th re the aftermath of #seanref . .", "labels": [], "entities": [{"text": "GaryMurphyDCU", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9416946172714233}]}, {"text": "While linguists will agree that in is a preposition, and panel discussion a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa.", "labels": [], "entities": []}, {"text": "Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun?", "labels": [], "entities": []}, {"text": "Some linguistic controversies maybe resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to or words ending in -ing.", "labels": [], "entities": []}, {"text": "However, standardized label sets have practical advantages in NLP (.", "labels": [], "entities": []}, {"text": "For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences.", "labels": [], "entities": []}, {"text": "The strategy inmost previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our main experiments, we use structured perceptron) with random corruptions using a drop-out rate of 0.1 for regularization, following S\u00f8gaard (2013a).", "labels": [], "entities": []}, {"text": "We use the LXMLS toolkit implementation 1 with default parameters.", "labels": [], "entities": [{"text": "LXMLS toolkit implementation", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.9185210466384888}]}, {"text": "We present learning curves across iterations, and only set parameters using held-out data for our downstream experiments.", "labels": [], "entities": []}, {"text": "We have seen that our POS tagging model improves over the baseline model on three out-ofsample test sets.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.7666002511978149}]}, {"text": "The question remains whether training a POS tagger that takes inter-annotator agreement scores into consideration is also effective on downstream tasks.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.7300620377063751}]}, {"text": "Therefore, we evaluate our best model, the CM-weighted learner, in two downstream tasks: shallow parsing-also known as chunking-and named entity recognition (NER).", "labels": [], "entities": [{"text": "shallow parsing-also known as chunking-and named entity recognition (NER)", "start_pos": 89, "end_pos": 162, "type": "TASK", "confidence": 0.8246546170928262}]}, {"text": "For the downstream evaluation, we used the baseline and CM models trained over 13 epochs, as they performed best on FOSTER-DEV (cf..", "labels": [], "entities": [{"text": "FOSTER-DEV", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.5564953088760376}]}, {"text": "Thus, parameters were optimized only on POS tagging data, not on the downstream evaluation tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.6516067832708359}]}, {"text": "We use a publicly available implementation of conditional random fields () for the chunking and NER experiments, and provide the POS tags from our CM learner as features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Downstream results on chunking. Overall  F1 score (All) as well as F1 for NP, VP and PP.", "labels": [], "entities": [{"text": "Downstream", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9863866567611694}, {"text": "chunking", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.9735574126243591}, {"text": "F1 score", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9862319529056549}, {"text": "F1", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9997038245201111}]}, {"text": " Table 1: Results across regularizers (after 13 epochs).", "labels": [], "entities": []}, {"text": " Table 3: Downstream results for named entity  recognition (F1 scores).", "labels": [], "entities": [{"text": "Downstream", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9073367714881897}, {"text": "named entity  recognition", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6244105597337087}, {"text": "F1 scores", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9757380485534668}]}]}