{"title": [{"text": "Some Experiments with a Convex IBM Model 2", "labels": [], "entities": []}], "abstractContent": [{"text": "Using a recent convex formulation of IBM Model 2, we propose anew initialization scheme which has some favorable comparisons to the standard method of initializing IBM Model 2 with IBM Model 1.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9061616261800131}, {"text": "initializing IBM Model 2", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.7512658387422562}]}, {"text": "Additionally , we derive the Viterbi alignment for the convex relaxation of IBM Model 2 and show that it leads to better F-Measure scores than those of IBM Model 2.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.9541035493214926}, {"text": "F-Measure", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9974321722984314}]}], "introductionContent": [{"text": "The IBM translation models are widely used in modern statistical translation systems.", "labels": [], "entities": [{"text": "IBM translation", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5679969489574432}, {"text": "statistical translation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7618655860424042}]}, {"text": "Unfortunately, apart from Model 1, the IBM models lead to non-convex objective functions, leading to methods (such as EM) which are not guaranteed to reach the global maximum of the log-likelihood function.", "labels": [], "entities": []}, {"text": "Ina recent paper, Simion et al. introduced a convex relaxation of IBM Model 2, I2CR-2, and showed that it has performance on par with the standard IBM Model 2 (.", "labels": [], "entities": []}, {"text": "In this paper we make the following contributions: \u2022 We explore some applications of I2CR-2.", "labels": [], "entities": []}, {"text": "In particular, we show how this model can be used to seed IBM Model 2 and compare the speed/performance gains of our initialization under various settings.", "labels": [], "entities": []}, {"text": "We show that initializing IBM Model 2 with aversion of I2CR-2 that uses large batch size yields a method that has similar run time to IBM Model 1 initialization and at times has better performance.", "labels": [], "entities": [{"text": "initializing IBM Model 2", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.6746341288089752}]}, {"text": "\u2022 We derive the Viterbi alignment for I2CR-2 and compare it directly with that of IBM Model 2.", "labels": [], "entities": [{"text": "Viterbi alignment", "start_pos": 16, "end_pos": 33, "type": "METRIC", "confidence": 0.7090587019920349}]}, {"text": "Previously, had compared IBM Model 2 and I2CR-2 by using IBM Model 2's Viterbi alignment rule, which is not necessarily the optimal alignment for I2CR-2.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.8959823846817017}, {"text": "Viterbi alignment rule", "start_pos": 71, "end_pos": 93, "type": "METRIC", "confidence": 0.5769136548042297}]}, {"text": "We show that by comparing I2CR-2 with IBM Model 2 by using each model's optimal Viterbi alignment the convex model consistently has a higher F-Measure.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9980521202087402}]}, {"text": "F-Measure is an important metric because it has been shown to be correlated with BLEU scores ().", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9755162000656128}, {"text": "BLEU scores", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9769102334976196}]}, {"text": "We adopt the notation introduced in) of having 1 m 2 n denote the training scheme of m IBM Model 1 EM iterations followed by initializing Model 2 with these parameters and running n IBM Model 2 EM iterations.", "labels": [], "entities": []}, {"text": "The notation EG m B 2 n means that we run m iterations of I2CR-2's EG algorithm () with batch size of B, initialize IBM Model 2 with I2CR-2's parameters, and then run n iterations of Model 2's EM.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe experiments using the I2CR-2 optimization problem combined with the stochastic EG algorithm (Simion et al., 2013) for parameter estimation.", "labels": [], "entities": [{"text": "I2CR-2 optimization", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7214639484882355}, {"text": "parameter estimation", "start_pos": 146, "end_pos": 166, "type": "TASK", "confidence": 0.6124206632375717}]}, {"text": "The experiments conducted here use a similar setup to those in ().", "labels": [], "entities": []}, {"text": "We first describe the data we use, and then describe the experiments we ran.", "labels": [], "entities": []}, {"text": "We first report the summary statistics on the test set using a model trained only in the English-French direction.", "labels": [], "entities": []}, {"text": "In these experiments we seeded IBM Model 2's parameters either with those of IBM Model 1 run for 5, 10 or 15 EM iterations or I2CR-2 run for 1 iteration of EG with a batch size of either B = 125 or 1250.", "labels": [], "entities": [{"text": "B", "start_pos": 187, "end_pos": 188, "type": "METRIC", "confidence": 0.9500126838684082}]}, {"text": "For uniform comparison, all of our implementations were written in C++ using STL/Boost containers.", "labels": [], "entities": [{"text": "STL/Boost containers", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.7047719806432724}]}, {"text": "There are several takeaways from our experiments, which are presented in.", "labels": [], "entities": []}, {"text": "We first note that with B = 1250 we get higher F-Measure and lower AER even though we useless training time: 5 iterations of IBM Model 1 EM training takes about 3.3 minutes, which is about the time it takes for 1 iteration of EG with a batch size of 125 (4.1 minutes); on the other hand, using B = 1250 takes EG 1.7 minutes and produces the best results across almost all iterations.", "labels": [], "entities": [{"text": "B", "start_pos": 24, "end_pos": 25, "type": "METRIC", "confidence": 0.954128623008728}, {"text": "F-Measure", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9971352815628052}, {"text": "AER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9995172023773193}]}, {"text": "Additionally, we note that the initial solution given to IBM Model 2 by running I2CR-2 for 1 iteration with B = 1250 is fairly strong and allows for further progress: IBM2 EM training improves upon this solution during the first few iterations.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.8562045296033224}, {"text": "B = 1250", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9398104747136434}]}, {"text": "We also note that this behavior is global: no IBM 1 initialization scheme produced subsequent solutions for IBM 2 with as low in AER or high in F-Measure.", "labels": [], "entities": [{"text": "IBM 1 initialization", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.5013327896595001}, {"text": "AER", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.9987319111824036}, {"text": "F-Measure", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9774609804153442}]}, {"text": "Finally, comparing which lists alignment statistics, we see that although the objective progression is similar throughout, the alignment quality is different.", "labels": [], "entities": []}, {"text": "To complement the above, we also ran intersection experiments.", "labels": [], "entities": []}, {"text": "Seeding IBM Model 2 by Model 1 and intersecting the alignments produced by the English-French and French-English models gave both AER and F-Measure which were better than those that we obtained by any seeding of IBM Model 2 with I2CR-2.", "labels": [], "entities": [{"text": "AER", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9997406601905823}, {"text": "F-Measure", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9961726069450378}]}, {"text": "However, there are still reasons why I2CR-2 would be useful in this context.", "labels": [], "entities": []}, {"text": "In particular, we note that I2CR-2 takes roughly half the time to progress to a better solution than IBM Model 1 run for 5 EM iterations.", "labels": [], "entities": []}, {"text": "Second, a possible remedy to the above loss in marginal improvement when taking intersections would be to use a more refined method for obtaining the joint alignment of the English-French and French-English models, such as \"grow-diagonal\").", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Objective results for the English \u2192 French IBM  Model 2 seeded with either uniform parameters, IBM  Model 1 ran for 5 EM iterations, or I2CR-2 ran for 1 iter- ation with either B = 125 or 1250. Iteration 0 denotes the  starting IBM 2 objective depending on the initialization.", "labels": [], "entities": [{"text": "IBM  Model 1", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.8585429588953654}]}, {"text": " Table 2. We first note  that with B = 1250 we get higher F-Measure and", "labels": [], "entities": [{"text": "B", "start_pos": 35, "end_pos": 36, "type": "METRIC", "confidence": 0.9934414625167847}, {"text": "F-Measure", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9989179372787476}]}, {"text": " Table 2: Results on the Hansards data for English \u2192  French IBM Model 2 seeded using different methods.  The first three columns are for a model seeded with IBM  Model 1 ran for 5, 10 or 15 EM iterations. The fourth  and fifth columns show results when we seed with I2CR- 2 ran for 1 iteration either with B = 125 or 1250. Iteration  0 denotes the starting statistics.", "labels": [], "entities": [{"text": "Hansards data", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9694195985794067}]}]}