{"title": [{"text": "Empirically-motivated Generalizations of CCG Semantic Parsing Learning Algorithms", "labels": [], "entities": []}], "abstractContent": [{"text": "Learning algorithms for semantic parsing have improved drastically over the past decade, as steady improvements on benchmark datasets have shown.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8884120583534241}]}, {"text": "In this paper we investigate whether they can generalize to a novel biomedical dataset that differs in important respects from the traditional geography and air travel benchmark datasets.", "labels": [], "entities": []}, {"text": "Empirical results for two state-of-the-art PCCG semantic parsers indicates that learning algorithms are sensitive to the kinds of semantic and syntactic constructions used in a domain.", "labels": [], "entities": [{"text": "PCCG semantic parsers", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.6807828148206075}]}, {"text": "In response , we develop a novel learning algorithm that can produce an effective semantic parser for geography, as well as a much better semantic parser for the biomedical dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing is the task of converting natural language utterances into formal representations of their meaning.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8334295153617859}]}, {"text": "In this paper, we consider in particular a grounded form of semantic parsing, in which the meaning representation language takes its logical constants from a given, fixed ontology.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.724761351943016}]}, {"text": "Several recent systems have demonstrated the ability to learn semantic parsers for domains like the GeoQuery database containing geography relations, or the ATIS database of air travel information.", "labels": [], "entities": [{"text": "GeoQuery database", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.911597341299057}, {"text": "ATIS database of air travel information", "start_pos": 157, "end_pos": 196, "type": "DATASET", "confidence": 0.8726159731547037}]}, {"text": "In these settings, existing systems can produce correct meaning representations with F1 scores approaching 0.9 ().", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.984214186668396}]}, {"text": "These benchmark datasets have supported a diverse and influential line of research into semantic parsing learning algorithms for sophisticated semantic constructions, with continuing advances inaccuracy.", "labels": [], "entities": [{"text": "semantic parsing learning", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.8144108454386393}]}, {"text": "However, the focus on these datasets leads to a natural question -do other natural datasets have similar syntax and semantics, and if not, can existing algorithms handle the variability in syntax and semantics?", "labels": [], "entities": []}, {"text": "In an effort to investigate and improve the generalization capacity of existing learning algorithms for semantic parsing, we develop a novel, natural experimental setting, and we test whether current semantic parsers generalize to the new setting.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.8121714293956757}]}, {"text": "For our datset, we use descriptions of clinical trials of experimental drugs in the United States, available from the U.S. National Institutes of Health . Much of the text in the description of these clinical trials can be mapped neatly onto biomedical ontologies, thus permitting grounded semantic analysis.", "labels": [], "entities": []}, {"text": "Crucially, the dataset was not designed specifically with semantic parsing or question-answering in mind, and as a result, it provides a natural source for the variety and complexity of utterances that humans use in this domain.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7781151235103607}]}, {"text": "As an added benefit, a successful semantic parser in this domain could yield a variety of useful bioinformatics applications by permitting comparisons between and across clinical trials using structured representations of the data, rather than unstructured text.", "labels": [], "entities": []}, {"text": "In this initial investigation of semantic parsing in this context, we ask: \u2022 Can existing semantic parsing learning algorithms handle the variety and complexity of the clinical trials dataset?", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7949764132499695}, {"text": "semantic parsing learning", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.8104885617891947}]}, {"text": "We show that two representative learning algorithms fare poorly on the clinical trials data: the best one achieves a 0.41 F1 in our tests.", "labels": [], "entities": [{"text": "F1", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9310613870620728}]}, {"text": "\u2022 What types of constructions are the major cause of errors on the clinical trials dataset, and can semantic parsers be extended to handle them?", "labels": [], "entities": []}, {"text": "While this initial investigation does not coverall types of constructions, we identify three important types of constructions that existing learning algorithms do not handle.", "labels": [], "entities": []}, {"text": "We propose anew learning algorithm that can handle these types of constructions, and we demonstrate empirically that the new algorithm produces a semantic parser that improves by over 23 points in F1 on the clinical trials dataset compared with existing parsers.", "labels": [], "entities": [{"text": "F1", "start_pos": 197, "end_pos": 199, "type": "METRIC", "confidence": 0.9972257018089294}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section provides background information on CCG and semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.7504884302616119}]}, {"text": "Section 3 describes the text and ontology that form the new clinical trials dataset for semantic parsing, as well as some of the problems that exising approaches have on this dataset.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.8965414464473724}, {"text": "exising", "start_pos": 143, "end_pos": 150, "type": "TASK", "confidence": 0.9678792953491211}]}, {"text": "Sections 4 describes our semantic parsing model, and learning and inference algorithms.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7321171015501022}]}, {"text": "Section 5 presents our experiments and results, and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Clinical trials are scientific experiments that measure the effects of a medical procedure, instrument, or product on humans.", "labels": [], "entities": []}, {"text": "Since September 2009 in the United States, any clinical trial that is funded by the federal government must make its results publicly available online at clinicaltrials.gov.", "labels": [], "entities": []}, {"text": "This site provides a wealth of biomedical text and structured data, which we use to produce a novel test set for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.8113883435726166}]}, {"text": "In our experiments, we test the generality of our learning algorithm by testing its ability to handle both GeoQuery and the Clinical Trials datasets.", "labels": [], "entities": [{"text": "Clinical Trials datasets", "start_pos": 124, "end_pos": 148, "type": "DATASET", "confidence": 0.9295339981714884}]}, {"text": "The clinical trials dataset is described above in Section 3.", "labels": [], "entities": [{"text": "clinical trials dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.5949731568495432}]}, {"text": "GeoQuery consists of a database of   2400 geographical entities, such as nations, rivers, and mountains, as well as 8 geography relations, such as the location of a mountain, and whether one state borders another.", "labels": [], "entities": [{"text": "GeoQuery", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8311933875083923}]}, {"text": "The text for semantic parsing consists of a set of 880 geography questions, labeled with a lambda-calculus representation of the sentence's meaning.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7757057249546051}]}, {"text": "We follow the procedure described by in splitting these sentences into training, development, and test sentences.", "labels": [], "entities": []}, {"text": "This dataset allows us to provide a comparison with other semantic parsers on a well-known dataset.", "labels": [], "entities": []}, {"text": "We measured performance based on exact-match of the full logical form, modulo re-ordering of arguments to symmetric relations (like conjunction and disjunction).", "labels": [], "entities": []}, {"text": "show the results of semantic parsers learned by the UBL, FUBL, and GLL learning algorithms on the GeoQuery and clinical trials datasets, respectively.", "labels": [], "entities": [{"text": "FUBL", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.7011045813560486}, {"text": "GeoQuery and clinical trials datasets", "start_pos": 98, "end_pos": 135, "type": "DATASET", "confidence": 0.7874607801437378}]}, {"text": "On the GeoQuery dataset, all three parsers perform very similarly, although GLL's performance is slightly worse.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.9551224410533905}]}, {"text": "However, on the clinical trials dataset, GLL significantly outperforms both UBL and FUBL in terms of precision, recall, and F1.", "labels": [], "entities": [{"text": "GLL", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9472994208335876}, {"text": "FUBL", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9975113868713379}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9996939897537231}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.999754011631012}, {"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9996011853218079}]}, {"text": "Of course, there clearly remain many syntactic and semantic constructions that none of these algorithms can currently handle, as all systems perform significantly worse on clinical trials than on GeoQuery.", "labels": [], "entities": []}, {"text": "Tables 6 shows the overall size of UBL's and GLL's learned lexicons, and shows the number of learned entries for selected lexical: For certain common and critical lexical items in the clinical trials dataset, GLL learns far fewer (but more general) lexical entries; for the word \"or\", GLL learns only 3.5% of the entries that UBL learns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: GLL performs comparably to two state- of-the-art learning algorithms for PCCG semantic  parsing on the benchmark GeoQuery dataset.", "labels": [], "entities": [{"text": "PCCG semantic  parsing", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.6561214824517568}, {"text": "GeoQuery dataset", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.9341529309749603}]}, {"text": " Table 5: On the clinical trials dataset, GLL outper- forms UBL and FUBL by more than 23 points in  F1, for a reduction in error (i.e., 1-F1) of nearly  40% over FUBL.", "labels": [], "entities": [{"text": "GLL", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.8090772032737732}, {"text": "FUBL", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.993883490562439}, {"text": "F1", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9995158910751343}, {"text": "error", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.5123248100280762}, {"text": "FUBL", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9702299237251282}]}, {"text": " Table 6: GLL learns a lexicon that is 27% smaller  than UBL's lexicon on clinical trials data.", "labels": [], "entities": [{"text": "GLL", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6798807978630066}]}, {"text": " Table 7: For certain common and critical lexical  items in the clinical trials dataset, GLL learns far  fewer (but more general) lexical entries; for the  word \"or\", GLL learns only 3.5% of the entries  that UBL learns.", "labels": [], "entities": []}]}