{"title": [{"text": "Improving Distributional Semantic Vectors through Context Selection and Normalisation", "labels": [], "entities": [{"text": "Improving Distributional Semantic Vectors", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8644571006298065}, {"text": "Context Selection", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7537192106246948}, {"text": "Normalisation", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.7246600985527039}]}], "abstractContent": [{"text": "Distributional semantic models (DSMs) have been effective at representing semantics at the word level, and research has recently moved onto building distributional representations for larger segments of text.", "labels": [], "entities": [{"text": "Distributional semantic models (DSMs)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6926831205685934}]}, {"text": "In this paper, we introduce novel ways of applying context selection and normalisa-tion to vary model sparsity and the range of values of the DSM vectors.", "labels": [], "entities": []}, {"text": "We show how these methods enhance the quality of the vectors and thus result in improved low dimensional and composed representations.", "labels": [], "entities": []}, {"text": "We demonstrate these effects on standard word and phrase datasets, and on anew definition retrieval task and dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantic models (DSMs) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector.", "labels": [], "entities": []}, {"text": "Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6859096686045328}, {"text": "query expansion", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7471656501293182}]}, {"text": "More recently, researchers have been exploring methods that combine word vectors to represent phrases () and sentences (.", "labels": [], "entities": []}, {"text": "In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt the vectors to particular lexical and compositional tasks.", "labels": [], "entities": []}, {"text": "The quality of the word vectors is generally assessed on standard datasets that consist of a list of word pairs and a corresponding list of gold standard scores.", "labels": [], "entities": []}, {"text": "These scores are gathered through an annotation task and reflect the similarity between the words as perceived by human judges).", "labels": [], "entities": []}, {"text": "Evaluation is conducted by comparing the word similarity predicted by the model with the gold standard using a correlation test such as Spearman's \u03c1.", "labels": [], "entities": []}, {"text": "While words, and perhaps some frequent shorter phrases, can be represented by distributional vectors learned through co-occurrence statistics, infrequent phrases and novel constructions are impossible to represent in that way.", "labels": [], "entities": []}, {"text": "The goal of compositional DSMs is to find methods of combining word vectors, or perhaps higher-order tensors, into a single vector that represents the meaning of the whole segment of text.", "labels": [], "entities": [{"text": "compositional DSMs", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7061146795749664}]}, {"text": "Elementary approaches to composition employ simple operations, such as addition and elementwise product, directly on the word vectors.", "labels": [], "entities": []}, {"text": "These have been shown to be effective for phrase similarity evaluation () and detection of anomalous phrases.", "labels": [], "entities": [{"text": "phrase similarity evaluation", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.8392934004465739}]}, {"text": "The methods that will be introduced in this paper can be applied to co-occurrence vectors to produce improvements on word similarity and compositional tasks with simple operators.", "labels": [], "entities": []}, {"text": "We chose to examine the use of sum, elementwise product, and circular convolution, because they are often used due to their simplicity, or as components of more complex models (.", "labels": [], "entities": []}, {"text": "The first method is context selection (CS), in which the top N highest weighted context words per vector are selected, and the rest of the values are discarded (by setting to zero).", "labels": [], "entities": [{"text": "context selection (CS)", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.8365281820297241}]}, {"text": "This technique is similar to the way that Explicit Semantic Analysis (ESA) () selects the number of topics that represent a word, and the word filtering approach in.", "labels": [], "entities": [{"text": "Explicit Semantic Analysis (ESA)", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.7120515505472819}, {"text": "word filtering", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.7590686976909637}]}, {"text": "It has the advantage of improving word representations and vector sum representations (for compositional tasks) while using vectors with fewer non-zero elements.", "labels": [], "entities": []}, {"text": "Programming languages often have efficient strategies for stor-ing these sparse vectors, leading to lower memory usage.", "labels": [], "entities": []}, {"text": "As an example of the resulting accuracy improvements, when vectors with up to 10,000 non-zero elements are reduced to a maximum of N 240 non-zero elements, the Spearman \u03c1 improves from 0.61 to 0.76 on a standard word similarity task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9990379810333252}, {"text": "Spearman \u03c1", "start_pos": 160, "end_pos": 170, "type": "METRIC", "confidence": 0.6668690741062164}, {"text": "word similarity task", "start_pos": 212, "end_pos": 232, "type": "TASK", "confidence": 0.7741968433062235}]}, {"text": "We also see an improvement when used in conjunction with further, standard dimensionality reduction techniques: the CS sparse vectors lead to reduced-dimensional representations that produce higher correlations with human similarity judgements than the original full vectors.", "labels": [], "entities": []}, {"text": "The second method is a weighted l 2 -normalisation of the vectors prior to application of singular value decomposition (SVD)) or compositional vector operators.", "labels": [], "entities": []}, {"text": "It has the effect of drastically improving SVD with 100 or fewer dimensions.", "labels": [], "entities": [{"text": "SVD", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.91654372215271}]}, {"text": "For example, we find that applying normalisation before SVD improves correlation from \u03c1 0.48 to \u03c1 0.70 for 20 dimensions, on the word similarity task.", "labels": [], "entities": [{"text": "correlation", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9863924384117126}, {"text": "word similarity task", "start_pos": 129, "end_pos": 149, "type": "TASK", "confidence": 0.8166836897532145}]}, {"text": "This is an essential finding as many more complex models of compositional semantics) work with tensor objects and require good quality low-dimensional representations of words in order to lower computational costs.", "labels": [], "entities": []}, {"text": "This technique also improves the performance of vector addition on texts of any length and vector elementwise product on shorter texts, on both the similarity and definitions tasks.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7433401048183441}]}, {"text": "The definition task and dataset are an additional contribution.", "labels": [], "entities": [{"text": "definition task", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8730224668979645}]}, {"text": "We produced anew dataset of words and their definitions, which is separated into nine parts, each consisting of definitions of a particular length.", "labels": [], "entities": []}, {"text": "This allows us to examine how compositional operators interact with CS and normalisation as the number of vector operations increases.", "labels": [], "entities": []}, {"text": "This paper is divided into three main sections.", "labels": [], "entities": []}, {"text": "Section 2 describes the construction of the word vectors that underlie all of our experiments and the two methods for adaptation of the vectors to specific tasks.", "labels": [], "entities": []}, {"text": "In Section 3 we assess the effects of CS and normalisation on standard word similarity datasets.", "labels": [], "entities": []}, {"text": "In Section 4 we present the compositional experiments on phrase data and our new definitions dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we investigate the effects of context selection and normalisation on the quality of word vectors using standard word similarity datasets.", "labels": [], "entities": []}, {"text": "The datasets consist of word pairs and a gold standard score that indicates the human judgement of the similarity between the words within each pair.", "labels": [], "entities": []}, {"text": "We calculated the similarity between word vectors for each pair and compared our results with the gold standard using Spearman correlation.: Values N learned on dev (X) also improve performance on the test data.", "labels": [], "entities": [{"text": "Spearman correlation.", "start_pos": 118, "end_pos": 139, "type": "METRIC", "confidence": 0.6499798595905304}]}, {"text": "Max \u03c1 indicates correlation at the values of N that lead to the highest Spearman correlation on the development data.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 72, "end_pos": 92, "type": "METRIC", "confidence": 0.7117770165205002}]}, {"text": "For each weighting scheme these are: 140 (tTest), 240 (PPMI), and 20 (Freq).", "labels": [], "entities": []}, {"text": "Full \u03c1 indicates the correlation when using full vectors without CS.", "labels": [], "entities": [{"text": "correlation", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9628705382347107}]}, {"text": "The cosine, Jaccard, and Lin similarity measures were all used to ensure the results reflect genuine effects of context selection, and not an artefact of any particular similarity measure.", "labels": [], "entities": [{"text": "Lin similarity", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.8938876986503601}]}, {"text": "The similarity measure and value of N were chosen, given a particular weighting scheme, to maximise correlation on the development part of the MEN data () (MENdev).", "labels": [], "entities": [{"text": "correlation", "start_pos": 100, "end_pos": 111, "type": "METRIC", "confidence": 0.9571946859359741}, {"text": "MEN data", "start_pos": 143, "end_pos": 151, "type": "DATASET", "confidence": 0.9086393415927887}]}, {"text": "Testing was performed on the remaining section of MEN and the entire WS353 dataset ().", "labels": [], "entities": [{"text": "MEN", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.9657734632492065}, {"text": "WS353 dataset", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9855759739875793}]}, {"text": "The MEN dataset consists of 3,000 word pairs rated for similarity, which is divided into a 2,000-pair development set and a 1,000-pair test set.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9302513003349304}]}, {"text": "WS353 consists only of 353 pairs, but has been consistently used as a benchmark word similarity dataset throughout the past decade.", "labels": [], "entities": [{"text": "WS353", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9254697561264038}]}, {"text": "Results shows how correlation varies with N for the MEN development data.", "labels": [], "entities": [{"text": "correlation", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9798138737678528}, {"text": "MEN development data", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.781194269657135}]}, {"text": "The peak performance for tTest is achieved when using around 140 top-ranked contexts per word, while for PPMI it is at N 240, and for Freq N 20.", "labels": [], "entities": []}, {"text": "The dramatic drop in performance is demonstrated when using all three similarity measures, although Jaccard seems particularly sensitive to the negative tTest weights that are introduced when lowerranked contexts are added to the vectors.", "labels": [], "entities": []}, {"text": "The remaining experiments only consider cosine similarity.", "labels": [], "entities": []}, {"text": "We also find that context selection improves correlation for tTest, PPMI, and the unweighted Freq vectors on the test data.", "labels": [], "entities": [{"text": "correlation", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.98299640417099}]}, {"text": "Moreover, the lower the correlation from the full vectors, the larger the improvement when using CS.", "labels": [], "entities": []}, {"text": "shows the effects of dimensionality reduction described in the following experiments.", "labels": [], "entities": []}, {"text": "We examine the performance of vectors augmented by CS and normalisation in two compositional tasks.", "labels": [], "entities": []}, {"text": "The first is an extension of the word similarity task to phrase pairs, using the dataset of.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7683249910672506}]}, {"text": "Each entry in the dataset consists of two phrases, each consisting of two words (in various syntactic relations, such as verb-object and adjective noun), and a gold standard score.", "labels": [], "entities": []}, {"text": "We combine the two word vectors into a single phrase vector using various operators described below.", "labels": [], "entities": []}, {"text": "We then calculate the similarity between the phrase vectors using cosine and compare the resulting scores against the gold standard using Spearman correlation.", "labels": [], "entities": [{"text": "similarity", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9771571755409241}, {"text": "Spearman correlation", "start_pos": 138, "end_pos": 158, "type": "METRIC", "confidence": 0.7065626382827759}]}, {"text": "The second task is our new definitions task where, again, word vectors from each definition are composed to form a single vector, which can then be compared for similarity with the target term.", "labels": [], "entities": []}, {"text": "We use PPMI-and tTest-weighted vectors at three CS cutoff points: the best chosen N from Section 3, the top third of the ranked contexts at N 3300, and the full vectors without CS at N 10000.", "labels": [], "entities": []}, {"text": "This gives us a range of values to examine, without directly tuning on this dataset.", "labels": [], "entities": []}, {"text": "For dimensionality reduction we consider vectors reduced with SVD to 100 and 700 dimensions.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6903874278068542}]}, {"text": "In some cases we exclude the results for SVD 700 because they are very close to the scores for unreduced vectors.", "labels": [], "entities": []}, {"text": "We experiment with 3 values of D from t512, 1024, 4096u for the RI vectors.", "labels": [], "entities": [{"text": "D", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.9862788319587708}]}, {"text": "Operators To combine distributional vectors into a single-vector sentence representation, we use a representative set of methods from.", "labels": [], "entities": []}, {"text": "In particular, we use vector addition, elementwise (Hadamard) product, Kronecker product, and circular convolution, which are de-fined as follows for two word vectors x, y: Repeated application of the Sum operation adds contexts for each of the words that occur in a phrase, which maintains (and mixes) any noisy parts of the component word vectors.", "labels": [], "entities": []}, {"text": "Our intention was that use of the CS vectors would lead to less noisy word vectors and hence less noisy phrase and sentence vectors.", "labels": [], "entities": []}, {"text": "The Prod operator, on the other hand, provides a phrase or sentence representation consisting only of the contexts that are common to all of the words in the sentence (since zeros in any of the word vectors lead to zeros in the same position in the sentence vector).", "labels": [], "entities": []}, {"text": "This effect is particularly problematic for rare words which may have sparse vectors, leading to a sparse vector for the sentence.", "labels": [], "entities": []}, {"text": "We address the sparsity problem through the use of dimensionality reduction, which produces more dense vectors.", "labels": [], "entities": []}, {"text": "Kron, the Kronecker (or tensor) product of two vectors, produces a matrix (second order tensor) whose diagonal matches the result of the Prod operation, but whose off-diagonal entries are all the other products of elements of the two vectors.", "labels": [], "entities": []}, {"text": "We only apply Kron to SVD-reduced vectors, and to compare two matrices we turn them into vectors by concatenating matrix rows, and use cosine similarity on the resulting vectors.", "labels": [], "entities": []}, {"text": "While in the more complex, type-driven methods () tensors represent functions, and off-diagonal entries have a particular transformational interpretation as part of a linear map, the significance of the offdiagonal elements is difficult to interpret in our setting, apart from their role as encoders of the order of operands.", "labels": [], "entities": []}, {"text": "We only examine Kron as the unencoded version of the Conv operator to see how the performance is affected by the random indexing and the modular summation by which Conv differs from Kron.", "labels": [], "entities": []}, {"text": "We cannot use Kron for combining more than two words as the size of the resulting tensor grows exponentially with the num-1 Sparsity is a problem that maybe addressable through smoothing), although we do not investigate that avenue in this paper.", "labels": [], "entities": [{"text": "Sparsity", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9832625389099121}]}, {"text": "Conv also differs from Kron in that it is commutative, unless one of the operands is permuted.", "labels": [], "entities": []}, {"text": "In this paper we do not permute the operands.", "labels": [], "entities": []}, {"text": "ber of vector operations, but we can use Conv as an encoded alternative as it results in a vector of the same dimension as the two operands.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Values N learned on dev (X) also improve  performance on the test data. Max \u03c1 indicates cor- relation at the values of N that lead to the high- est Spearman correlation on the development data.  For each weighting scheme these are: 140 (tTest),  240 (PPMI), and 20 (Freq). Full \u03c1 indicates the  correlation when using full vectors without CS.", "labels": [], "entities": [{"text": "Max \u03c1", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9481905400753021}, {"text": "cor- relation", "start_pos": 98, "end_pos": 111, "type": "METRIC", "confidence": 0.8557711640993754}]}, {"text": " Table 3: Behaviour of vector operators with PPMI  vectors on ML2010 (Spearman correlation). Val- ues for normalised vectors in parentheses.", "labels": [], "entities": [{"text": "ML2010", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.961023211479187}, {"text": "Spearman correlation)", "start_pos": 70, "end_pos": 91, "type": "METRIC", "confidence": 0.773826003074646}, {"text": "Val- ues", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9590665300687155}]}, {"text": " Table 4: Number of definitions per dataset.", "labels": [], "entities": []}, {"text": " Table 5: Best settings for operators calculated  from the highest average MRR across all the  datasets, with and without normalisation. The  results for vectors with no normalisation or CS  are: Sum -P@1=0.1567, MRR=0.2624; Prod - P@1=0.0147, MRR=0.0542; Conv P@1=0.0027,  MRR=0.0192.", "labels": [], "entities": [{"text": "Sum -P@1", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9334135770797729}, {"text": "MRR", "start_pos": 213, "end_pos": 216, "type": "METRIC", "confidence": 0.9129586219787598}, {"text": "Prod - P@1", "start_pos": 225, "end_pos": 235, "type": "METRIC", "confidence": 0.7731857419013977}, {"text": "MRR", "start_pos": 244, "end_pos": 247, "type": "METRIC", "confidence": 0.9509910345077515}, {"text": "MRR", "start_pos": 274, "end_pos": 277, "type": "METRIC", "confidence": 0.9884916543960571}]}]}