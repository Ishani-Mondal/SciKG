{"title": [{"text": "Statistical Script Learning with Multi-Argument Events", "labels": [], "entities": [{"text": "Statistical Script Learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7814768552780151}]}], "abstractContent": [{"text": "Scripts represent knowledge of stereotyp-ical event sequences that can aid text understanding.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8477031588554382}]}, {"text": "Initial statistical methods have been developed to learn probabilis-tic scripts from raw text corpora; however , they utilize a very impoverished representation of events, consisting of a verb and one dependent argument.", "labels": [], "entities": []}, {"text": "We present a script learning approach that employs events with multiple arguments.", "labels": [], "entities": []}, {"text": "Unlike previous work, we model the interactions between multiple entities in a script.", "labels": [], "entities": []}, {"text": "Experiments on a large corpus using the task of inferring held-out events (the \"narrative cloze evaluation\") demonstrate that mod-eling multi-argument events improves pre-dictive accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9317304491996765}]}], "introductionContent": [{"text": "Scripts encode knowledge of stereotypical events, including information about their typical ordered sequences of sub-events and corresponding arguments (.", "labels": [], "entities": []}, {"text": "The classic example is the \"restaurant script,\" which encodes knowledge about what normally happens when dining out.", "labels": [], "entities": []}, {"text": "Such knowledge can be used to improve text understanding by supporting inference of missing actions and events, as well as resolution of lexical and syntactic ambiguities and anaphora.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.841249942779541}, {"text": "resolution of lexical and syntactic ambiguities and anaphora", "start_pos": 123, "end_pos": 183, "type": "TASK", "confidence": 0.7665693685412407}]}, {"text": "For example, given the text \"John went to Olive Garden and ordered lasagna.", "labels": [], "entities": []}, {"text": "He left a big tip and left,\" an inference that scripts would ideally allow us to make is \"John ate lasagna.\"", "labels": [], "entities": []}, {"text": "There is a small body of recent research on automatically learning probabilistic models of scripts from large corpora of raw text ().", "labels": [], "entities": []}, {"text": "However, this work uses a very impoverished representation of events that only includes a verb and a single dependent entity.", "labels": [], "entities": []}, {"text": "We propose a more complex multiargument event representation for use in statistical script models, capable of directly capturing interactions between multiple entities.", "labels": [], "entities": []}, {"text": "We present a method for learning such a model, and provide experimental evidence that modeling entity interactions allows for better prediction of events in documents, compared to previous single-entity \"chain\" models.", "labels": [], "entities": []}, {"text": "We also compare to a competitive baseline not used in previous work, and introduce a novel evaluation metric.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow previous work in using the narrative cloze task to evaluate statistical scripts).", "labels": [], "entities": []}, {"text": "The task is as follows: given a sequence of events a 1 , . .", "labels": [], "entities": []}, {"text": ", an from a document, holdout some event a p and attempt to predict that event, given the other events in the sequence.", "labels": [], "entities": []}, {"text": "As we cannot automatically evaluate the prediction of truly unmentioned events in a document, this evaluation acts as a straightforward proxy.", "labels": [], "entities": []}, {"text": "In the aforementioned work, the cloze task is to guess a pair event, given the other events in which the held-out pair's entity occurs.", "labels": [], "entities": []}, {"text": "In Section 4.2.2, we evaluate directly on this task of guessing pair events.", "labels": [], "entities": []}, {"text": "However, in Section 4.2.1, we evaluate on the task of guessing a multi-argument event, given all other events in a document and the entities mentioned in the held-out event.", "labels": [], "entities": []}, {"text": "This is, we argue, the most natural way to adapt the cloze evaluation to the multi-argument event setting: instead of guessing a held-out pair event based on the other events involving its lone entity, we will guess a held-out multi-argument event based on the other events involving any of its entities.", "labels": [], "entities": []}, {"text": "A document may contain arbitrarily many entities.", "labels": [], "entities": []}, {"text": "The script model described in Section 3.2.1, however, only models events involving entities from a closed class of four variables {x, y, z, O}.", "labels": [], "entities": []}, {"text": "We therefore rewrite entities in a document's sequences of events to the variables {x, y, z, O} in away that maintains all pairwise relationships between the held-out event and others.", "labels": [], "entities": []}, {"text": "That is, if the held-out event shares an entity with another event, this remains true after rewriting.", "labels": [], "entities": []}, {"text": "We perform entity rewriting relative to a single held-out event, proceeding as follows: \u2022 Any entity in the held-out event that is mentioned at least once in another event gets rewritten consistently to one of x, y, or z, such that distinct entities never get rewritten to the same variable.", "labels": [], "entities": [{"text": "entity rewriting", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7344241142272949}]}, {"text": "\u2022 Any entity mentioned only in the held-out event is rewritten as O.", "labels": [], "entities": [{"text": "O", "start_pos": 66, "end_pos": 67, "type": "METRIC", "confidence": 0.7765559554100037}]}, {"text": "\u2022 All entities not present in the held-out event are rewritten as O.", "labels": [], "entities": []}, {"text": "This simplification removes structure from the original sequence, but retains the important pairwise entity relationships between the held-out event and the other events.", "labels": [], "entities": []}, {"text": "For each document, we use the Stanford dependency parser) to get syntactic information about the document; we then use the Stanford coreference resolution engine () to get (noisy) equivalence classes of coreferent noun phrases in a document.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.6932810544967651}]}, {"text": "We train on approximately 1.1M articles from years 1994-2006 of the NYT portion of the Gigaword Corpus, Third Edition (, holding out a random subset of the articles from 1999 for development and test sets.", "labels": [], "entities": [{"text": "NYT portion of the Gigaword Corpus", "start_pos": 68, "end_pos": 102, "type": "DATASET", "confidence": 0.9293467700481415}]}, {"text": "Our test set consists of 10,000 randomly selected heldout events, and our development set is 500 disjoint randomly selected held-out events.", "labels": [], "entities": []}, {"text": "To remove duplicate documents, we hash the first 500 characters of each article and remove any articles with hash collisions.", "labels": [], "entities": []}, {"text": "We use add-one smoothing on all joint probabilities.", "labels": [], "entities": []}, {"text": "To reduce the size of our model, we remove all events that occur fewer than 50 times.", "labels": [], "entities": []}, {"text": "We evaluate performance using the following two metrics: 1.", "labels": [], "entities": []}, {"text": "Recall at 10: Following Jans et al.", "labels": [], "entities": []}, {"text": "(2012), we measure performance by outputting the top 10 guesses for each held-out event and calculating the percentage of such lists con- We use version 1.3.4 of the Stanford CoreNLP system.", "labels": [], "entities": [{"text": "Stanford CoreNLP system", "start_pos": 166, "end_pos": 189, "type": "DATASET", "confidence": 0.8810583353042603}]}, {"text": "3 A manual inspection reveals that the majority of these removed events come from noisy text or parse errors.", "labels": [], "entities": []}, {"text": "This value will be between 0 and 1, with 1 indicating perfect system performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for multi-argument events.", "labels": [], "entities": []}, {"text": " Table 2: Results for pair events.", "labels": [], "entities": []}]}