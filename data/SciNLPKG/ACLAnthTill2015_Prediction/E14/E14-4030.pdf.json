{"title": [], "abstractContent": [{"text": "Transition-based dependency parsing systems can utilize rich feature representations.", "labels": [], "entities": [{"text": "Transition-based dependency parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.621335486570994}]}, {"text": "However, in practice, features are generally limited to combinations of lexical tokens and part-of-speech tags.", "labels": [], "entities": []}, {"text": "In this paper, we investigate richer features based on supertags, which represent lexical templates extracted from dependency structure annotated corpus.", "labels": [], "entities": []}, {"text": "First, we develop two types of supertags that encode information about head position and dependency relations in different levels of granu-larity.", "labels": [], "entities": []}, {"text": "Then, we propose a transition-based dependency parser that incorporates the predictions from a CRF-based supertagger as new features.", "labels": [], "entities": []}, {"text": "On standard English Penn Treebank corpus, we show that our su-pertag features achieve parsing improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% incomplete tree accuracy.", "labels": [], "entities": [{"text": "English Penn Treebank corpus", "start_pos": 12, "end_pos": 40, "type": "DATASET", "confidence": 0.8573405295610428}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9030770063400269}]}], "introductionContent": [{"text": "One significant advantage of transition-based dependency parsing) is that they can utilize rich feature representations.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.5906426906585693}]}, {"text": "However, in practice, current state-of-the-art parsers generally utilize only features that are based on lexical tokens and part-of-speech (POS) tags.", "labels": [], "entities": []}, {"text": "In this paper, we argue that more complex features that capture finegrained syntactic phenomenon and long-distance dependencies represent a simple and effective way to improve transition-based dependency parsers.", "labels": [], "entities": []}, {"text": "We focus on defining supertags for English dependency parsing.", "labels": [], "entities": [{"text": "English dependency parsing", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.5840409199396769}]}, {"text": "Supertags, which are lexical templates extracted from dependency structure annotated corpus, encode linguistically rich information that imposes complex constraints in a local context ().", "labels": [], "entities": []}, {"text": "While supertags have been used in frameworks based on lexicalized grammars, e.g. Lexicalized TreeAdjoining Grammar (LTAG), Head-driven Phrase Structure Grammar (HPSG) and Combinatory Categorial Grammar (CCG), they have scarcely been utilized for dependency parsing so far.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 246, "end_pos": 264, "type": "TASK", "confidence": 0.8171719908714294}]}, {"text": "Previous work by demonstrate that supertags improve German dependency parsing under a Weighted Constraint Dependency Grammar (WCDG).", "labels": [], "entities": [{"text": "German dependency parsing", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.5379090507825216}]}, {"text": "Recent work by show that supertags based on CCG lexicon improves transition-based dependency parsing for Hindi.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 65, "end_pos": 100, "type": "TASK", "confidence": 0.6175464888413748}]}, {"text": "In particular, they argue that supertags can improve long distance dependencies (e.g. coordination, relative clause) in a morphologicallyrich free-word-order language.", "labels": [], "entities": []}, {"text": "define supertags that incorporate that longdistance dependency information for the purpose of HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.7813831865787506}]}, {"text": "All these works suggest the promising synergy between dependency parsing and supertagging.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8184235095977783}]}, {"text": "Our main contributions are: (1) an investigation of supertags that work well for English dependency parsing, and (2) a novel transition-based parser that effectively utilizes such supertag features.", "labels": [], "entities": [{"text": "English dependency parsing", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.5957988003889719}]}, {"text": "In the following, we first describe our supertag design (Section 2) and parser (Section 3).", "labels": [], "entities": []}, {"text": "Supertagging and parsing experiments on the Penn Treebank () are shown in Section 4.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.971933901309967}, {"text": "Penn Treebank", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9933860599994659}]}, {"text": "We show that using automatically predicted supertags, our parser can achieve improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% incomplete tree accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.96109938621521}]}], "datasetContent": [{"text": "To evaluate the effectiveness of supertags as features, we perform experiments on the Penn Treebank (PTB), converted into dependency format with Penn2Malt . Adopting standard approach, we split PTB sections 2-21 for training, section 22 for development and 23 for testing.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.9755157470703125}, {"text": "Penn2Malt", "start_pos": 145, "end_pos": 154, "type": "DATASET", "confidence": 0.987235963344574}]}, {"text": "We assigned POS-tags to the training data by ten-fold jackknifing following.", "labels": [], "entities": []}, {"text": "Development and test sets are automatically tagged by the tagger trained on the training set.", "labels": [], "entities": []}, {"text": "We use the training data set to train a supertagger of each model using Conditional Random Fields (CRF) and the test data set to evaluate the accuracies.", "labels": [], "entities": []}, {"text": "We use version 0.12 of CRFsuite 2 for our CRF implementation.", "labels": [], "entities": []}, {"text": "First-order transitions, and word/POS of uni, bi and trigrams in a 7-word window surrounding the target word are used as features.", "labels": [], "entities": []}, {"text": "shows the result of the supertagging accuracies.", "labels": [], "entities": []}, {"text": "The supertag accuracies are around 87-88% for both models, suggesting that most of the supertags can be effectively learned by standard CRFs.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9229351878166199}]}, {"text": "The tagger takes 0.001 and 0.005 second per sentence for Model 1 and 2 respectively.", "labels": [], "entities": []}, {"text": "In our error analysis, we find it is challenging to assign correct supertags for obligatory dependents of Model 2.", "labels": [], "entities": []}, {"text": "In the test set, the number of the supertags encoding obligatory dependents is 5432 and its accuracy is 74.61% (The accuracy of the corresponding supertags in Model 1 is 82.18%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9996067881584167}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9988679885864258}]}, {"text": "Among them, it is especially difficult to predict the supertags encoding obligatory dependents with ahead information of subordination conjunction 'SBAR', such as 'SBAR/L+SUB/L PRD/R'.", "labels": [], "entities": []}, {"text": "The accuracy of such supertags is around 60% (e.g., the accuracy of a supertag 'SBAR/L+SUB/L PRD/R' is 57.78%), while the supertags encoding dependents with a la-: Accuracies for English dependency parsing on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994221925735474}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9992522597312927}, {"text": "SBAR/L+SUB/L PRD/R", "start_pos": 80, "end_pos": 98, "type": "METRIC", "confidence": 0.5347150713205338}, {"text": "Accuracies", "start_pos": 164, "end_pos": 174, "type": "METRIC", "confidence": 0.9853031635284424}, {"text": "English dependency parsing", "start_pos": 179, "end_pos": 205, "type": "TASK", "confidence": 0.5961339871088663}]}, {"text": "UAS = unlabeled attachment score; Root = root attachment score; Complete = the percentage of sentences in which all tokens were assigned their correct heads.", "labels": [], "entities": [{"text": "UAS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.968079686164856}, {"text": "root attachment score", "start_pos": 41, "end_pos": 62, "type": "METRIC", "confidence": 0.6768402755260468}, {"text": "Complete", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9872463941574097}]}, {"text": "bel 'VC' are assigned almost correctly (e.g., the accuracy of 'VC/L+VC/R' is 97.41%).", "labels": [], "entities": [{"text": "accuracy of 'VC/L+VC/R'", "start_pos": 50, "end_pos": 73, "type": "METRIC", "confidence": 0.8117423545230519}]}, {"text": "A verb within a subordinating clause usually has the subordinating conjunction as its head and it tends to be long-range dependency, which is harder to predict.", "labels": [], "entities": []}, {"text": "'VC' represents verb complements.", "labels": [], "entities": []}, {"text": "A gerund and a past participle is often a dependent of the immediate front verb, so it is not so difficult to identify the dependency relation.", "labels": [], "entities": []}, {"text": "First, we evaluate the effectiveness of the feature templates proposed in Section 3.", "labels": [], "entities": []}, {"text": "Following the same procedure as our POS tagger, we first assign supertags to the training data by ten-fold jackknifing, then train our Easy-First dependency parser on these predicted supertags.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.7049546241760254}]}, {"text": "For development and test sets, we assign supertags based on a supertagger trained on the whole training data.", "labels": [], "entities": []}, {"text": "shows the effect of new supertag features on the development data.", "labels": [], "entities": []}, {"text": "We start with the baseline features, and incrementally add the unigrams, bigrams, and head-dependent feature templates.", "labels": [], "entities": []}, {"text": "For Model 1 we observe that adding unigram features improve the baseline UAS slightly by 0.34% while additionally adding bigram features give larger improvements of 0.78%.", "labels": [], "entities": [{"text": "UAS", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.7025957703590393}]}, {"text": "On the other hand, for Model 2 unigram features make bigger contribution on improvements by 0.51% than bigram ones 0.32%.", "labels": [], "entities": []}, {"text": "One possible explanation is that because each supertag of Model 2 encodes richer syntactic information, an individual tag can make bigger contribution on improvements than Model 1 as a unigram feature.", "labels": [], "entities": []}, {"text": "However, since supertags of Model 2 can be erroneous and noisy combination of multiple supertags, such as bigram features, can propagate errors.", "labels": [], "entities": []}, {"text": "Using all features, the accuracy of the accuracy of Model 2 improved further by 0.20%, while Model 1 dropped by 0.15%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996985197067261}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9996172189712524}]}, {"text": "It is unclear why Model 1 accuracy dropped, but one hypothesis is that coarse-grained supertags may conflate some head-dependent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9910711050033569}]}, {"text": "The development set UAS for combinations of all features are 91.22% (Model 1) and 91.28% (Model 2), corresponding to 0.97% and 1.03% improvement over the baseline.", "labels": [], "entities": [{"text": "UAS", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9983571171760559}]}, {"text": "Next, we show the parsing accuracies on the test set, using all unigram, bigram, and headdependents supertag features.", "labels": [], "entities": []}, {"text": "The UAS 3 , Root attachment scores, and Complete accuracy are shown in.", "labels": [], "entities": [{"text": "UAS 3", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9421656429767609}, {"text": "Root attachment scores", "start_pos": 12, "end_pos": 34, "type": "METRIC", "confidence": 0.7719752192497253}, {"text": "Complete", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9976886510848999}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9376099705696106}]}, {"text": "Both Model 1 and 2 outperform the baseline in all metrics.", "labels": [], "entities": []}, {"text": "UAS improvements for both models are statistically significant under the McNemar test, p < 0.05 (difference between Model 1 and 2 is not significant).", "labels": [], "entities": [{"text": "UAS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9396896958351135}, {"text": "McNemar test", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.7895252108573914}]}, {"text": "Notably, Model 1 achieves parsing improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% incomplete accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9747937321662903}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.8803449273109436}]}, {"text": "Comparing Model 1 to baseline, attachment improvements binned by distance to head are as follows: +0.54 F1 for distance 1, +0.81 for distance 2, +2.02 for distance 3 to 6, +2.95 for distance 7 or more, implying supertags are helpful for long distance dependencies.", "labels": [], "entities": [{"text": "attachment", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9438124895095825}, {"text": "F1", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9961456060409546}]}], "tableCaptions": [{"text": " Table 3: Supertag accuracy evaluated on develop- ment and test set. Dev = development set, PTB 22;  Test = test set, PTB 23", "labels": [], "entities": [{"text": "Supertag", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.7000371217727661}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8833855390548706}, {"text": "PTB", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.7843330502510071}, {"text": "PTB", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.8548881411552429}]}, {"text": " Table 4: Unlabeled attachment scores (UAS) on  the development set for each feature template.", "labels": [], "entities": [{"text": "Unlabeled attachment scores (UAS)", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.8323776225248972}]}, {"text": " Table 5: Accuracies for English dependency pars- ing on the test set. UAS = unlabeled attachment  score; Root = root attachment score; Complete =  the percentage of sentences in which all tokens  were assigned their correct heads.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9980252981185913}, {"text": "UAS = unlabeled attachment  score", "start_pos": 71, "end_pos": 104, "type": "METRIC", "confidence": 0.7394458830356598}, {"text": "Root = root attachment score", "start_pos": 106, "end_pos": 134, "type": "METRIC", "confidence": 0.6744313955307006}, {"text": "Complete", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9748230576515198}]}]}