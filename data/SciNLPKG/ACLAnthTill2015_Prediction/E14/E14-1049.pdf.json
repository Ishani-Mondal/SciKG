{"title": [{"text": "Improving Vector Space Word Representations Using Multilingual Correlation", "labels": [], "entities": [{"text": "Improving Vector Space Word Representations", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8869202971458435}]}], "abstractContent": [{"text": "The distributional hypothesis of Harris (1954), according to which the meaning of words is evidenced by the contexts they occur in, has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora.", "labels": [], "entities": []}, {"text": "This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually.", "labels": [], "entities": [{"text": "canonical correlation analysis (CCA)", "start_pos": 138, "end_pos": 174, "type": "TASK", "confidence": 0.6823431452115377}]}, {"text": "We evaluate the resulting word representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing.", "labels": [], "entities": [{"text": "Data-driven learning of vector-space word embeddings", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.6974347680807114}, {"text": "natural language processing", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.6371172666549683}]}, {"text": "Using cooccurrence statistics from a large corpus of text, 1 it is possible to construct high-quality semantic vectors -as judged by both correlations with human judgements of semantic relatedness) and as features for downstream applications.", "labels": [], "entities": []}, {"text": "The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis, famously articulated by Firth Related approaches use the internal representations from neural network models of word sequences or continuous bags-of-context wordsels () to arrive at vector representations that likewise capture cooccurence tendencies and meanings.", "labels": [], "entities": []}, {"text": "as You shall know a word by the company it keeps.", "labels": [], "entities": []}, {"text": "Although there is much evidence in favor of the distributional hypothesis, in this paper we argue for incorporating translational context when constructing vector space semantic models (VSMs).", "labels": [], "entities": []}, {"text": "Simply put: knowing how words translate is a valuable source of lexico-semantic information and should lead to better VSMs.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 118, "end_pos": 122, "type": "TASK", "confidence": 0.9064269661903381}]}, {"text": "Parallel corpora have long been recognized as valuable for lexical semantic applications, including identifying word senses) and paraphrase and synonymy relationships).", "labels": [], "entities": []}, {"text": "The latter work (which we build on) shows that if different words or phrases in one language often translate into a single word or phrase type in a second language, this is good evidence that they are synonymous.", "labels": [], "entities": []}, {"text": "To illustrate: the English word forms aeroplane, airplane, and plane are observed to translate into the same Hindi word: (vaayuyaan).", "labels": [], "entities": []}, {"text": "Thus, even if we did not know the relationship between the English words, this translation fact is evidence that they all have the same meaning.", "labels": [], "entities": []}, {"text": "How can we exploit information like this when constructing VSMs?", "labels": [], "entities": []}, {"text": "We propose a technique that first constructs independent VSMs in two languages and then projects them onto a common vector space such that translation pairs (as determined by automatic word alignments) should be maximally correlated ( \u00a72).", "labels": [], "entities": []}, {"text": "We review latent semantic analysis (LSA), which serves as our monolingual VSM baseline ( \u00a73), and a suite of standard evaluation tasks that we use to measure the quality of the embeddings ( \u00a74).", "labels": [], "entities": [{"text": "latent semantic analysis (LSA)", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7536266346772512}]}, {"text": "We then turn to experiments.", "labels": [], "entities": []}, {"text": "We first show that our technique leads to substantial improvements over monolingual LSA ( \u00a75), and then examine how our technique fares with vectors learned using two different neural networks, one that models word sequences and a second that models bags-of-context words.", "labels": [], "entities": []}, {"text": "We observe substantial improvements over the sequential model using multilingual evidence but more mixed results relative to using the bagsof-contexts model ( \u00a76).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the quality of our word vector representations on a number of tasks that test how well they capture both semantic and syntactic aspects of the representations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman's correlation (left) and accuracy (right) on different tasks.", "labels": [], "entities": [{"text": "correlation", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.528248131275177}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.999380350112915}]}, {"text": " Table 2: Spearman's correlation (left) and accuracy (right) on different tasks. Bold indicates best result  across all vector types. Mono: monolingual and Multi: multilingual.", "labels": [], "entities": [{"text": "correlation", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.692894458770752}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9994456171989441}]}]}