{"title": [{"text": "A Latent Variable Model for Discourse-aware Concept and Entity Disambiguation", "labels": [], "entities": [{"text": "Entity Disambiguation", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.75051149725914}]}], "abstractContent": [{"text": "This paper takes a discourse-oriented perspective for disambiguating common and proper noun mentions with respect to Wikipedia.", "labels": [], "entities": [{"text": "disambiguating common and proper noun mentions", "start_pos": 54, "end_pos": 100, "type": "TASK", "confidence": 0.7869975864887238}]}, {"text": "Our novel approach models the relationship between disambigua-tion and aspects of cohesion using Markov Logic Networks with latent variables.", "labels": [], "entities": []}, {"text": "Considering cohesive aspects consistently improves the disambiguation results on various commonly used data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "\"I have to review a paper\", the supervisor moaned from the office.", "labels": [], "entities": []}, {"text": "\"Please don't disturb me until I'm done with the review.\"", "labels": [], "entities": []}, {"text": "His student nodded, went to the cafeteria, sat down in the sunshine and started to read yesterday's paper.", "labels": [], "entities": []}, {"text": "This text snippet illustrates two aspects that have been neglected by previous disambiguation approaches.", "labels": [], "entities": []}, {"text": "(1) The interpretation of different mentions, i.e. common and proper nouns, is determined by different notions of context: some mentions depend more on a local sentence-level context (paper in read yesterday's paper; the global context is misleading), some more on a global one (review in I'm done with the review; the local context is not discriminative), some on both global and local context (paper in review a paper).", "labels": [], "entities": []}, {"text": "(2) The context relevant to disambiguate a mention depends on how it is embedded into discourse and is not bound to the surface form of a mention (paper in the first sentence vs. paper in the last one).", "labels": [], "entities": []}, {"text": "Starting from this observation, we argue that the context relevant to disambiguate a mention correlates with its cohesive scope, i.e. the text span within which a mention establishes cohesive relations.", "labels": [], "entities": []}, {"text": "Therefore, we propose to disambiguate mentions differently depending on their cohesive scopes (Section 2).", "labels": [], "entities": []}, {"text": "We distinguish between three different cohesive scopes of mentions and model them as latent variables using Markov Logic Networks (Section 3).", "labels": [], "entities": []}, {"text": "The use of latent variables allows us to learn and predict the cohesive scope and the disambiguation of a mention jointly.", "labels": [], "entities": []}, {"text": "This comes with the advantage that the learning of the scope assignment does not need annotated data by itself but is guided by the annotations available for the target prediction task, i.e. the disambiguation.", "labels": [], "entities": []}, {"text": "In this paper, we focus on concept and entity disambiguation 1 with respect to an inventory derived from Wikipedia and compare (1) to a stateof-the-art approach that treats all mentions alike and uses the same features for disambiguation, (2) to a pipeline-based approach, and (3) to other state-of-the-art approaches (Section 4).", "labels": [], "entities": [{"text": "concept and entity disambiguation 1", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.7417911052703857}]}, {"text": "While early work disambiguated concepts using the local context, current research focuses on exploiting the global document context.", "labels": [], "entities": []}, {"text": "Although such global approaches try to balance between local and global context, they treat all mentions alike, i.e., they apply the same model and the same weighting of local and global context features for disambiguating all mentions (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our novel scope-aware approach to our previous scope-ignorant approach) -which has achieved good results in the English monolingual and Chinese and Spanish cross-lingual entity linking tasks at) -and a scope-aware pipeline-based approach using the same features and preprocessing to ensure a fair comparison.", "labels": [], "entities": []}, {"text": "This allows us to identify the differences in the results that are due to scope-awareness and differences in the results that are due to different learning strategies (joint vs. pipeline-based).", "labels": [], "entities": []}, {"text": "In addition, we compare our joint scope-aware approach to state-of-the-art approaches using various data sets.", "labels": [], "entities": []}, {"text": "summarizes our test sets, MSNBC and TAC 2011) and our training and development sets derived from Wikipedia (WP Training, WP Dev).", "labels": [], "entities": [{"text": "MSNBC and TAC 2011", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.846277117729187}, {"text": "WP Training, WP Dev)", "start_pos": 108, "end_pos": 128, "type": "DATASET", "confidence": 0.8121829926967621}]}, {"text": "For each data set we report the total number of annotated mentions, the number of mentions with a corresponding concept in Wikipedia (non-NILs) and the number of NILs (i.e. mentions that do not refer to a Wikipedia con-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Features for cohesive scope distinction. m, m 1 , m 2 denote mentions, q a score. The predicates  are plugged in the template formula f 8 in Table 1.", "labels": [], "entities": []}, {"text": " Table 3: Statistics for data sets.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation on ACE 2005 data", "labels": [], "entities": [{"text": "ACE 2005 data", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9579523205757141}]}, {"text": " Table 5: Evaluation on ACE 2005 data across induced scopes. The accurracy of the two compared  systems is slightly higher than in", "labels": [], "entities": [{"text": "ACE 2005 data", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9490387638409933}, {"text": "accurracy", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9975160360336304}]}, {"text": " Table 6: Evaluation on various data sets using the respective standard evaluation metrics. BOC stands  for Bag-of-Concepts. We use the code of Ratinov et al. (2011) to evaluate on ACE 2004 and MSNBC.  For TAC 2011, we use the offical evaluation script and report the micro-average (Acc) and B 3 scores.", "labels": [], "entities": [{"text": "BOC", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9952249526977539}, {"text": "ACE 2004 and MSNBC", "start_pos": 181, "end_pos": 199, "type": "DATASET", "confidence": 0.884478747844696}, {"text": "TAC 2011", "start_pos": 206, "end_pos": 214, "type": "DATASET", "confidence": 0.7521876096725464}, {"text": "micro-average (Acc) and B 3 scores", "start_pos": 268, "end_pos": 302, "type": "METRIC", "confidence": 0.802297443151474}]}]}