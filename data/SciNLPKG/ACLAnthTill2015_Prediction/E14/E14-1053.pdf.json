{"title": [{"text": "Topical PageRank: A Model of Scientific Expertise for Bibliographic Search", "labels": [], "entities": [{"text": "Bibliographic Search", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7497101128101349}]}], "abstractContent": [{"text": "We model scientific expertise as a mixture of topics and authority.", "labels": [], "entities": []}, {"text": "Authority is calculated based on the network properties of each topic network.", "labels": [], "entities": [{"text": "Authority", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9878266453742981}]}, {"text": "ThemedPageRank, our combination of LDA-derived topics with PageRank differs from previous models in that topics influence both the bias and transition probabilities of PageRank.", "labels": [], "entities": []}, {"text": "It also incorporates the age of documents.", "labels": [], "entities": [{"text": "age of documents", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.7566541433334351}]}, {"text": "Our model is general in that it can be applied to all tasks which require an estimate of document-document, document-query, document-topic and topic-query similarities.", "labels": [], "entities": []}, {"text": "We present two evaluations, one on the task of restoring the reference lists of 10,000 articles, the other on the task of automatically creating reading lists that mimic reading lists created by experts.", "labels": [], "entities": []}, {"text": "In both evaluations , our system beats state-of-the-art, as well as Google Scholar and Google Search indexed againt the corpus.", "labels": [], "entities": []}, {"text": "Our experiments also allow us to quantify the beneficial effect of our two proposed modifications to PageRank.", "labels": [], "entities": [{"text": "PageRank", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.9595197439193726}]}], "introductionContent": [{"text": "For search, the presence of links in a document collection adds valuable information over that contained in the text of the documents alone.", "labels": [], "entities": []}, {"text": "Each act of linking can be interpreted as a latent judgement of authority or trust which is bestowed onto the linked documents.", "labels": [], "entities": []}, {"text": "This makes authority an objective measure of how important that paper is to a community who confer that authority.", "labels": [], "entities": []}, {"text": "The citation count is the simplest of these, which has been used successfully for decades for bibliometrics ( and for mapping out scientific fields via bibliometric coupling and co-citations.", "labels": [], "entities": [{"text": "citation count", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.8517712652683258}]}, {"text": "More recently, citation counts have been shown to improve effectiveness of ad-hoc retrieval.", "labels": [], "entities": []}, {"text": "In science, the peer review process ensures that the right to cite is hard-earned, but on the web, hyperlinking is infinitely cheap.", "labels": [], "entities": []}, {"text": "This means that that the authority of webpages cannot simply be approximated as the number of incoming links.", "labels": [], "entities": []}, {"text": "Algorithmically more complex authority such as the randomsurfer model PageRank ( ) or the authorities/hub based algorithm HITS) have spectacularly improved search results in comparison to standard IR models relying on similarity calculations based on the words in the text and other text-internal informatioh.", "labels": [], "entities": []}, {"text": "Much recent work in bibliographic search has been driven by the intuition that what works for the web should also work for science, even though citations are more comparable to each other in weight than hyperlinks.", "labels": [], "entities": []}, {"text": "Case studies comparing PageRankbased authority measures against citation counts alone report some cases where PageRank is superior (), but experimental proof of standard PageRank outperforming citation counts in a large-scale bibliographic search experiment is still outstanding.", "labels": [], "entities": []}, {"text": "In at least one such experiment, PageRank performed worse than citation count.", "labels": [], "entities": [{"text": "citation count", "start_pos": 63, "end_pos": 77, "type": "METRIC", "confidence": 0.8773470520973206}]}, {"text": "Straightforward PageRank calculations, when applied to the scientific literature, are hampered by two factors: on the one hand, the progression of time imposes a directional structure on the citation network.", "labels": [], "entities": []}, {"text": "Therefore, PageRank values of older papers are systematically inflated as PageRank can only ever flow from newer to older papers (.", "labels": [], "entities": []}, {"text": "Secondly, and more interestingly, researchers earn their expertise in particular, well-defined scientific fields.", "labels": [], "entities": []}, {"text": "We propose that this requires a more finegrained notion of specific -not global -expertise.", "labels": [], "entities": []}, {"text": "Our solution is to use LDA-derived topics ( as approximations for scientific fields, and to model the importance of a paper as a mixture of its relative expertise in each of the topics it covers.", "labels": [], "entities": []}, {"text": "The second aspect of our solution, somewhat more mundane but still necessary to adapt PageRank successfully to model scientific expertise, is to age-taper the resultant estimation.", "labels": [], "entities": []}, {"text": "In this paper, we present ThemedPageRank (TPR), our model of topic-specific scientific expertise, which incorporates the two modifications, and provide evidence that both are necessary for the adequate application of PageRank-style authority calculations to the scientific literature.", "labels": [], "entities": []}, {"text": "In two evaluations, our model beats standard PageRank and citation counts by a large margin.", "labels": [], "entities": []}, {"text": "Previous models exist which combine the idea of personalising PageRank by topics, but our manipulation of both PageRank's bias and transition probabilities differs from these.", "labels": [], "entities": []}, {"text": "Our experiments also support the claim of our system's superiority over these models.", "labels": [], "entities": []}, {"text": "We use two tasks to evaluate the system's performance.", "labels": [], "entities": []}, {"text": "The first is the reintroduction of an article's reference items that have been artificially removed.", "labels": [], "entities": []}, {"text": "The assumption here is that a good model of document-document similarity should be able to guess which articles any given paper would have cited.", "labels": [], "entities": []}, {"text": "The second task is the automatic creation of reading lists, of the kind that an expert might prepare for their students.", "labels": [], "entities": []}, {"text": "We asked experts to create a gold standard of such reading lists, and compare our system against the current de facto state-of-the-art in such tasks, Google Scholar, and again find that our system beats it comfortably.", "labels": [], "entities": []}, {"text": "This article is structured as follows: the next section describes our model, which section 3 contrasts to related work.", "labels": [], "entities": []}, {"text": "The evaluations are described in sections 4 and 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our authority-based search model using the 2010 ACL Anthology Network ().", "labels": [], "entities": [{"text": "2010 ACL Anthology Network", "start_pos": 55, "end_pos": 81, "type": "DATASET", "confidence": 0.9369278848171234}]}, {"text": "We removed from it corrupted documents, i.e., those of less than 100 characters or containing only control characters.", "labels": [], "entities": []}, {"text": "The ACL Anthology Network provides external meta-data about the articles, which was manually curated.", "labels": [], "entities": [{"text": "ACL Anthology Network", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9339597225189209}]}, {"text": "We do not use this meta-data because we wanted to build as system that can be applied to any large collection of articles, where external meta-data would not normally exist.", "labels": [], "entities": []}, {"text": "We therefore build an approximate citation graph from the paper text itself, as a one-off task when constructing the LDA space.", "labels": [], "entities": []}, {"text": "We extract titles, dates and full-text from every article and perform a search of each articles title in the full-text of all other We first compare our model to the state-of-theart ().", "labels": [], "entities": []}, {"text": "We emulate their experimental setup by including only the pre-2004 articles in the corpus and testing only on the roughly 800 2005/6 articles with more than 5 intra-corpus citations in their reference list, for which we have per-paper average precision scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 243, "end_pos": 252, "type": "METRIC", "confidence": 0.767950177192688}]}, {"text": "The top part of shows that our model (MAP=0.302) outperforms their best model (MAP=0.287; difference at 5% confidence with Wilcoxon Ranked Squares test), despite our model being a general, light-weight IR system, which relies on LDA and PageRank alone, and theirs is a specialised state-of-the art system, which relies on heavy-weight machine learning and on additional sociological features.", "labels": [], "entities": [{"text": "MAP", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.977466881275177}, {"text": "MAP", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9862727522850037}]}, {"text": "The lower part of compares the influence of citation count, global PageRank, topic similarity, and combinations of topic similarity with citation counts or global PageRank, and our model (TPR).", "labels": [], "entities": []}, {"text": "For these tests, we use the entire corpus of 10,000 papers with more than 5 citations.", "labels": [], "entities": []}, {"text": "Over the baseline (A), n-gram-frequency-inverse-documentfrequency (NFIDF), both citation counts (B) and global PageRank (C) make a small improvement.", "labels": [], "entities": [{"text": "citation counts (B)", "start_pos": 80, "end_pos": 99, "type": "METRIC", "confidence": 0.8627385973930359}]}, {"text": "Global LDA similarity scores (D) fare little better.", "labels": [], "entities": [{"text": "LDA similarity scores (D)", "start_pos": 7, "end_pos": 32, "type": "METRIC", "confidence": 0.8234482010205587}]}, {"text": "As the performance of the full model (G; MAP=0.268) shows, the inclusion of topic models lead to a large improvement over any of the above.", "labels": [], "entities": [{"text": "MAP", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8122611045837402}]}, {"text": "This is, as far as we are aware, the first time that a large-scale evaluation that finds significant improvements of a PageRank implementation over citation counts in scientific search.", "labels": [], "entities": []}, {"text": "We next consider our two modifications, ageadjusting (E) and double-biasing (F), in isolation.", "labels": [], "entities": [{"text": "ageadjusting (E)", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.9249282032251358}, {"text": "double-biasing (F)", "start_pos": 61, "end_pos": 79, "type": "METRIC", "confidence": 0.9210983365774155}]}, {"text": "We use two versions of our system where we switched off age-tapering and double-biasing (ie., we only work with a change in the bias probabilities, as do Nie etal.", "labels": [], "entities": []}, {"text": "(2006), Havaliwala (2003) (although their models do not include automatically generated topics) and).", "labels": [], "entities": []}, {"text": "Our model comfortably outperforms TPR-NoDB in both the 800 and 10,000 paper experiment.", "labels": [], "entities": [{"text": "TPR-NoDB", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7123412489891052}]}, {"text": "Similarly, the effect of age-tapering alone can be seen from the performance of TPR-NoAge (our model without age-adjusting), in the difference between 0.267 and 0.302 and that between 0.242 and 0.268 (significant at 99%).", "labels": [], "entities": []}, {"text": "This confirms our claim that a topicspecific age-tapered PageRank is superior to global PageRank in scientific citation networks.", "labels": [], "entities": []}, {"text": "The aim of the second experiment is to test our model against a much cleaner, albeit smaller gold standard: on the task of reconstructing the material of expert-created reading lists.", "labels": [], "entities": []}, {"text": "We compare our system's performance to three standard, commonly used search engines: Lucene TFIDF, the Googleindexed ACL Anthology, and Google Scholar.", "labels": [], "entities": [{"text": "Lucene TFIDF", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.8568534851074219}, {"text": "Googleindexed ACL Anthology", "start_pos": 103, "end_pos": 130, "type": "DATASET", "confidence": 0.9319451848665873}, {"text": "Google Scholar", "start_pos": 136, "end_pos": 150, "type": "DATASET", "confidence": 0.8587046265602112}]}, {"text": "We chose Google-index and Google Scholar because they represent commonly used state-of-the-art commercial search engines, and the Google-index is what is currently offered as the standard ACL Anthology search tool.", "labels": [], "entities": [{"text": "Google Scholar", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8566938638687134}]}, {"text": "In contrast, Lucene TFIDF was chosen to represent an easy-to-interpret, reproducible, out-of-the-box baseline implementing the simplest kind of lexical similarity search without any notion of authority.", "labels": [], "entities": [{"text": "Lucene TFIDF", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.8549620807170868}]}, {"text": "Of the three search engines, we would predict Google Scholar to be the toughest competitor to TPR, because it uses citation information directly and it is reasonable to expect that the Google Scholar algorithm employs some domain adaptation to the scientific domain.", "labels": [], "entities": []}, {"text": "We created gold standard expert-written reading lists using the following protocol.", "labels": [], "entities": []}, {"text": "Eight experts were recruited from the computational linguistics groups of two universities (3 from one, 5 from the other).", "labels": [], "entities": []}, {"text": "All experts had a PhD in computational linguistics and several years of research experience.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7043414860963821}]}, {"text": "They were asked to choose a subject for an (imaginary or existing) reading list for an MPhil student, concerning an area in which they know the literature well.", "labels": [], "entities": []}, {"text": "We purposefully did not give them guidance as to the size of the reading list as we wanted to observe how experts create reading lists.", "labels": [], "entities": []}, {"text": "During the interview, the experimenter documented the final list chosen by the expert and made sure all papers chosen were present in the 2010 version of the ACL Anthology Network.", "labels": [], "entities": [{"text": "2010 version of the ACL Anthology Network", "start_pos": 138, "end_pos": 179, "type": "DATASET", "confidence": 0.7542250411851066}]}, {"text": "This procedure resulted in reading lists of the following topics and sizes: statistical parsing (22 papers); parser evaluation (4); distributional semantics (14); domain adaptation for parsing (11); information extraction (9); lexical semantics (14); statistical machine translation models (5); and concept-to-text generation.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8555887937545776}, {"text": "parser evaluation", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.8778693079948425}, {"text": "information extraction", "start_pos": 199, "end_pos": 221, "type": "TASK", "confidence": 0.7917430996894836}, {"text": "statistical machine translation", "start_pos": 251, "end_pos": 282, "type": "TASK", "confidence": 0.7314393122990926}, {"text": "concept-to-text generation", "start_pos": 299, "end_pos": 325, "type": "TASK", "confidence": 0.7181023955345154}]}, {"text": "In our retrieval model, which topic distribution is chosen fora query depends on whether the query is an exact match to one of the technical terms found by our model.", "labels": [], "entities": []}, {"text": "If it is, then the topic distribution of the technical term is used directly as the query topic distribution \u03b8q, t (i.e. a transposed renormalized \u03c8 in).", "labels": [], "entities": []}, {"text": "If not, we perform a keywordbased search (using Lucene TFIDF), and use the average topic distribution of the top 20 documents returned as the query topic distribution (i.e. several \u03b8i in).", "labels": [], "entities": [{"text": "Lucene TFIDF", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8810162842273712}]}, {"text": "The query topic distribution is then used to linearly combine the topic-specific TPRs into a unique TPR tailored to the query.", "labels": [], "entities": []}, {"text": "The 20 documents with the highest TPR are recommended.", "labels": [], "entities": [{"text": "TPR", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9980523586273193}]}, {"text": "The three baselines are used as follows in the experiment: The experiment is performed by issuing the topic of the reading list (exactly as given to us by the experts) as a key-word based query to each system and recording the top 20 resulting papers answers.", "labels": [], "entities": []}, {"text": "For Lucene TFIDF, we downloaded Lucene.NET v2.9.2 and indexed our 2010 snapshot of the ACL Anthology using standard Lucene parameters for the TFIDF model.", "labels": [], "entities": [{"text": "Lucene TFIDF", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8724624216556549}, {"text": "2010 snapshot of the ACL Anthology", "start_pos": 66, "end_pos": 100, "type": "DATASET", "confidence": 0.6802087922890981}]}, {"text": "For the Googleindexed ACL Anthology (AAN), we use the interface provided on the ACL Anthology website.", "labels": [], "entities": [{"text": "Googleindexed ACL Anthology (AAN)", "start_pos": 8, "end_pos": 41, "type": "DATASET", "confidence": 0.9262028435866038}, {"text": "ACL Anthology website", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.8610819379488627}]}, {"text": "In order to provide an identical search ground, we automatically exclude from the return lists papers added after the creation of the AAN snapshot.", "labels": [], "entities": [{"text": "AAN snapshot", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.978971004486084}]}, {"text": "For Google Scholar (GS), we use the interface provided at scholar.google.com, and parse returns to exclude non-AAN material semi-automatically.", "labels": [], "entities": [{"text": "Google Scholar (GS)", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7167756617069244}]}, {"text": "In the case of Google Scholar, we restrict the search ground to the ACL Anthology by filtering the top 200 return sets (which may lead to fewer than 20 papers returned).", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9626933038234711}]}, {"text": "We report FCSC, RCSC and F-score for each algorithm.", "labels": [], "entities": [{"text": "FCSC", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.5673952698707581}, {"text": "RCSC", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8485201597213745}, {"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9975576400756836}]}, {"text": "FCSC and RCSC are new metrics which address the problem that F-score, being binary, does not support the notion of a \"close hit\", combined with the fact that we require a fine-grained comparison of the quality of different systems retrieved lists despite the small size of our gold standard.", "labels": [], "entities": [{"text": "FCSC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9381251335144043}, {"text": "F-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9391689300537109}]}, {"text": "Citation Substitution Coefficient (FCSC), anew metric for RLR, gives higher scores to papers closely related to the target papers by citation distance.", "labels": [], "entities": [{"text": "Citation Substitution Coefficient (FCSC)", "start_pos": 0, "end_pos": 40, "type": "METRIC", "confidence": 0.5429967045783997}, {"text": "RLR", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9319265484809875}]}, {"text": "The FCSC of each expert paper is the inverse of the number of nodes in the minimal citation graph connecting each expert paper to any system-retrieved paper (thus ranging between 0 and 1; non-connected expert papers receive a zero score).", "labels": [], "entities": [{"text": "FCSC", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.7597854733467102}]}, {"text": "We also introduce Reverse Citation Substitution Coefficient (RCSC), which measures the inverse of the number of nodes in the minimal citation graph connecting each system-retrieved paper to any expert paper.", "labels": [], "entities": [{"text": "Reverse Citation Substitution Coefficient (RCSC)", "start_pos": 18, "end_pos": 66, "type": "METRIC", "confidence": 0.7372180223464966}]}, {"text": "RCSC makes sure that systems cannot simply increase their FCSC values by returning many irrelevant papers.", "labels": [], "entities": [{"text": "RCSC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8737442493438721}]}, {"text": "RCSC thus corresponds to precision, while FCSC corresponds to recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.999576985836029}, {"text": "FCSC", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8930947780609131}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9971407651901245}]}, {"text": "The system RCSC and FCSC scores we report are the average scores of all the system-retrieved and expert papers, respectively.", "labels": [], "entities": []}, {"text": "Reporting both scores gives a good overall picture of system performance, particularly when read together with the F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 115, "end_pos": 122, "type": "METRIC", "confidence": 0.9640815854072571}]}, {"text": "shows that our model comfortably beats the competitor systems according to all metrics.", "labels": [], "entities": []}, {"text": "In particular, our model > GS/AAN > Lucene TFIDF . Concerning simpler methods of estimating authority, shows that a multiplication of TFIDF by citation count (as Fujii", "labels": [], "entities": [{"text": "Lucene TFIDF", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9009795784950256}]}], "tableCaptions": []}