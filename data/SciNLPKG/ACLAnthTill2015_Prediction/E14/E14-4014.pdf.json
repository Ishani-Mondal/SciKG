{"title": [{"text": "Passive-Aggressive Sequence Labeling with Discriminative Post-Editing for Recognising Person Entities in Tweets", "labels": [], "entities": [{"text": "Passive-Aggressive Sequence Labeling", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7020528117815653}]}], "abstractContent": [{"text": "Recognising entities in social media text is difficult.", "labels": [], "entities": [{"text": "Recognising entities in social media text", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8443840344746908}]}, {"text": "NER on newswire text is conventionally cast as a sequence labeling problem.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.872183084487915}, {"text": "sequence labeling", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6338039934635162}]}, {"text": "This makes implicit assumptions regarding its textual structure.", "labels": [], "entities": []}, {"text": "Social media text is rich in disfluency and often has poor or noisy structure, and intuitively does not always satisfy these assumptions.", "labels": [], "entities": []}, {"text": "We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7332465052604675}, {"text": "person recognition", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7721798419952393}, {"text": "F1", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.9993840456008911}]}], "introductionContent": [{"text": "The language of social media text is unusual and irregular (, with misspellings, non-standard capitalisation and jargon, disfluency and fragmentation.", "labels": [], "entities": []}, {"text": "Twitter is one of the sources of social media text most challenging for NLP).", "labels": [], "entities": []}, {"text": "In particular, traditional approaches to Named Entity Recognition (NER) perform poorly on tweets, especially on person mentions -for example, the default model of a leading system reaches an F1 of less than 0.5 on person entities in a major tweet corpus.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.8170433044433594}, {"text": "F1", "start_pos": 191, "end_pos": 193, "type": "METRIC", "confidence": 0.995835542678833}]}, {"text": "This indicates a need for approaches that can cope with the linguistic phenomena apparently common among social media authors, and operate outside of newswire with its comparatively low linguistic diversity.", "labels": [], "entities": []}, {"text": "So, how can we adapt?", "labels": [], "entities": []}, {"text": "This paper contributes two techniques.", "labels": [], "entities": []}, {"text": "Firstly, it demonstrates that entity recognition using noise-resistant sequence labeling outperforms state-of-the-art Twitter NER, although we find that recall is consistently lower than precision.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7906421720981598}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9994230270385742}, {"text": "precision", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9985156655311584}]}, {"text": "Secondly, to remedy this, we introduce a method for automatically post-editing the resulting entity annotations by using a discriminative classifier.", "labels": [], "entities": []}, {"text": "This improves recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9995538592338562}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.998823344707489}]}], "datasetContent": [{"text": "Candidate entity labelings are compared using the CoNLL NER evaluation tool, using precision, recall and F1.", "labels": [], "entities": [{"text": "CoNLL NER evaluation", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.7838643391927084}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9996752738952637}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9994671940803528}, {"text": "F1", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.999303936958313}]}, {"text": "Following Ritter, we use 25%/75% splits made at tweet, and not token, level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: With base features (base)", "labels": [], "entities": []}, {"text": " Table 2: With shape and lemma features (lem)", "labels": [], "entities": []}, {"text": " Table 3: Distribution of person entity lengths.", "labels": [], "entities": []}, {"text": " Table 4: Post-editing performance. Higher Cost sacrifices precision for recall.", "labels": [], "entities": [{"text": "Cost", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9768673181533813}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9994487166404724}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9958410859107971}]}]}