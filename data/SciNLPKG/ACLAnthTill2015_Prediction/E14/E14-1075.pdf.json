{"title": [{"text": "Improving the Estimation of Word Importance for News Multi-Document Summarization", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9756469130516052}, {"text": "Estimation of Word Importance", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.5605366453528404}, {"text": "News Multi-Document Summarization", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.559169332186381}]}], "abstractContent": [{"text": "We introduce a supervised model for predicting word importance that incorporates a rich set of features.", "labels": [], "entities": [{"text": "predicting word importance", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.8920206824938456}]}, {"text": "Our model is superior to prior approaches for identifying words used inhuman summaries.", "labels": [], "entities": [{"text": "identifying words used inhuman summaries", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.7531551241874694}]}, {"text": "Moreover we show that an extractive summarizer using these estimates of word importance is comparable in automatic evaluation with the state-of-the-art.", "labels": [], "entities": []}], "introductionContent": [{"text": "In automatic extractive summarization, sentence importance is calculated by taking into account, among possibly other features, the importance of words that appear in the sentence.", "labels": [], "entities": [{"text": "automatic extractive summarization", "start_pos": 3, "end_pos": 37, "type": "TASK", "confidence": 0.5535036623477936}]}, {"text": "In this paper, we describe experiments on identifying words from the input that are also included inhuman summaries; we call such words summary keywords.", "labels": [], "entities": []}, {"text": "We review several unsupervised approaches for summary keyword identification and further combine these, along with features including position, part-of-speech, subjectivity, topic categories, context and intrinsic importance, in a superior supervised model for predicting word importance.", "labels": [], "entities": [{"text": "summary keyword identification", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.7945763270060221}, {"text": "predicting word importance", "start_pos": 261, "end_pos": 287, "type": "TASK", "confidence": 0.8448516130447388}]}, {"text": "One of the novel features we develop aims to determine the intrinsic importance of words.", "labels": [], "entities": []}, {"text": "To this end, we analyze abstract-article pairs in the New York Times corpus to identify words that tend to be preserved in the abstracts.", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.7402397990226746}]}, {"text": "We demonstrate that judging word importance just based on this criterion leads to significantly higher performance than selecting sentences at random.", "labels": [], "entities": [{"text": "judging word importance", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7620094617207845}]}, {"text": "Identifying intrinsically important words allows us to generate summaries without doing any feature computation on the input, equivalent in quality to the standard baseline of extracting the first 100 words from the latest article in the input.", "labels": [], "entities": []}, {"text": "Finally, we integrate the schemes for assignment of word importance into a summarizer which greedily optimizes for the presence of important words.", "labels": [], "entities": [{"text": "assignment of word importance", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.799515962600708}]}, {"text": "We show that our better estimation of word importance leads to better extractive summaries.", "labels": [], "entities": [{"text": "estimation of word importance", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6716505512595177}, {"text": "extractive summaries", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7478529512882233}]}], "datasetContent": [{"text": "We carryout our experiments on two datasets from the Document Understanding Conference (DUC).", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.6268878281116486}]}, {"text": "DUC 2003 is used for training and development, DUC 2004 is used for testing.", "labels": [], "entities": [{"text": "DUC 2003", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9562470614910126}, {"text": "DUC 2004", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8598451912403107}]}, {"text": "These are the last two years in which generic summarization was evaluated at DUC workshops.", "labels": [], "entities": [{"text": "generic summarization", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.9011083543300629}, {"text": "DUC", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8828120231628418}]}, {"text": "There are 30 multi-document clusters in DUC, each with about 10 news articles on a related topic.", "labels": [], "entities": [{"text": "DUC", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9350751638412476}]}, {"text": "The task is to produce a 100-word generic summary.", "labels": [], "entities": []}, {"text": "Four human abstractive summaries are available for each cluster.", "labels": [], "entities": []}, {"text": "We compare different keyword extraction methods by the F-measure 1 they achieve against the gold-standard summary keywords.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8135669529438019}, {"text": "F-measure 1", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9780182242393494}]}, {"text": "We do not use stemming when calculating these scores.", "labels": [], "entities": [{"text": "stemming", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.9772627949714661}]}, {"text": "In our work, keywords for an input are defined as those words that appear in at least i of the human abstracts, yielding four gold-standard sets of keywords, denoted by G i . |G i | is thus the cardinality of the set for the input.", "labels": [], "entities": []}, {"text": "We only consider the words in the summary that also appear in the original input 2 , with stopwords excluded 3 . Mean |G i | 102 32 15 6: Average number of words in G i For the summarization task, we compare results using ROUGE).", "labels": [], "entities": [{"text": "Mean", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9528203010559082}, {"text": "summarization task", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.9070510268211365}, {"text": "ROUGE", "start_pos": 222, "end_pos": 227, "type": "METRIC", "confidence": 0.9715761542320251}]}, {"text": "We report ROUGE-1, -2, -4 recall, with stemming and without removing stopwords.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.993466317653656}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9963248372077942}]}, {"text": "We consider ROUGE-2 recall as the main metric for this comparison due to its effectiveness in comparing machine summaries).", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9756847620010376}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.8927634358406067}]}, {"text": "All of the summaries were truncated to the first 100 words by ROUGE 4 . We use Wilcoxon signed-rank test to examine the statistical significance as advocated by for both tasks, and consider differences to be significant if the p-value is less than 0.05.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9023259878158569}]}, {"text": "The performance of our logistic regression model is evaluated on two tasks: keyword identification and extractive summarization.", "labels": [], "entities": [{"text": "keyword identification", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7900831997394562}, {"text": "extractive summarization", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.6139210164546967}]}, {"text": "We name our system REGSUM.", "labels": [], "entities": [{"text": "REGSUM", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.5237268805503845}]}], "tableCaptions": [{"text": " Table 1: Average number of words in G i", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9775174260139465}]}, {"text": " Table 3:  Blind sentence extraction system,  compared with three baseline systems (%)", "labels": [], "entities": [{"text": "Blind sentence extraction", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.5790978173414866}]}, {"text": " Table 5: Keyword identification F-score (%) for different G i and different number of words selected.", "labels": [], "entities": [{"text": "Keyword identification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.787163645029068}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9130679368972778}]}, {"text": " Table 6: System performance comparison (%)", "labels": [], "entities": []}]}