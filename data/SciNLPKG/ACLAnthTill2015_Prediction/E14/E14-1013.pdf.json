{"title": [{"text": "Modelling the Lexicon in Unsupervised Part of Speech Induction", "labels": [], "entities": [{"text": "Modelling the Lexicon", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8205010692278544}, {"text": "Speech Induction", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7027120292186737}]}], "abstractContent": [{"text": "Automatically inducing the syntactic part-of-speech categories for words in text is a fundamental task in Computational Linguistics.", "labels": [], "entities": [{"text": "Computational Linguistics", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.8263121247291565}]}, {"text": "While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single part-of-speech tag.", "labels": [], "entities": []}, {"text": "This one-tag-per-type heuristic counters the tendency of Hidden Markov Model based taggers to over generate tags fora given word type.", "labels": [], "entities": []}, {"text": "However, it is clearly incompatible with basic syntactic theory.", "labels": [], "entities": []}, {"text": "In this paper we extend a state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model of the lexicon.", "labels": [], "entities": []}, {"text": "In doing so we are able to incorporate a soft bias towards inducing few tags per type.", "labels": [], "entities": []}, {"text": "We develop a particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation.", "labels": [], "entities": [{"text": "first language acquisition", "start_pos": 154, "end_pos": 180, "type": "TASK", "confidence": 0.643974632024765}, {"text": "Speech Recognition", "start_pos": 235, "end_pos": 253, "type": "TASK", "confidence": 0.7593280673027039}, {"text": "Machine Translation", "start_pos": 258, "end_pos": 277, "type": "TASK", "confidence": 0.7982091307640076}]}, {"text": "While there has been much prior work on this task (), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type.", "labels": [], "entities": []}, {"text": "Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling.", "labels": [], "entities": []}, {"text": "While the empirically observed expectation for the number of tags per word type is close to one, there are many exceptions, e.g. words that occur as both nouns and verbs (opening, increase, related etc.).", "labels": [], "entities": []}, {"text": "In this paper we extend the Pitman-Yor HMM tagger () to explicitly include a model of the lexicon that encodes from which tags a word type maybe generated.", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.5933050215244293}]}, {"text": "For each word type we draw an ambiguity class which is the set of tags that it may occur with, capturing the fact that words are often ambiguous between certain tags (e.g. Noun and Verb), while rarely between others (e.g. Determiner and Verb).", "labels": [], "entities": []}, {"text": "We extend the type based Sequential Monte Carlo (SMC) inference algorithm of to incorporate our model of the lexicon, removing the need for the heuristic inference technique of.", "labels": [], "entities": [{"text": "Sequential Monte Carlo (SMC) inference", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.7400109853063311}]}, {"text": "We start in Section 3 by introducing the original PYP-HMM model and our extended model of the lexicon.", "labels": [], "entities": []}, {"text": "Section 4 introduces a Particle Gibbs sampler for this model, a basic SMC method that generates samples from the model's posterior.", "labels": [], "entities": [{"text": "SMC", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9892141222953796}]}, {"text": "We evaluate these algorithms in Section 5, analyzing their behavior in comparisons to previously proposed state-of-the-art approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We provide an empirical evaluation of our proposed Lex-HMM in terms of the accuracy of the taggings learned according to the most popular metric, and the distributions over ambiguity classes.", "labels": [], "entities": [{"text": "Lex-HMM", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9438951015472412}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9992546439170837}]}, {"text": "Our experimental evaluation considers the impact of our improved Particle Gibbs inference algorithm both for the original PYP-HMM and when used for inference in our extended model.", "labels": [], "entities": [{"text": "PYP-HMM", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.8254196047782898}]}, {"text": "We intend to learn whether the lexicon model can match or exceed the performance of the other models despite focusing on only a subset of the possible tags each iteration.", "labels": [], "entities": []}, {"text": "We hypothesize that an accurate lexicon model and the sparsity it induces over the number of tags per word-type will improve the performance over the standard PYP-HMM model while also decreasing training time.", "labels": [], "entities": []}, {"text": "Furthermore, our lexicon model is novel, and its accuracy in representing ambiguity classes is an important aspect of its performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9964439272880554}]}, {"text": "The model focuses inference on the most likely tag choices, represented by ambiguity classes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: M-1 Accuracy on the WSJ Corpus:  Comparison of the accuracy of each of the sam- plers with and without the language model emis- sion prior on the English WSJ Corpus. The second  column reports run time in hours where available*.  Note the Lex-HMM+LM model matches the PYP- 1HMM+LM approximation despite finishing in  half the time. The abbreviations in parentheses  indicate that the results were reported in CGS10  (Christodoulopoulos et al., 2010) and BBDK10  (Berg-Kirkpatrick et al., 2010) *CGS10 reports  that the MEMM model takes approximately 40  hours on 16 cores.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9526396989822388}, {"text": "WSJ Corpus", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.9676981270313263}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9992949962615967}, {"text": "English WSJ Corpus", "start_pos": 156, "end_pos": 174, "type": "DATASET", "confidence": 0.6747486690680186}, {"text": "CGS10", "start_pos": 419, "end_pos": 424, "type": "DATASET", "confidence": 0.9664656519889832}, {"text": "BBDK10", "start_pos": 464, "end_pos": 470, "type": "DATASET", "confidence": 0.5262325406074524}]}, {"text": " Table 3: Selection of Predicted Ambiguity Classes: Common ambiguity classes from the predicted  part-of-speech assignments from the WSJ data set, and the five most common word types associated  with each ambiguity class. The sets are ranked according to the number of word types associated to  them. Words in bold are matched to exactly the same ambiguity set in the gold standard. The lower  five ambiguity classes are the most common with more than one part-of-speech. Numbers in parentheses  represent the proportion of tokens of that type assigned to each tag in the gold standard for that ambiguity  class.", "labels": [], "entities": [{"text": "WSJ data set", "start_pos": 133, "end_pos": 145, "type": "DATASET", "confidence": 0.9855214556058248}]}]}