{"title": [{"text": "Automated Verb Sense Labelling Based on Linked Lexical Resources", "labels": [], "entities": [{"text": "Automated Verb Sense Labelling", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6095788404345512}]}], "abstractContent": [{"text": "We present a novel approach for creating sense annotated corpora automatically.", "labels": [], "entities": []}, {"text": "Our approach employs shallow syntactico-semantic patterns derived from linked lexical resources to automatically identify instances of word senses in text corpora.", "labels": [], "entities": []}, {"text": "We evaluate our labelling method intrinsically on SemCor and extrinsically by using automatically labelled corpus text to train a classifier for verb sense disambiguation.", "labels": [], "entities": [{"text": "verb sense disambiguation", "start_pos": 145, "end_pos": 170, "type": "TASK", "confidence": 0.7127720912297567}]}, {"text": "Testing this classifier on verbs from the English MASC corpus and on verbs from the Senseval-3 all-words disambiguation task shows that it matches the performance of a classifier which has been trained on manually annotated data.", "labels": [], "entities": [{"text": "English MASC corpus", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.5701101223627726}, {"text": "Senseval-3 all-words disambiguation task", "start_pos": 84, "end_pos": 124, "type": "TASK", "confidence": 0.7054115161299706}]}], "introductionContent": [{"text": "Sense annotated corpora are important resources in NLP as they can be used as training data (e.g., for word sense disambiguation (WSD) or semantic role labelling) or as sources for the acquisition of lexical information (e.g., selectional preference information).", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.8094401409228643}, {"text": "semantic role labelling", "start_pos": 138, "end_pos": 161, "type": "TASK", "confidence": 0.6096401711304983}]}, {"text": "Typically, a particular sense inventory from a lexical resource is used to annotate some or all words with word senses from this sense inventory.", "labels": [], "entities": []}, {"text": "For instance, various sense-annotated corpora based on WordNet (WN;) exist, such as the data from the Senseval competitions, or the SemCor corpus.", "labels": [], "entities": [{"text": "WordNet (WN;)", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.8581263273954391}, {"text": "SemCor corpus", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.7198249399662018}]}, {"text": "Such corpora are usually created manually which is expensive and time consuming.", "labels": [], "entities": []}, {"text": "Furthermore, the corpora are often domain specific (e.g. newspaper texts) which makes statistical systems trained on them strongly biased.", "labels": [], "entities": []}, {"text": "We present a novel approach for creating sense annotated corpora automatically.", "labels": [], "entities": []}, {"text": "Our approach employs shallow syntactico-semantic patterns derived from linked lexical resources (LLRs) to automatically identify instances of word senses in text corpora.", "labels": [], "entities": []}, {"text": "We significantly extend previous work on this task by making two important contributions: (i) we employ a large-scale LLR for automatically creating sense annotated data and (ii) we perform meaningful intrinsic and application-based evaluations of our method on large sense annotated datasets.", "labels": [], "entities": []}, {"text": "LLRs are the result of integrating several lexical-semantic resources by linking them at the word sense level.", "labels": [], "entities": []}, {"text": "Examples of large LLRs are the multilingual BabelNet (, an integration of wordnets and Wikipedia 3 , or UBY, ( ), the resource we employ in our work here.", "labels": [], "entities": []}, {"text": "UBY is an integration of multiple resources, such as wordnets, Wikipedia, Wiktionary (WKT) , FrameNet (FN; () and VerbNet (VN;) for English and German.", "labels": [], "entities": [{"text": "UBY", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8375527858734131}]}, {"text": "A distinguishing feature of LLRs is the enriched sense representation for word senses that are interlinked since different resources provide different, often complementary information.", "labels": [], "entities": []}, {"text": "Annotating corpora with such enriched sense representations turns them into versatile training data for statistical systems.", "labels": [], "entities": []}, {"text": "Our first contribution (i) also addresses a considerable gap in recent research regarding automated sense labelling of verbs.", "labels": [], "entities": [{"text": "automated sense labelling of verbs", "start_pos": 90, "end_pos": 124, "type": "TASK", "confidence": 0.7409354865550994}]}, {"text": "Most previous work is done on nouns.", "labels": [], "entities": []}, {"text": "However, verbs pose a bigger challenge due to their high polysemy and the fact that, unlike nouns, syntax is of crucial importance because it often reflects particular aspects of verb meaning.", "labels": [], "entities": []}, {"text": "That is why, here we focus on verbs and present results and evaluations for this previously neglected part-of-speech (POS).", "labels": [], "entities": []}, {"text": "Our method, however, can be applied to other parts-of speech as well.", "labels": [], "entities": []}, {"text": "Regarding (ii), we are the first to perform meaningful intrinsic and extrinsic evaluations of automatically labelled data on a larger scale.", "labels": [], "entities": []}, {"text": "The intrinsic evaluation measures the performance of our method on the manually annotated SemCor corpus.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.8929051756858826}]}, {"text": "The extrinsic evaluation compares the performance of a classifier for verb sense disambiguation (VSD) which has been trained (a) on automatically sense labelled data and (b) on manually annotated data.", "labels": [], "entities": [{"text": "verb sense disambiguation (VSD)", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7764855325222015}]}, {"text": "Both settings achieve very similar results which means that competitive VSD can be performed without the need of costly manually created training data.", "labels": [], "entities": [{"text": "VSD", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9751620292663574}]}, {"text": "This could be beneficial in languages (e.g., German, Spanish) for which elaborate lexical-semantic resources exist but large, high-quality sense annotated corpora are unavailable.", "labels": [], "entities": []}, {"text": "Moreover, we experiment with various linkings between lexical resources in order to investigate how different resource combinations affect the performance of automated sense labelling.", "labels": [], "entities": [{"text": "automated sense labelling", "start_pos": 158, "end_pos": 183, "type": "TASK", "confidence": 0.6422949632008871}]}, {"text": "We show that combining all available resources might not be the best option.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents our method.", "labels": [], "entities": []}, {"text": "Section 3 describes the data used in the experiments.", "labels": [], "entities": []}, {"text": "Section 4 presents the results of the evaluations.", "labels": [], "entities": []}, {"text": "Section 5 analyses in detail the differences between our method and previous work.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Next, we present the intrinsic and the applicationbased evaluations of our method.", "labels": [], "entities": []}, {"text": "We intrinsically evaluate the performance of the automated labelling algorithm for the Senseval-3 verbs which occur in the SemCor corpus.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.7396789491176605}]}, {"text": "Occurrences of these 152 verbs in SemCor are processed  by the labelling algorithm with a window size w = 7 and the automatically annotated WN 3.0 senses are compared with the gold senses available in SemCor 3.0.", "labels": [], "entities": []}, {"text": "We calculated the accuracy as the percentage of correctly labelled instances and the instance coverage as the percentage of labelled instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9997724890708923}]}, {"text": "The sense coverage is calculated as the percentage of all predicted (not annotated) senses relative to all gold verb senses given in SemCor.", "labels": [], "entities": []}, {"text": "A random sense baseline yields 15% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.999618411064148}]}, {"text": "Note that a MFS baseline based on WN would not be meaningful, because the WordNet MFS is based on the frequency distribution of annotated senses in SemCor.", "labels": [], "entities": [{"text": "WordNet MFS", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9342055022716522}]}, {"text": "shows accuracy and coverage results of the automated labelling algorithm for different values of the threshold t and two combinations of sense links from UBY.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.999622106552124}, {"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9926482439041138}, {"text": "UBY", "start_pos": 154, "end_pos": 157, "type": "DATASET", "confidence": 0.9087057113647461}]}, {"text": "Depending on the threshold t, 2% to 55% of the verb instances in SemCor can automatically be labelled, and the instance coverage goes largely in parallel to the coverage of predicted WN senses.", "labels": [], "entities": []}, {"text": "Accuracy ranges between 32% and 47% and exceeds the random sense baseline by a large margin.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9969047904014587}]}, {"text": "Lowering the threshold increases the coverage of the labelling method, but it also leads to a decrease inaccuracy of 9 percentage points (12 for the configuration with VN).", "labels": [], "entities": [{"text": "coverage", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9698085188865662}, {"text": "inaccuracy", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9820299744606018}]}, {"text": "Adding more patterns from VN via the WN-VN alignment, leads to a decrease in both instance and sense coverage combined with an increase inaccuracy.", "labels": [], "entities": []}, {"text": "Since SemCor is a rather small corpus, the increase in instance coverage is not as clear as for large Web corpora such as the ukWaC corpus.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.9819848835468292}]}, {"text": "Labelling a 1GB subset of the ukWaC corpus based on patterns derived from the WN-FN-WKT alignments resulted in 15MB of labelled data, whereas 25MB labelled data could be created from the same subset with the additional patterns from the WN-VN alignment.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9919273257255554}]}, {"text": "In, we show examples of the highest ranking patterns and the corresponding labelled SemCor instances for senses that were correctly and falsely annotated.", "labels": [], "entities": []}, {"text": "The examples in show that the similarity metric assigns the highest values to instances where function words (e.g., in, to, who) or POS tags (e.g., PP, VV) from the ASP vocabulary occur in the immediate neighbourhood of the target verb.", "labels": [], "entities": []}, {"text": "Since such functions words play an important role in the ASPs derived from VN, the VN ASPs possibly tend to dominate over the SPs derived from sense examples, which explains the observed decrease in coverage (see).", "labels": [], "entities": [{"text": "coverage", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.954904317855835}]}, {"text": "The falsely labelled instances turnout to be examples of WN senses where the gold sense is very similar to the automatically attached sense as evident from the synset definition given in the rightmost column.", "labels": [], "entities": [{"text": "WN senses", "start_pos": 57, "end_pos": 66, "type": "TASK", "confidence": 0.8576573431491852}]}, {"text": "We extrinsically evaluate our method for automated verb sense labelling by using it for learning a classifier for VSD in a train-test setting.", "labels": [], "entities": [{"text": "verb sense labelling", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.6803882320721945}]}, {"text": "We use features which have been widely used in supervised WSD systems, in particular features based on dependency parsing.", "labels": [], "entities": [{"text": "WSD", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9252839684486389}, {"text": "dependency parsing", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7469641268253326}]}, {"text": "While this might seem to be in contrast to our labelling algorithm which is based on shallow linguistic preprocessing, it is fully justified by the purpose of our extrinsic evaluation: The main purpose of the extrinsic evaluation is not to outperform state-of-the-art VSD systems, but to show that, when operating with reasonable features, a classifier trained on the data automatically labelled with our method performs equally well as when this classifier is trained on manually annotated data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of the automated labelling  algorithm evaluated for occurrences of Senseval-3  verbs in SemCor.", "labels": [], "entities": []}, {"text": " Table 5: Performance of the various combinations  of lexical resources.", "labels": [], "entities": []}]}