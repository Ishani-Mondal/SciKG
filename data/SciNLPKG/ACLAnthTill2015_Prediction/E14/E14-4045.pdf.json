{"title": [{"text": "Coreference Resolution Evaluation for Higher Level Applications", "labels": [], "entities": [{"text": "Coreference Resolution Evaluation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9373296499252319}]}], "abstractContent": [{"text": "This paper presents an evaluation framework for coreference resolution geared towards interpretability for higher-level applications.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.97968590259552}]}, {"text": "Three application scenarios for coreference resolution are outlined and metrics for them are devised.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.984324723482132}]}, {"text": "The metrics provide detailed system analysis and aim at measuring the potential benefit of using coreference systems in preprocessing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference Resolution is often described as an important preprocessing step for higher-level applications.", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9344436526298523}]}, {"text": "However, the commonly used coreference evaluation metrics (MUC, BCUB, CEAF, BLANC) treat coreference as a generic clustering problem and perform cluster similarity measures to evaluate coreference system outputs.", "labels": [], "entities": [{"text": "MUC", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.8282703161239624}, {"text": "BCUB", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.5392195582389832}, {"text": "BLANC", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9937388896942139}]}, {"text": "Mentions are seen as unsorted generic items rather than linearly ordered linguistic objects.", "labels": [], "entities": []}, {"text": "This makes it arguably hard to interpret the scores and assess the potential benefit of using a coreference system as a preprocessing step.", "labels": [], "entities": []}, {"text": "Therefore, this paper proposes an evaluation framework for coreference systems which aims at bridging the gap between coreference system development, evaluation, and higher level applications.", "labels": [], "entities": []}, {"text": "For this purpose, we outline three types of application scenarios which coreference resolution can benefit and devise metrics for them which are easy to interpret and provide detailed system output analysis based on any available mention feature.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.9776662886142731}]}], "datasetContent": [{"text": "We apply our metrics to three available coreference systems, namely the Berkley system), the IMS system, and the Stanford system ( and their responses for the CoNLL 2012 shared task test set for English (.", "labels": [], "entities": [{"text": "Berkley system", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.8747134506702423}, {"text": "IMS system", "start_pos": 93, "end_pos": 103, "type": "DATASET", "confidence": 0.8117702007293701}, {"text": "Stanford system", "start_pos": 113, "end_pos": 128, "type": "DATASET", "confidence": 0.866664469242096}, {"text": "CoNLL 2012 shared task test set", "start_pos": 159, "end_pos": 190, "type": "DATASET", "confidence": 0.8523125251134237}]}, {"text": "We note that the system ranking based on the MELA score 5 is retained by our metrics.", "labels": [], "entities": [{"text": "MELA score 5", "start_pos": 45, "end_pos": 57, "type": "METRIC", "confidence": 0.573056717713674}]}, {"text": "MELA rates the Berkley system best (61.62), followed by the IMS system (57.42), and then the Stanford system (55.69).", "labels": [], "entities": [{"text": "MELA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9062964916229248}, {"text": "Berkley system", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.7752504944801331}, {"text": "IMS", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.7760009765625}]}, {"text": "Beside detailed analysis based on PoS tags, our metrics reveal interesting nuances.", "labels": [], "entities": []}, {"text": "Somewhat expectedly, noun resolution is worse when the immediate antecedent is evaluated, than if the next nominal antecedent is analyzed.", "labels": [], "entities": [{"text": "noun resolution", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8547263443470001}]}, {"text": "Symmetrically inverse, pronouns achieve higher scores when their direct antecedent is measured, as compared to when the next nominal antecedent has to be correct.", "labels": [], "entities": []}, {"text": "Our evaluation shows that the IMS system achieves a higher score for pronouns than the Berkley system when immediate antecedents are measured and has a higher Precision for pronouns regarding the inferred antecedents.", "labels": [], "entities": [{"text": "IMS", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.7951257228851318}, {"text": "Precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.999610960483551}]}, {"text": "The Berkley system performs best mainly due to Recall.", "labels": [], "entities": [{"text": "Berkley", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.4895631670951843}, {"text": "Recall", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.6668172478675842}]}, {"text": "For e.g. personal pronouns (PRP), Berkley has the 5 M U C+BCU B+CEAF E 3 following counts for the inferred antecedents: tp=2687, wl=1935, fn=871, fp=389, while IMS shows tp=2243, wl=1376, fn=1592, fp=287.", "labels": [], "entities": [{"text": "personal pronouns (PRP)", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7210649013519287}, {"text": "Berkley", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9506030678749084}, {"text": "BCU B+CEAF E", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9019806861877442}, {"text": "IMS", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.8501841425895691}]}, {"text": "This indicates that the IMS Recall is lower because of the high false negative count, rather than being due to too many wrong linkages.", "labels": [], "entities": [{"text": "IMS Recall", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.6678681373596191}, {"text": "false negative count", "start_pos": 64, "end_pos": 84, "type": "METRIC", "confidence": 0.8098755478858948}]}, {"text": "suggests that the IMS systems performs significantly worse in the PERSON class than the other systems and is outperformed by the Stanford system in the ORG class, but performs best in the GPE class.", "labels": [], "entities": [{"text": "GPE class", "start_pos": 188, "end_pos": 197, "type": "DATASET", "confidence": 0.8821325898170471}]}], "tableCaptions": [{"text": " Table 3: Antecedent based evaluation", "labels": [], "entities": [{"text": "Antecedent", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.7809814810752869}]}, {"text": " Table 4: Anchor mention based evaluation", "labels": [], "entities": [{"text": "Anchor", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9866069555282593}]}]}