{"title": [{"text": "Crowdsourcing Annotation of Non-Local Semantic Roles", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper reports on a study of crowd-sourcing the annotation of non-local (or implicit) frame-semantic roles, i.e., roles that are realized in the previous discourse context.", "labels": [], "entities": []}, {"text": "We describe two annotation setups (marking and gap filling) and find that gap filling works considerably better, attaining an acceptable quality relatively cheaply.", "labels": [], "entities": [{"text": "gap filling", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.6832117736339569}, {"text": "gap filling", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7892105281352997}]}], "introductionContent": [{"text": "In the last years, crowdsourcing, e.g., using Amazon's Mechanical Turk platform, has been used to collect data fora range of NLP tasks, e.g., MT evaluation), sentiment analysis (, and student answer rating.", "labels": [], "entities": [{"text": "MT evaluation)", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.9522356986999512}, {"text": "sentiment analysis", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.9614032804965973}]}, {"text": "Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems ().", "labels": [], "entities": [{"text": "Frame-semantic role annotation (FSRA)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7842086851596832}, {"text": "framesemantic role labeling (SRL)", "start_pos": 217, "end_pos": 250, "type": "TASK", "confidence": 0.7979398171106974}]}, {"text": "Thus, there are some studies that have investigated FSRA as a crowdsourcing task.", "labels": [], "entities": [{"text": "FSRA", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.8406670689582825}]}, {"text": "It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame.", "labels": [], "entities": []}, {"text": "have recently addressed the first step, experimenting with various ways of presenting the task.", "labels": [], "entities": []}, {"text": "have considered both steps and operationalized them separately and jointly, finding the best results when a single annotation task is presented to turkers (due to the interdependence of the two steps) and when the semantic role description are simplified.", "labels": [], "entities": []}, {"text": "Both studies conclude that crowdsourcing can produce usable results for FSRA but requires careful design.", "labels": [], "entities": [{"text": "FSRA", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.5482369661331177}]}, {"text": "Our study extends these previous studies to the phenomenon of implicit (non-locally realized) semantic roles where annotators are presented with a target sentence in paragraph context, and have to decide for every role whether it is realized in the target sentence, elsewhere in the paragraph, or not at all.", "labels": [], "entities": []}, {"text": "Our results shows that implicit roles can be annotated as well as locally realized roles in a crowdsourcing setup, again provided that good design choices are taken.", "labels": [], "entities": []}], "datasetContent": [{"text": "The final dataset consists of 384 predicate instances.", "labels": [], "entities": []}, {"text": "With four roles per predicate, a total of 1536 roles could have been realized.", "labels": [], "entities": []}, {"text": "We found that more than half (60%) of the roles remained unrealized even in context.", "labels": [], "entities": []}, {"text": "23% of the roles were realized locally, and 17% non-locally.", "labels": [], "entities": []}, {"text": "The distribution over locally realized, non-locally realized, and unrealized roles varies considerably among the four roles that we consider.", "labels": [], "entities": []}, {"text": "GOAL has the highest percentage of realized roles overall (unrealized only for 34% of all predicate instances), and at the same time the highest ratio of locally realized roles (48% locally realized, 18% non-locally).", "labels": [], "entities": [{"text": "GOAL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7347867488861084}]}, {"text": "This corresponds well to FrameNet's predictions about our chosen predicates which realize the Goal role generally as the direct object (reach) or an obligatory prepositional phrase (arrive).", "labels": [], "entities": []}, {"text": "In contrast, SOURCE is realized only for 36% of all instances, and then predominantly non-locally (24% non-local vs. 12% local).", "labels": [], "entities": [{"text": "SOURCE", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.9082936644554138}]}, {"text": "This shows once more that a substantial part of predicate-argument structure must be recovered from previous discourse context.", "labels": [], "entities": []}, {"text": "On average, each HIT page was annotated in 1 minute and 48 seconds, which means 27 seconds per each role and a total of 60 hours for the whole annotation.", "labels": [], "entities": [{"text": "HIT page", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8456375002861023}]}, {"text": "We paid 0.15 USD for each HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.6254972219467163}]}, {"text": "Since the number of roles in all HITs was fixed to four (source, goal, path and place), each role cost 0.04 USD, which corresponds to about USD 0.19 for every canonical role annotation.", "labels": [], "entities": []}, {"text": "This is about twice the amount paid by Fossati et al. and reflects the increased effort inherent in a task that involves discourse context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Raw agreement among annotators in the  \"marking\" task", "labels": [], "entities": []}, {"text": " Table 2: Raw agreement among annotators in the  \"gap filling\" task", "labels": [], "entities": [{"text": "gap filling\"", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.7827283640702566}]}, {"text": " Table 3: Raw agreement between canonical crowd- sourcing annotation and expert annotation by role", "labels": [], "entities": [{"text": "crowd- sourcing annotation", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.669940859079361}]}, {"text": " Table 4: Raw agreement between canonical anno- tation and expert annotation by realization status", "labels": [], "entities": [{"text": "canonical anno- tation", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.6507273092865944}]}]}