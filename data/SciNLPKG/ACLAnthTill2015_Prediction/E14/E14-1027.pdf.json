{"title": [{"text": "Incremental Bayesian Learning of Semantic Categories", "labels": [], "entities": [{"text": "Incremental Bayesian Learning of Semantic Categories", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7229633728663126}]}], "abstractContent": [{"text": "Models of category learning have been extensively studied in cognitive science and primarily tested on perceptual abstractions or artificial stimuli.", "labels": [], "entities": [{"text": "category learning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8702678680419922}]}, {"text": "In this paper we focus on categories acquired from natural language stimuli, that is words (e.g., chair is a member of the FURNITURE category).", "labels": [], "entities": [{"text": "FURNITURE", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.8222696185112}]}, {"text": "We present a Bayesian model which, unlike previous work, learns both categories and their features in a single process.", "labels": [], "entities": []}, {"text": "Our model employs particle filters, a sequential Monte Carlo method commonly used for approximate probabilistic inference in an incremental setting.", "labels": [], "entities": []}, {"text": "Comparison against a state-of-the-art graph-based approach reveals that our model learns qualitatively better categories and demonstrates cogni-tive plausibility during learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Considerable psychological research has shown that people reason about novel objects they encounter by identifying the category to which these objects belong and extrapolating from their past experiences with other members of that category.", "labels": [], "entities": []}, {"text": "Categorization is a classic problem in cognitive science, underlying a variety of common mental tasks including perception, learning, and the use of language.", "labels": [], "entities": []}, {"text": "Given its fundamental nature, categorization has been extensively studied both experimentally and in simulations.", "labels": [], "entities": []}, {"text": "Indeed, numerous models exist as to how humans categorize objects ranging from strict prototypes (categories are represented by a single idealized member which embodies their core properties; e.g.,) to full exemplar models (categories are represented by a list of previously encountered members; e.g., and combinations of the two (e.g.,.", "labels": [], "entities": []}, {"text": "A common feature across different studies is the use of stimuli involving realworld objects (e.g., children's toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals;, or artificial ones (e.g., binary strings, dot patterns or geometric shapes;.", "labels": [], "entities": []}, {"text": "Most existing models focus on adult categorization, in which it is assumed that a large number of categories have already been learnt (but see.", "labels": [], "entities": []}, {"text": "In this work we focus on categories acquired from natural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximated by large corpora) influence category formation (e.g., chair and table are FURNITURE whereas peach and apple are FRUIT 1 ).", "labels": [], "entities": [{"text": "FURNITURE", "start_pos": 245, "end_pos": 254, "type": "METRIC", "confidence": 0.9558420181274414}, {"text": "FRUIT", "start_pos": 283, "end_pos": 288, "type": "METRIC", "confidence": 0.9676965475082397}]}, {"text": "The idea of modeling categories using words as a stand-in for their referents has been previously used to explore categorization-related phenomena such as semantic priming) and typicality rating (, to evaluate prototype and exemplar models (), and to simulate early language category acquisition ().", "labels": [], "entities": [{"text": "early language category acquisition", "start_pos": 260, "end_pos": 295, "type": "TASK", "confidence": 0.601469412446022}]}, {"text": "The idea of using naturalistic corpora has received little attention.", "labels": [], "entities": []}, {"text": "Most existing studies use feature norms as a proxy for people's representation of semantic concepts.", "labels": [], "entities": [{"text": "people's representation of semantic concepts", "start_pos": 55, "end_pos": 99, "type": "TASK", "confidence": 0.7633120119571686}]}, {"text": "Ina typical procedure, participants are presented with a word and asked to generate the most relevant features or attributes for its referent concept.", "labels": [], "entities": []}, {"text": "The most notable collection of feature norms is probably the multi-year project of, which obtained features fora set of 541 common English nouns.", "labels": [], "entities": []}, {"text": "Our approach replaces feature norms with representations derived from words' contexts in corpora.", "labels": [], "entities": []}, {"text": "While this is an impoverished view of how categories are acquired -it is clear that they are learnt through exposure to the linguistic environment and the physical world -perceptual infor-mation relevant for extracting semantic categories is to a large extent redundantly encoded in linguistic experience (.", "labels": [], "entities": []}, {"text": "Besides, there are known difficulties with feature norms such as the small number of words for which these can be obtained, the quality of the attributes, and variability in the way people generate them (see Zeigenfuse and Lee 2010 for details).", "labels": [], "entities": []}, {"text": "Focusing on natural language categories allows us to build categorization models with theoretically unlimited scope.", "labels": [], "entities": []}, {"text": "To this end, we present a probabilistic Bayesian model of category acquisition based on the key idea that learners can adaptively form category representations that capture the structure expressed in the observed data.", "labels": [], "entities": [{"text": "category acquisition", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.8180980682373047}]}, {"text": "We model category induction as two interrelated sub-problems: (a) the acquisition of features that discriminate among categories, and (b) the grouping of concepts into categories based on those features.", "labels": [], "entities": [{"text": "category induction", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7402750849723816}]}, {"text": "An important modeling question concerns the exact mechanism with which categories are learned.", "labels": [], "entities": []}, {"text": "To maintain cognitive plausibility, we develop an incremental learning algorithm.", "labels": [], "entities": []}, {"text": "Incrementality is a central aspect of human learning which takes place sequentially and overtime.", "labels": [], "entities": []}, {"text": "Humans are capable of dealing with a situation even if only partial information is available.", "labels": [], "entities": []}, {"text": "They adaptively learn as new information is presented and locally update their internal knowledge state without systematically revising everything known about the situation at hand.", "labels": [], "entities": []}, {"text": "Memory and processing limitations also explain why humans must learn incrementally.", "labels": [], "entities": []}, {"text": "It is not possible to store and have easy access to all the information one has been exposed to.", "labels": [], "entities": []}, {"text": "It seems likely that people store the most prominent facts and generalizations, which they modify on they fly when new facts become available.", "labels": [], "entities": []}, {"text": "Our model learns categories using a particle filter, a Markov Chain Monte Carlo (MCMC) inference mechanism which sequentially integrates newly observed data and can be thus viewed as a plausible proxy for human learning.", "labels": [], "entities": []}, {"text": "Experimental results show that the incremental learner obtains meaningful categories which outperform the state of the art whilst at the same time acquiring semantic representations of words and their features.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experimental evaluation is to assess the quality of the inferred clusters by comparison to a gold standard and an existing graph-based model of category acquisition.", "labels": [], "entities": [{"text": "category acquisition", "start_pos": 160, "end_pos": 180, "type": "TASK", "confidence": 0.7366352081298828}]}, {"text": "In addition, we are interested in the incremental version of the model, whether it is able to learn meaningful categories and how these changeover time.", "labels": [], "entities": []}, {"text": "In the following, we give details on the corpora we used, describe how model parameters were selected, and explain our evaluation procedure.", "labels": [], "entities": []}, {"text": "Our aim is to learn a set of clusters each of which corresponds to one gold category, i.e., it contains all and only members of that gold category.", "labels": [], "entities": []}, {"text": "We report evaluation scores based on three metrics which measure this tradeoff.", "labels": [], "entities": []}, {"text": "Since in unsupervised clustering the cluster IDs are meaningless, all evaluation metrics involve a mapping from induced clusters to gold categories.", "labels": [], "entities": []}, {"text": "The first two metrics described below perform a cluster-based mapping and are thus not ideal for assessing the output of soft clustering algorithms.", "labels": [], "entities": []}, {"text": "The third metric performs an item-based mapping and can be directly used to evaluate soft clusters.", "labels": [], "entities": []}, {"text": "Purity/Collocation are based on member overlap between induced clusters and gold classes (.", "labels": [], "entities": []}, {"text": "Purity measures the degree to which each cluster contains instances that share the same gold class, while collocation measures the degree to which instances with the same gold class are assigned to a single cluster.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9464629292488098}]}, {"text": "We report the harmonic mean of purity and collocation as a single measure of clustering quality.", "labels": [], "entities": [{"text": "purity", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.975369393825531}]}, {"text": "V-Measure is the harmonic mean between homogeneity and collocation.", "labels": [], "entities": [{"text": "V-Measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.887301504611969}]}, {"text": "Like purity, V-Measure performs cluster-based comparisons but is an entropy-based method.", "labels": [], "entities": []}, {"text": "It measures the conditional entropy of a cluster given a class, and vice versa.", "labels": [], "entities": []}, {"text": "Cluster-F1 is an item-based evaluation metric which we propose drawing inspiration from the supervised metric presented in.", "labels": [], "entities": []}, {"text": "Cluster-F1 maps each target word type to a gold cluster based on its soft class membership, and is thus appropriate for evaluation of soft clustering output.", "labels": [], "entities": []}, {"text": "We first create a K \u00d7 G soft mapping matrix M from each induced category k i to gold classes g j from P(g j |k i ).", "labels": [], "entities": []}, {"text": "We then map each target word type to a gold class by multiplying its probability distribution over soft clusters with the mapping matrix M , and taking the maximum value.", "labels": [], "entities": []}, {"text": "Finally, we compute standard precision, recall and F1 between the mapped system categories and the gold classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9715933799743652}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9997103810310364}, {"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9994369149208069}]}], "tableCaptions": [{"text": " Table 1: Evaluation of model output against a gold standard. Results are reported for the BayesCat model  trained incrementally (BC-Inc) and in batch mode (BC-Batch), and Chinese Whispers (CW). The type  of clusters being evaluated is shown within parentheses.", "labels": [], "entities": []}]}