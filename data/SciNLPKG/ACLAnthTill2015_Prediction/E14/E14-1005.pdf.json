{"title": [{"text": "A Joint Model for Quotation Attribution and Coreference Resolution", "labels": [], "entities": [{"text": "Quotation Attribution and Coreference Resolution", "start_pos": 18, "end_pos": 66, "type": "TASK", "confidence": 0.7900985836982727}]}], "abstractContent": [{"text": "We address the problem of automatically attributing quotations to speakers, which has great relevance in text mining and media monitoring applications.", "labels": [], "entities": [{"text": "automatically attributing quotations to speakers", "start_pos": 26, "end_pos": 74, "type": "TASK", "confidence": 0.7541905641555786}, {"text": "text mining", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.8412397801876068}]}, {"text": "While current systems report high accuracies for this task, they either work at mention-level (getting credit for detecting uninfor-mative mentions such as pronouns), or assume the coreferent mentions have been detected beforehand; the inaccuracies in this preprocessing step may lead to error propagation.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a joint model for entity-level quotation attri-bution and coreference resolution, exploiting correlations between the two tasks.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.9581433832645416}]}, {"text": "We design an evaluation metric for attribu-tion that captures all speakers' mentions.", "labels": [], "entities": []}, {"text": "We present results showing that both tasks benefit from being treated jointly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quotations area crucial part of news stories, giving the perspectives of the participants in the narrated event, and making the news sound objective.", "labels": [], "entities": []}, {"text": "The ability of extracting and organizing these quotations is highly relevant for text mining applications, as it may aid journalists in fact-checking, help users browse news threads, and reduce human intervention in media monitoring.", "labels": [], "entities": [{"text": "extracting and organizing these quotations", "start_pos": 15, "end_pos": 57, "type": "TASK", "confidence": 0.6775946021080017}, {"text": "text mining", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.7639681100845337}]}, {"text": "This involves assigning the correct speaker to each quote-a problem called quotation attribution ( \u00a72).", "labels": [], "entities": [{"text": "quotation attribution", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.8947561681270599}]}, {"text": "There is significant literature devoted to this task, both for narrative genres ( and newswire domains.", "labels": [], "entities": []}, {"text": "While the earliest works focused on devising lexical and syntactic rules and hand-crafting grammars, there has been a recent shift toward machine learning approaches, with latest works reporting high accuracies for speaker identification in newswire (in the range 80-95% for direct and mixed quotes, according to).", "labels": [], "entities": [{"text": "speaker identification", "start_pos": 215, "end_pos": 237, "type": "TASK", "confidence": 0.7620471119880676}]}, {"text": "Despite these encouraging results, quotation mining systems are not yet fully satisfactory, even when only direct quotes are considered.", "labels": [], "entities": [{"text": "quotation mining", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.9858957529067993}]}, {"text": "Part of the problem, as we next describe, has to do with inaccuracies in coreference resolution ( \u00a73).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.9748547673225403}]}, {"text": "The \"easiest\" instances of quotation attribution problems arise when the speaker and the quote are semantically connected, e.g., through a reported speech verb like said.", "labels": [], "entities": [{"text": "quotation attribution", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.9582202732563019}]}, {"text": "However, in newswire text, the subject of this verb is commonly a pronoun or another uninformative anaphoric mention.", "labels": [], "entities": []}, {"text": "While the speaker thus determined may well be correctbeing inmost cases consistent with human annotation choices-from a practical perspective, it will be of little use without a coreference system that correctly resolves the anaphora.", "labels": [], "entities": []}, {"text": "Since the current state of the art in coreference resolution is far from perfect, errors at this stage tend to propagate to the quote attribution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.9749549031257629}]}, {"text": "Consider the following examples for illustration (taken from the WSJ-1057 and WSJ-0089 documents in the Penn Treebank), where we have annotated with subscripts some of the mentions: (a) Rivals carp at \"the principle of Since it is unlikely that the speaker is co-referent to a third-person pronoun he inside the quote, a pipeline system would likely attribute (incorrectly) this quote to Pilson.", "labels": [], "entities": [{"text": "WSJ-1057 and WSJ-0089 documents in the Penn Treebank", "start_pos": 65, "end_pos": 117, "type": "DATASET", "confidence": 0.8485420942306519}]}, {"text": "In example (b), there are two quotes with the same speaker entity (as indicated by the cue she added).", "labels": [], "entities": []}, {"text": "This gives evidence that M 1 and M 6 should be coreferent.", "labels": [], "entities": [{"text": "M 6", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.8050168752670288}]}, {"text": "A pipeline approach would not be able to exploit these correlations.", "labels": [], "entities": []}, {"text": "We argue that this type of mistakes, among others, can be prevented by a system that performs quote attribution and coreference resolution jointly ( \u00a74).", "labels": [], "entities": [{"text": "quote attribution", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.723234236240387}, {"text": "coreference resolution", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.8914332091808319}]}, {"text": "Our joint model is inspired by recent work in coreference resolution that independently ranks the possible mention's antecedents, forming a latent coreference tree structure).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.9560877680778503}]}, {"text": "We consider a generalization of these structures which we calla quotation-coreference tree.", "labels": [], "entities": []}, {"text": "To effectively couple the two tasks, we need to go beyond simple arc-factored models and consider paths in the tree.", "labels": [], "entities": []}, {"text": "We formulate the resulting problem as a logic program, which we tackle using a dual decomposition strategy ( \u00a75).", "labels": [], "entities": []}, {"text": "We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution ( \u00a76).", "labels": [], "entities": [{"text": "entity-level quotation attribution", "start_pos": 143, "end_pos": 177, "type": "TASK", "confidence": 0.6527812679608663}]}], "datasetContent": [{"text": "We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 33, "end_pos": 65, "type": "DATASET", "confidence": 0.9374767797333854}, {"text": "CoNLL-2011 coreference shared task (Pradhan et al., 2011)", "start_pos": 94, "end_pos": 151, "type": "TASK", "confidence": 0.5186986137520183}, {"text": "coreference resolution", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.9627945423126221}]}, {"text": "This dataset includes train, development and test partitions, annotated with coreference information, as well as gold and automatically generated syntactic and semantic information.", "labels": [], "entities": []}, {"text": "The CoNLL-2011 corpus does not contain annotations of quotation attribution.", "labels": [], "entities": [{"text": "CoNLL-2011 corpus", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9776251316070557}, {"text": "quotation attribution", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.8496069312095642}]}, {"text": "For that reason, we used the WSJ quotation annotations in the PARC dataset.", "labels": [], "entities": [{"text": "WSJ quotation annotations", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.8401983976364136}, {"text": "PARC dataset", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.9669325649738312}]}, {"text": "We used the same version of the corpus as O', but with different splits, to match the dataset partitions in the coreference resolution data.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.8683389723300934}]}, {"text": "This attribution corpus contains 279 documents of the 597 CoNLL-2011 files, having a total of 1199 annotated quotes.", "labels": [], "entities": [{"text": "CoNLL-2011 files", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9656363129615784}]}, {"text": "As in that work, we only considered directed speech quotes and the direct part of mixed quotes (quotes with both direct and undirected speech).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Coreference obtained with the CoNLL scorer (version 5) in the test partition of the WJS cor- pus, for the SURFACE system of Durrett and Klein (2013), our baseline implementation of the that sys- tem (SURFACE-OURS), and our JOINT approach. All systems were trained in the WSJ portion of the  Ontonotes.", "labels": [], "entities": [{"text": "WJS cor- pus", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.8788955211639404}, {"text": "WSJ", "start_pos": 281, "end_pos": 284, "type": "DATASET", "confidence": 0.8958740234375}, {"text": "Ontonotes", "start_pos": 301, "end_pos": 310, "type": "DATASET", "confidence": 0.9261317849159241}]}, {"text": " Table 5: Attribution results obtained, in the test  set, for the three baseline systems and our joint  system.", "labels": [], "entities": []}]}