{"title": [{"text": "Improving Word Alignment Using Linguistic Code Switching Data", "labels": [], "entities": [{"text": "Improving Word Alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.892453650633494}]}], "abstractContent": [{"text": "Linguist Code Switching (LCS) is a situation where two or more languages show up in the context of a single conversation.", "labels": [], "entities": [{"text": "Linguist Code Switching (LCS)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7594773272673289}]}, {"text": "For example, in English-Chinese code switching, there might be a sentence like \"\u00b7 \u201a15\u00a9\u00a8k\u201a15\u00a9\u00a8\u201a15\u00a9\u00a8k \u2021meeting (We will have a meeting in 15 minutes)\".", "labels": [], "entities": [{"text": "code switching", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7072494328022003}, {"text": "\u00a9\u00a8k\u201a15\u00a9\u00a8\u201a15\u00a9\u00a8k \u2021meeting", "start_pos": 85, "end_pos": 108, "type": "METRIC", "confidence": 0.7089760214090347}]}, {"text": "Traditional machine translation (MT) systems treat LCS data as noise, or just as regular sentences.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8521625638008118}]}, {"text": "However, if LCS data is processed intelligently, it can provide a useful signal for training word alignment and MT models.", "labels": [], "entities": [{"text": "training word alignment", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.6558851301670074}, {"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9914110898971558}]}, {"text": "Moreover, LCS data is from non-news sources which can enhance the diversity of training data for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9807682633399963}]}, {"text": "In this paper, we first extract constraints from this code switching data and then incorporate them into a word alignment model training procedure.", "labels": [], "entities": [{"text": "word alignment model training", "start_pos": 107, "end_pos": 136, "type": "TASK", "confidence": 0.8478283435106277}]}, {"text": "We also show that by using the code switching data, we can jointly train a word alignment model and a language model using co-training.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.7639694511890411}]}, {"text": "Our techniques for incorporating LCS data improve by 2.64 in BLEU score over a baseline MT system trained using only standard sentence-aligned corpora.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9801894128322601}]}], "introductionContent": [{"text": "Many language users are competent in multiple languages, and they often use elements of multiple languages in conversations with other speakers with competence in the same set of languages.", "labels": [], "entities": []}, {"text": "For example, native Mandarin speakers who also speak English might use English words in a Chinese sentence, like \"\\ \u2022 \u00f9 \u2021 \u00af K solution\u00ed \u00ba(Do you know the solution to this problem ?)\".", "labels": [], "entities": []}, {"text": "This phenomenon of mixing * *The author is working at Raytheon BBN Technologies now languages within a single utterance is known as Linguistic Code Switching (LCS).", "labels": [], "entities": [{"text": "Raytheon BBN", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.7058175802230835}, {"text": "Linguistic Code Switching (LCS)", "start_pos": 132, "end_pos": 163, "type": "TASK", "confidence": 0.7508785823980967}]}, {"text": "Examples of these utterances are common in communities of speakers with a shared competency in multiple languages, such as Web forums for Chinese emigr\u00e9s to the United States.", "labels": [], "entities": []}, {"text": "For example, more than 50% of the sentences we collected from a Web forum (MITBBS.com) contains both Chinese and English.", "labels": [], "entities": []}, {"text": "Traditional word alignment models take a sentence-level aligned corpus as input and generate word-level alignments for each pair of parallel sentences.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7501076757907867}]}, {"text": "Automatically-gathered LCS data typically contains no sentence-level alignments, but it still has some advantages for training word alignment models and machine translation (MT) systems which are worth exploring.", "labels": [], "entities": [{"text": "sentence-level alignments", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.6998995542526245}, {"text": "word alignment", "start_pos": 127, "end_pos": 141, "type": "TASK", "confidence": 0.726369321346283}, {"text": "machine translation (MT)", "start_pos": 153, "end_pos": 177, "type": "TASK", "confidence": 0.8430928468704224}]}, {"text": "First, because it contains multiple languages in the same sentence and still has a valid meaning, it will tell the relationship between the words from different languages to some extent.", "labels": [], "entities": []}, {"text": "Second, most LCS data is formed during people's daily conversation, and thus it contains a diversity of topics that people care about, such as home furnishings, cars, entertainment, etc, that may not show up in standard parallel corpora.", "labels": [], "entities": []}, {"text": "Moreover, LCS data is easily accessible from Web communities, such as MITBBS.com, Sina Weibo, Twitter, etc.", "labels": [], "entities": [{"text": "MITBBS.com", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.956478476524353}]}, {"text": "However, like most unedited natural language text on the Web, LCS data contains symbols like emotions, grammar and spelling mistakes, slang and strongly idiomatic usage, and a variety of other phenomena that are difficult to handle.", "labels": [], "entities": []}, {"text": "LCS data with different language pairs may also need special handling.", "labels": [], "entities": []}, {"text": "For instance, focus on words in mixed English and Hindi texts where a single word contains elements from both languages; they propose techniques for translating such words into both pure English and pure Hindi.", "labels": [], "entities": []}, {"text": "Our study focuses on ChineseEnglish LCS, where this is rarely a problem, 1 but for other language pairs, Sinha and Thakur's techniques maybe required as preprocessing steps.", "labels": [], "entities": [{"text": "ChineseEnglish LCS", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.7409860193729401}]}, {"text": "Primarily, though, LCS data requires special-purpose algorithms to use it for word alignment, since it contains no explicit alignment labels.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.7919230461120605}]}, {"text": "In this paper, we investigate two approaches to using LCS data for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7819737493991852}]}, {"text": "The first approach focuses exclusively on word alignment, and uses patterns extracted from LCS data to guide the EM training procedure for word alignment over a standard sentence-aligned parallel corpus.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7935546934604645}, {"text": "word alignment", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.7660986185073853}]}, {"text": "We focus on two types of patterns in the LCS data: first, English words are almost never correct translations for any Chinese word in the same LCS utterance.", "labels": [], "entities": [{"text": "LCS data", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.7918753623962402}]}, {"text": "Second, for sentences that are mostly Chinese but with some English words, if we propose substitutes for the English words using a Chinese language model, those substitutes are often good translations of the English words.", "labels": [], "entities": []}, {"text": "We incorporate these patterns into EM training via the posterior regularization framework ().", "labels": [], "entities": [{"text": "EM training", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9014356136322021}]}, {"text": "Our second approach treats the alignment and language model as two different and complementary views of the data.", "labels": [], "entities": []}, {"text": "We apply the cotraining paradigm for semi-supervised learning to incorporate the LCS data into the training procedures for the alignment model and the language model.", "labels": [], "entities": []}, {"text": "From the translation table of the alignment model, the training procedure finds candidate translations of the English words in the LCS data, and uses those to supplement the language model training data.", "labels": [], "entities": [{"text": "LCS data", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.7920169234275818}]}, {"text": "From the language model, the training procedure identifies Chinese words that complete the Chinese sentence with high probability, and it uses the English word paired with these completion words as additional training points for translation probabilities.", "labels": [], "entities": []}, {"text": "These models are trained repeatedly until they converge to similar predictions on the LCS data.", "labels": [], "entities": [{"text": "LCS data", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.7824601531028748}]}, {"text": "In combination with a larger phrase-based MT system (), these two training procedures yield an MT system that achieves a BLEU score of 31.79 on an English-to-Chinese translation task, an improvement of 2.64 in BLEU score over a baseline MT system trained on only our parallel corpora.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.8453083634376526}, {"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9766701459884644}, {"text": "BLEU score", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9820622205734253}, {"text": "English-to-Chinese translation task", "start_pos": 147, "end_pos": 182, "type": "TASK", "confidence": 0.7270476420720419}, {"text": "BLEU score", "start_pos": 210, "end_pos": 220, "type": "METRIC", "confidence": 0.9835336208343506}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section presents related work.", "labels": [], "entities": []}, {"text": "Section 3 gives an overview of word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7823257446289062}]}, {"text": "Sections 4 and 5 detail our two algorithms.", "labels": [], "entities": []}, {"text": "Section 6 presents our experiments and discusses results, and Section 7 concludes and discusses future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our LCS-driven training algorithms on an English-to-Chinese translation task.", "labels": [], "entities": [{"text": "English-to-Chinese translation task", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.7577691972255707}]}, {"text": "We use Moses (), a phrasebased translation system that learns from bilingual sentence-aligned corpora as the MT system.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.6902080476284027}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.8855234384536743}]}, {"text": "We supplement the baseline word alignment model in Moses with our LCS data, constrained training procedure, and co-training algorithm as well as IBM 3 model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.704913392663002}]}, {"text": "Because IBM 3 model is a fertility based model which might also alleviate Algorithm 2: Co-training for word alignment and language modeling 1: Input: parallel data X p , LCS data X LCS , language model training data X l 2: Initialize translation table tb for IBM1 model 3: For iteration from 1 to MAX tb \u2190 Train-IBM(X p ) tb \u2190 Train-HMM(X p |tb) 4: For each sentence xi in X LCS : For each source word s j in xi : 1) find the translation t j of s j with with probability p j from tb 2) replace s j with t j and update sentence's probability Extract the tri-gram gram 3 from LM 7: For each sentence xi in X LCS : run Algorithm 1: finding t similar 8: update tb using (t m , s j ) where t m \u2208 t similar and s j \u2208 xi 9: End For 10: Output: word alignment for X p and LM some of the problems caused by LCS data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.8082498013973236}, {"text": "End", "start_pos": 717, "end_pos": 720, "type": "METRIC", "confidence": 0.9848672747612}, {"text": "word alignment", "start_pos": 737, "end_pos": 751, "type": "TASK", "confidence": 0.7925397753715515}]}, {"text": "To clarify, we use IBM1 model and HMM models in succession for the baseline.", "labels": [], "entities": [{"text": "IBM1 model", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.8130811452865601}]}, {"text": "We trained the IBM1 model first and used the resulting parameters as the initial parameter values to train HMM model.", "labels": [], "entities": [{"text": "IBM1", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.8321061730384827}]}, {"text": "Parameters for the final MT system are tuned with Minimum Error Rate Training (MERT).", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9520581364631653}, {"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9832469820976257}, {"text": "Minimum Error Rate Training (MERT)", "start_pos": 50, "end_pos": 84, "type": "METRIC", "confidence": 0.9521230629512242}]}, {"text": "The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences.", "labels": [], "entities": [{"text": "MERT", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.9514577388763428}, {"text": "NIST MT06 data set", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9222688376903534}]}, {"text": "We test the system on NIST MT02 (878 sentences).", "labels": [], "entities": [{"text": "NIST MT02", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.8698065280914307}]}, {"text": "To evaluate the word alignment results, we manually aligned 250 sentences from NIST MT02 data set.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7237110137939453}, {"text": "NIST MT02 data set", "start_pos": 79, "end_pos": 97, "type": "DATASET", "confidence": 0.9364812672138214}]}, {"text": "For simplicity, we only have two types of labels for evaluating word alignments: either two words are aligned together or not.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7028844207525253}]}, {"text": "(Previous evaluation metrics also consider a third label for \"possible\" alignments.)", "labels": [], "entities": []}, {"text": "Out of the word-aligned data, we use 100 sentences as a development set and the rest as our testing set.", "labels": [], "entities": []}, {"text": "Our MT training corpus contains 2,636,692 sentence pairs from two parallel corpora: Hong Kong News (LDC2004T08) and Chinese English News Magazine Parallel Text (LDC2005T10).", "labels": [], "entities": [{"text": "MT training", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8917826116085052}, {"text": "Hong Kong News (LDC2004T08)", "start_pos": 84, "end_pos": 111, "type": "DATASET", "confidence": 0.9198140799999237}, {"text": "Chinese English News Magazine Parallel Text (LDC2005T10)", "start_pos": 116, "end_pos": 172, "type": "DATASET", "confidence": 0.7868024276362525}]}, {"text": "We use the Stanford Chinese segmenter to segment the Chinese data.", "labels": [], "entities": [{"text": "Stanford Chinese segmenter", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.8856041232744852}]}, {"text": "We use a ngram model package called SRILM) to train 6 the language model.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.4903460144996643}]}, {"text": "Because our modified ngram counts contain factions, we used WittenBell smoothing which supports fractional counts.", "labels": [], "entities": []}, {"text": "The 3-gram language model is trained on the Xinhua section of the Chinese Gigaword corpus (LDC2003T09) as well as the Chinese side of the parallel corpora.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus (LDC2003T09)", "start_pos": 66, "end_pos": 102, "type": "DATASET", "confidence": 0.8871259093284607}]}, {"text": "We also removed the sentences in MT02 from the Gigaword corpus if there is any to avoid the biases.", "labels": [], "entities": [{"text": "MT02", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8581913113594055}, {"text": "Gigaword corpus", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9319624304771423}]}, {"text": "We gather the LCS data from \"MITBBS.com,\" a popular forum for Chinese people living in the United States.", "labels": [], "entities": [{"text": "LCS data from \"MITBBS.com", "start_pos": 14, "end_pos": 39, "type": "DATASET", "confidence": 0.8392822623252869}]}, {"text": "This forum is separated by discussion topic, and includes topics such as \"Travel\", \"News\", and \"Living style\".", "labels": [], "entities": []}, {"text": "We extract data from 29 different topics.", "labels": [], "entities": []}, {"text": "To cleanup the LCS data, we get rid of HTML mark-up, and we remove patterns that are commonly repeated in forums, like \"Re:\" (for \"reply\" posts) and \"[= 1]\" (for \"repost\").", "labels": [], "entities": []}, {"text": "We change all English letters written in Chinese font into English font.", "labels": [], "entities": []}, {"text": "We stem the English words in both the parallel training data and the LCS data.", "labels": [], "entities": [{"text": "LCS data", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.8808068037033081}]}, {"text": "After the cleaning step, we have 245,470 sentences in the LCS data.", "labels": [], "entities": [{"text": "LCS data", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.837895005941391}]}, {"text": "120,922 of them actually contain both Chinese and English in the same sentence.", "labels": [], "entities": []}, {"text": "101,302 of them contain only Chinese, and we add these into the language model training data.", "labels": [], "entities": []}, {"text": "We discard the sentences that only contain English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word alignment results (PR + means PR+BA+EA).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6583972871303558}, {"text": "PR + means PR+BA+EA", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.6302804537117481}]}, {"text": " Table 2: Translation tables of \"badminton\" before and after incorporation of LCS data.", "labels": [], "entities": [{"text": "Translation tables of \"badminton\"", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6096055507659912}, {"text": "LCS data", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.6958774626255035}]}, {"text": " Table 3: Machine translation results. All entries marked", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8199309408664703}]}]}