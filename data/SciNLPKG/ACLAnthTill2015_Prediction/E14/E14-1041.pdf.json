{"title": [{"text": "Acquisition of Noncontiguous Class Attributes from Web Search Queries", "labels": [], "entities": [{"text": "Acquisition of Noncontiguous Class Attributes from Web Search Queries", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.6700432002544403}]}], "abstractContent": [{"text": "Previous methods for extracting attributes (e.g., capital, population) of classes (Em-pires) from Web documents or search queries assume that relevant attributes occur verbatim in the source text.", "labels": [], "entities": [{"text": "extracting attributes (e.g., capital, population) of classes (Em-pires) from Web documents or search queries", "start_pos": 21, "end_pos": 129, "type": "TASK", "confidence": 0.5377015814185142}]}, {"text": "The extracted attributes are short phrases that correspond to quantifiable properties of various instances (ottoman empire, ro-man empire, mughal empire) of the class.", "labels": [], "entities": []}, {"text": "This paper explores the extraction of non-contiguous class attributes (manner (it) claimed legitimacy of rule), from fact-seeking and explanation-seeking queries.", "labels": [], "entities": [{"text": "extraction of non-contiguous class attributes", "start_pos": 24, "end_pos": 69, "type": "TASK", "confidence": 0.7991669535636902}]}, {"text": "The attributes cover properties that are not always likely to be extracted as short phrases from inherently-noisy queries.", "labels": [], "entities": []}], "introductionContent": [{"text": "Motivation: Resources such as Wikipedia) and Freebase () aim at organizing knowledge around classes (Food ingredients, Astronomical objects, Religions) and their instances (wheat flower, uranus, hinduism).", "labels": [], "entities": []}, {"text": "Due to inherent limitations associated with maintaining and expanding human-curated resources, their content maybe incomplete.", "labels": [], "entities": []}, {"text": "For example, attributes representing the energy (or energy per 100g) or solubility in water are available in both Wikipedia and Freebase for many instances of Food ingredients (e.g., for olive oil, honey, fennel).", "labels": [], "entities": []}, {"text": "But the attributes are missing for some instances (e.g., cornmeal).", "labels": [], "entities": []}, {"text": "Moreover, structured information about how long (it) lasts unopened or manner (it) helps in weight loss is generally missing for Food ingredients, from both resources.", "labels": [], "entities": [{"text": "weight loss", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.7240703701972961}]}, {"text": "Such information is also often absent from among the attributes acquired from either documents or queries by previous extraction methods.", "labels": [], "entities": []}, {"text": "Previously extracted attributes tend to be short, often nominal, phrases like nutritional value and taste.", "labels": [], "entities": []}, {"text": "Even when extracted attributes are not nominal, they remain relatively short phrases such as good for skin.", "labels": [], "entities": []}, {"text": "As such, previous attributes have limited ability to capture the finer-grained properties being asked about in queries such as \"how long does olive oil last unopened\" and \"how does honey help in weight loss\".", "labels": [], "entities": [{"text": "weight loss", "start_pos": 195, "end_pos": 206, "type": "TASK", "confidence": 0.6455826014280319}]}, {"text": "The presence of such queries suggests that such information is relevant to Web users.", "labels": [], "entities": []}, {"text": "Identifying noncontiguous properties, or attributes of interest to Web users, helps filling some of the gaps in existing knowledge resources, which otherwise could not be filled by attributes extracted with previous methods.", "labels": [], "entities": []}, {"text": "Contributions: The contributions of this paper are twofold.", "labels": [], "entities": []}, {"text": "First, it introduces a method for the acquisition of noncontiguous class attributes, from factor explanation-seeking Web search queries like \"how long does olive oil last unopened\" or \"how does honey help in weight loss\".", "labels": [], "entities": [{"text": "weight loss", "start_pos": 208, "end_pos": 219, "type": "TASK", "confidence": 0.6798340678215027}]}, {"text": "The resulting attributes are more diverse than, and therefore subsume, the scope of attributes extracted by previous methods.", "labels": [], "entities": []}, {"text": "Indeed, previous methods are unlikely to extract attributes as specific as length/duration (it) lasts unopened and manner (it) helps in weight loss, for the instances olive oil and honey of the class Food ingredients.", "labels": [], "entities": []}, {"text": "Conversely, previously extracted attributes like nutritional value and solubility in water are roughly equivalent to the finer-grained nutritional value (it) has and reason (it) dissolves in water, extracted from the queries \"what nutritional value does honey have\" and \"why does glucose dissolve in water\" respectively.", "labels": [], "entities": []}, {"text": "Second, the noncontiguous attributes can be simultaneously interpreted as binary relations pertaining to instances and classes.", "labels": [], "entities": []}, {"text": "The relations (helps in weight loss) connect an instance (honey) or, more generally, a class (Food ingredients), on one hand; and a loosely-typed unknown argument (manner) whose value is of interest to Web users, on the other hand.", "labels": [], "entities": [{"text": "weight loss", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.7214014083147049}]}, {"text": "Because Web users already inquire about the value of one of their arguments, the extracted relations are more likely to be relevant for the respective instances and classes, than relations extracted from arbitrary document sentences).", "labels": [], "entities": []}], "datasetContent": [{"text": "Textual Data Sources: The experiments rely on a random sample of around 1 billion fullyanonymized queries in English, submitted to a general-purpose Web search engine.", "labels": [], "entities": []}, {"text": "Each query is available independently from other queries, and is accompanied by its frequency of occurrence in the query logs.", "labels": [], "entities": []}, {"text": "Target Classes: shows the set of 40 target classes for evaluating the attributes extracted from queries.", "labels": [], "entities": []}, {"text": "In an effort to reuse experimental setup proposed in previous work, each of the 40 manually-compiled classes introduced in) is mapped into the Wikipedia category that best matches it.", "labels": [], "entities": []}, {"text": "As a prerequisite, the portion I of the patterns from the table must match a disambiguated instance from a query.", "labels": [], "entities": []}, {"text": "A variation of the tagger introduced in maps query fragments to their disambiguated, corresponding Wikipedia instances (i.e., to Wikipedia articles).", "labels": [], "entities": []}, {"text": "The tagger is simplified to select the longest instance mentions, and does not use gazetteers or queries for training.", "labels": [], "entities": []}, {"text": "Depending on the sources of textual data available for training, any taggers) that disambiguate text fragments relative to Wikipedia entries can be employed.", "labels": [], "entities": []}, {"text": "Attribute Accuracy: The top 50 attributes, from the ranked lists extracted for each target class, are manually assigned correctness labels.", "labels": [], "entities": []}, {"text": "As shown in, an attribute is marked as vital, if it must be present among representative attributes of the: Accuracy of top 50 class attributes extracted from fact-seeking and explanation-seeking queries, over the evaluation set of 40 target classes class; okay, if it provides useful but non-essential information; and wrong, if it is incorrect.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9965851306915283}]}, {"text": "For example, the attributes manner (it) generates its energy, manner (it) became a constellation and reason (it) has arms are annotated as vital, okay and wrong respectively for the class Astronomical objects.", "labels": [], "entities": []}, {"text": "To compute the precision score over a set of attributes, the correctness labels are converted to numeric values: vital to 1.0, okay to 0.5, and wrong to 0.0.", "labels": [], "entities": [{"text": "precision score", "start_pos": 15, "end_pos": 30, "type": "METRIC", "confidence": 0.9831662774085999}]}, {"text": "Precision is the sum of the correctness values of the attributes, divided by the number of attributes.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9784768223762512}]}, {"text": "summarizes the precision scores over the evaluation set of target classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9994767308235168}]}, {"text": "The scores vary from one class to another, for example 0.71 for Food ingredients but 0.94 for Chemical elements.", "labels": [], "entities": []}, {"text": "The average score is 0.76, indicating that attributes extracted from fact and explanationseeking queries have encouraging levels of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9961491823196411}]}, {"text": "The results already take into account the detection of near-duplicate attributes.", "labels": [], "entities": []}, {"text": "More precisely, the highest-ranked attribute in each group of near-duplicate attributes, examples of which are shown in, is retained and evaluated; the lower-ranked attributes from each group are not considered in the evaluation.", "labels": [], "entities": []}, {"text": "Attributes like number of passengers (it) can hold, number of passengers it fits and number of passengers it seats are nearly equivalent, but are still not marked as near-duplicates for the class Aircraft, when they should.", "labels": [], "entities": []}, {"text": "Conversely, the attribute location (it) lives is marked as a near-duplicate of location (it) lives in new york, when it should not.", "labels": [], "entities": []}, {"text": "Nevertheless, a significant number of near-duplicates, which would otherwise crowd the ranked lists of attributes with redundant information, are identified and discarded.", "labels": [], "entities": []}, {"text": "Target Class: Group of Near-Duplicate Attributes Actors: movies (it) plays in, played in, acts in, acted in, played, played on Automobiles: date (it) was first manufactured, first produced, first made Battles and operations of World War II: reason (it) happened, took place, occurred Chemical elements: manner (it) returns to the atmosphere, gets back into the atmosphere, got into the atmosphere, gets into the atmosphere, enters the environment, enters the atmosphere Companies: location (it) makes its products, manufactures its products, produces its products, gets its products, makes its products, manufactures their products Companies: date/time (it) began outsourcing, started outsourcing, outsourced Countries: date (it) got its independence, gained independence, gained its independence, got independence, got their independence, won its independence, achieved independence, received its independence, gained its freedom Diseases and disorders: length/duration (it) takes to show symptoms, takes to show up, takes to show, takes to appear, takes to manifest, takes to come out: Groups of near-duplicate attributes identified for various classes.", "labels": [], "entities": []}, {"text": "Attributes within a group are ranked according to their individual scores.", "labels": [], "entities": []}, {"text": "Removing all but the first attribute of each group, from the ranked list of attributes of the respective class, improves the diversity of the list Discussion: The set of patterns shown in is extensible.", "labels": [], "entities": []}, {"text": "Moreover, the patterns are subject to errors.", "labels": [], "entities": []}, {"text": "They may cause false matches, resulting in erroneous extractions.", "labels": [], "entities": []}, {"text": "The extent to which this occurs is indirectly measured in the overall precision results.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9994828701019287}]}, {"text": "The modification of some of the patterns, or the addition of new ones, would likely affect the expected coverage and precision of the extracted attributes.", "labels": [], "entities": [{"text": "coverage", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9910277724266052}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9965736865997314}]}, {"text": "If a pattern is particularly noisy, it is likely to cause systematic errors, and therefore produce attributes of lower quality.", "labels": [], "entities": []}, {"text": "Since attributes in Wikipedia and Freebase are initially entered manually by human editors, their correctness is virtually guaranteed.", "labels": [], "entities": []}, {"text": "As for attributes extracted automatically, previous comparisons indicate that attributes tend to have higher quality when extracted from queries instead of documents.", "labels": [], "entities": []}, {"text": "Indeed, a set of extraction patterns applied to text produces attributes whose average precision at rank 50 is 0.44 when extracted from documents, vs. 0.63 from queries ).", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9956235289573669}]}, {"text": "More importantly, previously available or extracted attributes are virtually always simple, short noun phrases like nutritional value, taste or solubility in water.", "labels": [], "entities": []}, {"text": "Even if not confined to noun phrases, they are still short, Run: [Ranked Attributes fora Sample of Classes] Class: Automobiles: D: [(it) goes on sale, (it) will goon sale, (it) is an engineering playground, (it) will be available in japan, (it) shows up in japan, (it) is a technical tour de force, (it) unveiled at tas 2008, (it) runs a 7:38, (it) is a unique car, (it) uses a premium midship package, (it) features an all-new 3.8-litre, (it) is one of the fastest cars, (it) made a quick drive-by, ..]", "labels": [], "entities": [{"text": "Run", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9709058403968811}]}, {"text": "Q: [price/quantity/degree (it) weights, year (it) was banned from bathurst, manner (it) launch control works, engine (it) has, kind of engine (it) has, price/quantity/degree (it) costs in japan, number of horsepower (it) has, price/quantity/degree horsepower (it) has, number of seats (it) has, speed (it) goes, who designed (it), ..]", "labels": [], "entities": []}, {"text": "Class: Mobile phones: D: [(it) was announced on september 17 2008, (it) ceased with version, (it) was scheduled to be released in late 2010, (it) also supports qt (toolkit), (it) supports hardware capable, (it) can synchronize with microsoft outlook, (it) also supports python (programming language), ..]", "labels": [], "entities": []}, {"text": "Q: [date/time (it) came out in australia, who carries (it), reason (it) keeps rebooting, colours (it) comes in, video format (it) supports, date/time (it) was released, date/time (it) came out in the uk, length/duration (it)'s battery lasts, who sells (it), how much   like vegan, healthy or gluten free.", "labels": [], "entities": [{"text": "uk", "start_pos": 200, "end_pos": 202, "type": "DATASET", "confidence": 0.9787348508834839}]}, {"text": "In comparison, attributes extracted in this paper accommodate properties that are sometimes awkward or even impossible to express through short phrases.", "labels": [], "entities": []}, {"text": "Noncontiguous Attributes as Relations: Noncontiguous attributes extracted from fact-seeking queries are embodiments of relations linking the instances mentioned in the queries, on one hand, and the values being requested by the queries, on the other hand.", "labels": [], "entities": []}, {"text": "Therefore, the method proposed in this paper can also be regarded as a method for the acquisition of relevant relations of various classes.", "labels": [], "entities": []}, {"text": "The extracted relations specify the left argument (i.e., the instance) and the linking relation name (i.e., the attribute).", "labels": [], "entities": []}, {"text": "They only specify the type of the, but not the actual, right argument (i.e., the value being requested).", "labels": [], "entities": []}, {"text": "An additional experiment compares the accuracy of relations extracted as noncontiguous attributes from queries, vs. relations extracted by a previous open-domain method) from 500 million Web documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.999247670173645}]}, {"text": "The previous method, including its extraction patterns and its ranking scheme, is designed with instances rather than classes in mind.", "labels": [], "entities": []}, {"text": "For fairness to the method in), the evaluation procedure is slightly adjusted.", "labels": [], "entities": []}, {"text": "The set of instances associated with each target class, over which the two methods are evaluated, is reduced to a single representative instance selected a-priori.", "labels": [], "entities": []}, {"text": "The instances are shown as the first instances in parentheses for each class in the earlier.", "labels": [], "entities": []}, {"text": "Thus, the class attributes are extracted using only the instances keanu reeves, boeing 737 and bugs bunny in the case of the classes Actors, Aircraft and Animated characters respectively.", "labels": [], "entities": []}, {"text": "suggests that noncontiguous attributes extracted from queries tend to capture higherquality relations than arbitrary relations extracted from documents.", "labels": [], "entities": []}, {"text": "Because fact-seeking queries inquire about the value of some relations (attributes) of an instance, the relations themselves tends to be more relevant than relations extracted from arbitrary document sentences.", "labels": [], "entities": []}, {"text": "Nevertheless, relations derived from queries likely serve as a useful complement, rather than replacement, of relations from documents.", "labels": [], "entities": []}, {"text": "The former only discover what relations maybe relevant; the latter also identify their occurrences within text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Accuracy of top 50 class attributes ex- tracted from fact-seeking and explanation-seeking  queries, over the evaluation set of 40 target classes", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9897358417510986}]}]}