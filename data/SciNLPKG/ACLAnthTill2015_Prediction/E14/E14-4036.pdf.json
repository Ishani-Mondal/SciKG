{"title": [{"text": "Active Learning for Post-Editing Based Incrementally Retrained MT", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.8168790936470032}]}], "abstractContent": [{"text": "Machine translation, in particular statistical machine translation (SMT), is making big inroads into the localisation and translation industry.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8242919445037842}, {"text": "statistical machine translation (SMT)", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.82208351790905}, {"text": "translation", "start_pos": 122, "end_pos": 133, "type": "TASK", "confidence": 0.7780963182449341}]}, {"text": "In typical work-flows (S)MT output is checked and (where required) manually post-edited by human translators.", "labels": [], "entities": [{"text": "MT output", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.8151968419551849}]}, {"text": "Recently, a significant amount of research has concentrated on capturing human post-editing outputs as early as possible to incrementally up-date/modify SMT models to avoid repeat mistakes.", "labels": [], "entities": [{"text": "SMT", "start_pos": 153, "end_pos": 156, "type": "TASK", "confidence": 0.980519711971283}]}, {"text": "Typically in these approaches, MT and post-edits happen sequentially and chronologically, following the way unseen data (the translation job) is presented.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9929486513137817}]}, {"text": "In this paper, we add to the existing literature addressing the question whether and if so, to what extent, this process can be improved upon by Active Learning, where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to (whatever is) the remaining data.", "labels": [], "entities": []}, {"text": "We explore novel (source side-only) selection criteria and show performance increases of 0.67-2.65 points TER absolute on average on typical industry data sets compared to sequential PE-based incrementally retrained SMT.", "labels": [], "entities": [{"text": "TER absolute", "start_pos": 106, "end_pos": 118, "type": "METRIC", "confidence": 0.9775382280349731}, {"text": "SMT", "start_pos": 216, "end_pos": 219, "type": "TASK", "confidence": 0.641628086566925}]}], "introductionContent": [], "datasetContent": [{"text": "We use technical documentation data taken from Symantec translation memories for the EnglishFrench (EN-FR) and English-German (EN-DE) language pairs (both directions) for our experiments.", "labels": [], "entities": []}, {"text": "The statistics of the data (training and incremental splits) are shown in.", "labels": [], "entities": []}, {"text": "All the systems are trained using the Moses ( phrase-based statistical MT system, with IRSTLM () for language modelling (n-grams up to order five) and with the alignment heuristic grow-diag-final-and.", "labels": [], "entities": [{"text": "phrase-based statistical MT", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.4363996187845866}, {"text": "IRSTLM", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9747270345687866}, {"text": "language modelling", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7206713557243347}]}, {"text": "For the experiments, we considered two settings for each language pair in each direction.", "labels": [], "entities": []}, {"text": "In the first setting, the initial MT system is trained using the training set (39,679 and 54,907 sentence pairs for EN-FR and EN-DE, respectively).", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9859451651573181}]}, {"text": "Then, a batch of 500 source sentences is selected from the incremental dataset according to each of the selection criteria, and translations are obtained with the initial MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 171, "end_pos": 173, "type": "TASK", "confidence": 0.8608313798904419}]}, {"text": "These translations are postedited and the corrected translations are added to the training data.", "labels": [], "entities": []}, {"text": "We then train anew MT system using the updated training data (initial training data plus PEs of the first batch of sentences).", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9873242378234863}, {"text": "PEs", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9801227450370789}]}, {"text": "The updated model will be used to translate the next batch.", "labels": [], "entities": [{"text": "translate", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.9821109175682068}]}, {"text": "The same process is repeated until the incremental dataset is finished (16 and 20 iterations for English-French and English-German, respectively).", "labels": [], "entities": []}, {"text": "For each batch we compute the TER score between the MT output and the refererence translations for the sentences of that batch.", "labels": [], "entities": [{"text": "TER score", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9823683798313141}, {"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.7616974115371704}]}, {"text": "We then compute the average TER score for all the batches.", "labels": [], "entities": [{"text": "TER score", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9841597080230713}]}, {"text": "These average scores, for each selection criterion, are reported in.", "labels": [], "entities": []}, {"text": "In the second setting, instead of using the whole training data, we used a subset of (randomly selected) 5,000 sentence pairs for training the initial MT system and a subset of 20,000 sentences from the remaining data as the incremental dataset.", "labels": [], "entities": [{"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.9427151083946228}]}, {"text": "Here we take batches of 1,000 sentences (thus 20 batches).", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The first setting aims to reflect the situation where a translation job is to be completed fora domain for which we have a considerable amount of data available.", "labels": [], "entities": [{"text": "translation job", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.9187211692333221}]}, {"text": "Conversely, the second setting reflects the situation where a translation job is to be carried out fora domain with little (if any) available data.", "labels": [], "entities": [{"text": "translation job", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.8989488482475281}]}, {"text": "shows the TER scores per iteration for each of the criteria, for the scenario DE\u2192EN Setting 2 (the trends are similar for the other scenarios).", "labels": [], "entities": [{"text": "TER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9990517497062683}, {"text": "DE\u2192EN Setting 2", "start_pos": 78, "end_pos": 93, "type": "METRIC", "confidence": 0.7410483121871948}]}, {"text": "The two baselines exhibit slight improvement over the iterations, both starting at around .35 TER points and finishing at around .25 points.", "labels": [], "entities": [{"text": "TER", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9984866380691528}]}, {"text": "Conversely, all the three criteria start at very high scores (in the range [.5,.6]) and then improve considerably to arrive at scores below .1 for the last iterations.", "labels": [], "entities": []}, {"text": "Compared to Ngram and CED, CEDN reaches better scores earlier on, being the criterion with the lowest score up to iteration 13.", "labels": [], "entities": [{"text": "CED", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.7045474648475647}, {"text": "CEDN", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.508289635181427}]}, {"text": "show that AL for PE-based incremental MT retraining really works: all AL based methods (Ngram, CED, CEDN) show strong improvements over both baselines after the initial 8-9 iterations and best performance on the complete incremental data sets, resulting in a noticeable decrease of the overall TER score).", "labels": [], "entities": [{"text": "PE-based incremental MT retraining", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.7780072391033173}, {"text": "TER score", "start_pos": 294, "end_pos": 303, "type": "METRIC", "confidence": 0.9869917035102844}]}, {"text": "In six out of eight scenarios, our novel metric CEDN obtains the best result.", "labels": [], "entities": [{"text": "CEDN", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.6386713981628418}]}], "tableCaptions": [{"text": " Table 2: TER average scores for Setting 1", "labels": [], "entities": [{"text": "TER average", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9782684445381165}, {"text": "Setting", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9892127513885498}]}, {"text": " Table 3: TER average scores for Setting 2", "labels": [], "entities": [{"text": "TER average", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9777129292488098}, {"text": "Setting", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.98489910364151}]}, {"text": " Table 1: Data Statistics for English-French and English-German Symantec Translation Memory Data.  SL stands for sentence length, EN stands for English, FR stands for French and DE stands for German", "labels": [], "entities": [{"text": "Symantec Translation Memory Data", "start_pos": 64, "end_pos": 96, "type": "DATASET", "confidence": 0.7052133083343506}, {"text": "FR", "start_pos": 153, "end_pos": 155, "type": "METRIC", "confidence": 0.9482041001319885}, {"text": "DE", "start_pos": 178, "end_pos": 180, "type": "METRIC", "confidence": 0.9781033992767334}]}]}