{"title": [{"text": "A New Entity Salience Task with Millions of Training Examples", "labels": [], "entities": [{"text": "Entity Salience Task", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.7691688537597656}]}], "abstractContent": [{"text": "Although many NLP systems are moving toward entity-based processing, most still identify important phrases using classical keyword-based approaches.", "labels": [], "entities": []}, {"text": "To bridge this gap, we introduce the task of entity salience: assigning a relevance score to each entity in a document.", "labels": [], "entities": []}, {"text": "We demonstrate how a labeled corpus for the task can be automatically generated from a corpus of documents and accompanying abstracts.", "labels": [], "entities": []}, {"text": "We then show how a classifier with features derived from a standard NLP pipeline outperforms a strong baseline by 34%.", "labels": [], "entities": []}, {"text": "Finally, we outline initial experiments on further improving accuracy by leveraging background knowledge about the relationships between entities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9959880709648132}]}], "introductionContent": [{"text": "Information retrieval, summarization, and online advertising rely on identifying the most important words and phrases in web documents.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8264900147914886}, {"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9915728569030762}]}, {"text": "While traditional techniques treat documents as collections of keywords, many NLP systems are shifting toward understanding documents in terms of entities.", "labels": [], "entities": []}, {"text": "Accordingly, we need new algorithms to determine the prominence -the salience -of each entity in the document.", "labels": [], "entities": []}, {"text": "Toward this end, we describe three primary contributions.", "labels": [], "entities": []}, {"text": "First, we show how a labeled corpus for this task can be automatically constructed from a corpus of documents with accompanying abstracts.", "labels": [], "entities": []}, {"text": "We also demonstrate the validity of the corpus with a manual annotation study.", "labels": [], "entities": [{"text": "validity", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9463522434234619}]}, {"text": "Second, we train an entity salience model using features derived from a coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.8526080250740051}]}, {"text": "This model significantly outperforms a baseline model based on sentence position.", "labels": [], "entities": []}, {"text": "Third, we suggest how our model can be improved by leveraging background information about the entities and their relationships -information not specifically provided in the document in question.", "labels": [], "entities": []}, {"text": "Our notion of salience is similar to that of Boguraev and: \"discourse objects with high salience are the focus of attention\", inspired by earlier work on Centering Theory ().", "labels": [], "entities": []}, {"text": "Here we take a more empirical approach: salient entities are those that human readers deem most relevant to the document.", "labels": [], "entities": []}, {"text": "The entity salience task in particular is briefly alluded to by, and addressed in the context of Twitter messages by.", "labels": [], "entities": []}, {"text": "It is also similar in spirit to the much more common keyword extraction task).", "labels": [], "entities": [{"text": "keyword extraction task", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.8222394585609436}]}], "datasetContent": [{"text": "To validate our alignment method for inferring entity salience, we conducted a manual evaluation.", "labels": [], "entities": []}, {"text": "Two expert linguists discussed the task and generated a rubric, giving them a chance to calibrate their scores.", "labels": [], "entities": []}, {"text": "They then independently annotated all detected entities in 50 random documents from our corpus (a total of 744 entities), without reading the accompanying abstracts.", "labels": [], "entities": []}, {"text": "Each entity was assigned a salience score in {1, 2, 3, 4}, where 1 is most salient.", "labels": [], "entities": []}, {"text": "We then thresholded the annotators' scores as salient/non-salient for comparison to the binary NYT labels.", "labels": [], "entities": []}, {"text": "summarizes the agreement results, measured by Cohen's kappa.", "labels": [], "entities": []}, {"text": "The experts' agreement is probably best described as moderate, 1 indicating that this is a difficult, subjective task, though deciding on the most salient entities (with score 1) is easier.", "labels": [], "entities": []}, {"text": "Even without calibrating to the induced NYT salience scores, the expert vs. NYT agreement is close enough to the inter-expert agreement to convince us that our induced labels area reasonable if somewhat noisy proxy for the experts' definition of salience.: Annotator agreement for entity salience as a binary classification.", "labels": [], "entities": []}, {"text": "A1 and A2 are expert annotators; NYT represents the induced labels.", "labels": [], "entities": [{"text": "NYT", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.8584482669830322}]}, {"text": "The first \u03ba column assumes annotator scores {1, 2} are salient and {3, 4} are non-salient, while the second \u03ba column assumes only scores of 1 are salient.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotator agreement for entity salience  as a binary classification. A1 and A2 are expert an- notators; NYT represents the induced labels. The  first \u03ba column assumes annotator scores {1, 2} are  salient and {3, 4} are non-salient, while the second  \u03ba column assumes only scores of 1 are salient.", "labels": [], "entities": [{"text": "NYT", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.8771355748176575}]}, {"text": " Table 3: Test set (P)recision, (R)ecall, and (F)  measure of the salient class for some com- binations of features listed in", "labels": [], "entities": [{"text": "F)  measure", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.8926243782043457}]}, {"text": " Table 2. The  centrality feature is discussed in Section 4.", "labels": [], "entities": [{"text": "centrality", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.9178832769393921}]}]}