{"title": [{"text": "Semi-supervised learning of morphological paradigms and lexicons", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a semi-supervised approach to the problem of paradigm induction from inflection tables.", "labels": [], "entities": [{"text": "paradigm induction", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7117790430784225}]}, {"text": "Our system extracts generalizations from inflection tables , representing the resulting paradigms in an abstract form.", "labels": [], "entities": []}, {"text": "The process is intended to be language-independent, and to provide human-readable generalizations of paradigms.", "labels": [], "entities": []}, {"text": "The tools we provide can be used by linguists for the rapid creation of lexical resources.", "labels": [], "entities": []}, {"text": "We evaluate the system through an inflection table reconstruction task using Wiktionary data for German, Spanish, and Finnish.", "labels": [], "entities": [{"text": "inflection table reconstruction", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.7286648750305176}]}, {"text": "With no additional corpus information available, the evaluation yields per word form accuracy scores on inflecting unseen base forms in different languages ranging from 87.81% (German nouns) to 99.52% (Span-ish verbs); with additional unlabeled text corpora available for training the scores range from 91.81% (German nouns) to 99.58% (Spanish verbs).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.994817316532135}]}, {"text": "We separately evaluate the system in a simulated task of Swedish lexicon creation, and show that on the basis of a small number of inflection tables, the system can accurately collect from a list of noun forms a lexicon with inflection information ranging from 100.0% correct (collect 100 words), to 96.4% correct (collect 1000 words).", "labels": [], "entities": [{"text": "Swedish lexicon creation", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.7500027020772299}]}], "introductionContent": [{"text": "Large scale morphologically accurate lexicon construction for natural language is a very timeconsuming task, if done manually.", "labels": [], "entities": [{"text": "Large scale morphologically accurate lexicon construction", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.6314291208982468}]}, {"text": "Usually, the construction of large-scale lexical resources presupposes a linguist who constructs a detailed morphological grammar that models inflection, compounding, and other morphological and phonological phenomena, and additionally performs a manual classification of lemmas in the language according to their paradigmatic behavior.", "labels": [], "entities": []}, {"text": "In this paper we address the problem of lexicon construction by constructing a semi-supervised system that accepts concrete inflection tables as input, generalizes inflection paradigms from the tables provided, and subsequently allows the use of unannotated corpora to expand the inflection tables and the automatically generated paradigms.", "labels": [], "entities": [{"text": "lexicon construction", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7221477925777435}]}, {"text": "In contrast to many machine learning approaches that address the problem of paradigm extraction, the current method is intended to produce human-readable output of its generalizations.", "labels": [], "entities": [{"text": "paradigm extraction", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7540273368358612}]}, {"text": "That is, the paradigms provided by the system can be inspected for errors by a linguist, and if necessary, corrected and improved.", "labels": [], "entities": []}, {"text": "Decisions made by the extraction algorithms are intended to be transparent, permitting morphological system development in tandem with linguist-provided knowledge.", "labels": [], "entities": []}, {"text": "Some of the practical tasks tackled by the system include the following: \u2022 Given a small number of known inflection tables, extract from a corpus a lexicon of those lemmas that behave like the examples provided by the linguist.", "labels": [], "entities": []}, {"text": "\u2022 Given a large number of inflection tablessuch as those provided by the crowdsourced lexical resource, Wiktionary-generalize the tables into a smaller number of abstract paradigms.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the method, we have conducted three experiments.", "labels": [], "entities": []}, {"text": "First we repeat an experiment presented in using the same data and experiment setup, but with our generalization method.", "labels": [], "entities": []}, {"text": "In this experiment, we are given a number of complete inflection tables scraped from Wiktionary.", "labels": [], "entities": []}, {"text": "The task is to reconstruct complete inflection tables from 200 held-out base forms.", "labels": [], "entities": []}, {"text": "For this task, we evaluate perform accuracy as well as per table accuracy for reconstruction.", "labels": [], "entities": [{"text": "perform", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9623337984085083}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8820787072181702}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8223492503166199}, {"text": "reconstruction", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.9602898359298706}]}, {"text": "The second experiment is the same as the first, but with additional access to an unlabeled text dump for the language from Wikipedia.", "labels": [], "entities": []}, {"text": "In the last experiment we try to mimic the situation of a linguist starting out to describe anew language.", "labels": [], "entities": []}, {"text": "The experiment uses a large-scale Swedish morphology as reference and evaluates how reliably a lexicon can be gathered from a word list using only a few manually specified inflection tables generalized into abstract paradigms by our system.", "labels": [], "entities": []}, {"text": "In our first experiment we start from the inflection tables in the development and test set from, henceforth D&DN13.", "labels": [], "entities": [{"text": "D&DN13", "start_pos": 109, "end_pos": 115, "type": "DATASET", "confidence": 0.9164643883705139}]}, {"text": "shows the number of input tables as well as the number of paradigms that they result in after generalization and collapsing.", "labels": [], "entities": []}, {"text": "For all cases, the number of output paradigms are below 10% of the number of input inflection tables.", "labels": [], "entities": []}, {"text": "shows the generalization rate achieved with the paradigms.", "labels": [], "entities": []}, {"text": "For instance, the 20 most common resulting German noun paradigms are sufficient to model almost 95% of the 2,564 separate inflection tables given as input.", "labels": [], "entities": []}, {"text": "As described earlier, in the reconstruction task, the input base forms are compared to the abstract: Generalization of paradigms.", "labels": [], "entities": []}, {"text": "The number of paradigms produced from Wiktionary inflection tables by generalization and collapsing of abstract paradigms.", "labels": [], "entities": []}, {"text": "paradigms by measuring the longest common suffix length for each input base form compared to the ones seen during training.", "labels": [], "entities": []}, {"text": "This approach is memory-based: it simply measures the similarity of a given lemma to the lemmas encountered during the learning phase.", "labels": [], "entities": []}, {"text": "presents our results juxtaposed with the ones reported by D&DN13.", "labels": [], "entities": [{"text": "D&DN13", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.7824230194091797}]}, {"text": "While scoring slightly below D&DN13 for the majority of the languages when measuring form accuracy, our method shows an advantage when measuring the accuracy of complete tables.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9492720365524292}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9980087876319885}]}, {"text": "Interestingly, the only case where we improve upon the form accuracy of D&DN13 is German verbs, where we get our lowest table accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.8265813589096069}, {"text": "D&DN13", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.7690626382827759}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.7342531681060791}]}, {"text": "further shows an oracle score, giving an upper bound for our method that would be achieved if we were always able to pick the best fitting paradigm available.", "labels": [], "entities": []}, {"text": "This upper bound ranges from 99% (Finnish verbs) to 100% (three out of five tests).", "labels": [], "entities": []}, {"text": "In our second experiment, we extend the previous experiment by adding access to a corpus.", "labels": [], "entities": []}, {"text": "Apart from measuring the longest common suffix length, we now also compute the frequency of the hypothetical candidate forms in every generated table and use this to favor paradigms that generate a large number of attested forms.", "labels": [], "entities": []}, {"text": "For this, we use a Wikipedia dump, from which we have extracted word-form frequencies.", "labels": [], "entities": []}, {"text": "In total, the number of word types in the Wikipedia corpus was 8.9M (German), 3.4M (Spanish), 0.7M (Finnish), and 2.7M (Swedish).", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.8579904437065125}]}, {"text": "where an increased accuracy is noted for all languages, as is to be expected since we have added more knowledge to the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9992613196372986}]}, {"text": "The bold numbers mark the cases where we outperform the result in, which is now the casein four out of five tests for table accuracy, scoring between 76.50% for German verbs and 98.00% for Spanish verbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9806580543518066}]}, {"text": "Measuring form accuracy, we achieve scores between 91.81% and 99.58%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9912492036819458}]}, {"text": "The smallest improvement is noted for Finnish verbs, which has the largest number of paradigms, but also the smallest corpus.", "labels": [], "entities": []}, {"text": "In this experiment we consider a task where we only have a small number of inflection tables, mimicking the situation where a linguist has manually entered a few inflection tables, allowed the system to generalize these into paradigms, and now faces the task of culling from a corpus-in this case labeled with basic POS information-the candidate words/lemmas that best fit the induced paradigms.", "labels": [], "entities": []}, {"text": "This would be atypical task during lexicon creation.", "labels": [], "entities": [{"text": "lexicon creation", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7131660282611847}]}, {"text": "We selected the 20 most frequent noun paradigms (from a total of 346), with one inflection table each, from our gold standard, the: Top-1000 rank for all nouns in SALDO Swedish lexical resource SALDO (.", "labels": [], "entities": [{"text": "SALDO Swedish lexical resource SALDO", "start_pos": 163, "end_pos": 199, "type": "DATASET", "confidence": 0.6643655061721802}]}, {"text": "From this set, we discarded paradigms that lack plural forms.", "labels": [], "entities": []}, {"text": "We also removed from the paradigms special compounding forms that Swedish nouns have, since compound information is not taken into account in this experiment.", "labels": [], "entities": []}, {"text": "The compounding forms are part of the original paradigm specification, and after a collapsing procedure after compound-form removal, we were left with a total of 11 paradigms.", "labels": [], "entities": []}, {"text": "In the next step we ranked all nouns in SALDO (79.6k lemmas) according to our confidence score, which indicates how well a noun fits a given paradigm.", "labels": [], "entities": [{"text": "SALDO", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.8923739790916443}, {"text": "confidence score", "start_pos": 78, "end_pos": 94, "type": "METRIC", "confidence": 0.9547058939933777}]}, {"text": "We then evaluated the paradigm assignment for the top-1000 lemmas.", "labels": [], "entities": []}, {"text": "Among these top-1000 words, we found 44 that were outside the 20 most frequent noun paradigms.", "labels": [], "entities": []}, {"text": "These words were not necessarily incorrectly assigned, since they may only differ in their compound forms; as a heuristic, we considered them correct if they had the same declension and gender as the paradigm, and incorrect otherwise.", "labels": [], "entities": []}, {"text": "displays the results, including a total accuracy of 96.4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9932154417037964}]}, {"text": "Next, we investigated the top-1000 distribution for individual paradigms.", "labels": [], "entities": []}, {"text": "This corresponds to the situation where a linguist has just entered anew inflection table and is looking for words that fit the resulting paradigm.", "labels": [], "entities": []}, {"text": "The result is presented in two The paradigms that lack plural forms are subsets of other paradigms.", "labels": [], "entities": []}, {"text": "In other words: when no plural forms are attested, we would need a procedure to decide if plural forms are even possible, which is currently beyond the scope of our method.", "labels": [], "entities": []}, {"text": "error rate plots: shows the low precision and high precision paradigms in two plots, where error rates range from 0-2% and 16-44% for the top 100 words.", "labels": [], "entities": [{"text": "error rate", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9221959114074707}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9626357555389404}]}, {"text": "We further investigated the worst-performing paradigm, p akademi (academy), to determine the reason for the high error rate for this particular item.", "labels": [], "entities": [{"text": "error rate", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9605422616004944}]}, {"text": "The main source of error (334 out of 1000) is confusion with p akribi (accuracy), which has no plural.", "labels": [], "entities": [{"text": "error", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9881623387336731}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9637203216552734}]}, {"text": "However, it is on semantic grounds that the paradigm has no plural; a native Swedish speaker would pluralize akribi like akademi (disregarding the fact that akribi is defective).", "labels": [], "entities": []}, {"text": "The second main type of error (210 out of 1000) is confusion with the unseen paradigm of parti (party), which inflects similarly to akademi, but with a difference in gender-difficult to predict from surface forms-that manifests itself in two out of eight word forms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Generalization of paradigms. The num- ber of paradigms produced from Wiktionary in- flection tables by generalization and collapsing of  abstract paradigms.", "labels": [], "entities": []}, {"text": " Table 4: Experiment 1: Accuracy of reconstructing 200 inflection tables given only base forms from  held-out data when paradigms are learned from the Wiktionary dataset. For comparison, figures from  Durrett and DeNero (2013) are included (shown as D&DN13).", "labels": [], "entities": [{"text": "Wiktionary dataset", "start_pos": 151, "end_pos": 169, "type": "DATASET", "confidence": 0.843595027923584}]}, {"text": " Table 5: Experiment 2: Reconstructing 200 held- out inflection tables with paradigms induced from  Wiktionary and further access to raw text from  Wikipedia.", "labels": [], "entities": []}, {"text": " Table 6: Top-1000 rank for all nouns in SALDO", "labels": [], "entities": [{"text": "Top-1000 rank", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9316419959068298}, {"text": "SALDO", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.6231656670570374}]}]}