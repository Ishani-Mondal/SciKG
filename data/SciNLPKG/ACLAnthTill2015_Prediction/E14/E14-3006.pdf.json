{"title": [{"text": "Resolving Coreferent and Associative Noun Phrases in Scientific Text", "labels": [], "entities": [{"text": "Resolving Coreferent and Associative Noun Phrases", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.9332581957181295}]}], "abstractContent": [{"text": "We present a study of information status in scientific text as well as ongoing work on the resolution of coreferent and associative anaphora in two different scientific disciplines, namely computational linguistics and genetics.", "labels": [], "entities": [{"text": "resolution of coreferent and associative anaphora", "start_pos": 91, "end_pos": 140, "type": "TASK", "confidence": 0.806542714436849}]}, {"text": "We present an annotated corpus of over 8000 definite descriptions in scientific articles.", "labels": [], "entities": []}, {"text": "To adapt a state-of-the-art coreference resolver to the new domain, we develop features aimed at modelling technical terminology and integrate these into the coreference resolver.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8211929202079773}, {"text": "coreference resolver", "start_pos": 158, "end_pos": 178, "type": "TASK", "confidence": 0.8459368348121643}]}, {"text": "Our results indicate that this integration, combined with domain-dependent training data, can outperform the performance of an out-of-the-box coreference resolver.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 142, "end_pos": 162, "type": "TASK", "confidence": 0.8535067737102509}]}, {"text": "For the (much harder) task of resolving as-sociative anaphora, our preliminary results show the need for and the effect of semantic features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Resolving anaphoric relations automatically requires annotated data for training and testing.", "labels": [], "entities": []}, {"text": "Anaphora and coreference resolution systems have been tested and evaluated on different genres, mainly news articles and dialogue.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.9011151194572449}]}, {"text": "However, for scientific text, annotated data are scarce and coreference resolution systems are lacking).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8647252321243286}]}, {"text": "We present a study of anaphora in scientific literature and show the difficulties that arise when resolving coreferent and associative entities in two different scientific disciplines, namely computational linguistics and genetics.", "labels": [], "entities": []}, {"text": "Coreference resolution in scientific articles is considered difficult due to the high proportion of definite descriptions (, which typically require domain knowledge to be resolved.", "labels": [], "entities": [{"text": "Coreference resolution in scientific articles", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.9074912190437316}]}, {"text": "The more complex nature of the texts is also reflected in the heavy use of abstract entities such as results or variables, while easy-to-resolve named entities are less frequently used.", "labels": [], "entities": []}, {"text": "We test an existing, state-of-the-art coreference resolution tool on scientific text, a domain on which it has not been trained, and adapt it to this new domain.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.8790175020694733}]}, {"text": "We also address the resolution of associative anaphora, a related phenomenon, which is also called bridging anaphora.", "labels": [], "entities": []}, {"text": "The interpretation of an associative anaphor is based on the associated antecedent, but the two are not coreferent.", "labels": [], "entities": []}, {"text": "Examples 1 and 2 show two science-specific cases of associative anaphora from our data.", "labels": [], "entities": []}, {"text": "(1) Xe-Ar was found to be in a layered structure with Aron the surface 1 . (2) We base our experiments on the Penn treebank.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.9954169690608978}]}, {"text": "The resolution of associative links is important because it can help in tasks which use the concept of textual coherence, e.g.'s entity grid or's text segmentation.", "labels": [], "entities": []}, {"text": "They might also be of use in higherlevel text understanding tasks such as textual entailment ( or summarisation based on argument overlap.", "labels": [], "entities": [{"text": "higherlevel text understanding", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.6393753488858541}, {"text": "textual entailment", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7349787056446075}, {"text": "summarisation based on argument overlap", "start_pos": 98, "end_pos": 137, "type": "TASK", "confidence": 0.678586483001709}]}, {"text": "showed that biological texts differ considerably from other text genres, such as news text or dialogue.", "labels": [], "entities": []}, {"text": "In this respect, our results confirm that the proportion between non-referring and referring entities in scientific text differs from that reported for other genres.", "labels": [], "entities": []}, {"text": "The same holds for the type and relative number of linguistic expressions used for reference.", "labels": [], "entities": []}, {"text": "To address this issue, we decided to investigate information status) of noun phrases.", "labels": [], "entities": []}, {"text": "Information status tells us whether a noun phrase refers to an already known entity, or whether it can be treated as nonreferring.", "labels": [], "entities": []}, {"text": "Since no corpus of full-text scientific articles annotated with both information status and anaphoric relations was available, we had to create and annotate our own corpus.", "labels": [], "entities": []}, {"text": "The main contributions of this work are (i) anew information status-based annotation scheme and an annotated corpus of scientific articles, (ii) a study of information status in scientific text that compares the distribution of the different categories in scientific text with the distribution in news text, as well as between the two scientific disciplines, (iii) experiments on the resolution of coreferent anaphora: we devise domain adaptation for science and show how this improves an out-of-the-box coreference resolver, and (iv) experiments on the resolution of associative anaphora with a coreference resolver that is adapted to this new notion of \"reference\" by including semantic features.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work on anaphora resolution in multi-discipline, full-text scientific papers that also deals with associative anaphora.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8965352177619934}]}], "datasetContent": [{"text": "We now compare the performance of an outof-the-box coreference system with the resolver trained on our annotated scientific corpus (Section 6.2).", "labels": [], "entities": []}, {"text": "We also show the effect of adding additional features aimed at capturing technical terminology.", "labels": [], "entities": []}, {"text": "In the experiments on the resolution of associative anaphora (Section 6.3), we test the hypothesis that the coreference resolver is able to adjust to the new notion of reference and show the effect of semantic features.", "labels": [], "entities": [{"text": "resolution of associative anaphora", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.8255455940961838}, {"text": "coreference resolver", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.8609042763710022}]}, {"text": "We perform our experiments using the IMS coreference resolver as a state-of-the-art coreference resolution system . The algorithm and the features included have not been changed except where otherwise stated.", "labels": [], "entities": [{"text": "IMS coreference resolver", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6693382064501444}, {"text": "coreference resolution", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7551661729812622}]}, {"text": "We use the OntoNotes datasets from the CoNLL 2011 shared task), only for training the out-of-the-box system.", "labels": [], "entities": [{"text": "OntoNotes datasets from the CoNLL 2011 shared task", "start_pos": 11, "end_pos": 61, "type": "DATASET", "confidence": 0.9111227318644524}]}, {"text": "We also use WordNet version 3.0 as provided in the 2012 shared task 7 as well as JAWS, the Java API for WordNet searching 8 . Performance is reported on our annotated corpus, using 8-fold cross-validation and the official CoNLL scorer (version 5).", "labels": [], "entities": [{"text": "JAWS", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.7738044857978821}, {"text": "CoNLL scorer", "start_pos": 222, "end_pos": 234, "type": "DATASET", "confidence": 0.5690067112445831}]}, {"text": "See: www.ims.uni-stuttgart.de/forschung/ ressourcen/werkzeuge/IMSCoref.html We follow their strategy to use the AMP decoder as the first decoder and the PCF decoder, a pairwise decoder, as a second.", "labels": [], "entities": [{"text": "PCF decoder", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.9487468898296356}]}, {"text": "The probability threshold is set to 0.5 for the first and 0.65 for the second decoder.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Properties of the annotated two subcor- pora, genetics (GEN) and computational linguis- tics (CL)", "labels": [], "entities": []}, {"text": " Table 3: Distribution of information status cate- gories and links in the two disciplines, in absolute  numbers", "labels": [], "entities": []}, {"text": " Table 4: Distribution of information status cate- gories in different domains, in percent", "labels": [], "entities": []}, {"text": " Table 7: Resolving coreferent references:  CoNLL metric scores for different training sets", "labels": [], "entities": [{"text": "Resolving coreferent", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9360390305519104}, {"text": "CoNLL metric scores", "start_pos": 44, "end_pos": 63, "type": "METRIC", "confidence": 0.7769012252489725}]}, {"text": " Table 8: Resolving coreferent references:  CoNLL scores for the extended feature sets", "labels": [], "entities": [{"text": "Resolving coreferent", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9257853925228119}, {"text": "CoNLL", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.6734315752983093}]}, {"text": " Table 9: Resolving associative references:  CoNLL metric scores for the extended feature sets", "labels": [], "entities": [{"text": "Resolving associative", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9077682495117188}, {"text": "CoNLL metric scores", "start_pos": 45, "end_pos": 64, "type": "METRIC", "confidence": 0.667571485042572}]}]}