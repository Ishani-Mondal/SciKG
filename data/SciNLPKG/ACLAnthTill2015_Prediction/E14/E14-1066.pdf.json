{"title": [{"text": "A Generative Model for User Simulation in a Spatial Navigation Domain", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.760353684425354}, {"text": "Spatial Navigation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7385115623474121}]}], "abstractContent": [{"text": "We propose the use of a generative model to simulate user behaviour in a novel task-oriented dialog domain, where user goals are spatial routes across artificial landscapes.", "labels": [], "entities": []}, {"text": "We show how to derive an efficient feature-based representation of spatial goals, admitting exact inference and generalising to new routes.", "labels": [], "entities": []}, {"text": "The use of a generative model allows us to capture a range of plausible behaviour given the same underlying goal.", "labels": [], "entities": []}, {"text": "We evaluate intrin-sically using held-out probability and per-plexity, and find a substantial reduction in uncertainty brought by our spatial representation.", "labels": [], "entities": []}, {"text": "We evaluate extrinsically in a human judgement task and find that our model's behaviour does not differ significantly from the behaviour of real users.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated dialog management is an area of research that has undergone rapid advancement in the last decade.", "labels": [], "entities": [{"text": "Automated dialog management", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8482436935106913}]}, {"text": "The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (.", "labels": [], "entities": []}, {"text": "Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (.", "labels": [], "entities": [{"text": "Statistical dialog management", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6684239606062571}]}, {"text": "Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains  and relational domains.", "labels": [], "entities": []}, {"text": "Although navigational dialogs have received much attention in studies of human conversational behaviour (, they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted.", "labels": [], "entities": [{"text": "navigational dialogs", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.9517936408519745}, {"text": "statistical dialog management", "start_pos": 141, "end_pos": 170, "type": "TASK", "confidence": 0.6298394203186035}]}, {"text": "Navigational domains present an interesting challenge, due to the disparity between the spatial goals and their grounding as utterances.", "labels": [], "entities": [{"text": "Navigational domains", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.884261429309845}]}, {"text": "This disparity renders much of the statistical management literature inapplicable.", "labels": [], "entities": [{"text": "statistical management", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7810199856758118}]}, {"text": "In this paper, we address this deficiency.", "labels": [], "entities": []}, {"text": "We focus on the task of simulating user behaviour, both because of the important role simulators plays in the induction of dialog managers, and because it provides a self-contained means of developing the domain representations which facilitate dialog reasoning.", "labels": [], "entities": [{"text": "dialog reasoning", "start_pos": 245, "end_pos": 261, "type": "TASK", "confidence": 0.8276391923427582}]}, {"text": "We show how a generative model of user behaviour can be induced from data, alleviating the manual effort typically involved in the development of simulators, and providing an elegant mechanism for reproducing the natural variability observed inhuman behaviour.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first evaluation metric is an information theoretic one, based on notion that better models find new instances of data (not used to train them) to be more predictable.", "labels": [], "entities": []}, {"text": "One such metric is the probability a model assigns to the data, (higher is better).", "labels": [], "entities": []}, {"text": "A second metric is perplexity, which computes how surprising a model finds the data (lower is better).", "labels": [], "entities": []}, {"text": "Both metrics have been used to evaluate user simulators in the literature (.", "labels": [], "entities": []}, {"text": "We compute the per-utterance probability of held-out data, instead of the per-dialog probability, since the latter was deemed incompatible across dialogs of different lengths by.", "labels": [], "entities": []}, {"text": "Perplexity is 2 \u2212log 2 (d) where dis the probability of the instance in question.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9825761318206787}]}, {"text": "We evaluate using leave-oneout validation, which estimates the model from all but one dialog, then evaluates the probability of that dialog.", "labels": [], "entities": []}, {"text": "We repeat this process until all dialogs have been evaluated as the unseen dialog.", "labels": [], "entities": []}, {"text": "Because we evaluate on held-out dialogs, we need to be able to assign probabilities to previously unseen instances.", "labels": [], "entities": []}, {"text": "We therefore smooth our models (at training time) by learning a background model which we estimate from all the training data.", "labels": [], "entities": []}, {"text": "This results in high variance in the distribution over features and a flat overall distribution.", "labels": [], "entities": [{"text": "variance", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9832431674003601}]}, {"text": "Where no model can be estimated fora particular semantic unit, we use that semantic unit's smoothed prior probability combined with the background model for its likelihood.", "labels": [], "entities": []}, {"text": "We first consider the suitability of the different feature sets for predicting utterances.", "labels": [], "entities": [{"text": "predicting utterances", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8947862982749939}]}, {"text": "shows the mean per-utterance probability our model assigns to held-out data when using different sets.", "labels": [], "entities": []}, {"text": "The more predictable the model finds the data, the higher the probability.", "labels": [], "entities": []}, {"text": "Note that the target metric here is not 1, as there is no single correct answer.", "labels": [], "entities": []}, {"text": "It can be seen that the most successful features in order of predictiveness are: Absolute, then Polynomial, then Landmark, and finally Edge.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9387087225914001}]}, {"text": "The combination of all buys us further improvement.", "labels": [], "entities": []}, {"text": "Secondly, we consider two baselines inspired by similar approaches of comparison in the literature ().", "labels": [], "entities": []}, {"text": "Both are variants of our model that lack the spatial component, i.e. they are not goal-based.", "labels": [], "entities": []}, {"text": "Although the baselines are weak, they allow us to measure the reduction in uncertainty brought by the introduction of the spatial componenet to our model, which is the purpose of this comparison. while.", "labels": [], "entities": []}, {"text": "The first tells us how predictable giver utterances are (in the held-out data), based only on the normalised frequencies.", "labels": [], "entities": []}, {"text": "The   Edge (E) 17.92 \u00b1 8.47 All 4.66 \u00b1 2.22: Perplexity scores (and standard deviations) of our model, computed over the four feature sets and their combination, estimated through leave-one-out validation.", "labels": [], "entities": [{"text": "Edge (E) 17.92 \u00b1 8.47", "start_pos": 6, "end_pos": 27, "type": "METRIC", "confidence": 0.9015214102608817}]}, {"text": "(A) outperforms all individual sets, while the combination performs best.", "labels": [], "entities": []}, {"text": "second tells us how predictable they become when we condition on the previous follower act.", "labels": [], "entities": []}, {"text": "Details of the baselines are similar to Section 3.1.", "labels": [], "entities": []}, {"text": "shows the mean per-utterance probability our model assigns to held-out data when compared to the two baselines.", "labels": [], "entities": []}, {"text": "Baseline 2 slightly improves our predictions over Baseline 1, although not reliably so, when considering the small increase in perplexity in.", "labels": [], "entities": []}, {"text": "SSM demonstrates a much larger relative improvement across both metrics.", "labels": [], "entities": [{"text": "SSM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8802178502082825}]}, {"text": "The results demonstrate that our spatial component enables substantial reduction in uncertainty, brought by the transfer of information from the maps to the utterances.", "labels": [], "entities": []}, {"text": "Intrinsic metrics, such as the probability of held-out data and perplexity, provide us with an elegant way of evaluating probabilistic models in a setting where there is no single correct answer, but a range of plausible answers, because they exploit the model's inherit ability to assign probability to behaviour.", "labels": [], "entities": []}, {"text": "However, the metrics can be hard to interpret in an absolute sense, providing much better: Mean per-utterance probability, assigned to held-out data by our model (SSM), compared to two baselines which lack the spatial componenet, estimated through leave-one-out validation.", "labels": [], "entities": [{"text": "Mean per-utterance probability", "start_pos": 91, "end_pos": 121, "type": "METRIC", "confidence": 0.9466890295346578}]}, {"text": "Error bars are standard deviations.", "labels": [], "entities": [{"text": "Error bars", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.987596869468689}]}, {"text": "In this section, we undertake a task-based evaluation of model output.", "labels": [], "entities": []}, {"text": "We train on 22 of the dialogs, holding out 3 at random for testing.", "labels": [], "entities": []}, {"text": "The task is to then generate, for each sub-route in the test dialogs, the most probable unit to describe it 6 . shows some examples of sub-routes taken from the test dialogs, and shows the the most probable unit to describe each under our model, SSM.", "labels": [], "entities": []}, {"text": "We first explore a naive notion of accuracy: the percentage of model-generated units matching Real Giver units observed in the test dialogs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9989870190620422}]}, {"text": "We compute the same for Baseline 1 from Section 5 as a lower bound.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.746689647436142}]}, {"text": "A quick glance at the results in might suggest that both models have little utility: SSM is \"correct\" only 33% of the time.", "labels": [], "entities": [{"text": "SSM", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9518153071403503}]}, {"text": "However, the extent to which this conclusion follows depends on the suitability of accuracy as a The models can generate 1 of the 87 units observed in the training set, but are made to output the most probable in this experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9992523789405823}]}, {"text": "Baseline SSM Match to Real Giver 7.69% 33.08%: Percentage of model-generated units that match Real Giver units in the test set.", "labels": [], "entities": [{"text": "Match", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.7928788065910339}]}, {"text": "The models output the most probable unit to describe a given sub-route.", "labels": [], "entities": []}, {"text": "We argue that this metric is unsuitable as it assumes one correct answer.", "labels": [], "entities": []}, {"text": "Mismatch Baseline SSM Real Giver 1.45 3.04 5.27 5.11: Average scores assigned by human judges to model-generated units on a 7-level Likert scale.", "labels": [], "entities": [{"text": "Mismatch Baseline SSM Real Giver 1.45", "start_pos": 0, "end_pos": 37, "type": "DATASET", "confidence": 0.47706474363803864}]}, {"text": "Mismatch is judged to be the worst, followed by Baseline.", "labels": [], "entities": [{"text": "Mismatch", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9115620851516724}, {"text": "Baseline", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9011940360069275}]}, {"text": "SSM and Real Giver are scored well, and are judged to be of similar quality.", "labels": [], "entities": [{"text": "Real Giver", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8381677567958832}]}, {"text": "In most situations, there is not a single correct description and a host of incorrect ones, but rather a gradient of descriptions from the highly informative and appropriate to the nonsensical and confusing.", "labels": [], "entities": []}, {"text": "Such subtleties are not captured by an accuracy test (or the closely related recall and precision).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9991337656974792}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9987125396728516}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.8593150973320007}]}, {"text": "In demonstration of this point, we next conduct qualitative evaluation of model output.", "labels": [], "entities": []}, {"text": "We ask humans to rate, on a Likert scale of 7, the degree to which a given unit provides a suitable description of a given sub-route.", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.8674262464046478}]}, {"text": "Sub-routes are taken from the test dialogs, and are marked similarly to but on the complete map.", "labels": [], "entities": []}, {"text": "Units are generated from SSM, Baseline, Real Giver, and a control condition: a deliberate Mismatch to the sub-route.", "labels": [], "entities": [{"text": "Mismatch", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9459267258644104}]}, {"text": "The Mismatch is generated automatically by taking the least probable unit under SSM, of the form MOVE(TOWARD, x) where x is one of the four compass directions.", "labels": [], "entities": [{"text": "Mismatch", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.6591103076934814}, {"text": "MOVE", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9978976249694824}, {"text": "TOWARD", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.8587646484375}]}, {"text": "We collect 5 judgements for each sub-route-unit combinations on Mechanical Turk, and randomise so that no judge sees the same order of pairs.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9272866547107697}]}, {"text": "Test dialogs contained 94 distinct sub-routes.", "labels": [], "entities": []}, {"text": "We analyse the results with a two-way ANOVA, with the first factor being model, and the second being the sub-route, fora 4\u00d794 design.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9296115040779114}]}, {"text": "The means of the \"model\" factor are shown in.", "labels": [], "entities": []}, {"text": "It can be seen that Mismatch and Baseline are scored sensibly poorly, while SSM and Real Giver are scored reasonably well, and are judged to be of a similar quality.", "labels": [], "entities": []}, {"text": "We thus proceed with a more rigorous analysis.", "labels": [], "entities": []}, {"text": "The ANOVA summary is shown in.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8059480786323547}]}, {"text": "A significant effect of the model fac-), the quality of the units are not judged to be significantly different.", "labels": [], "entities": []}, {"text": "tor is present, meaning that the scores assigned by human judges to the units are significantly influenced by which model was used to generate the units.", "labels": [], "entities": [{"text": "tor", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9720620512962341}]}, {"text": "Additionally, a significant effect for the subroute factor can be seen, which is due to some subroutes being harder to describe than others.", "labels": [], "entities": []}, {"text": "An interaction effect is also present, which is expected given such a large number of examples.", "labels": [], "entities": []}, {"text": "Note how the model factor accounts for the largest amount of variance of all the factors.", "labels": [], "entities": []}, {"text": "Having confirmed the presence of a model effect, we conduct a post-hoc analysis of the model factors.", "labels": [], "entities": []}, {"text": "shows a Tukey HSD test, demonstrating that all models are significantly different from one another, except Real Giver and SSM.", "labels": [], "entities": [{"text": "Tukey HSD test", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.7695712844530741}]}, {"text": "Results show that, despite the large number of judgements collected, we are unable to separate the quality of our model's unit from that in the original data, against which accuracy was being judged in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9976978898048401}]}, {"text": "This demonstrates that when many answers are feasible, scoring correctness against the original human units is unsuitable.", "labels": [], "entities": []}, {"text": "It also firmly demonstrates the suitability of our spatial representation, and the strength of the generative model we have induced for the task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Two way ANOVA with factors model (4  possibilities), and sub-route (94 possibilities). Re- sults show a model effect accounting for most of  the variance. Meaning that the scores assigned to  the units by human judges are significantly influ- enced by the model used to generate the units.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 18, "end_pos": 23, "type": "TASK", "confidence": 0.8405097126960754}, {"text": "Re- sults", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9551142255465189}]}, {"text": " Table 7: Tukey HSD shows that all models are  assigned significantly different scores by judges,  apart from SSM and Real Giver. This asserts that,  although only 33% of SSM units match Real Giver  units (as shown in", "labels": [], "entities": [{"text": "Tukey HSD", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.8827578723430634}]}]}