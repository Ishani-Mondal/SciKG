{"title": [{"text": "Accelerated Estimation of Conditional Random Fields using a Pseudo-Likelihood-inspired Perceptron Variant", "labels": [], "entities": []}], "abstractContent": [{"text": "We discuss a simple estimation approach for conditional random fields (CRFs).", "labels": [], "entities": []}, {"text": "The approach is derived heuristically by defining a variant of the classic perceptron algorithm in spirit of pseudo-likelihood for maximum likelihood estimation.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 131, "end_pos": 160, "type": "TASK", "confidence": 0.5517010390758514}]}, {"text": "The resulting approximative algorithm has a linear time complexity in the size of the label set and contains a minimal amount of tunable hyper-parameters.", "labels": [], "entities": []}, {"text": "Consequently, the algorithm is suitable for learning CRF-based part-of-speech (POS) taggers in presence of large POS label sets.", "labels": [], "entities": [{"text": "learning CRF-based part-of-speech (POS) taggers", "start_pos": 44, "end_pos": 91, "type": "TASK", "confidence": 0.5420230201312474}]}, {"text": "We present experiments on five languages.", "labels": [], "entities": []}, {"text": "Despite its heuristic nature, the algorithm provides surprisingly competetive accuracies and running times against reference methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The conditional random field (CRF) model) has been successfully applied to several sequence labeling tasks in natural language processing, including part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.6413760662078858}]}, {"text": "In this work, we discuss accelerating the CRF model estimation in presence of a large number of labels, say, hundreds or thousands.", "labels": [], "entities": [{"text": "CRF model estimation", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.6802950203418732}]}, {"text": "Large label sets occur in POS tagging of morphologically rich languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9264842867851257}]}, {"text": "CRF training is most commonly associated with the (conditional) maximum likelihood (ML) criterion employed in the original work of.", "labels": [], "entities": [{"text": "maximum likelihood (ML) criterion", "start_pos": 64, "end_pos": 97, "type": "METRIC", "confidence": 0.8349454353253046}]}, {"text": "In this work, we focus on an alternative training approach using the averaged perceptron algorithm of.", "labels": [], "entities": []}, {"text": "While yielding competitive accuracy, the perceptron algorithm avoids extensive tuning of hyper-parameters and regularization required by the stochastic gradient descent algorithm employed in ML estimation ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9983078241348267}, {"text": "ML estimation", "start_pos": 191, "end_pos": 204, "type": "TASK", "confidence": 0.9446816444396973}]}, {"text": "Additionally, while ML and perceptron training share an identical time complexity, the perceptron is in practice faster due to sparser parameter updates.", "labels": [], "entities": []}, {"text": "Despite its simplicity, running the perceptron algorithm can be tedious in case the data contains a large number of labels.", "labels": [], "entities": []}, {"text": "Previously, this problem has been addressed using, for example, k-best beam search () and parallelization (.", "labels": [], "entities": [{"text": "parallelization", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.9470125436782837}]}, {"text": "In this work, we explore an alternative strategy, in which we modify the perceptron algorithm in spirit of the classic pseudo-likelihood approximation for ML estimation.", "labels": [], "entities": [{"text": "ML estimation", "start_pos": 155, "end_pos": 168, "type": "TASK", "confidence": 0.9255238771438599}]}, {"text": "The resulting novel algorithm has linear complexity w.r.t. the label set size and contains only a single hyper-parameter, namely, the number of passes taken over the training data set.", "labels": [], "entities": []}, {"text": "We evaluate the algorithm, referred to as the pseudo-perceptron, empirically in POS tagging on five languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.7166162431240082}]}, {"text": "The results suggest that the approach can yield competitive accuracy compared to perceptron training accelerated using a violation-fixed 1-best beam search () which also provides a linear time complexity in label set size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9968922734260559}]}, {"text": "The rest of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the pseudo-perceptron algorithm and discuss related work.", "labels": [], "entities": []}, {"text": "In Sections 3 and 4, we describe our experiment setup and the results, respectively.", "labels": [], "entities": []}, {"text": "Conclusions on the work are presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Overview on data. The training (train.),  development (dev.) and test set sizes are given in  sentences. The columns titled tags and train. tags  correspond to total number of tags in the data set  and number of tags in the training set, respectively.", "labels": [], "entities": []}]}