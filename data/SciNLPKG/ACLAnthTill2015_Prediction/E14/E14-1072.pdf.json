{"title": [], "abstractContent": [{"text": "This paper investigates redundancy detection in ESL writings.", "labels": [], "entities": [{"text": "redundancy detection in ESL writings", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.6367483317852021}]}, {"text": "We propose a measure that assigns high scores to words and phrases that are likely to be redundant within a given sentence.", "labels": [], "entities": []}, {"text": "The measure is composed of two components: one captures fluency with a language model; the other captures meaning preservation based on analyzing alignments between words and their translations.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.7653625011444092}]}, {"text": "Experiments show that the proposed measure is five times more accurate than the random baseline.", "labels": [], "entities": [{"text": "accurate", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9895888566970825}]}], "introductionContent": [{"text": "It is especially the case when writing in a foreign language that one is still learning.", "labels": [], "entities": []}, {"text": "As a non-native speaker, it is more difficult to judge whether a word or a phrase is redundant.", "labels": [], "entities": []}, {"text": "This study focuses on automatically detecting redundancies in English as a Second Language learners' writings.", "labels": [], "entities": [{"text": "automatically detecting redundancies in English as a Second Language learners' writings", "start_pos": 22, "end_pos": 109, "type": "TASK", "confidence": 0.841543129899285}]}, {"text": "Redundancies occur when the writer includes some extraneous word or phrase that do not add to the meaning of the sentence but possibly make the sentence more awkward to read.", "labels": [], "entities": []}, {"text": "Upon removal of the unnecessary words or phrases, the sentence should improve in its fluency while maintaining the original meaning.", "labels": [], "entities": []}, {"text": "In the NUCLE corpus (, an annotated learner corpus comprised of essays written by primarily Singaporean students, 13.71% errors are tagged as \"local redundancy errors\", making redundancy error the second most frequent problem.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9755015969276428}]}, {"text": "Although redundancies occur frequently, it has not been studied as widely as other ESL errors.", "labels": [], "entities": []}, {"text": "A major challenge is that, unlike mistakes that violate the grammaticality of a sentence, redundancies do not necessarily \"break\" the sentence.", "labels": [], "entities": []}, {"text": "Determining which word or phrase is redundant is more of a stylistic question; it is more subjective, and sometimes difficult even fora native speaker.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this paper reports a first study on redundancy detection.", "labels": [], "entities": [{"text": "redundancy detection", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.9502258896827698}]}, {"text": "In particular, we focus on the task of defining a redundancy measure that estimates the likelihood that a given word or phrase within a sentence might be extraneous.", "labels": [], "entities": []}, {"text": "We propose a measure that takes into account each word's contribution to fluency and meaning.", "labels": [], "entities": []}, {"text": "The fluency component computes the language model score of the sentence after the deletion of a word or a phrase.", "labels": [], "entities": []}, {"text": "The meaning preservation component makes use of the sentence's translation into another language as pivot, then it applies a statistical machine translation (SMT) alignment model to infer the contribution of each word/phrase to the meaning of the sentence.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8565382063388824}, {"text": "statistical machine translation (SMT) alignment", "start_pos": 125, "end_pos": 172, "type": "TASK", "confidence": 0.8186899253300258}]}, {"text": "As a first experiment, we evaluate our measures on their abilities in picking the most redundant phrase of a given length.", "labels": [], "entities": []}, {"text": "We show that our measure is five times more accurate than a random baseline.", "labels": [], "entities": [{"text": "accurate", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9782860279083252}]}], "datasetContent": [{"text": "A fully automated redundancy detector has to decide (1) whether a given sentence contains any redundancy errors; (2) how many words constitute the redundant part; and (3) which exact words are redundant.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the third part while assuming the first two are given.", "labels": [], "entities": []}, {"text": "Thus, our experimental task is: given a sentence known to contain a redundant phrase of a particular length, can that redundant phrase be accurately identified?", "labels": [], "entities": []}, {"text": "For most sentences in our study, this results in choosing one from around 20 words/phrases.", "labels": [], "entities": []}, {"text": "While the task has a somewhat limited scope, it allows us to see how we could formally measure the difference between redundant words/phrases and non-redundant ones.", "labels": [], "entities": []}, {"text": "For each measure, we observe whether it has assigned the highest score to the redundant part of the sentence.", "labels": [], "entities": []}, {"text": "We compare the proposed redundancy model described in Section 3 against a set of baselines and other potential redundancy measures (to be described shortly).", "labels": [], "entities": []}, {"text": "To better understand different measures' performance on function words vs. content words, we also calculate the percentage of redundant function/content words that are detected successfullyaccuracy in both categories.", "labels": [], "entities": []}, {"text": "In our experiments, we consider prepositions and determiners as function words; and we consider other words/phrases as content words/phrases.", "labels": [], "entities": []}, {"text": "The experiment aims to address the following questions: (1) Does a sentence's translation serve as a reasonable approximation for its meaning?", "labels": [], "entities": []}, {"text": "Byelorussian (be), Bulgarian (bg), Icelandic (is), Polish (pl), Persian (fa), Boolean (language ((Afrikaans) (af), Danish (da), German (de), Russian (ru), French (fr), Tagalog (tl), Finnish (fi), Khmer (km), Georgian (ka), Gujarati (gu), Haitian (Creole (ht), Korean (ko), Dutch (nl), Galician (gl), Catalan (ca), Czech (cs), Kannada (kn), Croatian (hr), Latin (la), Latvian (lv), Lao (lo), Lithuanian (lt), Romanian (ro), Maltese (mt), Malay (ms), Macedonian (mk), Bengali (bn), Norwegian (no), Portuguese (pt), Japanese (ja), Swedish (sv), Serbian (sr), Esperanto (eo), Slovak (sk), Slovenian (sl), Swahili (sw), Telugu (te), Tamil (ta), Thai (th), Turkish (tr), Welsh (cy), Urdu (ur), Ukrainian (uk), Hebrew (iw), Greek (el), Spanish (es), Hungarian (hu), Armenian (hy), Italian (it), Yiddish (yi), Hindi (hi), Indonesian (id), English (en), Vietnamese (vi), Simplified Chinese (zh-CN), Traditional Chinese (zh-TW).", "labels": [], "entities": []}, {"text": "we compare using different pivot languages in our proposed model; in we compare using different redundancy metrics for the same pivot language -French.", "labels": [], "entities": []}, {"text": "First, compared to other measures, our proposed model best captures redundancy.", "labels": [], "entities": []}, {"text": "In particular, our model picks the correct redundant chunk 21.63% of the time, which is five times higher than the random baseline.", "labels": [], "entities": []}, {"text": "This suggests that using translation to approximate sentence meanings is a plausible option.", "labels": [], "entities": []}, {"text": "Note that one partial reason for the low figures is the limitation of data resources.", "labels": [], "entities": []}, {"text": "During error analysis, we found linkers/connectors (e.g. moreover, however) and modal auxiliaries (e.g. can, had) are often marked redundant when they actually carry undesirable meanings ).", "labels": [], "entities": []}, {"text": "These cases comprise a 16% portion among our model's failures.", "labels": [], "entities": []}, {"text": "Despite this limitation, the evaluation still suggests that current approaches are not ready fora full redundancy detection pipeline.", "labels": [], "entities": [{"text": "redundancy detection", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.6927467584609985}]}, {"text": "Second, we find that the choice of pivot language does make a difference.", "labels": [], "entities": []}, {"text": "Experimental result suggests that the system tends to achieve higher redundancy detection accuracy when using translations of a language more similar to English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9558267593383789}]}, {"text": "In particular, when using European languages (e.g. German (de), French (fr), Hungarian (hu) etc.) as pivot, the system performs much better than using Asian languages (e.g. Chinese (zh-CN), Japanese (ja), Thai (th) etc.).", "labels": [], "entities": []}, {"text": "One reason for this phenomenon is that the default Google translation output in Asian languages (as well as the alignment between English and these languages) are organized into characters, while characters are not the minimum meaning component.", "labels": [], "entities": []}, {"text": "For example, in Chinese, \" \" is the translation of \"explanation\", but the two characters \"\" and \"\" mean \"to solve\" and \"to release\" respectively.", "labels": [], "entities": []}, {"text": "In the alignment output, this will cause certain words being associated with more or less alignments than others.", "labels": [], "entities": []}, {"text": "In this case, the number of alignments no longer directly reflect how many meaning units a certain word helps to express.", "labels": [], "entities": []}, {"text": "To confirm this phenomenon, we tried improving the system using Simplified Chinese as the pivot language by merging characters together.", "labels": [], "entities": []}, {"text": "In particular, we applied Chinese tokenization (, and then merged alignments accordingly.", "labels": [], "entities": []}, {"text": "This raised the system's accuracy from 17.74% to 20.11%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.999794065952301}]}, {"text": "Third, to better understand the salient features of a successful redundancy measure, we experimented with using different components in isolation.", "labels": [], "entities": []}, {"text": "We find that the language model component is better at detecting redundant content words, while the alignment analysis component is better at detecting redundant function words.", "labels": [], "entities": []}, {"text": "The language model detects the function word redundancies with a worse accuracy than the random baseline; the alignment analysis component also has a worse accuracy than the random baseline on content words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9846801161766052}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9758308529853821}]}, {"text": "However, the English language model and the alignment analysis result can build on top of each other when we analyze the redundancies.", "labels": [], "entities": [{"text": "alignment analysis", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9477055966854095}]}, {"text": "We also found that alignments help us to better account for each word's contribution to the \"meaning\" of the sentence.", "labels": [], "entities": []}, {"text": "A linear combination of a language model score and our proposed measure based on analysis of alignments best captures redundancy.", "labels": [], "entities": []}, {"text": "However, as our experimental results suggest, it is necessary both to use alignments in translation outputs, and to use them in a good way.", "labels": [], "entities": []}, {"text": "Alignments help isolating fluency from the meaning component -making them easy to integrate.", "labels": [], "entities": []}, {"text": "As our experiments demonstrated, although methods comparing Google round-trip translation's output with the original sentence could lead to a 10.69% prediction accuracy, it is harder to combine it with the English language model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.7120651006698608}]}, {"text": "This is partly because of the non-orthogonality of these two measures -the English language model has already been used in the round-trip translation result.", "labels": [], "entities": []}, {"text": "Also, an information theoretical interpretation of alignments is essential for the model's success.", "labels": [], "entities": []}, {"text": "For example, a more naive way of using alignment results, align #, which counts the number of alignments, leads to a much lower accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9977598190307617}]}], "tableCaptions": [{"text": " Table 1: Length distribution of redundant chunks'  lengths in the evaluation data.", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9653700590133667}]}, {"text": " Table 2: Redundancy part identification accuracies  for different redundancy metrics on NUCLE cor- pus, using French as the pivot language.", "labels": [], "entities": [{"text": "Redundancy part identification", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.6808366378148397}, {"text": "NUCLE cor- pus", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.8490126430988312}]}]}