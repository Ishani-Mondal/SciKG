{"title": [{"text": "Identifying fake Amazon reviews as learning from crowds", "labels": [], "entities": [{"text": "Identifying fake Amazon", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8794400691986084}]}], "abstractContent": [{"text": "Customers who buy products such as books online often rely on other customers reviews more than on reviews found on specialist magazines.", "labels": [], "entities": []}, {"text": "Unfortunately the confidence in such reviews is often misplaced due to the explosion of so-called sock puppetry-authors writing glowing reviews of their own books.", "labels": [], "entities": []}, {"text": "Identifying such deceptive reviews is not easy.", "labels": [], "entities": [{"text": "Identifying", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.96584552526474}]}, {"text": "The first contribution of our work is the creation of a collection including a number of genuinely deceptive Amazon book reviews in collaboration with crime writer Jeremy Duns, who has devoted a great deal of effort in unmasking sock puppet-ing among his colleagues.", "labels": [], "entities": []}, {"text": "But there can be no certainty concerning the other reviews in the collection: all we have is a number of cues, also developed in collaboration with Duns, suggesting that a review maybe genuine or deceptive.", "labels": [], "entities": [{"text": "Duns", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.927145779132843}]}, {"text": "Thus this corpus is an example of a collection where it is not possible to acquire the actual label for all instances, and where clues of deception were treated as anno-tators who assign them heuristic labels.", "labels": [], "entities": []}, {"text": "A number of approaches have been proposed for such cases; we adopt here the 'learn-ing from crowds' approach proposed by Raykar et al.", "labels": [], "entities": []}, {"text": "Thanks to Duns' certainly fake reviews, the second contribution of this work consists in the evaluation of the effectiveness of different methods of annotation, according to the performance of models trained to detect deceptive reviews .", "labels": [], "entities": []}], "introductionContent": [{"text": "Customer reviews of books, hotels and other products are widely perceived as an important reason for the success of e-commerce sites such as amazon.com or tripadvisor.com.", "labels": [], "entities": []}, {"text": "However, customer confidence in such reviews is often misplaced, due to the growth of the so-called sock puppetry phenomenon: authors / hoteliers writing glowing reviews of their own works / hotels (and occasionally also negative reviews of the competitors).", "labels": [], "entities": []}, {"text": "The prevalence of this phenomenon has been revealed by campaigners such as crime writer Jeremy Duns, who exposed a number of fellow authors involved in such practices.", "labels": [], "entities": [{"text": "crime writer Jeremy Duns", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.56136704236269}]}, {"text": "A number of sites have also emerged offering Amazon reviews to authors fora fee.", "labels": [], "entities": []}, {"text": "Several automatic techniques for exposing such deceptive reviews have been proposed in recent years ().", "labels": [], "entities": []}, {"text": "But like all work on deceptive language (computational or otherwise) (, such works suffer from a serious problem: the lack of a gold standard containing 'real life' examples of deceptive uses of language.", "labels": [], "entities": []}, {"text": "This is because it is very difficult to find definite proof that an Amazon review is either deceptive or genuine.", "labels": [], "entities": [{"text": "Amazon review", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9191598892211914}]}, {"text": "Thus most researchers recreate deceptive behavior in the lab, as done by.", "labels": [], "entities": []}, {"text": "For instance,, and used crowdsourcing, asking turkers to produce instances of deceptive behavior.", "labels": [], "entities": []}, {"text": "Finally, Li et al.", "labels": [], "entities": []}, {"text": "(2011) classify reviews as deceptive or truthful by hand on the basis of a series of heuristics: they start by excluding anonymous reviews, then use their helpfulness and other criteria to decide whether they are deceptive or not.", "labels": [], "entities": []}, {"text": "Clearly a more rigorous approach to establishing the truth or otherwise of reviews on the basis of such heuristic criteria would be useful.", "labels": [], "entities": []}, {"text": "In this work we develop a system for identifying deceptive reviews in Amazon.", "labels": [], "entities": []}, {"text": "Our proposal makes two main contributions: 1.", "labels": [], "entities": []}, {"text": "we identified in collaboration with Jeremy Duns a series of criteria used by Duns and other 'sock puppet hunters' to find suspicious reviews / reviewers, and collected a dataset of reviews some of which are certainly false as the authors admitted so, whereas others maybe genuine or deceptive.", "labels": [], "entities": []}, {"text": "2. we developed an approach to the truthfulness of reviews based on the notion that the truthfulness of a review is a latent variable whose value cannot be known, but can be estimated using some criteria as potential indicators of such value-as annotators-and then we used the learning from crowds algorithm proposed by to assign a class to each review in the dataset.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe how we collected our dataset; in Section 3 we show the experiments we carried out and in Section 4 we discuss the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out two experiments, in which the classes assigned to the reviews of DEREV were found adopting two different strategies.", "labels": [], "entities": [{"text": "DEREV", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.612259030342102}]}, {"text": "In the first experiment the classes of the reviews were determined using majority voting of our deception clues.", "labels": [], "entities": []}, {"text": "This experiment is thus conceptually similar to those of, who trained models using supervised methods with the aim of identifying fake reviews.", "labels": [], "entities": []}, {"text": "We discuss this experiment in the next Section.", "labels": [], "entities": []}, {"text": "In the second experiment, learning from crowds was used ().", "labels": [], "entities": []}, {"text": "This approach is discussed in Section 3.2.1.", "labels": [], "entities": []}, {"text": "In both experiments we carried out a 10-fold cross-validation wherein each iteration feature selection and training were carried out using 90% of the part of the corpus with only silver standard annotation and 90% of the subset with gold.", "labels": [], "entities": []}, {"text": "The test set used in each iteration consisted of the remaining tenth of reviews with gold standard classes, which were employed in order to evaluate the predictions of the models.", "labels": [], "entities": []}, {"text": "This allowed to estimate the efficiency of the strategies we used to determine our silver standard classes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The distribution of deception clues in the  reviews", "labels": [], "entities": []}, {"text": " Table 3: The experiment with the majority voting classes", "labels": [], "entities": []}, {"text": " Table 4: The experiment with Raykar et al.'s algorithm classes", "labels": [], "entities": []}]}