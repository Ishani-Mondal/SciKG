{"title": [], "abstractContent": [{"text": "Computational creativity is one of the central research topics of Artificial Intelligence and Natural Language Processing today.", "labels": [], "entities": [{"text": "Computational creativity", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8122154176235199}]}, {"text": "Irony, a creative use of language, has received very little attention from the computational linguistics research point of view.", "labels": [], "entities": []}, {"text": "In this study we investigate the automatic detection of irony casting it as a classification problem.", "labels": [], "entities": [{"text": "automatic detection of irony casting it as a classification problem", "start_pos": 33, "end_pos": 100, "type": "TASK", "confidence": 0.76571706533432}]}, {"text": "We propose a model capable of detecting irony in the social network Twit-ter.", "labels": [], "entities": [{"text": "Twit-ter", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.8308656215667725}]}, {"text": "In cross-domain classification experiments our model based on lexical features outperforms a word-based baseline previously used in opinion mining and achieves state-of-the-art performance.", "labels": [], "entities": [{"text": "cross-domain classification", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7583944201469421}, {"text": "opinion mining", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.7438355982303619}]}, {"text": "Our features are simple to implement making the approach easily replicable.", "labels": [], "entities": []}], "introductionContent": [{"text": "Irony, a creative use of language, has received very little attention from the computational linguistics research point of view.", "labels": [], "entities": []}, {"text": "It is however considered an important aspect of language which deserves special attention given its relevance in fields such as sentiment analysis and opinion mining.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.9735714197158813}, {"text": "opinion mining", "start_pos": 151, "end_pos": 165, "type": "TASK", "confidence": 0.8595007956027985}]}, {"text": "Irony detection appears as a difficult problem since ironic statements are used to express the contrary of what is being said, therefore being a tough nut to crack by current systems.", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.834772378206253}]}, {"text": "Being a creative form of language, there is no consensual agreement in the literature on how verbal irony should be defined.", "labels": [], "entities": []}, {"text": "Only recently irony detection has been approached from a computational perspective.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.9873766899108887}]}, {"text": "cast the problem as one of classification training machine learning algorithms to sepatare ironic from non-ironic statements.", "labels": [], "entities": []}, {"text": "Ina similar vein, we propose and evaluate anew model to detect irony, using seven sets of lexical features, most of them based on our intuitions about \"unexpectedness\", a key component of ironic statements.", "labels": [], "entities": []}, {"text": "Indeed, claims that irony is strictly connected to surprise, showing that unexpectedness is the feature most related to situational ironies.", "labels": [], "entities": []}, {"text": "In this paper we reduce the complexity of the problem by studying irony detection in the microblogging service Twitter 1 that allows users to send and read text messages (shorter than 140 characters) called tweets.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8740804195404053}]}, {"text": "We do not adopt any formal definition of irony, instead we rely on a dataset created for the study of irony detection which allows us to compare our findings with recent state-of-the-art approaches ().", "labels": [], "entities": [{"text": "irony detection", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.8457348644733429}]}, {"text": "The contributions of this paper are as follows: \u2022 a novel set of linguistically motivated, easyto-compute features \u2022 a comparison of our model with the state-ofthe-art; and \u2022 a novel set of experiments to demonstrate cross-domain adaptation.", "labels": [], "entities": [{"text": "cross-domain adaptation", "start_pos": 217, "end_pos": 240, "type": "TASK", "confidence": 0.7306515574455261}]}, {"text": "The paper will show that our model outperforms a baseline, achieves state-of-the-art performance, and can be applied to different domains.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows: in the next Section we describe related work.", "labels": [], "entities": []}, {"text": "In Section 3 we described the corpus and text processing tools used and in Section 4 we present our approach to tackle the irony detection problem.", "labels": [], "entities": [{"text": "irony detection problem", "start_pos": 123, "end_pos": 146, "type": "TASK", "confidence": 0.9213066498438517}]}, {"text": "Section 5 describes the experiments while Section 6 interprets the results.", "labels": [], "entities": []}, {"text": "Finally we close the paper in Section 7 with conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to carryout experimentation and to be able to compare our approach to that of () we use three datasets derived from the corpus in Section 3.", "labels": [], "entities": []}, {"text": "Irony vs Education, Irony vs Humour and Irony vs Politics.", "labels": [], "entities": []}, {"text": "Each topic combination was balanced with 10.000 ironic and 10.000 of non-ironic examples.", "labels": [], "entities": []}, {"text": "The task at hand it to train a classifier to identify ironic and non-ironic tweets.", "labels": [], "entities": []}, {"text": "We perform two types of experiments: \u2022 we run in each of the datasets a 10-fold crossvalidation classification; \u2022 across datasets, we train the classifier in one dataset and apply it to the other two datasets.", "labels": [], "entities": []}, {"text": "To perform these experiments, we create three balanced datasets containing each one third of the original 10.000 ironic tweets (so that the datasets are disjoint) and one third of the original domain tweets.", "labels": [], "entities": []}, {"text": "The experimental framework is executed for the word-based baseline model and our model.", "labels": [], "entities": []}, {"text": "In we present precision, recall, and F-measure figures for the different runs of the experiments.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9996063113212585}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9992652535438538}, {"text": "F-measure", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9979739785194397}]}, {"text": "shows precision, recall, and F-measure figures for our approach compared to).", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.999596893787384}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9993570446968079}, {"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9988805651664734}]}, {"text": "compares two different algorithms: Decision Tree and Random Forest using our model.", "labels": [], "entities": []}, {"text": "In order to have a clear understanding about the contribution of each set of features in our model, we also studied the behaviour of information gain in each dataset.", "labels": [], "entities": []}, {"text": "We compute information gain experiments over the three balanced corpora and present the results in.", "labels": [], "entities": []}, {"text": "The graphic shows the mean information gain for each group of features.", "labels": [], "entities": []}, {"text": "We also report in the information gain of each single feature, where one can understand if a feature will be important to distinguish ironic from non-ironic tweets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision, Recall and F-Measure of each topic combination for word based algorithm and our  algorithm in the form \"Word Based / Ours\". Decision Tree has been used as classifier for both algorithms.  We marked in bold the results that, according to the t-test, are significantly better.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9986945986747742}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9978609681129456}, {"text": "F-Measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9986391663551331}]}, {"text": " Table 2: Precision, Recall and F-Measure for each topic combination of our model when Decision Tree  and Random Forest are used. Data are in the format \"Random Forest / Decision Tree\". We marked in  bold the F-Measures that are better.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990478157997131}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9962030053138733}, {"text": "F-Measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9986121654510498}, {"text": "F-Measures", "start_pos": 209, "end_pos": 219, "type": "METRIC", "confidence": 0.9374136328697205}]}, {"text": " Table 3: Precision, Recall, and F-Measure over the three corpora Education, Humour, and Politics. Both  our and Reyes et al. results are shown; the classifier used is Decision Tree for both models. We marked  in bold the F-Measures that are better compared to the other model.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9493061304092407}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9187295436859131}, {"text": "F-Measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9964414238929749}, {"text": "F-Measures", "start_pos": 222, "end_pos": 232, "type": "METRIC", "confidence": 0.933070719242096}]}]}