{"title": [{"text": "Multi-Granular Aspect Aggregation in Aspect-Based Sentiment Analysis", "labels": [], "entities": [{"text": "Multi-Granular Aspect Aggregation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5807184974352518}, {"text": "Aspect-Based Sentiment Analysis", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6677426099777222}]}], "abstractContent": [{"text": "Aspect-based sentiment analysis estimates the sentiment expressed for each particular aspect (e.g., battery, screen) of an entity (e.g., smartphone).", "labels": [], "entities": [{"text": "Aspect-based sentiment analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8109446962674459}]}, {"text": "Different words or phrases, however, maybe used to refer to the same aspect, and similar aspects may need to be aggregated at coarser or finer granularities to fit the available space or satisfy user preferences.", "labels": [], "entities": []}, {"text": "We introduce the problem of aspect aggrega-tion at multiple granularities.", "labels": [], "entities": []}, {"text": "We decompose it in two processing phases, to allow previous work on term similarity and hierarchical clustering to be reused.", "labels": [], "entities": []}, {"text": "We show that the second phase, where aspects are clustered, is almost a solved problem , whereas further research is needed in the first phase, where semantic similarity measures are employed.", "labels": [], "entities": []}, {"text": "We also introduce a novel sense pruning mechanism for WordNet-based similarity measures , which improves their performance in the first phase.", "labels": [], "entities": []}, {"text": "Finally, we provide publicly available benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a set of texts discussing a particular entity (e.g., reviews of a laptop), aspect-based sentiment analysis (ABSA) attempts to identify the most prominent (e.g., frequently discussed) aspects of the entity (e.g., battery, screen) and the average sentiment (e.g., 1 to 5 stars) for each aspect or group of aspects, as in.", "labels": [], "entities": [{"text": "aspect-based sentiment analysis (ABSA)", "start_pos": 81, "end_pos": 119, "type": "TASK", "confidence": 0.7571398317813873}]}, {"text": "Most ABSA systems perform all or some of the following: subjectivity detection to retain only sentences (or other spans) expressing subjective opinions; aspect extraction to extract (and possibly rank) terms corresponding to aspects (e.g., 'battery'); aspect aggregation to group aspect terms that are nearsynonyms (e.g., 'price', 'cost') or to obtain aspects at a coarser granularity (e.g., 'chicken','steak', and 'fish' maybe replaced by 'food' in restaurant reviews); and aspect sentiment score estimation to estimate the average sentiment for each aspect or group of aspects.", "labels": [], "entities": [{"text": "subjectivity detection", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7338264286518097}, {"text": "aspect extraction", "start_pos": 153, "end_pos": 170, "type": "TASK", "confidence": 0.7031156271696091}, {"text": "aspect sentiment score estimation", "start_pos": 475, "end_pos": 508, "type": "METRIC", "confidence": 0.6378621459007263}]}, {"text": "In this paper, we focus on aspect aggregation, the least studied stage of the four.", "labels": [], "entities": [{"text": "aspect aggregation", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8021951913833618}]}, {"text": "Aspect aggregation is needed to avoid reporting separate sentiment scores for aspect terms that are very similar.", "labels": [], "entities": []}, {"text": "In, for example, showing separate lines for 'money', 'price', and 'cost' would be confusing.", "labels": [], "entities": []}, {"text": "The extent to which aspect terms should be aggregated, however, also depends on the available space and user preferences.", "labels": [], "entities": []}, {"text": "On devices with smaller screens, it maybe desirable to aggregate aspect terms that are similar, though not necessarily near-synonyms (e.g., 'design', 'color', 'feeling') to show fewer lines (), but finer aspects maybe preferable on larger screens.", "labels": [], "entities": []}, {"text": "Users may also wish to adjust the granularity of aspects, e.g., by stretching or narrowing the height of on a smartphone to view more or fewer lines.", "labels": [], "entities": []}, {"text": "Hence, aspect aggregation should be able to produce groups of aspect terms for multiple granularities.", "labels": [], "entities": []}, {"text": "We assume that the aggregated aspects are displayed as lists of terms, as in.", "labels": [], "entities": []}, {"text": "We make no effort to order (e.g., by frequency) the terms in each list, nor do we attempt to produce a single (more general) term to describe each aggregated aspect, leaving such tasks for future work.", "labels": [], "entities": []}, {"text": "gregating only synonyms (or near-synonyms), however, does not allow users to select the desirable aspect granularity, and ignores the hierarchical relations between aspect terms.", "labels": [], "entities": []}, {"text": "For example, 'pizza' and 'steak' are kinds of 'food' and, hence, the three terms can be aggregated to show fewer, coarser aspects, even though they are not synonyms.", "labels": [], "entities": []}, {"text": "used a predefined domain-specific taxonomy to hierarchically aggregate aspect terms, but taxonomies of this kind are often not available.", "labels": [], "entities": []}, {"text": "By contrast, we use only general-purpose taxonomies (e.g., WordNet), term similarity measures based on general-purpose taxonomies or corpora, and hierarchical clustering.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9267722964286804}]}, {"text": "We define multi-granular aspect aggregation to be the task of partitioning a given set of aspect terms (generated by a previous aspect extraction stage) into k non-overlapping clusters, for multiple values of k.", "labels": [], "entities": [{"text": "multi-granular aspect aggregation", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6043717761834463}]}, {"text": "A further constraint is that the clusters have to be consistent for different k values, meaning that if two aspect terms t 1 , t 2 are placed in the same cluster fork = k 1 , then t 1 and t 2 must also be grouped together (in the same cluster) for every k = k 2 with k 2 < k 1 , i.e., for every coarser grouping.", "labels": [], "entities": []}, {"text": "For example, if 'waiter' and 'service' are grouped together fork = 5, they must also be grouped together fork = 4, 3, 2 and (trivially) k = 1, to allow the user to feel that selecting a smaller number of aspect groups (narrowing the height of) has the effect of zooming out (without aspect terms jumping unexpectedly to other aspect groups), and similarly for zooming in.", "labels": [], "entities": []}, {"text": "1 This requirement is satisfied by using agglomerative hierarchical clustering algorithms (), which in our case produce term hierarchies like the ones of.", "labels": [], "entities": []}, {"text": "By using slices (nodes at a particular depth) of the hierarchies that are closer to the root or the leaves, we obtain fewer or more clusters.", "labels": [], "entities": []}, {"text": "The vertical dotted lines of illustrate two slices fork = 4.", "labels": [], "entities": []}, {"text": "By contrast, flat clustering algorithms (e.g., k-means) do not satisfy the consistency constraint for different k values.", "labels": [], "entities": []}, {"text": "Agglomerative clustering algorithms require a measure of the distance between individuals, in our case a measure of how similar two aspect terms are, and a linkage criterion to specify which clusters should be merged to form larger (coarser) clusters.", "labels": [], "entities": []}, {"text": "To experiment with different term sim-, with scores showing the similarity of each pair of input aspect terms; the matrix in effect defines the distance measure to be used by agglomerative clustering.", "labels": [], "entities": []}, {"text": "In Phase B, the aspect terms are grouped into k non-overlapping clusters, for varying values of k, given the matrix of Phase A and a linkage criterion; a hierarchy like the ones of is first formed via agglomerative clustering, and fewer or more clusters (for different values of k) are then obtained by using different slices of the hierarchy, as already discussed.", "labels": [], "entities": []}, {"text": "Our two-phase decomposition can also accommodate non-hierarchical clustering algorithms, provided that the consistency constraint is satisfied, but we consider only agglomerative hierarchical clustering in this paper.", "labels": [], "entities": []}, {"text": "The decomposition in two phases has three main advantages.", "labels": [], "entities": []}, {"text": "Firstly, it allows reusing previous work on term similarity measures (, which can be used to fill in the matrix of Phase A. Secondly, the decomposition allows different linkage criteria to be experimentally compared (in Phase B) using the same similarity matrix (of Phase A), i.e., the same distance measure.", "labels": [], "entities": []}, {"text": "Thirdly, the decomposition leads to high inter-annotator agreement, as we show experimentally.", "labels": [], "entities": [{"text": "agreement", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.8762596249580383}]}, {"text": "By contrast, in preliminary experiments we found that asking humans to directly evaluate aspect hierarchies produced by hierarchical clustering, or to manually create gold aspect hierarchies led to poor inter-annotator agreement.", "labels": [], "entities": []}, {"text": "We show that existing term similarity measures perform reasonably well in Phase A, especially when combined, but there is a large scope for improvement.", "labels": [], "entities": []}, {"text": "We also propose a novel sense pruning method for WordNet-based similarity measures, which leads to significant improvements in Phase A. In Phase B, we experiment with agglomerative clustering using four different linkage criteria, concluding that they all perform equally well and that Phase B is almost a solved problem when the gold similarity matrix of Phase A is used; however, further improvements are needed in the similarity measures of Phase A to produce a sufficiently good similarity matrix.", "labels": [], "entities": [{"text": "WordNet-based similarity measures", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.6525300741195679}]}, {"text": "We also make publicly available the datasets of our experiments.", "labels": [], "entities": []}, {"text": "Our main contributions are: (i) to the best of our knowledge, we are the first to consider multi-granular aspect aggregation (not just merging near-synonyms) in ABSA without manually crafted domain-specific ontologies; (ii) we propose a two-phase decomposition that allows previous work on term similarity and hierarchical clustering to be reused and evaluated with high interannotator agreement; (iii) we introduce a novel sense pruning mechanism that improves WordNetbased similarity measures; (iv) we provide the first public datasets for multi-granular aspect aggregation; (v) we show that the second phase of our decomposition is almost a solved problem, and that research should focus on the first phase.", "labels": [], "entities": [{"text": "multi-granular aspect aggregation", "start_pos": 91, "end_pos": 124, "type": "TASK", "confidence": 0.6621291836102804}]}, {"text": "Although we experiment with customer reviews of products and services, ABSA and the work of this paper in particular are, at least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations).", "labels": [], "entities": [{"text": "ABSA", "start_pos": 71, "end_pos": 75, "type": "TASK", "confidence": 0.855333685874939}]}, {"text": "Section 2 below discusses related work.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 present our work for Phase A and B, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used two benchmark datasets that we had previously constructed to evaluate ABSA methods for subjectivity detection, aspect extraction, and aspect score estimation, but not aspect aggregation.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 78, "end_pos": 82, "type": "TASK", "confidence": 0.7760069370269775}, {"text": "subjectivity detection", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.7719561457633972}, {"text": "aspect extraction", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.8359063267707825}, {"text": "aspect score estimation", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.6181997060775757}]}, {"text": "We extended them to support aspect aggregation, and we make them publicly available.", "labels": [], "entities": [{"text": "aspect aggregation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7843071222305298}]}, {"text": "The two original datasets contain sentences from customer reviews of restaurants and laptops, respectively.", "labels": [], "entities": []}, {"text": "The reviews are manually split into sentences, and each sentence is manually annotated as 'subjective' (expressing opinion) or 'objective' (not expressing opinion).", "labels": [], "entities": []}, {"text": "The restaurants dataset contains 3,710 English sentences from the restaurant reviews of.", "labels": [], "entities": [{"text": "restaurants dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7045719772577286}]}, {"text": "The laptops dataset contains 3,085 English sentences from 394 customer reviews, collected from sites that host customer reviews.", "labels": [], "entities": [{"text": "laptops dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7126977741718292}]}, {"text": "In the experiments of this paper, we use only the 3,057 (out of 3,710) subjective restaurant sentences and the 2,631 (out of 3,085) subjective laptop sentences.", "labels": [], "entities": []}, {"text": "For each subjective sentence, our datasets show the words that human annotators marked as aspect terms.", "labels": [], "entities": []}, {"text": "For example, in \"The dessert was divine!\" the aspect term is 'dessert', and in \"Really bad waiter.\" it is 'waiter'.", "labels": [], "entities": []}, {"text": "Among the 3,057 subjective restaurant sentences, 1,129 contain exactly one aspect term, 829 more than one, and 1,099 no aspect term; a subjective sentence may express an opinion about the restaurant (or laptop) being reviewed without mentioning a specific aspect (e.g., \"Really nice restaurant!\"), which is why no aspect terms are present in some subjective sentences.", "labels": [], "entities": []}, {"text": "There are 558 distinct multi-word aspect terms and 431 distinct single-word aspect terms in the subjective restaurant sentences.", "labels": [], "entities": []}, {"text": "Among the 2,631 subjective sentences of the laptop reviews, 823 contain exactly one aspect term, 389 more than one, and 1,419 no aspect term.", "labels": [], "entities": []}, {"text": "There are 273 distinct multiword aspect terms and 330 distinct single-word aspect terms in the subjective laptop sentences.", "labels": [], "entities": []}, {"text": "From each dataset, we selected the 20 (distinct) aspect terms that the human annotators had annotated most frequently, taking annotation frequency to bean indicator of importance; there are only two multi-word aspect terms ('hard drive', 'bat-tery life') among the 20 most frequent ones in the laptops dataset, and none among the 20 most frequent aspect terms of the restaurants dataset.", "labels": [], "entities": [{"text": "restaurants dataset", "start_pos": 367, "end_pos": 386, "type": "DATASET", "confidence": 0.7214296609163284}]}, {"text": "We then formed all the 190 possible pairs of the 20 terms and constructed an empty similarity matrix, one for each dataset, which was given to three human judges to fill in (1: strong dissimilarity, 5: strong similarity).", "labels": [], "entities": []}, {"text": "For each aspect term, all the subjective sentences mentioning the term were also provided, to help the judges understand how the terms are used in the particular domains (e.g., 'window' and 'Windows' have domain-specific meanings in laptop reviews).", "labels": [], "entities": []}, {"text": "The Pearson correlation coefficient indicated high inter-annotator agreement (0.81 for restaurants, 0.74 for laptops).", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 4, "end_pos": 35, "type": "METRIC", "confidence": 0.9265926877657572}, {"text": "agreement", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.7233085632324219}]}, {"text": "We also measured the absolute inter-annotator agreement a(l 1 , l 2 ), defined below, where l 1 , l 2 are lists containing the scores (similarity matrix values) of two judges, N is the length of each list, and v max , v min are the largest and smallest possible scores (5 and 1).", "labels": [], "entities": [{"text": "absolute inter-annotator agreement a", "start_pos": 21, "end_pos": 57, "type": "METRIC", "confidence": 0.668653666973114}]}, {"text": "The absolute interannotator agreement was also high (0.90 for restaurants, 0.91 for laptops).", "labels": [], "entities": [{"text": "absolute interannotator agreement", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.812505046526591}]}, {"text": "With both measures, we compute the agreement of each judge with the averaged (for each matrix cell) scores of the other two judges, and we report the mean of the three agreement estimates.", "labels": [], "entities": [{"text": "agreement", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9804477691650391}]}, {"text": "Finally, we created the gold similarity matrix of each dataset by placing in each cell the average scores that the three judges had provided for that cell.", "labels": [], "entities": [{"text": "gold similarity matrix", "start_pos": 24, "end_pos": 46, "type": "METRIC", "confidence": 0.7040135860443115}]}, {"text": "In preliminary experiments, we gave aspect terms to human judges, asking them to group any terms they considered near-synonyms.", "labels": [], "entities": []}, {"text": "We then asked the judges to group the aspect terms into fewer, coarser groups by grouping terms that could be viewed as direct hyponyms of the same broader term (e.g., 'pizza' and 'steak' are both kinds of 'food'), or that stood in a hyponym-hypernym relation (e.g., 'pizza' and 'food').", "labels": [], "entities": []}, {"text": "We used the Dice coefficient to measure inter-annotator agreement, and we obtained reasonably good agreement for near-synonyms (0.77 for restaurants, 0.81 for laptops), but poor agreement for the coarser as-pects (0.25 and 0.11).", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.6610387563705444}]}, {"text": "In other preliminary experiments, we asked human judges to rank alternative aspect hierarchies that had been produced by applying agglomerative clustering with different linkage criteria to 20 aspect terms, but we obtained very poor inter-annotator agreement (Pearson score \u22120.83 for restaurants and 0 for laptops).", "labels": [], "entities": [{"text": "Pearson score", "start_pos": 260, "end_pos": 273, "type": "METRIC", "confidence": 0.9778386354446411}]}, {"text": "Each similarity measure was evaluated by computing its Pearson correlation with the scores of the gold similarity matrix.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 55, "end_pos": 74, "type": "METRIC", "confidence": 0.9745438992977142}]}, {"text": "Our sense pruning consistently improves all four WordNet-based measures.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9225161075592041}]}, {"text": "It does not apply to DS , which is why the DS results are identical with and without pruning.", "labels": [], "entities": [{"text": "DS", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9633026719093323}]}, {"text": "A paired t test indicates that the other differences (with and without pruning) of are statistically significant (p < 0.05).", "labels": [], "entities": []}, {"text": "We used the senses with the top five rel (s ij ) scores for each aspect term ti during sense pruning.", "labels": [], "entities": []}, {"text": "We also experimented with keeping fewer senses, but the results were inferior or there was no improvement.", "labels": [], "entities": []}, {"text": "Lin's measure performed better when information content was estimated on the (much larger, but domain-independent) Brown corpus (LIN @Brown), as opposed to using the (domainspecific) reviews of our datasets (LIN @domain), but we observed no similar consistent pattern for JCN . Given its simplicity, PATH performed remarkably well in the restaurants dataset; it was the best measure (including combinations) without sense pruning, and the best uncombined measure with sense pruning.", "labels": [], "entities": [{"text": "JCN", "start_pos": 272, "end_pos": 275, "type": "DATASET", "confidence": 0.9092310070991516}, {"text": "PATH", "start_pos": 300, "end_pos": 304, "type": "METRIC", "confidence": 0.8874084949493408}, {"text": "restaurants dataset", "start_pos": 338, "end_pos": 357, "type": "DATASET", "confidence": 0.7971201539039612}]}, {"text": "It performed worse, however, compared to several other measures in the laptops dataset.", "labels": [], "entities": [{"text": "laptops dataset", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.7529764175415039}]}, {"text": "Similar comments apply to WP , which is among the top-performing uncombined measures in restaurants, both with and without sense pruning, but the worst overall measure in laptops.", "labels": [], "entities": []}, {"text": "DS is the best overall measure in laptops when compared to measures without sense pruning, and the third best overall when compared to measures that use sense pruning, but the worst overall in restaurants both with and without pruning.", "labels": [], "entities": [{"text": "DS", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9167771339416504}]}, {"text": "LIN and JCN , which use both WordNet and corpus statistics, have a more balanced performance across the two datasets, but they are not top-performers in any of the two.", "labels": [], "entities": [{"text": "LIN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9071610569953918}, {"text": "JCN", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.8904033303260803}, {"text": "WordNet", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9314718246459961}]}, {"text": "Combinations of similarity measures seem more stable across domains, as the results of AVG, WN , and WNDS indicate, though experiments with more domains are needed to investigate this issue.", "labels": [], "entities": [{"text": "AVG", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.7639096975326538}]}, {"text": "WNDS is the best overall method with sense pruning, and among the best three methods without pruning in both datasets.", "labels": [], "entities": [{"text": "WNDS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8537861704826355}]}, {"text": "To get a better view of the performance of WNDS with sense pruning, i.e., the best overall measure of, we compared it to two state of the art semantic similarity systems.", "labels": [], "entities": []}, {"text": "First, we applied the system of, one of the best systems of the recent *Sem 2013 semantic text similarity competition, to our Phase A data.", "labels": [], "entities": [{"text": "Sem 2013 semantic text similarity competition", "start_pos": 72, "end_pos": 117, "type": "TASK", "confidence": 0.6108502596616745}]}, {"text": "The performance (Pearson correlation with gold similarities) of the same system on the widely used WordSim353 word similarity dataset () is 0.73, much higher than the same system's performance on our Phase A data (see  which suggests that our data are more difficult.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 17, "end_pos": 36, "type": "METRIC", "confidence": 0.9349353313446045}, {"text": "WordSim353 word similarity dataset", "start_pos": 99, "end_pos": 133, "type": "DATASET", "confidence": 0.8674426227807999}]}, {"text": "We also employed the recent Word2Vec system, which computes continuous vector space representations of words from large corpora and has been reported to improve results in word similarity tasks.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 172, "end_pos": 193, "type": "TASK", "confidence": 0.8069153626759847}]}, {"text": "We used the English Wikipedia to compute word vectors with 200 features.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.8947025537490845}]}, {"text": "The similarity between two aspect terms was taken to be the cosine similarity of their vectors.", "labels": [], "entities": []}, {"text": "This system performed better than Han et al.'s with laptops, but not with restaurants.", "labels": [], "entities": []}, {"text": "shows that WNDS (with sense pruning) performed clearly better than the system of Han et al. and Word2Vec.", "labels": [], "entities": [{"text": "WNDS", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.43145260214805603}, {"text": "Word2Vec", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9551244974136353}]}, {"text": "also shows the Pearson correlation of each judge's scores to the gold similarity scores, as an indication of the best achievable results.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 15, "end_pos": 34, "type": "METRIC", "confidence": 0.9584003686904907}, {"text": "similarity", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.7241426706314087}]}, {"text": "Although WNDS (with sense pruning) performs reasonably well in both domains, there is large scope for improvement.", "labels": [], "entities": []}, {"text": "To evaluate the k clusters produced at each aspect granularity by the different linkage criteria, we used the Silhouette Index (SI ), a cluster evaluation measure that considers both inter-and intra-cluster coherence.", "labels": [], "entities": [{"text": "Silhouette Index (SI )", "start_pos": 110, "end_pos": 132, "type": "METRIC", "confidence": 0.8926024079322815}]}, {"text": "15 Given a set of clusters {C 1 , . .", "labels": [], "entities": []}, {"text": ", C k }, each SI (C i ) is defined as: where a j is the mean distance from the j-th instance of Ci to the other instances in Ci , and b j is the mean distance from the j-th instance of Ci to the instances in the cluster nearest to Ci . Then: We always use the correct (gold) distances of the instances (terms) when computing the SI scores.", "labels": [], "entities": [{"text": "SI", "start_pos": 329, "end_pos": 331, "type": "TASK", "confidence": 0.9592171311378479}]}, {"text": "As shown in, no linkage criterion clearly outperforms the others, when the gold matrix of Phase A is used; all four criteria perform reasonably well.", "labels": [], "entities": []}, {"text": "Note that the SI ranges from \u22121 to 1, with higher values indicating better clustering.", "labels": [], "entities": [{"text": "SI", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9196069836616516}]}, {"text": "shows that when the similarity matrix of WNDS (with SP) is used, the SI scores deteriorate significantly; again, there is no clear winner among the linkage criteria, but average and Ward seem to be overall better than the others.", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9565009474754333}]}, {"text": "Ina final experiment, we showed clusterings of varying granularities (k values) to four human judges (graduate CS students).", "labels": [], "entities": []}, {"text": "The clusterings were produced by two systems: one that used the gold similarity matrix of Phase A and agglomerative clustering with average linkage in Phase B, and one that used the similarity matrix of WNDS (with SP) and again agglomerative clustering with average linkage.", "labels": [], "entities": [{"text": "WNDS", "start_pos": 203, "end_pos": 207, "type": "DATASET", "confidence": 0.763590395450592}]}, {"text": "We showed all the clusterings to all the judges.", "labels": [], "entities": []}, {"text": "Each judge was asked to evaluate each clustering on a 1-5 scale.", "labels": [], "entities": []}, {"text": "We measured the absolute inter-annotator agreement, as in Section 3.1, and found high agreement in all cases (0.93 and 0.83 for the two systems, respectively, in restaurants; 0.85 for both in laptops).", "labels": [], "entities": [{"text": "agreement", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9878852963447571}]}, {"text": "16 shows the average human scores of the two systems for different granularities.", "labels": [], "entities": []}, {"text": "The judges considered the aspect groups always perfect or near-perfect when the gold similarity matrix of Phase A was used, but they found the aspect groups to be of rather poor quality when the similarity matrix of the best Phase A measure was used.", "labels": [], "entities": []}, {"text": "These results, along with those of show that more effort needs to be devoted to improving the similarity measures of Phase A, whereas Phase B is in effect an almost solved problem, if a good similarity matrix is available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An aspect term similarity matrix.", "labels": [], "entities": []}, {"text": " Table 2: Phase A results (Pearson correlation to  gold similarities) with and without sense pruning.", "labels": [], "entities": [{"text": "Pearson correlation to  gold similarities", "start_pos": 27, "end_pos": 68, "type": "METRIC", "confidence": 0.8796430110931397}]}, {"text": " Table 3: Phase A results (Pearson correlation to  gold similarities) of WNDS with SP against se- mantic similarity systems and human judges.", "labels": [], "entities": [{"text": "WNDS", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.47668927907943726}]}]}